!!python/object:huggingface_hub.community.DiscussionWithDetails
author: usrme
conflicting_files: null
created_at: 2023-04-05 02:32:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
      fullname: foo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: usrme
      type: user
    createdAt: '2023-04-05T03:32:02.000Z'
    data:
      edited: false
      editors:
      - usrme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
          fullname: foo
          isHf: false
          isPro: false
          name: usrme
          type: user
        html: '<p>this line went wrong:<br>model = AutoModel.from_pretrained("d:/chatglm-6b-int4-slim",
          trust_remote_code=True).half()#.quantize(4).cuda()</p>

          <p>RuntimeError: [enforce fail at ..\c10\core\impl\alloc_cpu.cpp:81] data.
          DefaultCPUAllocator: not enough memory: you tried to allocate 134217728
          bytes.</p>

          <p>but actually I have 16G ram, no gpu.<br>this model is supposed to run
          on 6G ram/cpu, how to handle it ?<br>thanks!</p>

          '
        raw: "this line went wrong:\r\nmodel = AutoModel.from_pretrained(\"d:/chatglm-6b-int4-slim\"\
          , trust_remote_code=True).half()#.quantize(4).cuda()\r\n\r\nRuntimeError:\
          \ [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator:\
          \ not enough memory: you tried to allocate 134217728 bytes.\r\n\r\nbut actually\
          \ I have 16G ram, no gpu. \r\nthis model is supposed to run on 6G ram/cpu,\
          \ how to handle it ?\r\nthanks!"
        updatedAt: '2023-04-05T03:32:02.382Z'
      numEdits: 0
      reactions: []
    id: 642cebb2c3684e5a4e821281
    type: comment
  author: usrme
  content: "this line went wrong:\r\nmodel = AutoModel.from_pretrained(\"d:/chatglm-6b-int4-slim\"\
    , trust_remote_code=True).half()#.quantize(4).cuda()\r\n\r\nRuntimeError: [enforce\
    \ fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not\
    \ enough memory: you tried to allocate 134217728 bytes.\r\n\r\nbut actually I\
    \ have 16G ram, no gpu. \r\nthis model is supposed to run on 6G ram/cpu, how to\
    \ handle it ?\r\nthanks!"
  created_at: 2023-04-05 02:32:02+00:00
  edited: false
  hidden: false
  id: 642cebb2c3684e5a4e821281
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
      fullname: Silver
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: silver
      type: user
    createdAt: '2023-04-05T08:06:56.000Z'
    data:
      edited: false
      editors:
      - silver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
          fullname: Silver
          isHf: false
          isPro: false
          name: silver
          type: user
        html: '<p>Maybe your memory have been consumed by other programs?</p>

          <p>You can try to use this one:</p>

          <p><a href="https://huggingface.co/silver/chatglm-6b-int4-qe-slim">https://huggingface.co/silver/chatglm-6b-int4-qe-slim</a></p>

          <p>It consums less memories</p>

          '
        raw: 'Maybe your memory have been consumed by other programs?


          You can try to use this one:


          https://huggingface.co/silver/chatglm-6b-int4-qe-slim


          It consums less memories'
        updatedAt: '2023-04-05T08:06:56.819Z'
      numEdits: 0
      reactions: []
    id: 642d2c2078f66d87767d2911
    type: comment
  author: silver
  content: 'Maybe your memory have been consumed by other programs?


    You can try to use this one:


    https://huggingface.co/silver/chatglm-6b-int4-qe-slim


    It consums less memories'
  created_at: 2023-04-05 07:06:56+00:00
  edited: false
  hidden: false
  id: 642d2c2078f66d87767d2911
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
      fullname: foo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: usrme
      type: user
    createdAt: '2023-04-06T08:16:25.000Z'
    data:
      edited: false
      editors:
      - usrme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
          fullname: foo
          isHf: false
          isPro: false
          name: usrme
          type: user
        html: '<p>sorry, i tried chatglm-6b-int4-qe-slim model ,but still the same
          error.<br>I just registered yesterday to post feedback, but platform forbid
          new users to frequently post messages, now<br>1 comment per day. so I have
          to wait a whole day to feedback.</p>

          <p>I tried the pure python code, given in the modelcard. no C code is involved.
          Is that the root cause of errors?<br>I''m pretty sure my laptop has sufficient
          RAM.<br>any other clues?<br>thanks.</p>

          '
        raw: 'sorry, i tried chatglm-6b-int4-qe-slim model ,but still the same error.

          I just registered yesterday to post feedback, but platform forbid new users
          to frequently post messages, now

          1 comment per day. so I have to wait a whole day to feedback.


          I tried the pure python code, given in the modelcard. no C code is involved.
          Is that the root cause of errors?

          I''m pretty sure my laptop has sufficient RAM.

          any other clues?

          thanks.'
        updatedAt: '2023-04-06T08:16:25.923Z'
      numEdits: 0
      reactions: []
    id: 642e7fd9dab00a4f233e249b
    type: comment
  author: usrme
  content: 'sorry, i tried chatglm-6b-int4-qe-slim model ,but still the same error.

    I just registered yesterday to post feedback, but platform forbid new users to
    frequently post messages, now

    1 comment per day. so I have to wait a whole day to feedback.


    I tried the pure python code, given in the modelcard. no C code is involved. Is
    that the root cause of errors?

    I''m pretty sure my laptop has sufficient RAM.

    any other clues?

    thanks.'
  created_at: 2023-04-06 07:16:25+00:00
  edited: false
  hidden: false
  id: 642e7fd9dab00a4f233e249b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
      fullname: Silver
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: silver
      type: user
    createdAt: '2023-04-06T08:43:27.000Z'
    data:
      edited: false
      editors:
      - silver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
          fullname: Silver
          isHf: false
          isPro: false
          name: silver
          type: user
        html: '<p>Could please try to print out how much memories are left before
          loading the model?</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          psutil

          psutil.virtual_memory()

          </code></pre>

          '
        raw: 'Could please try to print out how much memories are left before loading
          the model?


          ```python

          import psutil

          psutil.virtual_memory()

          ```'
        updatedAt: '2023-04-06T08:43:27.571Z'
      numEdits: 0
      reactions: []
    id: 642e862f72c805c1cb307d03
    type: comment
  author: silver
  content: 'Could please try to print out how much memories are left before loading
    the model?


    ```python

    import psutil

    psutil.virtual_memory()

    ```'
  created_at: 2023-04-06 07:43:27+00:00
  edited: false
  hidden: false
  id: 642e862f72c805c1cb307d03
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
      fullname: foo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: usrme
      type: user
    createdAt: '2023-04-06T08:56:15.000Z'
    data:
      edited: false
      editors:
      - usrme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
          fullname: foo
          isHf: false
          isPro: false
          name: usrme
          type: user
        html: "<p>2023-04-06 16:55:27.196480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
          \ Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll\
          \ not found<br>2023-04-06 16:55:27.196696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29]\
          \ Ignore above cudart dlerror if you do not have a GPU set up on your machine.<br>svmem(total=16727375872,\
          \ available=7232872448, percent=56.8, used=9494503424, free=7232872448)<br>Explicitly\
          \ passing a <code>revision</code> is encouraged when loading a model with\
          \ custom code to ensure no malicious code has been contributed in a newer\
          \ revision.<br>ChatGLMTokenizer(name_or_path='d:/chatglm-6b-int4-slim',\
          \ vocab_size=130344, model_max_length=1000000000000000019884624838656, is_fast=False,\
          \ padding_side='left', truncation_side='right', special_tokens={'bos_token':\
          \ '', 'eos_token': '', 'unk_token': '', 'pad_token': '', 'mask_token': '[MASK]'})\
          \ \u2588<br>Explicitly passing a <code>revision</code> is encouraged when\
          \ loading a configuration with custom code to ensure no malicious code has\
          \ been contributed in a newer revision.<br>Explicitly passing a <code>revision</code>\
          \ is encouraged when loading a model with custom code to ensure no malicious\
          \ code has been contributed in a newer revision.<br>Traceback (most recent\
          \ call last):<br>  File \"D:\\chatglm-6b-int4-slim\\run.py\", line 6, in\
          \ <br>    model = AutoModel.from_pretrained(\"d:/chatglm-6b-int4-slim\"\
          , trust_remote_code=True).half()#.quantize(4).cuda()<br>  File \"D:\\soft\\\
          prog\\python\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
          , line 459, in from_pretrained<br>    return model_class.from_pretrained(<br>\
          \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 2362, in from_pretrained<br>    model = cls(config, *model_args,\
          \ **model_kwargs)<br>  File \"C:\\Users\\10020252/.cache\\huggingface\\\
          modules\\transformers_modules\\local\\modeling_chatglm.py\", line 1019,\
          \ in <strong>init</strong><br>    self.transformer = ChatGLMModel(config)<br>\
          \  File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
          local\\modeling_chatglm.py\", line 825, in <strong>init</strong><br>   \
          \ [get_layer(layer_id) for layer_id in range(self.num_layers)]<br>  File\
          \ \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
          local\\modeling_chatglm.py\", line 825, in <br>    [get_layer(layer_id)\
          \ for layer_id in range(self.num_layers)]<br>  File \"C:\\Users\\10020252/.cache\\\
          huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py\"\
          , line 811, in get_layer<br>    return GLMBlock(<br>  File \"C:\\Users\\\
          10020252/.cache\\huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py\"\
          , line 586, in <strong>init</strong><br>    self.mlp = GLU(<br>  File \"\
          C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
          local\\modeling_chatglm.py\", line 513, in <strong>init</strong><br>   \
          \ self.dense_h_to_4h = skip_init(<br>  File \"D:\\soft\\prog\\python\\lib\\\
          site-packages\\torch\\nn\\utils\\init.py\", line 51, in skip_init<br>  \
          \  return module_cls(*args, **kwargs).to_empty(device=final_device)<br>\
          \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 788, in to_empty<br>    return self._apply(lambda t: torch.empty_like(t,\
          \ device=device))<br>  File \"D:\\soft\\prog\\python\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 601, in _apply<br>    param_applied\
          \ = fn(param)<br>  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 788, in <br>    return self._apply(lambda\
          \ t: torch.empty_like(t, device=device))<br>RuntimeError: [enforce fail\
          \ at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not\
          \ enough memory: you tried to allocate 134217728 bytes.<br>\u8BF7\u6309\u4EFB\
          \u610F\u952E\u7EE7\u7EED. . .</p>\n"
        raw: "2023-04-06 16:55:27.196480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
          \ Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll\
          \ not found\n2023-04-06 16:55:27.196696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29]\
          \ Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\
          svmem(total=16727375872, available=7232872448, percent=56.8, used=9494503424,\
          \ free=7232872448)\nExplicitly passing a `revision` is encouraged when loading\
          \ a model with custom code to ensure no malicious code has been contributed\
          \ in a newer revision.\nChatGLMTokenizer(name_or_path='d:/chatglm-6b-int4-slim',\
          \ vocab_size=130344, model_max_length=1000000000000000019884624838656, is_fast=False,\
          \ padding_side='left', truncation_side='right', special_tokens={'bos_token':\
          \ '<sop>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>',\
          \ 'mask_token': '[MASK]'}) \u2588\nExplicitly passing a `revision` is encouraged\
          \ when loading a configuration with custom code to ensure no malicious code\
          \ has been contributed in a newer revision.\nExplicitly passing a `revision`\
          \ is encouraged when loading a model with custom code to ensure no malicious\
          \ code has been contributed in a newer revision.\nTraceback (most recent\
          \ call last):\n  File \"D:\\chatglm-6b-int4-slim\\run.py\", line 6, in <module>\n\
          \    model = AutoModel.from_pretrained(\"d:/chatglm-6b-int4-slim\", trust_remote_code=True).half()#.quantize(4).cuda()\n\
          \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\transformers\\models\\\
          auto\\auto_factory.py\", line 459, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 2362, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n\
          \  File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
          local\\modeling_chatglm.py\", line 1019, in __init__\n    self.transformer\
          \ = ChatGLMModel(config)\n  File \"C:\\Users\\10020252/.cache\\huggingface\\\
          modules\\transformers_modules\\local\\modeling_chatglm.py\", line 825, in\
          \ __init__\n    [get_layer(layer_id) for layer_id in range(self.num_layers)]\n\
          \  File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
          local\\modeling_chatglm.py\", line 825, in <listcomp>\n    [get_layer(layer_id)\
          \ for layer_id in range(self.num_layers)]\n  File \"C:\\Users\\10020252/.cache\\\
          huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py\"\
          , line 811, in get_layer\n    return GLMBlock(\n  File \"C:\\Users\\10020252/.cache\\\
          huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py\"\
          , line 586, in __init__\n    self.mlp = GLU(\n  File \"C:\\Users\\10020252/.cache\\\
          huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py\"\
          , line 513, in __init__\n    self.dense_h_to_4h = skip_init(\n  File \"\
          D:\\soft\\prog\\python\\lib\\site-packages\\torch\\nn\\utils\\init.py\"\
          , line 51, in skip_init\n    return module_cls(*args, **kwargs).to_empty(device=final_device)\n\
          \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 788, in to_empty\n    return self._apply(lambda t: torch.empty_like(t,\
          \ device=device))\n  File \"D:\\soft\\prog\\python\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 601, in _apply\n    param_applied\
          \ = fn(param)\n  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 788, in <lambda>\n    return self._apply(lambda\
          \ t: torch.empty_like(t, device=device))\nRuntimeError: [enforce fail at\
          \ ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not\
          \ enough memory: you tried to allocate 134217728 bytes.\n\u8BF7\u6309\u4EFB\
          \u610F\u952E\u7EE7\u7EED. . ."
        updatedAt: '2023-04-06T08:56:15.697Z'
      numEdits: 0
      reactions: []
    id: 642e892f7ea623215d8f3c33
    type: comment
  author: usrme
  content: "2023-04-06 16:55:27.196480: W tensorflow/stream_executor/platform/default/dso_loader.cc:64]\
    \ Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll\
    \ not found\n2023-04-06 16:55:27.196696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29]\
    \ Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\
    svmem(total=16727375872, available=7232872448, percent=56.8, used=9494503424,\
    \ free=7232872448)\nExplicitly passing a `revision` is encouraged when loading\
    \ a model with custom code to ensure no malicious code has been contributed in\
    \ a newer revision.\nChatGLMTokenizer(name_or_path='d:/chatglm-6b-int4-slim',\
    \ vocab_size=130344, model_max_length=1000000000000000019884624838656, is_fast=False,\
    \ padding_side='left', truncation_side='right', special_tokens={'bos_token': '<sop>',\
    \ 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token':\
    \ '[MASK]'}) \u2588\nExplicitly passing a `revision` is encouraged when loading\
    \ a configuration with custom code to ensure no malicious code has been contributed\
    \ in a newer revision.\nExplicitly passing a `revision` is encouraged when loading\
    \ a model with custom code to ensure no malicious code has been contributed in\
    \ a newer revision.\nTraceback (most recent call last):\n  File \"D:\\chatglm-6b-int4-slim\\\
    run.py\", line 6, in <module>\n    model = AutoModel.from_pretrained(\"d:/chatglm-6b-int4-slim\"\
    , trust_remote_code=True).half()#.quantize(4).cuda()\n  File \"D:\\soft\\prog\\\
    python\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line\
    \ 459, in from_pretrained\n    return model_class.from_pretrained(\n  File \"\
    D:\\soft\\prog\\python\\lib\\site-packages\\transformers\\modeling_utils.py\"\
    , line 2362, in from_pretrained\n    model = cls(config, *model_args, **model_kwargs)\n\
    \  File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
    local\\modeling_chatglm.py\", line 1019, in __init__\n    self.transformer = ChatGLMModel(config)\n\
    \  File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
    local\\modeling_chatglm.py\", line 825, in __init__\n    [get_layer(layer_id)\
    \ for layer_id in range(self.num_layers)]\n  File \"C:\\Users\\10020252/.cache\\\
    huggingface\\modules\\transformers_modules\\local\\modeling_chatglm.py\", line\
    \ 825, in <listcomp>\n    [get_layer(layer_id) for layer_id in range(self.num_layers)]\n\
    \  File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
    local\\modeling_chatglm.py\", line 811, in get_layer\n    return GLMBlock(\n \
    \ File \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\\
    local\\modeling_chatglm.py\", line 586, in __init__\n    self.mlp = GLU(\n  File\
    \ \"C:\\Users\\10020252/.cache\\huggingface\\modules\\transformers_modules\\local\\\
    modeling_chatglm.py\", line 513, in __init__\n    self.dense_h_to_4h = skip_init(\n\
    \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\nn\\utils\\init.py\"\
    , line 51, in skip_init\n    return module_cls(*args, **kwargs).to_empty(device=final_device)\n\
    \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
    , line 788, in to_empty\n    return self._apply(lambda t: torch.empty_like(t,\
    \ device=device))\n  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 601, in _apply\n    param_applied = fn(param)\n\
    \  File \"D:\\soft\\prog\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
    , line 788, in <lambda>\n    return self._apply(lambda t: torch.empty_like(t,\
    \ device=device))\nRuntimeError: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81]\
    \ data. DefaultCPUAllocator: not enough memory: you tried to allocate 134217728\
    \ bytes.\n\u8BF7\u6309\u4EFB\u610F\u952E\u7EE7\u7EED. . ."
  created_at: 2023-04-06 07:56:15+00:00
  edited: false
  hidden: false
  id: 642e892f7ea623215d8f3c33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
      fullname: Silver
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: silver
      type: user
    createdAt: '2023-04-06T13:42:33.000Z'
    data:
      edited: true
      editors:
      - silver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
          fullname: Silver
          isHf: false
          isPro: false
          name: silver
          type: user
        html: '<pre><code>svmem(total=16727375872, available=7232872448, percent=56.8,
          used=9494503424, free=7232872448)

          </code></pre>

          <p>You have about 6.7G free memory to use before loading the model. You
          better use machines with larger memories or free up more memories before
          loading the model.</p>

          '
        raw: '```

          svmem(total=16727375872, available=7232872448, percent=56.8, used=9494503424,
          free=7232872448)

          ```


          You have about 6.7G free memory to use before loading the model. You better
          use machines with larger memories or free up more memories before loading
          the model.'
        updatedAt: '2023-04-06T13:44:27.183Z'
      numEdits: 2
      reactions: []
    id: 642ecc4951dd583377f17200
    type: comment
  author: silver
  content: '```

    svmem(total=16727375872, available=7232872448, percent=56.8, used=9494503424,
    free=7232872448)

    ```


    You have about 6.7G free memory to use before loading the model. You better use
    machines with larger memories or free up more memories before loading the model.'
  created_at: 2023-04-06 12:42:33+00:00
  edited: true
  hidden: false
  id: 642ecc4951dd583377f17200
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
      fullname: foo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: usrme
      type: user
    createdAt: '2023-04-07T01:38:09.000Z'
    data:
      edited: false
      editors:
      - usrme
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/960fb813571c0922b9d64ea24b524595.svg
          fullname: foo
          isHf: false
          isPro: false
          name: usrme
          type: user
        html: '<p>thanks a lot~ seems that poor guys have little chances to try these
          modern toys.. :(<br>issue closed.</p>

          '
        raw: 'thanks a lot~ seems that poor guys have little chances to try these
          modern toys.. :(

          issue closed.'
        updatedAt: '2023-04-07T01:38:09.048Z'
      numEdits: 0
      reactions: []
    id: 642f7401084ea70b373c8aaf
    type: comment
  author: usrme
  content: 'thanks a lot~ seems that poor guys have little chances to try these modern
    toys.. :(

    issue closed.'
  created_at: 2023-04-07 00:38:09+00:00
  edited: false
  hidden: false
  id: 642f7401084ea70b373c8aaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653978482840-5fa0fa89a13e063b8b2b5d1e.jpeg?w=200&h=200&f=face
      fullname: Silver
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: silver
      type: user
    createdAt: '2023-04-10T13:09:03.000Z'
    data:
      status: closed
    id: 64340a6f1a1ba6b55b16d4b9
    type: status-change
  author: silver
  created_at: 2023-04-10 12:09:03+00:00
  id: 64340a6f1a1ba6b55b16d4b9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: silver/chatglm-6b-int4-slim
repo_type: model
status: closed
target_branch: null
title: error loading model
