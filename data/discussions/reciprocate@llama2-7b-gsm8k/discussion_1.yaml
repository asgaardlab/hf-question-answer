!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ekurtic
conflicting_files: null
created_at: 2023-09-28 14:22:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668801593252-628e0ce4e53bbd334577fcb0.jpeg?w=200&h=200&f=face
      fullname: Eldar Kurtic
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ekurtic
      type: user
    createdAt: '2023-09-28T15:22:23.000Z'
    data:
      edited: false
      editors:
      - ekurtic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8316999673843384
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668801593252-628e0ce4e53bbd334577fcb0.jpeg?w=200&h=200&f=face
          fullname: Eldar Kurtic
          isHf: false
          isPro: false
          name: ekurtic
          type: user
        html: '<p>Hello, thanks for sharing the model. Could you please provide more
          info about the finetuning recipe (LR, num-epochs, batch-size) on GSM8k dataset?<br>And
          also, have you used only GSM8k for finetuning or have you combined it with
          some other data (e.g. MathQA)?  </p>

          '
        raw: "Hello, thanks for sharing the model. Could you please provide more info\
          \ about the finetuning recipe (LR, num-epochs, batch-size) on GSM8k dataset?\r\
          \nAnd also, have you used only GSM8k for finetuning or have you combined\
          \ it with some other data (e.g. MathQA)?  "
        updatedAt: '2023-09-28T15:22:23.849Z'
      numEdits: 0
      reactions: []
    id: 65159a2f99e8060f1e745355
    type: comment
  author: ekurtic
  content: "Hello, thanks for sharing the model. Could you please provide more info\
    \ about the finetuning recipe (LR, num-epochs, batch-size) on GSM8k dataset?\r\
    \nAnd also, have you used only GSM8k for finetuning or have you combined it with\
    \ some other data (e.g. MathQA)?  "
  created_at: 2023-09-28 14:22:23+00:00
  edited: false
  hidden: false
  id: 65159a2f99e8060f1e745355
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: reciprocate/llama2-7b-gsm8k
repo_type: model
status: open
target_branch: null
title: Finetuning recipe
