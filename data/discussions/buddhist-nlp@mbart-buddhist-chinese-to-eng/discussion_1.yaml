!!python/object:huggingface_hub.community.DiscussionWithDetails
author: johnarmstrong
conflicting_files: null
created_at: 2022-08-28 13:34:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-08-28T14:34:15.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: "<p>I\u2019ve been very excited about your CBETA translation project\
          \ since I learned of it your ANNOUNCEMENT posted in the FB Digital Humanities\
          \ group a few months ago have been looking forward to seeing it up on HuggingFace.\
          \  I\u2019ve already looked for it a few times and didn\u2019t see it but\
          \ figured I might have missed it somehow.   But I looked today and there\
          \ it was!</p>\n<p>I figured it would be some form of encoder-decoder but\
          \ I wasn\u2019t sure what kind.  Now I see it\u2019s MBART \u2013 seems\
          \ like an excellent choice!</p>\n<p>Is this discussion page a good place\
          \ to ask questions, or would you recommend a better one?</p>\n"
        raw: "I\u2019ve been very excited about your CBETA translation project since\
          \ I learned of it your ANNOUNCEMENT posted in the FB Digital Humanities\
          \ group a few months ago have been looking forward to seeing it up on HuggingFace.\
          \  I\u2019ve already looked for it a few times and didn\u2019t see it but\
          \ figured I might have missed it somehow.   But I looked today and there\
          \ it was!\r\n\r\nI figured it would be some form of encoder-decoder but\
          \ I wasn\u2019t sure what kind.  Now I see it\u2019s MBART \u2013 seems\
          \ like an excellent choice!\r\n\r\nIs this discussion page a good place\
          \ to ask questions, or would you recommend a better one?\r\n"
        updatedAt: '2022-08-28T14:34:15.302Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - snehrdich
      - count: 1
        reaction: "\U0001F917"
        users:
        - victor
    id: 630b7ce7cd26ad7f60d7ffba
    type: comment
  author: johnarmstrong
  content: "I\u2019ve been very excited about your CBETA translation project since\
    \ I learned of it your ANNOUNCEMENT posted in the FB Digital Humanities group\
    \ a few months ago have been looking forward to seeing it up on HuggingFace. \
    \ I\u2019ve already looked for it a few times and didn\u2019t see it but figured\
    \ I might have missed it somehow.   But I looked today and there it was!\r\n\r\
    \nI figured it would be some form of encoder-decoder but I wasn\u2019t sure what\
    \ kind.  Now I see it\u2019s MBART \u2013 seems like an excellent choice!\r\n\r\
    \nIs this discussion page a good place to ask questions, or would you recommend\
    \ a better one?\r\n"
  created_at: 2022-08-28 13:34:15+00:00
  edited: false
  hidden: false
  id: 630b7ce7cd26ad7f60d7ffba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
      fullname: Sebastian Nehrdich
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: snehrdich
      type: user
    createdAt: '2022-08-28T14:55:51.000Z'
    data:
      edited: false
      editors:
      - snehrdich
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
          fullname: Sebastian Nehrdich
          isHf: false
          isPro: false
          name: snehrdich
          type: user
        html: '<p>Hi John, </p>

          <p>Thank you very much for your message! Indeed we uploaded the model just
          today since there have been (and still are) some issues with the model behavior.
          Most of it is sorted out now, some tasks remain to be solved in the future.<br>Sure
          feel free to ask any questions that you might have here!<br>Best, </p>

          <p>Sebastian</p>

          '
        raw: "Hi John, \n\nThank you very much for your message! Indeed we uploaded\
          \ the model just today since there have been (and still are) some issues\
          \ with the model behavior. Most of it is sorted out now, some tasks remain\
          \ to be solved in the future.\nSure feel free to ask any questions that\
          \ you might have here!\nBest, \n\nSebastian"
        updatedAt: '2022-08-28T14:55:51.356Z'
      numEdits: 0
      reactions: []
    id: 630b81f7e67c604e9b7c9947
    type: comment
  author: snehrdich
  content: "Hi John, \n\nThank you very much for your message! Indeed we uploaded\
    \ the model just today since there have been (and still are) some issues with\
    \ the model behavior. Most of it is sorted out now, some tasks remain to be solved\
    \ in the future.\nSure feel free to ask any questions that you might have here!\n\
    Best, \n\nSebastian"
  created_at: 2022-08-28 13:55:51+00:00
  edited: false
  hidden: false
  id: 630b81f7e67c604e9b7c9947
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-08-28T15:32:09.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: "<p>Hi Sebastian,</p>\n<p>It\u2019s great to know you welcome questions.\
          \  Here\u2019s a couple from me.:)</p>\n<p>I\u2019m just starting to explore\
          \ translation with transformers and think in terms of the original encoder-decoder\
          \ architecture of the 2017 All You Need is Attention paper and think of\
          \ it as the ultimate baseline.  Do you know how MBART perform compares to\
          \ it when trained on the same sentence-pair dataset.  </p>\n<p>Also, do\
          \ you know if it\u2019s it possible to use non-sentence-pair as well as\
          \ sentence pair training data to improve the modeling of source language\
          \ in the encoder and of the target language in the decoder?  Maybe in separate\
          \ pre-training and fine-tuning steps?</p>\n<p>-- John</p>\n"
        raw: "Hi Sebastian,\n\nIt\u2019s great to know you welcome questions.  Here\u2019\
          s a couple from me.:)\n\nI\u2019m just starting to explore translation with\
          \ transformers and think in terms of the original encoder-decoder architecture\
          \ of the 2017 All You Need is Attention paper and think of it as the ultimate\
          \ baseline.  Do you know how MBART perform compares to it when trained on\
          \ the same sentence-pair dataset.  \n\nAlso, do you know if it\u2019s it\
          \ possible to use non-sentence-pair as well as sentence pair training data\
          \ to improve the modeling of source language in the encoder and of the target\
          \ language in the decoder?  Maybe in separate pre-training and fine-tuning\
          \ steps?\n\n-- John"
        updatedAt: '2022-08-28T15:32:09.357Z'
      numEdits: 0
      reactions: []
    id: 630b8a7979d18d5e53e58f99
    type: comment
  author: johnarmstrong
  content: "Hi Sebastian,\n\nIt\u2019s great to know you welcome questions.  Here\u2019\
    s a couple from me.:)\n\nI\u2019m just starting to explore translation with transformers\
    \ and think in terms of the original encoder-decoder architecture of the 2017\
    \ All You Need is Attention paper and think of it as the ultimate baseline.  Do\
    \ you know how MBART perform compares to it when trained on the same sentence-pair\
    \ dataset.  \n\nAlso, do you know if it\u2019s it possible to use non-sentence-pair\
    \ as well as sentence pair training data to improve the modeling of source language\
    \ in the encoder and of the target language in the decoder?  Maybe in separate\
    \ pre-training and fine-tuning steps?\n\n-- John"
  created_at: 2022-08-28 14:32:09+00:00
  edited: false
  hidden: false
  id: 630b8a7979d18d5e53e58f99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-08-28T23:30:36.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: "<p>As to my second question, about using unpaired data to supplement\
          \ paired data, it makes it clear that training MBART is a two-step training\
          \ process consisting of (1) pre-training on a multilingual set of unpaired\
          \ language-tagged sentences  in autoencoder fashion, followed by (2) fine-tuning\
          \ on a set of paired language-tagged sentences like the original 2017 encoder-decoder.\
          \  The authors note that the improvement gotten from fine-tuning goes down\
          \ as the number of paired sentences gets larger and can ultimately \u201C\
          wash out\u201D the effects of the pre-training, but is usually significant\
          \ in the typical use case where the amount of pair data is modest in comparison\
          \ with that of the unpaired data.</p>\n<p>As to my first question, about\
          \ performance of MBART compared to the original transformer, the authors\
          \ don\u2019t seem to address it directly.  But they do use for their baselines\
          \ a pre-trained MBART model initialized with random parameters and not changed\
          \ by pre-training, which they called Random, and then fine-tuned with paired\
          \ data.  The performance of the Random model starts out at 0 but goes up\
          \ with fine-tuning and with enough pair data can get into the same performance\
          \ range as a real pre-training model with the same amount of fine-tuning\
          \ and ultimately match it.  I\u2019m thinking that their Random MBART model\
          \ is essentially equivalent to an original transformer model.  If so, the\
          \ original model would show similar performance (plus or minus other improvements\
          \ in MBART) to the Random model.  But of more practical significance, it\
          \ means that an original transfer model trained with a given set of source-target\
          \ pair data will not perform as well as an MBART model pre-trained with\
          \ unpaired data for the source and target languages and fine-tuned with\
          \ the same pair data as the original model.</p>\n<p>I\u2019m not sure I\
          \ have all this right, but given that you created your Linguae Dharmae model\
          \ using MBART, I\u2019m curious: did you include unpaired Buddhist Classical\
          \ Chinese and Buddhist translation English (or any Chinese and any English\
          \ data for that matter) beyond what\u2019s in the paired data in your multilingual\
          \ pre-training data?  I understand that you may not want to talk about your\
          \ system at this level of detail in this venue, but I\u2019d be interested\
          \ to hear anything you have to say on this topic.  </p>\n<p>-- John</p>\n"
        raw: "As to my second question, about using unpaired data to supplement paired\
          \ data, it makes it clear that training MBART is a two-step training process\
          \ consisting of (1) pre-training on a multilingual set of unpaired language-tagged\
          \ sentences  in autoencoder fashion, followed by (2) fine-tuning on a set\
          \ of paired language-tagged sentences like the original 2017 encoder-decoder.\
          \  The authors note that the improvement gotten from fine-tuning goes down\
          \ as the number of paired sentences gets larger and can ultimately \u201C\
          wash out\u201D the effects of the pre-training, but is usually significant\
          \ in the typical use case where the amount of pair data is modest in comparison\
          \ with that of the unpaired data.\n\nAs to my first question, about performance\
          \ of MBART compared to the original transformer, the authors don\u2019t\
          \ seem to address it directly.  But they do use for their baselines a pre-trained\
          \ MBART model initialized with random parameters and not changed by pre-training,\
          \ which they called Random, and then fine-tuned with paired data.  The performance\
          \ of the Random model starts out at 0 but goes up with fine-tuning and with\
          \ enough pair data can get into the same performance range as a real pre-training\
          \ model with the same amount of fine-tuning and ultimately match it.  I\u2019\
          m thinking that their Random MBART model is essentially equivalent to an\
          \ original transformer model.  If so, the original model would show similar\
          \ performance (plus or minus other improvements in MBART) to the Random\
          \ model.  But of more practical significance, it means that an original\
          \ transfer model trained with a given set of source-target pair data will\
          \ not perform as well as an MBART model pre-trained with unpaired data for\
          \ the source and target languages and fine-tuned with the same pair data\
          \ as the original model.\n\nI\u2019m not sure I have all this right, but\
          \ given that you created your Linguae Dharmae model using MBART, I\u2019\
          m curious: did you include unpaired Buddhist Classical Chinese and Buddhist\
          \ translation English (or any Chinese and any English data for that matter)\
          \ beyond what\u2019s in the paired data in your multilingual pre-training\
          \ data?  I understand that you may not want to talk about your system at\
          \ this level of detail in this venue, but I\u2019d be interested to hear\
          \ anything you have to say on this topic.  \n\n-- John"
        updatedAt: '2022-08-28T23:30:36.106Z'
      numEdits: 0
      reactions: []
    id: 630bfa9cd73c0a1dcf58b433
    type: comment
  author: johnarmstrong
  content: "As to my second question, about using unpaired data to supplement paired\
    \ data, it makes it clear that training MBART is a two-step training process consisting\
    \ of (1) pre-training on a multilingual set of unpaired language-tagged sentences\
    \  in autoencoder fashion, followed by (2) fine-tuning on a set of paired language-tagged\
    \ sentences like the original 2017 encoder-decoder.  The authors note that the\
    \ improvement gotten from fine-tuning goes down as the number of paired sentences\
    \ gets larger and can ultimately \u201Cwash out\u201D the effects of the pre-training,\
    \ but is usually significant in the typical use case where the amount of pair\
    \ data is modest in comparison with that of the unpaired data.\n\nAs to my first\
    \ question, about performance of MBART compared to the original transformer, the\
    \ authors don\u2019t seem to address it directly.  But they do use for their baselines\
    \ a pre-trained MBART model initialized with random parameters and not changed\
    \ by pre-training, which they called Random, and then fine-tuned with paired data.\
    \  The performance of the Random model starts out at 0 but goes up with fine-tuning\
    \ and with enough pair data can get into the same performance range as a real\
    \ pre-training model with the same amount of fine-tuning and ultimately match\
    \ it.  I\u2019m thinking that their Random MBART model is essentially equivalent\
    \ to an original transformer model.  If so, the original model would show similar\
    \ performance (plus or minus other improvements in MBART) to the Random model.\
    \  But of more practical significance, it means that an original transfer model\
    \ trained with a given set of source-target pair data will not perform as well\
    \ as an MBART model pre-trained with unpaired data for the source and target languages\
    \ and fine-tuned with the same pair data as the original model.\n\nI\u2019m not\
    \ sure I have all this right, but given that you created your Linguae Dharmae\
    \ model using MBART, I\u2019m curious: did you include unpaired Buddhist Classical\
    \ Chinese and Buddhist translation English (or any Chinese and any English data\
    \ for that matter) beyond what\u2019s in the paired data in your multilingual\
    \ pre-training data?  I understand that you may not want to talk about your system\
    \ at this level of detail in this venue, but I\u2019d be interested to hear anything\
    \ you have to say on this topic.  \n\n-- John"
  created_at: 2022-08-28 22:30:36+00:00
  edited: false
  hidden: false
  id: 630bfa9cd73c0a1dcf58b433
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-08-29T01:31:37.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: '<p>A P.S. to my previous message.  I realize after looking at your
          repo and the linked huggingface docs that the most likely answer as to what
          pre-training data you used was the data embodied in the already pretrained
          hf MBART model (or more likely the MBART-50 model to judge from your tokenizer
          config ) and that you did not need to do any pre-training at all.  (Though
          maybe you could have done an additional pre-training checkpoint adding Buddhist
          or other Classical Chinese texts to the zh_CN data in the model.  The fact
          that you are depending on the pre-trained zh_CN = PRC Chinese data  for
          your Chinese modeling fits with the instruction in your README.md to make
          sure the  Chinese input data you pass to the inference engine is in Simplified
          = PRC spelling.)</p>

          '
        raw: A P.S. to my previous message.  I realize after looking at your repo
          and the linked huggingface docs that the most likely answer as to what pre-training
          data you used was the data embodied in the already pretrained hf MBART model
          (or more likely the MBART-50 model to judge from your tokenizer config )
          and that you did not need to do any pre-training at all.  (Though maybe
          you could have done an additional pre-training checkpoint adding Buddhist
          or other Classical Chinese texts to the zh_CN data in the model.  The fact
          that you are depending on the pre-trained zh_CN = PRC Chinese data  for
          your Chinese modeling fits with the instruction in your README.md to make
          sure the  Chinese input data you pass to the inference engine is in Simplified
          = PRC spelling.)
        updatedAt: '2022-08-29T01:31:37.253Z'
      numEdits: 0
      reactions: []
    id: 630c16f9a20a53678121584b
    type: comment
  author: johnarmstrong
  content: A P.S. to my previous message.  I realize after looking at your repo and
    the linked huggingface docs that the most likely answer as to what pre-training
    data you used was the data embodied in the already pretrained hf MBART model (or
    more likely the MBART-50 model to judge from your tokenizer config ) and that
    you did not need to do any pre-training at all.  (Though maybe you could have
    done an additional pre-training checkpoint adding Buddhist or other Classical
    Chinese texts to the zh_CN data in the model.  The fact that you are depending
    on the pre-trained zh_CN = PRC Chinese data  for your Chinese modeling fits with
    the instruction in your README.md to make sure the  Chinese input data you pass
    to the inference engine is in Simplified = PRC spelling.)
  created_at: 2022-08-29 00:31:37+00:00
  edited: false
  hidden: false
  id: 630c16f9a20a53678121584b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
      fullname: Sebastian Nehrdich
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: snehrdich
      type: user
    createdAt: '2022-08-31T09:52:02.000Z'
    data:
      edited: false
      editors:
      - snehrdich
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
          fullname: Sebastian Nehrdich
          isHf: false
          isPro: false
          name: snehrdich
          type: user
        html: '<p>Hi John, </p>

          <p>Regarding the question of performance of a vanilla transformer vs. pretrained
          encoder-decoder models such as MT5 or MBART, the consensus here is that
          for low resource languages (&lt;1M sentence paris training), the pretrained
          models can perform better (always depends on the individual case of course).
          Once enough training data is available, the difference diminishes.<br> To
          my understanding as well, an MBART model initialized with random parameters
          is essentially just a Transformer.<br>The current Linguae Dharmae Chinese-&gt;English
          model has not seen any additional monolingual pretraining for Chinese, English
          or any other language, but we are working on this.<br>Indeed we finetuned
          the MBART-50-many-to-one model. We are preparing a detailed paper where
          we will compare all the different settings, parameters etc. that we have
          tried so far.<br>And yes we use simplified Chinese since that is what the
          tokenizers of the pretrained models generally use.</p>

          '
        raw: "Hi John, \n\nRegarding the question of performance of a vanilla transformer\
          \ vs. pretrained encoder-decoder models such as MT5 or MBART, the consensus\
          \ here is that for low resource languages (<1M sentence paris training),\
          \ the pretrained models can perform better (always depends on the individual\
          \ case of course). Once enough training data is available, the difference\
          \ diminishes.\n To my understanding as well, an MBART model initialized\
          \ with random parameters is essentially just a Transformer.\nThe current\
          \ Linguae Dharmae Chinese->English model has not seen any additional monolingual\
          \ pretraining for Chinese, English or any other language, but we are working\
          \ on this. \nIndeed we finetuned the MBART-50-many-to-one model. We are\
          \ preparing a detailed paper where we will compare all the different settings,\
          \ parameters etc. that we have tried so far.   \nAnd yes we use simplified\
          \ Chinese since that is what the tokenizers of the pretrained models generally\
          \ use."
        updatedAt: '2022-08-31T09:52:02.195Z'
      numEdits: 0
      reactions: []
    id: 630f2f424dbbbb6f667e31eb
    type: comment
  author: snehrdich
  content: "Hi John, \n\nRegarding the question of performance of a vanilla transformer\
    \ vs. pretrained encoder-decoder models such as MT5 or MBART, the consensus here\
    \ is that for low resource languages (<1M sentence paris training), the pretrained\
    \ models can perform better (always depends on the individual case of course).\
    \ Once enough training data is available, the difference diminishes.\n To my understanding\
    \ as well, an MBART model initialized with random parameters is essentially just\
    \ a Transformer.\nThe current Linguae Dharmae Chinese->English model has not seen\
    \ any additional monolingual pretraining for Chinese, English or any other language,\
    \ but we are working on this. \nIndeed we finetuned the MBART-50-many-to-one model.\
    \ We are preparing a detailed paper where we will compare all the different settings,\
    \ parameters etc. that we have tried so far.   \nAnd yes we use simplified Chinese\
    \ since that is what the tokenizers of the pretrained models generally use."
  created_at: 2022-08-31 08:52:02+00:00
  edited: false
  hidden: false
  id: 630f2f424dbbbb6f667e31eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-09-04T00:15:17.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: "<p>Hi Sebastian,</p>\n<p>Apologies for not seeing your comment and\
          \ responding to it sooner.  Thanks for sanity checking my previous suppositions\
          \ and for sharing information about your training.</p>\n<p>I am excited\
          \ to hear that you and your team are working on the paper you describe and\
          \ look forward to reading it when it comes out.  I hope you will consider\
          \ putting it in an accessible online place and including a link to it in\
          \ your model card.</p>\n<p>I\u2019m very interested in the potential of\
          \ Deep Learning to play a significant role in Digital Humanities, especially\
          \ in fields in which texts are abundant and a major focus of study.   Your\
          \ Linguae Dharmae project belongs to two such fields, Sinology and  Buddhist\
          \ Studies, and is probably the first or second practical Deep Learning-based\
          \ NLP application in both fields.  (I believe some systems now used for\
          \ Chinese OCR and possibly also character component analysis are also Deep\
          \ Learning-based, though they may not use transformers.) </p>\n<p>But IMO\
          \ your project can also claim a place the field of AI research, on the grounds\
          \ that you, as \u201Cdomain experts\u201D, are able to evaluate the performance\
          \ of your system at a much more sophisticated  level than  the dominant\
          \ practice within that field, which mostly involves running standard benchmarks\
          \ with standard tasks and datasets and evaluating the results with standard\
          \ metrics (in your case BLEU).  </p>\n<p>You have already identified recurring\
          \ problems (for example relating to personal and place names), and for specific\
          \ changes that improve performance.  Even if you stay with your current\
          \ architecture and limit your experiments to changes in parameters and training\
          \ data, your findings should be of interest to AI researchers, and could\
          \ even inspire them to make architectural changes that will give better\
          \ results \u201Cout of the box\u201D.  Or you could move on to experimenting\
          \ with changes in the architecture and maybe find some good ones yourselves\
          \ and pass them on to the AI research community.</p>\n<p>An area where this\
          \ seems especially likely to me is vocabulary control, including both single\
          \ languages models and on all the  languages in a multi-language model \u2013\
          \  or maybe just a selection them \u2013 all at once.  One possibility I\
          \ can imagine is a new fine-tuning head for a large pre-trained single or\
          \ multi-language model body.  It could be a self-standing head that could\
          \ be run on the original pre-trained model to produce a new version of the\
          \ pre-trained model that the existing fine-tuning head could then be run\
          \ on, or it could be prepended to the existing fine-tuning head that in\
          \ its modified form could be run directly on the original pre-trained model.</p>\n\
          <p>I\u2019m not looking for any answers here, but I\u2019d be interested\
          \ in any comments you might have.</p>\n"
        raw: "Hi Sebastian,\n\nApologies for not seeing your comment and responding\
          \ to it sooner.  Thanks for sanity checking my previous suppositions and\
          \ for sharing information about your training.\n\nI am excited to hear that\
          \ you and your team are working on the paper you describe and look forward\
          \ to reading it when it comes out.  I hope you will consider putting it\
          \ in an accessible online place and including a link to it in your model\
          \ card.\n\nI\u2019m very interested in the potential of Deep Learning to\
          \ play a significant role in Digital Humanities, especially in fields in\
          \ which texts are abundant and a major focus of study.   Your Linguae Dharmae\
          \ project belongs to two such fields, Sinology and  Buddhist Studies, and\
          \ is probably the first or second practical Deep Learning-based NLP application\
          \ in both fields.  (I believe some systems now used for Chinese OCR and\
          \ possibly also character component analysis are also Deep Learning-based,\
          \ though they may not use transformers.) \n\nBut IMO your project can also\
          \ claim a place the field of AI research, on the grounds that you, as \u201C\
          domain experts\u201D, are able to evaluate the performance of your system\
          \ at a much more sophisticated  level than  the dominant practice within\
          \ that field, which mostly involves running standard benchmarks with standard\
          \ tasks and datasets and evaluating the results with standard metrics (in\
          \ your case BLEU).  \n\nYou have already identified recurring problems (for\
          \ example relating to personal and place names), and for specific changes\
          \ that improve performance.  Even if you stay with your current architecture\
          \ and limit your experiments to changes in parameters and training data,\
          \ your findings should be of interest to AI researchers, and could even\
          \ inspire them to make architectural changes that will give better results\
          \ \u201Cout of the box\u201D.  Or you could move on to experimenting with\
          \ changes in the architecture and maybe find some good ones yourselves and\
          \ pass them on to the AI research community.\n\nAn area where this seems\
          \ especially likely to me is vocabulary control, including both single languages\
          \ models and on all the  languages in a multi-language model \u2013  or\
          \ maybe just a selection them \u2013 all at once.  One possibility I can\
          \ imagine is a new fine-tuning head for a large pre-trained single or multi-language\
          \ model body.  It could be a self-standing head that could be run on the\
          \ original pre-trained model to produce a new version of the pre-trained\
          \ model that the existing fine-tuning head could then be run on, or it could\
          \ be prepended to the existing fine-tuning head that in its modified form\
          \ could be run directly on the original pre-trained model.\n\nI\u2019m not\
          \ looking for any answers here, but I\u2019d be interested in any comments\
          \ you might have."
        updatedAt: '2022-09-04T00:15:17.989Z'
      numEdits: 0
      reactions: []
    id: 6313ee15289cf15634cb359f
    type: comment
  author: johnarmstrong
  content: "Hi Sebastian,\n\nApologies for not seeing your comment and responding\
    \ to it sooner.  Thanks for sanity checking my previous suppositions and for sharing\
    \ information about your training.\n\nI am excited to hear that you and your team\
    \ are working on the paper you describe and look forward to reading it when it\
    \ comes out.  I hope you will consider putting it in an accessible online place\
    \ and including a link to it in your model card.\n\nI\u2019m very interested in\
    \ the potential of Deep Learning to play a significant role in Digital Humanities,\
    \ especially in fields in which texts are abundant and a major focus of study.\
    \   Your Linguae Dharmae project belongs to two such fields, Sinology and  Buddhist\
    \ Studies, and is probably the first or second practical Deep Learning-based NLP\
    \ application in both fields.  (I believe some systems now used for Chinese OCR\
    \ and possibly also character component analysis are also Deep Learning-based,\
    \ though they may not use transformers.) \n\nBut IMO your project can also claim\
    \ a place the field of AI research, on the grounds that you, as \u201Cdomain experts\u201D\
    , are able to evaluate the performance of your system at a much more sophisticated\
    \  level than  the dominant practice within that field, which mostly involves\
    \ running standard benchmarks with standard tasks and datasets and evaluating\
    \ the results with standard metrics (in your case BLEU).  \n\nYou have already\
    \ identified recurring problems (for example relating to personal and place names),\
    \ and for specific changes that improve performance.  Even if you stay with your\
    \ current architecture and limit your experiments to changes in parameters and\
    \ training data, your findings should be of interest to AI researchers, and could\
    \ even inspire them to make architectural changes that will give better results\
    \ \u201Cout of the box\u201D.  Or you could move on to experimenting with changes\
    \ in the architecture and maybe find some good ones yourselves and pass them on\
    \ to the AI research community.\n\nAn area where this seems especially likely\
    \ to me is vocabulary control, including both single languages models and on all\
    \ the  languages in a multi-language model \u2013  or maybe just a selection them\
    \ \u2013 all at once.  One possibility I can imagine is a new fine-tuning head\
    \ for a large pre-trained single or multi-language model body.  It could be a\
    \ self-standing head that could be run on the original pre-trained model to produce\
    \ a new version of the pre-trained model that the existing fine-tuning head could\
    \ then be run on, or it could be prepended to the existing fine-tuning head that\
    \ in its modified form could be run directly on the original pre-trained model.\n\
    \nI\u2019m not looking for any answers here, but I\u2019d be interested in any\
    \ comments you might have."
  created_at: 2022-09-03 23:15:17+00:00
  edited: false
  hidden: false
  id: 6313ee15289cf15634cb359f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
      fullname: Sebastian Nehrdich
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: snehrdich
      type: user
    createdAt: '2022-09-07T10:41:48.000Z'
    data:
      edited: false
      editors:
      - snehrdich
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
          fullname: Sebastian Nehrdich
          isHf: false
          isPro: false
          name: snehrdich
          type: user
        html: "<p>Hi John, </p>\n<p>Thank you very much for your encouraging remarks!\
          \ Indeed, in some applications such as  OCR deepl learning techniques have\
          \ been used for a while on Chinese data as well. Also the current BLOOM\
          \ model is getting very good at Literary and Buddhist Chinese content (just\
          \ ask it a question such as '\u7B2C\u516B\u8B58\u8B02' for a demonstration):\
          \  <a href=\"https://huggingface.co/bigscience/bloom\">https://huggingface.co/bigscience/bloom</a><br>So\
          \ yes, in the coming years we will very likely see big changes to how we\
          \ apply deep learning techniques to Chinese source material.<br>I hope we\
          \ can make a contribution to AI research as such, let's see! Your thoughts\
          \ on how to tackle vocabulary control are very interesting. I think this\
          \ is perhaps one of the biggest weaknesses of the current neural based models,\
          \ as impressive as they are on translation tasks, the unpredictability of\
          \ named entity translation is an issue that even with bigger amounts of\
          \ training data seems to be difficult to resolve. We will see what we can\
          \ achieve here.... Its a bit early yet to propose changes in the architecture\
          \ as we don't even have the 'baseline' for Buddhist NMT settled, but the\
          \ ideas you gave a certainly worthwhile to explore at a point further down\
          \ the road.<br>This is all very exciting indeed!</p>\n"
        raw: "Hi John, \n\nThank you very much for your encouraging remarks! Indeed,\
          \ in some applications such as  OCR deepl learning techniques have been\
          \ used for a while on Chinese data as well. Also the current BLOOM model\
          \ is getting very good at Literary and Buddhist Chinese content (just ask\
          \ it a question such as '\u7B2C\u516B\u8B58\u8B02' for a demonstration):\
          \  https://huggingface.co/bigscience/bloom\nSo yes, in the coming years\
          \ we will very likely see big changes to how we apply deep learning techniques\
          \ to Chinese source material.  \nI hope we can make a contribution to AI\
          \ research as such, let's see! Your thoughts on how to tackle vocabulary\
          \ control are very interesting. I think this is perhaps one of the biggest\
          \ weaknesses of the current neural based models, as impressive as they are\
          \ on translation tasks, the unpredictability of named entity translation\
          \ is an issue that even with bigger amounts of training data seems to be\
          \ difficult to resolve. We will see what we can achieve here.... Its a bit\
          \ early yet to propose changes in the architecture as we don't even have\
          \ the 'baseline' for Buddhist NMT settled, but the ideas you gave a certainly\
          \ worthwhile to explore at a point further down the road.\nThis is all very\
          \ exciting indeed!"
        updatedAt: '2022-09-07T10:41:48.684Z'
      numEdits: 0
      reactions: []
    id: 6318756c09146074fe8ec297
    type: comment
  author: snehrdich
  content: "Hi John, \n\nThank you very much for your encouraging remarks! Indeed,\
    \ in some applications such as  OCR deepl learning techniques have been used for\
    \ a while on Chinese data as well. Also the current BLOOM model is getting very\
    \ good at Literary and Buddhist Chinese content (just ask it a question such as\
    \ '\u7B2C\u516B\u8B58\u8B02' for a demonstration):  https://huggingface.co/bigscience/bloom\n\
    So yes, in the coming years we will very likely see big changes to how we apply\
    \ deep learning techniques to Chinese source material.  \nI hope we can make a\
    \ contribution to AI research as such, let's see! Your thoughts on how to tackle\
    \ vocabulary control are very interesting. I think this is perhaps one of the\
    \ biggest weaknesses of the current neural based models, as impressive as they\
    \ are on translation tasks, the unpredictability of named entity translation is\
    \ an issue that even with bigger amounts of training data seems to be difficult\
    \ to resolve. We will see what we can achieve here.... Its a bit early yet to\
    \ propose changes in the architecture as we don't even have the 'baseline' for\
    \ Buddhist NMT settled, but the ideas you gave a certainly worthwhile to explore\
    \ at a point further down the road.\nThis is all very exciting indeed!"
  created_at: 2022-09-07 09:41:48+00:00
  edited: false
  hidden: false
  id: 6318756c09146074fe8ec297
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-09-12T21:08:12.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: "<p>Hi Sebastien,</p>\n<p>I wasn\u2019t aware of the BigScience Bloom\
          \ project, thanks for bringing it to my attention.  But so far I haven\u2019\
          t noticed any instances of applying big multilingual causal LM decoders\
          \ (GPT-type models) to translation or related tasks such as cross-language\
          \ summarization.  Also, unless I missed it, there don\u2019t seem to be\
          \ any ways to fine-tune them after (pre-)training such as are there are\
          \ with multilingual encoders (mBERT etc.) and encoder-decoder combos (mBART\
          \ etc.).  Yes you can do another round of training the base model with additional\
          \ data pertinent to your task, but otherwise all you have is one- or few-shot\
          \ learning via prompts. </p>\n<p>Big LMs can already do surprisingly well\
          \ on NLP tasks classed as NLU (as in Glue and SuperGLUE), and no doubt will\
          \ keep getting getter  - and this includes the multilingual models, which\
          \ seem to exhibit significant transfer between languages. Again I may have\
          \ missed something, but as far as I know their ability to do translation\
          \ has not yet been demonstrated.  However there is one very big LM that\
          \ is trained on equal, very large, amounts of English and Chinese text,\
          \ namely Beijing Academy of Artificial Intelligence (BAAI)\u2019s \u609F\
          \u9053Wu Dao. I\u2019ve only read a little about it and I saw no mention\
          \ of translation, but that doesn\u2019t mean it can\u2019t do it.  And all\
          \ it would take to find out is to access the public API and try a prompt\
          \ or two. :)  But even if it can\u2019t, it is still possible that the ability\
          \ will \u201Cemerge\u201D as the models get bigger and become more balanced\
          \ in their language coverage.   </p>\n<p>Re what I call vocabulary control,\
          \ I was browsing through the papers in Proceedings of the Sixth Conference\
          \ on Machine Translation (2022), all of which are linked (with pdf download\
          \ options) here:</p>\n<p><a rel=\"nofollow\" href=\"https://aclanthology.org/volumes/2021.wmt-1/\"\
          >https://aclanthology.org/volumes/2021.wmt-1/</a></p>\n<p>and I discovered\
          \ a shared task in the Findings of the 2021 Conference on Machine Translation\
          \ (WMT21) called \u201Cmachine translation using terminologies\u201D.  They\
          \ are treated in a separate Document Alam et al., 2021, Findings of the\
          \ WMT Shared Task on Machine Translation Using Terminologies (link in Proceedings).\
          \ The domain was COVID-19, a newcomer (as we all remember) to general biomedical\
          \ field.  </p>\n<p>The provided training data included, for each language\
          \ pair, parallel terminology lists and a substantial number of source and\
          \ target sentence pairs annotated identically with the source and target\
          \ terms. Most of the entered  system used their own annotations of the training\
          \ data (which they could derive from the provided ones ) to \u201Cforce\u201D\
          \ the desired target terms into the translations and got positive results\
          \ from doing so.  But as the authors of the findings observe in their discussion,\
          \ annotations are a rigid approach as well as av sometimes brittle one (especially\
          \ in the face of non-trivial morphology), and that while it may be appropriate\
          \ in which a completely consistent mapping of terms is required,  it may\
          \ not be the best choice for general translation tasks.</p>\n<p>But one\
          \ system, from Huawei, (Wang et al.,2021, \u201CHuawei aarc\u2019s submission\
          \ to the wmt21 biomedical task: domain adaptation as a practical task),\
          \ is different.  It makes no use of annotations and does inference on \u201C\
          raw\u201D source inputs, no differently from any typical translation system.\
          \  Yet it scores highest of the English--Chinese task entries on all criteria.\
          \  And the reason is clear: the base system has already been adapted to\
          \ the general biomedical domain, including COVID-19.  In fact, as far as\
          \ I can tell, the system used for the terminologies task is identical to\
          \ that developed for the biomedical task, and did not use the terminologies\
          \ data for anything except for final testing.  (Note that it was disqualified\
          \ as the official winner because it doesn\u2019t limit itself to the permitted\
          \ range of training data).</p>\n<p>But the Huawei system does incorporate\
          \ cross-language terminology, including COVID-19 terminology,  in an interesting\
          \ indirect way.  It is called Dictionary-Based Data Augmentation (DDA) and\
          \ is described in detail in Wei Peng et al., 2020, \u201CDictionary-based\
          \ Data Augmentation for Cross-Domain Neural Machine Translation\u201D (<a\
          \ rel=\"nofollow\" href=\"https://arxiv.org/abs/2004.02577\">https://arxiv.org/abs/2004.02577</a>).\
          \  The basic idea is to generate source-target sentence pairs (actually\
          \ lots of them including multiples for the individual word-pairs) that contain\
          \ the source words and target words in parallel contexts appropriate for\
          \ the words.  The generated sentence pairs  are effectively translations\
          \ and, as such, and can be used for fine-tuning base generic multilingual\
          \ model, no differently that whatever \u201Creal\u201D sentence pairs are\
          \ available.</p>\n<p>You may already be familiar with this concept, but\
          \ if not, I thought you might be interested in it.  I\u2019m sure that,\
          \ as a very mature field, Buddhist Studies must have its share of onomastic\
          \ (or as we would now say, named entity) resources covering people (or rather\
          \ personal beings, including both human and non-human,  mortal and divine),\
          \ places, and unique or otherwise special things  (all cases including both\
          \ historical and mythological), as well as resources focused on religious\
          \ concepts and terms.  Plus there will inevitably be indices or concordances\
          \ to the sutras and other foundational texts.  And at least some of these\
          \ resources are bound to be available in digital form.</p>\n<p>If the situation\
          \ is more or less as I imagine it, It should be possible to get Chinese-English\
          \ lists of various categories of named entities, either directly from already\
          \ existing sources or by linking through Sanskrit.  And, once you select\
          \ what you want (say, only items occurring in the sutras, or maybe a subset\
          \ of that), you have your dictionary of terms, with basic categorical information\
          \ as a bonus.  And with that you can start trying out ways to expand them\
          \ into fabricated sentence pairs to use for additional fine-tuning.  </p>\n\
          <p>A simple metric of how well a given model is doing on named entities\
          \ is to get a translation of some sample of your to-be-translated dataset\
          \ and collect the output sentence pairs that contain at least one dictionary\
          \ term either on the source side or the target side, and count how many\
          \ paired terms appear between the two sides (good) vs. mismatched terms\
          \ or no term at all on one side (not good).  Using this metric you can do\
          \ a new translation of your chosen sample and compare it to the earlier\
          \ translation, and see how the performance (good \u2013 not good) changed.\
          \  And if results look at all promising, you can start experimenting with\
          \ your expansion method (including both sentence formation and number of\
          \ sentence pairs to be generated and go on from there.</p>\n"
        raw: "Hi Sebastien,\n\nI wasn\u2019t aware of the BigScience Bloom project,\
          \ thanks for bringing it to my attention.  But so far I haven\u2019t noticed\
          \ any instances of applying big multilingual causal LM decoders (GPT-type\
          \ models) to translation or related tasks such as cross-language summarization.\
          \  Also, unless I missed it, there don\u2019t seem to be any ways to fine-tune\
          \ them after (pre-)training such as are there are with multilingual encoders\
          \ (mBERT etc.) and encoder-decoder combos (mBART etc.).  Yes you can do\
          \ another round of training the base model with additional data pertinent\
          \ to your task, but otherwise all you have is one- or few-shot learning\
          \ via prompts. \n \nBig LMs can already do surprisingly well on NLP tasks\
          \ classed as NLU (as in Glue and SuperGLUE), and no doubt will keep getting\
          \ getter  - and this includes the multilingual models, which seem to exhibit\
          \ significant transfer between languages. Again I may have missed something,\
          \ but as far as I know their ability to do translation has not yet been\
          \ demonstrated.  However there is one very big LM that is trained on equal,\
          \ very large, amounts of English and Chinese text, namely Beijing Academy\
          \ of Artificial Intelligence (BAAI)\u2019s \u609F\u9053Wu Dao. I\u2019ve\
          \ only read a little about it and I saw no mention of translation, but that\
          \ doesn\u2019t mean it can\u2019t do it.  And all it would take to find\
          \ out is to access the public API and try a prompt or two. :)  But even\
          \ if it can\u2019t, it is still possible that the ability will \u201Cemerge\u201D\
          \ as the models get bigger and become more balanced in their language coverage.\
          \   \n\nRe what I call vocabulary control, I was browsing through the papers\
          \ in Proceedings of the Sixth Conference on Machine Translation (2022),\
          \ all of which are linked (with pdf download options) here:\n\nhttps://aclanthology.org/volumes/2021.wmt-1/\n\
          \nand I discovered a shared task in the Findings of the 2021 Conference\
          \ on Machine Translation (WMT21) called \u201Cmachine translation using\
          \ terminologies\u201D.  They are treated in a separate Document Alam et\
          \ al., 2021, Findings of the WMT Shared Task on Machine Translation Using\
          \ Terminologies (link in Proceedings). The domain was COVID-19, a newcomer\
          \ (as we all remember) to general biomedical field.  \n\nThe provided training\
          \ data included, for each language pair, parallel terminology lists and\
          \ a substantial number of source and target sentence pairs annotated identically\
          \ with the source and target terms. Most of the entered  system used their\
          \ own annotations of the training data (which they could derive from the\
          \ provided ones ) to \u201Cforce\u201D the desired target terms into the\
          \ translations and got positive results from doing so.  But as the authors\
          \ of the findings observe in their discussion, annotations are a rigid approach\
          \ as well as av sometimes brittle one (especially in the face of non-trivial\
          \ morphology), and that while it may be appropriate in which a completely\
          \ consistent mapping of terms is required,  it may not be the best choice\
          \ for general translation tasks.\n\nBut one system, from Huawei, (Wang et\
          \ al.,2021, \u201CHuawei aarc\u2019s submission to the wmt21 biomedical\
          \ task: domain adaptation as a practical task), is different.  It makes\
          \ no use of annotations and does inference on \u201Craw\u201D source inputs,\
          \ no differently from any typical translation system.  Yet it scores highest\
          \ of the English--Chinese task entries on all criteria.  And the reason\
          \ is clear: the base system has already been adapted to the general biomedical\
          \ domain, including COVID-19.  In fact, as far as I can tell, the system\
          \ used for the terminologies task is identical to that developed for the\
          \ biomedical task, and did not use the terminologies data for anything except\
          \ for final testing.  (Note that it was disqualified as the official winner\
          \ because it doesn\u2019t limit itself to the permitted range of training\
          \ data).\n\nBut the Huawei system does incorporate cross-language terminology,\
          \ including COVID-19 terminology,  in an interesting indirect way.  It is\
          \ called Dictionary-Based Data Augmentation (DDA) and is described in detail\
          \ in Wei Peng et al., 2020, \u201CDictionary-based Data Augmentation for\
          \ Cross-Domain Neural Machine Translation\u201D (https://arxiv.org/abs/2004.02577).\
          \  The basic idea is to generate source-target sentence pairs (actually\
          \ lots of them including multiples for the individual word-pairs) that contain\
          \ the source words and target words in parallel contexts appropriate for\
          \ the words.  The generated sentence pairs  are effectively translations\
          \ and, as such, and can be used for fine-tuning base generic multilingual\
          \ model, no differently that whatever \u201Creal\u201D sentence pairs are\
          \ available.\n\nYou may already be familiar with this concept, but if not,\
          \ I thought you might be interested in it.  I\u2019m sure that, as a very\
          \ mature field, Buddhist Studies must have its share of onomastic (or as\
          \ we would now say, named entity) resources covering people (or rather personal\
          \ beings, including both human and non-human,  mortal and divine), places,\
          \ and unique or otherwise special things  (all cases including both historical\
          \ and mythological), as well as resources focused on religious concepts\
          \ and terms.  Plus there will inevitably be indices or concordances to the\
          \ sutras and other foundational texts.  And at least some of these resources\
          \ are bound to be available in digital form.\n\nIf the situation is more\
          \ or less as I imagine it, It should be possible to get Chinese-English\
          \ lists of various categories of named entities, either directly from already\
          \ existing sources or by linking through Sanskrit.  And, once you select\
          \ what you want (say, only items occurring in the sutras, or maybe a subset\
          \ of that), you have your dictionary of terms, with basic categorical information\
          \ as a bonus.  And with that you can start trying out ways to expand them\
          \ into fabricated sentence pairs to use for additional fine-tuning.  \n\n\
          A simple metric of how well a given model is doing on named entities is\
          \ to get a translation of some sample of your to-be-translated dataset and\
          \ collect the output sentence pairs that contain at least one dictionary\
          \ term either on the source side or the target side, and count how many\
          \ paired terms appear between the two sides (good) vs. mismatched terms\
          \ or no term at all on one side (not good).  Using this metric you can do\
          \ a new translation of your chosen sample and compare it to the earlier\
          \ translation, and see how the performance (good \u2013 not good) changed.\
          \  And if results look at all promising, you can start experimenting with\
          \ your expansion method (including both sentence formation and number of\
          \ sentence pairs to be generated and go on from there."
        updatedAt: '2022-09-12T21:08:12.901Z'
      numEdits: 0
      reactions: []
    id: 631f9fbc2225f12fc0f514a3
    type: comment
  author: johnarmstrong
  content: "Hi Sebastien,\n\nI wasn\u2019t aware of the BigScience Bloom project,\
    \ thanks for bringing it to my attention.  But so far I haven\u2019t noticed any\
    \ instances of applying big multilingual causal LM decoders (GPT-type models)\
    \ to translation or related tasks such as cross-language summarization.  Also,\
    \ unless I missed it, there don\u2019t seem to be any ways to fine-tune them after\
    \ (pre-)training such as are there are with multilingual encoders (mBERT etc.)\
    \ and encoder-decoder combos (mBART etc.).  Yes you can do another round of training\
    \ the base model with additional data pertinent to your task, but otherwise all\
    \ you have is one- or few-shot learning via prompts. \n \nBig LMs can already\
    \ do surprisingly well on NLP tasks classed as NLU (as in Glue and SuperGLUE),\
    \ and no doubt will keep getting getter  - and this includes the multilingual\
    \ models, which seem to exhibit significant transfer between languages. Again\
    \ I may have missed something, but as far as I know their ability to do translation\
    \ has not yet been demonstrated.  However there is one very big LM that is trained\
    \ on equal, very large, amounts of English and Chinese text, namely Beijing Academy\
    \ of Artificial Intelligence (BAAI)\u2019s \u609F\u9053Wu Dao. I\u2019ve only\
    \ read a little about it and I saw no mention of translation, but that doesn\u2019\
    t mean it can\u2019t do it.  And all it would take to find out is to access the\
    \ public API and try a prompt or two. :)  But even if it can\u2019t, it is still\
    \ possible that the ability will \u201Cemerge\u201D as the models get bigger and\
    \ become more balanced in their language coverage.   \n\nRe what I call vocabulary\
    \ control, I was browsing through the papers in Proceedings of the Sixth Conference\
    \ on Machine Translation (2022), all of which are linked (with pdf download options)\
    \ here:\n\nhttps://aclanthology.org/volumes/2021.wmt-1/\n\nand I discovered a\
    \ shared task in the Findings of the 2021 Conference on Machine Translation (WMT21)\
    \ called \u201Cmachine translation using terminologies\u201D.  They are treated\
    \ in a separate Document Alam et al., 2021, Findings of the WMT Shared Task on\
    \ Machine Translation Using Terminologies (link in Proceedings). The domain was\
    \ COVID-19, a newcomer (as we all remember) to general biomedical field.  \n\n\
    The provided training data included, for each language pair, parallel terminology\
    \ lists and a substantial number of source and target sentence pairs annotated\
    \ identically with the source and target terms. Most of the entered  system used\
    \ their own annotations of the training data (which they could derive from the\
    \ provided ones ) to \u201Cforce\u201D the desired target terms into the translations\
    \ and got positive results from doing so.  But as the authors of the findings\
    \ observe in their discussion, annotations are a rigid approach as well as av\
    \ sometimes brittle one (especially in the face of non-trivial morphology), and\
    \ that while it may be appropriate in which a completely consistent mapping of\
    \ terms is required,  it may not be the best choice for general translation tasks.\n\
    \nBut one system, from Huawei, (Wang et al.,2021, \u201CHuawei aarc\u2019s submission\
    \ to the wmt21 biomedical task: domain adaptation as a practical task), is different.\
    \  It makes no use of annotations and does inference on \u201Craw\u201D source\
    \ inputs, no differently from any typical translation system.  Yet it scores highest\
    \ of the English--Chinese task entries on all criteria.  And the reason is clear:\
    \ the base system has already been adapted to the general biomedical domain, including\
    \ COVID-19.  In fact, as far as I can tell, the system used for the terminologies\
    \ task is identical to that developed for the biomedical task, and did not use\
    \ the terminologies data for anything except for final testing.  (Note that it\
    \ was disqualified as the official winner because it doesn\u2019t limit itself\
    \ to the permitted range of training data).\n\nBut the Huawei system does incorporate\
    \ cross-language terminology, including COVID-19 terminology,  in an interesting\
    \ indirect way.  It is called Dictionary-Based Data Augmentation (DDA) and is\
    \ described in detail in Wei Peng et al., 2020, \u201CDictionary-based Data Augmentation\
    \ for Cross-Domain Neural Machine Translation\u201D (https://arxiv.org/abs/2004.02577).\
    \  The basic idea is to generate source-target sentence pairs (actually lots of\
    \ them including multiples for the individual word-pairs) that contain the source\
    \ words and target words in parallel contexts appropriate for the words.  The\
    \ generated sentence pairs  are effectively translations and, as such, and can\
    \ be used for fine-tuning base generic multilingual model, no differently that\
    \ whatever \u201Creal\u201D sentence pairs are available.\n\nYou may already be\
    \ familiar with this concept, but if not, I thought you might be interested in\
    \ it.  I\u2019m sure that, as a very mature field, Buddhist Studies must have\
    \ its share of onomastic (or as we would now say, named entity) resources covering\
    \ people (or rather personal beings, including both human and non-human,  mortal\
    \ and divine), places, and unique or otherwise special things  (all cases including\
    \ both historical and mythological), as well as resources focused on religious\
    \ concepts and terms.  Plus there will inevitably be indices or concordances to\
    \ the sutras and other foundational texts.  And at least some of these resources\
    \ are bound to be available in digital form.\n\nIf the situation is more or less\
    \ as I imagine it, It should be possible to get Chinese-English lists of various\
    \ categories of named entities, either directly from already existing sources\
    \ or by linking through Sanskrit.  And, once you select what you want (say, only\
    \ items occurring in the sutras, or maybe a subset of that), you have your dictionary\
    \ of terms, with basic categorical information as a bonus.  And with that you\
    \ can start trying out ways to expand them into fabricated sentence pairs to use\
    \ for additional fine-tuning.  \n\nA simple metric of how well a given model is\
    \ doing on named entities is to get a translation of some sample of your to-be-translated\
    \ dataset and collect the output sentence pairs that contain at least one dictionary\
    \ term either on the source side or the target side, and count how many paired\
    \ terms appear between the two sides (good) vs. mismatched terms or no term at\
    \ all on one side (not good).  Using this metric you can do a new translation\
    \ of your chosen sample and compare it to the earlier translation, and see how\
    \ the performance (good \u2013 not good) changed.  And if results look at all\
    \ promising, you can start experimenting with your expansion method (including\
    \ both sentence formation and number of sentence pairs to be generated and go\
    \ on from there."
  created_at: 2022-09-12 20:08:12+00:00
  edited: false
  hidden: false
  id: 631f9fbc2225f12fc0f514a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
      fullname: Sebastian Nehrdich
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: snehrdich
      type: user
    createdAt: '2022-09-27T10:55:39.000Z'
    data:
      edited: false
      editors:
      - snehrdich
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6230db08d93e84e233885587/zHSObHXrlr-BcCwt_xttq.jpeg?w=200&h=200&f=face
          fullname: Sebastian Nehrdich
          isHf: false
          isPro: false
          name: snehrdich
          type: user
        html: '<p>HI John, </p>

          <p>Thank you very much for all this input! I am a bit overwhelmed (to be
          honest), but I will see if we can digest this in the coming months.<br>Best,
          Sebastian</p>

          '
        raw: "HI John, \n\nThank you very much for all this input! I am a bit overwhelmed\
          \ (to be honest), but I will see if we can digest this in the coming months.\
          \ \nBest, Sebastian"
        updatedAt: '2022-09-27T10:55:39.531Z'
      numEdits: 0
      reactions: []
    id: 6332d6ab9bf698ce68a1c67e
    type: comment
  author: snehrdich
  content: "HI John, \n\nThank you very much for all this input! I am a bit overwhelmed\
    \ (to be honest), but I will see if we can digest this in the coming months. \n\
    Best, Sebastian"
  created_at: 2022-09-27 09:55:39+00:00
  edited: false
  hidden: false
  id: 6332d6ab9bf698ce68a1c67e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
      fullname: John Armstrong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johnarmstrong
      type: user
    createdAt: '2022-09-28T02:22:12.000Z'
    data:
      edited: false
      editors:
      - johnarmstrong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/12918a684c5019abafe049bcb5ca4817.svg
          fullname: John Armstrong
          isHf: false
          isPro: false
          name: johnarmstrong
          type: user
        html: '<p>Hi Sebestian, </p>

          <p>It''s a tricky problem, I wish you luck with it.  By a strange coincidence
          I''m now grappling with a similar problem, and am feeling a bit overwhelmed
          myself, despite the research I did a few weeks ago.</p>

          <p>-- John</p>

          '
        raw: "Hi Sebestian, \n\nIt's a tricky problem, I wish you luck with it.  By\
          \ a strange coincidence I'm now grappling with a similar problem, and am\
          \ feeling a bit overwhelmed myself, despite the research I did a few weeks\
          \ ago.\n\n-- John"
        updatedAt: '2022-09-28T02:22:12.386Z'
      numEdits: 0
      reactions: []
    id: 6333afd4ea2dcff925256aa3
    type: comment
  author: johnarmstrong
  content: "Hi Sebestian, \n\nIt's a tricky problem, I wish you luck with it.  By\
    \ a strange coincidence I'm now grappling with a similar problem, and am feeling\
    \ a bit overwhelmed myself, despite the research I did a few weeks ago.\n\n--\
    \ John"
  created_at: 2022-09-28 01:22:12+00:00
  edited: false
  hidden: false
  id: 6333afd4ea2dcff925256aa3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: buddhist-nlp/mbart-buddhist-chinese-to-eng
repo_type: model
status: open
target_branch: null
title: Congratulations on your new Huggingface Repo!
