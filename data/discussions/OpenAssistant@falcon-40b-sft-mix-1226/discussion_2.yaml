!!python/object:huggingface_hub.community.DiscussionWithDetails
author: daryl149
conflicting_files: null
created_at: 2023-06-04 18:07:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-06-04T19:07:09.000Z'
    data:
      edited: false
      editors:
      - daryl149
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3028436601161957
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: "<p>I am running the code with the transformers repo that was recommended\
          \ in the llama model repos:</p>\n<pre><code>git clone https://github.com/huggingface/transformers.git\n\
          cd transformers\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\n\
          pip install .\n</code></pre>\n<p>However, I get an error when trying to\
          \ run:</p>\n<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ TextStreamer\ntokenizer = AutoTokenizer.from_pretrained(\"falcon-40b-sft-mix-1226\"\
          , trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          falcon-40b-sft-mix-1226\", device_map=\"sequential\", offload_folder=\"\
          offload\", load_in_8bit=True, trust_remote_code=True) \nstreamer = TextStreamer(tokenizer,\
          \ skip_prompt=True)\nmessage = \"&lt;|prompter|&gt;This is a demo of a text\
          \ streamer. What's a cool fact about ducks?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          \ninputs = tokenizer(message, return_tensors=\"pt\").to(model.device)\n\n\
          tokens = model.generate(**inputs,  max_new_tokens=25, do_sample=True, temperature=0.9,\
          \ streamer=streamer)\n</code></pre>\n<p>returns this error:</p>\n<pre><code>dev_1/lib/python3.10/site-packages/transformers/generation/utils.py:1250:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
          \  warnings.warn(\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 &lt;stdin&gt;:1\
          \ in &lt;module&gt;                                                    \
          \         \u2502\n\u2502                                               \
          \                                    \u2502\n\u2502 dev_1/lib/python3.10/site-p\
          \ \u2502\n\u2502 ackages/torch/utils/_contextlib.py:115 in decorate_context\
          \                        \u2502\n\u2502                                \
          \                                                   \u2502\n\u2502   112\
          \ \u2502   @functools.wraps(func)                                      \
          \            \u2502\n\u2502   113 \u2502   def decorate_context(*args, **kwargs):\
          \                                  \u2502\n\u2502   114 \u2502   \u2502\
          \   with ctx_factory():                                                \
          \ \u2502\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args,\
          \ **kwargs)                                    \u2502\n\u2502   116 \u2502\
          \                                                                      \
          \     \u2502\n\u2502   117 \u2502   return decorate_context            \
          \                                     \u2502\n\u2502   118             \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                  \u2502\n\u2502dev_1/lib/python3.10/site-p \u2502\n\u2502\
          \ ackages/transformers/generation/utils.py:1262 in generate            \
          \             \u2502\n\u2502                                           \
          \                                        \u2502\n\u2502   1259 \u2502  \
          \ \u2502   generation_config = copy.deepcopy(generation_config)        \
          \       \u2502\n\u2502   1260 \u2502   \u2502   model_kwargs = generation_config.update(**kwargs)\
          \  # All unused kw \u2502\n\u2502   1261 \u2502   \u2502   generation_config.validate()\
          \                                       \u2502\n\u2502 \u2771 1262 \u2502\
          \   \u2502   self._validate_model_kwargs(model_kwargs.copy())          \
          \         \u2502\n\u2502   1263 \u2502   \u2502                        \
          \                                              \u2502\n\u2502   1264 \u2502\
          \   \u2502   # 2. Set generation parameters if not already defined     \
          \         \u2502\n\u2502   1265 \u2502   \u2502   logits_processor = logits_processor\
          \ if logits_processor is not Non \u2502\n\u2502                        \
          \                                                           \u2502\n\u2502\
          \ dev_1/lib/python3.10/site-p \u2502\n\u2502 ackages/transformers/generation/utils.py:1135\
          \ in _validate_model_kwargs           \u2502\n\u2502                   \
          \                                                                \u2502\n\
          \u2502   1132 \u2502   \u2502   \u2502   \u2502   unused_model_args.append(key)\
          \                              \u2502\n\u2502   1133 \u2502   \u2502   \
          \                                                                   \u2502\
          \n\u2502   1134 \u2502   \u2502   if unused_model_args:                \
          \                              \u2502\n\u2502 \u2771 1135 \u2502   \u2502\
          \   \u2502   raise ValueError(                                         \
          \     \u2502\n\u2502   1136 \u2502   \u2502   \u2502   \u2502   f\"The following\
          \ `model_kwargs` are not used by the model:  \u2502\n\u2502   1137 \u2502\
          \   \u2502   \u2502   \u2502   \" generate arguments will also show up in\
          \ this list)\"      \u2502\n\u2502   1138 \u2502   \u2502   \u2502   ) \
          \                                                             \u2502\n\u2570\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          \nValueError: The following `model_kwargs` are not used by the model: \n\
          ['token_type_ids'] (note: typos in the generate arguments will also show\
          \ up in this \nlist)\n</code></pre>\n"
        raw: "I am running the code with the transformers repo that was recommended\
          \ in the llama model repos:\r\n```\r\ngit clone https://github.com/huggingface/transformers.git\r\
          \ncd transformers\r\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\r\
          \npip install .\r\n```\r\n\r\nHowever, I get an error when trying to run:\r\
          \n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"falcon-40b-sft-mix-1226\"\
          , trust_remote_code=True)\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          falcon-40b-sft-mix-1226\", device_map=\"sequential\", offload_folder=\"\
          offload\", load_in_8bit=True, trust_remote_code=True) \r\nstreamer = TextStreamer(tokenizer,\
          \ skip_prompt=True)\r\nmessage = \"<|prompter|>This is a demo of a text\
          \ streamer. What's a cool fact about ducks?<|endoftext|><|assistant|>\"\r\
          \ninputs = tokenizer(message, return_tensors=\"pt\").to(model.device)\r\n\
          \r\ntokens = model.generate(**inputs,  max_new_tokens=25, do_sample=True,\
          \ temperature=0.9, streamer=streamer)\r\n\r\n```\r\n\r\nreturns this error:\r\
          \n```\r\ndev_1/lib/python3.10/site-packages/transformers/generation/utils.py:1250:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\r\
          \n  warnings.warn(\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 <stdin>:1\
          \ in <module>                                                          \
          \   \u2502\r\n\u2502                                                   \
          \                                \u2502\r\n\u2502 dev_1/lib/python3.10/site-p\
          \ \u2502\r\n\u2502 ackages/torch/utils/_contextlib.py:115 in decorate_context\
          \                        \u2502\r\n\u2502                              \
          \                                                     \u2502\r\n\u2502 \
          \  112 \u2502   @functools.wraps(func)                                 \
          \                 \u2502\r\n\u2502   113 \u2502   def decorate_context(*args,\
          \ **kwargs):                                  \u2502\r\n\u2502   114 \u2502\
          \   \u2502   with ctx_factory():                                       \
          \          \u2502\r\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return\
          \ func(*args, **kwargs)                                    \u2502\r\n\u2502\
          \   116 \u2502                                                         \
          \                  \u2502\r\n\u2502   117 \u2502   return decorate_context\
          \                                                 \u2502\r\n\u2502   118\
          \                                                                      \
          \       \u2502\r\n\u2502                                               \
          \                                    \u2502\r\n\u2502dev_1/lib/python3.10/site-p\
          \ \u2502\r\n\u2502 ackages/transformers/generation/utils.py:1262 in generate\
          \                         \u2502\r\n\u2502                             \
          \                                                      \u2502\r\n\u2502\
          \   1259 \u2502   \u2502   generation_config = copy.deepcopy(generation_config)\
          \               \u2502\r\n\u2502   1260 \u2502   \u2502   model_kwargs =\
          \ generation_config.update(**kwargs)  # All unused kw \u2502\r\n\u2502 \
          \  1261 \u2502   \u2502   generation_config.validate()                 \
          \                      \u2502\r\n\u2502 \u2771 1262 \u2502   \u2502   self._validate_model_kwargs(model_kwargs.copy())\
          \                   \u2502\r\n\u2502   1263 \u2502   \u2502            \
          \                                                          \u2502\r\n\u2502\
          \   1264 \u2502   \u2502   # 2. Set generation parameters if not already\
          \ defined              \u2502\r\n\u2502   1265 \u2502   \u2502   logits_processor\
          \ = logits_processor if logits_processor is not Non \u2502\r\n\u2502   \
          \                                                                      \
          \          \u2502\r\n\u2502 dev_1/lib/python3.10/site-p \u2502\r\n\u2502\
          \ ackages/transformers/generation/utils.py:1135 in _validate_model_kwargs\
          \           \u2502\r\n\u2502                                           \
          \                                        \u2502\r\n\u2502   1132 \u2502\
          \   \u2502   \u2502   \u2502   unused_model_args.append(key)           \
          \                   \u2502\r\n\u2502   1133 \u2502   \u2502            \
          \                                                          \u2502\r\n\u2502\
          \   1134 \u2502   \u2502   if unused_model_args:                       \
          \                       \u2502\r\n\u2502 \u2771 1135 \u2502   \u2502   \u2502\
          \   raise ValueError(                                              \u2502\
          \r\n\u2502   1136 \u2502   \u2502   \u2502   \u2502   f\"The following `model_kwargs`\
          \ are not used by the model:  \u2502\r\n\u2502   1137 \u2502   \u2502  \
          \ \u2502   \u2502   \" generate arguments will also show up in this list)\"\
          \      \u2502\r\n\u2502   1138 \u2502   \u2502   \u2502   )            \
          \                                                  \u2502\r\n\u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nValueError:\
          \ The following `model_kwargs` are not used by the model: \r\n['token_type_ids']\
          \ (note: typos in the generate arguments will also show up in this \r\n\
          list)\r\n\r\n```"
        updatedAt: '2023-06-04T19:07:09.160Z'
      numEdits: 0
      reactions: []
    id: 647ce0ddd412b3b37659c170
    type: comment
  author: daryl149
  content: "I am running the code with the transformers repo that was recommended\
    \ in the llama model repos:\r\n```\r\ngit clone https://github.com/huggingface/transformers.git\r\
    \ncd transformers\r\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\r\n\
    pip install .\r\n```\r\n\r\nHowever, I get an error when trying to run:\r\n```\r\
    \nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"falcon-40b-sft-mix-1226\", trust_remote_code=True)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(\"falcon-40b-sft-mix-1226\", device_map=\"\
    sequential\", offload_folder=\"offload\", load_in_8bit=True, trust_remote_code=True)\
    \ \r\nstreamer = TextStreamer(tokenizer, skip_prompt=True)\r\nmessage = \"<|prompter|>This\
    \ is a demo of a text streamer. What's a cool fact about ducks?<|endoftext|><|assistant|>\"\
    \r\ninputs = tokenizer(message, return_tensors=\"pt\").to(model.device)\r\n\r\n\
    tokens = model.generate(**inputs,  max_new_tokens=25, do_sample=True, temperature=0.9,\
    \ streamer=streamer)\r\n\r\n```\r\n\r\nreturns this error:\r\n```\r\ndev_1/lib/python3.10/site-packages/transformers/generation/utils.py:1250:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation)\r\
    \n  warnings.warn(\r\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502 <stdin>:1 in <module>         \
    \                                                    \u2502\r\n\u2502        \
    \                                                                           \u2502\
    \r\n\u2502 dev_1/lib/python3.10/site-p \u2502\r\n\u2502 ackages/torch/utils/_contextlib.py:115\
    \ in decorate_context                        \u2502\r\n\u2502                \
    \                                                                   \u2502\r\n\
    \u2502   112 \u2502   @functools.wraps(func)                                 \
    \                 \u2502\r\n\u2502   113 \u2502   def decorate_context(*args,\
    \ **kwargs):                                  \u2502\r\n\u2502   114 \u2502  \
    \ \u2502   with ctx_factory():                                               \
    \  \u2502\r\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)\
    \                                    \u2502\r\n\u2502   116 \u2502           \
    \                                                                \u2502\r\n\u2502\
    \   117 \u2502   return decorate_context                                     \
    \            \u2502\r\n\u2502   118                                          \
    \                                   \u2502\r\n\u2502                         \
    \                                                          \u2502\r\n\u2502dev_1/lib/python3.10/site-p\
    \ \u2502\r\n\u2502 ackages/transformers/generation/utils.py:1262 in generate \
    \                        \u2502\r\n\u2502                                    \
    \                                               \u2502\r\n\u2502   1259 \u2502\
    \   \u2502   generation_config = copy.deepcopy(generation_config)            \
    \   \u2502\r\n\u2502   1260 \u2502   \u2502   model_kwargs = generation_config.update(**kwargs)\
    \  # All unused kw \u2502\r\n\u2502   1261 \u2502   \u2502   generation_config.validate()\
    \                                       \u2502\r\n\u2502 \u2771 1262 \u2502  \
    \ \u2502   self._validate_model_kwargs(model_kwargs.copy())                  \
    \ \u2502\r\n\u2502   1263 \u2502   \u2502                                    \
    \                                  \u2502\r\n\u2502   1264 \u2502   \u2502   #\
    \ 2. Set generation parameters if not already defined              \u2502\r\n\u2502\
    \   1265 \u2502   \u2502   logits_processor = logits_processor if logits_processor\
    \ is not Non \u2502\r\n\u2502                                                \
    \                                   \u2502\r\n\u2502 dev_1/lib/python3.10/site-p\
    \ \u2502\r\n\u2502 ackages/transformers/generation/utils.py:1135 in _validate_model_kwargs\
    \           \u2502\r\n\u2502                                                 \
    \                                  \u2502\r\n\u2502   1132 \u2502   \u2502   \u2502\
    \   \u2502   unused_model_args.append(key)                              \u2502\
    \r\n\u2502   1133 \u2502   \u2502                                            \
    \                          \u2502\r\n\u2502   1134 \u2502   \u2502   if unused_model_args:\
    \                                              \u2502\r\n\u2502 \u2771 1135 \u2502\
    \   \u2502   \u2502   raise ValueError(                                      \
    \        \u2502\r\n\u2502   1136 \u2502   \u2502   \u2502   \u2502   f\"The following\
    \ `model_kwargs` are not used by the model:  \u2502\r\n\u2502   1137 \u2502  \
    \ \u2502   \u2502   \u2502   \" generate arguments will also show up in this list)\"\
    \      \u2502\r\n\u2502   1138 \u2502   \u2502   \u2502   )                  \
    \                                            \u2502\r\n\u2570\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u256F\r\nValueError: The following `model_kwargs` are not used by\
    \ the model: \r\n['token_type_ids'] (note: typos in the generate arguments will\
    \ also show up in this \r\nlist)\r\n\r\n```"
  created_at: 2023-06-04 18:07:09+00:00
  edited: false
  hidden: false
  id: 647ce0ddd412b3b37659c170
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-06-04T19:16:21.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.793865978717804
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: "<p>Lol, ok the quick fix:<br>open <code>transformers/generation/utils.py</code>\
          \ and comment out the if statement on line 1134-1138, like so:</p>\n<pre><code>\
          \        # if unused_model_args:\n        #     raise ValueError(\n    \
          \    #         f\"The following `model_kwargs` are not used by the model:\
          \ {unused_model_args} (note: typos in the\"\n        #         \" generate\
          \ arguments will also show up in this list)\"\n        #     )\n</code></pre>\n\
          <p>If it looks stupid, but it works...</p>\n<p>New output (as expected for\
          \ 25 <code>max_new_tokens</code>):</p>\n<pre><code>dev_1/lib/python3.10/site-packages/transformers/generation/utils.py:1250:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
          \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\nDucks have a waterproof coating on their feathers, which\
          \ allows them to swim and preen themselves in water.\nTheir web\n</code></pre>\n\
          <p><strong>update:</strong> huggingface indicated they will not fix this\
          \ in the transformers repo, so this is now a mandatory step.</p>\n"
        raw: "Lol, ok the quick fix:\nopen `transformers/generation/utils.py` and\
          \ comment out the if statement on line 1134-1138, like so:\n```\n      \
          \  # if unused_model_args:\n        #     raise ValueError(\n        # \
          \        f\"The following `model_kwargs` are not used by the model: {unused_model_args}\
          \ (note: typos in the\"\n        #         \" generate arguments will also\
          \ show up in this list)\"\n        #     )\n```\nIf it looks stupid, but\
          \ it works...\n\nNew output (as expected for 25 `max_new_tokens`):\n```\n\
          dev_1/lib/python3.10/site-packages/transformers/generation/utils.py:1250:\
          \ UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use a generation configuration\
          \ file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
          \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end\
          \ generation.\nDucks have a waterproof coating on their feathers, which\
          \ allows them to swim and preen themselves in water.\nTheir web\n```\n\n\
          **update:** huggingface indicated they will not fix this in the transformers\
          \ repo, so this is now a mandatory step."
        updatedAt: '2023-06-05T22:57:27.564Z'
      numEdits: 1
      reactions: []
    id: 647ce3051c0644de8d306308
    type: comment
  author: daryl149
  content: "Lol, ok the quick fix:\nopen `transformers/generation/utils.py` and comment\
    \ out the if statement on line 1134-1138, like so:\n```\n        # if unused_model_args:\n\
    \        #     raise ValueError(\n        #         f\"The following `model_kwargs`\
    \ are not used by the model: {unused_model_args} (note: typos in the\"\n     \
    \   #         \" generate arguments will also show up in this list)\"\n      \
    \  #     )\n```\nIf it looks stupid, but it works...\n\nNew output (as expected\
    \ for 25 `max_new_tokens`):\n```\ndev_1/lib/python3.10/site-packages/transformers/generation/utils.py:1250:\
    \ UserWarning: You have modified the pretrained model configuration to control\
    \ generation. This is a deprecated strategy to control generation and will be\
    \ removed soon, in a future version. Please use a generation configuration file\
    \ (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n\
    \  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n\
    Ducks have a waterproof coating on their feathers, which allows them to swim and\
    \ preen themselves in water.\nTheir web\n```\n\n**update:** huggingface indicated\
    \ they will not fix this in the transformers repo, so this is now a mandatory\
    \ step."
  created_at: 2023-06-04 18:16:21+00:00
  edited: true
  hidden: false
  id: 647ce3051c0644de8d306308
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b3de57c1d192da852fbe51c4d5e15d6.svg
      fullname: Stergiadis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: steremma
      type: user
    createdAt: '2023-06-27T09:53:49.000Z'
    data:
      edited: true
      editors:
      - steremma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4797336459159851
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b3de57c1d192da852fbe51c4d5e15d6.svg
          fullname: Stergiadis
          isHf: false
          isPro: false
          name: steremma
          type: user
        html: '<p>simpler solution, replace <code>**inputs</code> with <code>input_ids=inputs[''input_ids''],
          attention_mask=inputs[''attention_mask'']</code><br>the extra arg is returned
          by the tokenizer.</p>

          '
        raw: 'simpler solution, replace `**inputs` with `input_ids=inputs[''input_ids''],
          attention_mask=inputs[''attention_mask'']`

          the extra arg is returned by the tokenizer.'
        updatedAt: '2023-06-27T09:54:05.068Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - daryl149
        - andrewm4894
    id: 649ab1ad807a6d1fea274772
    type: comment
  author: steremma
  content: 'simpler solution, replace `**inputs` with `input_ids=inputs[''input_ids''],
    attention_mask=inputs[''attention_mask'']`

    the extra arg is returned by the tokenizer.'
  created_at: 2023-06-27 08:53:49+00:00
  edited: true
  hidden: false
  id: 649ab1ad807a6d1fea274772
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-07-05T21:53:39.000Z'
    data:
      edited: true
      editors:
      - daryl149
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5907436013221741
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<blockquote>

          <p>simpler solution, replace <code>**inputs</code> with <code>input_ids=inputs[''input_ids''],
          attention_mask=inputs[''attention_mask'']</code><br>the extra arg is returned
          by the tokenizer.</p>

          </blockquote>

          <p>yooo, that''s way easier, thanks :D</p>

          '
        raw: '> simpler solution, replace `**inputs` with `input_ids=inputs[''input_ids''],
          attention_mask=inputs[''attention_mask'']`

          > the extra arg is returned by the tokenizer.


          yooo, that''s way easier, thanks :D

          '
        updatedAt: '2023-07-05T21:53:48.449Z'
      numEdits: 1
      reactions: []
      relatedEventId: 64a5e663500beb5096872ded
    id: 64a5e663500beb5096872dec
    type: comment
  author: daryl149
  content: '> simpler solution, replace `**inputs` with `input_ids=inputs[''input_ids''],
    attention_mask=inputs[''attention_mask'']`

    > the extra arg is returned by the tokenizer.


    yooo, that''s way easier, thanks :D

    '
  created_at: 2023-07-05 20:53:39+00:00
  edited: true
  hidden: false
  id: 64a5e663500beb5096872dec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-07-05T21:53:39.000Z'
    data:
      status: closed
    id: 64a5e663500beb5096872ded
    type: status-change
  author: daryl149
  created_at: 2023-07-05 20:53:39+00:00
  id: 64a5e663500beb5096872ded
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/falcon-40b-sft-mix-1226
repo_type: model
status: closed
target_branch: null
title: 'ValueError: the following model_kwargs are not used by model'
