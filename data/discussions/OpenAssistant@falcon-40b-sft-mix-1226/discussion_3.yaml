!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gdcoder
conflicting_files: null
created_at: 2023-06-14 22:33:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/82ae68b2e74ff8fa4b15c85cfea401a2.svg
      fullname: Drakos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gdcoder
      type: user
    createdAt: '2023-06-14T23:33:02.000Z'
    data:
      edited: false
      editors:
      - gdcoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5303406119346619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/82ae68b2e74ff8fa4b15c85cfea401a2.svg
          fullname: Drakos
          isHf: false
          isPro: false
          name: gdcoder
          type: user
        html: "<p>I get back a response from the model but it is not complete. I manage\
          \ to deploy it on ml.g5.12xlarge following the instructions.</p>\n<pre><code>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\n\
          import torch\nimport json \nmodel = \"OpenAssistant/falcon-40b-sft-mix-1226\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\n# grab environment\
          \ variables\nENDPOINT_NAME = \"huggingface-pytorch-tgi-inference-2023-06-14-22-44-39-458\"\
          \nruntime= boto3.client('runtime.sagemaker')\nprompt = \"&lt;|prompter|&gt;What\
          \ is a meme, and what's the history behind this word?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          \ninput_data = {\n  \"inputs\": prompt,\n  \"parameters\": {\n    \"do_sample\"\
          : True,\n    \"temperature\":0.1,\n    \"include_prompt_in_result\": False,\n\
          \    \"top_k\":10,\n    \"num_return_sequences\":10,\n    \"max_length\"\
          : 10,\n    #\"eos_token_id\":tokenizer.eos_token_id,\n    \"return_full_text\"\
          :False,\n  }\n}\n\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n\
          \                                   ContentType='application/json',\n  \
          \                                 Body=json.dumps(input_data).encode('utf-8'))\n\
          response_json = json.loads(response['Body'].read().decode(\"utf-8\"))\n\
          response_json\n</code></pre>\n"
        raw: "I get back a response from the model but it is not complete. I manage\
          \ to deploy it on ml.g5.12xlarge following the instructions.\r\n\r\n```\r\
          \nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\nimport\
          \ transformers\r\nimport torch\r\nimport json \r\nmodel = \"OpenAssistant/falcon-40b-sft-mix-1226\"\
          \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\n\r\n# grab environment\
          \ variables\r\nENDPOINT_NAME = \"huggingface-pytorch-tgi-inference-2023-06-14-22-44-39-458\"\
          \r\nruntime= boto3.client('runtime.sagemaker')\r\nprompt = \"<|prompter|>What\
          \ is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\
          \r\ninput_data = {\r\n  \"inputs\": prompt,\r\n  \"parameters\": {\r\n \
          \   \"do_sample\": True,\r\n    \"temperature\":0.1,\r\n    \"include_prompt_in_result\"\
          : False,\r\n    \"top_k\":10,\r\n    \"num_return_sequences\":10,\r\n  \
          \  \"max_length\": 10,\r\n    #\"eos_token_id\":tokenizer.eos_token_id,\r\
          \n    \"return_full_text\":False,\r\n  }\r\n}\r\n\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\
          \n                                   ContentType='application/json',\r\n\
          \                                   Body=json.dumps(input_data).encode('utf-8'))\r\
          \nresponse_json = json.loads(response['Body'].read().decode(\"utf-8\"))\r\
          \nresponse_json\r\n```"
        updatedAt: '2023-06-14T23:33:02.500Z'
      numEdits: 0
      reactions: []
    id: 648a4e2e4b86ebc8803cefee
    type: comment
  author: gdcoder
  content: "I get back a response from the model but it is not complete. I manage\
    \ to deploy it on ml.g5.12xlarge following the instructions.\r\n\r\n```\r\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM\r\nimport transformers\r\
    \nimport torch\r\nimport json \r\nmodel = \"OpenAssistant/falcon-40b-sft-mix-1226\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model)\r\n\r\n# grab environment\
    \ variables\r\nENDPOINT_NAME = \"huggingface-pytorch-tgi-inference-2023-06-14-22-44-39-458\"\
    \r\nruntime= boto3.client('runtime.sagemaker')\r\nprompt = \"<|prompter|>What\
    \ is a meme, and what's the history behind this word?<|endoftext|><|assistant|>\"\
    \r\ninput_data = {\r\n  \"inputs\": prompt,\r\n  \"parameters\": {\r\n    \"do_sample\"\
    : True,\r\n    \"temperature\":0.1,\r\n    \"include_prompt_in_result\": False,\r\
    \n    \"top_k\":10,\r\n    \"num_return_sequences\":10,\r\n    \"max_length\"\
    : 10,\r\n    #\"eos_token_id\":tokenizer.eos_token_id,\r\n    \"return_full_text\"\
    :False,\r\n  }\r\n}\r\n\r\nresponse = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\r\
    \n                                   ContentType='application/json',\r\n     \
    \                              Body=json.dumps(input_data).encode('utf-8'))\r\n\
    response_json = json.loads(response['Body'].read().decode(\"utf-8\"))\r\nresponse_json\r\
    \n```"
  created_at: 2023-06-14 22:33:02+00:00
  edited: false
  hidden: false
  id: 648a4e2e4b86ebc8803cefee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-06-15T00:25:00.000Z'
    data:
      edited: true
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9364590048789978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<p>By "not complete" do you mean that it cuts off early? If so it''s
          likely because of the <code>"max_length": 10</code> parameter you pass.
          That limits the generation to 10 tokens, which is really not a lot. If you
          want a somewhat detailed answer you should set it to at least 300. Though
          keep in mind that it is a max length, not an enforced length, so the answer
          can be shorter than this length.</p>

          '
        raw: 'By "not complete" do you mean that it cuts off early? If so it''s likely
          because of the `"max_length": 10` parameter you pass. That limits the generation
          to 10 tokens, which is really not a lot. If you want a somewhat detailed
          answer you should set it to at least 300. Though keep in mind that it is
          a max length, not an enforced length, so the answer can be shorter than
          this length.'
        updatedAt: '2023-06-15T00:27:27.496Z'
      numEdits: 1
      reactions: []
    id: 648a5a5c9dbe814bd9c5f19c
    type: comment
  author: Mikael110
  content: 'By "not complete" do you mean that it cuts off early? If so it''s likely
    because of the `"max_length": 10` parameter you pass. That limits the generation
    to 10 tokens, which is really not a lot. If you want a somewhat detailed answer
    you should set it to at least 300. Though keep in mind that it is a max length,
    not an enforced length, so the answer can be shorter than this length.'
  created_at: 2023-06-14 23:25:00+00:00
  edited: true
  hidden: false
  id: 648a5a5c9dbe814bd9c5f19c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: OpenAssistant/falcon-40b-sft-mix-1226
repo_type: model
status: open
target_branch: null
title: Expand Output after deploying it on SageMaker
