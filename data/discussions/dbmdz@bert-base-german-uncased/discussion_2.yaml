!!python/object:huggingface_hub.community.DiscussionWithDetails
author: birgitbrause
conflicting_files: null
created_at: 2022-12-13 14:14:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b458600c9325e0357f8704f35215d25a.svg
      fullname: Birgit Brause
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: birgitbrause
      type: user
    createdAt: '2022-12-13T14:14:51.000Z'
    data:
      edited: true
      editors:
      - birgitbrause
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b458600c9325e0357f8704f35215d25a.svg
          fullname: Birgit Brause
          isHf: false
          isPro: false
          name: birgitbrause
          type: user
        html: '<p>I might have noticed a bug in tokenizer. It seems that in its normalizer,
          <code>strip_accents</code> is automatically set to True when loaded, because
          it is not explicitly set to False in <code>tokenizer_config.json</code>.<br>Because
          of this, the tokenizer normalizes German Umlauts. In the tokenizer''s vocabulary
          however, I can see tokens having Umlauts.</p>

          <p>When loaded from hub the tokenizer behaves like this:<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/1670940730794-6340911d733f9eef46953dbd.png"><img
          alt="dbmdz_tokenizer_before.png" src="https://cdn-uploads.huggingface.co/production/uploads/1670940730794-6340911d733f9eef46953dbd.png"></a></p>

          <p>If I deactivate the normalization:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1670940759436-6340911d733f9eef46953dbd.png"><img
          alt="dbmdz_tokenizer_after.png" src="https://cdn-uploads.huggingface.co/production/uploads/1670940759436-6340911d733f9eef46953dbd.png"></a></p>

          <p>Because of the Umlauts in the vocabulary, I guess <code>strip_accents
          == False</code> is the originally intended behaviour?<br>Do you have an
          idea if the model was trained with or without seeing Umlauts?</p>

          <p>I tested this with transformers versions 2.3.0, 4.6.1, 4.25.1.</p>

          '
        raw: "I might have noticed a bug in tokenizer. It seems that in its normalizer,\
          \ `strip_accents` is automatically set to True when loaded, because it is\
          \ not explicitly set to False in `tokenizer_config.json`.\nBecause of this,\
          \ the tokenizer normalizes German Umlauts. In the tokenizer's vocabulary\
          \ however, I can see tokens having Umlauts.\n\nWhen loaded from hub the\
          \ tokenizer behaves like this:\n![dbmdz_tokenizer_before.png](https://cdn-uploads.huggingface.co/production/uploads/1670940730794-6340911d733f9eef46953dbd.png)\n\
          \nIf I deactivate the normalization:\n![dbmdz_tokenizer_after.png](https://cdn-uploads.huggingface.co/production/uploads/1670940759436-6340911d733f9eef46953dbd.png)\n\
          \nBecause of the Umlauts in the vocabulary, I guess `strip_accents == False`\
          \ is the originally intended behaviour? \nDo you have an idea if the model\
          \ was trained with or without seeing Umlauts?\n\nI tested this with transformers\
          \ versions 2.3.0, 4.6.1, 4.25.1."
        updatedAt: '2022-12-13T14:16:06.519Z'
      numEdits: 1
      reactions: []
    id: 639888db20ea1b91e4d1432d
    type: comment
  author: birgitbrause
  content: "I might have noticed a bug in tokenizer. It seems that in its normalizer,\
    \ `strip_accents` is automatically set to True when loaded, because it is not\
    \ explicitly set to False in `tokenizer_config.json`.\nBecause of this, the tokenizer\
    \ normalizes German Umlauts. In the tokenizer's vocabulary however, I can see\
    \ tokens having Umlauts.\n\nWhen loaded from hub the tokenizer behaves like this:\n\
    ![dbmdz_tokenizer_before.png](https://cdn-uploads.huggingface.co/production/uploads/1670940730794-6340911d733f9eef46953dbd.png)\n\
    \nIf I deactivate the normalization:\n![dbmdz_tokenizer_after.png](https://cdn-uploads.huggingface.co/production/uploads/1670940759436-6340911d733f9eef46953dbd.png)\n\
    \nBecause of the Umlauts in the vocabulary, I guess `strip_accents == False` is\
    \ the originally intended behaviour? \nDo you have an idea if the model was trained\
    \ with or without seeing Umlauts?\n\nI tested this with transformers versions\
    \ 2.3.0, 4.6.1, 4.25.1."
  created_at: 2022-12-13 14:14:51+00:00
  edited: true
  hidden: false
  id: 639888db20ea1b91e4d1432d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: dbmdz/bert-base-german-uncased
repo_type: model
status: open
target_branch: null
title: Bug in tokenizer?
