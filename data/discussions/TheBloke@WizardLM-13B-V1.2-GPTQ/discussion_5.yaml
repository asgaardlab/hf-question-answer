!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ayenem
conflicting_files: null
created_at: 2023-08-01 23:00:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
      fullname: Ahmed Moubtahij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayenem
      type: user
    createdAt: '2023-08-02T00:00:51.000Z'
    data:
      edited: true
      editors:
      - Ayenem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.520708441734314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
          fullname: Ahmed Moubtahij
          isHf: false
          isPro: false
          name: Ayenem
          type: user
        html: "<pre><code class=\"language-py\">LLM_ID = <span class=\"hljs-string\"\
          >\"TheBloke/WizardLM-13B-V1.2-GPTQ\"</span>\nmodel_basename = <span class=\"\
          hljs-string\">\"gptq_model-4bit-128g\"</span>\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \    LLM_ID,\n    use_fast=<span class=\"hljs-literal\">True</span>,\n)\n\
          model = AutoGPTQForCausalLM.from_quantized( <span class=\"hljs-comment\"\
          ># FileNotFoundError</span>\n    LLM_ID,\n    revision=<span class=\"hljs-string\"\
          >\"gptq-8bit-64g-actorder_True\"</span>,\n    model_basename=model_basename,\n\
          \    use_safetensors=<span class=\"hljs-literal\">True</span>,\n    trust_remote_code=<span\
          \ class=\"hljs-literal\">False</span>,\n    device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n    quantize_config=<span class=\"hljs-literal\">None</span>,\n\
          )\n</code></pre>\n<pre><code>Traceback (most recent call last):\n  File\
          \ \".../demo.py\", line 91, in &lt;module&gt;\n    model = AutoGPTQForCausalLM.from_quantized(\n\
          \  File \".../auto_gptq/modeling/auto.py\", line 105, in from_quantized\n\
          \    return quant_func(\n  File \".../auto_gptq/modeling/_base.py\", line\
          \ 768, in from_quantized\n    raise FileNotFoundError(f\"Could not find\
          \ model in {model_name_or_path}\")\nFileNotFoundError: Could not find model\
          \ in TheBloke/WizardLM-13B-V1.2-GPTQ\n</code></pre>\n<p>What could I be\
          \ doing wrong?</p>\n"
        raw: "```py\nLLM_ID = \"TheBloke/WizardLM-13B-V1.2-GPTQ\"\nmodel_basename\
          \ = \"gptq_model-4bit-128g\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \    LLM_ID,\n    use_fast=True,\n)\nmodel = AutoGPTQForCausalLM.from_quantized(\
          \ # FileNotFoundError\n    LLM_ID,\n    revision=\"gptq-8bit-64g-actorder_True\"\
          ,\n    model_basename=model_basename,\n    use_safetensors=True,\n    trust_remote_code=False,\n\
          \    device=\"cuda:0\",\n    quantize_config=None,\n)\n```\n```\nTraceback\
          \ (most recent call last):\n  File \".../demo.py\", line 91, in <module>\n\
          \    model = AutoGPTQForCausalLM.from_quantized(\n  File \".../auto_gptq/modeling/auto.py\"\
          , line 105, in from_quantized\n    return quant_func(\n  File \".../auto_gptq/modeling/_base.py\"\
          , line 768, in from_quantized\n    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\nFileNotFoundError: Could not find\
          \ model in TheBloke/WizardLM-13B-V1.2-GPTQ\n```\nWhat could I be doing wrong?"
        updatedAt: '2023-08-02T00:02:31.408Z'
      numEdits: 1
      reactions: []
    id: 64c99cb37dba66c3a7f4baaa
    type: comment
  author: Ayenem
  content: "```py\nLLM_ID = \"TheBloke/WizardLM-13B-V1.2-GPTQ\"\nmodel_basename =\
    \ \"gptq_model-4bit-128g\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n   \
    \ LLM_ID,\n    use_fast=True,\n)\nmodel = AutoGPTQForCausalLM.from_quantized(\
    \ # FileNotFoundError\n    LLM_ID,\n    revision=\"gptq-8bit-64g-actorder_True\"\
    ,\n    model_basename=model_basename,\n    use_safetensors=True,\n    trust_remote_code=False,\n\
    \    device=\"cuda:0\",\n    quantize_config=None,\n)\n```\n```\nTraceback (most\
    \ recent call last):\n  File \".../demo.py\", line 91, in <module>\n    model\
    \ = AutoGPTQForCausalLM.from_quantized(\n  File \".../auto_gptq/modeling/auto.py\"\
    , line 105, in from_quantized\n    return quant_func(\n  File \".../auto_gptq/modeling/_base.py\"\
    , line 768, in from_quantized\n    raise FileNotFoundError(f\"Could not find model\
    \ in {model_name_or_path}\")\nFileNotFoundError: Could not find model in TheBloke/WizardLM-13B-V1.2-GPTQ\n\
    ```\nWhat could I be doing wrong?"
  created_at: 2023-08-01 23:00:51+00:00
  edited: true
  hidden: false
  id: 64c99cb37dba66c3a7f4baaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
      fullname: Ahmed Moubtahij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayenem
      type: user
    createdAt: '2023-08-02T13:25:51.000Z'
    data:
      edited: false
      editors:
      - Ayenem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.738573431968689
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
          fullname: Ahmed Moubtahij
          isHf: false
          isPro: false
          name: Ayenem
          type: user
        html: '<p>It works when I match the number of bits and group size in the <code>model_basename</code>
          i.e. <code>model_basename = "gptq_model-8bit-64g"</code><br>Seems obvious
          in retrospect, but I wish the <code>revision</code> example in <a href="https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GPTQ#how-to-use-this-gptq-model-from-python-code">https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GPTQ#how-to-use-this-gptq-model-from-python-code</a>
          was clearer</p>

          '
        raw: 'It works when I match the number of bits and group size in the `model_basename`
          i.e. `model_basename = "gptq_model-8bit-64g"`

          Seems obvious in retrospect, but I wish the `revision` example in https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GPTQ#how-to-use-this-gptq-model-from-python-code
          was clearer'
        updatedAt: '2023-08-02T13:25:51.789Z'
      numEdits: 0
      reactions: []
    id: 64ca595ff103036e2398d1dd
    type: comment
  author: Ayenem
  content: 'It works when I match the number of bits and group size in the `model_basename`
    i.e. `model_basename = "gptq_model-8bit-64g"`

    Seems obvious in retrospect, but I wish the `revision` example in https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GPTQ#how-to-use-this-gptq-model-from-python-code
    was clearer'
  created_at: 2023-08-02 12:25:51+00:00
  edited: false
  hidden: false
  id: 64ca595ff103036e2398d1dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/f17bee7bfc587b5899586c4193f65986.svg
      fullname: Ahmed Moubtahij
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayenem
      type: user
    createdAt: '2023-08-02T13:29:47.000Z'
    data:
      from: FileNotFoundError(f"Could not find model in {model_name_or_path}")
      to: '[SOLVED] FileNotFoundError(f"Could not find model in {model_name_or_path}")'
    id: 64ca5a4bcbf85b5731206432
    type: title-change
  author: Ayenem
  created_at: 2023-08-02 12:29:47+00:00
  id: 64ca5a4bcbf85b5731206432
  new_title: '[SOLVED] FileNotFoundError(f"Could not find model in {model_name_or_path}")'
  old_title: FileNotFoundError(f"Could not find model in {model_name_or_path}")
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/WizardLM-13B-V1.2-GPTQ
repo_type: model
status: open
target_branch: null
title: '[SOLVED] FileNotFoundError(f"Could not find model in {model_name_or_path}")'
