!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anon7463435254
conflicting_files: null
created_at: 2023-07-26 07:13:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T08:13:22.000Z'
    data:
      edited: true
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6189181208610535
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>There is something strange happening (same happen with the new Nous-Hermes):
          the model is loaded, but any message I send I receive a totally empty response
          (it does not even show my question). I tried this with AutoGPTQ, ExLLama
          and GPTQ-for-LLaMA. I''ll show you an example:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/JOPgpjSZEUIbKir07gfUq.png"><img
          alt="instruction_template.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/JOPgpjSZEUIbKir07gfUq.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/3ebSJR6OnmxaopsS2PQ_y.png"><img
          alt="model.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/3ebSJR6OnmxaopsS2PQ_y.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/nVRGqjASsS3EYd0wN9TjO.png"><img
          alt="params.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/nVRGqjASsS3EYd0wN9TjO.png"></a></p>

          <p>After loading the model, if I try to send "Hello", this happens:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rMrB-BF-wVB632lLQMQi2.png"><img
          alt="chat.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rMrB-BF-wVB632lLQMQi2.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/78fH03sY6LzbXXRmy-BDq.png"><img
          alt="error.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/78fH03sY6LzbXXRmy-BDq.png"></a></p>

          <p>Am I missing something?</p>

          <p>Thank you and sorry about bothering you, I hope this can help.</p>

          '
        raw: 'There is something strange happening (same happen with the new Nous-Hermes):
          the model is loaded, but any message I send I receive a totally empty response
          (it does not even show my question). I tried this with AutoGPTQ, ExLLama
          and GPTQ-for-LLaMA. I''ll show you an example:


          ![instruction_template.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/JOPgpjSZEUIbKir07gfUq.png)

          ![model.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/3ebSJR6OnmxaopsS2PQ_y.png)

          ![params.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/nVRGqjASsS3EYd0wN9TjO.png)


          After loading the model, if I try to send "Hello", this happens:


          ![chat.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rMrB-BF-wVB632lLQMQi2.png)

          ![error.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/78fH03sY6LzbXXRmy-BDq.png)


          Am I missing something?


          Thank you and sorry about bothering you, I hope this can help.

          '
        updatedAt: '2023-07-26T08:14:21.627Z'
      numEdits: 1
      reactions: []
    id: 64c0d5a2e062a72f15be922e
    type: comment
  author: anon7463435254
  content: 'There is something strange happening (same happen with the new Nous-Hermes):
    the model is loaded, but any message I send I receive a totally empty response
    (it does not even show my question). I tried this with AutoGPTQ, ExLLama and GPTQ-for-LLaMA.
    I''ll show you an example:


    ![instruction_template.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/JOPgpjSZEUIbKir07gfUq.png)

    ![model.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/3ebSJR6OnmxaopsS2PQ_y.png)

    ![params.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/nVRGqjASsS3EYd0wN9TjO.png)


    After loading the model, if I try to send "Hello", this happens:


    ![chat.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/rMrB-BF-wVB632lLQMQi2.png)

    ![error.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/78fH03sY6LzbXXRmy-BDq.png)


    Am I missing something?


    Thank you and sorry about bothering you, I hope this can help.

    '
  created_at: 2023-07-26 07:13:22+00:00
  edited: true
  hidden: false
  id: 64c0d5a2e062a72f15be922e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T08:15:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7143486738204956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah there''s something wrong with your install. Have you updated
          text-generation-webui to the latest version?  Are you using one-click installer
          or manual installer?  If manual install, make sure to git pull on both text-generation-webui
          and exllama, and to re-do <code>pip3 install -r requirements.txt</code>
          in text-generation-webui</p>

          '
        raw: Yeah there's something wrong with your install. Have you updated text-generation-webui
          to the latest version?  Are you using one-click installer or manual installer?  If
          manual install, make sure to git pull on both text-generation-webui and
          exllama, and to re-do `pip3 install -r requirements.txt` in text-generation-webui
        updatedAt: '2023-07-26T08:15:59.408Z'
      numEdits: 0
      reactions: []
    id: 64c0d63f5a8f5b0d13c408fd
    type: comment
  author: TheBloke
  content: Yeah there's something wrong with your install. Have you updated text-generation-webui
    to the latest version?  Are you using one-click installer or manual installer?  If
    manual install, make sure to git pull on both text-generation-webui and exllama,
    and to re-do `pip3 install -r requirements.txt` in text-generation-webui
  created_at: 2023-07-26 07:15:59+00:00
  edited: false
  hidden: false
  id: 64c0d63f5a8f5b0d13c408fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T08:25:13.000Z'
    data:
      edited: true
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5121386051177979
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>I did this inside a Colab:</p>

          <p>!git clone <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a><br>%cd
          text-generation-webui<br>!pip install -r requirements.txt</p>

          <p>and only If I want to use GPTQ-for-LLaMA I run this:</p>

          <p>%mkdir /content/text-generation-webui/repositories/<br>%cd /content/text-generation-webui/repositories/<br>!git
          clone <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa.git">https://github.com/oobabooga/GPTQ-for-LLaMa.git</a>
          -b cuda<br>%cd GPTQ-for-LLaMa<br>!pip install ninja<br>!pip install -r requirements.txt<br>!python
          setup_cuda.py install</p>

          '
        raw: 'I did this inside a Colab:


          !git clone https://github.com/oobabooga/text-generation-webui

          %cd text-generation-webui

          !pip install -r requirements.txt


          and only If I want to use GPTQ-for-LLaMA I run this:


          %mkdir /content/text-generation-webui/repositories/

          %cd /content/text-generation-webui/repositories/

          !git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda

          %cd GPTQ-for-LLaMa

          !pip install ninja

          !pip install -r requirements.txt

          !python setup_cuda.py install'
        updatedAt: '2023-07-26T08:26:40.429Z'
      numEdits: 1
      reactions: []
    id: 64c0d869a20269b8bd899056
    type: comment
  author: anon7463435254
  content: 'I did this inside a Colab:


    !git clone https://github.com/oobabooga/text-generation-webui

    %cd text-generation-webui

    !pip install -r requirements.txt


    and only If I want to use GPTQ-for-LLaMA I run this:


    %mkdir /content/text-generation-webui/repositories/

    %cd /content/text-generation-webui/repositories/

    !git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda

    %cd GPTQ-for-LLaMa

    !pip install ninja

    !pip install -r requirements.txt

    !python setup_cuda.py install'
  created_at: 2023-07-26 07:25:13+00:00
  edited: true
  hidden: false
  id: 64c0d869a20269b8bd899056
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T08:26:33.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7992684245109558
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>And what about exllama? Did you install that?  You need to install
          it before you use it</p>

          <pre><code># in text-generation-webui directory

          !mkdir repositories

          !git clone https://github.com/turboderp/exllama repositories/exllama

          !pip3 install -r repositories/exllama/requirements.txt

          </code></pre>

          '
        raw: 'And what about exllama? Did you install that?  You need to install it
          before you use it

          ```

          # in text-generation-webui directory

          !mkdir repositories

          !git clone https://github.com/turboderp/exllama repositories/exllama

          !pip3 install -r repositories/exllama/requirements.txt

          ```


          '
        updatedAt: '2023-07-26T08:26:45.117Z'
      numEdits: 1
      reactions: []
    id: 64c0d8b9200a5155968fa128
    type: comment
  author: TheBloke
  content: 'And what about exllama? Did you install that?  You need to install it
    before you use it

    ```

    # in text-generation-webui directory

    !mkdir repositories

    !git clone https://github.com/turboderp/exllama repositories/exllama

    !pip3 install -r repositories/exllama/requirements.txt

    ```


    '
  created_at: 2023-07-26 07:26:33+00:00
  edited: true
  hidden: false
  id: 64c0d8b9200a5155968fa128
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T08:33:23.000Z'
    data:
      edited: false
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9928531050682068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>Yes, I installed also ExLLama, but the problem I''ve shown previously
          it doesn''t happen only with it anyway. It happens also with the other loaders,
          such as AutoGPTQ.</p>

          '
        raw: Yes, I installed also ExLLama, but the problem I've shown previously
          it doesn't happen only with it anyway. It happens also with the other loaders,
          such as AutoGPTQ.
        updatedAt: '2023-07-26T08:33:23.663Z'
      numEdits: 0
      reactions: []
    id: 64c0da53d71cdcdcc81ff60f
    type: comment
  author: anon7463435254
  content: Yes, I installed also ExLLama, but the problem I've shown previously it
    doesn't happen only with it anyway. It happens also with the other loaders, such
    as AutoGPTQ.
  created_at: 2023-07-26 07:33:23+00:00
  edited: false
  hidden: false
  id: 64c0da53d71cdcdcc81ff60f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T08:47:34.000Z'
    data:
      edited: true
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8873983025550842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>Just to be sure, I replaced my code with the snippet you sent me,
          and this is the complete code of the Colab I''m running:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/c4lvaIAQTg8ZtxkMacvGI.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/c4lvaIAQTg8ZtxkMacvGI.png"></a></p>

          <p>Same as before. The model is loaded (after doing the usual stuff on the
          model tab and selecting ExLLAMA), but it replies with that empty string.</p>

          '
        raw: 'Just to be sure, I replaced my code with the snippet you sent me, and
          this is the complete code of the Colab I''m running:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/c4lvaIAQTg8ZtxkMacvGI.png)


          Same as before. The model is loaded (after doing the usual stuff on the
          model tab and selecting ExLLAMA), but it replies with that empty string.


          '
        updatedAt: '2023-07-26T08:48:46.864Z'
      numEdits: 1
      reactions: []
    id: 64c0dda6f04093c088305418
    type: comment
  author: anon7463435254
  content: 'Just to be sure, I replaced my code with the snippet you sent me, and
    this is the complete code of the Colab I''m running:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/c4lvaIAQTg8ZtxkMacvGI.png)


    Same as before. The model is loaded (after doing the usual stuff on the model
    tab and selecting ExLLAMA), but it replies with that empty string.


    '
  created_at: 2023-07-26 07:47:34+00:00
  edited: true
  hidden: false
  id: 64c0dda6f04093c088305418
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T09:01:49.000Z'
    data:
      edited: false
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.82253098487854
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>i always encounter this issue whenever I am not selecting the correct
          instruction template model. Nouse Hermes is compatible with Alpaca prompting.
          probably same with your problem. </p>

          '
        raw: 'i always encounter this issue whenever I am not selecting the correct
          instruction template model. Nouse Hermes is compatible with Alpaca prompting.
          probably same with your problem. '
        updatedAt: '2023-07-26T09:01:49.140Z'
      numEdits: 0
      reactions: []
    id: 64c0e0fd72e951e3fe4855aa
    type: comment
  author: Dxtrmst
  content: 'i always encounter this issue whenever I am not selecting the correct
    instruction template model. Nouse Hermes is compatible with Alpaca prompting.
    probably same with your problem. '
  created_at: 2023-07-26 08:01:49+00:00
  edited: false
  hidden: false
  id: 64c0e0fd72e951e3fe4855aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T09:35:55.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-26T09:43:15.503Z'
      numEdits: 0
      reactions: []
    id: 64c0e8fb9832a455bc2e8aad
    type: comment
  author: anon7463435254
  content: This comment has been hidden
  created_at: 2023-07-26 08:35:55+00:00
  edited: true
  hidden: true
  id: 64c0e8fb9832a455bc2e8aad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T09:42:00.000Z'
    data:
      edited: true
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8577911257743835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<blockquote>

          <p>i always encounter this issue whenever I am not selecting the correct
          instruction template model. Nouse Hermes is compatible with Alpaca prompting.
          probably same with your problem.</p>

          </blockquote>

          <p>Just tried with Nous Hermes, using the colab code I put in the previous
          message and Alpaca instruction template and nothing changes.<br>I get this
          error:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/vTWO79TghOQU8MGF3CfIA.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/vTWO79TghOQU8MGF3CfIA.png"></a></p>

          <p>I''m starting to think that I have something wrong with the installation
          of something.<br>Could you please provide me the code you use to do the
          whole process, from the installation to run the models?</p>

          '
        raw: "> i always encounter this issue whenever I am not selecting the correct\
          \ instruction template model. Nouse Hermes is compatible with Alpaca prompting.\
          \ probably same with your problem.\n\nJust tried with Nous Hermes, using\
          \ the colab code I put in the previous message and Alpaca instruction template\
          \ and nothing changes. \nI get this error:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/vTWO79TghOQU8MGF3CfIA.png)\n\
          \nI'm starting to think that I have something wrong with the installation\
          \ of something. \nCould you please provide me the code you use to do the\
          \ whole process, from the installation to run the models?\n\n"
        updatedAt: '2023-07-26T09:42:51.940Z'
      numEdits: 1
      reactions: []
    id: 64c0ea68b85ee9e4222fa03a
    type: comment
  author: anon7463435254
  content: "> i always encounter this issue whenever I am not selecting the correct\
    \ instruction template model. Nouse Hermes is compatible with Alpaca prompting.\
    \ probably same with your problem.\n\nJust tried with Nous Hermes, using the colab\
    \ code I put in the previous message and Alpaca instruction template and nothing\
    \ changes. \nI get this error:\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/vTWO79TghOQU8MGF3CfIA.png)\n\
    \nI'm starting to think that I have something wrong with the installation of something.\
    \ \nCould you please provide me the code you use to do the whole process, from\
    \ the installation to run the models?\n\n"
  created_at: 2023-07-26 08:42:00+00:00
  edited: true
  hidden: false
  id: 64c0ea68b85ee9e4222fa03a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T12:21:45.000Z'
    data:
      edited: false
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2823665142059326
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>i still use this same code in colab google</p>

          <p>#@title 2. Install the Web UI &amp; LLM<br>import os<br>import shutil<br>from
          IPython.display import clear_output<br>%cd /content/<br>!apt-get -y install
          -qq aria2</p>

          <p>!git clone <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a><br>%cd
          /content/text-generation-webui</p>

          <p>!pip install -r requirements.txt<br>!pip install -U gradio==3.28.3</p>

          <p>!mkdir /content/text-generation-webui/repositories<br>%cd /content/text-generation-webui/repositories<br>!git
          clone -b cuda <a rel="nofollow" href="https://github.com/oobabooga/GPTQ-for-LLaMa.git">https://github.com/oobabooga/GPTQ-for-LLaMa.git</a><br>%cd
          GPTQ-for-LLaMa<br>!python setup_cuda.py install</p>

          <p>%cd /content/text-generation-webui/extensions/api<br>!pip install -r
          requirements.txt</p>

          <p>%cd /content/text-generation-webui<br>!python server.py --share --chat
          --api --public-api</p>

          '
        raw: 'i still use this same code in colab google


          #@title 2. Install the Web UI & LLM

          import os

          import shutil

          from IPython.display import clear_output

          %cd /content/

          !apt-get -y install -qq aria2


          !git clone https://github.com/oobabooga/text-generation-webui

          %cd /content/text-generation-webui


          !pip install -r requirements.txt

          !pip install -U gradio==3.28.3


          !mkdir /content/text-generation-webui/repositories

          %cd /content/text-generation-webui/repositories

          !git clone -b cuda https://github.com/oobabooga/GPTQ-for-LLaMa.git

          %cd GPTQ-for-LLaMa

          !python setup_cuda.py install


          %cd /content/text-generation-webui/extensions/api

          !pip install -r requirements.txt


          %cd /content/text-generation-webui

          !python server.py --share --chat --api --public-api'
        updatedAt: '2023-07-26T12:21:45.333Z'
      numEdits: 0
      reactions: []
    id: 64c10fd99832a455bc348417
    type: comment
  author: Dxtrmst
  content: 'i still use this same code in colab google


    #@title 2. Install the Web UI & LLM

    import os

    import shutil

    from IPython.display import clear_output

    %cd /content/

    !apt-get -y install -qq aria2


    !git clone https://github.com/oobabooga/text-generation-webui

    %cd /content/text-generation-webui


    !pip install -r requirements.txt

    !pip install -U gradio==3.28.3


    !mkdir /content/text-generation-webui/repositories

    %cd /content/text-generation-webui/repositories

    !git clone -b cuda https://github.com/oobabooga/GPTQ-for-LLaMa.git

    %cd GPTQ-for-LLaMa

    !python setup_cuda.py install


    %cd /content/text-generation-webui/extensions/api

    !pip install -r requirements.txt


    %cd /content/text-generation-webui

    !python server.py --share --chat --api --public-api'
  created_at: 2023-07-26 11:21:45+00:00
  edited: false
  hidden: false
  id: 64c10fd99832a455bc348417
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T12:32:18.000Z'
    data:
      edited: true
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5989463925361633
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>just checking it now, it worked with Vicuna and alpaca (both)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/odQ4J7T3nLr0imuTCexgw.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/odQ4J7T3nLr0imuTCexgw.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/C6LmHOUUno_GJTElloeg7.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/C6LmHOUUno_GJTElloeg7.png"></a></p>

          <p>then tested in prompt</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/aC9Q1pBR2DHapyxeVR11M.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/aC9Q1pBR2DHapyxeVR11M.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/Ui5P2oLyW9SLSEpGUg8VZ.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/Ui5P2oLyW9SLSEpGUg8VZ.png"></a></p>

          '
        raw: 'just checking it now, it worked with Vicuna and alpaca (both)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/odQ4J7T3nLr0imuTCexgw.png)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/C6LmHOUUno_GJTElloeg7.png)


          then tested in prompt


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/aC9Q1pBR2DHapyxeVR11M.png)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/Ui5P2oLyW9SLSEpGUg8VZ.png)

          '
        updatedAt: '2023-07-26T12:57:16.207Z'
      numEdits: 1
      reactions: []
    id: 64c112528137192a1e36b86c
    type: comment
  author: Dxtrmst
  content: 'just checking it now, it worked with Vicuna and alpaca (both)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/odQ4J7T3nLr0imuTCexgw.png)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/C6LmHOUUno_GJTElloeg7.png)


    then tested in prompt


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/aC9Q1pBR2DHapyxeVR11M.png)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/Ui5P2oLyW9SLSEpGUg8VZ.png)

    '
  created_at: 2023-07-26 11:32:18+00:00
  edited: true
  hidden: false
  id: 64c112528137192a1e36b86c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T12:41:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8453934788703918
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh, that old CUDA version of GPTQ-for-LLaMA  is no longer supported</p>

          <p>Please use AutoGPTQ 0.3.1 or exllama.  ExLlama is much faster, and that
          is the recommended option in text-generation-webui.</p>

          '
        raw: 'Oh, that old CUDA version of GPTQ-for-LLaMA  is no longer supported


          Please use AutoGPTQ 0.3.1 or exllama.  ExLlama is much faster, and that
          is the recommended option in text-generation-webui.'
        updatedAt: '2023-07-26T12:41:44.005Z'
      numEdits: 0
      reactions: []
    id: 64c114886441eb1558767e56
    type: comment
  author: TheBloke
  content: 'Oh, that old CUDA version of GPTQ-for-LLaMA  is no longer supported


    Please use AutoGPTQ 0.3.1 or exllama.  ExLlama is much faster, and that is the
    recommended option in text-generation-webui.'
  created_at: 2023-07-26 11:41:44+00:00
  edited: false
  hidden: false
  id: 64c114886441eb1558767e56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T12:44:53.000Z'
    data:
      edited: false
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7363537549972534
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>yes bro. i still haven''t update that part. i did not mind because
          when i get inside the webui, i can find exllama already.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/mRLR-sKgENA6utfMkNh1L.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/mRLR-sKgENA6utfMkNh1L.png"></a></p>

          '
        raw: 'yes bro. i still haven''t update that part. i did not mind because when
          i get inside the webui, i can find exllama already.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/mRLR-sKgENA6utfMkNh1L.png)

          '
        updatedAt: '2023-07-26T12:44:53.157Z'
      numEdits: 0
      reactions: []
    id: 64c1154562983511b966117a
    type: comment
  author: Dxtrmst
  content: 'yes bro. i still haven''t update that part. i did not mind because when
    i get inside the webui, i can find exllama already.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/mRLR-sKgENA6utfMkNh1L.png)

    '
  created_at: 2023-07-26 11:44:53+00:00
  edited: false
  hidden: false
  id: 64c1154562983511b966117a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T12:47:28.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9804075956344604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK so it is working OK for you with ExLlama? That''s fine then!</p>

          '
        raw: OK so it is working OK for you with ExLlama? That's fine then!
        updatedAt: '2023-07-26T12:47:34.403Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LaferriereJC
    id: 64c115e0f613170e7bfc84b2
    type: comment
  author: TheBloke
  content: OK so it is working OK for you with ExLlama? That's fine then!
  created_at: 2023-07-26 11:47:28+00:00
  edited: true
  hidden: false
  id: 64c115e0f613170e7bfc84b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T12:49:13.000Z'
    data:
      edited: false
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9357237219810486
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>yes. working fine. thanks.</p>

          '
        raw: yes. working fine. thanks.
        updatedAt: '2023-07-26T12:49:13.212Z'
      numEdits: 0
      reactions: []
    id: 64c1164972e951e3fe50a512
    type: comment
  author: Dxtrmst
  content: yes. working fine. thanks.
  created_at: 2023-07-26 11:49:13+00:00
  edited: false
  hidden: false
  id: 64c1164972e951e3fe50a512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-26T18:21:03.000Z'
    data:
      edited: false
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272530674934387
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>I found the problem, guys. The problem is in the value of "max_new_tokens".
          If you want to try, try to set it to 4096 and then send messages. The error
          will appear.<br>I also noticed that the greater the value the more distant
          from the context and confused is the response. This behaviour is clearly
          visible with values greater enough than 2048 and with models like Nous Hermes,
          Wizard LM and Dolphin.<br>It seems that LLaMA2-chat-GPTQ can handle it (meaning
          that it does not show those empty messages), but if you set an high value,
          it will forget the context (even the previous response) and send more confused
          responses.<br>Let me know if you can try it.</p>

          '
        raw: "I found the problem, guys. The problem is in the value of \"max_new_tokens\"\
          . If you want to try, try to set it to 4096 and then send messages. The\
          \ error will appear.\nI also noticed that the greater the value the more\
          \ distant from the context and confused is the response. This behaviour\
          \ is clearly visible with values greater enough than 2048 and with models\
          \ like Nous Hermes, Wizard LM and Dolphin.  \nIt seems that LLaMA2-chat-GPTQ\
          \ can handle it (meaning that it does not show those empty messages), but\
          \ if you set an high value, it will forget the context (even the previous\
          \ response) and send more confused responses.\nLet me know if you can try\
          \ it."
        updatedAt: '2023-07-26T18:21:03.734Z'
      numEdits: 0
      reactions: []
    id: 64c1640fc9ed8d21e64b57b8
    type: comment
  author: anon7463435254
  content: "I found the problem, guys. The problem is in the value of \"max_new_tokens\"\
    . If you want to try, try to set it to 4096 and then send messages. The error\
    \ will appear.\nI also noticed that the greater the value the more distant from\
    \ the context and confused is the response. This behaviour is clearly visible\
    \ with values greater enough than 2048 and with models like Nous Hermes, Wizard\
    \ LM and Dolphin.  \nIt seems that LLaMA2-chat-GPTQ can handle it (meaning that\
    \ it does not show those empty messages), but if you set an high value, it will\
    \ forget the context (even the previous response) and send more confused responses.\n\
    Let me know if you can try it."
  created_at: 2023-07-26 17:21:03+00:00
  edited: false
  hidden: false
  id: 64c1640fc9ed8d21e64b57b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T18:49:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581642150878906
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ahh yeah, I''ve heard that - that''s likely a bug in text-generation-webui
          or ExLlama I believe.  Note this isn''t directly related to the context
          of the model which is set on the Model screen.  But yes I was told that
          if you let it generate more than 2K new tokens, it might not respond or
          might throw errors.  </p>

          '
        raw: 'Ahh yeah, I''ve heard that - that''s likely a bug in text-generation-webui
          or ExLlama I believe.  Note this isn''t directly related to the context
          of the model which is set on the Model screen.  But yes I was told that
          if you let it generate more than 2K new tokens, it might not respond or
          might throw errors.  '
        updatedAt: '2023-07-26T18:49:48.626Z'
      numEdits: 0
      reactions: []
    id: 64c16acc5258457cc524cb33
    type: comment
  author: TheBloke
  content: 'Ahh yeah, I''ve heard that - that''s likely a bug in text-generation-webui
    or ExLlama I believe.  Note this isn''t directly related to the context of the
    model which is set on the Model screen.  But yes I was told that if you let it
    generate more than 2K new tokens, it might not respond or might throw errors.  '
  created_at: 2023-07-26 17:49:48+00:00
  edited: false
  hidden: false
  id: 64c16acc5258457cc524cb33
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardLM-13B-V1.2-GPTQ
repo_type: model
status: open
target_branch: null
title: Receive empty response, regardless of which loader I choose
