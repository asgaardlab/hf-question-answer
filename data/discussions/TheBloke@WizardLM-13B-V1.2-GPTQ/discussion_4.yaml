!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nobitha
conflicting_files: null
created_at: 2023-07-27 08:52:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16b6cef3995753c1c131e419fde2fd1c.svg
      fullname: 'prameela reddi '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nobitha
      type: user
    createdAt: '2023-07-27T09:52:07.000Z'
    data:
      edited: false
      editors:
      - nobitha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7153712511062622
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16b6cef3995753c1c131e419fde2fd1c.svg
          fullname: 'prameela reddi '
          isHf: false
          isPro: false
          name: nobitha
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ ...<br>Great thanks !!!<br>can I connect my database to model... like\
          \ Postgres and MySQL ....</p>\n"
        raw: "Hello @TheBloke ...\r\nGreat thanks !!! \r\ncan I connect my database\
          \ to model... like Postgres and MySQL ...."
        updatedAt: '2023-07-27T09:52:07.788Z'
      numEdits: 0
      reactions: []
    id: 64c23e47c8a5077628388f9e
    type: comment
  author: nobitha
  content: "Hello @TheBloke ...\r\nGreat thanks !!! \r\ncan I connect my database\
    \ to model... like Postgres and MySQL ...."
  created_at: 2023-07-27 08:52:07+00:00
  edited: false
  hidden: false
  id: 64c23e47c8a5077628388f9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T10:01:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9738001823425293
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Potentially yes, using something like LangChain.  But you would
          need to investigate how to use a GPTQ model with LangChain; I''ve not looked
          into that myself yet.</p>

          '
        raw: Potentially yes, using something like LangChain.  But you would need
          to investigate how to use a GPTQ model with LangChain; I've not looked into
          that myself yet.
        updatedAt: '2023-07-27T10:01:15.279Z'
      numEdits: 0
      reactions: []
    id: 64c2406b2ae1946c297f146b
    type: comment
  author: TheBloke
  content: Potentially yes, using something like LangChain.  But you would need to
    investigate how to use a GPTQ model with LangChain; I've not looked into that
    myself yet.
  created_at: 2023-07-27 09:01:15+00:00
  edited: false
  hidden: false
  id: 64c2406b2ae1946c297f146b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d23de0ea0323e9b759995395958594cf.svg
      fullname: Florian S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Florian0585
      type: user
    createdAt: '2023-07-27T11:37:29.000Z'
    data:
      edited: true
      editors:
      - Florian0585
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7376379370689392
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d23de0ea0323e9b759995395958594cf.svg
          fullname: Florian S
          isHf: false
          isPro: false
          name: Florian0585
          type: user
        html: "<p>Hi  <span data-props=\"{&quot;user&quot;:&quot;nobitha&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nobitha\"\
          >@<span class=\"underline\">nobitha</span></a></span>\n\n\t</span></span>,\
          \  I did't connect to a database, but the first step of the idea of <span\
          \ data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I did (use GPTQ\
          \ model within Langchain), see below for a rough code snippet (no parameters,\
          \ device or model name selected, you could do it like described in the readme).\
          \ Maybe you can continue from there on.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline\n<span class=\"hljs-keyword\">from</span> langchain <span class=\"\
          hljs-keyword\">import</span> HuggingFacePipeline\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,model_basename=model_basename)\n\
          pipe = pipeline(<span class=\"hljs-string\">\"text-generation\"</span>,model=model,tokenizer=tokenizer)\n\
          llm = HuggingFacePipeline(pipeline=pipe)\n</code></pre>\n<p> The <code>llm</code>\
          \ object can then be used in any Langchain function which needs <code>llm</code>\
          \ input, e.g. <code>LLMChain</code> or <code>ConversationChain</code>. Maybe\
          \ that helps or someone has a more direct way?</p>\n"
        raw: "Hi  @nobitha,  I did't connect to a database, but the first step of\
          \ the idea of @TheBloke I did (use GPTQ model within Langchain), see below\
          \ for a rough code snippet (no parameters, device or model name selected,\
          \ you could do it like described in the readme). Maybe you can continue\
          \ from there on.\n\n```python\nfrom auto_gptq import AutoGPTQForCausalLM\n\
          from transformers import AutoTokenizer, pipeline\nfrom langchain import\
          \ HuggingFacePipeline\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,model_basename=model_basename)\n\
          pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\nllm\
          \ = HuggingFacePipeline(pipeline=pipe)\n```\n The `llm` object can then\
          \ be used in any Langchain function which needs `llm` input, e.g. `LLMChain`\
          \ or `ConversationChain`. Maybe that helps or someone has a more direct\
          \ way?"
        updatedAt: '2023-07-27T11:45:33.498Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - sauravm8
    id: 64c256f99a861c567330813c
    type: comment
  author: Florian0585
  content: "Hi  @nobitha,  I did't connect to a database, but the first step of the\
    \ idea of @TheBloke I did (use GPTQ model within Langchain), see below for a rough\
    \ code snippet (no parameters, device or model name selected, you could do it\
    \ like described in the readme). Maybe you can continue from there on.\n\n```python\n\
    from auto_gptq import AutoGPTQForCausalLM\nfrom transformers import AutoTokenizer,\
    \ pipeline\nfrom langchain import HuggingFacePipeline\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,model_basename=model_basename)\n\
    pipe = pipeline(\"text-generation\",model=model,tokenizer=tokenizer)\nllm = HuggingFacePipeline(pipeline=pipe)\n\
    ```\n The `llm` object can then be used in any Langchain function which needs\
    \ `llm` input, e.g. `LLMChain` or `ConversationChain`. Maybe that helps or someone\
    \ has a more direct way?"
  created_at: 2023-07-27 10:37:29+00:00
  edited: true
  hidden: false
  id: 64c256f99a861c567330813c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/WizardLM-13B-V1.2-GPTQ
repo_type: model
status: open
target_branch: null
title: database connection ?
