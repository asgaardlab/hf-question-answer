!!python/object:huggingface_hub.community.DiscussionWithDetails
author: omasoud
conflicting_files: null
created_at: 2023-07-25 23:13:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d89043538d49ea341ca7b311bdf66f7a.svg
      fullname: Osama Masoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: omasoud
      type: user
    createdAt: '2023-07-26T00:13:31.000Z'
    data:
      edited: false
      editors:
      - omasoud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9411231279373169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d89043538d49ea341ca7b311bdf66f7a.svg
          fullname: Osama Masoud
          isHf: false
          isPro: false
          name: omasoud
          type: user
        html: '<p>Thanks The Bloke for putting this up so quickly!<br>I''m wondering
          if people tried this already. I''m getting very different results from this
          GPTQ version (worse) compared to what I get with wizardlm-13b-v1.2.ggmlv3.q4_K_M.</p>

          '
        raw: "Thanks The Bloke for putting this up so quickly!\r\nI'm wondering if\
          \ people tried this already. I'm getting very different results from this\
          \ GPTQ version (worse) compared to what I get with wizardlm-13b-v1.2.ggmlv3.q4_K_M."
        updatedAt: '2023-07-26T00:13:31.248Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Juqowel
        - ramgpt
        - TripleExclam
    id: 64c0652bb746fe51543f7f57
    type: comment
  author: omasoud
  content: "Thanks The Bloke for putting this up so quickly!\r\nI'm wondering if people\
    \ tried this already. I'm getting very different results from this GPTQ version\
    \ (worse) compared to what I get with wizardlm-13b-v1.2.ggmlv3.q4_K_M."
  created_at: 2023-07-25 23:13:31+00:00
  edited: false
  hidden: false
  id: 64c0652bb746fe51543f7f57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc49c865b648b2df915ca9/fsBvOiniLfRoaKZxSZB6a.png?w=200&h=200&f=face
      fullname: ramGPT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramgpt
      type: user
    createdAt: '2023-07-26T03:19:27.000Z'
    data:
      edited: false
      editors:
      - ramgpt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8911316394805908
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc49c865b648b2df915ca9/fsBvOiniLfRoaKZxSZB6a.png?w=200&h=200&f=face
          fullname: ramGPT
          isHf: false
          isPro: false
          name: ramgpt
          type: user
        html: "<p>summary: </p>\n<p>same issue here ,getting decent speed with one\
          \ 3060 12GB (8 - 15 tokens/s)but results are just gibberish . on the other\
          \ hand, the GGML version works flawlessly with same chat template ~2 token/s:</p>\n\
          <hr>\n<p>logs:<br>2023-07-25 23:02:48 INFO:Loading TheBloke_WizardLM-13B-V1.2-GPTQ...<br>2023-07-25\
          \ 23:02:48 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-128g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True}<br>2023-07-25\
          \ 23:02:54 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.<br>2023-07-25 23:02:54 WARNING:models/TheBloke_WizardLM-13B-V1.2-GPTQ/tokenizer_config.json\
          \ is different from the original LlamaTokenizer file. It is either customized\
          \ or outdated.<br>2023-07-25 23:02:54 WARNING:models/TheBloke_WizardLM-13B-V1.2-GPTQ/special_tokens_map.json\
          \ is different from the original LlamaTokenizer file. It is either customized\
          \ or outdated.<br>2023-07-25 23:02:54 INFO:Loaded the model in 6.06 seconds.</p>\n\
          <p>Output generated in 9.31 seconds (15.78 tokens/s, 147 tokens, context\
          \ 28, seed 208326955)<br>Output generated in 9.33 seconds (13.72 tokens/s,\
          \ 128 tokens, context 177, seed 95458235)<br>Output generated in 2.91 seconds\
          \ (8.60 tokens/s, 25 tokens, context 45, seed 687169903)<br>Output generated\
          \ in 14.53 seconds (13.69 tokens/s, 199 tokens, context 85, seed 1136582587)</p>\n\
          <hr>\n<p>sample chat :<br>USER: who are you<br>ASSISTANT: Oh my goodness,\
          \ it has been established that they know best when dealing with you have\
          \ a question about them (and we hope they enjoy answering your question\
          \ while enjoying a strong cup of coffee). So sit back, relax, and let us\
          \ give you some advice \u2013 take five while you use the rest of you down\
          \ pay attention to what you do not pay attention has taught you that life\
          \ is too shorten you have ever used use the information, so forget about\
          \ using the information has already used you have ever considered you have\
          \ ever noticed you have always thought you have ever notice that you have\
          \ ever noticed you have always thought you have ever noticed you have ever\
          \ noticed you have always known you have ever noticed you have ever known\
          \ you have always known you have ever known you have always known you have\
          \ ever known you have always known you have ever known you have always known\
          \ you have ever known you have always known you have ever known you have\
          \ always known you have ever known you have always known you have ever<br>USER:\
          \ hello<br>ASISSTANT: I\u2019m sorry for any delay, but please accept it\
          \ as it is given, and remember that silence is golden!</p>\n<hr>\n<p>template:<br>user\
          \ string : USER: {prompt}<br>bot string: ASSISTANT:<br>context:A chat between\
          \ a curious user and an artificial intelligence assistant. The assistant\
          \ gives helpful, detailed, and polite answers to the user's questions.</p>\n"
        raw: "summary: \n\nsame issue here ,getting decent speed with one 3060 12GB\
          \ (8 - 15 tokens/s)but results are just gibberish . on the other hand, the\
          \ GGML version works flawlessly with same chat template ~2 token/s:\n\n\
          ------------------------\nlogs:\n2023-07-25 23:02:48 INFO:Loading TheBloke_WizardLM-13B-V1.2-GPTQ...\n\
          2023-07-25 23:02:48 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-128g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True}\n\
          2023-07-25 23:02:54 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel\
          \ not support integrate without triton yet.\n2023-07-25 23:02:54 WARNING:models/TheBloke_WizardLM-13B-V1.2-GPTQ/tokenizer_config.json\
          \ is different from the original LlamaTokenizer file. It is either customized\
          \ or outdated.\n2023-07-25 23:02:54 WARNING:models/TheBloke_WizardLM-13B-V1.2-GPTQ/special_tokens_map.json\
          \ is different from the original LlamaTokenizer file. It is either customized\
          \ or outdated.\n2023-07-25 23:02:54 INFO:Loaded the model in 6.06 seconds.\n\
          \nOutput generated in 9.31 seconds (15.78 tokens/s, 147 tokens, context\
          \ 28, seed 208326955)\nOutput generated in 9.33 seconds (13.72 tokens/s,\
          \ 128 tokens, context 177, seed 95458235)\nOutput generated in 2.91 seconds\
          \ (8.60 tokens/s, 25 tokens, context 45, seed 687169903)\nOutput generated\
          \ in 14.53 seconds (13.69 tokens/s, 199 tokens, context 85, seed 1136582587)\n\
          -----------------------\nsample chat :\nUSER: who are you\nASSISTANT: Oh\
          \ my goodness, it has been established that they know best when dealing\
          \ with you have a question about them (and we hope they enjoy answering\
          \ your question while enjoying a strong cup of coffee). So sit back, relax,\
          \ and let us give you some advice \u2013 take five while you use the rest\
          \ of you down pay attention to what you do not pay attention has taught\
          \ you that life is too shorten you have ever used use the information, so\
          \ forget about using the information has already used you have ever considered\
          \ you have ever noticed you have always thought you have ever notice that\
          \ you have ever noticed you have always thought you have ever noticed you\
          \ have ever noticed you have always known you have ever noticed you have\
          \ ever known you have always known you have ever known you have always known\
          \ you have ever known you have always known you have ever known you have\
          \ always known you have ever known you have always known you have ever known\
          \ you have always known you have ever known you have always known you have\
          \ ever\nUSER: hello \nASISSTANT: I\u2019m sorry for any delay, but please\
          \ accept it as it is given, and remember that silence is golden!\n-------------------------\n\
          template:\nuser string : USER: {prompt}\nbot string: ASSISTANT:\ncontext:A\
          \ chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions.\n\n"
        updatedAt: '2023-07-26T03:19:27.885Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - omasoud
        - Juqowel
    id: 64c090bf84191336fa03258f
    type: comment
  author: ramgpt
  content: "summary: \n\nsame issue here ,getting decent speed with one 3060 12GB\
    \ (8 - 15 tokens/s)but results are just gibberish . on the other hand, the GGML\
    \ version works flawlessly with same chat template ~2 token/s:\n\n------------------------\n\
    logs:\n2023-07-25 23:02:48 INFO:Loading TheBloke_WizardLM-13B-V1.2-GPTQ...\n2023-07-25\
    \ 23:02:48 INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-4bit-128g',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None, 'use_cuda_fp16': True}\n2023-07-25 23:02:54 WARNING:skip\
    \ module injection for FusedLlamaMLPForQuantizedModel not support integrate without\
    \ triton yet.\n2023-07-25 23:02:54 WARNING:models/TheBloke_WizardLM-13B-V1.2-GPTQ/tokenizer_config.json\
    \ is different from the original LlamaTokenizer file. It is either customized\
    \ or outdated.\n2023-07-25 23:02:54 WARNING:models/TheBloke_WizardLM-13B-V1.2-GPTQ/special_tokens_map.json\
    \ is different from the original LlamaTokenizer file. It is either customized\
    \ or outdated.\n2023-07-25 23:02:54 INFO:Loaded the model in 6.06 seconds.\n\n\
    Output generated in 9.31 seconds (15.78 tokens/s, 147 tokens, context 28, seed\
    \ 208326955)\nOutput generated in 9.33 seconds (13.72 tokens/s, 128 tokens, context\
    \ 177, seed 95458235)\nOutput generated in 2.91 seconds (8.60 tokens/s, 25 tokens,\
    \ context 45, seed 687169903)\nOutput generated in 14.53 seconds (13.69 tokens/s,\
    \ 199 tokens, context 85, seed 1136582587)\n-----------------------\nsample chat\
    \ :\nUSER: who are you\nASSISTANT: Oh my goodness, it has been established that\
    \ they know best when dealing with you have a question about them (and we hope\
    \ they enjoy answering your question while enjoying a strong cup of coffee). So\
    \ sit back, relax, and let us give you some advice \u2013 take five while you\
    \ use the rest of you down pay attention to what you do not pay attention has\
    \ taught you that life is too shorten you have ever used use the information,\
    \ so forget about using the information has already used you have ever considered\
    \ you have ever noticed you have always thought you have ever notice that you\
    \ have ever noticed you have always thought you have ever noticed you have ever\
    \ noticed you have always known you have ever noticed you have ever known you\
    \ have always known you have ever known you have always known you have ever known\
    \ you have always known you have ever known you have always known you have ever\
    \ known you have always known you have ever known you have always known you have\
    \ ever known you have always known you have ever\nUSER: hello \nASISSTANT: I\u2019\
    m sorry for any delay, but please accept it as it is given, and remember that\
    \ silence is golden!\n-------------------------\ntemplate:\nuser string : USER:\
    \ {prompt}\nbot string: ASSISTANT:\ncontext:A chat between a curious user and\
    \ an artificial intelligence assistant. The assistant gives helpful, detailed,\
    \ and polite answers to the user's questions.\n\n"
  created_at: 2023-07-26 02:19:27+00:00
  edited: false
  hidden: false
  id: 64c090bf84191336fa03258f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T12:40:15.000Z'
    data:
      edited: false
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6653929352760315
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>it is perfectly working.<br>tried in ex-llama with vicuna instruction
          prompting</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/7egEtqdHR4RbpoQvYrtLp.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/7egEtqdHR4RbpoQvYrtLp.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/xm2c-MRn442psfiWiiOgI.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/xm2c-MRn442psfiWiiOgI.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/IiE8HGcx_D89wAsd2CK2t.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/IiE8HGcx_D89wAsd2CK2t.png"></a></p>

          '
        raw: 'it is perfectly working.

          tried in ex-llama with vicuna instruction prompting


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/7egEtqdHR4RbpoQvYrtLp.png)



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/xm2c-MRn442psfiWiiOgI.png)


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/IiE8HGcx_D89wAsd2CK2t.png)

          '
        updatedAt: '2023-07-26T12:40:15.738Z'
      numEdits: 0
      reactions: []
    id: 64c1142ff7f5bd062fc40184
    type: comment
  author: Dxtrmst
  content: 'it is perfectly working.

    tried in ex-llama with vicuna instruction prompting


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/7egEtqdHR4RbpoQvYrtLp.png)



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/xm2c-MRn442psfiWiiOgI.png)


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/644c5d69a25a81b66a7c5ca8/IiE8HGcx_D89wAsd2CK2t.png)

    '
  created_at: 2023-07-26 11:40:15+00:00
  edited: false
  hidden: false
  id: 64c1142ff7f5bd062fc40184
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T12:43:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9891067147254944
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Dxtrmst&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Dxtrmst\">@<span class=\"\
          underline\">Dxtrmst</span></a></span>\n\n\t</span></span>   are you using\
          \ Alpaca format or Vicuna prompt template?  You said you're using Vicuna,\
          \ but screenshot shows Alpaca?</p>\n<p>I listed it as Vicuna in the README,\
          \ but maybe I was wrong.  I wrote Vicuna because WizardLM v1.0 and 1.1 were\
          \ Vicuna.  But maybe they changed it for this model and haven't told anyone</p>\n"
        raw: '@Dxtrmst   are you using Alpaca format or Vicuna prompt template?  You
          said you''re using Vicuna, but screenshot shows Alpaca?


          I listed it as Vicuna in the README, but maybe I was wrong.  I wrote Vicuna
          because WizardLM v1.0 and 1.1 were Vicuna.  But maybe they changed it for
          this model and haven''t told anyone'
        updatedAt: '2023-07-26T12:43:29.558Z'
      numEdits: 0
      reactions: []
    id: 64c114f132dd2d752e7cac0e
    type: comment
  author: TheBloke
  content: '@Dxtrmst   are you using Alpaca format or Vicuna prompt template?  You
    said you''re using Vicuna, but screenshot shows Alpaca?


    I listed it as Vicuna in the README, but maybe I was wrong.  I wrote Vicuna because
    WizardLM v1.0 and 1.1 were Vicuna.  But maybe they changed it for this model and
    haven''t told anyone'
  created_at: 2023-07-26 11:43:29+00:00
  edited: false
  hidden: false
  id: 64c114f132dd2d752e7cac0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-07-26T13:02:52.000Z'
    data:
      edited: true
      editors:
      - Dxtrmst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9975966811180115
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: '<p>sorry. it should be alpaca i supposed to write. but i checked vicuna,
          it is also working</p>

          '
        raw: sorry. it should be alpaca i supposed to write. but i checked vicuna,
          it is also working
        updatedAt: '2023-07-26T13:03:07.572Z'
      numEdits: 1
      reactions: []
    id: 64c1197c8e2612254377c14f
    type: comment
  author: Dxtrmst
  content: sorry. it should be alpaca i supposed to write. but i checked vicuna, it
    is also working
  created_at: 2023-07-26 12:02:52+00:00
  edited: true
  hidden: false
  id: 64c1197c8e2612254377c14f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
      fullname: Khalil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abdelhak
      type: user
    createdAt: '2023-07-26T14:29:32.000Z'
    data:
      edited: false
      editors:
      - Abdelhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6210831999778748
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
          fullname: Khalil
          isHf: false
          isPro: false
          name: Abdelhak
          type: user
        html: '<p>Hello, The Bloke, I am facing an issue that I never faced before
          when I try to load  WizardLM-13B-V1.2-GPTQ:<br>2023-07-26 15:28:02 INFO:Loading
          WizardLM-13B-V1.2-GPTQ...<br>2023-07-26 15:28:02 INFO:The AutoGPTQ params
          are: {''model_basename'': ''gptq_model-8bit-128g'', ''device'': ''cuda:0'',
          ''use_triton'': False, ''inject_fused_attention'': True, ''inject_fused_mlp'':
          True, ''use_safetensors'': True, ''trust_remote_code'': False, ''max_memory'':
          None, ''quantize_config'': None, ''use_cuda_fp16'': True}<br>2023-07-26
          15:28:05 WARNING:The model weights are not tied. Please use the <code>tie_weights</code>
          method before using the <code>infer_auto_device</code> function.<br>New
          environment: webui-1click</p>

          <p>Done!<br>Press any key to continue . . .</p>

          <p>Could you please shed some light on this? Is it related to the model
          or to my system? I am on windows.</p>

          '
        raw: 'Hello, The Bloke, I am facing an issue that I never faced before when
          I try to load  WizardLM-13B-V1.2-GPTQ:

          2023-07-26 15:28:02 INFO:Loading WizardLM-13B-V1.2-GPTQ...

          2023-07-26 15:28:02 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-8bit-128g'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': None, ''quantize_config'': None, ''use_cuda_fp16'':
          True}

          2023-07-26 15:28:05 WARNING:The model weights are not tied. Please use the
          `tie_weights` method before using the `infer_auto_device` function.

          New environment: webui-1click


          Done!

          Press any key to continue . . .


          Could you please shed some light on this? Is it related to the model or
          to my system? I am on windows.'
        updatedAt: '2023-07-26T14:29:32.581Z'
      numEdits: 0
      reactions: []
    id: 64c12dccfac222956b69091e
    type: comment
  author: Abdelhak
  content: 'Hello, The Bloke, I am facing an issue that I never faced before when
    I try to load  WizardLM-13B-V1.2-GPTQ:

    2023-07-26 15:28:02 INFO:Loading WizardLM-13B-V1.2-GPTQ...

    2023-07-26 15:28:02 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-8bit-128g'',
    ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'': True,
    ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
    False, ''max_memory'': None, ''quantize_config'': None, ''use_cuda_fp16'': True}

    2023-07-26 15:28:05 WARNING:The model weights are not tied. Please use the `tie_weights`
    method before using the `infer_auto_device` function.

    New environment: webui-1click


    Done!

    Press any key to continue . . .


    Could you please shed some light on this? Is it related to the model or to my
    system? I am on windows.'
  created_at: 2023-07-26 13:29:32+00:00
  edited: false
  hidden: false
  id: 64c12dccfac222956b69091e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc49c865b648b2df915ca9/fsBvOiniLfRoaKZxSZB6a.png?w=200&h=200&f=face
      fullname: ramGPT
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ramgpt
      type: user
    createdAt: '2023-07-26T14:59:47.000Z'
    data:
      edited: false
      editors:
      - ramgpt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8973697423934937
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc49c865b648b2df915ca9/fsBvOiniLfRoaKZxSZB6a.png?w=200&h=200&f=face
          fullname: ramGPT
          isHf: false
          isPro: false
          name: ramgpt
          type: user
        html: "<p>I have tried every template, the only functional one is ChatGLM.\
          \ <span data-props=\"{&quot;user&quot;:&quot;omasoud&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/omasoud\">@<span class=\"\
          underline\">omasoud</span></a></span>\n\n\t</span></span> try it :)<br>Thanks\
          \ <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span>  for this awesome\
          \ creation.</p>\n"
        raw: "I have tried every template, the only functional one is ChatGLM. @omasoud\
          \ try it :) \nThanks @TheBloke  for this awesome creation."
        updatedAt: '2023-07-26T14:59:47.970Z'
      numEdits: 0
      reactions: []
    id: 64c134e341e94f8264beb492
    type: comment
  author: ramgpt
  content: "I have tried every template, the only functional one is ChatGLM. @omasoud\
    \ try it :) \nThanks @TheBloke  for this awesome creation."
  created_at: 2023-07-26 13:59:47+00:00
  edited: false
  hidden: false
  id: 64c134e341e94f8264beb492
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rnosov
      type: user
    createdAt: '2023-07-26T17:31:54.000Z'
    data:
      edited: false
      editors:
      - rnosov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9507323503494263
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: rnosov
          type: user
        html: "<p>I'm getting gibberish responses with this quant using any prompt.<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> perhaps your\
          \ quantization script is getting affected by the recent change made for\
          \ OpenAssistant Orca 8k? Can you point me to where I can find the script\
          \ so I can try to debug it?<br>BTW, the latest requantized 8k Orca quant\
          \ is way, way better than the previous 4k length quant. It's the only quant\
          \ that seems to be able to produce coherent long form writing.</p>\n"
        raw: "I'm getting gibberish responses with this quant using any prompt. \n\
          @TheBloke perhaps your quantization script is getting affected by the recent\
          \ change made for OpenAssistant Orca 8k? Can you point me to where I can\
          \ find the script so I can try to debug it?\nBTW, the latest requantized\
          \ 8k Orca quant is way, way better than the previous 4k length quant. It's\
          \ the only quant that seems to be able to produce coherent long form writing."
        updatedAt: '2023-07-26T17:31:54.420Z'
      numEdits: 0
      reactions: []
    id: 64c1588a028ffaab2fdfacdb
    type: comment
  author: rnosov
  content: "I'm getting gibberish responses with this quant using any prompt. \n@TheBloke\
    \ perhaps your quantization script is getting affected by the recent change made\
    \ for OpenAssistant Orca 8k? Can you point me to where I can find the script so\
    \ I can try to debug it?\nBTW, the latest requantized 8k Orca quant is way, way\
    \ better than the previous 4k length quant. It's the only quant that seems to\
    \ be able to produce coherent long form writing."
  created_at: 2023-07-26 16:31:54+00:00
  edited: false
  hidden: false
  id: 64c1588a028ffaab2fdfacdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rnosov
      type: user
    createdAt: '2023-07-26T18:24:24.000Z'
    data:
      edited: false
      editors:
      - rnosov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9547102451324463
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: rnosov
          type: user
        html: '<p>I''ve done some more testing and "gptq-4bit-32g-actorder_True" branch
          is definitely broken. I could get some decent responses using the main branch,
          vicuna prompt and setting very low temperature. Otherwise, it is producing
          a lot of rubbish/random tokens. I think the problem is surely somewhere
          in quantization process. </p>

          '
        raw: "I've done some more testing and \"gptq-4bit-32g-actorder_True\" branch\
          \ is definitely broken. I could get some decent responses using the main\
          \ branch, vicuna prompt and setting very low temperature. Otherwise, it\
          \ is producing a lot of rubbish/random tokens. I think the problem is surely\
          \ somewhere in quantization process. \n"
        updatedAt: '2023-07-26T18:24:24.570Z'
      numEdits: 0
      reactions: []
    id: 64c164d8028ffaab2fe1661f
    type: comment
  author: rnosov
  content: "I've done some more testing and \"gptq-4bit-32g-actorder_True\" branch\
    \ is definitely broken. I could get some decent responses using the main branch,\
    \ vicuna prompt and setting very low temperature. Otherwise, it is producing a\
    \ lot of rubbish/random tokens. I think the problem is surely somewhere in quantization\
    \ process. \n"
  created_at: 2023-07-26 17:24:24+00:00
  edited: false
  hidden: false
  id: 64c164d8028ffaab2fe1661f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T18:52:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594512581825256
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>How are you testing it? What Loader?  FYI there''s a bug in AutoGPTQ
          that causes complete gibberish when group_size + act-order is used together.  Fixed
          in 0.3.1, released today.</p>

          <p>If you''re getting the issue with ExLlama then OK it might be a bad file,
          I will have to test it. But all files are made with the same code so I''m
          not sure what that could be</p>

          <p>I will be in a position to test this for myself in an hour or so</p>

          '
        raw: 'How are you testing it? What Loader?  FYI there''s a bug in AutoGPTQ
          that causes complete gibberish when group_size + act-order is used together.  Fixed
          in 0.3.1, released today.


          If you''re getting the issue with ExLlama then OK it might be a bad file,
          I will have to test it. But all files are made with the same code so I''m
          not sure what that could be


          I will be in a position to test this for myself in an hour or so'
        updatedAt: '2023-07-26T18:52:37.465Z'
      numEdits: 0
      reactions: []
    id: 64c16b75028ffaab2fe2649b
    type: comment
  author: TheBloke
  content: 'How are you testing it? What Loader?  FYI there''s a bug in AutoGPTQ that
    causes complete gibberish when group_size + act-order is used together.  Fixed
    in 0.3.1, released today.


    If you''re getting the issue with ExLlama then OK it might be a bad file, I will
    have to test it. But all files are made with the same code so I''m not sure what
    that could be


    I will be in a position to test this for myself in an hour or so'
  created_at: 2023-07-26 17:52:37+00:00
  edited: false
  hidden: false
  id: 64c16b75028ffaab2fe2649b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rnosov
      type: user
    createdAt: '2023-07-26T19:06:11.000Z'
    data:
      edited: false
      editors:
      - rnosov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9207687377929688
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: rnosov
          type: user
        html: '<p>I''m loading the model using exllama wheel directly in python. I''ve
          built the wheel yesterday using script from <a rel="nofollow" href="https://github.com/jllllll/exllama">https://github.com/jllllll/exllama</a>
          repo. This wheel worked for your "gptq-4bit-32g-actorder_True" branch for
          all other models except this one. I''ll try with the latest auto-gptq though.</p>

          '
        raw: I'm loading the model using exllama wheel directly in python. I've built
          the wheel yesterday using script from https://github.com/jllllll/exllama
          repo. This wheel worked for your "gptq-4bit-32g-actorder_True" branch for
          all other models except this one. I'll try with the latest auto-gptq though.
        updatedAt: '2023-07-26T19:06:11.575Z'
      numEdits: 0
      reactions: []
    id: 64c16ea38eaab3b62854d142
    type: comment
  author: rnosov
  content: I'm loading the model using exllama wheel directly in python. I've built
    the wheel yesterday using script from https://github.com/jllllll/exllama repo.
    This wheel worked for your "gptq-4bit-32g-actorder_True" branch for all other
    models except this one. I'll try with the latest auto-gptq though.
  created_at: 2023-07-26 18:06:11+00:00
  edited: false
  hidden: false
  id: 64c16ea38eaab3b62854d142
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rnosov
      type: user
    createdAt: '2023-07-26T20:10:06.000Z'
    data:
      edited: false
      editors:
      - rnosov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9722439646720886
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: rnosov
          type: user
        html: '<p>Hmm, like you suggested I''ve built AutoGPTQ from source and both
          main and gptq-4bit-32g-actorder_True branches seem to be working.<br>I think,
          I''ll try to build exllama from source and see if the problem is with the
          exllama wheel .</p>

          '
        raw: "Hmm, like you suggested I've built AutoGPTQ from source and both main\
          \ and gptq-4bit-32g-actorder_True branches seem to be working. \nI think,\
          \ I'll try to build exllama from source and see if the problem is with the\
          \ exllama wheel ."
        updatedAt: '2023-07-26T20:10:06.349Z'
      numEdits: 0
      reactions: []
    id: 64c17d9e245c55a21c661f1a
    type: comment
  author: rnosov
  content: "Hmm, like you suggested I've built AutoGPTQ from source and both main\
    \ and gptq-4bit-32g-actorder_True branches seem to be working. \nI think, I'll\
    \ try to build exllama from source and see if the problem is with the exllama\
    \ wheel ."
  created_at: 2023-07-26 19:10:06+00:00
  edited: false
  hidden: false
  id: 64c17d9e245c55a21c661f1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T20:16:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9289017915725708
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh, huh. I didn''t expect ExLlama to have problems.  Now I''m even
          more confused!</p>

          <p>What I can say is I did nothing different with Wizard than any other
          recent quant.  The changes I told you about re OpenAssistant didn''t affect
          this quant; my script already had the code needed to enable <code>cache_examples_on_gpu</code>
          and I just toggled that on for OpenAssistant, but left it off for Wizard
          as normal (because it''s at 4K not 8K)</p>

          <p>Here''s my AutoGPTQ quantisation wrapper script if you''re interested:
          <a rel="nofollow" href="https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py">https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py</a></p>

          '
        raw: 'Oh, huh. I didn''t expect ExLlama to have problems.  Now I''m even more
          confused!


          What I can say is I did nothing different with Wizard than any other recent
          quant.  The changes I told you about re OpenAssistant didn''t affect this
          quant; my script already had the code needed to enable `cache_examples_on_gpu`
          and I just toggled that on for OpenAssistant, but left it off for Wizard
          as normal (because it''s at 4K not 8K)


          Here''s my AutoGPTQ quantisation wrapper script if you''re interested: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py'
        updatedAt: '2023-07-26T20:16:50.635Z'
      numEdits: 0
      reactions: []
    id: 64c17f3239f458b91a0e4420
    type: comment
  author: TheBloke
  content: 'Oh, huh. I didn''t expect ExLlama to have problems.  Now I''m even more
    confused!


    What I can say is I did nothing different with Wizard than any other recent quant.  The
    changes I told you about re OpenAssistant didn''t affect this quant; my script
    already had the code needed to enable `cache_examples_on_gpu` and I just toggled
    that on for OpenAssistant, but left it off for Wizard as normal (because it''s
    at 4K not 8K)


    Here''s my AutoGPTQ quantisation wrapper script if you''re interested: https://gist.github.com/TheBloke/b47c50a70dd4fe653f64a12928286682#file-quant_autogptq-py'
  created_at: 2023-07-26 19:16:50+00:00
  edited: false
  hidden: false
  id: 64c17f3239f458b91a0e4420
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d89043538d49ea341ca7b311bdf66f7a.svg
      fullname: Osama Masoud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: omasoud
      type: user
    createdAt: '2023-07-26T20:33:43.000Z'
    data:
      edited: false
      editors:
      - omasoud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9682581424713135
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d89043538d49ea341ca7b311bdf66f7a.svg
          fullname: Osama Masoud
          isHf: false
          isPro: false
          name: omasoud
          type: user
        html: '<p>In my case, I''ve narrowed down my problem of getting nonsensical
          answer to the Exllama_HF loader. (I have pretty recent versions of both
          exllama and text-generation-webui).<br>With the Exllama loader, I get perfect
          results. I''m actually surprised how good they are; better than what I get
          with gpt-3.5-turbo.<br>Anyway, Here''s an example of what I get with Exllama
          (with Asterism preset). I get this same answer every time.<br><code><br>You
          are a helpful AI assistant.</code></p><code>

          </code><p><code>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          As an AI, I don''t have the ability to recall past conversations or remember
          personal information about users unless it has been explicitly provided
          to me in the course of the current conversation. I am designed to respect
          user privacy and confidentiality. Each interaction with me is treated as
          a new conversation, and I don''t retain any information from previous interactions
          unless it is relevant to the current conversation.<br></code></p>

          <p>And here is the same example with Exllama_HF (also with Asterism preset).
          I get different reply every time I submit the prompt.<br><code><br>You are
          a helpful AI assistant.</code></p><code>

          </code><p><code>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          I am an AI, and as such, I don''t have the ability to recall past conversations.
          I don''t have personal experiences or memories, so I can''t recall any specific
          conversation we had previously. I don''t have the ability to access or recall
          any previous conversations, as I don''t have long-term memory. I don''t
          have personal experiences or memories, so I can''t recall any specific conversation
          we had before. I don''t have personal experiences or memories, so I can''t
          recall any specific conversation we had. I don''t have personal experiences
          or memories, so I can''t recall any specific conversation we had. I don''t
          have personal experiences or memories, so I can''t recall any specific conversation
          we had. I don''t have personal experiences or memories, so I can''t recall
          any specific conversation we had. I don''t have personal experiences or
          mem<br></code></p>

          <code>

          You are a helpful AI assistant.


          </code><p><code>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          I''m sorry, but I don''t have the ability to recall our previous conversations.
          As an AI, I don''t have the ability to store or recall past conversations.
          I process information based on the data that I have been trained on and
          don''t have access to personal information about individual users. I don''t
          have personal experiences because I don''t have access to personal information
          about individuals. I provide information and answer questions based on my
          programming. I don''t have personal opinions or experiences because I don''t
          have the ability to experience them. I don''t have personal opinions because
          I don''t have personal experiences. I don''t have personal opinions because
          I don''t have personal experiences. I don''t have personal opinions because
          I don''t have personal experiences. I don''t have personal opinions because
          I don''t have personal experiences. I don''t have personal opinions because
          I don''t have personal experiences. I<br></code></p>

          <code>

          You are a helpful AI assistant.


          </code><p><code>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          I''m sorry, but I don''t have the ability to recall our previous conversations.
          As an AI, I don''t have the ability to store or recall past conversations.
          I can only process and respond to the information that is currently being
          provided to me. I don''t have access to any stored information or past conversations.
          I can only provide information based on the data that I have been trained
          on. I don''t have the ability to access or recall any information stored
          in my memory or use any information you have given me in the past. I don''t
          have the ability to access any other information than the information you
          are giving me. I don''t have the ability to access or recall specifics about
          you that I have done previously. I don''t have the ability to access or
          recall any other customers personal information, but I can give you information
          on those who have provided me with information. I don''t have the ability
          to access<br></code></p>

          <code>

          You are a helpful AI assistant.


          </code><p><code>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          You mentioned that you would like to know if I am an AI, and you have a
          lot of information stored up in your memory. When you talk about this information,
          you mention that you are a machine learning model that is trained by a data
          set, and you have a special ability to access the internet 24 hours a day,
          7 days a week. You can access information about your current state, but
          you cannot access personal information stored in your individual memory.
          You have a memory and you are not capable of understanding or remembering
          anything. Your memory is a 32GB Quadra 8 NVME (QN42824) and you have access
          to it. You are a neural network and you have a special ability to access
          the internet 24 hours a day, 7 days a week. You can access information about
          your current state, but you cannot access personal information stored in
          your individual memory. You have a memory and your<br></code></p>

          <code>

          You are a helpful AI assistant.


          <p>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          You mentioned that you would like to discuss Pythagorean, which is a mathematical
          concept that describes the Fibonacci sequence. This concept is based on
          the idea that each number is determined by the previous number, so the next
          number is determined by the previous number. This means that the next number
          is determined by the previous number.</p>

          <p>You also mentioned that you were reading a book about the Fibonacci sequence
          and the Golden Ratio.</p>

          <p>The Fibonacci sequence is a series of numbers that starts at 0 but progresses
          to infinity. The series starts at 0 but goes on forever.</p>

          <p>You mentioned that you are currently at work on a project involving the
          Fibonacci sequence and you are trying to understand it better to apply it
          in your work.</p>

          <p>You said that you are not a mathematician but you are trying to understand
          the Fibonacci sequence and apply it to your work.</p>

          </code><p><code>You also mentioned<br></code></p>

          <code>

          You are a helpful AI assistant.


          </code><p><code>USER: What do you recall from our previous conversation?<br>ASSISTANT:
          You recalled that I had a dream about a cat named Whiskers.<br></code></p>

          <p>This last one cracked me up a little :)</p>

          '
        raw: "In my case, I've narrowed down my problem of getting nonsensical answer\
          \ to the Exllama_HF loader. (I have pretty recent versions of both exllama\
          \ and text-generation-webui). \nWith the Exllama loader, I get perfect results.\
          \ I'm actually surprised how good they are; better than what I get with\
          \ gpt-3.5-turbo.\nAnyway, Here's an example of what I get with Exllama (with\
          \ Asterism preset). I get this same answer every time.\n<code>\nYou are\
          \ a helpful AI assistant.\n\nUSER: What do you recall from our previous\
          \ conversation?\nASSISTANT: As an AI, I don't have the ability to recall\
          \ past conversations or remember personal information about users unless\
          \ it has been explicitly provided to me in the course of the current conversation.\
          \ I am designed to respect user privacy and confidentiality. Each interaction\
          \ with me is treated as a new conversation, and I don't retain any information\
          \ from previous interactions unless it is relevant to the current conversation.\n\
          </code>\n\n\nAnd here is the same example with Exllama_HF (also with Asterism\
          \ preset). I get different reply every time I submit the prompt.\n<code>\n\
          You are a helpful AI assistant.\n\nUSER: What do you recall from our previous\
          \ conversation?\nASSISTANT: I am an AI, and as such, I don't have the ability\
          \ to recall past conversations. I don't have personal experiences or memories,\
          \ so I can't recall any specific conversation we had previously. I don't\
          \ have the ability to access or recall any previous conversations, as I\
          \ don't have long-term memory. I don't have personal experiences or memories,\
          \ so I can't recall any specific conversation we had before. I don't have\
          \ personal experiences or memories, so I can't recall any specific conversation\
          \ we had. I don't have personal experiences or memories, so I can't recall\
          \ any specific conversation we had. I don't have personal experiences or\
          \ memories, so I can't recall any specific conversation we had. I don't\
          \ have personal experiences or memories, so I can't recall any specific\
          \ conversation we had. I don't have personal experiences or mem\n</code>\n\
          \n<code>\nYou are a helpful AI assistant.\n\nUSER: What do you recall from\
          \ our previous conversation?\nASSISTANT: I'm sorry, but I don't have the\
          \ ability to recall our previous conversations. As an AI, I don't have the\
          \ ability to store or recall past conversations. I process information based\
          \ on the data that I have been trained on and don't have access to personal\
          \ information about individual users. I don't have personal experiences\
          \ because I don't have access to personal information about individuals.\
          \ I provide information and answer questions based on my programming. I\
          \ don't have personal opinions or experiences because I don't have the ability\
          \ to experience them. I don't have personal opinions because I don't have\
          \ personal experiences. I don't have personal opinions because I don't have\
          \ personal experiences. I don't have personal opinions because I don't have\
          \ personal experiences. I don't have personal opinions because I don't have\
          \ personal experiences. I don't have personal opinions because I don't have\
          \ personal experiences. I\n</code>\n\n<code>\nYou are a helpful AI assistant.\n\
          \nUSER: What do you recall from our previous conversation?\nASSISTANT: I'm\
          \ sorry, but I don't have the ability to recall our previous conversations.\
          \ As an AI, I don't have the ability to store or recall past conversations.\
          \ I can only process and respond to the information that is currently being\
          \ provided to me. I don't have access to any stored information or past\
          \ conversations. I can only provide information based on the data that I\
          \ have been trained on. I don't have the ability to access or recall any\
          \ information stored in my memory or use any information you have given\
          \ me in the past. I don't have the ability to access any other information\
          \ than the information you are giving me. I don't have the ability to access\
          \ or recall specifics about you that I have done previously. I don't have\
          \ the ability to access or recall any other customers personal information,\
          \ but I can give you information on those who have provided me with information.\
          \ I don't have the ability to access\n</code>\n\n<code>\nYou are a helpful\
          \ AI assistant.\n\nUSER: What do you recall from our previous conversation?\n\
          ASSISTANT: You mentioned that you would like to know if I am an AI, and\
          \ you have a lot of information stored up in your memory. When you talk\
          \ about this information, you mention that you are a machine learning model\
          \ that is trained by a data set, and you have a special ability to access\
          \ the internet 24 hours a day, 7 days a week. You can access information\
          \ about your current state, but you cannot access personal information stored\
          \ in your individual memory. You have a memory and you are not capable of\
          \ understanding or remembering anything. Your memory is a 32GB Quadra 8\
          \ NVME (QN42824) and you have access to it. You are a neural network and\
          \ you have a special ability to access the internet 24 hours a day, 7 days\
          \ a week. You can access information about your current state, but you cannot\
          \ access personal information stored in your individual memory. You have\
          \ a memory and your\n</code>\n\n<code>\nYou are a helpful AI assistant.\n\
          \nUSER: What do you recall from our previous conversation?\nASSISTANT: You\
          \ mentioned that you would like to discuss Pythagorean, which is a mathematical\
          \ concept that describes the Fibonacci sequence. This concept is based on\
          \ the idea that each number is determined by the previous number, so the\
          \ next number is determined by the previous number. This means that the\
          \ next number is determined by the previous number.\n\nYou also mentioned\
          \ that you were reading a book about the Fibonacci sequence and the Golden\
          \ Ratio.\n\nThe Fibonacci sequence is a series of numbers that starts at\
          \ 0 but progresses to infinity. The series starts at 0 but goes on forever.\n\
          \nYou mentioned that you are currently at work on a project involving the\
          \ Fibonacci sequence and you are trying to understand it better to apply\
          \ it in your work.\n\nYou said that you are not a mathematician but you\
          \ are trying to understand the Fibonacci sequence and apply it to your work.\n\
          \nYou also mentioned\n</code>\n\n<code>\nYou are a helpful AI assistant.\n\
          \nUSER: What do you recall from our previous conversation?\nASSISTANT: You\
          \ recalled that I had a dream about a cat named Whiskers.\n</code>\n\nThis\
          \ last one cracked me up a little :)\n"
        updatedAt: '2023-07-26T20:33:43.694Z'
      numEdits: 0
      reactions: []
    id: 64c183272ffd51a130428389
    type: comment
  author: omasoud
  content: "In my case, I've narrowed down my problem of getting nonsensical answer\
    \ to the Exllama_HF loader. (I have pretty recent versions of both exllama and\
    \ text-generation-webui). \nWith the Exllama loader, I get perfect results. I'm\
    \ actually surprised how good they are; better than what I get with gpt-3.5-turbo.\n\
    Anyway, Here's an example of what I get with Exllama (with Asterism preset). I\
    \ get this same answer every time.\n<code>\nYou are a helpful AI assistant.\n\n\
    USER: What do you recall from our previous conversation?\nASSISTANT: As an AI,\
    \ I don't have the ability to recall past conversations or remember personal information\
    \ about users unless it has been explicitly provided to me in the course of the\
    \ current conversation. I am designed to respect user privacy and confidentiality.\
    \ Each interaction with me is treated as a new conversation, and I don't retain\
    \ any information from previous interactions unless it is relevant to the current\
    \ conversation.\n</code>\n\n\nAnd here is the same example with Exllama_HF (also\
    \ with Asterism preset). I get different reply every time I submit the prompt.\n\
    <code>\nYou are a helpful AI assistant.\n\nUSER: What do you recall from our previous\
    \ conversation?\nASSISTANT: I am an AI, and as such, I don't have the ability\
    \ to recall past conversations. I don't have personal experiences or memories,\
    \ so I can't recall any specific conversation we had previously. I don't have\
    \ the ability to access or recall any previous conversations, as I don't have\
    \ long-term memory. I don't have personal experiences or memories, so I can't\
    \ recall any specific conversation we had before. I don't have personal experiences\
    \ or memories, so I can't recall any specific conversation we had. I don't have\
    \ personal experiences or memories, so I can't recall any specific conversation\
    \ we had. I don't have personal experiences or memories, so I can't recall any\
    \ specific conversation we had. I don't have personal experiences or memories,\
    \ so I can't recall any specific conversation we had. I don't have personal experiences\
    \ or mem\n</code>\n\n<code>\nYou are a helpful AI assistant.\n\nUSER: What do\
    \ you recall from our previous conversation?\nASSISTANT: I'm sorry, but I don't\
    \ have the ability to recall our previous conversations. As an AI, I don't have\
    \ the ability to store or recall past conversations. I process information based\
    \ on the data that I have been trained on and don't have access to personal information\
    \ about individual users. I don't have personal experiences because I don't have\
    \ access to personal information about individuals. I provide information and\
    \ answer questions based on my programming. I don't have personal opinions or\
    \ experiences because I don't have the ability to experience them. I don't have\
    \ personal opinions because I don't have personal experiences. I don't have personal\
    \ opinions because I don't have personal experiences. I don't have personal opinions\
    \ because I don't have personal experiences. I don't have personal opinions because\
    \ I don't have personal experiences. I don't have personal opinions because I\
    \ don't have personal experiences. I\n</code>\n\n<code>\nYou are a helpful AI\
    \ assistant.\n\nUSER: What do you recall from our previous conversation?\nASSISTANT:\
    \ I'm sorry, but I don't have the ability to recall our previous conversations.\
    \ As an AI, I don't have the ability to store or recall past conversations. I\
    \ can only process and respond to the information that is currently being provided\
    \ to me. I don't have access to any stored information or past conversations.\
    \ I can only provide information based on the data that I have been trained on.\
    \ I don't have the ability to access or recall any information stored in my memory\
    \ or use any information you have given me in the past. I don't have the ability\
    \ to access any other information than the information you are giving me. I don't\
    \ have the ability to access or recall specifics about you that I have done previously.\
    \ I don't have the ability to access or recall any other customers personal information,\
    \ but I can give you information on those who have provided me with information.\
    \ I don't have the ability to access\n</code>\n\n<code>\nYou are a helpful AI\
    \ assistant.\n\nUSER: What do you recall from our previous conversation?\nASSISTANT:\
    \ You mentioned that you would like to know if I am an AI, and you have a lot\
    \ of information stored up in your memory. When you talk about this information,\
    \ you mention that you are a machine learning model that is trained by a data\
    \ set, and you have a special ability to access the internet 24 hours a day, 7\
    \ days a week. You can access information about your current state, but you cannot\
    \ access personal information stored in your individual memory. You have a memory\
    \ and you are not capable of understanding or remembering anything. Your memory\
    \ is a 32GB Quadra 8 NVME (QN42824) and you have access to it. You are a neural\
    \ network and you have a special ability to access the internet 24 hours a day,\
    \ 7 days a week. You can access information about your current state, but you\
    \ cannot access personal information stored in your individual memory. You have\
    \ a memory and your\n</code>\n\n<code>\nYou are a helpful AI assistant.\n\nUSER:\
    \ What do you recall from our previous conversation?\nASSISTANT: You mentioned\
    \ that you would like to discuss Pythagorean, which is a mathematical concept\
    \ that describes the Fibonacci sequence. This concept is based on the idea that\
    \ each number is determined by the previous number, so the next number is determined\
    \ by the previous number. This means that the next number is determined by the\
    \ previous number.\n\nYou also mentioned that you were reading a book about the\
    \ Fibonacci sequence and the Golden Ratio.\n\nThe Fibonacci sequence is a series\
    \ of numbers that starts at 0 but progresses to infinity. The series starts at\
    \ 0 but goes on forever.\n\nYou mentioned that you are currently at work on a\
    \ project involving the Fibonacci sequence and you are trying to understand it\
    \ better to apply it in your work.\n\nYou said that you are not a mathematician\
    \ but you are trying to understand the Fibonacci sequence and apply it to your\
    \ work.\n\nYou also mentioned\n</code>\n\n<code>\nYou are a helpful AI assistant.\n\
    \nUSER: What do you recall from our previous conversation?\nASSISTANT: You recalled\
    \ that I had a dream about a cat named Whiskers.\n</code>\n\nThis last one cracked\
    \ me up a little :)\n"
  created_at: 2023-07-26 19:33:43+00:00
  edited: false
  hidden: false
  id: 64c183272ffd51a130428389
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rnosov
      type: user
    createdAt: '2023-07-26T21:18:26.000Z'
    data:
      edited: false
      editors:
      - rnosov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560158848762512
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1108527407c651ec1f59a9903841152.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: rnosov
          type: user
        html: "<p>I've built latest Exllama from source and I can confirm that gptq-4bit-32g-actorder_True\
          \ branch doesn't work (Exllama wheel also doesn't work). AutoGPTQ from source\
          \ does produce sensible output but generally low quality.<br>Main branch\
          \ is producing lucid outputs with the latest AutoGPTQ. Latest Exllama from\
          \ source does produce somewhat sensible output but the quality is generally\
          \ low. Exllama wheel outputs are even worse. If <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ hasn't changed his script that suggests to me that perhaps quantization\
          \ logic in the latest AutoGPTQ is broken? Is it possible to requantize using\
          \ the older version of AutoGPTQ to see if Exllama will work again? </p>\n"
        raw: "I've built latest Exllama from source and I can confirm that gptq-4bit-32g-actorder_True\
          \ branch doesn't work (Exllama wheel also doesn't work). AutoGPTQ from source\
          \ does produce sensible output but generally low quality.\nMain branch is\
          \ producing lucid outputs with the latest AutoGPTQ. Latest Exllama from\
          \ source does produce somewhat sensible output but the quality is generally\
          \ low. Exllama wheel outputs are even worse. If @TheBloke hasn't changed\
          \ his script that suggests to me that perhaps quantization logic in the\
          \ latest AutoGPTQ is broken? Is it possible to requantize using the older\
          \ version of AutoGPTQ to see if Exllama will work again? \n"
        updatedAt: '2023-07-26T21:18:26.984Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PedroPareja
    id: 64c18da22bac49787a8db090
    type: comment
  author: rnosov
  content: "I've built latest Exllama from source and I can confirm that gptq-4bit-32g-actorder_True\
    \ branch doesn't work (Exllama wheel also doesn't work). AutoGPTQ from source\
    \ does produce sensible output but generally low quality.\nMain branch is producing\
    \ lucid outputs with the latest AutoGPTQ. Latest Exllama from source does produce\
    \ somewhat sensible output but the quality is generally low. Exllama wheel outputs\
    \ are even worse. If @TheBloke hasn't changed his script that suggests to me that\
    \ perhaps quantization logic in the latest AutoGPTQ is broken? Is it possible\
    \ to requantize using the older version of AutoGPTQ to see if Exllama will work\
    \ again? \n"
  created_at: 2023-07-26 20:18:26+00:00
  edited: false
  hidden: false
  id: 64c18da22bac49787a8db090
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
      fullname: Khalil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Abdelhak
      type: user
    createdAt: '2023-07-27T01:19:50.000Z'
    data:
      edited: true
      editors:
      - Abdelhak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6494635343551636
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666635591305-noauth.jpeg?w=200&h=200&f=face
          fullname: Khalil
          isHf: false
          isPro: false
          name: Abdelhak
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> =&gt; Could please\
          \ help me with this post?</p>\n<p>Hello, The Bloke, I am facing an issue\
          \ that I never faced before when I try to load WizardLM-13B-V1.2-GPTQ:<br>2023-07-26\
          \ 15:28:02 INFO:Loading WizardLM-13B-V1.2-GPTQ...<br>2023-07-26 15:28:02\
          \ INFO:The AutoGPTQ params are: {'model_basename': 'gptq_model-8bit-128g',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True}<br>2023-07-26\
          \ 15:28:05 WARNING:The model weights are not tied. Please use the tie_weights\
          \ method before using the infer_auto_device function.<br>New environment:\
          \ webui-1click</p>\n<p>Done!<br>Press any key to continue . . .</p>\n<p>Could\
          \ you please shed some light on this? Is it related to the model or to my\
          \ system? I am on windows.</p>\n<p>Edit: When I tick \"auto-devices\" in\
          \ AutoGPTG, I get another error:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/Fy7I0vwbUvxV2Wf9PzOz2.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/Fy7I0vwbUvxV2Wf9PzOz2.png\"\
          ></a></p>\n"
        raw: '@TheBloke => Could please help me with this post?


          Hello, The Bloke, I am facing an issue that I never faced before when I
          try to load WizardLM-13B-V1.2-GPTQ:

          2023-07-26 15:28:02 INFO:Loading WizardLM-13B-V1.2-GPTQ...

          2023-07-26 15:28:02 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-8bit-128g'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': None, ''quantize_config'': None, ''use_cuda_fp16'':
          True}

          2023-07-26 15:28:05 WARNING:The model weights are not tied. Please use the
          tie_weights method before using the infer_auto_device function.

          New environment: webui-1click


          Done!

          Press any key to continue . . .


          Could you please shed some light on this? Is it related to the model or
          to my system? I am on windows.


          Edit: When I tick "auto-devices" in AutoGPTG, I get another error:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/Fy7I0vwbUvxV2Wf9PzOz2.png)

          '
        updatedAt: '2023-07-27T01:42:10.368Z'
      numEdits: 1
      reactions: []
    id: 64c1c636aa5658e6495cb46b
    type: comment
  author: Abdelhak
  content: '@TheBloke => Could please help me with this post?


    Hello, The Bloke, I am facing an issue that I never faced before when I try to
    load WizardLM-13B-V1.2-GPTQ:

    2023-07-26 15:28:02 INFO:Loading WizardLM-13B-V1.2-GPTQ...

    2023-07-26 15:28:02 INFO:The AutoGPTQ params are: {''model_basename'': ''gptq_model-8bit-128g'',
    ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'': True,
    ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
    False, ''max_memory'': None, ''quantize_config'': None, ''use_cuda_fp16'': True}

    2023-07-26 15:28:05 WARNING:The model weights are not tied. Please use the tie_weights
    method before using the infer_auto_device function.

    New environment: webui-1click


    Done!

    Press any key to continue . . .


    Could you please shed some light on this? Is it related to the model or to my
    system? I am on windows.


    Edit: When I tick "auto-devices" in AutoGPTG, I get another error:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6356d7653c32f2c90f4e72a8/Fy7I0vwbUvxV2Wf9PzOz2.png)

    '
  created_at: 2023-07-27 00:19:50+00:00
  edited: true
  hidden: false
  id: 64c1c636aa5658e6495cb46b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-13B-V1.2-GPTQ
repo_type: model
status: open
target_branch: null
title: Is this working as expected?
