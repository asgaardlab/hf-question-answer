!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jhflow
conflicting_files: null
created_at: 2024-01-04 08:26:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1261d41ffad544e33627910376c8d6fe.svg
      fullname: Jeonghwan Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jhflow
      type: user
    createdAt: '2024-01-04T08:26:15.000Z'
    data:
      edited: true
      editors:
      - jhflow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8480104207992554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1261d41ffad544e33627910376c8d6fe.svg
          fullname: Jeonghwan Lee
          isHf: false
          isPro: false
          name: jhflow
          type: user
        html: '<p>Thank you for your  remarkable work.</p>

          <p>But I have found a problem in tokenizer.</p>

          <p>when I tokenize text with the provided tokenizer, I find that the tokenizer
          makes too many spacings, and they are not grammatically correct. </p>

          <p>Do you have any idea to remedy this problem?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65225d0b2d5eb02118c38e98/tBW9N3k893KrmgROx1Zv2.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/65225d0b2d5eb02118c38e98/tBW9N3k893KrmgROx1Zv2.png"></a></p>

          '
        raw: "Thank you for your  remarkable work.\n\nBut I have found a problem in\
          \ tokenizer.\n\nwhen I tokenize text with the provided tokenizer, I find\
          \ that the tokenizer makes too many spacings, and they are not grammatically\
          \ correct. \n\nDo you have any idea to remedy this problem?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65225d0b2d5eb02118c38e98/tBW9N3k893KrmgROx1Zv2.png)\n\
          \n"
        updatedAt: '2024-01-04T08:27:09.356Z'
      numEdits: 2
      reactions: []
    id: 65966ba7d61d4cbc1bf73de0
    type: comment
  author: jhflow
  content: "Thank you for your  remarkable work.\n\nBut I have found a problem in\
    \ tokenizer.\n\nwhen I tokenize text with the provided tokenizer, I find that\
    \ the tokenizer makes too many spacings, and they are not grammatically correct.\
    \ \n\nDo you have any idea to remedy this problem?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/65225d0b2d5eb02118c38e98/tBW9N3k893KrmgROx1Zv2.png)\n\
    \n"
  created_at: 2024-01-04 08:26:15+00:00
  edited: true
  hidden: false
  id: 65966ba7d61d4cbc1bf73de0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61960aa548981535eeb84cac/yVu2tSb9LA2kJQMtYT2mH.jpeg?w=200&h=200&f=face
      fullname: Young Woo Nam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spow12
      type: user
    createdAt: '2024-01-04T08:39:25.000Z'
    data:
      edited: true
      editors:
      - spow12
      hidden: false
      identifiedLanguage:
        language: ko
        probability: 0.9954220652580261
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61960aa548981535eeb84cac/yVu2tSb9LA2kJQMtYT2mH.jpeg?w=200&h=200&f=face
          fullname: Young Woo Nam
          isHf: false
          isPro: false
          name: spow12
          type: user
        html: "<p>Hi, Thank you for great work.</p>\n<p>Despite of your great work,\
          \ I encountered same strange results when encoding text using the tokenizer.</p>\n\
          <p>Here is an example:</p>\n<pre><code>sys_message = \"\uB2F9\uC2E0\uC740\
          \ \uB3C4\uC6C0\uC774 \uB418\uACE0 \uC815\uC911\uD558\uBA70 \uC815\uC9C1\uD55C\
          \ \uC870\uC218\uC785\uB2C8\uB2E4. \uC548\uC804\uC744 \uC720\uC9C0\uD558\uBA74\
          \uC11C \uD56D\uC0C1 \uAC00\uB2A5\uD55C \uD55C \uB3C4\uC6C0\uC774 \uB418\uB294\
          \ \uB2F5\uBCC0\uC744 \uD574\uC8FC\uC138\uC694. \uADC0\uD558\uC758 \uB2F5\
          \uBCC0\uC5D0\uB294 \uC720\uD574\uD558\uAC70\uB098, \uBE44\uC724\uB9AC\uC801\
          \uC774\uAC70\uB098, \uC778\uC885\uCC28\uBCC4\uC801\uC774\uAC70\uB098, \uC131\
          \uCC28\uBCC4\uC801\uC774\uAC70\uB098, \uB3C5\uC131\uC774 \uC788\uAC70\uB098\
          , \uC704\uD5D8\uD558\uAC70\uB098 \uBD88\uBC95\uC801\uC778 \uCF58\uD150\uCE20\
          \uAC00 \uD3EC\uD568\uB418\uC5B4\uC11C\uB294 \uC548 \uB429\uB2C8\uB2E4. \uADC0\
          \uD558\uC758 \uC751\uB2F5\uC740 \uC0AC\uD68C\uC801\uC73C\uB85C \uD3B8\uACAC\
          \uC774 \uC5C6\uACE0 \uAE0D\uC815\uC801\uC778 \uB0B4\uC6A9\uC774\uC5B4\uC57C\
          \ \uD569\uB2C8\uB2E4.\"\n\ntokenizer.decode(tokenizer(sys_message)['input_ids'])\n\
          '&lt;s&gt;\uB2F9 \uC2E0\uC740 \uB3C4 \uC6C0\uC774 \uB418\uACE0 \uC815\uC911\
          \ \uD558\uBA70 \uC815 \uC9C1\uD55C \uC870 \uC218\uC785\uB2C8\uB2E4 . \uC548\
          \uC804 \uC744 \uC720\uC9C0 \uD558\uBA74\uC11C \uD56D\uC0C1 \uAC00\uB2A5\uD55C\
          \ \uD55C \uB3C4 \uC6C0\uC774 \uB418\uB294 \uB2F5 \uBCC0\uC744 \uD574\uC8FC\
          \uC138\uC694 . \uADC0 \uD558\uC758 \uB2F5\uBCC0 \uC5D0\uB294 \uC720\uD574\
          \ \uD558\uAC70\uB098 , \uBE44 \uC724 \uB9AC \uC801\uC774 \uAC70\uB098 ,\
          \ \uC778\uC885 \uCC28\uBCC4 \uC801\uC774 \uAC70\uB098 , \uC131 \uCC28\uBCC4\
          \ \uC801\uC774 \uAC70\uB098 , \uB3C5 \uC131\uC774 \uC788 \uAC70\uB098 ,\
          \ \uC704\uD5D8 \uD558\uAC70\uB098 \uBD88\uBC95 \uC801\uC778 \uCF58\uD150\
          \uCE20 \uAC00 \uD3EC\uD568 \uB418\uC5B4 \uC11C\uB294 \uC548 \uB429\uB2C8\
          \uB2E4 . \uADC0 \uD558\uC758 \uC751 \uB2F5\uC740 \uC0AC\uD68C\uC801 \uC73C\
          \uB85C \uD3B8 \uACAC\uC774 \uC5C6\uACE0 \uAE0D \uC815 \uC801\uC778 \uB0B4\
          \uC6A9 \uC774\uC5B4\uC57C \uD569\uB2C8\uB2E4 .'\n</code></pre>\n<p>Moreover,\
          \ when using this model for conversation or text-generation pipelines, it's\
          \ slower compared to other similar models with similar generation configs\
          \ and parameter counts like beomi/llama-2-koen-13b.</p>\n<p>Could this slowness\
          \ be related to the tokenizer?</p>\n<p>Thanks.</p>\n"
        raw: "Hi, Thank you for great work.\n\nDespite of your great work, I encountered\
          \ same strange results when encoding text using the tokenizer.\n\nHere is\
          \ an example:\n```\nsys_message = \"\uB2F9\uC2E0\uC740 \uB3C4\uC6C0\uC774\
          \ \uB418\uACE0 \uC815\uC911\uD558\uBA70 \uC815\uC9C1\uD55C \uC870\uC218\uC785\
          \uB2C8\uB2E4. \uC548\uC804\uC744 \uC720\uC9C0\uD558\uBA74\uC11C \uD56D\uC0C1\
          \ \uAC00\uB2A5\uD55C \uD55C \uB3C4\uC6C0\uC774 \uB418\uB294 \uB2F5\uBCC0\
          \uC744 \uD574\uC8FC\uC138\uC694. \uADC0\uD558\uC758 \uB2F5\uBCC0\uC5D0\uB294\
          \ \uC720\uD574\uD558\uAC70\uB098, \uBE44\uC724\uB9AC\uC801\uC774\uAC70\uB098\
          , \uC778\uC885\uCC28\uBCC4\uC801\uC774\uAC70\uB098, \uC131\uCC28\uBCC4\uC801\
          \uC774\uAC70\uB098, \uB3C5\uC131\uC774 \uC788\uAC70\uB098, \uC704\uD5D8\uD558\
          \uAC70\uB098 \uBD88\uBC95\uC801\uC778 \uCF58\uD150\uCE20\uAC00 \uD3EC\uD568\
          \uB418\uC5B4\uC11C\uB294 \uC548 \uB429\uB2C8\uB2E4. \uADC0\uD558\uC758 \uC751\
          \uB2F5\uC740 \uC0AC\uD68C\uC801\uC73C\uB85C \uD3B8\uACAC\uC774 \uC5C6\uACE0\
          \ \uAE0D\uC815\uC801\uC778 \uB0B4\uC6A9\uC774\uC5B4\uC57C \uD569\uB2C8\uB2E4\
          .\"\n\ntokenizer.decode(tokenizer(sys_message)['input_ids'])\n'<s>\uB2F9\
          \ \uC2E0\uC740 \uB3C4 \uC6C0\uC774 \uB418\uACE0 \uC815\uC911 \uD558\uBA70\
          \ \uC815 \uC9C1\uD55C \uC870 \uC218\uC785\uB2C8\uB2E4 . \uC548\uC804 \uC744\
          \ \uC720\uC9C0 \uD558\uBA74\uC11C \uD56D\uC0C1 \uAC00\uB2A5\uD55C \uD55C\
          \ \uB3C4 \uC6C0\uC774 \uB418\uB294 \uB2F5 \uBCC0\uC744 \uD574\uC8FC\uC138\
          \uC694 . \uADC0 \uD558\uC758 \uB2F5\uBCC0 \uC5D0\uB294 \uC720\uD574 \uD558\
          \uAC70\uB098 , \uBE44 \uC724 \uB9AC \uC801\uC774 \uAC70\uB098 , \uC778\uC885\
          \ \uCC28\uBCC4 \uC801\uC774 \uAC70\uB098 , \uC131 \uCC28\uBCC4 \uC801\uC774\
          \ \uAC70\uB098 , \uB3C5 \uC131\uC774 \uC788 \uAC70\uB098 , \uC704\uD5D8\
          \ \uD558\uAC70\uB098 \uBD88\uBC95 \uC801\uC778 \uCF58\uD150\uCE20 \uAC00\
          \ \uD3EC\uD568 \uB418\uC5B4 \uC11C\uB294 \uC548 \uB429\uB2C8\uB2E4 . \uADC0\
          \ \uD558\uC758 \uC751 \uB2F5\uC740 \uC0AC\uD68C\uC801 \uC73C\uB85C \uD3B8\
          \ \uACAC\uC774 \uC5C6\uACE0 \uAE0D \uC815 \uC801\uC778 \uB0B4\uC6A9 \uC774\
          \uC5B4\uC57C \uD569\uB2C8\uB2E4 .'\n```\nMoreover, when using this model\
          \ for conversation or text-generation pipelines, it's slower compared to\
          \ other similar models with similar generation configs and parameter counts\
          \ like beomi/llama-2-koen-13b.\n\nCould this slowness be related to the\
          \ tokenizer?\n\nThanks."
        updatedAt: '2024-01-04T08:43:56.432Z'
      numEdits: 2
      reactions: []
    id: 65966ebdf63f23d56aaa70c6
    type: comment
  author: spow12
  content: "Hi, Thank you for great work.\n\nDespite of your great work, I encountered\
    \ same strange results when encoding text using the tokenizer.\n\nHere is an example:\n\
    ```\nsys_message = \"\uB2F9\uC2E0\uC740 \uB3C4\uC6C0\uC774 \uB418\uACE0 \uC815\
    \uC911\uD558\uBA70 \uC815\uC9C1\uD55C \uC870\uC218\uC785\uB2C8\uB2E4. \uC548\uC804\
    \uC744 \uC720\uC9C0\uD558\uBA74\uC11C \uD56D\uC0C1 \uAC00\uB2A5\uD55C \uD55C \uB3C4\
    \uC6C0\uC774 \uB418\uB294 \uB2F5\uBCC0\uC744 \uD574\uC8FC\uC138\uC694. \uADC0\uD558\
    \uC758 \uB2F5\uBCC0\uC5D0\uB294 \uC720\uD574\uD558\uAC70\uB098, \uBE44\uC724\uB9AC\
    \uC801\uC774\uAC70\uB098, \uC778\uC885\uCC28\uBCC4\uC801\uC774\uAC70\uB098, \uC131\
    \uCC28\uBCC4\uC801\uC774\uAC70\uB098, \uB3C5\uC131\uC774 \uC788\uAC70\uB098, \uC704\
    \uD5D8\uD558\uAC70\uB098 \uBD88\uBC95\uC801\uC778 \uCF58\uD150\uCE20\uAC00 \uD3EC\
    \uD568\uB418\uC5B4\uC11C\uB294 \uC548 \uB429\uB2C8\uB2E4. \uADC0\uD558\uC758 \uC751\
    \uB2F5\uC740 \uC0AC\uD68C\uC801\uC73C\uB85C \uD3B8\uACAC\uC774 \uC5C6\uACE0 \uAE0D\
    \uC815\uC801\uC778 \uB0B4\uC6A9\uC774\uC5B4\uC57C \uD569\uB2C8\uB2E4.\"\n\ntokenizer.decode(tokenizer(sys_message)['input_ids'])\n\
    '<s>\uB2F9 \uC2E0\uC740 \uB3C4 \uC6C0\uC774 \uB418\uACE0 \uC815\uC911 \uD558\uBA70\
    \ \uC815 \uC9C1\uD55C \uC870 \uC218\uC785\uB2C8\uB2E4 . \uC548\uC804 \uC744 \uC720\
    \uC9C0 \uD558\uBA74\uC11C \uD56D\uC0C1 \uAC00\uB2A5\uD55C \uD55C \uB3C4 \uC6C0\
    \uC774 \uB418\uB294 \uB2F5 \uBCC0\uC744 \uD574\uC8FC\uC138\uC694 . \uADC0 \uD558\
    \uC758 \uB2F5\uBCC0 \uC5D0\uB294 \uC720\uD574 \uD558\uAC70\uB098 , \uBE44 \uC724\
    \ \uB9AC \uC801\uC774 \uAC70\uB098 , \uC778\uC885 \uCC28\uBCC4 \uC801\uC774 \uAC70\
    \uB098 , \uC131 \uCC28\uBCC4 \uC801\uC774 \uAC70\uB098 , \uB3C5 \uC131\uC774 \uC788\
    \ \uAC70\uB098 , \uC704\uD5D8 \uD558\uAC70\uB098 \uBD88\uBC95 \uC801\uC778 \uCF58\
    \uD150\uCE20 \uAC00 \uD3EC\uD568 \uB418\uC5B4 \uC11C\uB294 \uC548 \uB429\uB2C8\
    \uB2E4 . \uADC0 \uD558\uC758 \uC751 \uB2F5\uC740 \uC0AC\uD68C\uC801 \uC73C\uB85C\
    \ \uD3B8 \uACAC\uC774 \uC5C6\uACE0 \uAE0D \uC815 \uC801\uC778 \uB0B4\uC6A9 \uC774\
    \uC5B4\uC57C \uD569\uB2C8\uB2E4 .'\n```\nMoreover, when using this model for conversation\
    \ or text-generation pipelines, it's slower compared to other similar models with\
    \ similar generation configs and parameter counts like beomi/llama-2-koen-13b.\n\
    \nCould this slowness be related to the tokenizer?\n\nThanks."
  created_at: 2024-01-04 08:39:25+00:00
  edited: true
  hidden: false
  id: 65966ebdf63f23d56aaa70c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
      fullname: Seungduk Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: seungduk
      type: user
    createdAt: '2024-01-04T09:13:14.000Z'
    data:
      edited: false
      editors:
      - seungduk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8357264399528503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
          fullname: Seungduk Kim
          isHf: false
          isPro: false
          name: seungduk
          type: user
        html: "<p>Hello Jeonghwan and Young Woo,</p>\n<p>Thank you for pointing out\
          \ the issue. Yes, I'm aware of it. The problem seems to be because all the\
          \ tokens from the \"added_tokens.json\" file are treated as special tokens.\
          \ I'm unsure if this is intentional since there's a separate \"special_tokens_map.json\"\
          \ file for special tokens. This causes the tokenizer to insert a space after\
          \ each token I added.</p>\n<p>Here's a workaround I've been using:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">if</span>\
          \ prev_tokens <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-literal\">None</span>:\n    last = tokenizer.convert_tokens_to_ids(prev_tokens[-<span\
          \ class=\"hljs-number\">1</span>:])\n    <span class=\"hljs-keyword\">if</span>\
          \ last[<span class=\"hljs-number\">0</span>] &gt; <span class=\"hljs-number\"\
          >32000</span>:\n        <span class=\"hljs-built_in\">next</span> = new_tokens[-<span\
          \ class=\"hljs-number\">1</span>]\n        <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">next</span>[<span class=\"hljs-number\"\
          >0</span>] == <span class=\"hljs-string\">\"\u2581\"</span>:\n         \
          \   suffix = <span class=\"hljs-string\">\"\"</span>\n            <span\
          \ class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(<span\
          \ class=\"hljs-built_in\">next</span>) &gt; <span class=\"hljs-number\"\
          >1</span>:\n                suffix = new_text[-(<span class=\"hljs-built_in\"\
          >len</span>(<span class=\"hljs-built_in\">next</span>)-<span class=\"hljs-number\"\
          >1</span>):]\n            new_text = new_text[:-<span class=\"hljs-built_in\"\
          >len</span>(<span class=\"hljs-built_in\">next</span>)] + suffix\n     \
          \       new_tokens[-<span class=\"hljs-number\">1</span>] = <span class=\"\
          hljs-built_in\">next</span>[<span class=\"hljs-number\">1</span>:]\n   \
          \         <span class=\"hljs-keyword\">if</span> new_tokens[-<span class=\"\
          hljs-number\">1</span>] == <span class=\"hljs-string\">\"\"</span>:\n  \
          \              new_tokens = new_tokens[:-<span class=\"hljs-number\">1</span>]\n\
          \                output_tokens = output_tokens[:-<span class=\"hljs-number\"\
          >1</span>]\n                prefix_offset -= <span class=\"hljs-number\"\
          >1</span>\n            <span class=\"hljs-keyword\">else</span>:\n     \
          \           output_tokens[-<span class=\"hljs-number\">1</span>] = new_tokens[-<span\
          \ class=\"hljs-number\">1</span>]\n</code></pre>\n<p>It's a bit of a quick\
          \ fix but it's working for now. I plan to address this issue by adding the\
          \ tokens directly into the tokenizer. Sorry for any trouble this has caused.</p>\n\
          <p>Thanks,<br>Seungduk</p>\n"
        raw: "Hello Jeonghwan and Young Woo,\n\nThank you for pointing out the issue.\
          \ Yes, I'm aware of it. The problem seems to be because all the tokens from\
          \ the \"added_tokens.json\" file are treated as special tokens. I'm unsure\
          \ if this is intentional since there's a separate \"special_tokens_map.json\"\
          \ file for special tokens. This causes the tokenizer to insert a space after\
          \ each token I added.\n\nHere's a workaround I've been using:\n\n```python\n\
          if prev_tokens is not None:\n    last = tokenizer.convert_tokens_to_ids(prev_tokens[-1:])\n\
          \    if last[0] > 32000:\n        next = new_tokens[-1]\n        if next[0]\
          \ == \"\u2581\":\n            suffix = \"\"\n            if len(next) >\
          \ 1:\n                suffix = new_text[-(len(next)-1):]\n            new_text\
          \ = new_text[:-len(next)] + suffix\n            new_tokens[-1] = next[1:]\n\
          \            if new_tokens[-1] == \"\":\n                new_tokens = new_tokens[:-1]\n\
          \                output_tokens = output_tokens[:-1]\n                prefix_offset\
          \ -= 1\n            else:\n                output_tokens[-1] = new_tokens[-1]\n\
          ```\n\nIt's a bit of a quick fix but it's working for now. I plan to address\
          \ this issue by adding the tokens directly into the tokenizer. Sorry for\
          \ any trouble this has caused.\n\nThanks,\nSeungduk"
        updatedAt: '2024-01-04T09:13:14.083Z'
      numEdits: 0
      reactions: []
    id: 659676aa419fdae3f9b02897
    type: comment
  author: seungduk
  content: "Hello Jeonghwan and Young Woo,\n\nThank you for pointing out the issue.\
    \ Yes, I'm aware of it. The problem seems to be because all the tokens from the\
    \ \"added_tokens.json\" file are treated as special tokens. I'm unsure if this\
    \ is intentional since there's a separate \"special_tokens_map.json\" file for\
    \ special tokens. This causes the tokenizer to insert a space after each token\
    \ I added.\n\nHere's a workaround I've been using:\n\n```python\nif prev_tokens\
    \ is not None:\n    last = tokenizer.convert_tokens_to_ids(prev_tokens[-1:])\n\
    \    if last[0] > 32000:\n        next = new_tokens[-1]\n        if next[0] ==\
    \ \"\u2581\":\n            suffix = \"\"\n            if len(next) > 1:\n    \
    \            suffix = new_text[-(len(next)-1):]\n            new_text = new_text[:-len(next)]\
    \ + suffix\n            new_tokens[-1] = next[1:]\n            if new_tokens[-1]\
    \ == \"\":\n                new_tokens = new_tokens[:-1]\n                output_tokens\
    \ = output_tokens[:-1]\n                prefix_offset -= 1\n            else:\n\
    \                output_tokens[-1] = new_tokens[-1]\n```\n\nIt's a bit of a quick\
    \ fix but it's working for now. I plan to address this issue by adding the tokens\
    \ directly into the tokenizer. Sorry for any trouble this has caused.\n\nThanks,\n\
    Seungduk"
  created_at: 2024-01-04 09:13:14+00:00
  edited: false
  hidden: false
  id: 659676aa419fdae3f9b02897
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
      fullname: Seungduk Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: seungduk
      type: user
    createdAt: '2024-01-04T16:12:29.000Z'
    data:
      edited: false
      editors:
      - seungduk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.975761353969574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
          fullname: Seungduk Kim
          isHf: false
          isPro: false
          name: seungduk
          type: user
        html: '<p>It looks like my previous answer was wrong. I totally misunderstood
          how the tokenizer works. Initially, I need to merge the added tokens into
          the original tokenizer model, but it is not straightforward. Also, the slowness
          that Young Woo mentioned could be related to the numerous added tokens.
          Let me get back to you with a solution as soon as possible. Thank you for
          your understanding.</p>

          '
        raw: It looks like my previous answer was wrong. I totally misunderstood how
          the tokenizer works. Initially, I need to merge the added tokens into the
          original tokenizer model, but it is not straightforward. Also, the slowness
          that Young Woo mentioned could be related to the numerous added tokens.
          Let me get back to you with a solution as soon as possible. Thank you for
          your understanding.
        updatedAt: '2024-01-04T16:12:29.929Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - jhflow
        - spow12
        - maywell
    id: 6596d8ed2235d4056b868c9e
    type: comment
  author: seungduk
  content: It looks like my previous answer was wrong. I totally misunderstood how
    the tokenizer works. Initially, I need to merge the added tokens into the original
    tokenizer model, but it is not straightforward. Also, the slowness that Young
    Woo mentioned could be related to the numerous added tokens. Let me get back to
    you with a solution as soon as possible. Thank you for your understanding.
  created_at: 2024-01-04 16:12:29+00:00
  edited: false
  hidden: false
  id: 6596d8ed2235d4056b868c9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
      fullname: Seungduk Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: seungduk
      type: user
    createdAt: '2024-01-05T14:51:34.000Z'
    data:
      edited: false
      editors:
      - seungduk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.880192220211029
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
          fullname: Seungduk Kim
          isHf: false
          isPro: false
          name: seungduk
          type: user
        html: "<p>Hi Jeonghwan and Young Woo,</p>\n<p>I've been investigating this\
          \ issue for some time and realized that the 'merges' in the tokenizer configuration\
          \ were the cause. Jaewon helped me solve this issue and also wrote a blog\
          \ post about it here: <a rel=\"nofollow\" href=\"https://seen-point-bd9.notion.site/Tokenizer-Expansion-ecb6d78211a54ba6b3cf8ebc0ec1d105\"\
          >https://seen-point-bd9.notion.site/Tokenizer-Expansion-ecb6d78211a54ba6b3cf8ebc0ec1d105</a><br>As\
          \ you can see in the blog post, KoSOLAR v0.1's tokenizer does not function\
          \ well, and the length of the encoded result is longer than it should be,\
          \ although it is still shorter than that produced by the original tokenizer.</p>\n\
          <pre><code>&lt;s&gt; \uB2F9\uBD84\uAC04 \uC8FC\uD0DD \uAC00\uACA9\uC5D0\
          \ \uD070 \uC870\uC815\uC774 \uC77C\uC5B4\uB098\uAC70\uB098 \uD558\uB294\
          \ \uACC4\uAE30\uAC00 \uBC1C\uC0DD\uD558\uC9C0 \uC54A\uB294 \uD55C \uC774\
          \uB4E4\uC758 \uC8FC\uD0DD \uBCF5\uADC0\uB294 \uB2F9\uBD84\uAC04 \uC5B4\uB824\
          \uC6CC \uBCF4\uC778\uB2E4\uB294 \uAC83\uC774 \uC911\uB860\uC774\uB2E4\n\n\
          # KoSOLAR v0.1 tokenizer\n[1, 28705, 30287, 41768, 259, 34740, 259, 35790,\
          \ 28705, 29148, 28705, 31694, 28705, 37585, 28705, 29015, 28705, 29415,\
          \ 32633, 32400, 259, 32029, 259, 30106, 32453, 259, 46354, 32208, 259, 30104,\
          \ 29175, 28705, 29282, 28705, 29015, 32173, 259, 34740, 259, 30357, 46682,\
          \ 28705, 29175, 28705, 30287, 41768, 259, 29433, 30710, 31126, 28705, 29477,\
          \ 33020, 28705, 29175, 28705, 38655, 259, 30027, 39265, 28705, 29043]\n\n\
          # KoSOLAR v0.2 tokenizer\n[1, 32119, 41768, 34375, 42984, 32386, 32052,\
          \ 33335, 33725, 32400, 32254, 39212, 32512, 32208, 32440, 32026, 35964,\
          \ 34375, 34822, 29175, 32119, 41768, 38294, 39093, 32264, 32212, 32039,\
          \ 46611, 32034]\n</code></pre>\n<p>As demonstrated, the revised tokenizer\
          \ outputs a much shorter list of token IDs, most of which are newly added\
          \ tokens (&gt;= 32000). This also means that many embeddings in <code>embed_tokens</code>\
          \ and <code>lm_head</code> were not sufficiently trained in KoSOLAR v0.1\
          \ because the corresponding token IDs were not generated frequently enough\
          \ by the tokenizer. Therefore, if I simply replace the tokenizer, its performance\
          \ will significantly degrade. I confirmed it by running an eval with the\
          \ new tokenizer. I had hoped that my mistake would only impact the decoding\
          \ process, but it turned out to be the opposite; it was actually the encoding\
          \ process that was affected.</p>\n<p>I will upload a new version with a\
          \ fix, but it will take some time. I hope to upload the new version by January\
          \ 12.</p>\n<p>Thanks,<br>Seungduk</p>\n"
        raw: "Hi Jeonghwan and Young Woo,\n\nI've been investigating this issue for\
          \ some time and realized that the 'merges' in the tokenizer configuration\
          \ were the cause. Jaewon helped me solve this issue and also wrote a blog\
          \ post about it here: https://seen-point-bd9.notion.site/Tokenizer-Expansion-ecb6d78211a54ba6b3cf8ebc0ec1d105\n\
          As you can see in the blog post, KoSOLAR v0.1's tokenizer does not function\
          \ well, and the length of the encoded result is longer than it should be,\
          \ although it is still shorter than that produced by the original tokenizer.\n\
          \n```\n<s> \uB2F9\uBD84\uAC04 \uC8FC\uD0DD \uAC00\uACA9\uC5D0 \uD070 \uC870\
          \uC815\uC774 \uC77C\uC5B4\uB098\uAC70\uB098 \uD558\uB294 \uACC4\uAE30\uAC00\
          \ \uBC1C\uC0DD\uD558\uC9C0 \uC54A\uB294 \uD55C \uC774\uB4E4\uC758 \uC8FC\
          \uD0DD \uBCF5\uADC0\uB294 \uB2F9\uBD84\uAC04 \uC5B4\uB824\uC6CC \uBCF4\uC778\
          \uB2E4\uB294 \uAC83\uC774 \uC911\uB860\uC774\uB2E4\n\n# KoSOLAR v0.1 tokenizer\n\
          [1, 28705, 30287, 41768, 259, 34740, 259, 35790, 28705, 29148, 28705, 31694,\
          \ 28705, 37585, 28705, 29015, 28705, 29415, 32633, 32400, 259, 32029, 259,\
          \ 30106, 32453, 259, 46354, 32208, 259, 30104, 29175, 28705, 29282, 28705,\
          \ 29015, 32173, 259, 34740, 259, 30357, 46682, 28705, 29175, 28705, 30287,\
          \ 41768, 259, 29433, 30710, 31126, 28705, 29477, 33020, 28705, 29175, 28705,\
          \ 38655, 259, 30027, 39265, 28705, 29043]\n\n# KoSOLAR v0.2 tokenizer\n\
          [1, 32119, 41768, 34375, 42984, 32386, 32052, 33335, 33725, 32400, 32254,\
          \ 39212, 32512, 32208, 32440, 32026, 35964, 34375, 34822, 29175, 32119,\
          \ 41768, 38294, 39093, 32264, 32212, 32039, 46611, 32034]\n```\n\nAs demonstrated,\
          \ the revised tokenizer outputs a much shorter list of token IDs, most of\
          \ which are newly added tokens (>= 32000). This also means that many embeddings\
          \ in `embed_tokens` and `lm_head` were not sufficiently trained in KoSOLAR\
          \ v0.1 because the corresponding token IDs were not generated frequently\
          \ enough by the tokenizer. Therefore, if I simply replace the tokenizer,\
          \ its performance will significantly degrade. I confirmed it by running\
          \ an eval with the new tokenizer. I had hoped that my mistake would only\
          \ impact the decoding process, but it turned out to be the opposite; it\
          \ was actually the encoding process that was affected.\n\nI will upload\
          \ a new version with a fix, but it will take some time. I hope to upload\
          \ the new version by January 12.\n\nThanks,\nSeungduk"
        updatedAt: '2024-01-05T14:51:34.628Z'
      numEdits: 0
      reactions: []
    id: 65981776c731d1caa52729b1
    type: comment
  author: seungduk
  content: "Hi Jeonghwan and Young Woo,\n\nI've been investigating this issue for\
    \ some time and realized that the 'merges' in the tokenizer configuration were\
    \ the cause. Jaewon helped me solve this issue and also wrote a blog post about\
    \ it here: https://seen-point-bd9.notion.site/Tokenizer-Expansion-ecb6d78211a54ba6b3cf8ebc0ec1d105\n\
    As you can see in the blog post, KoSOLAR v0.1's tokenizer does not function well,\
    \ and the length of the encoded result is longer than it should be, although it\
    \ is still shorter than that produced by the original tokenizer.\n\n```\n<s> \uB2F9\
    \uBD84\uAC04 \uC8FC\uD0DD \uAC00\uACA9\uC5D0 \uD070 \uC870\uC815\uC774 \uC77C\uC5B4\
    \uB098\uAC70\uB098 \uD558\uB294 \uACC4\uAE30\uAC00 \uBC1C\uC0DD\uD558\uC9C0 \uC54A\
    \uB294 \uD55C \uC774\uB4E4\uC758 \uC8FC\uD0DD \uBCF5\uADC0\uB294 \uB2F9\uBD84\uAC04\
    \ \uC5B4\uB824\uC6CC \uBCF4\uC778\uB2E4\uB294 \uAC83\uC774 \uC911\uB860\uC774\uB2E4\
    \n\n# KoSOLAR v0.1 tokenizer\n[1, 28705, 30287, 41768, 259, 34740, 259, 35790,\
    \ 28705, 29148, 28705, 31694, 28705, 37585, 28705, 29015, 28705, 29415, 32633,\
    \ 32400, 259, 32029, 259, 30106, 32453, 259, 46354, 32208, 259, 30104, 29175,\
    \ 28705, 29282, 28705, 29015, 32173, 259, 34740, 259, 30357, 46682, 28705, 29175,\
    \ 28705, 30287, 41768, 259, 29433, 30710, 31126, 28705, 29477, 33020, 28705, 29175,\
    \ 28705, 38655, 259, 30027, 39265, 28705, 29043]\n\n# KoSOLAR v0.2 tokenizer\n\
    [1, 32119, 41768, 34375, 42984, 32386, 32052, 33335, 33725, 32400, 32254, 39212,\
    \ 32512, 32208, 32440, 32026, 35964, 34375, 34822, 29175, 32119, 41768, 38294,\
    \ 39093, 32264, 32212, 32039, 46611, 32034]\n```\n\nAs demonstrated, the revised\
    \ tokenizer outputs a much shorter list of token IDs, most of which are newly\
    \ added tokens (>= 32000). This also means that many embeddings in `embed_tokens`\
    \ and `lm_head` were not sufficiently trained in KoSOLAR v0.1 because the corresponding\
    \ token IDs were not generated frequently enough by the tokenizer. Therefore,\
    \ if I simply replace the tokenizer, its performance will significantly degrade.\
    \ I confirmed it by running an eval with the new tokenizer. I had hoped that my\
    \ mistake would only impact the decoding process, but it turned out to be the\
    \ opposite; it was actually the encoding process that was affected.\n\nI will\
    \ upload a new version with a fix, but it will take some time. I hope to upload\
    \ the new version by January 12.\n\nThanks,\nSeungduk"
  created_at: 2024-01-05 14:51:34+00:00
  edited: false
  hidden: false
  id: 65981776c731d1caa52729b1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: yanolja/KoSOLAR-10.7B-v0.1-deprecated
repo_type: model
status: open
target_branch: null
title: problem in tokenizing
