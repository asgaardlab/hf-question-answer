!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DopeorNope
conflicting_files: null
created_at: 2023-12-29 21:56:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba8cfcc0f19c90256cb56f/rCnkvHMmouJi159-6OUUp.jpeg?w=200&h=200&f=face
      fullname: Seungyoo Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DopeorNope
      type: user
    createdAt: '2023-12-29T21:56:50.000Z'
    data:
      edited: false
      editors:
      - DopeorNope
      hidden: false
      identifiedLanguage:
        language: ko
        probability: 0.9997745752334595
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba8cfcc0f19c90256cb56f/rCnkvHMmouJi159-6OUUp.jpeg?w=200&h=200&f=face
          fullname: Seungyoo Lee
          isHf: false
          isPro: false
          name: DopeorNope
          type: user
        html: "<p>\uC548\uB155\uD558\uC138\uC694 \uC88B\uC740 \uBAA8\uB378 \uAC10\uC0AC\
          \uD569\uB2C8\uB2E4. </p>\n<p>\uB2E4\uB9CC, \uBAA8\uB378 \uC124\uBA85 \uC77D\
          \uC5B4\uBCF4\uB2C8\uAE4C, pretraining\uC774 \uC544\uB2C8\uB77C full finetuning\uB41C\
          \ \uAC83 \uAC19\uC740\uB370, pretraining\uC73C\uB85C \uC62C\uB77C\uC628\uAC83\
          \ \uAC19\uC544\uC11C \uC9C8\uBB38 \uB0A8\uACA8\uBD05\uB2C8\uB2E4. </p>\n\
          <p>\uAC10\uC0AC\uD569\uB2C8\uB2E4. </p>\n"
        raw: "\uC548\uB155\uD558\uC138\uC694 \uC88B\uC740 \uBAA8\uB378 \uAC10\uC0AC\
          \uD569\uB2C8\uB2E4. \r\n\r\n\uB2E4\uB9CC, \uBAA8\uB378 \uC124\uBA85 \uC77D\
          \uC5B4\uBCF4\uB2C8\uAE4C, pretraining\uC774 \uC544\uB2C8\uB77C full finetuning\uB41C\
          \ \uAC83 \uAC19\uC740\uB370, pretraining\uC73C\uB85C \uC62C\uB77C\uC628\uAC83\
          \ \uAC19\uC544\uC11C \uC9C8\uBB38 \uB0A8\uACA8\uBD05\uB2C8\uB2E4. \r\n\r\
          \n\uAC10\uC0AC\uD569\uB2C8\uB2E4. "
        updatedAt: '2023-12-29T21:56:50.131Z'
      numEdits: 0
      reactions: []
    id: 658f40a20f4519bfc25b08a5
    type: comment
  author: DopeorNope
  content: "\uC548\uB155\uD558\uC138\uC694 \uC88B\uC740 \uBAA8\uB378 \uAC10\uC0AC\uD569\
    \uB2C8\uB2E4. \r\n\r\n\uB2E4\uB9CC, \uBAA8\uB378 \uC124\uBA85 \uC77D\uC5B4\uBCF4\
    \uB2C8\uAE4C, pretraining\uC774 \uC544\uB2C8\uB77C full finetuning\uB41C \uAC83\
    \ \uAC19\uC740\uB370, pretraining\uC73C\uB85C \uC62C\uB77C\uC628\uAC83 \uAC19\uC544\
    \uC11C \uC9C8\uBB38 \uB0A8\uACA8\uBD05\uB2C8\uB2E4. \r\n\r\n\uAC10\uC0AC\uD569\
    \uB2C8\uB2E4. "
  created_at: 2023-12-29 21:56:50+00:00
  edited: false
  hidden: false
  id: 658f40a20f4519bfc25b08a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0af9ddc48929ca837573ec41f41868c5.svg
      fullname: Seungduk Kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seungdeok
      type: user
    createdAt: '2023-12-30T03:15:00.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/0af9ddc48929ca837573ec41f41868c5.svg
          fullname: Seungduk Kim
          isHf: false
          isPro: false
          name: seungdeok
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-30T03:37:20.277Z'
      numEdits: 0
      reactions: []
    id: 658f8b3454f8826173ba077f
    type: comment
  author: seungdeok
  content: This comment has been hidden
  created_at: 2023-12-30 03:15:00+00:00
  edited: true
  hidden: true
  id: 658f8b3454f8826173ba077f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
      fullname: Seungduk Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: seungduk
      type: user
    createdAt: '2023-12-30T03:37:27.000Z'
    data:
      edited: false
      editors:
      - seungduk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9636780619621277
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
          fullname: Seungduk Kim
          isHf: false
          isPro: false
          name: seungduk
          type: user
        html: '<p>The question in English is:</p>

          <blockquote>

          <p>"Hello, thank you for the good model.</p>

          <p>However, reading the model description, it seems like it''s fully fine-tuned
          rather than pre-trained, so I''m leaving a question since it seems to be
          uploaded as pre-training.</p>

          <p>Thank you."</p>

          </blockquote>

          <p>Answering the question:</p>

          <p>Thank you for reaching out and for your kind words about the model. It''s
          important to clarify the difference between a pre-trained model and a fine-tuned
          model. A pre-trained model is trained on a large, general dataset to learn
          a wide range of features that can be useful across many tasks. This model
          has indeed been pre-trained in such a manner. Fine-tuning, on the other
          hand, is when you take a pre-trained model and continue the training on
          a more specific dataset or task to adapt the model to particular requirements.</p>

          <p>For this particular model, the core layers, except for the <code>embed_tokens</code>
          and <code>lm_head</code>, remain as they were during the initial pre-training,
          meaning most of the model''s parameters are from the original pre-training
          without further fine-tuning. This approach ensures that the foundational
          knowledge from the pre-training is retained.</p>

          <p>Regarding the new tokens, they were introduced and trained during the
          pre-training phase, not fine-tuned from existing parameters. The embeddings
          for these new tokens were learned from scratch, enriching the model''s vocabulary
          and capabilities. As such, the resulting model is essentially a pre-trained
          model with expanded linguistic understanding, accommodating both the original
          and newly introduced tokens.</p>

          <p>I hope this clears up any confusion regarding the model''s training status.
          The model is primarily pre-trained with specific enhancements to include
          the new tokens. If you have further questions or need more information,
          please feel free to ask. Your feedback is invaluable in helping us improve
          our communication and model descriptions. Thank you!</p>

          '
        raw: 'The question in English is:

          > "Hello, thank you for the good model.

          >

          > However, reading the model description, it seems like it''s fully fine-tuned
          rather than pre-trained, so I''m leaving a question since it seems to be
          uploaded as pre-training.

          >

          > Thank you."


          Answering the question:


          Thank you for reaching out and for your kind words about the model. It''s
          important to clarify the difference between a pre-trained model and a fine-tuned
          model. A pre-trained model is trained on a large, general dataset to learn
          a wide range of features that can be useful across many tasks. This model
          has indeed been pre-trained in such a manner. Fine-tuning, on the other
          hand, is when you take a pre-trained model and continue the training on
          a more specific dataset or task to adapt the model to particular requirements.


          For this particular model, the core layers, except for the `embed_tokens`
          and `lm_head`, remain as they were during the initial pre-training, meaning
          most of the model''s parameters are from the original pre-training without
          further fine-tuning. This approach ensures that the foundational knowledge
          from the pre-training is retained.


          Regarding the new tokens, they were introduced and trained during the pre-training
          phase, not fine-tuned from existing parameters. The embeddings for these
          new tokens were learned from scratch, enriching the model''s vocabulary
          and capabilities. As such, the resulting model is essentially a pre-trained
          model with expanded linguistic understanding, accommodating both the original
          and newly introduced tokens.


          I hope this clears up any confusion regarding the model''s training status.
          The model is primarily pre-trained with specific enhancements to include
          the new tokens. If you have further questions or need more information,
          please feel free to ask. Your feedback is invaluable in helping us improve
          our communication and model descriptions. Thank you!'
        updatedAt: '2023-12-30T03:37:27.045Z'
      numEdits: 0
      reactions: []
    id: 658f90773f3faee8eceb475a
    type: comment
  author: seungduk
  content: 'The question in English is:

    > "Hello, thank you for the good model.

    >

    > However, reading the model description, it seems like it''s fully fine-tuned
    rather than pre-trained, so I''m leaving a question since it seems to be uploaded
    as pre-training.

    >

    > Thank you."


    Answering the question:


    Thank you for reaching out and for your kind words about the model. It''s important
    to clarify the difference between a pre-trained model and a fine-tuned model.
    A pre-trained model is trained on a large, general dataset to learn a wide range
    of features that can be useful across many tasks. This model has indeed been pre-trained
    in such a manner. Fine-tuning, on the other hand, is when you take a pre-trained
    model and continue the training on a more specific dataset or task to adapt the
    model to particular requirements.


    For this particular model, the core layers, except for the `embed_tokens` and
    `lm_head`, remain as they were during the initial pre-training, meaning most of
    the model''s parameters are from the original pre-training without further fine-tuning.
    This approach ensures that the foundational knowledge from the pre-training is
    retained.


    Regarding the new tokens, they were introduced and trained during the pre-training
    phase, not fine-tuned from existing parameters. The embeddings for these new tokens
    were learned from scratch, enriching the model''s vocabulary and capabilities.
    As such, the resulting model is essentially a pre-trained model with expanded
    linguistic understanding, accommodating both the original and newly introduced
    tokens.


    I hope this clears up any confusion regarding the model''s training status. The
    model is primarily pre-trained with specific enhancements to include the new tokens.
    If you have further questions or need more information, please feel free to ask.
    Your feedback is invaluable in helping us improve our communication and model
    descriptions. Thank you!'
  created_at: 2023-12-30 03:37:27+00:00
  edited: false
  hidden: false
  id: 658f90773f3faee8eceb475a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba8cfcc0f19c90256cb56f/rCnkvHMmouJi159-6OUUp.jpeg?w=200&h=200&f=face
      fullname: Seungyoo Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DopeorNope
      type: user
    createdAt: '2023-12-30T04:28:06.000Z'
    data:
      edited: true
      editors:
      - DopeorNope
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774595499038696
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba8cfcc0f19c90256cb56f/rCnkvHMmouJi159-6OUUp.jpeg?w=200&h=200&f=face
          fullname: Seungyoo Lee
          isHf: false
          isPro: false
          name: DopeorNope
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;seungduk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/seungduk\">@<span class=\"\
          underline\">seungduk</span></a></span>\n\n\t</span></span>  I've definitely\
          \ understood it now.</p>\n<p>So, as I understand it, it sounds like the\
          \ training was done using the lamma2/full fine-tuning config yaml of the\
          \ axolotl library. Would that be correct?</p>\n<p>Additionally, I'm curious\
          \ about what learning rate and number of epochs you used for training.</p>\n\
          <p>Thank you..!</p>\n"
        raw: '@seungduk  I''ve definitely understood it now.


          So, as I understand it, it sounds like the training was done using the lamma2/full
          fine-tuning config yaml of the axolotl library. Would that be correct?


          Additionally, I''m curious about what learning rate and number of epochs
          you used for training.


          Thank you..!'
        updatedAt: '2023-12-30T04:40:19.347Z'
      numEdits: 2
      reactions: []
    id: 658f9c560c9931290501d99c
    type: comment
  author: DopeorNope
  content: '@seungduk  I''ve definitely understood it now.


    So, as I understand it, it sounds like the training was done using the lamma2/full
    fine-tuning config yaml of the axolotl library. Would that be correct?


    Additionally, I''m curious about what learning rate and number of epochs you used
    for training.


    Thank you..!'
  created_at: 2023-12-30 04:28:06+00:00
  edited: true
  hidden: false
  id: 658f9c560c9931290501d99c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
      fullname: Seungduk Kim
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: seungduk
      type: user
    createdAt: '2023-12-30T06:54:22.000Z'
    data:
      edited: false
      editors:
      - seungduk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9471120834350586
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cbe953e092b46ae52b020ac859ba074c.svg
          fullname: Seungduk Kim
          isHf: false
          isPro: false
          name: seungduk
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;seungduk&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/seungduk\"\
          >@<span class=\"underline\">seungduk</span></a></span>\n\n\t</span></span>\
          \  I've definitely understood it now.</p>\n<p>So, as I understand it, it\
          \ sounds like the training was done using the lamma2/full fine-tuning config\
          \ yaml of the axolotl library. Would that be correct?</p>\n<p>Additionally,\
          \ I'm curious about what learning rate and number of epochs you used for\
          \ training.</p>\n<p>Thank you..!</p>\n</blockquote>\n<p>Yes, you're correct.\
          \ Since the SOLAR model is built upon the LLaMA architecture, we used the\
          \ same. Indeed, we employed Axolotl for the training process, albeit with\
          \ specific modifications tailored to our needs, including freezing the embeddings\
          \ for the existing tokens.</p>\n<p>Interestingly, while unfreezing the embeddings\
          \ for the existing tokens did improve the evaluation scores, our practical\
          \ tests indicated that maintaining them frozen, as done in the KoSOLAR approach,\
          \ yielded better real-world performance. This discrepancy has led me to\
          \ be cautious about relying solely on evaluation scores.</p>\n<p>For detailed\
          \ information regarding the learning rate and the number of training steps,\
          \ I would recommend referring to the model card, where these specifics are\
          \ documented. Thank you.</p>\n"
        raw: "> @seungduk  I've definitely understood it now.\n> \n> So, as I understand\
          \ it, it sounds like the training was done using the lamma2/full fine-tuning\
          \ config yaml of the axolotl library. Would that be correct?\n> \n> Additionally,\
          \ I'm curious about what learning rate and number of epochs you used for\
          \ training.\n> \n> Thank you..!\n\nYes, you're correct. Since the SOLAR\
          \ model is built upon the LLaMA architecture, we used the same. Indeed,\
          \ we employed Axolotl for the training process, albeit with specific modifications\
          \ tailored to our needs, including freezing the embeddings for the existing\
          \ tokens.\n\nInterestingly, while unfreezing the embeddings for the existing\
          \ tokens did improve the evaluation scores, our practical tests indicated\
          \ that maintaining them frozen, as done in the KoSOLAR approach, yielded\
          \ better real-world performance. This discrepancy has led me to be cautious\
          \ about relying solely on evaluation scores.\n\nFor detailed information\
          \ regarding the learning rate and the number of training steps, I would\
          \ recommend referring to the model card, where these specifics are documented.\
          \ Thank you."
        updatedAt: '2023-12-30T06:54:22.331Z'
      numEdits: 0
      reactions: []
    id: 658fbe9edfca9fad613d9847
    type: comment
  author: seungduk
  content: "> @seungduk  I've definitely understood it now.\n> \n> So, as I understand\
    \ it, it sounds like the training was done using the lamma2/full fine-tuning config\
    \ yaml of the axolotl library. Would that be correct?\n> \n> Additionally, I'm\
    \ curious about what learning rate and number of epochs you used for training.\n\
    > \n> Thank you..!\n\nYes, you're correct. Since the SOLAR model is built upon\
    \ the LLaMA architecture, we used the same. Indeed, we employed Axolotl for the\
    \ training process, albeit with specific modifications tailored to our needs,\
    \ including freezing the embeddings for the existing tokens.\n\nInterestingly,\
    \ while unfreezing the embeddings for the existing tokens did improve the evaluation\
    \ scores, our practical tests indicated that maintaining them frozen, as done\
    \ in the KoSOLAR approach, yielded better real-world performance. This discrepancy\
    \ has led me to be cautious about relying solely on evaluation scores.\n\nFor\
    \ detailed information regarding the learning rate and the number of training\
    \ steps, I would recommend referring to the model card, where these specifics\
    \ are documented. Thank you."
  created_at: 2023-12-30 06:54:22+00:00
  edited: false
  hidden: false
  id: 658fbe9edfca9fad613d9847
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba8cfcc0f19c90256cb56f/rCnkvHMmouJi159-6OUUp.jpeg?w=200&h=200&f=face
      fullname: Seungyoo Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DopeorNope
      type: user
    createdAt: '2023-12-30T17:20:14.000Z'
    data:
      edited: false
      editors:
      - DopeorNope
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9562451839447021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ba8cfcc0f19c90256cb56f/rCnkvHMmouJi159-6OUUp.jpeg?w=200&h=200&f=face
          fullname: Seungyoo Lee
          isHf: false
          isPro: false
          name: DopeorNope
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;seungduk&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/seungduk\">@<span class=\"\
          underline\">seungduk</span></a></span>\n\n\t</span></span> Ha ha ha, it\
          \ seems that by unfreezing the layer, you have expanded the vocabulary and\
          \ learned very well about the new tokens. Thank you for providing a good\
          \ approach and insights.<br>Thank you for contributing to the open-source\
          \ community with your clear explanation.</p>\n<p>I hope that we can get\
          \ a grateful fruit in Korean open-source LLM.</p>\n<p>Best Regards</p>\n"
        raw: '@seungduk Ha ha ha, it seems that by unfreezing the layer, you have
          expanded the vocabulary and learned very well about the new tokens. Thank
          you for providing a good approach and insights.

          Thank you for contributing to the open-source community with your clear
          explanation.


          I hope that we can get a grateful fruit in Korean open-source LLM.


          Best Regards'
        updatedAt: '2023-12-30T17:20:14.744Z'
      numEdits: 0
      reactions: []
    id: 6590514e50d39af7f4021443
    type: comment
  author: DopeorNope
  content: '@seungduk Ha ha ha, it seems that by unfreezing the layer, you have expanded
    the vocabulary and learned very well about the new tokens. Thank you for providing
    a good approach and insights.

    Thank you for contributing to the open-source community with your clear explanation.


    I hope that we can get a grateful fruit in Korean open-source LLM.


    Best Regards'
  created_at: 2023-12-30 17:20:14+00:00
  edited: false
  hidden: false
  id: 6590514e50d39af7f4021443
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yanolja/KoSOLAR-10.7B-v0.1-deprecated
repo_type: model
status: open
target_branch: null
title: Question about model.
