!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aarunsoman
conflicting_files: null
created_at: 2023-07-05 11:57:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96070a949488fa5c0eebe808fa2f0641.svg
      fullname: arun soman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aarunsoman
      type: user
    createdAt: '2023-07-05T12:57:05.000Z'
    data:
      edited: false
      editors:
      - aarunsoman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4109513759613037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96070a949488fa5c0eebe808fa2f0641.svg
          fullname: arun soman
          isHf: false
          isPro: false
          name: aarunsoman
          type: user
        html: '<p>model = transformers.AutoModelForCausalLM.from_pretrained(<br>    ''h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3'',<br>    trust_remote_code=True,<br>    load_8bit=True,<br>    torch_dtype=bfloat16,<br>)<br>doesn''t
          work</p>

          '
        raw: "\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\n \
          \   'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3',\r\n    trust_remote_code=True,\r\
          \n    load_8bit=True,\r\n    torch_dtype=bfloat16,\r\n)\r\ndoesn't work"
        updatedAt: '2023-07-05T12:57:05.338Z'
      numEdits: 0
      reactions: []
    id: 64a568a1f95d48e7c51c6bae
    type: comment
  author: aarunsoman
  content: "\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\n   \
    \ 'h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3',\r\n    trust_remote_code=True,\r\
    \n    load_8bit=True,\r\n    torch_dtype=bfloat16,\r\n)\r\ndoesn't work"
  created_at: 2023-07-05 11:57:05+00:00
  edited: false
  hidden: false
  id: 64a568a1f95d48e7c51c6bae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
      fullname: Pascal Pfeiffer
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ilu000
      type: user
    createdAt: '2023-07-05T19:44:19.000Z'
    data:
      edited: false
      editors:
      - ilu000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46078601479530334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
          fullname: Pascal Pfeiffer
          isHf: false
          isPro: false
          name: ilu000
          type: user
        html: "<pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> h2oai_pipeline <span class=\"\
          hljs-keyword\">import</span> H2OTextGenerationPipeline\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer, BitsAndBytesConfig\n\nquantization_config = <span class=\"\
          hljs-literal\">None</span>\n<span class=\"hljs-comment\"># optional quantization</span>\n\
          quantization_config = BitsAndBytesConfig(\n    load_in_8bit=<span class=\"\
          hljs-literal\">True</span>,\n    llm_int8_threshold=<span class=\"hljs-number\"\
          >6.0</span>,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    <span\
          \ class=\"hljs-string\">\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\"\
          </span>,\n    use_fast=<span class=\"hljs-literal\">False</span>,\n    padding_side=<span\
          \ class=\"hljs-string\">\"left\"</span>,\n    trust_remote_code=<span class=\"\
          hljs-literal\">True</span>,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    <span class=\"hljs-string\">\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\"\
          </span>,\n    trust_remote_code=<span class=\"hljs-literal\">True</span>,\n\
          \    torch_dtype=torch.float16,\n    device_map={<span class=\"hljs-string\"\
          >\"\"</span>: <span class=\"hljs-string\">\"cuda:0\"</span>},\n    quantization_config=quantization_config\n\
          ).<span class=\"hljs-built_in\">eval</span>()\ngenerate_text = H2OTextGenerationPipeline(model=model,\
          \ tokenizer=tokenizer)\n\nres = generate_text(\n    <span class=\"hljs-string\"\
          >\"Why is drinking water so healthy?\"</span>,\n    min_new_tokens=<span\
          \ class=\"hljs-number\">2</span>,\n    max_new_tokens=<span class=\"hljs-number\"\
          >1024</span>,\n    do_sample=<span class=\"hljs-literal\">False</span>,\n\
          \    num_beams=<span class=\"hljs-number\">1</span>,\n    temperature=<span\
          \ class=\"hljs-built_in\">float</span>(<span class=\"hljs-number\">0.3</span>),\n\
          \    repetition_penalty=<span class=\"hljs-built_in\">float</span>(<span\
          \ class=\"hljs-number\">1.2</span>),\n    renormalize_logits=<span class=\"\
          hljs-literal\">True</span>\n)\n<span class=\"hljs-built_in\">print</span>(res[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">\"generated_text\"\
          </span>])\n</code></pre>\n<p>Something like this should work, but this model\
          \ has been trained in float16, quantization will likely lead to degraded\
          \ results. </p>\n"
        raw: "```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\
          \nquantization_config = None\n# optional quantization\nquantization_config\
          \ = BitsAndBytesConfig(\n    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n\
          )\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\"\
          ,\n    use_fast=False,\n    padding_side=\"left\",\n    trust_remote_code=True,\n\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\"\
          ,\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map={\"\
          \": \"cuda:0\"},\n    quantization_config=quantization_config\n).eval()\n\
          generate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\
          \nres = generate_text(\n    \"Why is drinking water so healthy?\",\n   \
          \ min_new_tokens=2,\n    max_new_tokens=1024,\n    do_sample=False,\n  \
          \  num_beams=1,\n    temperature=float(0.3),\n    repetition_penalty=float(1.2),\n\
          \    renormalize_logits=True\n)\nprint(res[0][\"generated_text\"])\n```\n\
          \nSomething like this should work, but this model has been trained in float16,\
          \ quantization will likely lead to degraded results. "
        updatedAt: '2023-07-05T19:44:19.001Z'
      numEdits: 0
      reactions: []
    id: 64a5c813ac8923d9252f0f9c
    type: comment
  author: ilu000
  content: "```python\nimport torch\nfrom h2oai_pipeline import H2OTextGenerationPipeline\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\
    \nquantization_config = None\n# optional quantization\nquantization_config = BitsAndBytesConfig(\n\
    \    load_in_8bit=True,\n    llm_int8_threshold=6.0,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
    \    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\",\n    use_fast=False,\n \
    \   padding_side=\"left\",\n    trust_remote_code=True,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3\",\n    trust_remote_code=True,\n\
    \    torch_dtype=torch.float16,\n    device_map={\"\": \"cuda:0\"},\n    quantization_config=quantization_config\n\
    ).eval()\ngenerate_text = H2OTextGenerationPipeline(model=model, tokenizer=tokenizer)\n\
    \nres = generate_text(\n    \"Why is drinking water so healthy?\",\n    min_new_tokens=2,\n\
    \    max_new_tokens=1024,\n    do_sample=False,\n    num_beams=1,\n    temperature=float(0.3),\n\
    \    repetition_penalty=float(1.2),\n    renormalize_logits=True\n)\nprint(res[0][\"\
    generated_text\"])\n```\n\nSomething like this should work, but this model has\
    \ been trained in float16, quantization will likely lead to degraded results. "
  created_at: 2023-07-05 18:44:19+00:00
  edited: false
  hidden: false
  id: 64a5c813ac8923d9252f0f9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676417502037-6316fc44c92fd6fee3161e9a.png?w=200&h=200&f=face
      fullname: Pascal Pfeiffer
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ilu000
      type: user
    createdAt: '2023-07-09T14:38:24.000Z'
    data:
      status: closed
    id: 64aac6603c974392fbaa609d
    type: status-change
  author: ilu000
  created_at: 2023-07-09 13:38:24+00:00
  id: 64aac6603c974392fbaa609d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3
repo_type: model
status: closed
target_branch: null
title: how to enable low memory version
