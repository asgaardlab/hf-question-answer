!!python/object:huggingface_hub.community.DiscussionWithDetails
author: carlosbdw
conflicting_files: null
created_at: 2023-06-04 06:27:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
      fullname: dawei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosbdw
      type: user
    createdAt: '2023-06-04T07:27:38.000Z'
    data:
      edited: false
      editors:
      - carlosbdw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4338052570819855
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
          fullname: dawei
          isHf: false
          isPro: false
          name: carlosbdw
          type: user
        html: '<p>GPU: A40(48GB) * 1<br>CPU: 15 vCPU AMD EPYC 7543 32-Core Processor<br>MEM:
          80GB</p>

          <p>/root/text-generation-webui<br>bin /root/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so<br>INFO:Loading
          guanaco-65B-GPTQ...<br>CUDA extension not installed.<br>INFO:Found the following
          quantized model: models/guanaco-65B-GPTQ/Guanaco-65B-GPTQ-4bit.act-order.safetensors<br>Traceback
          (most recent call last):<br>  File "/root/text-generation-webui/server.py",
          line 1102, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "/root/text-generation-webui/modules/models.py", line 97, in load_model<br>    output
          = load_func(model_name)<br>  File "/root/text-generation-webui/modules/models.py",
          line 291, in GPTQ_loader<br>    model = modules.GPTQ_loader.load_quantized(model_name)<br>  File
          "/root/text-generation-webui/modules/GPTQ_loader.py", line 177, in load_quantized<br>    model
          = load_quant(str(path_to_model), str(pt_path), shared.args.wbits, shared.args.groupsize,
          kernel_switch_threshold=threshold)<br>  File "/root/text-generation-webui/modules/GPTQ_loader.py",
          line 84, in _load_quant<br>    model.load_state_dict(safe_load(checkpoint),
          strict=False)<br>  File "/root/miniconda3/lib/python3.10/site-packages/safetensors/torch.py",
          line 259, in load_file<br>    with safe_open(filename, framework="pt", device=device)
          as f:<br>safetensors_rust.SafetensorError: Error while deserializing header:
          MetadataIncompleteBuffer</p>

          '
        raw: "GPU: A40(48GB) * 1\r\nCPU: 15 vCPU AMD EPYC 7543 32-Core Processor\r\
          \nMEM: 80GB\r\n\r\n\r\n/root/text-generation-webui\r\nbin /root/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so\r\
          \nINFO:Loading guanaco-65B-GPTQ...\r\nCUDA extension not installed.\r\n\
          INFO:Found the following quantized model: models/guanaco-65B-GPTQ/Guanaco-65B-GPTQ-4bit.act-order.safetensors\r\
          \nTraceback (most recent call last):\r\n  File \"/root/text-generation-webui/server.py\"\
          , line 1102, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \n  File \"/root/text-generation-webui/modules/models.py\", line 97, in\
          \ load_model\r\n    output = load_func(model_name)\r\n  File \"/root/text-generation-webui/modules/models.py\"\
          , line 291, in GPTQ_loader\r\n    model = modules.GPTQ_loader.load_quantized(model_name)\r\
          \n  File \"/root/text-generation-webui/modules/GPTQ_loader.py\", line 177,\
          \ in load_quantized\r\n    model = load_quant(str(path_to_model), str(pt_path),\
          \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\r\
          \n  File \"/root/text-generation-webui/modules/GPTQ_loader.py\", line 84,\
          \ in _load_quant\r\n    model.load_state_dict(safe_load(checkpoint), strict=False)\r\
          \n  File \"/root/miniconda3/lib/python3.10/site-packages/safetensors/torch.py\"\
          , line 259, in load_file\r\n    with safe_open(filename, framework=\"pt\"\
          , device=device) as f:\r\nsafetensors_rust.SafetensorError: Error while\
          \ deserializing header: MetadataIncompleteBuffer"
        updatedAt: '2023-06-04T07:27:38.168Z'
      numEdits: 0
      reactions: []
    id: 647c3cea1b6ecd15f4b9bcea
    type: comment
  author: carlosbdw
  content: "GPU: A40(48GB) * 1\r\nCPU: 15 vCPU AMD EPYC 7543 32-Core Processor\r\n\
    MEM: 80GB\r\n\r\n\r\n/root/text-generation-webui\r\nbin /root/miniconda3/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda116.so\r\
    \nINFO:Loading guanaco-65B-GPTQ...\r\nCUDA extension not installed.\r\nINFO:Found\
    \ the following quantized model: models/guanaco-65B-GPTQ/Guanaco-65B-GPTQ-4bit.act-order.safetensors\r\
    \nTraceback (most recent call last):\r\n  File \"/root/text-generation-webui/server.py\"\
    , line 1102, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"/root/text-generation-webui/modules/models.py\", line 97, in load_model\r\
    \n    output = load_func(model_name)\r\n  File \"/root/text-generation-webui/modules/models.py\"\
    , line 291, in GPTQ_loader\r\n    model = modules.GPTQ_loader.load_quantized(model_name)\r\
    \n  File \"/root/text-generation-webui/modules/GPTQ_loader.py\", line 177, in\
    \ load_quantized\r\n    model = load_quant(str(path_to_model), str(pt_path), shared.args.wbits,\
    \ shared.args.groupsize, kernel_switch_threshold=threshold)\r\n  File \"/root/text-generation-webui/modules/GPTQ_loader.py\"\
    , line 84, in _load_quant\r\n    model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\r\n  File \"/root/miniconda3/lib/python3.10/site-packages/safetensors/torch.py\"\
    , line 259, in load_file\r\n    with safe_open(filename, framework=\"pt\", device=device)\
    \ as f:\r\nsafetensors_rust.SafetensorError: Error while deserializing header:\
    \ MetadataIncompleteBuffer"
  created_at: 2023-06-04 06:27:38+00:00
  edited: false
  hidden: false
  id: 647c3cea1b6ecd15f4b9bcea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T09:51:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960014820098877
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This looks like the model didn''t download fully.  Check you have
          enough disk space, and then please try the download again.  text-gen-ui''s
          downloader will auto resume, so you don''t need to download the whole thing
          again, it will download whatever parts are missing.</p>

          '
        raw: This looks like the model didn't download fully.  Check you have enough
          disk space, and then please try the download again.  text-gen-ui's downloader
          will auto resume, so you don't need to download the whole thing again, it
          will download whatever parts are missing.
        updatedAt: '2023-06-04T09:51:22.708Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - manfred-exz
    id: 647c5e9a1c0644de8d20c1e0
    type: comment
  author: TheBloke
  content: This looks like the model didn't download fully.  Check you have enough
    disk space, and then please try the download again.  text-gen-ui's downloader
    will auto resume, so you don't need to download the whole thing again, it will
    download whatever parts are missing.
  created_at: 2023-06-04 08:51:22+00:00
  edited: false
  hidden: false
  id: 647c5e9a1c0644de8d20c1e0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: error when uing the text-generation-webui api with the model
