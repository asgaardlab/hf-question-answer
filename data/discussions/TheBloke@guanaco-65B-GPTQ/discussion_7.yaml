!!python/object:huggingface_hub.community.DiscussionWithDetails
author: carlosbdw
conflicting_files: null
created_at: 2023-06-01 07:53:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
      fullname: dawei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosbdw
      type: user
    createdAt: '2023-06-01T08:53:29.000Z'
    data:
      edited: false
      editors:
      - carlosbdw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
          fullname: dawei
          isHf: false
          isPro: false
          name: carlosbdw
          type: user
        html: '<p>I have tried this:<br>#############################################################################<br>from
          transformers import AutoTokenizer, pipeline, logging,AutoTokenizer<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import argparse</p>

          <p>quantized_model_dir = "/media/galaxy/guanaco/guanaco-65B-GPTQ"<br>model_basename
          = "Guanaco-65B-GPTQ-4bit.act-order"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)</p>

          <p>quantize_config = BaseQuantizeConfig(<br>        bits=4,<br>        group_size=128,<br>        desc_act=False<br>    )</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>        use_safetensors=True,<br>        model_basename=model_basename,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=quantize_config)</p>

          <h1 id="prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq">Prevent
          printing spurious transformers error when using pipeline with AutoGPTQ</h1>

          <p>logging.set_verbosity(logging.CRITICAL)</p>

          <p>prompt = "Tell me about AI"<br>prompt_template=f''''''### Human: {prompt}</p>

          <h3 id="assistant">Assistant:''''''</h3>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tok,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])</p>

          <p>print("\n\n*** Generate:")</p>

          <p>input_ids = tok(prompt_template, return_tensors=''pt'').input_ids.cuda()<br>output
          = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>

          <p>#############################################################################<br>but
          got an error :<br>Exception: data did not match any variant of untagged
          enum PyNormalizerTypeWrapper at line 49<br>column 3</p>

          <p>I am using an 4x 40G station , any suggestion ? or it will be great if
          a demo python file could be provided ,thank you .</p>

          '
        raw: "I have tried this:\r\n#############################################################################\r\
          \nfrom transformers import AutoTokenizer, pipeline, logging,AutoTokenizer\r\
          \nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nimport\
          \ argparse\r\n\r\nquantized_model_dir = \"/media/galaxy/guanaco/guanaco-65B-GPTQ\"\
          \r\nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\r\n\r\nuse_triton\
          \ = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\r\n\r\nquantize_config = BaseQuantizeConfig(\r\n      \
          \  bits=4,\r\n        group_size=128,\r\n        desc_act=False\r\n    )\r\
          \n\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\r\n\
          \        use_safetensors=True,\r\n        model_basename=model_basename,\r\
          \n        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n     \
          \   quantize_config=quantize_config)\r\n\r\n# Prevent printing spurious\
          \ transformers error when using pipeline with AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\
          \n\r\nprompt = \"Tell me about AI\"\r\nprompt_template=f'''### Human: {prompt}\r\
          \n### Assistant:'''\r\n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\
          \n    \"text-generation\",\r\n    model=model,\r\n    tokenizer=tok,\r\n\
          \    max_new_tokens=512,\r\n    temperature=0.7,\r\n    top_p=0.95,\r\n\
          \    repetition_penalty=1.15\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\
          \n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tok(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n\r\n#############################################################################\r\
          \nbut got an error :\r\nException: data did not match any variant of untagged\
          \ enum PyNormalizerTypeWrapper at line 49 \r\ncolumn 3\r\n\r\nI am using\
          \ an 4x 40G station , any suggestion ? or it will be great if a demo python\
          \ file could be provided ,thank you ."
        updatedAt: '2023-06-01T08:53:29.222Z'
      numEdits: 0
      reactions: []
    id: 64785c892b3730b6b63b7501
    type: comment
  author: carlosbdw
  content: "I have tried this:\r\n#############################################################################\r\
    \nfrom transformers import AutoTokenizer, pipeline, logging,AutoTokenizer\r\n\
    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\nimport argparse\r\
    \n\r\nquantized_model_dir = \"/media/galaxy/guanaco/guanaco-65B-GPTQ\"\r\nmodel_basename\
    \ = \"Guanaco-65B-GPTQ-4bit.act-order\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\r\n\r\n\
    quantize_config = BaseQuantizeConfig(\r\n        bits=4,\r\n        group_size=128,\r\
    \n        desc_act=False\r\n    )\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\r\
    \n        use_safetensors=True,\r\n        model_basename=model_basename,\r\n\
    \        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=quantize_config)\r\
    \n\r\n# Prevent printing spurious transformers error when using pipeline with\
    \ AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\n\r\nprompt = \"Tell me\
    \ about AI\"\r\nprompt_template=f'''### Human: {prompt}\r\n### Assistant:'''\r\
    \n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=model,\r\n    tokenizer=tok,\r\n    max_new_tokens=512,\r\n   \
    \ temperature=0.7,\r\n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\n\r\
    \nprint(pipe(prompt_template)[0]['generated_text'])\r\n\r\nprint(\"\\n\\n*** Generate:\"\
    )\r\n\r\ninput_ids = tok(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n\r\n#############################################################################\r\
    \nbut got an error :\r\nException: data did not match any variant of untagged\
    \ enum PyNormalizerTypeWrapper at line 49 \r\ncolumn 3\r\n\r\nI am using an 4x\
    \ 40G station , any suggestion ? or it will be great if a demo python file could\
    \ be provided ,thank you ."
  created_at: 2023-06-01 07:53:29+00:00
  edited: false
  hidden: false
  id: 64785c892b3730b6b63b7501
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-01T08:55:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please show the full output log, and put ``` before and after it.
          </p>

          <pre><code>output should look like this

          </code></pre>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VCggOKWg7vZ0DiBPjLqwO.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VCggOKWg7vZ0DiBPjLqwO.png"></a></p>

          '
        raw: "Please show the full output log, and put \\`\\`\\` before and after\
          \ it. \n\n```\noutput should look like this\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VCggOKWg7vZ0DiBPjLqwO.png)"
        updatedAt: '2023-06-01T08:55:12.109Z'
      numEdits: 0
      reactions: []
    id: 64785cf01f9756aa89cc997d
    type: comment
  author: TheBloke
  content: "Please show the full output log, and put \\`\\`\\` before and after it.\
    \ \n\n```\noutput should look like this\n```\n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/VCggOKWg7vZ0DiBPjLqwO.png)"
  created_at: 2023-06-01 07:55:12+00:00
  edited: false
  hidden: false
  id: 64785cf01f9756aa89cc997d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: How to run it ?
