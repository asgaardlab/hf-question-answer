!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sweaty-lettuce
conflicting_files: null
created_at: 2023-06-28 07:29:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed40b3d5a42a146e29712c3da8d0ae6e.svg
      fullname: Ben
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sweaty-lettuce
      type: user
    createdAt: '2023-06-28T08:29:42.000Z'
    data:
      edited: false
      editors:
      - sweaty-lettuce
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6671314835548401
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed40b3d5a42a146e29712c3da8d0ae6e.svg
          fullname: Ben
          isHf: false
          isPro: false
          name: sweaty-lettuce
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ absolutely love your work, you're an absolute beast.</p>\n<p>Just wanted\
          \ to point out that I've started getting a new issue with your RunPod template\
          \ (TheBloke Local LLMs One-Click UI and API). It seems to have an issue\
          \ loading GPTQ-for-LLaMa now (though seems the same for all models atm).</p>\n\
          <p>These are the errors I'm getting:</p>\n<p>Traceback (most recent call\
          \ last): File \u201C/workspace/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 17, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019</p>\n<p>During handling of the\
          \ above exception, another exception occurred:</p>\n<p>Traceback (most recent\
          \ call last): File \u201C/workspace/text-generation-webui/server.py\u201D\
          , line 67, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201C/workspace/text-generation-webui/modules/models.py\u201D\
          , line 74, in load_model output = load_func_maploader File \u201C/workspace/text-generation-webui/modules/models.py\u201D\
          , line 270, in GPTQ_loader import modules.GPTQ_loader File \u201C/workspace/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 21, in sys.exit(-1) SystemExit: -1</p>\n<p>Not sure if something\
          \ has happened to the template?</p>\n<p>Thanks for all you do!</p>\n"
        raw: "Hey @TheBloke, absolutely love your work, you're an absolute beast.\r\
          \n\r\nJust wanted to point out that I've started getting a new issue with\
          \ your RunPod template (TheBloke Local LLMs One-Click UI and API). It seems\
          \ to have an issue loading GPTQ-for-LLaMa now (though seems the same for\
          \ all models atm).\r\n\r\nThese are the errors I'm getting:\r\n\r\nTraceback\
          \ (most recent call last): File \u201C/workspace/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 17, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019\r\n\r\nDuring handling of the\
          \ above exception, another exception occurred:\r\n\r\nTraceback (most recent\
          \ call last): File \u201C/workspace/text-generation-webui/server.py\u201D\
          , line 67, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader) File \u201C/workspace/text-generation-webui/modules/models.py\u201D\
          , line 74, in load_model output = load_func_maploader File \u201C/workspace/text-generation-webui/modules/models.py\u201D\
          , line 270, in GPTQ_loader import modules.GPTQ_loader File \u201C/workspace/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 21, in sys.exit(-1) SystemExit: -1\r\n\r\nNot sure if something has\
          \ happened to the template?\r\n\r\nThanks for all you do!"
        updatedAt: '2023-06-28T08:29:42.330Z'
      numEdits: 0
      reactions: []
    id: 649bef76d36f5bd63ac0ba6d
    type: comment
  author: sweaty-lettuce
  content: "Hey @TheBloke, absolutely love your work, you're an absolute beast.\r\n\
    \r\nJust wanted to point out that I've started getting a new issue with your RunPod\
    \ template (TheBloke Local LLMs One-Click UI and API). It seems to have an issue\
    \ loading GPTQ-for-LLaMa now (though seems the same for all models atm).\r\n\r\
    \nThese are the errors I'm getting:\r\n\r\nTraceback (most recent call last):\
    \ File \u201C/workspace/text-generation-webui/modules/GPTQ_loader.py\u201D, line\
    \ 17, in import llama_inference_offload ModuleNotFoundError: No module named \u2018\
    llama_inference_offload\u2019\r\n\r\nDuring handling of the above exception, another\
    \ exception occurred:\r\n\r\nTraceback (most recent call last): File \u201C/workspace/text-generation-webui/server.py\u201D\
    , line 67, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader) File \u201C/workspace/text-generation-webui/modules/models.py\u201D\
    , line 74, in load_model output = load_func_maploader File \u201C/workspace/text-generation-webui/modules/models.py\u201D\
    , line 270, in GPTQ_loader import modules.GPTQ_loader File \u201C/workspace/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 21, in sys.exit(-1) SystemExit: -1\r\n\r\nNot sure if something has happened\
    \ to the template?\r\n\r\nThanks for all you do!"
  created_at: 2023-06-28 07:29:42+00:00
  edited: false
  hidden: false
  id: 649bef76d36f5bd63ac0ba6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-28T08:32:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.826377272605896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I removed GPTQ-for-Llama. It has AutoGPTQ and ExLlama</p>

          <p>ExLlama is the performance option, providing 2-3x the performance of
          GPTQ-for-LLaMa and AutoGPTQ - but only for 4bit Llama models, not any other
          model types or other bit sizes.<br>And AutoGPTQ is the compatibility option,
          supporting non-llama models and a wide range of bit sizes (although all
          my GPTQs are only 4bit so far)</p>

          <p>Is there a reason you need GPTQ-for-LLaMa specifically? If not, use one
          of those; ExLlama if you can, otherwise AutoGPTQ.</p>

          <p>Check the template README for more details on that, including instructions
          on using ExLlama</p>

          '
        raw: 'Yeah I removed GPTQ-for-Llama. It has AutoGPTQ and ExLlama


          ExLlama is the performance option, providing 2-3x the performance of GPTQ-for-LLaMa
          and AutoGPTQ - but only for 4bit Llama models, not any other model types
          or other bit sizes.

          And AutoGPTQ is the compatibility option, supporting non-llama models and
          a wide range of bit sizes (although all my GPTQs are only 4bit so far)


          Is there a reason you need GPTQ-for-LLaMa specifically? If not, use one
          of those; ExLlama if you can, otherwise AutoGPTQ.


          Check the template README for more details on that, including instructions
          on using ExLlama'
        updatedAt: '2023-06-28T08:32:02.412Z'
      numEdits: 0
      reactions: []
    id: 649bf002aacbf5cb416a6d7c
    type: comment
  author: TheBloke
  content: 'Yeah I removed GPTQ-for-Llama. It has AutoGPTQ and ExLlama


    ExLlama is the performance option, providing 2-3x the performance of GPTQ-for-LLaMa
    and AutoGPTQ - but only for 4bit Llama models, not any other model types or other
    bit sizes.

    And AutoGPTQ is the compatibility option, supporting non-llama models and a wide
    range of bit sizes (although all my GPTQs are only 4bit so far)


    Is there a reason you need GPTQ-for-LLaMa specifically? If not, use one of those;
    ExLlama if you can, otherwise AutoGPTQ.


    Check the template README for more details on that, including instructions on
    using ExLlama'
  created_at: 2023-06-28 07:32:02+00:00
  edited: false
  hidden: false
  id: 649bf002aacbf5cb416a6d7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed40b3d5a42a146e29712c3da8d0ae6e.svg
      fullname: Ben
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sweaty-lettuce
      type: user
    createdAt: '2023-06-28T08:41:35.000Z'
    data:
      edited: false
      editors:
      - sweaty-lettuce
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277802109718323
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed40b3d5a42a146e29712c3da8d0ae6e.svg
          fullname: Ben
          isHf: false
          isPro: false
          name: sweaty-lettuce
          type: user
        html: '<p>Amazing, thanks for the quick response!</p>

          <p>No - just force of habit I suppose. Will make the switch to ExLlama and
          give it a go, thanks again.</p>

          '
        raw: 'Amazing, thanks for the quick response!


          No - just force of habit I suppose. Will make the switch to ExLlama and
          give it a go, thanks again.'
        updatedAt: '2023-06-28T08:41:35.793Z'
      numEdits: 0
      reactions: []
      relatedEventId: 649bf23f85b05ee0271f1143
    id: 649bf23f85b05ee0271f113f
    type: comment
  author: sweaty-lettuce
  content: 'Amazing, thanks for the quick response!


    No - just force of habit I suppose. Will make the switch to ExLlama and give it
    a go, thanks again.'
  created_at: 2023-06-28 07:41:35+00:00
  edited: false
  hidden: false
  id: 649bf23f85b05ee0271f113f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ed40b3d5a42a146e29712c3da8d0ae6e.svg
      fullname: Ben
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sweaty-lettuce
      type: user
    createdAt: '2023-06-28T08:41:35.000Z'
    data:
      status: closed
    id: 649bf23f85b05ee0271f1143
    type: status-change
  author: sweaty-lettuce
  created_at: 2023-06-28 07:41:35+00:00
  id: 649bf23f85b05ee0271f1143
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: closed
target_branch: null
title: RunPod - New Issue
