!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shawei3000
conflicting_files: null
created_at: 2023-05-29 17:07:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
      fullname: jimsha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawei3000
      type: user
    createdAt: '2023-05-29T18:07:00.000Z'
    data:
      edited: false
      editors:
      - shawei3000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
          fullname: jimsha
          isHf: false
          isPro: false
          name: shawei3000
          type: user
        html: '<p>I could not, tried many ways, on torch 2.0 ; would you be able to
          provide script how you load the downloaded 4bit safetensors?</p>

          '
        raw: I could not, tried many ways, on torch 2.0 ; would you be able to provide
          script how you load the downloaded 4bit safetensors?
        updatedAt: '2023-05-29T18:07:00.328Z'
      numEdits: 0
      reactions: []
    id: 6474e9c46d4dda6f7c6d6be8
    type: comment
  author: shawei3000
  content: I could not, tried many ways, on torch 2.0 ; would you be able to provide
    script how you load the downloaded 4bit safetensors?
  created_at: 2023-05-29 17:07:00+00:00
  edited: false
  hidden: false
  id: 6474e9c46d4dda6f7c6d6be8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-29T21:07:57.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You can't load GPTQ files directly with transformers AutoModelForCausalLM.\
          \  It's not supported without additional code. </p>\n<p>I recommend using\
          \ <a rel=\"nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ\">AutoGPTQ</a></p>\n\
          <p>Here's an example script using AutoGPTQ. Note that first you need to\
          \ download the model locally (eg with <code>git clone</code> - AutoGPTQ\
          \ doesn't yet support directly downloading from HF, but it will very soon)</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\nquantized_model_dir\
          \ = <span class=\"hljs-string\">\"/path/to/guanaco-65B-GPTQ\"</span>\nmodel_basename\
          \ = <span class=\"hljs-string\">\"Guanaco-65B-GPTQ-4bit.act-order\"</span>\n\
          \nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer =\
          \ AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=<span\
          \ class=\"hljs-literal\">None</span>)\n\n<span class=\"hljs-comment\">#\
          \ Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "You can't load GPTQ files directly with transformers AutoModelForCausalLM.\
          \  It's not supported without additional code. \n\nI recommend using [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n\
          \nHere's an example script using AutoGPTQ. Note that first you need to download\
          \ the model locally (eg with `git clone` - AutoGPTQ doesn't yet support\
          \ directly downloading from HF, but it will very soon)\n\n```python\nfrom\
          \ transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
          \ AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nquantized_model_dir\
          \ = \"/path/to/guanaco-65B-GPTQ\"\nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \n# Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me\
          \ about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```"
        updatedAt: '2023-06-09T16:11:03.561Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - ThisUserNameWillNotExist
        - hipozz
        - radames
    id: 6475142d5ada8510bc481e93
    type: comment
  author: TheBloke
  content: "You can't load GPTQ files directly with transformers AutoModelForCausalLM.\
    \  It's not supported without additional code. \n\nI recommend using [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n\
    \nHere's an example script using AutoGPTQ. Note that first you need to download\
    \ the model locally (eg with `git clone` - AutoGPTQ doesn't yet support directly\
    \ downloading from HF, but it will very soon)\n\n```python\nfrom transformers\
    \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\nimport argparse\n\nquantized_model_dir = \"/path/to/guanaco-65B-GPTQ\"\
    \nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\n\nuse_triton = False\n\
    \ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)\n\
    \nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n        use_safetensors=True,\n\
    \        model_basename=model_basename,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
    \        quantize_config=None)\n\n# Prevent printing spurious transformers error\
    \ when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
    \nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n###\
    \ Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    temperature=0.7,\n\
    \    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```"
  created_at: 2023-05-29 20:07:57+00:00
  edited: true
  hidden: false
  id: 6475142d5ada8510bc481e93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
      fullname: jimsha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawei3000
      type: user
    createdAt: '2023-05-29T23:51:07.000Z'
    data:
      edited: false
      editors:
      - shawei3000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
          fullname: jimsha
          isHf: false
          isPro: false
          name: shawei3000
          type: user
        html: '<p>thnx!</p>

          '
        raw: thnx!
        updatedAt: '2023-05-29T23:51:07.416Z'
      numEdits: 0
      reactions: []
    id: 64753a6bd56974d0c066b9c2
    type: comment
  author: shawei3000
  content: thnx!
  created_at: 2023-05-29 22:51:07+00:00
  edited: false
  hidden: false
  id: 64753a6bd56974d0c066b9c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
      fullname: jimsha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawei3000
      type: user
    createdAt: '2023-05-29T23:53:25.000Z'
    data:
      edited: false
      editors:
      - shawei3000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
          fullname: jimsha
          isHf: false
          isPro: false
          name: shawei3000
          type: user
        html: '<p>also thanks for your super fast relase of quantified models for
          the comunity, very helpful, great work!</p>

          '
        raw: also thanks for your super fast relase of quantified models for the comunity,
          very helpful, great work!
        updatedAt: '2023-05-29T23:53:25.509Z'
      numEdits: 0
      reactions: []
    id: 64753af5f9e3e0b312f6b5ed
    type: comment
  author: shawei3000
  content: also thanks for your super fast relase of quantified models for the comunity,
    very helpful, great work!
  created_at: 2023-05-29 22:53:25+00:00
  edited: false
  hidden: false
  id: 64753af5f9e3e0b312f6b5ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-30T00:17:31.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re very welcome!</p>

          '
        raw: You're very welcome!
        updatedAt: '2023-05-30T00:17:31.235Z'
      numEdits: 0
      reactions: []
    id: 6475409b82907acdddf7c94e
    type: comment
  author: TheBloke
  content: You're very welcome!
  created_at: 2023-05-29 23:17:31+00:00
  edited: false
  hidden: false
  id: 6475409b82907acdddf7c94e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
      fullname: Paul CHEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxiangyi10
      type: user
    createdAt: '2023-06-01T09:28:18.000Z'
    data:
      edited: false
      editors:
      - chenxiangyi10
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
          fullname: Paul CHEN
          isHf: false
          isPro: false
          name: chenxiangyi10
          type: user
        html: "<blockquote>\n<p>You can't load GPTQ files directly with transformers\
          \ AutoModelForCausalLM.  It's not supported without additional code. </p>\n\
          <p>I recommend using <a rel=\"nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ\"\
          >AutoGPTQ</a></p>\n<p>Here's an example script using AutoGPTQ. Note that\
          \ first you need to download the model locally (eg with <code>git clone</code>\
          \ - AutoGPTQ doesn't yet support directly downloading from HF, but it will\
          \ very soon)</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\nquantized_model_dir\
          \ = <span class=\"hljs-string\">\"/path/to/guanaco-65B-GPTQ\"</span>\nmodel_basename\
          \ = <span class=\"hljs-string\">\"Guanaco-65B-GPTQ-4bit.act-order.\"</span>\n\
          \nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer =\
          \ AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nquantize_config = BaseQuantizeConfig(\n \
          \       bits=<span class=\"hljs-number\">4</span>,\n        group_size=<span\
          \ class=\"hljs-number\">128</span>,\n        desc_act=<span class=\"hljs-literal\"\
          >False</span>\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=quantize_config)\n\
          \n<span class=\"hljs-comment\"># Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprompt = <span class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n</blockquote>\n<p>Should\
          \ group_size be set as group_size=-1 instead of 128?</p>\n"
        raw: "> You can't load GPTQ files directly with transformers AutoModelForCausalLM.\
          \  It's not supported without additional code. \n> \n> I recommend using\
          \ [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n> \n> Here's an example\
          \ script using AutoGPTQ. Note that first you need to download the model\
          \ locally (eg with `git clone` - AutoGPTQ doesn't yet support directly downloading\
          \ from HF, but it will very soon)\n> \n> ```python\n> from transformers\
          \ import AutoTokenizer, pipeline, logging\n> from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n> import argparse\n> \n> quantized_model_dir = \"\
          /path/to/guanaco-65B-GPTQ\"\n> model_basename = \"Guanaco-65B-GPTQ-4bit.act-order.\"\
          \n> \n> use_triton = False\n> \n> tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n> \n> quantize_config = BaseQuantizeConfig(\n>       \
          \  bits=4,\n>         group_size=128,\n>         desc_act=False\n>     )\n\
          > \n> model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          >         use_safetensors=True,\n>         model_basename=model_basename,\n\
          >         device=\"cuda:0\",\n>         use_triton=use_triton,\n>      \
          \   quantize_config=quantize_config)\n> \n> # Prevent printing spurious\
          \ transformers error when using pipeline with AutoGPTQ\n> logging.set_verbosity(logging.CRITICAL)\n\
          > \n> prompt = \"Tell me about AI\"\n> prompt_template=f'''### Human: {prompt}\n\
          > ### Assistant:'''\n> \n> print(\"*** Pipeline:\")\n> pipe = pipeline(\n\
          >     \"text-generation\",\n>     model=model,\n>     tokenizer=tokenizer,\n\
          >     max_new_tokens=512,\n>     temperature=0.7,\n>     top_p=0.95,\n>\
          \     repetition_penalty=1.15\n> )\n> \n> print(pipe(prompt_template)[0]['generated_text'])\n\
          > \n> print(\"\\n\\n*** Generate:\")\n> \n> input_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\n> output = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\n> print(tokenizer.decode(output[0]))\n\
          > ```\n\nShould group_size be set as group_size=-1 instead of 128?"
        updatedAt: '2023-06-01T09:28:18.926Z'
      numEdits: 0
      reactions: []
    id: 647864b29c1f42c1f4dab455
    type: comment
  author: chenxiangyi10
  content: "> You can't load GPTQ files directly with transformers AutoModelForCausalLM.\
    \  It's not supported without additional code. \n> \n> I recommend using [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)\n\
    > \n> Here's an example script using AutoGPTQ. Note that first you need to download\
    \ the model locally (eg with `git clone` - AutoGPTQ doesn't yet support directly\
    \ downloading from HF, but it will very soon)\n> \n> ```python\n> from transformers\
    \ import AutoTokenizer, pipeline, logging\n> from auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\n> import argparse\n> \n> quantized_model_dir = \"/path/to/guanaco-65B-GPTQ\"\
    \n> model_basename = \"Guanaco-65B-GPTQ-4bit.act-order.\"\n> \n> use_triton =\
    \ False\n> \n> tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n> \n> quantize_config = BaseQuantizeConfig(\n>         bits=4,\n\
    >         group_size=128,\n>         desc_act=False\n>     )\n> \n> model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    >         use_safetensors=True,\n>         model_basename=model_basename,\n> \
    \        device=\"cuda:0\",\n>         use_triton=use_triton,\n>         quantize_config=quantize_config)\n\
    > \n> # Prevent printing spurious transformers error when using pipeline with\
    \ AutoGPTQ\n> logging.set_verbosity(logging.CRITICAL)\n> \n> prompt = \"Tell me\
    \ about AI\"\n> prompt_template=f'''### Human: {prompt}\n> ### Assistant:'''\n\
    > \n> print(\"*** Pipeline:\")\n> pipe = pipeline(\n>     \"text-generation\"\
    ,\n>     model=model,\n>     tokenizer=tokenizer,\n>     max_new_tokens=512,\n\
    >     temperature=0.7,\n>     top_p=0.95,\n>     repetition_penalty=1.15\n> )\n\
    > \n> print(pipe(prompt_template)[0]['generated_text'])\n> \n> print(\"\\n\\n***\
    \ Generate:\")\n> \n> input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    > output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    > print(tokenizer.decode(output[0]))\n> ```\n\nShould group_size be set as group_size=-1\
    \ instead of 128?"
  created_at: 2023-06-01 08:28:18+00:00
  edited: false
  hidden: false
  id: 647864b29c1f42c1f4dab455
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-01T09:38:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>err yes, sorry!</p>\n<p>In fact, you don't even need to specify\
          \ quantize_config because it's provided with the model:</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, pipeline, logging\n\
          <span class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\nquantized_model_dir\
          \ = <span class=\"hljs-string\">\"/path/to/guanaco-65B-GPTQ\"</span>\nmodel_basename\
          \ = <span class=\"hljs-string\">\"Guanaco-65B-GPTQ-4bit.act-order.\"</span>\n\
          \nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer =\
          \ AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=<span\
          \ class=\"hljs-literal\">None</span>)\n\n<span class=\"hljs-comment\">#\
          \ Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n"
        raw: "err yes, sorry!\n\nIn fact, you don't even need to specify quantize_config\
          \ because it's provided with the model:\n```python\nfrom transformers import\
          \ AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n\nquantized_model_dir = \"/path/to/guanaco-65B-GPTQ\"\
          \nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order.\"\n\nuse_triton =\
          \ False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          \n# Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me\
          \ about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```"
        updatedAt: '2023-06-01T09:38:14.443Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Ichsan2895
        - chenxiangyi10
    id: 64786706159a889d0020d4a6
    type: comment
  author: TheBloke
  content: "err yes, sorry!\n\nIn fact, you don't even need to specify quantize_config\
    \ because it's provided with the model:\n```python\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    \nquantized_model_dir = \"/path/to/guanaco-65B-GPTQ\"\nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order.\"\
    \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        model_basename=model_basename,\n     \
    \   device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
    \n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n\
    logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
    \ Human: {prompt}\n### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```"
  created_at: 2023-06-01 08:38:14+00:00
  edited: false
  hidden: false
  id: 64786706159a889d0020d4a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
      fullname: jimsha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawei3000
      type: user
    createdAt: '2023-06-01T18:33:06.000Z'
    data:
      edited: false
      editors:
      - shawei3000
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
          fullname: jimsha
          isHf: false
          isPro: false
          name: shawei3000
          type: user
        html: '<p>thanks!, Bloke, this works! </p>

          <h2 id="i-can-fine-tune-65b-lora-on-your-gptq-but-could-not-merge-end-lora-with-base-gptq-model-in-such-case-do-you-have-inference-code-that-join-foundation-model-and-lora-1-by-1-and-make-prediction-i-have-following-code-using-two-48gb-gpus-but-seems-out-of-memory-error-is-it-possible-to-put-lora-on-different-gpu">I
          can fine tune 65B lora on your GPTQ, but could not merge end lora with base
          GPTQ model... in such case, do you have inference code that join foundation
          model and lora 1 by 1, and make prediction? I have following code (using
          two 48GB GPUs), but seems out of memory error, is it possible to put lora
          on different GPU?</h2>

          <p>model, tokenizer  = load_llama_model_4bit_low_ram_and_offload(''/media/jimsha/E/gpt4-alpaca-lora_mlp-65B-GPTQ'',<br>                                                  ''/media/jimsha/E/gpt4-alpaca-lora_mlp-65B-GPTQ/gpt4-alpaca-lora_mlp-65B-GPTQ-4bit.safetensors'',<br>                                                  #device_map=''auto'',<br>                                                  groupsize=-1,<br>                                                  is_v1_model=False,<br>                                                  max_memory
          = {0: ''42Gib'', 1:''42Gib'' ,''cpu'':''70Gib''})</p>

          <p>model = PeftModel.from_pretrained(model, ''/media/jimsha/E/ML_Tests/Jim/LLMZoo/alpaca_lora_4bit-main/alpaca_lora/'',<br>                                  #device_map=''auto'',<br>                                  max_memory
          = {0: ''42Gib'', 1:''42Gib'' ,''cpu'':''70Gib''},<br>                                  torch_dtype=torch.float32,<br>                                  is_trainable=True)<br>......<br>with
          torch.no_grad():<br>        generation_output = model.generate(<br>            input_ids=input_ids,<br>            generation_config=generation_config,<br>            return_dict_in_generate=True,<br>            output_scores=True,<br>            max_new_tokens=max_new_tokens,<br>        )</p>

          <hr>

          <p>ERROR:<br> "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 47.50
          GiB total capacity; 44.36 GiB already allocated; 31.12 MiB free; 45.71 GiB
          reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory
          try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF"</p>

          '
        raw: "thanks!, Bloke, this works! \n\nI can fine tune 65B lora on your GPTQ,\
          \ but could not merge end lora with base GPTQ model... in such case, do\
          \ you have inference code that join foundation model and lora 1 by 1, and\
          \ make prediction? I have following code (using two 48GB GPUs), but seems\
          \ out of memory error, is it possible to put lora on different GPU?\n----------------------------------------------------------------------------------------------------------------------\n\
          model, tokenizer  = load_llama_model_4bit_low_ram_and_offload('/media/jimsha/E/gpt4-alpaca-lora_mlp-65B-GPTQ',\n\
          \                                                  '/media/jimsha/E/gpt4-alpaca-lora_mlp-65B-GPTQ/gpt4-alpaca-lora_mlp-65B-GPTQ-4bit.safetensors',\n\
          \                                                  #device_map='auto',\n\
          \                                                  groupsize=-1,\n     \
          \                                             is_v1_model=False,\n     \
          \                                             max_memory = {0: '42Gib',\
          \ 1:'42Gib' ,'cpu':'70Gib'})\n\nmodel = PeftModel.from_pretrained(model,\
          \ '/media/jimsha/E/ML_Tests/Jim/LLMZoo/alpaca_lora_4bit-main/alpaca_lora/',\
          \ \n                                  #device_map='auto', \n           \
          \                       max_memory = {0: '42Gib', 1:'42Gib' ,'cpu':'70Gib'},\n\
          \                                  torch_dtype=torch.float32, \n       \
          \                           is_trainable=True)\n......\nwith torch.no_grad():\n\
          \        generation_output = model.generate(\n            input_ids=input_ids,\n\
          \            generation_config=generation_config,\n            return_dict_in_generate=True,\n\
          \            output_scores=True,\n            max_new_tokens=max_new_tokens,\n\
          \        )\n--------------------------------------------------------------------------------------------------------------------\n\
          ERROR:\n \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 47.50\
          \ GiB total capacity; 44.36 GiB already allocated; 31.12 MiB free; 45.71\
          \ GiB reserved in total by PyTorch) If reserved memory is >> allocated memory\
          \ try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\""
        updatedAt: '2023-06-01T18:33:06.243Z'
      numEdits: 0
      reactions: []
    id: 6478e462c68a021fbba257d0
    type: comment
  author: shawei3000
  content: "thanks!, Bloke, this works! \n\nI can fine tune 65B lora on your GPTQ,\
    \ but could not merge end lora with base GPTQ model... in such case, do you have\
    \ inference code that join foundation model and lora 1 by 1, and make prediction?\
    \ I have following code (using two 48GB GPUs), but seems out of memory error,\
    \ is it possible to put lora on different GPU?\n----------------------------------------------------------------------------------------------------------------------\n\
    model, tokenizer  = load_llama_model_4bit_low_ram_and_offload('/media/jimsha/E/gpt4-alpaca-lora_mlp-65B-GPTQ',\n\
    \                                                  '/media/jimsha/E/gpt4-alpaca-lora_mlp-65B-GPTQ/gpt4-alpaca-lora_mlp-65B-GPTQ-4bit.safetensors',\n\
    \                                                  #device_map='auto',\n     \
    \                                             groupsize=-1,\n                \
    \                                  is_v1_model=False,\n                      \
    \                            max_memory = {0: '42Gib', 1:'42Gib' ,'cpu':'70Gib'})\n\
    \nmodel = PeftModel.from_pretrained(model, '/media/jimsha/E/ML_Tests/Jim/LLMZoo/alpaca_lora_4bit-main/alpaca_lora/',\
    \ \n                                  #device_map='auto', \n                 \
    \                 max_memory = {0: '42Gib', 1:'42Gib' ,'cpu':'70Gib'},\n     \
    \                             torch_dtype=torch.float32, \n                  \
    \                is_trainable=True)\n......\nwith torch.no_grad():\n        generation_output\
    \ = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n\
    \            return_dict_in_generate=True,\n            output_scores=True,\n\
    \            max_new_tokens=max_new_tokens,\n        )\n--------------------------------------------------------------------------------------------------------------------\n\
    ERROR:\n \"CUDA out of memory. Tried to allocate 44.00 MiB (GPU 0; 47.50 GiB total\
    \ capacity; 44.36 GiB already allocated; 31.12 MiB free; 45.71 GiB reserved in\
    \ total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\""
  created_at: 2023-06-01 17:33:06+00:00
  edited: false
  hidden: false
  id: 6478e462c68a021fbba257d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:24:53.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.13248005509376526
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I always do a hard merge of the LoRA and base model, saving to a\
          \ new model.  I use this code for that:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM, AutoTokenizer\n<span class=\"hljs-keyword\"\
          >from</span> peft <span class=\"hljs-keyword\">import</span> PeftModel\n\
          <span class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-keyword\"\
          >import</span> os\n<span class=\"hljs-keyword\">import</span> argparse\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >get_args</span>():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(<span\
          \ class=\"hljs-string\">\"--base_model_name_or_path\"</span>, <span class=\"\
          hljs-built_in\">type</span>=<span class=\"hljs-built_in\">str</span>)\n\
          \    parser.add_argument(<span class=\"hljs-string\">\"--peft_model_path\"\
          </span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\"\
          >str</span>)\n    parser.add_argument(<span class=\"hljs-string\">\"--output_dir\"\
          </span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\"\
          >str</span>)\n    parser.add_argument(<span class=\"hljs-string\">\"--device\"\
          </span>, <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\"\
          >str</span>, default=<span class=\"hljs-string\">\"auto\"</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">\"--push_to_hub\"</span>, action=<span class=\"\
          hljs-string\">\"store_true\"</span>)\n    parser.add_argument(<span class=\"\
          hljs-string\">\"--trust_remote_code\"</span>, action=<span class=\"hljs-string\"\
          >\"store_true\"</span>)\n\n    <span class=\"hljs-keyword\">return</span>\
          \ parser.parse_args()\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">main</span>():\n    args = get_args()\n\n    <span\
          \ class=\"hljs-keyword\">if</span> args.device == <span class=\"hljs-string\"\
          >'auto'</span>:\n        device_arg = { <span class=\"hljs-string\">'device_map'</span>:\
          \ <span class=\"hljs-string\">'auto'</span> }\n    <span class=\"hljs-keyword\"\
          >else</span>:\n        device_arg = { <span class=\"hljs-string\">'device_map'</span>:\
          \ { <span class=\"hljs-string\">\"\"</span>: args.device} }\n\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Loading\
          \ base model: <span class=\"hljs-subst\">{args.base_model_name_or_path}</span>\"\
          </span>)\n    base_model = AutoModelForCausalLM.from_pretrained(\n     \
          \   args.base_model_name_or_path,\n        return_dict=<span class=\"hljs-literal\"\
          >True</span>,\n        torch_dtype=torch.float16,\n        trust_remote_code=args.trust_remote_code,\n\
          \        **device_arg\n    )\n\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Loading PEFT: <span class=\"hljs-subst\">{args.peft_model_path}</span>\"\
          </span>)\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path,\
          \ **device_arg)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Running merge_and_unload\"</span>)\n    model = model.merge_and_unload()\n\
          \n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
          \n    <span class=\"hljs-keyword\">if</span> args.push_to_hub:\n       \
          \ <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Saving to hub ...\"</span>)\n        model.push_to_hub(<span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"</span>,\
          \ use_temp_dir=<span class=\"hljs-literal\">False</span>)\n        tokenizer.push_to_hub(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"\
          </span>, use_temp_dir=<span class=\"hljs-literal\">False</span>)\n    <span\
          \ class=\"hljs-keyword\">else</span>:\n        model.save_pretrained(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"\
          </span>)\n        tokenizer.save_pretrained(<span class=\"hljs-string\"\
          >f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"</span>)\n    \
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Model saved to <span class=\"hljs-subst\">{args.output_dir}</span>\"\
          </span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"\
          hljs-string\">\"__main__\"</span> :\n    main()\n</code></pre>\n"
        raw: "I always do a hard merge of the LoRA and base model, saving to a new\
          \ model.  I use this code for that:\n```python\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport\
          \ torch\n\nimport os\nimport argparse\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n\
          \    parser.add_argument(\"--base_model_name_or_path\", type=str)\n    parser.add_argument(\"\
          --peft_model_path\", type=str)\n    parser.add_argument(\"--output_dir\"\
          , type=str)\n    parser.add_argument(\"--device\", type=str, default=\"\
          auto\")\n    parser.add_argument(\"--push_to_hub\", action=\"store_true\"\
          )\n    parser.add_argument(\"--trust_remote_code\", action=\"store_true\"\
          )\n\n    return parser.parse_args()\n\ndef main():\n    args = get_args()\n\
          \n    if args.device == 'auto':\n        device_arg = { 'device_map': 'auto'\
          \ }\n    else:\n        device_arg = { 'device_map': { \"\": args.device}\
          \ }\n\n    print(f\"Loading base model: {args.base_model_name_or_path}\"\
          )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.base_model_name_or_path,\n\
          \        return_dict=True,\n        torch_dtype=torch.float16,\n       \
          \ trust_remote_code=args.trust_remote_code,\n        **device_arg\n    )\n\
          \n    print(f\"Loading PEFT: {args.peft_model_path}\")\n    model = PeftModel.from_pretrained(base_model,\
          \ args.peft_model_path, **device_arg)\n    print(f\"Running merge_and_unload\"\
          )\n    model = model.merge_and_unload()\n\n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
          \n    if args.push_to_hub:\n        print(f\"Saving to hub ...\")\n    \
          \    model.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n   \
          \     tokenizer.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n\
          \    else:\n        model.save_pretrained(f\"{args.output_dir}\")\n    \
          \    tokenizer.save_pretrained(f\"{args.output_dir}\")\n        print(f\"\
          Model saved to {args.output_dir}\")\n\nif __name__ == \"__main__\" :\n \
          \   main()\n\n```"
        updatedAt: '2023-06-05T10:24:53.077Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - shawei3000
    id: 647db7f510b7a3b157fe33d7
    type: comment
  author: TheBloke
  content: "I always do a hard merge of the LoRA and base model, saving to a new model.\
    \  I use this code for that:\n```python\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\nfrom peft import PeftModel\nimport torch\n\nimport os\nimport\
    \ argparse\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"\
    --base_model_name_or_path\", type=str)\n    parser.add_argument(\"--peft_model_path\"\
    , type=str)\n    parser.add_argument(\"--output_dir\", type=str)\n    parser.add_argument(\"\
    --device\", type=str, default=\"auto\")\n    parser.add_argument(\"--push_to_hub\"\
    , action=\"store_true\")\n    parser.add_argument(\"--trust_remote_code\", action=\"\
    store_true\")\n\n    return parser.parse_args()\n\ndef main():\n    args = get_args()\n\
    \n    if args.device == 'auto':\n        device_arg = { 'device_map': 'auto' }\n\
    \    else:\n        device_arg = { 'device_map': { \"\": args.device} }\n\n  \
    \  print(f\"Loading base model: {args.base_model_name_or_path}\")\n    base_model\
    \ = AutoModelForCausalLM.from_pretrained(\n        args.base_model_name_or_path,\n\
    \        return_dict=True,\n        torch_dtype=torch.float16,\n        trust_remote_code=args.trust_remote_code,\n\
    \        **device_arg\n    )\n\n    print(f\"Loading PEFT: {args.peft_model_path}\"\
    )\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path, **device_arg)\n\
    \    print(f\"Running merge_and_unload\")\n    model = model.merge_and_unload()\n\
    \n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
    \n    if args.push_to_hub:\n        print(f\"Saving to hub ...\")\n        model.push_to_hub(f\"\
    {args.output_dir}\", use_temp_dir=False)\n        tokenizer.push_to_hub(f\"{args.output_dir}\"\
    , use_temp_dir=False)\n    else:\n        model.save_pretrained(f\"{args.output_dir}\"\
    )\n        tokenizer.save_pretrained(f\"{args.output_dir}\")\n        print(f\"\
    Model saved to {args.output_dir}\")\n\nif __name__ == \"__main__\" :\n    main()\n\
    \n```"
  created_at: 2023-06-05 09:24:53+00:00
  edited: false
  hidden: false
  id: 647db7f510b7a3b157fe33d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23ae3018512739242870d785f35fdc27.svg
      fullname: Amit Jain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: amitj
      type: user
    createdAt: '2023-06-05T16:54:22.000Z'
    data:
      edited: false
      editors:
      - amitj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5195735096931458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23ae3018512739242870d785f35fdc27.svg
          fullname: Amit Jain
          isHf: false
          isPro: false
          name: amitj
          type: user
        html: '<p>I get a message without any error, autogptq is from the latest<br><code>skip
          module injection for FusedLlamaMLPForQuantizedModel not support integrate
          without triton yet.</code></p>

          '
        raw: 'I get a message without any error, autogptq is from the latest

          `skip module injection for FusedLlamaMLPForQuantizedModel not support integrate
          without triton yet.`'
        updatedAt: '2023-06-05T16:54:22.022Z'
      numEdits: 0
      reactions: []
    id: 647e133e32c471a7fa90e591
    type: comment
  author: amitj
  content: 'I get a message without any error, autogptq is from the latest

    `skip module injection for FusedLlamaMLPForQuantizedModel not support integrate
    without triton yet.`'
  created_at: 2023-06-05 15:54:22+00:00
  edited: false
  hidden: false
  id: 647e133e32c471a7fa90e591
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T16:55:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.77330482006073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>I get a message without any error, autogptq is from the latest<br><code>skip
          module injection for FusedLlamaMLPForQuantizedModel not support integrate
          without triton yet.</code></p>

          </blockquote>

          <p>That''s fine, it''s just an informational message.  Completely normal.
          Please ignore it.  In future releases of AutoGPTQ the message will likely
          be removed</p>

          '
        raw: '> I get a message without any error, autogptq is from the latest

          > `skip module injection for FusedLlamaMLPForQuantizedModel not support
          integrate without triton yet.`


          That''s fine, it''s just an informational message.  Completely normal. Please
          ignore it.  In future releases of AutoGPTQ the message will likely be removed'
        updatedAt: '2023-06-05T16:55:24.497Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - amitj
    id: 647e137c5214d172cbbeecfc
    type: comment
  author: TheBloke
  content: '> I get a message without any error, autogptq is from the latest

    > `skip module injection for FusedLlamaMLPForQuantizedModel not support integrate
    without triton yet.`


    That''s fine, it''s just an informational message.  Completely normal. Please
    ignore it.  In future releases of AutoGPTQ the message will likely be removed'
  created_at: 2023-06-05 15:55:24+00:00
  edited: false
  hidden: false
  id: 647e137c5214d172cbbeecfc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
      fullname: Paul CHEN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenxiangyi10
      type: user
    createdAt: '2023-06-05T21:00:19.000Z'
    data:
      edited: false
      editors:
      - chenxiangyi10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39107444882392883
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e021978dc5c38289766f4ca535a24ee5.svg
          fullname: Paul CHEN
          isHf: false
          isPro: false
          name: chenxiangyi10
          type: user
        html: "<blockquote>\n<p>err yes, sorry!</p>\n<p>In fact, you don't even need\
          \ to specify quantize_config because it's provided with the model:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          \nquantized_model_dir = <span class=\"hljs-string\">\"/path/to/guanaco-65B-GPTQ\"\
          </span>\nmodel_basename = <span class=\"hljs-string\">\"Guanaco-65B-GPTQ-4bit.act-order.\"\
          </span>\n\nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=model_basename,\n        device=<span class=\"hljs-string\"\
          >\"cuda:0\"</span>,\n        use_triton=use_triton,\n        quantize_config=<span\
          \ class=\"hljs-literal\">None</span>)\n\n<span class=\"hljs-comment\">#\
          \ Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n</blockquote>\n<p>Thank\
          \ you for this information.</p>\n"
        raw: "> err yes, sorry!\n> \n> In fact, you don't even need to specify quantize_config\
          \ because it's provided with the model:\n> ```python\n> from transformers\
          \ import AutoTokenizer, pipeline, logging\n> from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n> \n> quantized_model_dir = \"/path/to/guanaco-65B-GPTQ\"\
          \n> model_basename = \"Guanaco-65B-GPTQ-4bit.act-order.\"\n> \n> use_triton\
          \ = False\n> \n> tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n> \n> model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          >         use_safetensors=True,\n>         model_basename=model_basename,\n\
          >         device=\"cuda:0\",\n>         use_triton=use_triton,\n>      \
          \   quantize_config=None)\n> \n> # Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ\n> logging.set_verbosity(logging.CRITICAL)\n\
          > \n> prompt = \"Tell me about AI\"\n> prompt_template=f'''### Human: {prompt}\n\
          > ### Assistant:'''\n> \n> print(\"*** Pipeline:\")\n> pipe = pipeline(\n\
          >     \"text-generation\",\n>     model=model,\n>     tokenizer=tokenizer,\n\
          >     max_new_tokens=512,\n>     temperature=0.7,\n>     top_p=0.95,\n>\
          \     repetition_penalty=1.15\n> )\n> \n> print(pipe(prompt_template)[0]['generated_text'])\n\
          > \n> print(\"\\n\\n*** Generate:\")\n> \n> input_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\n> output = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\n> print(tokenizer.decode(output[0]))\n\
          > ```\n\nThank you for this information."
        updatedAt: '2023-06-05T21:00:19.546Z'
      numEdits: 0
      reactions: []
    id: 647e4ce3f14eafc3b45df069
    type: comment
  author: chenxiangyi10
  content: "> err yes, sorry!\n> \n> In fact, you don't even need to specify quantize_config\
    \ because it's provided with the model:\n> ```python\n> from transformers import\
    \ AutoTokenizer, pipeline, logging\n> from auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\n> \n> quantized_model_dir = \"/path/to/guanaco-65B-GPTQ\"\
    \n> model_basename = \"Guanaco-65B-GPTQ-4bit.act-order.\"\n> \n> use_triton =\
    \ False\n> \n> tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n> \n> model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    >         use_safetensors=True,\n>         model_basename=model_basename,\n> \
    \        device=\"cuda:0\",\n>         use_triton=use_triton,\n>         quantize_config=None)\n\
    > \n> # Prevent printing spurious transformers error when using pipeline with\
    \ AutoGPTQ\n> logging.set_verbosity(logging.CRITICAL)\n> \n> prompt = \"Tell me\
    \ about AI\"\n> prompt_template=f'''### Human: {prompt}\n> ### Assistant:'''\n\
    > \n> print(\"*** Pipeline:\")\n> pipe = pipeline(\n>     \"text-generation\"\
    ,\n>     model=model,\n>     tokenizer=tokenizer,\n>     max_new_tokens=512,\n\
    >     temperature=0.7,\n>     top_p=0.95,\n>     repetition_penalty=1.15\n> )\n\
    > \n> print(pipe(prompt_template)[0]['generated_text'])\n> \n> print(\"\\n\\n***\
    \ Generate:\")\n> \n> input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    > output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    > print(tokenizer.decode(output[0]))\n> ```\n\nThank you for this information."
  created_at: 2023-06-05 20:00:19+00:00
  edited: false
  hidden: false
  id: 647e4ce3f14eafc3b45df069
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
      fullname: jimsha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shawei3000
      type: user
    createdAt: '2023-06-07T03:23:43.000Z'
    data:
      edited: true
      editors:
      - shawei3000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9287175536155701
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f80766d347ce66690760db514f9d6b80.svg
          fullname: jimsha
          isHf: false
          isPro: false
          name: shawei3000
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> : regarding \"\
          I always do a hard merge of the LoRA and base model, saving to a new model.\
          \ I use this code for that:...\"<br>I acturally trained Lora using the 65B\
          \ GPQT 4bit base model, is there a way merge Lora and GPTQ 4bit base model\
          \ that is used during the fine-tune?<br>or,<br>maybe you are suggesting\
          \ merging Lora (trained on 65B 4bit base) with 65B HF base model would be\
          \ just fine?</p>\n"
        raw: '@TheBloke : regarding "I always do a hard merge of the LoRA and base
          model, saving to a new model. I use this code for that:..."

          I acturally trained Lora using the 65B GPQT 4bit base model, is there a
          way merge Lora and GPTQ 4bit base model that is used during the fine-tune?

          or,

          maybe you are suggesting merging Lora (trained on 65B 4bit base) with 65B
          HF base model would be just fine?'
        updatedAt: '2023-06-07T04:13:37.381Z'
      numEdits: 1
      reactions: []
    id: 647ff83f1637c1c0e6f6048c
    type: comment
  author: shawei3000
  content: '@TheBloke : regarding "I always do a hard merge of the LoRA and base model,
    saving to a new model. I use this code for that:..."

    I acturally trained Lora using the 65B GPQT 4bit base model, is there a way merge
    Lora and GPTQ 4bit base model that is used during the fine-tune?

    or,

    maybe you are suggesting merging Lora (trained on 65B 4bit base) with 65B HF base
    model would be just fine?'
  created_at: 2023-06-07 02:23:43+00:00
  edited: true
  hidden: false
  id: 647ff83f1637c1c0e6f6048c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-07T20:37:13.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9568449854850769
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve never looked at 4bit LoRA at all so I''m not immediately sure.
          It should be possible, but the script I provided wouldn''t work.     </p>

          <p>AutoGPTQ is adding PEFT support now (it''s already merged in the latest
          dev version). That might provide a way.</p>

          <p>Otherwise I''m not sure. Maybe ask on the Alpaca Lora 4bit repo?</p>

          '
        raw: "I've never looked at 4bit LoRA at all so I'm not immediately sure. It\
          \ should be possible, but the script I provided wouldn't work.     \n\n\
          AutoGPTQ is adding PEFT support now (it's already merged in the latest dev\
          \ version). That might provide a way.\n\nOtherwise I'm not sure. Maybe ask\
          \ on the Alpaca Lora 4bit repo?"
        updatedAt: '2023-06-07T20:37:13.163Z'
      numEdits: 0
      reactions: []
    id: 6480ea79bb25a636c9e23f7c
    type: comment
  author: TheBloke
  content: "I've never looked at 4bit LoRA at all so I'm not immediately sure. It\
    \ should be possible, but the script I provided wouldn't work.     \n\nAutoGPTQ\
    \ is adding PEFT support now (it's already merged in the latest dev version).\
    \ That might provide a way.\n\nOtherwise I'm not sure. Maybe ask on the Alpaca\
    \ Lora 4bit repo?"
  created_at: 2023-06-07 19:37:13+00:00
  edited: false
  hidden: false
  id: 6480ea79bb25a636c9e23f7c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: can you load the provided safetensors with AutoModelForCausalLM.from_pretrained?
