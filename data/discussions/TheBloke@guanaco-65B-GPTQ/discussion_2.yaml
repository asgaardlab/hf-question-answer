!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-05-26 01:04:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-26T02:04:35.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>HI,</p>

          <p>it seems <a href="https://huggingface.co/timdettmers/guanaco-65b">https://huggingface.co/timdettmers/guanaco-65b</a>
          is a lora weight, how did you apply this lora weight on a 75B model? Is
          there any repo/script that I can refer to?</p>

          <p>Thanks!</p>

          '
        raw: "HI,\r\n\r\nit seems https://huggingface.co/timdettmers/guanaco-65b is\
          \ a lora weight, how did you apply this lora weight on a 75B model? Is there\
          \ any repo/script that I can refer to?\r\n\r\nThanks!"
        updatedAt: '2023-05-26T02:04:35.365Z'
      numEdits: 0
      reactions: []
    id: 647013b3850a938d6c5655f1
    type: comment
  author: Yhyu13
  content: "HI,\r\n\r\nit seems https://huggingface.co/timdettmers/guanaco-65b is\
    \ a lora weight, how did you apply this lora weight on a 75B model? Is there any\
    \ repo/script that I can refer to?\r\n\r\nThanks!"
  created_at: 2023-05-26 01:04:35+00:00
  edited: false
  hidden: false
  id: 647013b3850a938d6c5655f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72660963812ee19b654fb3111cb7e5ad.svg
      fullname: Nicholai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmitchko
      type: user
    createdAt: '2023-05-26T03:22:41.000Z'
    data:
      edited: true
      editors:
      - nmitchko
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72660963812ee19b654fb3111cb7e5ad.svg
          fullname: Nicholai
          isHf: false
          isPro: false
          name: nmitchko
          type: user
        html: "<p>To apply a lora to a model, load the model and then load the lora\
          \ on top using perft model.</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-comment\"># Some llama or alpaca model 65b</span>\nbase_model\
          \ = <span class=\"hljs-string\">\"decapoda-research/llama-65b-hf\"</span>\n\
          model = LlamaForCausalLM.from_pretrained(\n    base_model,    \n    load_in_8bit=load_8bit,\n\
          \    torch_dtype=torch.float16\n)\n<span class=\"hljs-comment\"># Load the\
          \ LORA on top</span>\nlora_weights = <span class=\"hljs-string\">\"timdettmers/guanaco-65b\"\
          </span>\nmodel = PeftModel.from_pretrained(\n    model,\n    lora_weights,\n\
          \    torch_dtype=torch.float16\n)\n</code></pre>\n<p>Then to save the lora\
          \ applied model:</p>\n<pre><code class=\"language-python\">out_folder =\
          \ args.output <span class=\"hljs-keyword\">or</span> Path(<span class=\"\
          hljs-string\">f\"models/somename\"</span>)\nmodel.save_pretrained(out_folder,\
          \ max_shard_size=<span class=\"hljs-string\">\"2GB\"</span>, safe_serialization=<span\
          \ class=\"hljs-literal\">True</span>)\n<span class=\"hljs-comment\"># If\
          \ you have to save the tokenizer too ...</span>\ntokenizer.save_pretrained(out_folder)\n\
          </code></pre>\n"
        raw: "To apply a lora to a model, load the model and then load the lora on\
          \ top using perft model.\n```python\n# Some llama or alpaca model 65b\n\
          base_model = \"decapoda-research/llama-65b-hf\"\nmodel = LlamaForCausalLM.from_pretrained(\n\
          \    base_model,    \n    load_in_8bit=load_8bit,\n    torch_dtype=torch.float16\n\
          )\n# Load the LORA on top\nlora_weights = \"timdettmers/guanaco-65b\"\n\
          model = PeftModel.from_pretrained(\n    model,\n    lora_weights,\n    torch_dtype=torch.float16\n\
          )\n```\n\nThen to save the lora applied model:\n\n```python\nout_folder\
          \ = args.output or Path(f\"models/somename\")\nmodel.save_pretrained(out_folder,\
          \ max_shard_size=\"2GB\", safe_serialization=True)\n# If you have to save\
          \ the tokenizer too ...\ntokenizer.save_pretrained(out_folder)\n```"
        updatedAt: '2023-05-26T03:23:28.098Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
        - cymusix
    id: 64702601bc42f4b0023961a7
    type: comment
  author: nmitchko
  content: "To apply a lora to a model, load the model and then load the lora on top\
    \ using perft model.\n```python\n# Some llama or alpaca model 65b\nbase_model\
    \ = \"decapoda-research/llama-65b-hf\"\nmodel = LlamaForCausalLM.from_pretrained(\n\
    \    base_model,    \n    load_in_8bit=load_8bit,\n    torch_dtype=torch.float16\n\
    )\n# Load the LORA on top\nlora_weights = \"timdettmers/guanaco-65b\"\nmodel =\
    \ PeftModel.from_pretrained(\n    model,\n    lora_weights,\n    torch_dtype=torch.float16\n\
    )\n```\n\nThen to save the lora applied model:\n\n```python\nout_folder = args.output\
    \ or Path(f\"models/somename\")\nmodel.save_pretrained(out_folder, max_shard_size=\"\
    2GB\", safe_serialization=True)\n# If you have to save the tokenizer too ...\n\
    tokenizer.save_pretrained(out_folder)\n```"
  created_at: 2023-05-26 02:22:41+00:00
  edited: true
  hidden: false
  id: 64702601bc42f4b0023961a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-26T05:07:34.000Z'
    data:
      status: closed
    id: 64703e96bc42f4b0023aa582
    type: status-change
  author: Yhyu13
  created_at: 2023-05-26 04:07:34+00:00
  id: 64703e96bc42f4b0023aa582
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-26T07:41:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Here's the script I used specifically. Requires peft version 0.3\
          \ installed</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n<span class=\"hljs-keyword\">from</span> peft <span class=\"\
          hljs-keyword\">import</span> PeftModel\n<span class=\"hljs-keyword\">import</span>\
          \ torch\n\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"\
          hljs-keyword\">import</span> argparse\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">get_args</span>():\n    parser =\
          \ argparse.ArgumentParser()\n    parser.add_argument(<span class=\"hljs-string\"\
          >\"--base_model_name_or_path\"</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">str</span>)\n    parser.add_argument(<span class=\"\
          hljs-string\">\"--peft_model_path\"</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">\"--output_dir\"</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">\"--device\"</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, default=<span class=\"\
          hljs-string\">\"auto\"</span>)\n    parser.add_argument(<span class=\"hljs-string\"\
          >\"--push_to_hub\"</span>, action=<span class=\"hljs-string\">\"store_true\"\
          </span>)\n\n    <span class=\"hljs-keyword\">return</span> parser.parse_args()\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >main</span>():\n    args = get_args()\n\n    <span class=\"hljs-keyword\"\
          >if</span> args.device == <span class=\"hljs-string\">'auto'</span>:\n \
          \       device_arg = { <span class=\"hljs-string\">'device_map'</span>:\
          \ <span class=\"hljs-string\">'auto'</span> }\n    <span class=\"hljs-keyword\"\
          >else</span>:\n        device_arg = { <span class=\"hljs-string\">'device_map'</span>:\
          \ { <span class=\"hljs-string\">\"\"</span>: args.device} }\n\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Loading\
          \ base model: <span class=\"hljs-subst\">{args.base_model_name_or_path}</span>\"\
          </span>)\n    base_model = AutoModelForCausalLM.from_pretrained(\n     \
          \   args.base_model_name_or_path,\n        return_dict=<span class=\"hljs-literal\"\
          >True</span>,\n        torch_dtype=torch.float16,\n        **device_arg\n\
          \    )\n\n    <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Loading PEFT: <span class=\"hljs-subst\">{args.peft_model_path}</span>\"\
          </span>)\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path,\
          \ **device_arg)\n    <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">f\"Running merge_and_unload\"</span>)\n    model = model.merge_and_unload()\n\
          \n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
          \n    <span class=\"hljs-keyword\">if</span> args.push_to_hub:\n       \
          \ <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Saving to hub ...\"</span>)\n        model.push_to_hub(<span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"</span>,\
          \ use_temp_dir=<span class=\"hljs-literal\">False</span>)\n        tokenizer.push_to_hub(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"\
          </span>, use_temp_dir=<span class=\"hljs-literal\">False</span>)\n    <span\
          \ class=\"hljs-keyword\">else</span>:\n        model.save_pretrained(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"\
          </span>)\n        tokenizer.save_pretrained(<span class=\"hljs-string\"\
          >f\"<span class=\"hljs-subst\">{args.output_dir}</span>\"</span>)\n    \
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Model saved to <span class=\"hljs-subst\">{args.output_dir}</span>\"\
          </span>)\n\n<span class=\"hljs-keyword\">if</span> __name__ == <span class=\"\
          hljs-string\">\"__main__\"</span> :\n    main()\n</code></pre>\n"
        raw: "Here's the script I used specifically. Requires peft version 0.3 installed\n\
          \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          from peft import PeftModel\nimport torch\n\nimport os\nimport argparse\n\
          \ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"\
          --base_model_name_or_path\", type=str)\n    parser.add_argument(\"--peft_model_path\"\
          , type=str)\n    parser.add_argument(\"--output_dir\", type=str)\n    parser.add_argument(\"\
          --device\", type=str, default=\"auto\")\n    parser.add_argument(\"--push_to_hub\"\
          , action=\"store_true\")\n\n    return parser.parse_args()\n\ndef main():\n\
          \    args = get_args()\n\n    if args.device == 'auto':\n        device_arg\
          \ = { 'device_map': 'auto' }\n    else:\n        device_arg = { 'device_map':\
          \ { \"\": args.device} }\n\n    print(f\"Loading base model: {args.base_model_name_or_path}\"\
          )\n    base_model = AutoModelForCausalLM.from_pretrained(\n        args.base_model_name_or_path,\n\
          \        return_dict=True,\n        torch_dtype=torch.float16,\n       \
          \ **device_arg\n    )\n\n    print(f\"Loading PEFT: {args.peft_model_path}\"\
          )\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path,\
          \ **device_arg)\n    print(f\"Running merge_and_unload\")\n    model = model.merge_and_unload()\n\
          \n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
          \n    if args.push_to_hub:\n        print(f\"Saving to hub ...\")\n    \
          \    model.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n   \
          \     tokenizer.push_to_hub(f\"{args.output_dir}\", use_temp_dir=False)\n\
          \    else:\n        model.save_pretrained(f\"{args.output_dir}\")\n    \
          \    tokenizer.save_pretrained(f\"{args.output_dir}\")\n        print(f\"\
          Model saved to {args.output_dir}\")\n\nif __name__ == \"__main__\" :\n \
          \   main()\n```"
        updatedAt: '2023-05-26T07:41:38.295Z'
      numEdits: 0
      reactions:
      - count: 13
        reaction: "\U0001F44D"
        users:
        - nmitchko
        - i617
        - twang2218
        - jkeisling
        - Darkhand-786
        - phi0112358
        - usholanb
        - dsmonk
        - Johnyquest7
        - iseesaw
        - YogurtYu
        - humza-sami
        - AalianK
      - count: 9
        reaction: "\u2764\uFE0F"
        users:
        - disarmyouwitha
        - nacs
        - dsmonk
        - MatthewK
        - tomdeore
        - ParisNeo
        - hallh
        - humza-sami
        - Brandoko
      - count: 1
        reaction: "\U0001F91D"
        users:
        - humza-sami
      - count: 1
        reaction: "\U0001F917"
        users:
        - humza-sami
    id: 647062b2806c7d87fa11a19c
    type: comment
  author: TheBloke
  content: "Here's the script I used specifically. Requires peft version 0.3 installed\n\
    \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom\
    \ peft import PeftModel\nimport torch\n\nimport os\nimport argparse\n\ndef get_args():\n\
    \    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--base_model_name_or_path\"\
    , type=str)\n    parser.add_argument(\"--peft_model_path\", type=str)\n    parser.add_argument(\"\
    --output_dir\", type=str)\n    parser.add_argument(\"--device\", type=str, default=\"\
    auto\")\n    parser.add_argument(\"--push_to_hub\", action=\"store_true\")\n\n\
    \    return parser.parse_args()\n\ndef main():\n    args = get_args()\n\n    if\
    \ args.device == 'auto':\n        device_arg = { 'device_map': 'auto' }\n    else:\n\
    \        device_arg = { 'device_map': { \"\": args.device} }\n\n    print(f\"\
    Loading base model: {args.base_model_name_or_path}\")\n    base_model = AutoModelForCausalLM.from_pretrained(\n\
    \        args.base_model_name_or_path,\n        return_dict=True,\n        torch_dtype=torch.float16,\n\
    \        **device_arg\n    )\n\n    print(f\"Loading PEFT: {args.peft_model_path}\"\
    )\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path, **device_arg)\n\
    \    print(f\"Running merge_and_unload\")\n    model = model.merge_and_unload()\n\
    \n    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name_or_path)\n\
    \n    if args.push_to_hub:\n        print(f\"Saving to hub ...\")\n        model.push_to_hub(f\"\
    {args.output_dir}\", use_temp_dir=False)\n        tokenizer.push_to_hub(f\"{args.output_dir}\"\
    , use_temp_dir=False)\n    else:\n        model.save_pretrained(f\"{args.output_dir}\"\
    )\n        tokenizer.save_pretrained(f\"{args.output_dir}\")\n        print(f\"\
    Model saved to {args.output_dir}\")\n\nif __name__ == \"__main__\" :\n    main()\n\
    ```"
  created_at: 2023-05-26 06:41:38+00:00
  edited: false
  hidden: false
  id: 647062b2806c7d87fa11a19c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-26T14:41:57.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>YTMND, thank you so much for explaining the process / script!</p>

          '
        raw: YTMND, thank you so much for explaining the process / script!
        updatedAt: '2023-05-26T14:41:57.782Z'
      numEdits: 0
      reactions: []
    id: 6470c5359fe78d69a8b25b13
    type: comment
  author: disarmyouwitha
  content: YTMND, thank you so much for explaining the process / script!
  created_at: 2023-05-26 13:41:57+00:00
  edited: false
  hidden: false
  id: 6470c5359fe78d69a8b25b13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72660963812ee19b654fb3111cb7e5ad.svg
      fullname: Nicholai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmitchko
      type: user
    createdAt: '2023-06-08T00:39:13.000Z'
    data:
      edited: false
      editors:
      - nmitchko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4690279960632324
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72660963812ee19b654fb3111cb7e5ad.svg
          fullname: Nicholai
          isHf: false
          isPro: false
          name: nmitchko
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> One thing I ran\
          \ into trying to merge my LoRAs using your code is that <code>merge_and_unload()</code>\
          \ doesn't work if the base model (mostly safetensors) has no config file\
          \ associated with it. You can change in the above code </p>\n<ul>\n<li><code>model\
          \ = model.merge_and_unload()</code> to <code>model = model.base_model.model</code></li>\n\
          </ul>\n<p>and it should work (if it doesn't you can manually set the Config\
          \ on the AutoModelForCausalLM object <code>model.config = AutoConfig.from_pretrained('some/model')</code>).\
          \ </p>\n<h1 id=\"if-using-text-generation-ui\">If using text-generation-ui</h1>\n\
          <p>If you want to merge a GPTQ lora created in <code>text-generation-ui</code>\
          \ with <code>--monkey-patch</code> you can use the below code.</p>\n<p><strong>merge-lora.py</strong></p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer, AutoConfig\n<span class=\"hljs-keyword\">from</span> peft\
          \ <span class=\"hljs-keyword\">import</span> PeftModel\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">import</span> modules.shared\
          \ <span class=\"hljs-keyword\">as</span> shared\n<span class=\"hljs-keyword\"\
          >import</span> modules.monkey_patch_gptq_lora_export <span class=\"hljs-keyword\"\
          >as</span> monkeypatch\n<span class=\"hljs-keyword\">import</span> os\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\n<span class=\"hljs-comment\"\
          ># your hf access token</span>\naccess_token= <span class=\"hljs-string\"\
          >'hf_deadbeef010101'</span>\n<span class=\"hljs-comment\"># your uploaded\
          \ hf repo</span>\nhf_repo = <span class=\"hljs-string\">'myuser/myrepo-65b-GPTQ'</span>\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >main</span>():\n    device_arg = { <span class=\"hljs-string\">'device_map'</span>:\
          \ <span class=\"hljs-string\">'auto'</span> }\n\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Loading base model: <span class=\"\
          hljs-subst\">{shared.args.model}</span>\"</span>)\n\n    config = AutoConfig.from_pretrained(<span\
          \ class=\"hljs-string\">f'<span class=\"hljs-subst\">{shared.args.model_dir}</span>/<span\
          \ class=\"hljs-subst\">{shared.args.model}</span>/config.json'</span>)\n\
          \    model, tokenizer = monkeypatch.load_model_llama(shared.args.model)\n\
          \    model.config = config\n\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Loading PEFT: <span class=\"hljs-subst\">{shared.args.lora}</span>\"\
          </span>)\n    model = PeftModel.from_pretrained(model, shared.args.lora[<span\
          \ class=\"hljs-number\">0</span>], **device_arg)\n\n    model = model.base_model.model\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Saving to hub ...\"</span>)\n\n    <span class=\"hljs-comment\"># to\
          \ push to HF</span>\n    model.push_to_hub(<span class=\"hljs-string\">f\"\
          <span class=\"hljs-subst\">{hf_repo}</span>\"</span>, use_temp_dir=<span\
          \ class=\"hljs-literal\">True</span>, use_auth_token=access_token)\n   \
          \ tokenizer.push_to_hub(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\"\
          >{hf_repo}</span>\"</span>, use_temp_dir=<span class=\"hljs-literal\">True</span>,\
          \ use_auth_token=access_token)\n\n    <span class=\"hljs-comment\"># Or\
          \ save locally</span>\n    <span class=\"hljs-comment\"># model.save_pretrained(f\"\
          my/output/dir\")</span>\n    <span class=\"hljs-comment\"># tokenizer.save_pretrained(f\"\
          my/output/dir\")</span>\n\n<span class=\"hljs-keyword\">if</span> __name__\
          \ == <span class=\"hljs-string\">\"__main__\"</span> :\n    main()\n</code></pre>\n\
          <p>Place <code>merge-lora.py</code> in the root of text-generation-ui and\
          \ then run the below commands (adjusting your cuda visible devices).</p>\n\
          <pre><code class=\"language-bash\">CURRENTDATEONLY=`<span class=\"hljs-built_in\"\
          >date</span> +<span class=\"hljs-string\">\"%b %d %Y\"</span>`\n\n<span\
          \ class=\"hljs-built_in\">export</span> CUDA_VISIBLE_DEVICES=0\n\npython\
          \ merge-lora.py \\\n   --model <span class=\"hljs-string\">'TheBloke_guanaco-65B-GPTQ'</span>\
          \ \\\n   --lora <span class=\"hljs-string\">'/media/nmitchko/SSD-PUT/text-generation-webui/loras/medguanaco/'</span>\
          \ \\\n   --wbits 4 \\\n   --monkey-patch \\\n   --listen \\\n   --listen-port\
          \ 7890 \\\n   --chat \\\n   --extensions api google_translate | <span class=\"\
          hljs-built_in\">tee</span> <span class=\"hljs-string\">\"export-<span class=\"\
          hljs-variable\">${CURRENTDATEONLY}</span>-start.log\"</span>\n</code></pre>\n"
        raw: "@TheBloke One thing I ran into trying to merge my LoRAs using your code\
          \ is that `merge_and_unload()` doesn't work if the base model (mostly safetensors)\
          \ has no config file associated with it. You can change in the above code\
          \ \n* `model = model.merge_and_unload()` to `model = model.base_model.model`\
          \ \n\nand it should work (if it doesn't you can manually set the Config\
          \ on the AutoModelForCausalLM object `model.config = AutoConfig.from_pretrained('some/model')`).\
          \ \n\n# If using text-generation-ui\n\nIf you want to merge a GPTQ lora\
          \ created in `text-generation-ui` with `--monkey-patch` you can use the\
          \ below code.\n\n**merge-lora.py**\n```python\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, AutoConfig\nfrom peft import PeftModel\n\
          import torch\nimport modules.shared as shared\nimport modules.monkey_patch_gptq_lora_export\
          \ as monkeypatch\nimport os\nimport argparse\n\n# your hf access token\n\
          access_token= 'hf_deadbeef010101'\n# your uploaded hf repo\nhf_repo = 'myuser/myrepo-65b-GPTQ'\n\
          \ndef main():\n    device_arg = { 'device_map': 'auto' }\n\n    print(f\"\
          Loading base model: {shared.args.model}\")\n\n    config = AutoConfig.from_pretrained(f'{shared.args.model_dir}/{shared.args.model}/config.json')\n\
          \    model, tokenizer = monkeypatch.load_model_llama(shared.args.model)\n\
          \    model.config = config\n\n    print(f\"Loading PEFT: {shared.args.lora}\"\
          )\n    model = PeftModel.from_pretrained(model, shared.args.lora[0], **device_arg)\n\
          \n    model = model.base_model.model\n    print(f\"Saving to hub ...\")\n\
          \n    # to push to HF\n    model.push_to_hub(f\"{hf_repo}\", use_temp_dir=True,\
          \ use_auth_token=access_token)\n    tokenizer.push_to_hub(f\"{hf_repo}\"\
          , use_temp_dir=True, use_auth_token=access_token)\n\n    # Or save locally\n\
          \    # model.save_pretrained(f\"my/output/dir\")\n    # tokenizer.save_pretrained(f\"\
          my/output/dir\")\n\nif __name__ == \"__main__\" :\n    main()\n```\nPlace\
          \ `merge-lora.py` in the root of text-generation-ui and then run the below\
          \ commands (adjusting your cuda visible devices).\n\n```bash\nCURRENTDATEONLY=`date\
          \ +\"%b %d %Y\"`\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython merge-lora.py\
          \ \\\n   --model 'TheBloke_guanaco-65B-GPTQ' \\\n   --lora '/media/nmitchko/SSD-PUT/text-generation-webui/loras/medguanaco/'\
          \ \\\n   --wbits 4 \\\n   --monkey-patch \\\n   --listen \\\n   --listen-port\
          \ 7890 \\\n   --chat \\\n   --extensions api google_translate | tee \"export-${CURRENTDATEONLY}-start.log\"\
          \n```"
        updatedAt: '2023-06-08T00:39:13.771Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - Darkhand-786
      - count: 2
        reaction: "\U0001F44D"
        users:
        - auntieD
        - webpolis
    id: 64812331bb25a636c9e56292
    type: comment
  author: nmitchko
  content: "@TheBloke One thing I ran into trying to merge my LoRAs using your code\
    \ is that `merge_and_unload()` doesn't work if the base model (mostly safetensors)\
    \ has no config file associated with it. You can change in the above code \n*\
    \ `model = model.merge_and_unload()` to `model = model.base_model.model` \n\n\
    and it should work (if it doesn't you can manually set the Config on the AutoModelForCausalLM\
    \ object `model.config = AutoConfig.from_pretrained('some/model')`). \n\n# If\
    \ using text-generation-ui\n\nIf you want to merge a GPTQ lora created in `text-generation-ui`\
    \ with `--monkey-patch` you can use the below code.\n\n**merge-lora.py**\n```python\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\nfrom\
    \ peft import PeftModel\nimport torch\nimport modules.shared as shared\nimport\
    \ modules.monkey_patch_gptq_lora_export as monkeypatch\nimport os\nimport argparse\n\
    \n# your hf access token\naccess_token= 'hf_deadbeef010101'\n# your uploaded hf\
    \ repo\nhf_repo = 'myuser/myrepo-65b-GPTQ'\n\ndef main():\n    device_arg = {\
    \ 'device_map': 'auto' }\n\n    print(f\"Loading base model: {shared.args.model}\"\
    )\n\n    config = AutoConfig.from_pretrained(f'{shared.args.model_dir}/{shared.args.model}/config.json')\n\
    \    model, tokenizer = monkeypatch.load_model_llama(shared.args.model)\n    model.config\
    \ = config\n\n    print(f\"Loading PEFT: {shared.args.lora}\")\n    model = PeftModel.from_pretrained(model,\
    \ shared.args.lora[0], **device_arg)\n\n    model = model.base_model.model\n \
    \   print(f\"Saving to hub ...\")\n\n    # to push to HF\n    model.push_to_hub(f\"\
    {hf_repo}\", use_temp_dir=True, use_auth_token=access_token)\n    tokenizer.push_to_hub(f\"\
    {hf_repo}\", use_temp_dir=True, use_auth_token=access_token)\n\n    # Or save\
    \ locally\n    # model.save_pretrained(f\"my/output/dir\")\n    # tokenizer.save_pretrained(f\"\
    my/output/dir\")\n\nif __name__ == \"__main__\" :\n    main()\n```\nPlace `merge-lora.py`\
    \ in the root of text-generation-ui and then run the below commands (adjusting\
    \ your cuda visible devices).\n\n```bash\nCURRENTDATEONLY=`date +\"%b %d %Y\"\
    `\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython merge-lora.py \\\n   --model 'TheBloke_guanaco-65B-GPTQ'\
    \ \\\n   --lora '/media/nmitchko/SSD-PUT/text-generation-webui/loras/medguanaco/'\
    \ \\\n   --wbits 4 \\\n   --monkey-patch \\\n   --listen \\\n   --listen-port\
    \ 7890 \\\n   --chat \\\n   --extensions api google_translate | tee \"export-${CURRENTDATEONLY}-start.log\"\
    \n```"
  created_at: 2023-06-07 23:39:13+00:00
  edited: false
  hidden: false
  id: 64812331bb25a636c9e56292
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-06-08T03:54:01.000Z'
    data:
      status: open
    id: 648150d9bb25a636c9e82dea
    type: status-change
  author: Yhyu13
  created_at: 2023-06-08 02:54:01+00:00
  id: 648150d9bb25a636c9e82dea
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-06-08T03:54:29.000Z'
    data:
      status: closed
    id: 648150f540facadc557a4fd6
    type: status-change
  author: Yhyu13
  created_at: 2023-06-08 02:54:29+00:00
  id: 648150f540facadc557a4fd6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d1435435aad2ee850202eaa4ab2522.svg
      fullname: Mahmood Mohsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maki99
      type: user
    createdAt: '2023-09-15T22:50:42.000Z'
    data:
      edited: false
      editors:
      - Maki99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4749165177345276
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d1435435aad2ee850202eaa4ab2522.svg
          fullname: Mahmood Mohsen
          isHf: false
          isPro: false
          name: Maki99
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ One thing I ran into trying to merge my LoRAs using your code is that\
          \ <code>merge_and_unload()</code> doesn't work if the base model (mostly\
          \ safetensors) has no config file associated with it. You can change in\
          \ the above code </p>\n<ul>\n<li><code>model = model.merge_and_unload()</code>\
          \ to <code>model = model.base_model.model</code></li>\n</ul>\n<p>and it\
          \ should work (if it doesn't you can manually set the Config on the AutoModelForCausalLM\
          \ object <code>model.config = AutoConfig.from_pretrained('some/model')</code>).\
          \ </p>\n<h1 id=\"if-using-text-generation-ui\">If using text-generation-ui</h1>\n\
          <p>If you want to merge a GPTQ lora created in <code>text-generation-ui</code>\
          \ with <code>--monkey-patch</code> you can use the below code.</p>\n<p><strong>merge-lora.py</strong></p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer, AutoConfig\n<span class=\"hljs-keyword\">from</span> peft\
          \ <span class=\"hljs-keyword\">import</span> PeftModel\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">import</span> modules.shared\
          \ <span class=\"hljs-keyword\">as</span> shared\n<span class=\"hljs-keyword\"\
          >import</span> modules.monkey_patch_gptq_lora_export <span class=\"hljs-keyword\"\
          >as</span> monkeypatch\n<span class=\"hljs-keyword\">import</span> os\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\n<span class=\"hljs-comment\"\
          ># your hf access token</span>\naccess_token= <span class=\"hljs-string\"\
          >'hf_deadbeef010101'</span>\n<span class=\"hljs-comment\"># your uploaded\
          \ hf repo</span>\nhf_repo = <span class=\"hljs-string\">'myuser/myrepo-65b-GPTQ'</span>\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >main</span>():\n    device_arg = { <span class=\"hljs-string\">'device_map'</span>:\
          \ <span class=\"hljs-string\">'auto'</span> }\n\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Loading base model: <span class=\"\
          hljs-subst\">{shared.args.model}</span>\"</span>)\n\n    config = AutoConfig.from_pretrained(<span\
          \ class=\"hljs-string\">f'<span class=\"hljs-subst\">{shared.args.model_dir}</span>/<span\
          \ class=\"hljs-subst\">{shared.args.model}</span>/config.json'</span>)\n\
          \    model, tokenizer = monkeypatch.load_model_llama(shared.args.model)\n\
          \    model.config = config\n\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"Loading PEFT: <span class=\"hljs-subst\">{shared.args.lora}</span>\"\
          </span>)\n    model = PeftModel.from_pretrained(model, shared.args.lora[<span\
          \ class=\"hljs-number\">0</span>], **device_arg)\n\n    model = model.base_model.model\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Saving to hub ...\"</span>)\n\n    <span class=\"hljs-comment\"># to\
          \ push to HF</span>\n    model.push_to_hub(<span class=\"hljs-string\">f\"\
          <span class=\"hljs-subst\">{hf_repo}</span>\"</span>, use_temp_dir=<span\
          \ class=\"hljs-literal\">True</span>, use_auth_token=access_token)\n   \
          \ tokenizer.push_to_hub(<span class=\"hljs-string\">f\"<span class=\"hljs-subst\"\
          >{hf_repo}</span>\"</span>, use_temp_dir=<span class=\"hljs-literal\">True</span>,\
          \ use_auth_token=access_token)\n\n    <span class=\"hljs-comment\"># Or\
          \ save locally</span>\n    <span class=\"hljs-comment\"># model.save_pretrained(f\"\
          my/output/dir\")</span>\n    <span class=\"hljs-comment\"># tokenizer.save_pretrained(f\"\
          my/output/dir\")</span>\n\n<span class=\"hljs-keyword\">if</span> __name__\
          \ == <span class=\"hljs-string\">\"__main__\"</span> :\n    main()\n</code></pre>\n\
          <p>Place <code>merge-lora.py</code> in the root of text-generation-ui and\
          \ then run the below commands (adjusting your cuda visible devices).</p>\n\
          <pre><code class=\"language-bash\">CURRENTDATEONLY=`<span class=\"hljs-built_in\"\
          >date</span> +<span class=\"hljs-string\">\"%b %d %Y\"</span>`\n\n<span\
          \ class=\"hljs-built_in\">export</span> CUDA_VISIBLE_DEVICES=0\n\npython\
          \ merge-lora.py \\\n   --model <span class=\"hljs-string\">'TheBloke_guanaco-65B-GPTQ'</span>\
          \ \\\n   --lora <span class=\"hljs-string\">'/media/nmitchko/SSD-PUT/text-generation-webui/loras/medguanaco/'</span>\
          \ \\\n   --wbits 4 \\\n   --monkey-patch \\\n   --listen \\\n   --listen-port\
          \ 7890 \\\n   --chat \\\n   --extensions api google_translate | <span class=\"\
          hljs-built_in\">tee</span> <span class=\"hljs-string\">\"export-<span class=\"\
          hljs-variable\">${CURRENTDATEONLY}</span>-start.log\"</span>\n</code></pre>\n\
          </blockquote>\n<p>how did you manage to train a QLoRa on Text web Ui? i\
          \ kept getting errors everytime i tried to train.. </p>\n"
        raw: "> @TheBloke One thing I ran into trying to merge my LoRAs using your\
          \ code is that `merge_and_unload()` doesn't work if the base model (mostly\
          \ safetensors) has no config file associated with it. You can change in\
          \ the above code \n> * `model = model.merge_and_unload()` to `model = model.base_model.model`\
          \ \n> \n> and it should work (if it doesn't you can manually set the Config\
          \ on the AutoModelForCausalLM object `model.config = AutoConfig.from_pretrained('some/model')`).\
          \ \n> \n> # If using text-generation-ui\n> \n> If you want to merge a GPTQ\
          \ lora created in `text-generation-ui` with `--monkey-patch` you can use\
          \ the below code.\n> \n> **merge-lora.py**\n> ```python\n> from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n> from peft import\
          \ PeftModel\n> import torch\n> import modules.shared as shared\n> import\
          \ modules.monkey_patch_gptq_lora_export as monkeypatch\n> import os\n> import\
          \ argparse\n> \n> # your hf access token\n> access_token= 'hf_deadbeef010101'\n\
          > # your uploaded hf repo\n> hf_repo = 'myuser/myrepo-65b-GPTQ'\n> \n> def\
          \ main():\n>     device_arg = { 'device_map': 'auto' }\n> \n>     print(f\"\
          Loading base model: {shared.args.model}\")\n> \n>     config = AutoConfig.from_pretrained(f'{shared.args.model_dir}/{shared.args.model}/config.json')\n\
          >     model, tokenizer = monkeypatch.load_model_llama(shared.args.model)\n\
          >     model.config = config\n> \n>     print(f\"Loading PEFT: {shared.args.lora}\"\
          )\n>     model = PeftModel.from_pretrained(model, shared.args.lora[0], **device_arg)\n\
          > \n>     model = model.base_model.model\n>     print(f\"Saving to hub ...\"\
          )\n> \n>     # to push to HF\n>     model.push_to_hub(f\"{hf_repo}\", use_temp_dir=True,\
          \ use_auth_token=access_token)\n>     tokenizer.push_to_hub(f\"{hf_repo}\"\
          , use_temp_dir=True, use_auth_token=access_token)\n> \n>     # Or save locally\n\
          >     # model.save_pretrained(f\"my/output/dir\")\n>     # tokenizer.save_pretrained(f\"\
          my/output/dir\")\n> \n> if __name__ == \"__main__\" :\n>     main()\n> ```\n\
          > Place `merge-lora.py` in the root of text-generation-ui and then run the\
          \ below commands (adjusting your cuda visible devices).\n> \n> ```bash\n\
          > CURRENTDATEONLY=`date +\"%b %d %Y\"`\n> \n> export CUDA_VISIBLE_DEVICES=0\n\
          > \n> python merge-lora.py \\\n>    --model 'TheBloke_guanaco-65B-GPTQ'\
          \ \\\n>    --lora '/media/nmitchko/SSD-PUT/text-generation-webui/loras/medguanaco/'\
          \ \\\n>    --wbits 4 \\\n>    --monkey-patch \\\n>    --listen \\\n>   \
          \ --listen-port 7890 \\\n>    --chat \\\n>    --extensions api google_translate\
          \ | tee \"export-${CURRENTDATEONLY}-start.log\"\n> ```\n\nhow did you manage\
          \ to train a QLoRa on Text web Ui? i kept getting errors everytime i tried\
          \ to train.. "
        updatedAt: '2023-09-15T22:50:42.492Z'
      numEdits: 0
      reactions: []
    id: 6504dfc2d3de67a546d14625
    type: comment
  author: Maki99
  content: "> @TheBloke One thing I ran into trying to merge my LoRAs using your code\
    \ is that `merge_and_unload()` doesn't work if the base model (mostly safetensors)\
    \ has no config file associated with it. You can change in the above code \n>\
    \ * `model = model.merge_and_unload()` to `model = model.base_model.model` \n\
    > \n> and it should work (if it doesn't you can manually set the Config on the\
    \ AutoModelForCausalLM object `model.config = AutoConfig.from_pretrained('some/model')`).\
    \ \n> \n> # If using text-generation-ui\n> \n> If you want to merge a GPTQ lora\
    \ created in `text-generation-ui` with `--monkey-patch` you can use the below\
    \ code.\n> \n> **merge-lora.py**\n> ```python\n> from transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, AutoConfig\n> from peft import PeftModel\n> import torch\n> import\
    \ modules.shared as shared\n> import modules.monkey_patch_gptq_lora_export as\
    \ monkeypatch\n> import os\n> import argparse\n> \n> # your hf access token\n\
    > access_token= 'hf_deadbeef010101'\n> # your uploaded hf repo\n> hf_repo = 'myuser/myrepo-65b-GPTQ'\n\
    > \n> def main():\n>     device_arg = { 'device_map': 'auto' }\n> \n>     print(f\"\
    Loading base model: {shared.args.model}\")\n> \n>     config = AutoConfig.from_pretrained(f'{shared.args.model_dir}/{shared.args.model}/config.json')\n\
    >     model, tokenizer = monkeypatch.load_model_llama(shared.args.model)\n>  \
    \   model.config = config\n> \n>     print(f\"Loading PEFT: {shared.args.lora}\"\
    )\n>     model = PeftModel.from_pretrained(model, shared.args.lora[0], **device_arg)\n\
    > \n>     model = model.base_model.model\n>     print(f\"Saving to hub ...\")\n\
    > \n>     # to push to HF\n>     model.push_to_hub(f\"{hf_repo}\", use_temp_dir=True,\
    \ use_auth_token=access_token)\n>     tokenizer.push_to_hub(f\"{hf_repo}\", use_temp_dir=True,\
    \ use_auth_token=access_token)\n> \n>     # Or save locally\n>     # model.save_pretrained(f\"\
    my/output/dir\")\n>     # tokenizer.save_pretrained(f\"my/output/dir\")\n> \n\
    > if __name__ == \"__main__\" :\n>     main()\n> ```\n> Place `merge-lora.py`\
    \ in the root of text-generation-ui and then run the below commands (adjusting\
    \ your cuda visible devices).\n> \n> ```bash\n> CURRENTDATEONLY=`date +\"%b %d\
    \ %Y\"`\n> \n> export CUDA_VISIBLE_DEVICES=0\n> \n> python merge-lora.py \\\n\
    >    --model 'TheBloke_guanaco-65B-GPTQ' \\\n>    --lora '/media/nmitchko/SSD-PUT/text-generation-webui/loras/medguanaco/'\
    \ \\\n>    --wbits 4 \\\n>    --monkey-patch \\\n>    --listen \\\n>    --listen-port\
    \ 7890 \\\n>    --chat \\\n>    --extensions api google_translate | tee \"export-${CURRENTDATEONLY}-start.log\"\
    \n> ```\n\nhow did you manage to train a QLoRa on Text web Ui? i kept getting\
    \ errors everytime i tried to train.. "
  created_at: 2023-09-15 21:50:42+00:00
  edited: false
  hidden: false
  id: 6504dfc2d3de67a546d14625
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d1435435aad2ee850202eaa4ab2522.svg
      fullname: Mahmood Mohsen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maki99
      type: user
    createdAt: '2023-09-15T22:51:09.000Z'
    data:
      edited: false
      editors:
      - Maki99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36653661727905273
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d1435435aad2ee850202eaa4ab2522.svg
          fullname: Mahmood Mohsen
          isHf: false
          isPro: false
          name: Maki99
          type: user
        html: '<p>i even used --monkey-patch</p>

          '
        raw: 'i even used --monkey-patch

          '
        updatedAt: '2023-09-15T22:51:09.612Z'
      numEdits: 0
      reactions: []
    id: 6504dfdd2a9cebcc9bc11f95
    type: comment
  author: Maki99
  content: 'i even used --monkey-patch

    '
  created_at: 2023-09-15 21:51:09+00:00
  edited: false
  hidden: false
  id: 6504dfdd2a9cebcc9bc11f95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/72660963812ee19b654fb3111cb7e5ad.svg
      fullname: Nicholai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmitchko
      type: user
    createdAt: '2023-09-16T21:48:58.000Z'
    data:
      edited: false
      editors:
      - nmitchko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8880180716514587
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/72660963812ee19b654fb3111cb7e5ad.svg
          fullname: Nicholai
          isHf: false
          isPro: false
          name: nmitchko
          type: user
        html: '<blockquote>

          <p>i even used --monkey-patch</p>

          </blockquote>

          <p>I no longer use this workflow with TG-webui because of various bugs.
          Now I just use Qlora and point it to the model directory where I want to
          start and end. </p>

          '
        raw: '> i even used --monkey-patch


          I no longer use this workflow with TG-webui because of various bugs. Now
          I just use Qlora and point it to the model directory where I want to start
          and end. '
        updatedAt: '2023-09-16T21:48:58.413Z'
      numEdits: 0
      reactions: []
    id: 650622cadacc94cd6cf7a424
    type: comment
  author: nmitchko
  content: '> i even used --monkey-patch


    I no longer use this workflow with TG-webui because of various bugs. Now I just
    use Qlora and point it to the model directory where I want to start and end. '
  created_at: 2023-09-16 20:48:58+00:00
  edited: false
  hidden: false
  id: 650622cadacc94cd6cf7a424
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb70ac0712d265896d845960532abb71.svg
      fullname: Suryaa Kumar Relan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: skrelan
      type: user
    createdAt: '2023-10-23T23:00:51.000Z'
    data:
      edited: true
      editors:
      - skrelan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9352758526802063
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb70ac0712d265896d845960532abb71.svg
          fullname: Suryaa Kumar Relan
          isHf: false
          isPro: false
          name: skrelan
          type: user
        html: "<p>if someone has a working requirements.txt or pip freeze output for\
          \ your environment, can you please share? :) </p>\n<p>I keep getting hit\
          \ with the below</p>\n<pre><code>Traceback (most recent call last):\n  File\
          \ \"/app/merge.py\", line 70, in &lt;module&gt;\n    main()\n  File \"/app/merge.py\"\
          , line 38, in main\n    base_model = AutoModelForCausalLM.from_pretrained(\n\
          \  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 434, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
          \  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 873, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 579, in __getitem__\n\n    raise KeyError(key)\nKeyError: 'llama'\n\
          </code></pre>\n"
        raw: "if someone has a working requirements.txt or pip freeze output for your\
          \ environment, can you please share? :) \n\nI keep getting hit with the\
          \ below\n```\nTraceback (most recent call last):\n  File \"/app/merge.py\"\
          , line 70, in <module>\n    main()\n  File \"/app/merge.py\", line 38, in\
          \ main\n    base_model = AutoModelForCausalLM.from_pretrained(\n  File \"\
          /app/env/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 434, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
          \  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 873, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\n  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 579, in __getitem__\n\n    raise KeyError(key)\nKeyError: 'llama'\n\
          ```"
        updatedAt: '2023-10-23T23:21:55.434Z'
      numEdits: 2
      reactions: []
    id: 6536fb2325f780fed2190e5e
    type: comment
  author: skrelan
  content: "if someone has a working requirements.txt or pip freeze output for your\
    \ environment, can you please share? :) \n\nI keep getting hit with the below\n\
    ```\nTraceback (most recent call last):\n  File \"/app/merge.py\", line 70, in\
    \ <module>\n    main()\n  File \"/app/merge.py\", line 38, in main\n    base_model\
    \ = AutoModelForCausalLM.from_pretrained(\n  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 434, in from_pretrained\n    config, kwargs = AutoConfig.from_pretrained(\n\
    \  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 873, in from_pretrained\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\n  File \"/app/env/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 579, in __getitem__\n\n    raise KeyError(key)\nKeyError: 'llama'\n```"
  created_at: 2023-10-23 22:00:51+00:00
  edited: true
  hidden: false
  id: 6536fb2325f780fed2190e5e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: closed
target_branch: null
title: How to merge adapter into original llama?
