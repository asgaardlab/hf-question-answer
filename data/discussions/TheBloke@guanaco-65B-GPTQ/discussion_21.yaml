!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gameveloster
conflicting_files: null
created_at: 2023-06-18 19:12:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/367e1c9cbb77e454b57832240bf2adf5.svg
      fullname: Gameveloster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gameveloster
      type: user
    createdAt: '2023-06-18T20:12:06.000Z'
    data:
      edited: false
      editors:
      - gameveloster
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5329731106758118
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/367e1c9cbb77e454b57832240bf2adf5.svg
          fullname: Gameveloster
          isHf: false
          isPro: false
          name: gameveloster
          type: user
        html: "<p>Tried loading the model using Exllama on two 3090 but kept getting\
          \ the out of memory error. When this crashes, the first GPU VRAM was fully\
          \ utilized (23.69GB) but the 2nd GPU only used  7.87 GB of VRAM.</p>\n<pre><code>$\
          \ python server.py --model TheBloke_guanaco-65B-GPTQ --listen --chat --loader\
          \ exllama --gpu-split 24,24\nbin /home/gameveloster/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n\
          2023-06-18 15:57:03 INFO:Loading TheBloke_guanaco-65B-GPTQ...\nTraceback\
          \ (most recent call last):\n  File \"/mnt/md0/text-generation-webui/server.py\"\
          , line 1014, in &lt;module&gt;\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/mnt/md0/text-generation-webui/modules/models.py\", line 65, in\
          \ load_model\n    output = load_func_map[loader](model_name)\n  File \"\
          /mnt/md0/text-generation-webui/modules/models.py\", line 277, in ExLlama_loader\n\
          \    model, tokenizer = ExllamaModel.from_pretrained(model_name)\n  File\
          \ \"/mnt/md0/text-generation-webui/modules/exllama.py\", line 41, in from_pretrained\n\
          \    model = ExLlama(config)\n  File \"/mnt/md0/text-generation-webui/repositories/exllama/model.py\"\
          , line 630, in __init__\n    tensor = tensor.to(device, non_blocking = True)\n\
          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00\
          \ MiB (GPU 0; 23.69 GiB total capacity; 23.01 GiB already allocated; 35.12\
          \ MiB free; 23.01 GiB reserved in total by PyTorch) If reserved memory is\
          \ &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          </code></pre>\n<p>Should this model be loadable on two 3090 when using Exllama?</p>\n"
        raw: "Tried loading the model using Exllama on two 3090 but kept getting the\
          \ out of memory error. When this crashes, the first GPU VRAM was fully utilized\
          \ (23.69GB) but the 2nd GPU only used  7.87 GB of VRAM.\r\n\r\n```\r\n$\
          \ python server.py --model TheBloke_guanaco-65B-GPTQ --listen --chat --loader\
          \ exllama --gpu-split 24,24\r\nbin /home/gameveloster/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\r\
          \n2023-06-18 15:57:03 INFO:Loading TheBloke_guanaco-65B-GPTQ...\r\nTraceback\
          \ (most recent call last):\r\n  File \"/mnt/md0/text-generation-webui/server.py\"\
          , line 1014, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \n  File \"/mnt/md0/text-generation-webui/modules/models.py\", line 65,\
          \ in load_model\r\n    output = load_func_map[loader](model_name)\r\n  File\
          \ \"/mnt/md0/text-generation-webui/modules/models.py\", line 277, in ExLlama_loader\r\
          \n    model, tokenizer = ExllamaModel.from_pretrained(model_name)\r\n  File\
          \ \"/mnt/md0/text-generation-webui/modules/exllama.py\", line 41, in from_pretrained\r\
          \n    model = ExLlama(config)\r\n  File \"/mnt/md0/text-generation-webui/repositories/exllama/model.py\"\
          , line 630, in __init__\r\n    tensor = tensor.to(device, non_blocking =\
          \ True)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 86.00 MiB (GPU 0; 23.69 GiB total capacity; 23.01 GiB already allocated;\
          \ 35.12 MiB free; 23.01 GiB reserved in total by PyTorch) If reserved memory\
          \ is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n```\r\n\r\nShould this model be loadable on two 3090 when using Exllama?"
        updatedAt: '2023-06-18T20:12:06.210Z'
      numEdits: 0
      reactions: []
    id: 648f6516806d866590aafb14
    type: comment
  author: gameveloster
  content: "Tried loading the model using Exllama on two 3090 but kept getting the\
    \ out of memory error. When this crashes, the first GPU VRAM was fully utilized\
    \ (23.69GB) but the 2nd GPU only used  7.87 GB of VRAM.\r\n\r\n```\r\n$ python\
    \ server.py --model TheBloke_guanaco-65B-GPTQ --listen --chat --loader exllama\
    \ --gpu-split 24,24\r\nbin /home/gameveloster/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\r\
    \n2023-06-18 15:57:03 INFO:Loading TheBloke_guanaco-65B-GPTQ...\r\nTraceback (most\
    \ recent call last):\r\n  File \"/mnt/md0/text-generation-webui/server.py\", line\
    \ 1014, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"/mnt/md0/text-generation-webui/modules/models.py\", line 65, in load_model\r\
    \n    output = load_func_map[loader](model_name)\r\n  File \"/mnt/md0/text-generation-webui/modules/models.py\"\
    , line 277, in ExLlama_loader\r\n    model, tokenizer = ExllamaModel.from_pretrained(model_name)\r\
    \n  File \"/mnt/md0/text-generation-webui/modules/exllama.py\", line 41, in from_pretrained\r\
    \n    model = ExLlama(config)\r\n  File \"/mnt/md0/text-generation-webui/repositories/exllama/model.py\"\
    , line 630, in __init__\r\n    tensor = tensor.to(device, non_blocking = True)\r\
    \ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 86.00 MiB\
    \ (GPU 0; 23.69 GiB total capacity; 23.01 GiB already allocated; 35.12 MiB free;\
    \ 23.01 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory\
    \ try setting max_split_size_mb to avoid fragmentation.  See documentation for\
    \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\n\r\nShould this model\
    \ be loadable on two 3090 when using Exllama?"
  created_at: 2023-06-18 19:12:06+00:00
  edited: false
  hidden: false
  id: 648f6516806d866590aafb14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T10:12:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6946882009506226
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes, but you need an unequal split to allow for context on GPU 1.  From
          the exllama README:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pYReYe9vjKVMC8ctGikX0.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pYReYe9vjKVMC8ctGikX0.png"></a></p>

          <p>So try <code>--gpu-split 17.2,24</code> or similar</p>

          '
        raw: 'Yes, but you need an unequal split to allow for context on GPU 1.  From
          the exllama README:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pYReYe9vjKVMC8ctGikX0.png)


          So try `--gpu-split 17.2,24` or similar'
        updatedAt: '2023-06-20T10:12:49.014Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F917"
        users:
        - gameveloster
        - Renegadesoffun
        - zaidorx
        - minkis
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sdc-huggingface
    id: 64917ba1f5f899d75974adb7
    type: comment
  author: TheBloke
  content: 'Yes, but you need an unequal split to allow for context on GPU 1.  From
    the exllama README:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/pYReYe9vjKVMC8ctGikX0.png)


    So try `--gpu-split 17.2,24` or similar'
  created_at: 2023-06-20 09:12:49+00:00
  edited: false
  hidden: false
  id: 64917ba1f5f899d75974adb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/75e1cc4b81029d77455742bf56170d34.svg
      fullname: Osmani Diaz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zaidorx
      type: user
    createdAt: '2023-06-26T15:42:39.000Z'
    data:
      edited: false
      editors:
      - zaidorx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9422851204872131
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/75e1cc4b81029d77455742bf56170d34.svg
          fullname: Osmani Diaz
          isHf: false
          isPro: false
          name: zaidorx
          type: user
        html: '<p>I was running into this OOM issue even before exllama.  Following
          this recomedation, --gpu-split 17.2,24, now it works perfectly and I am
          getting 12 tokens/s. Impressive!</p>

          '
        raw: I was running into this OOM issue even before exllama.  Following this
          recomedation, --gpu-split 17.2,24, now it works perfectly and I am getting
          12 tokens/s. Impressive!
        updatedAt: '2023-06-26T15:42:39.460Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - minkis
    id: 6499b1efb865802e1d35f6cf
    type: comment
  author: zaidorx
  content: I was running into this OOM issue even before exllama.  Following this
    recomedation, --gpu-split 17.2,24, now it works perfectly and I am getting 12
    tokens/s. Impressive!
  created_at: 2023-06-26 14:42:39+00:00
  edited: false
  hidden: false
  id: 6499b1efb865802e1d35f6cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24b748c32136cb291d9390abef06e96b.svg
      fullname: William Fouvy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fouvy
      type: user
    createdAt: '2023-08-08T11:34:50.000Z'
    data:
      edited: false
      editors:
      - fouvy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37278667092323303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24b748c32136cb291d9390abef06e96b.svg
          fullname: William Fouvy
          isHf: false
          isPro: false
          name: fouvy
          type: user
        html: '<p>use_triton = False<br>m = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_cuda_fp16
          = False,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device_map="auto",<br>        max_memory=
          {i: ''24000MB'' for i in range(torch.cuda.device_count())},<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          '
        raw: "use_triton = False\nm = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_cuda_fp16 = False,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  device_map=\"auto\",\n        max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n\
          \        use_triton=use_triton,\n        quantize_config=None)"
        updatedAt: '2023-08-08T11:34:50.742Z'
      numEdits: 0
      reactions: []
    id: 64d2285a8527ac12775597a8
    type: comment
  author: fouvy
  content: "use_triton = False\nm = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_cuda_fp16 = False,\n    \
    \    use_safetensors=True,\n        trust_remote_code=True,\n        device_map=\"\
    auto\",\n        max_memory= {i: '24000MB' for i in range(torch.cuda.device_count())},\n\
    \        use_triton=use_triton,\n        quantize_config=None)"
  created_at: 2023-08-08 10:34:50+00:00
  edited: false
  hidden: false
  id: 64d2285a8527ac12775597a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-08T11:39:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7743595838546753
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to put less on GPU 1 to allow for context.  Try 16GB GPU1,
          24GB GPU 2</p>

          <p>Or you''ll get much better performance with ExLlama, and lower GPU usage
          too.  Here''s example code using ExLlama (there''s more examples in the
          same repo): <a rel="nofollow" href="https://github.com/turboderp/exllama/blob/c16cf49c3f19e887da31d671a713619c8626484e/example_basic.py">https://github.com/turboderp/exllama/blob/c16cf49c3f19e887da31d671a713619c8626484e/example_basic.py</a></p>

          <p>To that basic ExLlama code you would add <code>config.set_auto_map("17.2,24")
          ; config.gpu_peer_fix = True</code> for splitting over two GPUs.</p>

          '
        raw: 'You need to put less on GPU 1 to allow for context.  Try 16GB GPU1,
          24GB GPU 2


          Or you''ll get much better performance with ExLlama, and lower GPU usage
          too.  Here''s example code using ExLlama (there''s more examples in the
          same repo): https://github.com/turboderp/exllama/blob/c16cf49c3f19e887da31d671a713619c8626484e/example_basic.py


          To that basic ExLlama code you would add `config.set_auto_map("17.2,24")
          ; config.gpu_peer_fix = True` for splitting over two GPUs.'
        updatedAt: '2023-08-08T11:39:55.928Z'
      numEdits: 0
      reactions: []
    id: 64d2298b62a5593f17ccc6a4
    type: comment
  author: TheBloke
  content: 'You need to put less on GPU 1 to allow for context.  Try 16GB GPU1, 24GB
    GPU 2


    Or you''ll get much better performance with ExLlama, and lower GPU usage too.  Here''s
    example code using ExLlama (there''s more examples in the same repo): https://github.com/turboderp/exllama/blob/c16cf49c3f19e887da31d671a713619c8626484e/example_basic.py


    To that basic ExLlama code you would add `config.set_auto_map("17.2,24") ; config.gpu_peer_fix
    = True` for splitting over two GPUs.'
  created_at: 2023-08-08 10:39:55+00:00
  edited: false
  hidden: false
  id: 64d2298b62a5593f17ccc6a4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: Out of memory on two 3090
