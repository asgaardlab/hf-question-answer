!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xzqxnet
conflicting_files: null
created_at: 2023-06-07 03:10:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32793488906ec7347fade2cad33b304a.svg
      fullname: simeng wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xzqxnet
      type: user
    createdAt: '2023-06-07T04:10:23.000Z'
    data:
      edited: false
      editors:
      - xzqxnet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6508286595344543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32793488906ec7347fade2cad33b304a.svg
          fullname: simeng wang
          isHf: false
          isPro: false
          name: xzqxnet
          type: user
        html: '<p>I met the problem when using the command<br>CUDA_VISIBLE_DEVICES=0,1
          python llama_inference.py ${MODEL_DIR} --wbits 4 --groupsize 128 --load
          ${MODEL_DIR}/Guanaco-65B-GPTQ.safetensors --text "this is llama" --device=0</p>

          <p>Traceback (most recent call last):<br>  File "llama_inference.py", line
          110, in <br>    model = load_quant(args.model, args.load, args.wbits, args.groupsize,
          fused_mlp=args.fused_mlp)<br>  File "llama_inference.py", line 56, in load_quant<br>    model.load_state_dict(safe_load(checkpoint),
          strict=False)<br>  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py",
          line 1497, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for LlamaForCausalLM:<br>        size mismatch for model.layers.0.self_attn.k_proj.qzeros:
          copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape
          in current model is torch.Size([64, 1024]).<br>        size mismatch for
          model.layers.0.self_attn.k_proj.scales: copying a param with shape torch.Size([1,
          8192]) from checkpoint, the shape in current model is torch.Size([64, 8192]).<br>        size
          mismatch for model.layers.0.self_attn.o_proj.qzeros: copying a param with
          shape torch.Size([1, 1024]) from checkpoint, the shape in current model
          is torch.Size([64, 1024]).<br>        size mismatch for model.layers.0.self_attn.o_proj.scales:
          copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape
          in current model is torch.Size([64, 8192]).<br>        size mismatch for
          model.layers.0.self_attn.q_proj.qzeros: copying a param with shape torch.Size([1,
          1024]) from checkpoint, the shape in current model is torch.Size([64, 1024]).<br>        size
          mismatch for model.layers.0.self_attn.q_proj.scales: copying a param with
          shape torch.Size([1, 8192]) from checkpoint, the shape in current model
          is torch.Size([64, 8192]).<br>        size mismatch for model.layers.0.self_attn.v_proj.qzeros:
          copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape
          in current model is torch.Size([64, 1024]).<br>        size mismatch for
          model.layers.0.self_attn.v_proj.scales: copying a param with shape torch.Size([1,
          8192]) from checkpoint, the shape in current model is torch.Size([64, 8192]).<br>        size
          mismatch for model.layers.0.mlp.down_proj.qzeros: copying a param with shape
          torch.Size([1, 1024]) from checkpoint, the shape in current model is torch.Size([172,
          1024]).<br>        size mismatch for model.layers.0.mlp.down_proj.scales:
          copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape
          in current model is torch.Size([172, 8192]).<br>        size mismatch for
          model.layers.0.mlp.gate_proj.qzeros: copying a param with shape torch.Size([1,
          2752]) from checkpoint, the shape in current model is torch.Size([64, 2752]).<br>        size
          mismatch for model.layers.0.mlp.gate_proj.scales: copying a param with shape
          torch.Size([1, 22016]) from checkpoint, the shape in current model is torch.Size([64,
          22016]).<br>        size mismatch for model.layers.0.mlp.up_proj.qzeros:
          copying a param with shape torch.Size([1, 2752]) from checkpoint, the shape
          in current model is torch.Size([64, 2752]).<br>        size mismatch for
          model.layers.0.mlp.up_proj.scales: copying a param with shape torch.Size([1,
          22016]) from checkpoint, the shape in current model is torch.Size([64, 22016]).<br>        size
          mismatch for model.layers.1.self_attn.k_proj.qzeros: copying a param with
          shape torch.Size([1, 1024]) from checkpoint, the shape in current model
          is torch.Size([64, 1024]).<br>        size mismatch for model.layers.1.self_attn.k_proj.scales:
          copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape
          in current model is torch.Size([64, 8192]).<br>        size mismatch for
          model.layers.1.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,
          1024]) from checkpoint, the shape in current model is torch.Size([64, 1024]).<br>        size
          mismatch for model.layers.1.self_attn.o_proj.scales: copying a param with
          shape torch.Size([1, 8192]) from checkpoint, the shape in current model
          is torch.Size([64, 8192]).<br>        size mismatch for model.layers.1.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape
          in current model is torch.Size([64, 1024]).<br>        size mismatch for
          model.layers.1.self_attn.q_proj.scales: copying a param with shape torch.Size([1,
          8192]) from checkpoint, the shape in current model is torch.Size([64, 8192]).<br>        size
          mismatch for model.layers.1.self_attn.v_proj.qzeros: copying a param with
          shape torch.Size([1, 1024]) from checkpoint, the shape in current model
          is torch.Size([64, 1024]).<br>        size mismatch for model.layers.1.self_attn.v_proj.scales:
          copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape
          in current model is torch.Size([64, 8192]).<br>        size mismatch for
          model.layers.1.mlp.down_proj.qzeros: copying a param with shape torch.Size</p>

          '
        raw: "I met the problem when using the command \r\nCUDA_VISIBLE_DEVICES=0,1\
          \ python llama_inference.py ${MODEL_DIR} --wbits 4 --groupsize 128 --load\
          \ ${MODEL_DIR}/Guanaco-65B-GPTQ.safetensors --text \"this is llama\" --device=0\r\
          \n\r\nTraceback (most recent call last):\r\n  File \"llama_inference.py\"\
          , line 110, in <module>\r\n    model = load_quant(args.model, args.load,\
          \ args.wbits, args.groupsize, fused_mlp=args.fused_mlp)\r\n  File \"llama_inference.py\"\
          , line 56, in load_quant\r\n    model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1497, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\r\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.q_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.v_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.0.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([172, 1024]).\r\n        size mismatch\
          \ for model.layers.0.mlp.down_proj.scales: copying a param with shape torch.Size([1,\
          \ 8192]) from checkpoint, the shape in current model is torch.Size([172,\
          \ 8192]).\r\n        size mismatch for model.layers.0.mlp.gate_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2752]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 2752]).\r\n        size mismatch\
          \ for model.layers.0.mlp.gate_proj.scales: copying a param with shape torch.Size([1,\
          \ 22016]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 22016]).\r\n        size mismatch for model.layers.0.mlp.up_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 2752]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 2752]).\r\n        size mismatch\
          \ for model.layers.0.mlp.up_proj.scales: copying a param with shape torch.Size([1,\
          \ 22016]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 22016]).\r\n        size mismatch for model.layers.1.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.1.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.1.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.1.self_attn.o_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.1.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.1.self_attn.q_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.1.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([64, 1024]).\r\n        size mismatch\
          \ for model.layers.1.self_attn.v_proj.scales: copying a param with shape\
          \ torch.Size([1, 8192]) from checkpoint, the shape in current model is torch.Size([64,\
          \ 8192]).\r\n        size mismatch for model.layers.1.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size"
        updatedAt: '2023-06-07T04:10:23.973Z'
      numEdits: 0
      reactions: []
    id: 6480032f095af0bf116e9d92
    type: comment
  author: xzqxnet
  content: "I met the problem when using the command \r\nCUDA_VISIBLE_DEVICES=0,1\
    \ python llama_inference.py ${MODEL_DIR} --wbits 4 --groupsize 128 --load ${MODEL_DIR}/Guanaco-65B-GPTQ.safetensors\
    \ --text \"this is llama\" --device=0\r\n\r\nTraceback (most recent call last):\r\
    \n  File \"llama_inference.py\", line 110, in <module>\r\n    model = load_quant(args.model,\
    \ args.load, args.wbits, args.groupsize, fused_mlp=args.fused_mlp)\r\n  File \"\
    llama_inference.py\", line 56, in load_quant\r\n    model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\r\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1497, in load_state_dict\r\n    raise RuntimeError('Error(s) in loading\
    \ state_dict for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict\
    \ for LlamaForCausalLM:\r\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.0.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.0.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.0.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([172, 1024]).\r\n        size mismatch for model.layers.0.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([172, 8192]).\r\n        size mismatch for model.layers.0.mlp.gate_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2752]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 2752]).\r\n        size mismatch for model.layers.0.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([1, 22016]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 22016]).\r\n        size mismatch for model.layers.0.mlp.up_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 2752]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 2752]).\r\n        size mismatch for model.layers.0.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([1, 22016]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 22016]).\r\n        size mismatch for model.layers.1.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.1.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.1.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.1.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.1.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.1.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.1.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 1024]).\r\n        size mismatch for model.layers.1.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 8192]) from checkpoint, the shape\
    \ in current model is torch.Size([64, 8192]).\r\n        size mismatch for model.layers.1.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size"
  created_at: 2023-06-07 03:10:23+00:00
  edited: false
  hidden: false
  id: 6480032f095af0bf116e9d92
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-08T09:01:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7299869060516357
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>groupsize is <code>-1</code>, not <code>128</code></p>

          '
        raw: groupsize is `-1`, not `128`
        updatedAt: '2023-06-08T09:01:34.865Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - xzqxnet
    id: 648198ee15c5dc529064661d
    type: comment
  author: TheBloke
  content: groupsize is `-1`, not `128`
  created_at: 2023-06-08 08:01:34+00:00
  edited: false
  hidden: false
  id: 648198ee15c5dc529064661d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: RuntimeError
