!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vijaysb
conflicting_files: null
created_at: 2023-06-01 14:21:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
      fullname: vijay bhadrashetti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijaysb
      type: user
    createdAt: '2023-06-01T15:21:03.000Z'
    data:
      edited: true
      editors:
      - vijaysb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
          fullname: vijay bhadrashetti
          isHf: false
          isPro: false
          name: vijaysb
          type: user
        html: "<p>Message in terminal,<br>INFO:Loading TheBloke_guanaco-65B-GPTQ...<br>ERROR:Failed\
          \ to load GPTQ-for-LLaMa<br>ERROR:See <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\"\
          >https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a></p>\n\
          <p>I'm getting following error in WebUI:</p>\n<p>Traceback (most recent\
          \ call last): File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 18, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019</p>\n<p>During handling of the\
          \ above exception, another exception occurred:</p>\n<p>Traceback (most recent\
          \ call last): File \u201C/Users/vij/development/text-generation-webui/server.py\u201D\
          , line 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 97, in load_model output = load_func(model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 289, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 22, in sys.exit(-1) SystemExit: -1</p>\n<p>any idea how to fix this?</p>\n"
        raw: "Message in terminal,\nINFO:Loading TheBloke_guanaco-65B-GPTQ...\nERROR:Failed\
          \ to load GPTQ-for-LLaMa\nERROR:See https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\n\
          \nI'm getting following error in WebUI:\n\nTraceback (most recent call last):\
          \ File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 18, in import llama_inference_offload ModuleNotFoundError: No module\
          \ named \u2018llama_inference_offload\u2019\n\nDuring handling of the above\
          \ exception, another exception occurred:\n\nTraceback (most recent call\
          \ last): File \u201C/Users/vij/development/text-generation-webui/server.py\u201D\
          , line 71, in load_model_wrapper shared.model, shared.tokenizer = load_model(shared.model_name)\
          \ File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 97, in load_model output = load_func(model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
          , line 289, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
          , line 22, in sys.exit(-1) SystemExit: -1\n\n\nany idea how to fix this?"
        updatedAt: '2023-06-01T15:28:17.699Z'
      numEdits: 1
      reactions: []
    id: 6478b75f5bf35e70ab5ea890
    type: comment
  author: vijaysb
  content: "Message in terminal,\nINFO:Loading TheBloke_guanaco-65B-GPTQ...\nERROR:Failed\
    \ to load GPTQ-for-LLaMa\nERROR:See https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md\n\
    \nI'm getting following error in WebUI:\n\nTraceback (most recent call last):\
    \ File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 18, in import llama_inference_offload ModuleNotFoundError: No module named\
    \ \u2018llama_inference_offload\u2019\n\nDuring handling of the above exception,\
    \ another exception occurred:\n\nTraceback (most recent call last): File \u201C\
    /Users/vij/development/text-generation-webui/server.py\u201D, line 71, in load_model_wrapper\
    \ shared.model, shared.tokenizer = load_model(shared.model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
    , line 97, in load_model output = load_func(model_name) File \u201C/Users/vij/development/text-generation-webui/modules/models.py\u201D\
    , line 289, in GPTQ_loader import modules.GPTQ_loader File \u201C/Users/vij/development/text-generation-webui/modules/GPTQ_loader.py\u201D\
    , line 22, in sys.exit(-1) SystemExit: -1\n\n\nany idea how to fix this?"
  created_at: 2023-06-01 14:21:03+00:00
  edited: true
  hidden: false
  id: 6478b75f5bf35e70ab5ea890
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-01T15:33:04.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Gptq is not supported on macos at this time.</p>

          <p>Please use the ggml version, assuming you have 64+GB Ram. If not, please
          try a smaller model eg 33B GGML.</p>

          '
        raw: 'Gptq is not supported on macos at this time.


          Please use the ggml version, assuming you have 64+GB Ram. If not, please
          try a smaller model eg 33B GGML.'
        updatedAt: '2023-06-01T15:33:04.260Z'
      numEdits: 0
      reactions: []
    id: 6478ba301f9756aa89d57a6b
    type: comment
  author: TheBloke
  content: 'Gptq is not supported on macos at this time.


    Please use the ggml version, assuming you have 64+GB Ram. If not, please try a
    smaller model eg 33B GGML.'
  created_at: 2023-06-01 14:33:04+00:00
  edited: false
  hidden: false
  id: 6478ba301f9756aa89d57a6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
      fullname: vijay bhadrashetti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijaysb
      type: user
    createdAt: '2023-06-02T04:15:28.000Z'
    data:
      edited: false
      editors:
      - vijaysb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
          fullname: vijay bhadrashetti
          isHf: false
          isPro: false
          name: vijaysb
          type: user
        html: '<p>Thanks, I have exactly 64GB Ram, will it be slow? </p>

          <p>additionally what configurations we need to fine tune it?</p>

          '
        raw: "Thanks, I have exactly 64GB Ram, will it be slow? \n\nadditionally what\
          \ configurations we need to fine tune it?"
        updatedAt: '2023-06-02T04:15:28.603Z'
      numEdits: 0
      reactions: []
    id: 64796ce0ba447930a6ffe4d6
    type: comment
  author: vijaysb
  content: "Thanks, I have exactly 64GB Ram, will it be slow? \n\nadditionally what\
    \ configurations we need to fine tune it?"
  created_at: 2023-06-02 03:15:28+00:00
  edited: false
  hidden: false
  id: 64796ce0ba447930a6ffe4d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:21:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8543922901153564
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah it''ll be pretty slow. You might prefer to try a 30B model
          instead, like TheBloke/Guanaco-33B-GGML or, even better, TheBloke/WizardLM-30B-Uncensored-GGML</p>

          '
        raw: Yeah it'll be pretty slow. You might prefer to try a 30B model instead,
          like TheBloke/Guanaco-33B-GGML or, even better, TheBloke/WizardLM-30B-Uncensored-GGML
        updatedAt: '2023-06-05T10:21:52.163Z'
      numEdits: 0
      reactions: []
    id: 647db74032c471a7fa849816
    type: comment
  author: TheBloke
  content: Yeah it'll be pretty slow. You might prefer to try a 30B model instead,
    like TheBloke/Guanaco-33B-GGML or, even better, TheBloke/WizardLM-30B-Uncensored-GGML
  created_at: 2023-06-05 09:21:52+00:00
  edited: false
  hidden: false
  id: 647db74032c471a7fa849816
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
      fullname: vijay bhadrashetti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijaysb
      type: user
    createdAt: '2023-06-05T14:24:49.000Z'
    data:
      edited: true
      editors:
      - vijaysb
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8754571080207825
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
          fullname: vijay bhadrashetti
          isHf: false
          isPro: false
          name: vijaysb
          type: user
        html: '<p>Yes tried TheBloke/Guanaco-33B-GGML, and it worked, a little slow
          ad initially takes around 30 sec to begin generating text.<br>Thanks for
          your support!!</p>

          '
        raw: 'Yes tried TheBloke/Guanaco-33B-GGML, and it worked, a little slow ad
          initially takes around 30 sec to begin generating text.

          Thanks for your support!!'
        updatedAt: '2023-06-05T14:25:55.003Z'
      numEdits: 1
      reactions: []
    id: 647df0315214d172cbba1718
    type: comment
  author: vijaysb
  content: 'Yes tried TheBloke/Guanaco-33B-GGML, and it worked, a little slow ad initially
    takes around 30 sec to begin generating text.

    Thanks for your support!!'
  created_at: 2023-06-05 13:24:49+00:00
  edited: true
  hidden: false
  id: 647df0315214d172cbba1718
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dd656f0b062bc103c7b9cbb7827786e7.svg
      fullname: vijay bhadrashetti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vijaysb
      type: user
    createdAt: '2023-06-05T14:24:52.000Z'
    data:
      status: closed
    id: 647df034f14eafc3b45227a5
    type: status-change
  author: vijaysb
  created_at: 2023-06-05 13:24:52+00:00
  id: 647df034f14eafc3b45227a5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: closed
target_branch: null
title: "ModuleNotFoundError: No module named \u2018llama_inference_offload\u2019 on\
  \ Mac m1 chip"
