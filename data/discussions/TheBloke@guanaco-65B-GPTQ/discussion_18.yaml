!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gr8ston
conflicting_files: null
created_at: 2023-06-12 08:43:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
      fullname: Greatston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gr8ston
      type: user
    createdAt: '2023-06-12T09:43:55.000Z'
    data:
      edited: false
      editors:
      - gr8ston
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9661771655082703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
          fullname: Greatston
          isHf: false
          isPro: false
          name: gr8ston
          type: user
        html: '<p>Hey Bloke, Great work! You and your team have been doing incredible
          work on this space. </p>

          <p>I have successfully run your template to load guanaco-65B in the pod
          and was able to use the UI to generate text to the prompts. However, I want
          to consume that pod''s loaded model in my local python code through an API
          so i can build my own application over it. I could see in interface options
          "API" is enabled , however I am not able to see any documentation around
          how to consume this through an API.</p>

          <p>Thanks in advance.</p>

          '
        raw: "Hey Bloke, Great work! You and your team have been doing incredible\
          \ work on this space. \r\n\r\nI have successfully run your template to load\
          \ guanaco-65B in the pod and was able to use the UI to generate text to\
          \ the prompts. However, I want to consume that pod's loaded model in my\
          \ local python code through an API so i can build my own application over\
          \ it. I could see in interface options \"API\" is enabled , however I am\
          \ not able to see any documentation around how to consume this through an\
          \ API.\r\n\r\nThanks in advance."
        updatedAt: '2023-06-12T09:43:55.515Z'
      numEdits: 0
      reactions: []
    id: 6486e8db9264d020ddb26ac9
    type: comment
  author: gr8ston
  content: "Hey Bloke, Great work! You and your team have been doing incredible work\
    \ on this space. \r\n\r\nI have successfully run your template to load guanaco-65B\
    \ in the pod and was able to use the UI to generate text to the prompts. However,\
    \ I want to consume that pod's loaded model in my local python code through an\
    \ API so i can build my own application over it. I could see in interface options\
    \ \"API\" is enabled , however I am not able to see any documentation around how\
    \ to consume this through an API.\r\n\r\nThanks in advance."
  created_at: 2023-06-12 08:43:55+00:00
  edited: false
  hidden: false
  id: 6486e8db9264d020ddb26ac9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
      fullname: Greatston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gr8ston
      type: user
    createdAt: '2023-06-12T09:44:16.000Z'
    data:
      from: How do I consume the successfully running pod with your one click UI through
        an API
      to: How do I consume the successfully running pod with your one click UI template
        through an API
    id: 6486e8f0a12230ee9a98f654
    type: title-change
  author: gr8ston
  created_at: 2023-06-12 08:44:16+00:00
  id: 6486e8f0a12230ee9a98f654
  new_title: How do I consume the successfully running pod with your one click UI
    template through an API
  old_title: How do I consume the successfully running pod with your one click UI
    through an API
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29811ddb400267e78d8d75b07b7a99d5.svg
      fullname: Lindsay Gray
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cognisant
      type: user
    createdAt: '2023-06-12T09:49:37.000Z'
    data:
      edited: false
      editors:
      - cognisant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9210740327835083
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29811ddb400267e78d8d75b07b7a99d5.svg
          fullname: Lindsay Gray
          isHf: false
          isPro: false
          name: cognisant
          type: user
        html: '<p>Much easier and probably cheaper (Depending on usage) to deploy
          on modal.com btw. I got it running there np. Just repurpose some of their
          example codes to get started.</p>

          '
        raw: Much easier and probably cheaper (Depending on usage) to deploy on modal.com
          btw. I got it running there np. Just repurpose some of their example codes
          to get started.
        updatedAt: '2023-06-12T09:49:37.147Z'
      numEdits: 0
      reactions: []
    id: 6486ea3108d6b2821e5dc655
    type: comment
  author: cognisant
  content: Much easier and probably cheaper (Depending on usage) to deploy on modal.com
    btw. I got it running there np. Just repurpose some of their example codes to
    get started.
  created_at: 2023-06-12 08:49:37+00:00
  edited: false
  hidden: false
  id: 6486ea3108d6b2821e5dc655
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T09:50:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8705207705497742
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>My team? ;)</p>

          <p>I have two templates. They run identical software, but the one called
          "Local LLMs One-Click UI" doesn''t expose the API, and the "One-Click UI
          and API" does.  So make sure you''re using the latter.</p>

          <p>The API one exposes two ports, 5000 and 5005.  5000 is an HTTPs port,
          5005 is TCP for use with web sockets.</p>

          <p>There are two example scripts included in text-generation-webui:</p>

          <ul>

          <li><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py">https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py</a></li>

          <li><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example-stream.py">https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example-stream.py</a></li>

          </ul>

          <p>I believe the streaming example (which returns text word-by-word, like
          ChatGPT) uses port 5005, websockets, and the other one uses 5000 with HTTPS.</p>

          <p>To connect to the HTTPS port:</p>

          <ul>

          <li>Click the CONNECT button in Runpod</li>

          <li>Right-click where it says "HTTP 5000", and copy the link address</li>

          <li>Now connect to that address on port 443 from your API client code</li>

          </ul>

          <p>To connect to the web sockets port:</p>

          <ul>

          <li>Click the CONNECT button in Runpod</li>

          <li>Click on the "TCP ports" tab (or similar name; I forget the exact wording)</li>

          <li>Find the entry that says Internal: 5005 and make a note of its external
          port </li>

          <li>Then connect to the pod''s external IP (will be shown on that tab, or
          under CONNECT) on that external port.   There''s a screenshot in the README
          demonstrating this, although the UI has changed slightly since I took the
          screenshot. I need to update that.</li>

          </ul>

          '
        raw: "My team? ;)\n\nI have two templates. They run identical software, but\
          \ the one called \"Local LLMs One-Click UI\" doesn't expose the API, and\
          \ the \"One-Click UI and API\" does.  So make sure you're using the latter.\n\
          \nThe API one exposes two ports, 5000 and 5005.  5000 is an HTTPs port,\
          \ 5005 is TCP for use with web sockets.\n\nThere are two example scripts\
          \ included in text-generation-webui:\n- https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py\n\
          - https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example-stream.py\n\
          \nI believe the streaming example (which returns text word-by-word, like\
          \ ChatGPT) uses port 5005, websockets, and the other one uses 5000 with\
          \ HTTPS.\n\nTo connect to the HTTPS port:\n- Click the CONNECT button in\
          \ Runpod\n- Right-click where it says \"HTTP 5000\", and copy the link address\n\
          - Now connect to that address on port 443 from your API client code\n\n\
          To connect to the web sockets port:\n- Click the CONNECT button in Runpod\n\
          - Click on the \"TCP ports\" tab (or similar name; I forget the exact wording)\n\
          - Find the entry that says Internal: 5005 and make a note of its external\
          \ port \n- Then connect to the pod's external IP (will be shown on that\
          \ tab, or under CONNECT) on that external port.   There's a screenshot in\
          \ the README demonstrating this, although the UI has changed slightly since\
          \ I took the screenshot. I need to update that."
        updatedAt: '2023-06-12T09:50:16.594Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mer0mingian
    id: 6486ea58c1e620f0915e5b20
    type: comment
  author: TheBloke
  content: "My team? ;)\n\nI have two templates. They run identical software, but\
    \ the one called \"Local LLMs One-Click UI\" doesn't expose the API, and the \"\
    One-Click UI and API\" does.  So make sure you're using the latter.\n\nThe API\
    \ one exposes two ports, 5000 and 5005.  5000 is an HTTPs port, 5005 is TCP for\
    \ use with web sockets.\n\nThere are two example scripts included in text-generation-webui:\n\
    - https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example.py\n\
    - https://github.com/oobabooga/text-generation-webui/blob/main/api-examples/api-example-stream.py\n\
    \nI believe the streaming example (which returns text word-by-word, like ChatGPT)\
    \ uses port 5005, websockets, and the other one uses 5000 with HTTPS.\n\nTo connect\
    \ to the HTTPS port:\n- Click the CONNECT button in Runpod\n- Right-click where\
    \ it says \"HTTP 5000\", and copy the link address\n- Now connect to that address\
    \ on port 443 from your API client code\n\nTo connect to the web sockets port:\n\
    - Click the CONNECT button in Runpod\n- Click on the \"TCP ports\" tab (or similar\
    \ name; I forget the exact wording)\n- Find the entry that says Internal: 5005\
    \ and make a note of its external port \n- Then connect to the pod's external\
    \ IP (will be shown on that tab, or under CONNECT) on that external port.   There's\
    \ a screenshot in the README demonstrating this, although the UI has changed slightly\
    \ since I took the screenshot. I need to update that."
  created_at: 2023-06-12 08:50:16+00:00
  edited: false
  hidden: false
  id: 6486ea58c1e620f0915e5b20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T09:54:19.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9708778858184814
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Much easier and probably cheaper (Depending on usage) to deploy on modal.com
          btw. I got it running there np. Just repurpose some of their example codes
          to get started.</p>

          </blockquote>

          <p>I agree that serverless could be a good solution. I''ve not investigated
          it myself yet, but my friend Wing Lian has done great work creating Docker
          containers that deploy GPU-accelerated GGML models on Runpod serverless:  <a
          rel="nofollow" href="https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml">https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml</a>  </p>

          <p>I''ve not tried Modal yet (I plan to; they gave me some free credits
          to try it out.)  But my first impression was that it looked much more expensive
          than Runpod on account of the huge RAM pricing. $0.04 per GB per hour. That
          basically kills pytorch inference, which has to load the full model into
          RAM as well as on to the GPU; a 30B GPTQ 4bit model would use at least 24GB
          RAM which is another $0.96/hr on top of the GPU cost, which is already higher
          than Runpod''s.   For GGML it might work better, given that a GPU-accelerated
          GGML model only uses a few gigs of RAM when fully loaded onto the GPU.   But
          it''s still going to end up more expensive than Runpod.</p>

          <p>I''ve not tried Serverless on either yet, but that was my impression
          just looking at their pricing. I guess if Modal ends up being more reliable
          or faster then that could balance out. But I was surprised at the cost.</p>

          '
        raw: "> Much easier and probably cheaper (Depending on usage) to deploy on\
          \ modal.com btw. I got it running there np. Just repurpose some of their\
          \ example codes to get started.\n\nI agree that serverless could be a good\
          \ solution. I've not investigated it myself yet, but my friend Wing Lian\
          \ has done great work creating Docker containers that deploy GPU-accelerated\
          \ GGML models on Runpod serverless:  https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml\
          \  \n\nI've not tried Modal yet (I plan to; they gave me some free credits\
          \ to try it out.)  But my first impression was that it looked much more\
          \ expensive than Runpod on account of the huge RAM pricing. $0.04 per GB\
          \ per hour. That basically kills pytorch inference, which has to load the\
          \ full model into RAM as well as on to the GPU; a 30B GPTQ 4bit model would\
          \ use at least 24GB RAM which is another $0.96/hr on top of the GPU cost,\
          \ which is already higher than Runpod's.   For GGML it might work better,\
          \ given that a GPU-accelerated GGML model only uses a few gigs of RAM when\
          \ fully loaded onto the GPU.   But it's still going to end up more expensive\
          \ than Runpod.\n\nI've not tried Serverless on either yet, but that was\
          \ my impression just looking at their pricing. I guess if Modal ends up\
          \ being more reliable or faster then that could balance out. But I was surprised\
          \ at the cost."
        updatedAt: '2023-06-12T09:55:39.680Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mer0mingian
    id: 6486eb4bbbec106d5b7b93aa
    type: comment
  author: TheBloke
  content: "> Much easier and probably cheaper (Depending on usage) to deploy on modal.com\
    \ btw. I got it running there np. Just repurpose some of their example codes to\
    \ get started.\n\nI agree that serverless could be a good solution. I've not investigated\
    \ it myself yet, but my friend Wing Lian has done great work creating Docker containers\
    \ that deploy GPU-accelerated GGML models on Runpod serverless:  https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml\
    \  \n\nI've not tried Modal yet (I plan to; they gave me some free credits to\
    \ try it out.)  But my first impression was that it looked much more expensive\
    \ than Runpod on account of the huge RAM pricing. $0.04 per GB per hour. That\
    \ basically kills pytorch inference, which has to load the full model into RAM\
    \ as well as on to the GPU; a 30B GPTQ 4bit model would use at least 24GB RAM\
    \ which is another $0.96/hr on top of the GPU cost, which is already higher than\
    \ Runpod's.   For GGML it might work better, given that a GPU-accelerated GGML\
    \ model only uses a few gigs of RAM when fully loaded onto the GPU.   But it's\
    \ still going to end up more expensive than Runpod.\n\nI've not tried Serverless\
    \ on either yet, but that was my impression just looking at their pricing. I guess\
    \ if Modal ends up being more reliable or faster then that could balance out.\
    \ But I was surprised at the cost."
  created_at: 2023-06-12 08:54:19+00:00
  edited: true
  hidden: false
  id: 6486eb4bbbec106d5b7b93aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
      fullname: Greatston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gr8ston
      type: user
    createdAt: '2023-06-12T09:58:02.000Z'
    data:
      edited: false
      editors:
      - gr8ston
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9310240745544434
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
          fullname: Greatston
          isHf: false
          isPro: false
          name: gr8ston
          type: user
        html: '<p>Thanks for the quick and detailed response. :-) </p>

          <p>Let me try it out and get back. Looking forward to see something from
          you on the serverless end soon.</p>

          '
        raw: "Thanks for the quick and detailed response. :-) \n\nLet me try it out\
          \ and get back. Looking forward to see something from you on the serverless\
          \ end soon."
        updatedAt: '2023-06-12T09:58:02.579Z'
      numEdits: 0
      reactions: []
    id: 6486ec2a95424cc351952fc9
    type: comment
  author: gr8ston
  content: "Thanks for the quick and detailed response. :-) \n\nLet me try it out\
    \ and get back. Looking forward to see something from you on the serverless end\
    \ soon."
  created_at: 2023-06-12 08:58:02+00:00
  edited: false
  hidden: false
  id: 6486ec2a95424cc351952fc9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29811ddb400267e78d8d75b07b7a99d5.svg
      fullname: Lindsay Gray
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cognisant
      type: user
    createdAt: '2023-06-13T14:34:32.000Z'
    data:
      edited: false
      editors:
      - cognisant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7912642359733582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29811ddb400267e78d8d75b07b7a99d5.svg
          fullname: Lindsay Gray
          isHf: false
          isPro: false
          name: cognisant
          type: user
        html: "<blockquote>\n<blockquote>\n<p>Much easier and probably cheaper (Depending\
          \ on usage) to deploy on modal.com btw. I got it running there np. Just\
          \ repurpose some of their example codes to get started.</p>\n</blockquote>\n\
          <p>I agree that serverless could be a good solution. I've not investigated\
          \ it myself yet, but my friend Wing Lian has done great work creating Docker\
          \ containers that deploy GPU-accelerated GGML models on Runpod serverless:\
          \  <a rel=\"nofollow\" href=\"https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml\"\
          >https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml</a>\
          \  </p>\n<p>I've not tried Modal yet (I plan to; they gave me some free\
          \ credits to try it out.)  But my first impression was that it looked much\
          \ more expensive than Runpod on account of the huge RAM pricing. $0.04 per\
          \ GB per hour. That basically kills pytorch inference, which has to load\
          \ the full model into RAM as well as on to the GPU; a 30B GPTQ 4bit model\
          \ would use at least 24GB RAM which is another $0.96/hr on top of the GPU\
          \ cost, which is already higher than Runpod's.   For GGML it might work\
          \ better, given that a GPU-accelerated GGML model only uses a few gigs of\
          \ RAM when fully loaded onto the GPU.   But it's still going to end up more\
          \ expensive than Runpod.</p>\n<p>I've not tried Serverless on either yet,\
          \ but that was my impression just looking at their pricing. I guess if Modal\
          \ ends up being more reliable or faster then that could balance out. But\
          \ I was surprised at the cost.</p>\n</blockquote>\n<p>nice share! I was\
          \ going to paste you the modal.com code but running it, I ran into an error\
          \ I have yet to resolve. Maybe it was the 33B model I had run. However,\
          \ here's at least <a rel=\"nofollow\" href=\"https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/falcon_gptq.py\"\
          >the Falcon example</a> they give from which I've been adapting others:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-comment\"># ---</span>\n\
          <span class=\"hljs-comment\"># integration-test: false</span>\n<span class=\"\
          hljs-comment\"># ---</span>\n<span class=\"hljs-comment\"># # Run Falcon-40B\
          \ with AutoGPTQ</span>\n\n<span class=\"hljs-comment\"># In this example,\
          \ we run a quantized 4-bit version of Falcon-40B, the first open-source\
          \ large language</span>\n<span class=\"hljs-comment\"># model of its size,\
          \ using HuggingFace's [transformers](https://huggingface.co/docs/transformers/index)</span>\n\
          <span class=\"hljs-comment\"># library and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).</span>\n\
          <span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># Due\
          \ to the current limitations of the library, the inference speed is a little\
          \ under 1 token/second and the</span>\n<span class=\"hljs-comment\"># cold\
          \ start time on Modal is around 25s.</span>\n<span class=\"hljs-comment\"\
          >#</span>\n<span class=\"hljs-comment\"># For faster inference at the expense\
          \ of a slower cold start, check out</span>\n<span class=\"hljs-comment\"\
          ># [Running Falcon-40B with `bitsandbytes` quantization](/docs/guide/ex/falcon_bitsandbytes).\
          \ You can also</span>\n<span class=\"hljs-comment\"># run a smaller, 7-billion-parameter\
          \ model with the [OpenLLaMa example](/docs/guide/ex/openllama).</span>\n\
          <span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># ##\
          \ Setup</span>\n<span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"\
          ># First we import the components we need from `modal`.</span>\n\n<span\
          \ class=\"hljs-keyword\">from</span> modal <span class=\"hljs-keyword\"\
          >import</span> Image, Stub, gpu, method, web_endpoint\n\n<span class=\"\
          hljs-comment\"># ## Define a container image</span>\n<span class=\"hljs-comment\"\
          >#</span>\n<span class=\"hljs-comment\"># To take advantage of Modal's blazing\
          \ fast cold-start times, we download model weights</span>\n<span class=\"\
          hljs-comment\"># into a folder inside our container image. These weights\
          \ come from a quantized model</span>\n<span class=\"hljs-comment\"># found\
          \ on Huggingface.</span>\nIMAGE_MODEL_DIR = <span class=\"hljs-string\"\
          >\"/model\"</span>\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">download_model</span>():\n    <span class=\"hljs-keyword\"\
          >from</span> huggingface_hub <span class=\"hljs-keyword\">import</span>\
          \ snapshot_download\n\n    model_name = <span class=\"hljs-string\">\"TheBloke/falcon-40b-instruct-GPTQ\"\
          </span>\n    snapshot_download(model_name, local_dir=IMAGE_MODEL_DIR)\n\n\
          \n<span class=\"hljs-comment\"># Now, we define our image. We'll use the\
          \ `debian-slim` base image, and install the dependencies we need</span>\n\
          <span class=\"hljs-comment\"># using [`pip_install`](/docs/reference/modal.Image#pip_install).\
          \ At the end, we'll use</span>\n<span class=\"hljs-comment\"># [`run_function`](/docs/guide/custom-container#running-a-function-as-a-build-step-beta)\
          \ to run the</span>\n<span class=\"hljs-comment\"># function defined above\
          \ as part of the image build.</span>\n\nimage = (\n    Image.debian_slim(python_version=<span\
          \ class=\"hljs-string\">\"3.10\"</span>)\n    .apt_install(<span class=\"\
          hljs-string\">\"git\"</span>)\n    .pip_install(\n        <span class=\"\
          hljs-string\">\"huggingface_hub==0.14.1\"</span>,\n        <span class=\"\
          hljs-string\">\"transformers @ git+https://github.com/huggingface/transformers.git@f49a3453caa6fe606bb31c571423f72264152fce\"\
          </span>,\n        <span class=\"hljs-string\">\"auto-gptq @ git+https://github.com/PanQiWei/AutoGPTQ.git@b5db750c00e5f3f195382068433a3408ec3e8f3c\"\
          </span>,\n        <span class=\"hljs-string\">\"einops==0.6.1\"</span>,\n\
          \    )\n    .run_function(download_model)\n)\n\n<span class=\"hljs-comment\"\
          ># Let's instantiate and name our [Stub](/docs/guide/apps).</span>\nstub\
          \ = Stub(name=<span class=\"hljs-string\">\"example-falcon-gptq\"</span>,\
          \ image=image)\n\n\n<span class=\"hljs-comment\"># ## The model class</span>\n\
          <span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># Next,\
          \ we write the model code. We want Modal to load the model into memory just\
          \ once every time a container starts up,</span>\n<span class=\"hljs-comment\"\
          ># so we use [class syntax](/docs/guide/lifecycle-functions) and the `__enter__`\
          \ method.</span>\n<span class=\"hljs-comment\">#</span>\n<span class=\"\
          hljs-comment\"># Within the [@stub.cls](/docs/reference/modal.Stub#cls)\
          \ decorator, we use the [gpu parameter](/docs/guide/gpu)</span>\n<span class=\"\
          hljs-comment\"># to specify that we want to run our function on an [A100\
          \ GPU](/pricing). We also allow each call 10 mintues to complete,</span>\n\
          <span class=\"hljs-comment\"># and request the runner to stay live for 5\
          \ minutes after its last request.</span>\n<span class=\"hljs-comment\">#</span>\n\
          <span class=\"hljs-comment\"># The rest is just using the `transformers`\
          \ library to run the model. Refer to the</span>\n<span class=\"hljs-comment\"\
          ># [documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)</span>\n\
          <span class=\"hljs-comment\"># for more parameters and tuning.</span>\n\
          <span class=\"hljs-comment\">#</span>\n<span class=\"hljs-comment\"># Note\
          \ that we need to create a separate thread to call the `generate` function\
          \ because we need to</span>\n<span class=\"hljs-comment\"># yield the text\
          \ back from the streamer in the main thread. This is an idiosyncrasy with\
          \ streaming in `transformers`.</span>\n<span class=\"hljs-meta\"><span data-props=\"\
          {&quot;user&quot;:&quot;stub&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/stub\">@<span class=\"underline\">stub</span></a></span>\n\
          \n\t</span></span>.cls(<span class=\"hljs-params\">gpu=gpu.A100(<span class=\"\
          hljs-params\"></span>), timeout=<span class=\"hljs-number\">60</span> *\
          \ <span class=\"hljs-number\">10</span>, container_idle_timeout=<span class=\"\
          hljs-number\">60</span> * <span class=\"hljs-number\">5</span></span>)</span>\n\
          <span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >Falcon40BGPTQ</span>:\n    <span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">__enter__</span>(<span class=\"hljs-params\"\
          >self</span>):\n        <span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer\n        <span\
          \ class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n\
          \            IMAGE_MODEL_DIR, use_fast=<span class=\"hljs-literal\">True</span>\n\
          \        )\n        <span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"Loaded tokenizer.\"</span>)\n\n        self.model = AutoGPTQForCausalLM.from_quantized(\n\
          \            IMAGE_MODEL_DIR,\n            trust_remote_code=<span class=\"\
          hljs-literal\">True</span>,\n            use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n            device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>,\n            use_triton=<span class=\"hljs-literal\">False</span>,\n\
          \            strict=<span class=\"hljs-literal\">False</span>,\n       \
          \ )\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"Loaded model.\"</span>)\n\n<span class=\"hljs-meta\">    <span data-props=\"\
          {&quot;user&quot;:&quot;method&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/method\">@<span class=\"underline\">method</span></a></span>\n\
          \n\t</span></span>()</span>\n    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">generate</span>(<span class=\"hljs-params\"\
          >self, prompt: <span class=\"hljs-built_in\">str</span></span>):\n     \
          \   <span class=\"hljs-keyword\">from</span> threading <span class=\"hljs-keyword\"\
          >import</span> Thread\n        <span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> TextIteratorStreamer\n\
          \n        inputs = self.tokenizer(prompt, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>)\n        streamer = TextIteratorStreamer(\n\
          \            self.tokenizer, skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>\n        )\n        generation_kwargs = <span class=\"hljs-built_in\"\
          >dict</span>(\n            inputs=inputs.input_ids.cuda(),\n           \
          \ attention_mask=inputs.attention_mask,\n            temperature=<span class=\"\
          hljs-number\">0.1</span>,\n            max_new_tokens=<span class=\"hljs-number\"\
          >512</span>,\n            streamer=streamer,\n        )\n\n        <span\
          \ class=\"hljs-comment\"># Run generation on separate thread to enable response\
          \ streaming.</span>\n        thread = Thread(target=self.model.generate,\
          \ kwargs=generation_kwargs)\n        thread.start()\n        <span class=\"\
          hljs-keyword\">for</span> new_text <span class=\"hljs-keyword\">in</span>\
          \ streamer:\n            <span class=\"hljs-keyword\">yield</span> new_text\n\
          \n        thread.join()\n\n\n<span class=\"hljs-comment\"># ## Run the model</span>\n\
          <span class=\"hljs-comment\"># We define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps)\
          \ to call our remote function</span>\n<span class=\"hljs-comment\"># sequentially\
          \ for a list of inputs. You can run this locally with `modal run -q falcon_gptq.py`.\
          \ The `-q` flag</span>\n<span class=\"hljs-comment\"># enables streaming\
          \ to work in the terminal output.</span>\nprompt_template = (\n    <span\
          \ class=\"hljs-string\">\"A chat between a curious human user and an artificial\
          \ intelligence assistant. The assistant give a helpful, detailed, and accurate\
          \ answer to the user's question.\"</span>\n    <span class=\"hljs-string\"\
          >\"\\n\\nUser:\\n{}\\n\\nAssistant:\\n\"</span>\n)\n\n\n<span class=\"hljs-meta\"\
          ><span data-props=\"{&quot;user&quot;:&quot;stub&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/stub\">@<span class=\"\
          underline\">stub</span></a></span>\n\n\t</span></span>.local_entrypoint()</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >cli</span>():\n    question = <span class=\"hljs-string\">\"What are the\
          \ main differences between Python and JavaScript programming languages?\"\
          </span>\n    model = Falcon40BGPTQ()\n    <span class=\"hljs-keyword\">for</span>\
          \ text <span class=\"hljs-keyword\">in</span> model.generate.call(prompt_template.<span\
          \ class=\"hljs-built_in\">format</span>(question)):\n        <span class=\"\
          hljs-built_in\">print</span>(text, end=<span class=\"hljs-string\">\"\"\
          </span>, flush=<span class=\"hljs-literal\">True</span>)\n\n\n<span class=\"\
          hljs-comment\"># ## Serve the model</span>\n<span class=\"hljs-comment\"\
          ># Finally, we can serve the model from a web endpoint with `modal deploy\
          \ falcon_gptq.py`. If</span>\n<span class=\"hljs-comment\"># you visit the\
          \ resulting URL with a question parameter in your URL, you can view the\
          \ model's</span>\n<span class=\"hljs-comment\"># stream back a response.</span>\n\
          <span class=\"hljs-comment\"># You can try our deployment [here](https://modal-labs--example-falcon-gptq-get.modal.run/?question=Why%20are%20manhole%20covers%20round?).</span>\n\
          <span class=\"hljs-meta\"><span data-props=\"{&quot;user&quot;:&quot;stub&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/stub\"\
          >@<span class=\"underline\">stub</span></a></span>\n\n\t</span></span>.function(<span\
          \ class=\"hljs-params\">timeout=<span class=\"hljs-number\">600</span></span>)</span>\n\
          <span class=\"hljs-meta\">@web_endpoint()</span>\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">get</span>(<span class=\"\
          hljs-params\">question: <span class=\"hljs-built_in\">str</span></span>):\n\
          \    <span class=\"hljs-keyword\">from</span> fastapi.responses <span class=\"\
          hljs-keyword\">import</span> StreamingResponse\n    <span class=\"hljs-keyword\"\
          >from</span> itertools <span class=\"hljs-keyword\">import</span> chain\n\
          \n    model = Falcon40BGPTQ()\n    <span class=\"hljs-keyword\">return</span>\
          \ StreamingResponse(\n        chain(\n            (<span class=\"hljs-string\"\
          >\"Loading model. This usually takes around 20s ...\\n\\n\"</span>),\n \
          \           model.generate.call(prompt_template.<span class=\"hljs-built_in\"\
          >format</span>(question)),\n        ),\n        media_type=<span class=\"\
          hljs-string\">\"text/event-stream\"</span>,\n    )\n</code></pre>\n"
        raw: "> > Much easier and probably cheaper (Depending on usage) to deploy\
          \ on modal.com btw. I got it running there np. Just repurpose some of their\
          \ example codes to get started.\n> \n> I agree that serverless could be\
          \ a good solution. I've not investigated it myself yet, but my friend Wing\
          \ Lian has done great work creating Docker containers that deploy GPU-accelerated\
          \ GGML models on Runpod serverless:  https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml\
          \  \n> \n> I've not tried Modal yet (I plan to; they gave me some free credits\
          \ to try it out.)  But my first impression was that it looked much more\
          \ expensive than Runpod on account of the huge RAM pricing. $0.04 per GB\
          \ per hour. That basically kills pytorch inference, which has to load the\
          \ full model into RAM as well as on to the GPU; a 30B GPTQ 4bit model would\
          \ use at least 24GB RAM which is another $0.96/hr on top of the GPU cost,\
          \ which is already higher than Runpod's.   For GGML it might work better,\
          \ given that a GPU-accelerated GGML model only uses a few gigs of RAM when\
          \ fully loaded onto the GPU.   But it's still going to end up more expensive\
          \ than Runpod.\n> \n> I've not tried Serverless on either yet, but that\
          \ was my impression just looking at their pricing. I guess if Modal ends\
          \ up being more reliable or faster then that could balance out. But I was\
          \ surprised at the cost.\n\nnice share! I was going to paste you the modal.com\
          \ code but running it, I ran into an error I have yet to resolve. Maybe\
          \ it was the 33B model I had run. However, here's at least [the Falcon example](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/falcon_gptq.py)\
          \ they give from which I've been adapting others:\n\n```python\n# ---\n\
          # integration-test: false\n# ---\n# # Run Falcon-40B with AutoGPTQ\n\n#\
          \ In this example, we run a quantized 4-bit version of Falcon-40B, the first\
          \ open-source large language\n# model of its size, using HuggingFace's [transformers](https://huggingface.co/docs/transformers/index)\n\
          # library and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).\n#\n# Due\
          \ to the current limitations of the library, the inference speed is a little\
          \ under 1 token/second and the\n# cold start time on Modal is around 25s.\n\
          #\n# For faster inference at the expense of a slower cold start, check out\n\
          # [Running Falcon-40B with `bitsandbytes` quantization](/docs/guide/ex/falcon_bitsandbytes).\
          \ You can also\n# run a smaller, 7-billion-parameter model with the [OpenLLaMa\
          \ example](/docs/guide/ex/openllama).\n#\n# ## Setup\n#\n# First we import\
          \ the components we need from `modal`.\n\nfrom modal import Image, Stub,\
          \ gpu, method, web_endpoint\n\n# ## Define a container image\n#\n# To take\
          \ advantage of Modal's blazing fast cold-start times, we download model\
          \ weights\n# into a folder inside our container image. These weights come\
          \ from a quantized model\n# found on Huggingface.\nIMAGE_MODEL_DIR = \"\
          /model\"\n\n\ndef download_model():\n    from huggingface_hub import snapshot_download\n\
          \n    model_name = \"TheBloke/falcon-40b-instruct-GPTQ\"\n    snapshot_download(model_name,\
          \ local_dir=IMAGE_MODEL_DIR)\n\n\n# Now, we define our image. We'll use\
          \ the `debian-slim` base image, and install the dependencies we need\n#\
          \ using [`pip_install`](/docs/reference/modal.Image#pip_install). At the\
          \ end, we'll use\n# [`run_function`](/docs/guide/custom-container#running-a-function-as-a-build-step-beta)\
          \ to run the\n# function defined above as part of the image build.\n\nimage\
          \ = (\n    Image.debian_slim(python_version=\"3.10\")\n    .apt_install(\"\
          git\")\n    .pip_install(\n        \"huggingface_hub==0.14.1\",\n      \
          \  \"transformers @ git+https://github.com/huggingface/transformers.git@f49a3453caa6fe606bb31c571423f72264152fce\"\
          ,\n        \"auto-gptq @ git+https://github.com/PanQiWei/AutoGPTQ.git@b5db750c00e5f3f195382068433a3408ec3e8f3c\"\
          ,\n        \"einops==0.6.1\",\n    )\n    .run_function(download_model)\n\
          )\n\n# Let's instantiate and name our [Stub](/docs/guide/apps).\nstub =\
          \ Stub(name=\"example-falcon-gptq\", image=image)\n\n\n# ## The model class\n\
          #\n# Next, we write the model code. We want Modal to load the model into\
          \ memory just once every time a container starts up,\n# so we use [class\
          \ syntax](/docs/guide/lifecycle-functions) and the `__enter__` method.\n\
          #\n# Within the [@stub.cls](/docs/reference/modal.Stub#cls) decorator, we\
          \ use the [gpu parameter](/docs/guide/gpu)\n# to specify that we want to\
          \ run our function on an [A100 GPU](/pricing). We also allow each call 10\
          \ mintues to complete,\n# and request the runner to stay live for 5 minutes\
          \ after its last request.\n#\n# The rest is just using the `transformers`\
          \ library to run the model. Refer to the\n# [documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n\
          # for more parameters and tuning.\n#\n# Note that we need to create a separate\
          \ thread to call the `generate` function because we need to\n# yield the\
          \ text back from the streamer in the main thread. This is an idiosyncrasy\
          \ with streaming in `transformers`.\n@stub.cls(gpu=gpu.A100(), timeout=60\
          \ * 10, container_idle_timeout=60 * 5)\nclass Falcon40BGPTQ:\n    def __enter__(self):\n\
          \        from transformers import AutoTokenizer\n        from auto_gptq\
          \ import AutoGPTQForCausalLM\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n\
          \            IMAGE_MODEL_DIR, use_fast=True\n        )\n        print(\"\
          Loaded tokenizer.\")\n\n        self.model = AutoGPTQForCausalLM.from_quantized(\n\
          \            IMAGE_MODEL_DIR,\n            trust_remote_code=True,\n   \
          \         use_safetensors=True,\n            device_map=\"auto\",\n    \
          \        use_triton=False,\n            strict=False,\n        )\n     \
          \   print(\"Loaded model.\")\n\n    @method()\n    def generate(self, prompt:\
          \ str):\n        from threading import Thread\n        from transformers\
          \ import TextIteratorStreamer\n\n        inputs = self.tokenizer(prompt,\
          \ return_tensors=\"pt\")\n        streamer = TextIteratorStreamer(\n   \
          \         self.tokenizer, skip_special_tokens=True\n        )\n        generation_kwargs\
          \ = dict(\n            inputs=inputs.input_ids.cuda(),\n            attention_mask=inputs.attention_mask,\n\
          \            temperature=0.1,\n            max_new_tokens=512,\n       \
          \     streamer=streamer,\n        )\n\n        # Run generation on separate\
          \ thread to enable response streaming.\n        thread = Thread(target=self.model.generate,\
          \ kwargs=generation_kwargs)\n        thread.start()\n        for new_text\
          \ in streamer:\n            yield new_text\n\n        thread.join()\n\n\n\
          # ## Run the model\n# We define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps)\
          \ to call our remote function\n# sequentially for a list of inputs. You\
          \ can run this locally with `modal run -q falcon_gptq.py`. The `-q` flag\n\
          # enables streaming to work in the terminal output.\nprompt_template = (\n\
          \    \"A chat between a curious human user and an artificial intelligence\
          \ assistant. The assistant give a helpful, detailed, and accurate answer\
          \ to the user's question.\"\n    \"\\n\\nUser:\\n{}\\n\\nAssistant:\\n\"\
          \n)\n\n\n@stub.local_entrypoint()\ndef cli():\n    question = \"What are\
          \ the main differences between Python and JavaScript programming languages?\"\
          \n    model = Falcon40BGPTQ()\n    for text in model.generate.call(prompt_template.format(question)):\n\
          \        print(text, end=\"\", flush=True)\n\n\n# ## Serve the model\n#\
          \ Finally, we can serve the model from a web endpoint with `modal deploy\
          \ falcon_gptq.py`. If\n# you visit the resulting URL with a question parameter\
          \ in your URL, you can view the model's\n# stream back a response.\n# You\
          \ can try our deployment [here](https://modal-labs--example-falcon-gptq-get.modal.run/?question=Why%20are%20manhole%20covers%20round?).\n\
          @stub.function(timeout=600)\n@web_endpoint()\ndef get(question: str):\n\
          \    from fastapi.responses import StreamingResponse\n    from itertools\
          \ import chain\n\n    model = Falcon40BGPTQ()\n    return StreamingResponse(\n\
          \        chain(\n            (\"Loading model. This usually takes around\
          \ 20s ...\\n\\n\"),\n            model.generate.call(prompt_template.format(question)),\n\
          \        ),\n        media_type=\"text/event-stream\",\n    )\n```"
        updatedAt: '2023-06-13T14:34:32.620Z'
      numEdits: 0
      reactions: []
    id: 64887e785815252b02af0297
    type: comment
  author: cognisant
  content: "> > Much easier and probably cheaper (Depending on usage) to deploy on\
    \ modal.com btw. I got it running there np. Just repurpose some of their example\
    \ codes to get started.\n> \n> I agree that serverless could be a good solution.\
    \ I've not investigated it myself yet, but my friend Wing Lian has done great\
    \ work creating Docker containers that deploy GPU-accelerated GGML models on Runpod\
    \ serverless:  https://github.com/OpenAccess-AI-Collective/servereless-runpod-ggml\
    \  \n> \n> I've not tried Modal yet (I plan to; they gave me some free credits\
    \ to try it out.)  But my first impression was that it looked much more expensive\
    \ than Runpod on account of the huge RAM pricing. $0.04 per GB per hour. That\
    \ basically kills pytorch inference, which has to load the full model into RAM\
    \ as well as on to the GPU; a 30B GPTQ 4bit model would use at least 24GB RAM\
    \ which is another $0.96/hr on top of the GPU cost, which is already higher than\
    \ Runpod's.   For GGML it might work better, given that a GPU-accelerated GGML\
    \ model only uses a few gigs of RAM when fully loaded onto the GPU.   But it's\
    \ still going to end up more expensive than Runpod.\n> \n> I've not tried Serverless\
    \ on either yet, but that was my impression just looking at their pricing. I guess\
    \ if Modal ends up being more reliable or faster then that could balance out.\
    \ But I was surprised at the cost.\n\nnice share! I was going to paste you the\
    \ modal.com code but running it, I ran into an error I have yet to resolve. Maybe\
    \ it was the 33B model I had run. However, here's at least [the Falcon example](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/falcon_gptq.py)\
    \ they give from which I've been adapting others:\n\n```python\n# ---\n# integration-test:\
    \ false\n# ---\n# # Run Falcon-40B with AutoGPTQ\n\n# In this example, we run\
    \ a quantized 4-bit version of Falcon-40B, the first open-source large language\n\
    # model of its size, using HuggingFace's [transformers](https://huggingface.co/docs/transformers/index)\n\
    # library and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ).\n#\n# Due to the\
    \ current limitations of the library, the inference speed is a little under 1\
    \ token/second and the\n# cold start time on Modal is around 25s.\n#\n# For faster\
    \ inference at the expense of a slower cold start, check out\n# [Running Falcon-40B\
    \ with `bitsandbytes` quantization](/docs/guide/ex/falcon_bitsandbytes). You can\
    \ also\n# run a smaller, 7-billion-parameter model with the [OpenLLaMa example](/docs/guide/ex/openllama).\n\
    #\n# ## Setup\n#\n# First we import the components we need from `modal`.\n\nfrom\
    \ modal import Image, Stub, gpu, method, web_endpoint\n\n# ## Define a container\
    \ image\n#\n# To take advantage of Modal's blazing fast cold-start times, we download\
    \ model weights\n# into a folder inside our container image. These weights come\
    \ from a quantized model\n# found on Huggingface.\nIMAGE_MODEL_DIR = \"/model\"\
    \n\n\ndef download_model():\n    from huggingface_hub import snapshot_download\n\
    \n    model_name = \"TheBloke/falcon-40b-instruct-GPTQ\"\n    snapshot_download(model_name,\
    \ local_dir=IMAGE_MODEL_DIR)\n\n\n# Now, we define our image. We'll use the `debian-slim`\
    \ base image, and install the dependencies we need\n# using [`pip_install`](/docs/reference/modal.Image#pip_install).\
    \ At the end, we'll use\n# [`run_function`](/docs/guide/custom-container#running-a-function-as-a-build-step-beta)\
    \ to run the\n# function defined above as part of the image build.\n\nimage =\
    \ (\n    Image.debian_slim(python_version=\"3.10\")\n    .apt_install(\"git\"\
    )\n    .pip_install(\n        \"huggingface_hub==0.14.1\",\n        \"transformers\
    \ @ git+https://github.com/huggingface/transformers.git@f49a3453caa6fe606bb31c571423f72264152fce\"\
    ,\n        \"auto-gptq @ git+https://github.com/PanQiWei/AutoGPTQ.git@b5db750c00e5f3f195382068433a3408ec3e8f3c\"\
    ,\n        \"einops==0.6.1\",\n    )\n    .run_function(download_model)\n)\n\n\
    # Let's instantiate and name our [Stub](/docs/guide/apps).\nstub = Stub(name=\"\
    example-falcon-gptq\", image=image)\n\n\n# ## The model class\n#\n# Next, we write\
    \ the model code. We want Modal to load the model into memory just once every\
    \ time a container starts up,\n# so we use [class syntax](/docs/guide/lifecycle-functions)\
    \ and the `__enter__` method.\n#\n# Within the [@stub.cls](/docs/reference/modal.Stub#cls)\
    \ decorator, we use the [gpu parameter](/docs/guide/gpu)\n# to specify that we\
    \ want to run our function on an [A100 GPU](/pricing). We also allow each call\
    \ 10 mintues to complete,\n# and request the runner to stay live for 5 minutes\
    \ after its last request.\n#\n# The rest is just using the `transformers` library\
    \ to run the model. Refer to the\n# [documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n\
    # for more parameters and tuning.\n#\n# Note that we need to create a separate\
    \ thread to call the `generate` function because we need to\n# yield the text\
    \ back from the streamer in the main thread. This is an idiosyncrasy with streaming\
    \ in `transformers`.\n@stub.cls(gpu=gpu.A100(), timeout=60 * 10, container_idle_timeout=60\
    \ * 5)\nclass Falcon40BGPTQ:\n    def __enter__(self):\n        from transformers\
    \ import AutoTokenizer\n        from auto_gptq import AutoGPTQForCausalLM\n\n\
    \        self.tokenizer = AutoTokenizer.from_pretrained(\n            IMAGE_MODEL_DIR,\
    \ use_fast=True\n        )\n        print(\"Loaded tokenizer.\")\n\n        self.model\
    \ = AutoGPTQForCausalLM.from_quantized(\n            IMAGE_MODEL_DIR,\n      \
    \      trust_remote_code=True,\n            use_safetensors=True,\n          \
    \  device_map=\"auto\",\n            use_triton=False,\n            strict=False,\n\
    \        )\n        print(\"Loaded model.\")\n\n    @method()\n    def generate(self,\
    \ prompt: str):\n        from threading import Thread\n        from transformers\
    \ import TextIteratorStreamer\n\n        inputs = self.tokenizer(prompt, return_tensors=\"\
    pt\")\n        streamer = TextIteratorStreamer(\n            self.tokenizer, skip_special_tokens=True\n\
    \        )\n        generation_kwargs = dict(\n            inputs=inputs.input_ids.cuda(),\n\
    \            attention_mask=inputs.attention_mask,\n            temperature=0.1,\n\
    \            max_new_tokens=512,\n            streamer=streamer,\n        )\n\n\
    \        # Run generation on separate thread to enable response streaming.\n \
    \       thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n\
    \        thread.start()\n        for new_text in streamer:\n            yield\
    \ new_text\n\n        thread.join()\n\n\n# ## Run the model\n# We define a [`local_entrypoint`](/docs/guide/apps#entrypoints-for-ephemeral-apps)\
    \ to call our remote function\n# sequentially for a list of inputs. You can run\
    \ this locally with `modal run -q falcon_gptq.py`. The `-q` flag\n# enables streaming\
    \ to work in the terminal output.\nprompt_template = (\n    \"A chat between a\
    \ curious human user and an artificial intelligence assistant. The assistant give\
    \ a helpful, detailed, and accurate answer to the user's question.\"\n    \"\\\
    n\\nUser:\\n{}\\n\\nAssistant:\\n\"\n)\n\n\n@stub.local_entrypoint()\ndef cli():\n\
    \    question = \"What are the main differences between Python and JavaScript\
    \ programming languages?\"\n    model = Falcon40BGPTQ()\n    for text in model.generate.call(prompt_template.format(question)):\n\
    \        print(text, end=\"\", flush=True)\n\n\n# ## Serve the model\n# Finally,\
    \ we can serve the model from a web endpoint with `modal deploy falcon_gptq.py`.\
    \ If\n# you visit the resulting URL with a question parameter in your URL, you\
    \ can view the model's\n# stream back a response.\n# You can try our deployment\
    \ [here](https://modal-labs--example-falcon-gptq-get.modal.run/?question=Why%20are%20manhole%20covers%20round?).\n\
    @stub.function(timeout=600)\n@web_endpoint()\ndef get(question: str):\n    from\
    \ fastapi.responses import StreamingResponse\n    from itertools import chain\n\
    \n    model = Falcon40BGPTQ()\n    return StreamingResponse(\n        chain(\n\
    \            (\"Loading model. This usually takes around 20s ...\\n\\n\"),\n \
    \           model.generate.call(prompt_template.format(question)),\n        ),\n\
    \        media_type=\"text/event-stream\",\n    )\n```"
  created_at: 2023-06-13 13:34:32+00:00
  edited: false
  hidden: false
  id: 64887e785815252b02af0297
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
      fullname: Greatston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gr8ston
      type: user
    createdAt: '2023-06-14T09:40:09.000Z'
    data:
      edited: false
      editors:
      - gr8ston
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710934162139893
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/17a4235c4a076cf3f6b71f02da0040d2.svg
          fullname: Greatston
          isHf: false
          isPro: false
          name: gr8ston
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> - I did successfully\
          \ get the API up and running and am able to consume the same in my VS Code.\
          \ However i keep getting CORS issue when i attach my app to a secured site.\
          \ I understand what CORS is and it happens because my x.com is trying to\
          \ load data from y.proxy.runpod.net and hence the issue. This usually gets\
          \ fixed on the server. But in Runpod and with API template of yours I am\
          \ not sure how to fix it. I tried many ways that i am aware of but nothing\
          \ worked out. Any help is appreciated.</p>\n"
        raw: '@TheBloke - I did successfully get the API up and running and am able
          to consume the same in my VS Code. However i keep getting CORS issue when
          i attach my app to a secured site. I understand what CORS is and it happens
          because my x.com is trying to load data from y.proxy.runpod.net and hence
          the issue. This usually gets fixed on the server. But in Runpod and with
          API template of yours I am not sure how to fix it. I tried many ways that
          i am aware of but nothing worked out. Any help is appreciated.'
        updatedAt: '2023-06-14T09:40:09.659Z'
      numEdits: 0
      reactions: []
    id: 64898af9fe0e0f9928eca067
    type: comment
  author: gr8ston
  content: '@TheBloke - I did successfully get the API up and running and am able
    to consume the same in my VS Code. However i keep getting CORS issue when i attach
    my app to a secured site. I understand what CORS is and it happens because my
    x.com is trying to load data from y.proxy.runpod.net and hence the issue. This
    usually gets fixed on the server. But in Runpod and with API template of yours
    I am not sure how to fix it. I tried many ways that i am aware of but nothing
    worked out. Any help is appreciated.'
  created_at: 2023-06-14 08:40:09+00:00
  edited: false
  hidden: false
  id: 64898af9fe0e0f9928eca067
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-16T11:44:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609347581863403
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry I''m really not sure about web development questions</p>

          <p>If you learn of any change I can make to the template that would help,
          then let me know and I can look into adding it.  But I have no idea how
          to fix CORS in general.</p>

          '
        raw: 'Sorry I''m really not sure about web development questions


          If you learn of any change I can make to the template that would help, then
          let me know and I can look into adding it.  But I have no idea how to fix
          CORS in general.'
        updatedAt: '2023-06-16T11:44:50.003Z'
      numEdits: 0
      reactions: []
    id: 648c4b32967489905bf3839e
    type: comment
  author: TheBloke
  content: 'Sorry I''m really not sure about web development questions


    If you learn of any change I can make to the template that would help, then let
    me know and I can look into adding it.  But I have no idea how to fix CORS in
    general.'
  created_at: 2023-06-16 10:44:50+00:00
  edited: false
  hidden: false
  id: 648c4b32967489905bf3839e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cc87706133867a826ce099b669ede437.svg
      fullname: Ananda Vamsi Chittoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anandvamsi1993
      type: user
    createdAt: '2023-09-25T07:44:48.000Z'
    data:
      edited: true
      editors:
      - Anandvamsi1993
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9228258728981018
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cc87706133867a826ce099b669ede437.svg
          fullname: Ananda Vamsi Chittoor
          isHf: false
          isPro: false
          name: Anandvamsi1993
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> , I am trying\
          \ to do the same. Basically, I am working on an RAG project (Retrieval AUgmented\
          \ .. ) and I need the model instance on my vscode. I am new to the linux\
          \ part. Once the model is downloaded into workspace after we use wget, how\
          \ can I create a variable that references the model object? </p>\n<p>I genuinely\
          \ appreciate your help and I look up to you.</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;gr8ston&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/gr8ston\">@<span class=\"underline\">gr8ston</span></a></span>\n\
          \n\t</span></span> , if you know how to do this, please let me know.</p>\n"
        raw: "@TheBloke , I am trying to do the same. Basically, I am working on an\
          \ RAG project (Retrieval AUgmented .. ) and I need the model instance on\
          \ my vscode. I am new to the linux part. Once the model is downloaded into\
          \ workspace after we use wget, how can I create a variable that references\
          \ the model object? \n\nI genuinely appreciate your help and I look up to\
          \ you.\n\n@gr8ston , if you know how to do this, please let me know."
        updatedAt: '2023-09-25T07:48:41.517Z'
      numEdits: 1
      reactions: []
    id: 65113a702a45730c3f2534e4
    type: comment
  author: Anandvamsi1993
  content: "@TheBloke , I am trying to do the same. Basically, I am working on an\
    \ RAG project (Retrieval AUgmented .. ) and I need the model instance on my vscode.\
    \ I am new to the linux part. Once the model is downloaded into workspace after\
    \ we use wget, how can I create a variable that references the model object? \n\
    \nI genuinely appreciate your help and I look up to you.\n\n@gr8ston , if you\
    \ know how to do this, please let me know."
  created_at: 2023-09-25 06:44:48+00:00
  edited: true
  hidden: false
  id: 65113a702a45730c3f2534e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: How do I consume the successfully running pod with your one click UI template
  through an API
