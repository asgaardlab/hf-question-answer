!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muneerhanif7
conflicting_files: null
created_at: 2023-08-22 09:15:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e495e213b6ad8b38b1949310c6b120ae.svg
      fullname: Muhammad Muneer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muneerhanif7
      type: user
    createdAt: '2023-08-22T10:15:58.000Z'
    data:
      edited: false
      editors:
      - muneerhanif7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5171310305595398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e495e213b6ad8b38b1949310c6b120ae.svg
          fullname: Muhammad Muneer
          isHf: false
          isPro: false
          name: muneerhanif7
          type: user
        html: "<p>I am getting this error on every TheBloke models, I have just simply\
          \ copy paste the code from repo.<br>this is the code:<br>from transformers\
          \ import AutoTokenizer, pipeline, logging<br>from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig</p>\n<p>model_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\
          <br>model_basename = \"Guanaco-65B-GPTQ-4bit-128g.no-act.order\"</p>\n<p>use_triton\
          \ = False</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)</p>\n<p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>\
          \        model_basename=model_basename,<br>        use_safetensors=True,<br>\
          \        trust_remote_code=True,<br>        device=\"cuda:0\",<br>     \
          \   use_triton=use_triton,<br>        quantize_config=None)</p>\n<p>and\
          \ this is the error I am getting :</p>\n<p>Downloading (\u2026)okenizer_config.json:\
          \ 100%<br>700/700 [00:00&lt;00:00, 44.8kB/s]<br>Downloading tokenizer.model:\
          \ 100%<br>500k/500k [00:00&lt;00:00, 23.9MB/s]<br>Downloading (\u2026)/main/tokenizer.json:\
          \ 100%<br>1.84M/1.84M [00:00&lt;00:00, 10.7MB/s]<br>Downloading (\u2026\
          )cial_tokens_map.json: 100%<br>411/411 [00:00&lt;00:00, 29.0kB/s]<br>Downloading\
          \ (\u2026)lve/main/config.json: 100%<br>820/820 [00:00&lt;00:00, 64.9kB/s]<br>Downloading\
          \ (\u2026)quantize_config.json: 100%<br>156/156 [00:00&lt;00:00, 13.0kB/s]</p>\n\
          <hr>\n<p>FileNotFoundError                         Traceback (most recent\
          \ call last)<br>Cell In[3], line 11<br>      7 use_triton = False<br>  \
          \    9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)<br>---&gt;\
          \ 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>\
          \     12         model_basename=model_basename,<br>     13         use_safetensors=True,<br>\
          \     14         trust_remote_code=True,<br>     15         device=\"cuda:0\"\
          ,<br>     16         use_triton=use_triton,<br>     17         quantize_config=None)<br>\
          \     19 \"\"\"<br>     20 To download from a specific branch, use the revision\
          \ parameter, as in this example:<br>     21<br>   (...)<br>     28     \
          \    quantize_config=None)<br>     29 \"\"\"</p>\n<p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:94,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, **kwargs)<br>     88 quant_func\
          \ = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized<br>     89 keywords\
          \ = {<br>     90     key: kwargs[key]<br>     91     for key in signature(quant_func).parameters<br>\
          \     92     if key in kwargs<br>     93 }<br>---&gt; 94 return quant_func(<br>\
          \     95     model_name_or_path=model_name_or_path,<br>     96     save_dir=save_dir,<br>\
          \     97     device_map=device_map,<br>     98     max_memory=max_memory,<br>\
          \     99     device=device,<br>    100     low_cpu_mem_usage=low_cpu_mem_usage,<br>\
          \    101     use_triton=use_triton,<br>    102     inject_fused_attention=inject_fused_attention,<br>\
          \    103     inject_fused_mlp=inject_fused_mlp,<br>    104     use_cuda_fp16=use_cuda_fp16,<br>\
          \    105     quantize_config=quantize_config,<br>    106     model_basename=model_basename,<br>\
          \    107     use_safetensors=use_safetensors,<br>    108     trust_remote_code=trust_remote_code,<br>\
          \    109     warmup_triton=warmup_triton,<br>    110     trainable=trainable,<br>\
          \    111     **keywords<br>    112 )</p>\n<p>File /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:714,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype,\
          \ inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config,\
          \ model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable,\
          \ **kwargs)<br>    711             break<br>    713 if resolved_archive_file\
          \ is None: # Could not find a model file to use<br>--&gt; 714     raise\
          \ FileNotFoundError(f\"Could not find model in {model_name_or_path}\")<br>\
          \    716 model_save_name = resolved_archive_file<br>    718 if not use_triton\
          \ and trainable:</p>\n<p>FileNotFoundError: Could not find model in TheBloke/guanaco-65B-GPTQ</p>\n"
        raw: "I am getting this error on every TheBloke models, I have just simply\
          \ copy paste the code from repo.\r\nthis is the code:\r\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\r\n\r\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\
          \r\nmodel_basename = \"Guanaco-65B-GPTQ-4bit-128g.no-act.order\"\r\n\r\n\
          use_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        model_basename=model_basename,\r\n        use_safetensors=True,\r\
          \n        trust_remote_code=True,\r\n        device=\"cuda:0\",\r\n    \
          \    use_triton=use_triton,\r\n        quantize_config=None)\r\n\r\n\r\n\
          \r\nand this is the error I am getting :\r\n\r\nDownloading (\u2026)okenizer_config.json:\
          \ 100%\r\n700/700 [00:00<00:00, 44.8kB/s]\r\nDownloading tokenizer.model:\
          \ 100%\r\n500k/500k [00:00<00:00, 23.9MB/s]\r\nDownloading (\u2026)/main/tokenizer.json:\
          \ 100%\r\n1.84M/1.84M [00:00<00:00, 10.7MB/s]\r\nDownloading (\u2026)cial_tokens_map.json:\
          \ 100%\r\n411/411 [00:00<00:00, 29.0kB/s]\r\nDownloading (\u2026)lve/main/config.json:\
          \ 100%\r\n820/820 [00:00<00:00, 64.9kB/s]\r\nDownloading (\u2026)quantize_config.json:\
          \ 100%\r\n156/156 [00:00<00:00, 13.0kB/s]\r\n---------------------------------------------------------------------------\r\
          \nFileNotFoundError                         Traceback (most recent call\
          \ last)\r\nCell In[3], line 11\r\n      7 use_triton = False\r\n      9\
          \ tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\
          \n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n     12         model_basename=model_basename,\r\n     13         use_safetensors=True,\r\
          \n     14         trust_remote_code=True,\r\n     15         device=\"cuda:0\"\
          ,\r\n     16         use_triton=use_triton,\r\n     17         quantize_config=None)\r\
          \n     19 \"\"\"\r\n     20 To download from a specific branch, use the\
          \ revision parameter, as in this example:\r\n     21 \r\n   (...)\r\n  \
          \   28         quantize_config=None)\r\n     29 \"\"\"\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:94,\
          \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention,\
          \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
          \ trust_remote_code, warmup_triton, trainable, **kwargs)\r\n     88 quant_func\
          \ = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\r\n     89 keywords\
          \ = {\r\n     90     key: kwargs[key]\r\n     91     for key in signature(quant_func).parameters\r\
          \n     92     if key in kwargs\r\n     93 }\r\n---> 94 return quant_func(\r\
          \n     95     model_name_or_path=model_name_or_path,\r\n     96     save_dir=save_dir,\r\
          \n     97     device_map=device_map,\r\n     98     max_memory=max_memory,\r\
          \n     99     device=device,\r\n    100     low_cpu_mem_usage=low_cpu_mem_usage,\r\
          \n    101     use_triton=use_triton,\r\n    102     inject_fused_attention=inject_fused_attention,\r\
          \n    103     inject_fused_mlp=inject_fused_mlp,\r\n    104     use_cuda_fp16=use_cuda_fp16,\r\
          \n    105     quantize_config=quantize_config,\r\n    106     model_basename=model_basename,\r\
          \n    107     use_safetensors=use_safetensors,\r\n    108     trust_remote_code=trust_remote_code,\r\
          \n    109     warmup_triton=warmup_triton,\r\n    110     trainable=trainable,\r\
          \n    111     **keywords\r\n    112 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:714,\
          \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir,\
          \ device_map, max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype,\
          \ inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config,\
          \ model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable,\
          \ **kwargs)\r\n    711             break\r\n    713 if resolved_archive_file\
          \ is None: # Could not find a model file to use\r\n--> 714     raise FileNotFoundError(f\"\
          Could not find model in {model_name_or_path}\")\r\n    716 model_save_name\
          \ = resolved_archive_file\r\n    718 if not use_triton and trainable:\r\n\
          \r\nFileNotFoundError: Could not find model in TheBloke/guanaco-65B-GPTQ"
        updatedAt: '2023-08-22T10:15:58.507Z'
      numEdits: 0
      reactions: []
    id: 64e48ade08e43b3420f27a7d
    type: comment
  author: muneerhanif7
  content: "I am getting this error on every TheBloke models, I have just simply copy\
    \ paste the code from repo.\r\nthis is the code:\r\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \n\r\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\r\nmodel_basename = \"\
    Guanaco-65B-GPTQ-4bit-128g.no-act.order\"\r\n\r\nuse_triton = False\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\r\n\r\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\n        model_basename=model_basename,\r\
    \n        use_safetensors=True,\r\n        trust_remote_code=True,\r\n       \
    \ device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=None)\r\
    \n\r\n\r\n\r\nand this is the error I am getting :\r\n\r\nDownloading (\u2026\
    )okenizer_config.json: 100%\r\n700/700 [00:00<00:00, 44.8kB/s]\r\nDownloading\
    \ tokenizer.model: 100%\r\n500k/500k [00:00<00:00, 23.9MB/s]\r\nDownloading (\u2026\
    )/main/tokenizer.json: 100%\r\n1.84M/1.84M [00:00<00:00, 10.7MB/s]\r\nDownloading\
    \ (\u2026)cial_tokens_map.json: 100%\r\n411/411 [00:00<00:00, 29.0kB/s]\r\nDownloading\
    \ (\u2026)lve/main/config.json: 100%\r\n820/820 [00:00<00:00, 64.9kB/s]\r\nDownloading\
    \ (\u2026)quantize_config.json: 100%\r\n156/156 [00:00<00:00, 13.0kB/s]\r\n---------------------------------------------------------------------------\r\
    \nFileNotFoundError                         Traceback (most recent call last)\r\
    \nCell In[3], line 11\r\n      7 use_triton = False\r\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n     12         model_basename=model_basename,\r\n     13         use_safetensors=True,\r\
    \n     14         trust_remote_code=True,\r\n     15         device=\"cuda:0\"\
    ,\r\n     16         use_triton=use_triton,\r\n     17         quantize_config=None)\r\
    \n     19 \"\"\"\r\n     20 To download from a specific branch, use the revision\
    \ parameter, as in this example:\r\n     21 \r\n   (...)\r\n     28         quantize_config=None)\r\
    \n     29 \"\"\"\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:94,\
    \ in AutoGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir, device_map,\
    \ max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp,\
    \ use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code,\
    \ warmup_triton, trainable, **kwargs)\r\n     88 quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\r\
    \n     89 keywords = {\r\n     90     key: kwargs[key]\r\n     91     for key\
    \ in signature(quant_func).parameters\r\n     92     if key in kwargs\r\n    \
    \ 93 }\r\n---> 94 return quant_func(\r\n     95     model_name_or_path=model_name_or_path,\r\
    \n     96     save_dir=save_dir,\r\n     97     device_map=device_map,\r\n   \
    \  98     max_memory=max_memory,\r\n     99     device=device,\r\n    100    \
    \ low_cpu_mem_usage=low_cpu_mem_usage,\r\n    101     use_triton=use_triton,\r\
    \n    102     inject_fused_attention=inject_fused_attention,\r\n    103     inject_fused_mlp=inject_fused_mlp,\r\
    \n    104     use_cuda_fp16=use_cuda_fp16,\r\n    105     quantize_config=quantize_config,\r\
    \n    106     model_basename=model_basename,\r\n    107     use_safetensors=use_safetensors,\r\
    \n    108     trust_remote_code=trust_remote_code,\r\n    109     warmup_triton=warmup_triton,\r\
    \n    110     trainable=trainable,\r\n    111     **keywords\r\n    112 )\r\n\r\
    \nFile /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:714,\
    \ in BaseGPTQForCausalLM.from_quantized(cls, model_name_or_path, save_dir, device_map,\
    \ max_memory, device, low_cpu_mem_usage, use_triton, torch_dtype, inject_fused_attention,\
    \ inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors,\
    \ trust_remote_code, warmup_triton, trainable, **kwargs)\r\n    711          \
    \   break\r\n    713 if resolved_archive_file is None: # Could not find a model\
    \ file to use\r\n--> 714     raise FileNotFoundError(f\"Could not find model in\
    \ {model_name_or_path}\")\r\n    716 model_save_name = resolved_archive_file\r\
    \n    718 if not use_triton and trainable:\r\n\r\nFileNotFoundError: Could not\
    \ find model in TheBloke/guanaco-65B-GPTQ"
  created_at: 2023-08-22 09:15:58+00:00
  edited: false
  hidden: false
  id: 64e48ade08e43b3420f27a7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-22T10:18:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8383830785751343
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I recently updated all my GPTQ models for direct Transformers compatibility
          (coming very soon)</p>

          <p>Please check the README again and you''ll see that the <code>model_basename</code>
          line is now: <code>model_basename = "model"</code>.  This is true for all
          branches in all GPTQ models.</p>

          <p>Or in fact you can simply leave out <code>model_basename</code> now:</p>

          <pre><code>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)

          </code></pre>

          <p>Because the model_basename is now also configured in <code>quantize_config.json</code>.</p>

          <p>In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain
          this in more detail, and provide example code for loading GPTQ models directly
          from Transformers.  I am waiting for the new Transformers release to happen
          before I do this, which will be today or tomorrow.</p>

          '
        raw: 'I recently updated all my GPTQ models for direct Transformers compatibility
          (coming very soon)


          Please check the README again and you''ll see that the `model_basename`
          line is now: `model_basename = "model"`.  This is true for all branches
          in all GPTQ models.


          Or in fact you can simply leave out `model_basename` now:

          ```

          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)

          ```


          Because the model_basename is now also configured in `quantize_config.json`.


          In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain
          this in more detail, and provide example code for loading GPTQ models directly
          from Transformers.  I am waiting for the new Transformers release to happen
          before I do this, which will be today or tomorrow.'
        updatedAt: '2023-08-22T10:18:19.833Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - muneerhanif7
    id: 64e48b6bae2516de414c80af
    type: comment
  author: TheBloke
  content: 'I recently updated all my GPTQ models for direct Transformers compatibility
    (coming very soon)


    Please check the README again and you''ll see that the `model_basename` line is
    now: `model_basename = "model"`.  This is true for all branches in all GPTQ models.


    Or in fact you can simply leave out `model_basename` now:

    ```

    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

    use_safetensors=True,

    trust_remote_code=True,

    device="cuda:0",

    use_triton=use_triton,

    quantize_config=None)

    ```


    Because the model_basename is now also configured in `quantize_config.json`.


    In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain this
    in more detail, and provide example code for loading GPTQ models directly from
    Transformers.  I am waiting for the new Transformers release to happen before
    I do this, which will be today or tomorrow.'
  created_at: 2023-08-22 09:18:19+00:00
  edited: false
  hidden: false
  id: 64e48b6bae2516de414c80af
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'FileNotFoundError: Could not find model in TheBloke/guanaco-65B-GPTQ'
