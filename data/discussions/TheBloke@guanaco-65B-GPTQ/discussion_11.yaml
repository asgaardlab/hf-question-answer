!!python/object:huggingface_hub.community.DiscussionWithDetails
author: carlosbdw
conflicting_files: null
created_at: 2023-06-04 02:54:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
      fullname: dawei
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carlosbdw
      type: user
    createdAt: '2023-06-04T03:54:23.000Z'
    data:
      edited: false
      editors:
      - carlosbdw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9169829487800598
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/10a0d62af0788d6533722fd18d1d47c7.svg
          fullname: dawei
          isHf: false
          isPro: false
          name: carlosbdw
          type: user
        html: "<p>I tried guanaco-7b_ggml and guanao_13b_ggml \uFF0Cand feel them\
          \ are amzing model .<br>Now I want to try the 33B_GPTQ and 65B_GPTQ, but\
          \ can not find a successful way. It will be great if an official demo could\
          \ be provided , thax!</p>\n"
        raw: "I tried guanaco-7b_ggml and guanao_13b_ggml \uFF0Cand feel them are\
          \ amzing model . \r\nNow I want to try the 33B_GPTQ and 65B_GPTQ, but can\
          \ not find a successful way. It will be great if an official demo could\
          \ be provided , thax!"
        updatedAt: '2023-06-04T03:54:23.224Z'
      numEdits: 0
      reactions: []
    id: 647c0aefc3c809c69d33d646
    type: comment
  author: carlosbdw
  content: "I tried guanaco-7b_ggml and guanao_13b_ggml \uFF0Cand feel them are amzing\
    \ model . \r\nNow I want to try the 33B_GPTQ and 65B_GPTQ, but can not find a\
    \ successful way. It will be great if an official demo could be provided , thax!"
  created_at: 2023-06-04 02:54:23+00:00
  edited: false
  hidden: false
  id: 647c0aefc3c809c69d33d646
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T09:59:43.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2226213961839676
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>First install <a rel=\"nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ\"\
          >AutoGPTQ</a>:</p>\n<pre><code>pip install auto-gptq\n</code></pre>\n<p>Note\
          \ that this only works automatically with CUDA toolkit 11.7 or 11.8.</p>\n\
          <p>If you have another version, or if the above command doesn't install\
          \ the CUDA extension (if you get a warning about \"CUDA extension not installed\"\
          ), then install from source :</p>\n<pre><code>git clone https://github.com/PanQiWei/AutoGPTQ\n\
          cd AutoGPTQ\npip install .\n</code></pre>\n<p>Then here is some sample code:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\nparser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Simple AutoGPTQ example'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'model_name_or_path'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Model folder or repo'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--model_basename'</span>,\
          \ <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\"\
          >str</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'Model file basename if model is not named gptq_model-Xb-Ygr'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--use_slow'</span>, action=<span\
          \ class=\"hljs-string\">\"store_true\"</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Use slow tokenizer'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--use_safetensors'</span>,\
          \ action=<span class=\"hljs-string\">\"store_true\"</span>, <span class=\"\
          hljs-built_in\">help</span>=<span class=\"hljs-string\">'Model file basename\
          \ if model is not named gptq_model-Xb-Ygr'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'--use_triton'</span>, action=<span class=\"hljs-string\"\
          >\"store_true\"</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'Use Triton for inference?'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'--bits'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">int</span>, default=<span class=\"hljs-number\"\
          >4</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'Specify GPTQ bits. Only needed if no quantize_config.json is provided'</span>)\n\
          parser.add_argument(<span class=\"hljs-string\">'--group_size'</span>, <span\
          \ class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\">int</span>,\
          \ default=<span class=\"hljs-number\">128</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'Specify GPTQ group_size. Only\
          \ needed if no quantize_config.json is provided'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'--desc_act'</span>, action=<span class=\"hljs-string\"\
          >\"store_true\"</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'Specify GPTQ desc_act. Only needed if no quantize_config.json\
          \ is provided'</span>)\n\nargs = parser.parse_args()\n\nquantized_model_dir\
          \ = args.model_name_or_path\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=<span class=\"hljs-keyword\">not</span> args.use_slow)\n\n<span\
          \ class=\"hljs-keyword\">try</span>:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
          <span class=\"hljs-keyword\">except</span>:\n    quantize_config = BaseQuantizeConfig(\n\
          \            bits=args.bits,\n            group_size=args.group_size,\n\
          \            desc_act=args.desc_act\n        )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     model_basename=args.model_basename,\n        device=<span class=\"\
          hljs-string\">\"cuda:0\"</span>,\n        use_triton=args.use_triton,\n\
          \        quantize_config=quantize_config)\n\n<span class=\"hljs-comment\"\
          ># Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>Then run:</p>\n<pre><code>python\
          \ simple_autogptq.py TheBloke/guanaco-65B-GPTQ --model_basename Guanaco-65B-GPTQ-4bit.act-order\
          \ --use_safetensors\n</code></pre>\n<p>Or if you already downloaded the\
          \ model locally, you can specify the local path like:</p>\n<pre><code>python\
          \ simple_autogptq.py /path/to/models/TheBloke_guanaco-65B-GPTQ --model_basename\
          \ Guanaco-65B-GPTQ-4bit.act-order --use_safetensors\n</code></pre>\n"
        raw: "First install [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ):\n\n\
          ```\npip install auto-gptq\n```\n\nNote that this only works automatically\
          \ with CUDA toolkit 11.7 or 11.8.\n\nIf you have another version, or if\
          \ the above command doesn't install the CUDA extension (if you get a warning\
          \ about \"CUDA extension not installed\"), then install from source :\n\n\
          ```\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd AutoGPTQ\npip install\
          \ .\n```\n\nThen here is some sample code:\n\n```python\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nimport argparse\n\nparser = argparse.ArgumentParser(description='Simple\
          \ AutoGPTQ example')\nparser.add_argument('model_name_or_path', type=str,\
          \ help='Model folder or repo')\nparser.add_argument('--model_basename',\
          \ type=str, help='Model file basename if model is not named gptq_model-Xb-Ygr')\n\
          parser.add_argument('--use_slow', action=\"store_true\", help='Use slow\
          \ tokenizer')\nparser.add_argument('--use_safetensors', action=\"store_true\"\
          , help='Model file basename if model is not named gptq_model-Xb-Ygr')\n\
          parser.add_argument('--use_triton', action=\"store_true\", help='Use Triton\
          \ for inference?')\nparser.add_argument('--bits', type=int, default=4, help='Specify\
          \ GPTQ bits. Only needed if no quantize_config.json is provided')\nparser.add_argument('--group_size',\
          \ type=int, default=128, help='Specify GPTQ group_size. Only needed if no\
          \ quantize_config.json is provided')\nparser.add_argument('--desc_act',\
          \ action=\"store_true\", help='Specify GPTQ desc_act. Only needed if no\
          \ quantize_config.json is provided')\n\nargs = parser.parse_args()\n\nquantized_model_dir\
          \ = args.model_name_or_path\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=not args.use_slow)\n\ntry:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
          except:\n    quantize_config = BaseQuantizeConfig(\n            bits=args.bits,\n\
          \            group_size=args.group_size,\n            desc_act=args.desc_act\n\
          \        )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=args.model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=args.use_triton,\n     \
          \   quantize_config=quantize_config)\n\n# Prevent printing spurious transformers\
          \ error when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprompt = \"Tell me about AI\"\nprompt_template=f'''### Human: {prompt}\n\
          ### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"\
          text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\
          \nprint(pipe(prompt_template)[0]['generated_text'])\n\nprint(\"\\n\\n***\
          \ Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```\n\nThen run:\n\n```\npython simple_autogptq.py\
          \ TheBloke/guanaco-65B-GPTQ --model_basename Guanaco-65B-GPTQ-4bit.act-order\
          \ --use_safetensors\n```\n\nOr if you already downloaded the model locally,\
          \ you can specify the local path like:\n\n```\npython simple_autogptq.py\
          \ /path/to/models/TheBloke_guanaco-65B-GPTQ --model_basename Guanaco-65B-GPTQ-4bit.act-order\
          \ --use_safetensors\n```"
        updatedAt: '2023-06-04T10:01:29.692Z'
      numEdits: 1
      reactions: []
    id: 647c608f60dfe0f35d4c9558
    type: comment
  author: TheBloke
  content: "First install [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ):\n\n```\n\
    pip install auto-gptq\n```\n\nNote that this only works automatically with CUDA\
    \ toolkit 11.7 or 11.8.\n\nIf you have another version, or if the above command\
    \ doesn't install the CUDA extension (if you get a warning about \"CUDA extension\
    \ not installed\"), then install from source :\n\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
    cd AutoGPTQ\npip install .\n```\n\nThen here is some sample code:\n\n```python\n\
    from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\nparser = argparse.ArgumentParser(description='Simple\
    \ AutoGPTQ example')\nparser.add_argument('model_name_or_path', type=str, help='Model\
    \ folder or repo')\nparser.add_argument('--model_basename', type=str, help='Model\
    \ file basename if model is not named gptq_model-Xb-Ygr')\nparser.add_argument('--use_slow',\
    \ action=\"store_true\", help='Use slow tokenizer')\nparser.add_argument('--use_safetensors',\
    \ action=\"store_true\", help='Model file basename if model is not named gptq_model-Xb-Ygr')\n\
    parser.add_argument('--use_triton', action=\"store_true\", help='Use Triton for\
    \ inference?')\nparser.add_argument('--bits', type=int, default=4, help='Specify\
    \ GPTQ bits. Only needed if no quantize_config.json is provided')\nparser.add_argument('--group_size',\
    \ type=int, default=128, help='Specify GPTQ group_size. Only needed if no quantize_config.json\
    \ is provided')\nparser.add_argument('--desc_act', action=\"store_true\", help='Specify\
    \ GPTQ desc_act. Only needed if no quantize_config.json is provided')\n\nargs\
    \ = parser.parse_args()\n\nquantized_model_dir = args.model_name_or_path\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=not args.use_slow)\n\
    \ntry:\n   quantize_config = BaseQuantizeConfig.from_pretrained(quantized_model_dir)\n\
    except:\n    quantize_config = BaseQuantizeConfig(\n            bits=args.bits,\n\
    \            group_size=args.group_size,\n            desc_act=args.desc_act\n\
    \        )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        model_basename=args.model_basename,\n\
    \        device=\"cuda:0\",\n        use_triton=args.use_triton,\n        quantize_config=quantize_config)\n\
    \n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n\
    logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
    \ Human: {prompt}\n### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```\n\nThen run:\n\n```\npython simple_autogptq.py\
    \ TheBloke/guanaco-65B-GPTQ --model_basename Guanaco-65B-GPTQ-4bit.act-order --use_safetensors\n\
    ```\n\nOr if you already downloaded the model locally, you can specify the local\
    \ path like:\n\n```\npython simple_autogptq.py /path/to/models/TheBloke_guanaco-65B-GPTQ\
    \ --model_basename Guanaco-65B-GPTQ-4bit.act-order --use_safetensors\n```"
  created_at: 2023-06-04 08:59:43+00:00
  edited: true
  hidden: false
  id: 647c608f60dfe0f35d4c9558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8b3b1452f7e9aa6b6e3af3b5215bb55.svg
      fullname: Abbas Akkasi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkasi
      type: user
    createdAt: '2023-08-17T14:59:21.000Z'
    data:
      edited: false
      editors:
      - akkasi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8331531882286072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8b3b1452f7e9aa6b6e3af3b5215bb55.svg
          fullname: Abbas Akkasi
          isHf: false
          isPro: false
          name: akkasi
          type: user
        html: '<p>With two RTX 3090 GPUs, the inference time is infinite! Why? I am
          running the code above.</p>

          '
        raw: With two RTX 3090 GPUs, the inference time is infinite! Why? I am running
          the code above.
        updatedAt: '2023-08-17T14:59:21.352Z'
      numEdits: 0
      reactions: []
    id: 64de35c9f1f6c8961995dd02
    type: comment
  author: akkasi
  content: With two RTX 3090 GPUs, the inference time is infinite! Why? I am running
    the code above.
  created_at: 2023-08-17 13:59:21+00:00
  edited: false
  hidden: false
  id: 64de35c9f1f6c8961995dd02
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: Could provide a demo python file?
