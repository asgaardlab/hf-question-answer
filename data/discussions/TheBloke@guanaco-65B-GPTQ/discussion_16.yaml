!!python/object:huggingface_hub.community.DiscussionWithDetails
author: balu548411
conflicting_files: null
created_at: 2023-06-09 18:40:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
      fullname: balu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balu548411
      type: user
    createdAt: '2023-06-09T19:40:15.000Z'
    data:
      edited: false
      editors:
      - balu548411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5116525292396545
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
          fullname: balu
          isHf: false
          isPro: false
          name: balu548411
          type: user
        html: '<p>Hi bro, I am newbie to qlora, I tried below code and it raises OSError.
          Can you tell me how to load and use this using python.<br>from transformers
          import AutoTokenizer, AutoModelForCausalLM</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("TheBloke/guanaco-65B-GPTQ")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("TheBloke/guanaco-65B-GPTQ")</p>

          <hr>

          <p>OSError Traceback (most recent call last)<br> in &lt;cell line: 5&gt;()<br>      3
          tokenizer = AutoTokenizer.from_pretrained("TheBloke/guanaco-65B-GPTQ")<br>      4<br>----&gt;
          5 model = AutoModelForCausalLM.from_pretrained("TheBloke/guanaco-65B-GPTQ")</p>

          <p>1 frames<br>/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py
          in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)<br>   2553
          )<br>   2554 else:<br>-&gt; 2555 raise EnvironmentError(<br>   2556 f"{pretrained_model_name_or_path}
          does not appear to have a file named"<br>   2557 f" {_add_variant(WEIGHTS_NAME,
          variant)}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME} or"</p>

          <p>OSError: TheBloke/guanaco-65B-GPTQ does not appear to have a file named
          pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.</p>

          '
        raw: "Hi bro, I am newbie to qlora, I tried below code and it raises OSError.\
          \ Can you tell me how to load and use this using python. \r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          TheBloke/guanaco-65B-GPTQ\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/guanaco-65B-GPTQ\")\r\n\r\n\r\n---------------------------------------------------------------------------\r\
          \nOSError Traceback (most recent call last)\r\n<ipython-input-14-12ff4263c0c1>\
          \ in <cell line: 5>()\r\n      3 tokenizer = AutoTokenizer.from_pretrained(\"\
          TheBloke/guanaco-65B-GPTQ\")\r\n      4 \r\n----> 5 model = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/guanaco-65B-GPTQ\")\r\n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
          \n   2553 )\r\n   2554 else:\r\n-> 2555 raise EnvironmentError(\r\n   2556\
          \ f\"{pretrained_model_name_or_path} does not appear to have a file named\"\
          \r\n   2557 f\" {_add_variant(WEIGHTS_NAME, variant)}, {TF2_WEIGHTS_NAME},\
          \ {TF_WEIGHTS_NAME} or\"\r\n\r\nOSError: TheBloke/guanaco-65B-GPTQ does\
          \ not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
          \ or flax_model.msgpack."
        updatedAt: '2023-06-09T19:40:15.206Z'
      numEdits: 0
      reactions: []
    id: 6483801f206a4ce1bc41952a
    type: comment
  author: balu548411
  content: "Hi bro, I am newbie to qlora, I tried below code and it raises OSError.\
    \ Can you tell me how to load and use this using python. \r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/guanaco-65B-GPTQ\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/guanaco-65B-GPTQ\")\r\n\r\n\r\n---------------------------------------------------------------------------\r\
    \nOSError Traceback (most recent call last)\r\n<ipython-input-14-12ff4263c0c1>\
    \ in <cell line: 5>()\r\n      3 tokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/guanaco-65B-GPTQ\")\r\n      4 \r\n----> 5 model = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/guanaco-65B-GPTQ\")\r\n\r\n1 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\r\
    \n   2553 )\r\n   2554 else:\r\n-> 2555 raise EnvironmentError(\r\n   2556 f\"\
    {pretrained_model_name_or_path} does not appear to have a file named\"\r\n   2557\
    \ f\" {_add_variant(WEIGHTS_NAME, variant)}, {TF2_WEIGHTS_NAME}, {TF_WEIGHTS_NAME}\
    \ or\"\r\n\r\nOSError: TheBloke/guanaco-65B-GPTQ does not appear to have a file\
    \ named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack."
  created_at: 2023-06-09 18:40:15+00:00
  edited: false
  hidden: false
  id: 6483801f206a4ce1bc41952a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-09T19:42:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4686252176761627
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You can't load GPTQ models from regular transformers, you need AutoGPTQ</p>\n\
          <p><code>pip install auto-gptq</code></p>\n<p>Here is example code:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> argparse\n\nmodel_name_or_path\
          \ = <span class=\"hljs-string\">\"TheBloke/guanaco-65B-GPTQ\"</span>\nmodel_basename\
          \ = <span class=\"hljs-string\">\"Guanaco-65B-GPTQ-4bit.act-order\"</span>\n\
          \nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer =\
          \ AutoTokenizer.from_pretrained(model_name_or_path, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=<span class=\"\
          hljs-literal\">True</span>,\n        trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>,\n        device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n\
          \        use_triton=use_triton,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n\nprompt = <span class=\"hljs-string\">\"Tell\
          \ me about AI\"</span>\nprompt_template=<span class=\"hljs-string\">f'''###\
          \ Instruction: <span class=\"hljs-subst\">{prompt}</span></span>\n<span\
          \ class=\"hljs-string\">### Response:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n\n<span class=\"hljs-comment\"># Inference\
          \ can also be done using transformers' pipeline</span>\n\n<span class=\"\
          hljs-comment\"># Prevent printing spurious transformers error when using\
          \ pipeline with AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"*** Pipeline:\"</span>)\npipe = pipeline(\n    <span class=\"hljs-string\"\
          >\"text-generation\"</span>,\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=<span class=\"hljs-number\">512</span>,\n    temperature=<span\
          \ class=\"hljs-number\">0.7</span>,\n    top_p=<span class=\"hljs-number\"\
          >0.95</span>,\n    repetition_penalty=<span class=\"hljs-number\">1.15</span>\n\
          )\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n"
        raw: "You can't load GPTQ models from regular transformers, you need AutoGPTQ\n\
          \n`pip install auto-gptq`\n\nHere is example code:\n```python\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\
          \nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\n\nuse_triton = False\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n     \
          \   model_basename=model_basename,\n        use_safetensors=True,\n    \
          \    trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
          \ Instruction: {prompt}\n### Response:'''\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\n# Prevent printing spurious transformers error\
          \ when using pipeline with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n\n```"
        updatedAt: '2023-06-09T19:42:54.656Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - balu548411
    id: 648380bef9256e215de1f894
    type: comment
  author: TheBloke
  content: "You can't load GPTQ models from regular transformers, you need AutoGPTQ\n\
    \n`pip install auto-gptq`\n\nHere is example code:\n```python\nfrom transformers\
    \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\
    \nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\n\nuse_triton = False\n\
    \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\nprompt\
    \ = \"Tell me about AI\"\nprompt_template=f'''### Instruction: {prompt}\n### Response:'''\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n\n# Prevent printing spurious transformers error when using pipeline\
    \ with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
    )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
    )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\n```"
  created_at: 2023-06-09 18:42:54+00:00
  edited: false
  hidden: false
  id: 648380bef9256e215de1f894
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
      fullname: balu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balu548411
      type: user
    createdAt: '2023-06-10T04:47:49.000Z'
    data:
      edited: false
      editors:
      - balu548411
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.21482005715370178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
          fullname: balu
          isHf: false
          isPro: false
          name: balu548411
          type: user
        html: "<p>Thank you bro \U0001F60A\U0001F60A</p>\n"
        raw: "Thank you bro \U0001F60A\U0001F60A"
        updatedAt: '2023-06-10T04:47:49.663Z'
      numEdits: 0
      reactions: []
    id: 648400750942800058d6d4d2
    type: comment
  author: balu548411
  content: "Thank you bro \U0001F60A\U0001F60A"
  created_at: 2023-06-10 03:47:49+00:00
  edited: false
  hidden: false
  id: 648400750942800058d6d4d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91ff1eb3e4cda3ddbc182f5565d5f41c.svg
      fullname: hzg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hzg0601
      type: user
    createdAt: '2023-07-03T14:56:18.000Z'
    data:
      edited: false
      editors:
      - hzg0601
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7326577305793762
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91ff1eb3e4cda3ddbc182f5565d5f41c.svg
          fullname: hzg
          isHf: false
          isPro: false
          name: hzg0601
          type: user
        html: "<p>First of all, thanks a lot for your work!<br>I encounter an issue\
          \ which is directly caused by following codes:</p>\n<pre><code>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        device_map=\"auto\",\n        trust_remote_code=True,\n       \
          \ device=\"cuda\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          </code></pre>\n<p>it first warns me:</p>\n<pre><code>WARNING 2023-07-03\
          \ 22:36:45,587-1d: CUDA extension not installed.\n....\nWARNING 2023-07-03\
          \ 22:36:58,012-1d: The safetensors archive passed at /home/mydir/.cache/huggingface/hub/models--TheBloke--guanaco-65B-GPTQ/snapshots/c1a31c76e7228a13bc542b25243b912f12e39c87/Guanaco-65B-GPTQ-4bit.act-order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n</code></pre>\n<p>after a huge amount\
          \ of information about device_map, it raises the following error:</p>\n\
          <pre><code>--------------------------------------\nC++ Traceback (most recent\
          \ call last):\n--------------------------------------\nNo stack trace in\
          \ paddle, may be caused by external reasons.\n\n----------------------\n\
          Error Message Summary:\n----------------------\nFatalError: `Access to an\
          \ undefined portion of a memory object` is detected by the operating system.\n\
          \  [TimeInfo: *** Aborted at 1688395081 (unix time) try \"date -d @1688395081\"\
          \ if you are using GNU date ***]\n  [SignalInfo: *** SIGBUS (@0x7fbce9c3dff0)\
          \ received by PID 424101 (TID 0x7fbea6e7e740) from PID 18446744073336512496\
          \ ***]\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6033ae93b5883695ce9d0918/LsYSFyg909xJjyhnCxeCj.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6033ae93b5883695ce9d0918/LsYSFyg909xJjyhnCxeCj.png\"\
          ></a></p>\n<p>I pretty sure that I have my cudatoolkit installted, do you\
          \ have any clue about the problerm?<br>Again, thanks for your work and hope\
          \ for your reply.</p>\n"
        raw: "First of all, thanks a lot for your work!\nI encounter an issue which\
          \ is directly caused by following codes:\n```\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        device_map=\"auto\",\n        trust_remote_code=True,\n       \
          \ device=\"cuda\",\n        use_triton=use_triton,\n        quantize_config=None)\n\
          ```\nit first warns me:\n```\nWARNING 2023-07-03 22:36:45,587-1d: CUDA extension\
          \ not installed.\n....\nWARNING 2023-07-03 22:36:58,012-1d: The safetensors\
          \ archive passed at /home/mydir/.cache/huggingface/hub/models--TheBloke--guanaco-65B-GPTQ/snapshots/c1a31c76e7228a13bc542b25243b912f12e39c87/Guanaco-65B-GPTQ-4bit.act-order.safetensors\
          \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
          \ method. Defaulting to 'pt' metadata.\n\n```\nafter a huge amount of information\
          \ about device_map, it raises the following error:\n```\n--------------------------------------\n\
          C++ Traceback (most recent call last):\n--------------------------------------\n\
          No stack trace in paddle, may be caused by external reasons.\n\n----------------------\n\
          Error Message Summary:\n----------------------\nFatalError: `Access to an\
          \ undefined portion of a memory object` is detected by the operating system.\n\
          \  [TimeInfo: *** Aborted at 1688395081 (unix time) try \"date -d @1688395081\"\
          \ if you are using GNU date ***]\n  [SignalInfo: *** SIGBUS (@0x7fbce9c3dff0)\
          \ received by PID 424101 (TID 0x7fbea6e7e740) from PID 18446744073336512496\
          \ ***]\n```\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6033ae93b5883695ce9d0918/LsYSFyg909xJjyhnCxeCj.png)\n\
          \nI pretty sure that I have my cudatoolkit installted, do you have any clue\
          \ about the problerm?\nAgain, thanks for your work and hope for your reply."
        updatedAt: '2023-07-03T14:56:18.112Z'
      numEdits: 0
      reactions: []
    id: 64a2e192b470c1b02ae33464
    type: comment
  author: hzg0601
  content: "First of all, thanks a lot for your work!\nI encounter an issue which\
    \ is directly caused by following codes:\n```\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        model_basename=model_basename,\n        use_safetensors=True,\n     \
    \   device_map=\"auto\",\n        trust_remote_code=True,\n        device=\"cuda\"\
    ,\n        use_triton=use_triton,\n        quantize_config=None)\n```\nit first\
    \ warns me:\n```\nWARNING 2023-07-03 22:36:45,587-1d: CUDA extension not installed.\n\
    ....\nWARNING 2023-07-03 22:36:58,012-1d: The safetensors archive passed at /home/mydir/.cache/huggingface/hub/models--TheBloke--guanaco-65B-GPTQ/snapshots/c1a31c76e7228a13bc542b25243b912f12e39c87/Guanaco-65B-GPTQ-4bit.act-order.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n\n```\nafter a huge amount of information\
    \ about device_map, it raises the following error:\n```\n--------------------------------------\n\
    C++ Traceback (most recent call last):\n--------------------------------------\n\
    No stack trace in paddle, may be caused by external reasons.\n\n----------------------\n\
    Error Message Summary:\n----------------------\nFatalError: `Access to an undefined\
    \ portion of a memory object` is detected by the operating system.\n  [TimeInfo:\
    \ *** Aborted at 1688395081 (unix time) try \"date -d @1688395081\" if you are\
    \ using GNU date ***]\n  [SignalInfo: *** SIGBUS (@0x7fbce9c3dff0) received by\
    \ PID 424101 (TID 0x7fbea6e7e740) from PID 18446744073336512496 ***]\n```\n\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6033ae93b5883695ce9d0918/LsYSFyg909xJjyhnCxeCj.png)\n\
    \nI pretty sure that I have my cudatoolkit installted, do you have any clue about\
    \ the problerm?\nAgain, thanks for your work and hope for your reply."
  created_at: 2023-07-03 13:56:18+00:00
  edited: false
  hidden: false
  id: 64a2e192b470c1b02ae33464
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T09:32:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8194733262062073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Firstly, just to check you''re running this on a system with an
          Nvidia GPU available, with at least 48GB VRAM? </p>

          <p>If so, the first problem is that the CUDA extension is not installed.  Please
          try re-installing auto-gptq with:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install auto-gptq

          </code></pre>

          <p>Not sure about the rest, let''s see if installing AutoGPTQ with the CUDA
          module available fixes that first.</p>

          '
        raw: "Firstly, just to check you're running this on a system with an Nvidia\
          \ GPU available, with at least 48GB VRAM? \n\nIf so, the first problem is\
          \ that the CUDA extension is not installed.  Please try re-installing auto-gptq\
          \ with:\n```\npip3 uninstall -y auto-gptq\nGITHUB_ACTIONS=true pip3 install\
          \ auto-gptq\n```\n\nNot sure about the rest, let's see if installing AutoGPTQ\
          \ with the CUDA module available fixes that first."
        updatedAt: '2023-07-08T09:32:22.718Z'
      numEdits: 0
      reactions: []
    id: 64a92d26e04e7f92244a3259
    type: comment
  author: TheBloke
  content: "Firstly, just to check you're running this on a system with an Nvidia\
    \ GPU available, with at least 48GB VRAM? \n\nIf so, the first problem is that\
    \ the CUDA extension is not installed.  Please try re-installing auto-gptq with:\n\
    ```\npip3 uninstall -y auto-gptq\nGITHUB_ACTIONS=true pip3 install auto-gptq\n\
    ```\n\nNot sure about the rest, let's see if installing AutoGPTQ with the CUDA\
    \ module available fixes that first."
  created_at: 2023-07-08 08:32:22+00:00
  edited: false
  hidden: false
  id: 64a92d26e04e7f92244a3259
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91ff1eb3e4cda3ddbc182f5565d5f41c.svg
      fullname: hzg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hzg0601
      type: user
    createdAt: '2023-07-09T09:41:06.000Z'
    data:
      edited: false
      editors:
      - hzg0601
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8138675689697266
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91ff1eb3e4cda3ddbc182f5565d5f41c.svg
          fullname: hzg
          isHf: false
          isPro: false
          name: hzg0601
          type: user
        html: '<blockquote>

          <p>Firstly, just to check you''re running this on a system with an Nvidia
          GPU available, with at least 48GB VRAM? </p>

          <p>If so, the first problem is that the CUDA extension is not installed.  Please
          try re-installing auto-gptq with:</p>

          <pre><code>pip3 uninstall -y auto-gptq

          GITHUB_ACTIONS=true pip3 install auto-gptq

          </code></pre>

          <p>Not sure about the rest, let''s see if installing AutoGPTQ with the CUDA
          module available fixes that first.</p>

          </blockquote>

          <p>Thank you so much</p>

          '
        raw: "> Firstly, just to check you're running this on a system with an Nvidia\
          \ GPU available, with at least 48GB VRAM? \n> \n> If so, the first problem\
          \ is that the CUDA extension is not installed.  Please try re-installing\
          \ auto-gptq with:\n> ```\n> pip3 uninstall -y auto-gptq\n> GITHUB_ACTIONS=true\
          \ pip3 install auto-gptq\n> ```\n> \n> Not sure about the rest, let's see\
          \ if installing AutoGPTQ with the CUDA module available fixes that first.\n\
          \nThank you so much"
        updatedAt: '2023-07-09T09:41:06.993Z'
      numEdits: 0
      reactions: []
    id: 64aa80b281dda481726895ea
    type: comment
  author: hzg0601
  content: "> Firstly, just to check you're running this on a system with an Nvidia\
    \ GPU available, with at least 48GB VRAM? \n> \n> If so, the first problem is\
    \ that the CUDA extension is not installed.  Please try re-installing auto-gptq\
    \ with:\n> ```\n> pip3 uninstall -y auto-gptq\n> GITHUB_ACTIONS=true pip3 install\
    \ auto-gptq\n> ```\n> \n> Not sure about the rest, let's see if installing AutoGPTQ\
    \ with the CUDA module available fixes that first.\n\nThank you so much"
  created_at: 2023-07-09 08:41:06+00:00
  edited: false
  hidden: false
  id: 64aa80b281dda481726895ea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Not able to load via transformers '
