!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tarun1986
conflicting_files: null
created_at: 2023-06-04 07:02:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a8fcbc35a348f47cd192f394aeaa72f.svg
      fullname: Tarun Mishra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tarun1986
      type: user
    createdAt: '2023-06-04T08:02:12.000Z'
    data:
      edited: true
      editors:
      - Tarun1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6493786573410034
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a8fcbc35a348f47cd192f394aeaa72f.svg
          fullname: Tarun Mishra
          isHf: false
          isPro: false
          name: Tarun1986
          type: user
        html: '<p>I am wandering, can we fine tune guanaco using QLora and run the
          finetuned model on CPUs using llama.cpp? The missing piece is how to convert
          the fine tuned model to 4bit format which llama.cpp can run?</p>

          '
        raw: I am wandering, can we fine tune guanaco using QLora and run the finetuned
          model on CPUs using llama.cpp? The missing piece is how to convert the fine
          tuned model to 4bit format which llama.cpp can run?
        updatedAt: '2023-06-04T08:03:13.130Z'
      numEdits: 1
      reactions: []
    id: 647c4504d412b3b37655e410
    type: comment
  author: Tarun1986
  content: I am wandering, can we fine tune guanaco using QLora and run the finetuned
    model on CPUs using llama.cpp? The missing piece is how to convert the fine tuned
    model to 4bit format which llama.cpp can run?
  created_at: 2023-06-04 07:02:12+00:00
  edited: true
  hidden: false
  id: 647c4504d412b3b37655e410
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-04T09:48:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9421677589416504
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>To use QLoRA you should start with my fp16 repo of this model, TheBloke/guanaco-65B-HF.  You
          can''t run QLoRA on an already-quantised model like this. </p>

          <p>At least not yet. There is a PR in the AutoGPTQ repo to add PEFT/LoRA
          support to AutoGPTQ.  That would allow fine tuning on GPTQ models.  But
          I don''t think it''s quite finished yet.  Check the Pull Requests on the
          AutoGPTQ repo for more info.</p>

          <p>As to running QLoRA from TheBloke/guanaco-65B-HF: I don''t yet have experience
          of using QLoRA personally, but here''s an introductory video: <a rel="nofollow"
          href="https://youtu.be/8vmWGX1nfNM">https://youtu.be/8vmWGX1nfNM</a></p>

          <p>If you have further questions, then come to my Discord. Aemon, the creator
          of that video, is on my Discord and will be willing to help further if he''s
          around.</p>

          '
        raw: "To use QLoRA you should start with my fp16 repo of this model, TheBloke/guanaco-65B-HF.\
          \  You can't run QLoRA on an already-quantised model like this. \n\nAt least\
          \ not yet. There is a PR in the AutoGPTQ repo to add PEFT/LoRA support to\
          \ AutoGPTQ.  That would allow fine tuning on GPTQ models.  But I don't think\
          \ it's quite finished yet.  Check the Pull Requests on the AutoGPTQ repo\
          \ for more info.\n\nAs to running QLoRA from TheBloke/guanaco-65B-HF: I\
          \ don't yet have experience of using QLoRA personally, but here's an introductory\
          \ video: https://youtu.be/8vmWGX1nfNM\n\nIf you have further questions,\
          \ then come to my Discord. Aemon, the creator of that video, is on my Discord\
          \ and will be willing to help further if he's around."
        updatedAt: '2023-06-04T09:48:51.390Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Ichsan2895
        - mikolodz
    id: 647c5e0360dfe0f35d4c5506
    type: comment
  author: TheBloke
  content: "To use QLoRA you should start with my fp16 repo of this model, TheBloke/guanaco-65B-HF.\
    \  You can't run QLoRA on an already-quantised model like this. \n\nAt least not\
    \ yet. There is a PR in the AutoGPTQ repo to add PEFT/LoRA support to AutoGPTQ.\
    \  That would allow fine tuning on GPTQ models.  But I don't think it's quite\
    \ finished yet.  Check the Pull Requests on the AutoGPTQ repo for more info.\n\
    \nAs to running QLoRA from TheBloke/guanaco-65B-HF: I don't yet have experience\
    \ of using QLoRA personally, but here's an introductory video: https://youtu.be/8vmWGX1nfNM\n\
    \nIf you have further questions, then come to my Discord. Aemon, the creator of\
    \ that video, is on my Discord and will be willing to help further if he's around."
  created_at: 2023-06-04 08:48:51+00:00
  edited: false
  hidden: false
  id: 647c5e0360dfe0f35d4c5506
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: how to fine tune guanaco and run on multi core cpu (>32 cores)?
