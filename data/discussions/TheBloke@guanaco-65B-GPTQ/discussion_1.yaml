!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Thireus
conflicting_files: null
created_at: 2023-05-25 19:54:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-05-25T20:54:09.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>I was just reading about Guanaco via <a rel="nofollow" href="https://github.com/artidoro/qlora">https://github.com/artidoro/qlora</a>
          &amp; <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/13r7pzg/gptqlora_efficient_finetuning_of_quantized_llms/">https://www.reddit.com/r/LocalLLaMA/comments/13r7pzg/gptqlora_efficient_finetuning_of_quantized_llms/</a>.
          And you''re already dropping the quantized models. Really impressive dedication!
          Keep up the good work.</p>

          '
        raw: I was just reading about Guanaco via https://github.com/artidoro/qlora
          & https://www.reddit.com/r/LocalLLaMA/comments/13r7pzg/gptqlora_efficient_finetuning_of_quantized_llms/.
          And you're already dropping the quantized models. Really impressive dedication!
          Keep up the good work.
        updatedAt: '2023-05-25T20:54:09.012Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - Yhyu13
        - PikolaNikola
        - P4l1ndr0m
        - amitj
    id: 646fcaf1dcddb2358a27195a
    type: comment
  author: Thireus
  content: I was just reading about Guanaco via https://github.com/artidoro/qlora
    & https://www.reddit.com/r/LocalLLaMA/comments/13r7pzg/gptqlora_efficient_finetuning_of_quantized_llms/.
    And you're already dropping the quantized models. Really impressive dedication!
    Keep up the good work.
  created_at: 2023-05-25 19:54:09+00:00
  edited: false
  hidden: false
  id: 646fcaf1dcddb2358a27195a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-05-25T21:26:07.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p>TopBloke!</p>

          '
        raw: TopBloke!
        updatedAt: '2023-05-25T21:26:07.467Z'
      numEdits: 0
      reactions:
      - count: 10
        reaction: "\U0001F44D"
        users:
        - Thireus
        - ShadowZ12
        - nichedreams
        - Yhyu13
        - PikolaNikola
        - abhishekbhakat
        - Crytoma
        - CodingJunkie
        - Biogoly
        - levoniust
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
        - ShadowZ12
        - Yhyu13
        - gmoshiko
        - abhishekbhakat
        - CodingJunkie
    id: 646fd26f6098ee820fc091ea
    type: comment
  author: disarmyouwitha
  content: TopBloke!
  created_at: 2023-05-25 20:26:07+00:00
  edited: false
  hidden: false
  id: 646fd26f6098ee820fc091ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45dc2b89009a8673a5d84bb2adb93614.svg
      fullname: Alain Rossmann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alain401
      type: user
    createdAt: '2023-05-25T22:39:27.000Z'
    data:
      edited: true
      editors:
      - alain401
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45dc2b89009a8673a5d84bb2adb93614.svg
          fullname: Alain Rossmann
          isHf: false
          isPro: false
          name: alain401
          type: user
        html: '<p>File name refers to 65b based model, model card info says 33b.</p>

          '
        raw: File name refers to 65b based model, model card info says 33b.
        updatedAt: '2023-05-25T22:39:47.350Z'
      numEdits: 1
      reactions: []
    id: 646fe39f150f4cab86340286
    type: comment
  author: alain401
  content: File name refers to 65b based model, model card info says 33b.
  created_at: 2023-05-25 21:39:27+00:00
  edited: true
  hidden: false
  id: 646fe39f150f4cab86340286
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-25T22:42:48.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks guys! </p>

          <blockquote>

          <p>File name refers to 65b based model, model card info says 33b.</p>

          </blockquote>

          <p>Thanks, fixed</p>

          '
        raw: "Thanks guys! \n\n> File name refers to 65b based model, model card info\
          \ says 33b.\n\nThanks, fixed"
        updatedAt: '2023-05-25T22:42:48.016Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Crytoma
    id: 646fe468d1f1b73079eeaad0
    type: comment
  author: TheBloke
  content: "Thanks guys! \n\n> File name refers to 65b based model, model card info\
    \ says 33b.\n\nThanks, fixed"
  created_at: 2023-05-25 21:42:48+00:00
  edited: false
  hidden: false
  id: 646fe468d1f1b73079eeaad0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e61122ee4169f13b16cd457ecf0bff8.svg
      fullname: Fiorini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AndersonFiorini
      type: user
    createdAt: '2023-05-27T22:38:26.000Z'
    data:
      edited: false
      editors:
      - AndersonFiorini
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e61122ee4169f13b16cd457ecf0bff8.svg
          fullname: Fiorini
          isHf: false
          isPro: false
          name: AndersonFiorini
          type: user
        html: '<p>The model card informs us that it is possible to run with under
          24GB of VRAM. Is that correct?</p>

          '
        raw: The model card informs us that it is possible to run with under 24GB
          of VRAM. Is that correct?
        updatedAt: '2023-05-27T22:38:26.613Z'
      numEdits: 0
      reactions: []
    id: 647286625afd6a69658d7c80
    type: comment
  author: AndersonFiorini
  content: The model card informs us that it is possible to run with under 24GB of
    VRAM. Is that correct?
  created_at: 2023-05-27 21:38:26+00:00
  edited: false
  hidden: false
  id: 647286625afd6a69658d7c80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-27T22:49:59.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry that was a copy and paste error.  It''s not possible to load
          a 65B model in under 24GB VRAM - you can with a 30B/33B model.</p>

          <p>To fully load a 65B model into VRAM needs 48GB VRAM, either 1 x 48GB
          GPU (eg L40 or A6000), or 2 x 24GB GPU (eg 4090 or 3090).  Or you can use
          CPU offloading, but that''s a lot slower.</p>

          <p>Or you could try a GGML model, with CUDA acceleration. Then you can load
          as many layers as possible to the GPU - probably around 50-60 - and performance
          will be much faster than it would be with GPTQ 65B on a 24GB card (but probably
          not as fast as a 65B model on 2 x 24GB GPUs, or a 33B GPTQ model on 1 x
          24GB)</p>

          '
        raw: 'Sorry that was a copy and paste error.  It''s not possible to load a
          65B model in under 24GB VRAM - you can with a 30B/33B model.


          To fully load a 65B model into VRAM needs 48GB VRAM, either 1 x 48GB GPU
          (eg L40 or A6000), or 2 x 24GB GPU (eg 4090 or 3090).  Or you can use CPU
          offloading, but that''s a lot slower.


          Or you could try a GGML model, with CUDA acceleration. Then you can load
          as many layers as possible to the GPU - probably around 50-60 - and performance
          will be much faster than it would be with GPTQ 65B on a 24GB card (but probably
          not as fast as a 65B model on 2 x 24GB GPUs, or a 33B GPTQ model on 1 x
          24GB)'
        updatedAt: '2023-05-27T22:52:16.392Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Rhystz
        - AndersonFiorini
    id: 647289170211f852700ce4bb
    type: comment
  author: TheBloke
  content: 'Sorry that was a copy and paste error.  It''s not possible to load a 65B
    model in under 24GB VRAM - you can with a 30B/33B model.


    To fully load a 65B model into VRAM needs 48GB VRAM, either 1 x 48GB GPU (eg L40
    or A6000), or 2 x 24GB GPU (eg 4090 or 3090).  Or you can use CPU offloading,
    but that''s a lot slower.


    Or you could try a GGML model, with CUDA acceleration. Then you can load as many
    layers as possible to the GPU - probably around 50-60 - and performance will be
    much faster than it would be with GPTQ 65B on a 24GB card (but probably not as
    fast as a 65B model on 2 x 24GB GPUs, or a 33B GPTQ model on 1 x 24GB)'
  created_at: 2023-05-27 21:49:59+00:00
  edited: true
  hidden: false
  id: 647289170211f852700ce4bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
      fullname: Pluck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BGLuck
      type: user
    createdAt: '2023-05-28T01:48:46.000Z'
    data:
      edited: false
      editors:
      - BGLuck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
          fullname: Pluck
          isHf: false
          isPro: false
          name: BGLuck
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Do you know if\
          \ GGML supports 2 GPUs? I have 2x4090 and while 65B loads, my max context\
          \ is about 1300 size (or 1700 with multigen) with groupsize 128, or 1500-1800\
          \ with g128. With some layers on the CPU I'm able to do it with pre_layer\
          \ at 2048 context, but it is a lot slower. GGML should be faster in that\
          \ case.</p>\n"
        raw: '@TheBloke Do you know if GGML supports 2 GPUs? I have 2x4090 and while
          65B loads, my max context is about 1300 size (or 1700 with multigen) with
          groupsize 128, or 1500-1800 with g128. With some layers on the CPU I''m
          able to do it with pre_layer at 2048 context, but it is a lot slower. GGML
          should be faster in that case.'
        updatedAt: '2023-05-28T01:48:46.238Z'
      numEdits: 0
      reactions: []
    id: 6472b2fe97a75cc77abd50d8
    type: comment
  author: BGLuck
  content: '@TheBloke Do you know if GGML supports 2 GPUs? I have 2x4090 and while
    65B loads, my max context is about 1300 size (or 1700 with multigen) with groupsize
    128, or 1500-1800 with g128. With some layers on the CPU I''m able to do it with
    pre_layer at 2048 context, but it is a lot slower. GGML should be faster in that
    case.'
  created_at: 2023-05-28 00:48:46+00:00
  edited: false
  hidden: false
  id: 6472b2fe97a75cc77abd50d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-28T02:32:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BGLuck&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BGLuck\">@<span class=\"\
          underline\">BGLuck</span></a></span>\n\n\t</span></span> Not yet, no.  But\
          \ judging by the rate they're adding new features, I wouldn't be at all\
          \ surprised if it was added soon. </p>\n<p>Regarding 2 x 4090 with 65B -\
          \ are you using CUDA or Triton?  If CUDA, try Triton. I found it can lower\
          \ VRAM requirements.  Also, you have to get the layers balanced and that\
          \ can take some fine tuning.  You need to put fewer layers on GPU 1 because\
          \ that also runs the context.</p>\n<p>A few days ago I was able to get a\
          \ full 2000 context out of a 65B model with 2 x 4090.  I can't remember\
          \ the exact details but I can try again soon.</p>\n<p>But in order to achieve\
          \ that it needs to be a no-groupsize model. All my 65Bs have group_size\
          \ = none (-1).  128 will use a lot more VRAM.</p>\n"
        raw: "@BGLuck Not yet, no.  But judging by the rate they're adding new features,\
          \ I wouldn't be at all surprised if it was added soon. \n\nRegarding 2 x\
          \ 4090 with 65B - are you using CUDA or Triton?  If CUDA, try Triton. I\
          \ found it can lower VRAM requirements.  Also, you have to get the layers\
          \ balanced and that can take some fine tuning.  You need to put fewer layers\
          \ on GPU 1 because that also runs the context.\n\nA few days ago I was able\
          \ to get a full 2000 context out of a 65B model with 2 x 4090.  I can't\
          \ remember the exact details but I can try again soon.\n\nBut in order to\
          \ achieve that it needs to be a no-groupsize model. All my 65Bs have group_size\
          \ = none (-1).  128 will use a lot more VRAM."
        updatedAt: '2023-05-28T02:32:02.428Z'
      numEdits: 0
      reactions: []
    id: 6472bd2297a75cc77abe1a4d
    type: comment
  author: TheBloke
  content: "@BGLuck Not yet, no.  But judging by the rate they're adding new features,\
    \ I wouldn't be at all surprised if it was added soon. \n\nRegarding 2 x 4090\
    \ with 65B - are you using CUDA or Triton?  If CUDA, try Triton. I found it can\
    \ lower VRAM requirements.  Also, you have to get the layers balanced and that\
    \ can take some fine tuning.  You need to put fewer layers on GPU 1 because that\
    \ also runs the context.\n\nA few days ago I was able to get a full 2000 context\
    \ out of a 65B model with 2 x 4090.  I can't remember the exact details but I\
    \ can try again soon.\n\nBut in order to achieve that it needs to be a no-groupsize\
    \ model. All my 65Bs have group_size = none (-1).  128 will use a lot more VRAM."
  created_at: 2023-05-28 01:32:02+00:00
  edited: false
  hidden: false
  id: 6472bd2297a75cc77abe1a4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
      fullname: Pluck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BGLuck
      type: user
    createdAt: '2023-05-28T03:58:18.000Z'
    data:
      edited: false
      editors:
      - BGLuck
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bc5b43c98dde99fc599771c0bc1393.svg
          fullname: Pluck
          isHf: false
          isPro: false
          name: BGLuck
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I'm on Windows,\
          \ so sadly I'm only on the CUDA branch. I can test in another drive with\
          \ Linux.</p>\n<p>I do about 16/21 on each GPU. Gonna keep testing though.</p>\n"
        raw: '@TheBloke I''m on Windows, so sadly I''m only on the CUDA branch. I
          can test in another drive with Linux.


          I do about 16/21 on each GPU. Gonna keep testing though.'
        updatedAt: '2023-05-28T03:58:18.689Z'
      numEdits: 0
      reactions: []
    id: 6472d15a97a75cc77abff353
    type: comment
  author: BGLuck
  content: '@TheBloke I''m on Windows, so sadly I''m only on the CUDA branch. I can
    test in another drive with Linux.


    I do about 16/21 on each GPU. Gonna keep testing though.'
  created_at: 2023-05-28 02:58:18+00:00
  edited: false
  hidden: false
  id: 6472d15a97a75cc77abff353
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b81b8593506370e4f3c2e0cea2929c7.svg
      fullname: balaji
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balajivantari
      type: user
    createdAt: '2023-06-12T04:24:39.000Z'
    data:
      edited: false
      editors:
      - balajivantari
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6027564406394958
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b81b8593506370e4f3c2e0cea2929c7.svg
          fullname: balaji
          isHf: false
          isPro: false
          name: balajivantari
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Thireus&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Thireus\">@<span class=\"\
          underline\">Thireus</span></a></span>\n\n\t</span></span> bro how did you\
          \ load this model, </p>\n<p>i followed </p>\n<p>from transformers import\
          \ AutoTokenizer, pipeline, logging<br>from auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig<br>import argparse</p>\n<p>model_name_or_path = \"\
          TheBloke/guanaco-65B-GPTQ\"<br>model_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\
          </p>\n<p>use_triton = False</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)</p>\n<p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>model_basename=model_basename,<br>use_safetensors=True,<br>trust_remote_code=True,<br>device=\"\
          cuda:0\",<br>use_triton=use_triton,<br>quantize_config=None)</p>\n<p>prompt\
          \ = \"who is the first president of US\"<br>prompt_template=f'''### Instruction:\
          \ {prompt}</p>\n<p>Response:'''<br>pipe = pipeline(<br>\"text-generation\"\
          ,<br>model=model,<br>tokenizer=tokenizer,<br>max_new_tokens=512,<br>temperature=0.7,<br>top_p=0.95,<br>repetition_penalty=1.15<br>)</p>\n\
          <p>print(pipe(prompt_template)[0]['generated_text'])</p>\n<p>it's giving\
          \ answers very slowly i mean the inference time,<br>can you suggest the\
          \ way to make it very fast</p>\n"
        raw: "@Thireus bro how did you load this model, \n\ni followed \n\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\
          \nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\n\nuse_triton = False\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\nmodel_basename=model_basename,\n\
          use_safetensors=True,\ntrust_remote_code=True,\ndevice=\"cuda:0\",\nuse_triton=use_triton,\n\
          quantize_config=None)\n\nprompt = \"who is the first president of US\"\n\
          prompt_template=f'''### Instruction: {prompt}\n\nResponse:'''\npipe = pipeline(\n\
          \"text-generation\",\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=512,\n\
          temperature=0.7,\ntop_p=0.95,\nrepetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
          \nit's giving answers very slowly i mean the inference time,\ncan you suggest\
          \ the way to make it very fast"
        updatedAt: '2023-06-12T04:24:39.426Z'
      numEdits: 0
      reactions: []
    id: 64869e07fb1399efc2083dc2
    type: comment
  author: balajivantari
  content: "@Thireus bro how did you load this model, \n\ni followed \n\nfrom transformers\
    \ import AutoTokenizer, pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM,\
    \ BaseQuantizeConfig\nimport argparse\n\nmodel_name_or_path = \"TheBloke/guanaco-65B-GPTQ\"\
    \nmodel_basename = \"Guanaco-65B-GPTQ-4bit.act-order\"\n\nuse_triton = False\n\
    \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\nmodel_basename=model_basename,\n\
    use_safetensors=True,\ntrust_remote_code=True,\ndevice=\"cuda:0\",\nuse_triton=use_triton,\n\
    quantize_config=None)\n\nprompt = \"who is the first president of US\"\nprompt_template=f'''###\
    \ Instruction: {prompt}\n\nResponse:'''\npipe = pipeline(\n\"text-generation\"\
    ,\nmodel=model,\ntokenizer=tokenizer,\nmax_new_tokens=512,\ntemperature=0.7,\n\
    top_p=0.95,\nrepetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nit's giving answers very slowly i mean the inference time,\ncan you suggest\
    \ the way to make it very fast"
  created_at: 2023-06-12 03:24:39+00:00
  edited: false
  hidden: false
  id: 64869e07fb1399efc2083dc2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/guanaco-65B-GPTQ
repo_type: model
status: open
target_branch: null
title: You're blazing fast!
