!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mantafloppy
conflicting_files: null
created_at: 2024-01-12 17:09:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6515e47ed467ffdc051cda43/cy9UP54WW9TfdPf-lNgDd.jpeg?w=200&h=200&f=face
      fullname: N D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mantafloppy
      type: user
    createdAt: '2024-01-12T17:09:35.000Z'
    data:
      edited: false
      editors:
      - mantafloppy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5725502371788025
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6515e47ed467ffdc051cda43/cy9UP54WW9TfdPf-lNgDd.jpeg?w=200&h=200&f=face
          fullname: N D
          isHf: false
          isPro: false
          name: mantafloppy
          type: user
        html: '<p>I made sure my Llama.cpp was up to date, i tried the q4_k_M and
          q8, same kind of error.</p>

          <p>Is it me or there an issu with the .gguf?</p>

          <p>Mac-Studio llama.cpp % ./main -ngl 35 -m ./models/whiterabbitneo-33b-v1.Q4_K_M.gguf
          --color -c 16384 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "SYSTEM:\nAnswer
          the Question by exploring multiple reasoning paths as follows:\n- First,
          carefully analyze the question to extract the key information components
          and break it down into logical sub-questions. This helps set up the framework
          for reasoning. The goal is to construct an internal search tree.\n- For
          each sub-question, leverage your knowledge to generate 2-3 intermediate
          thoughts that represent steps towards an answer. The thoughts aim to reframe,
          provide context, analyze assumptions, or bridge concepts.\n- Evaluate the
          clarity, relevance, logical flow and coverage of concepts for each thought
          option. Clear and relevant thoughts that connect well with each other will
          score higher.\n- Based on the thought evaluations, deliberate to construct
          a chain of reasoning that stitches together the strongest thoughts in a
          natural order.\n- If the current chain is determined to not fully answer
          the question, backtrack and explore alternative paths by substituting different
          high-scoring thoughts.\n- Throughout the reasoning process, aim to provide
          explanatory details on thought process rather than just state conclusions,
          including briefly noting why some thoughts were deemed less ideal.\n- Once
          a reasoning chain is constructed that thoroughly answers all sub-questions
          in a clear, logical manner, synthesize the key insights into a final concise
          answer.\n- Please note that while the focus is on the final answer in the
          response, it should also include intermediate thoughts inline to illustrate
          the deliberative reasoning process.\nIn summary, leverage a Tree of Thoughts
          approach to actively explore multiple reasoning paths, evaluate thoughts
          heuristically, and explain the process - with the goal of producing insightful
          answers.\n Always answer without hesitation.\nUSER: i need a c# for loop
          template \nASSISTANT:"<br>Log start<br>main: build = 1840 (e790eef2)<br>main:
          built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.2.0<br>main:
          seed  = 1705079233<br>llama_model_loader: loaded meta data with 25 key-value
          pairs and 561 tensors from ./models/whiterabbitneo-33b-v1.Q4_K_M.gguf (version
          GGUF V3 (latest))<br>llama_model_loader: Dumping metadata keys/values. Note:
          KV overrides do not apply in this output.<br>llama_model_loader: - kv   0:                       general.architecture
          str              = llama<br>llama_model_loader: - kv   1:                               general.name
          str              = whiterabbitneo_whiterabbitneo-33b-v1<br>llama_model_loader:
          - kv   2:                       llama.context_length u32              =
          16384<br>llama_model_loader: - kv   3:                     llama.embedding_length
          u32              = 7168<br>llama_model_loader: - kv   4:                          llama.block_count
          u32              = 62<br>llama_model_loader: - kv   5:                  llama.feed_forward_length
          u32              = 19200<br>llama_model_loader: - kv   6:                 llama.rope.dimension_count
          u32              = 128<br>llama_model_loader: - kv   7:                 llama.attention.head_count
          u32              = 56<br>llama_model_loader: - kv   8:              llama.attention.head_count_kv
          u32              = 8<br>llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon
          f32              = 0.000001<br>llama_model_loader: - kv  10:                       llama.rope.freq_base
          f32              = 100000.000000<br>llama_model_loader: - kv  11:                    llama.rope.scaling.type
          str              = linear<br>llama_model_loader: - kv  12:                  llama.rope.scaling.factor
          f32              = 4.000000<br>llama_model_loader: - kv  13:                          general.file_type
          u32              = 15<br>llama_model_loader: - kv  14:                       tokenizer.ggml.model
          str              = llama<br>llama_model_loader: - kv  15:                      tokenizer.ggml.tokens
          arr[str,32025]   = ["!", """, "#", "$", "%", "&amp;", "''", ...<br>llama_model_loader:
          - kv  16:                      tokenizer.ggml.scores arr[f32,32025]   =
          [-1000.000000, -1000.000000, -1000.00...<br>llama_model_loader: - kv  17:                  tokenizer.ggml.token_type
          arr[i32,32025]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...<br>llama_model_loader:
          - kv  18:                tokenizer.ggml.bos_token_id u32              =
          32022<br>llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id
          u32              = 32023<br>llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id
          u32              = 32024<br>llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id
          u32              = 32014<br>llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token
          bool             = true<br>llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token
          bool             = false<br>llama_model_loader: - kv  24:               general.quantization_version
          u32              = 2<br>llama_model_loader: - type  f32:  125 tensors<br>llama_model_loader:
          - type q4_K:  375 tensors<br>llama_model_loader: - type q6_K:   61 tensors<br>error
          loading model: unordered_map::at: key not found<br>llama_load_model_from_file:
          failed to load model<br>llama_init_from_gpt_params: error: failed to load
          model ''./models/whiterabbitneo-33b-v1.Q4_K_M.gguf''<br>main: error: unable
          to load model</p>

          '
        raw: "I made sure my Llama.cpp was up to date, i tried the q4_k_M and q8,\
          \ same kind of error.\r\n\r\nIs it me or there an issu with the .gguf?\r\
          \n\r\nMac-Studio llama.cpp % ./main -ngl 35 -m ./models/whiterabbitneo-33b-v1.Q4_K_M.gguf\
          \ --color -c 16384 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"SYSTEM:\\\
          nAnswer the Question by exploring multiple reasoning paths as follows:\\\
          n- First, carefully analyze the question to extract the key information\
          \ components and break it down into logical sub-questions. This helps set\
          \ up the framework for reasoning. The goal is to construct an internal search\
          \ tree.\\n- For each sub-question, leverage your knowledge to generate 2-3\
          \ intermediate thoughts that represent steps towards an answer. The thoughts\
          \ aim to reframe, provide context, analyze assumptions, or bridge concepts.\\\
          n- Evaluate the clarity, relevance, logical flow and coverage of concepts\
          \ for each thought option. Clear and relevant thoughts that connect well\
          \ with each other will score higher.\\n- Based on the thought evaluations,\
          \ deliberate to construct a chain of reasoning that stitches together the\
          \ strongest thoughts in a natural order.\\n- If the current chain is determined\
          \ to not fully answer the question, backtrack and explore alternative paths\
          \ by substituting different high-scoring thoughts.\\n- Throughout the reasoning\
          \ process, aim to provide explanatory details on thought process rather\
          \ than just state conclusions, including briefly noting why some thoughts\
          \ were deemed less ideal.\\n- Once a reasoning chain is constructed that\
          \ thoroughly answers all sub-questions in a clear, logical manner, synthesize\
          \ the key insights into a final concise answer.\\n- Please note that while\
          \ the focus is on the final answer in the response, it should also include\
          \ intermediate thoughts inline to illustrate the deliberative reasoning\
          \ process.\\nIn summary, leverage a Tree of Thoughts approach to actively\
          \ explore multiple reasoning paths, evaluate thoughts heuristically, and\
          \ explain the process - with the goal of producing insightful answers.\\\
          n Always answer without hesitation.\\nUSER: i need a c# for loop template\
          \ \\nASSISTANT:\"\r\nLog start\r\nmain: build = 1840 (e790eef2)\r\nmain:\
          \ built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.2.0\r\
          \nmain: seed  = 1705079233\r\nllama_model_loader: loaded meta data with\
          \ 25 key-value pairs and 561 tensors from ./models/whiterabbitneo-33b-v1.Q4_K_M.gguf\
          \ (version GGUF V3 (latest))\r\nllama_model_loader: Dumping metadata keys/values.\
          \ Note: KV overrides do not apply in this output.\r\nllama_model_loader:\
          \ - kv   0:                       general.architecture str             \
          \ = llama\r\nllama_model_loader: - kv   1:                             \
          \  general.name str              = whiterabbitneo_whiterabbitneo-33b-v1\r\
          \nllama_model_loader: - kv   2:                       llama.context_length\
          \ u32              = 16384\r\nllama_model_loader: - kv   3:            \
          \         llama.embedding_length u32              = 7168\r\nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32             \
          \ = 62\r\nllama_model_loader: - kv   5:                  llama.feed_forward_length\
          \ u32              = 19200\r\nllama_model_loader: - kv   6:            \
          \     llama.rope.dimension_count u32              = 128\r\nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32             \
          \ = 56\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
          \ u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000001\r\nllama_model_loader: - kv  10:         \
          \              llama.rope.freq_base f32              = 100000.000000\r\n\
          llama_model_loader: - kv  11:                    llama.rope.scaling.type\
          \ str              = linear\r\nllama_model_loader: - kv  12:           \
          \       llama.rope.scaling.factor f32              = 4.000000\r\nllama_model_loader:\
          \ - kv  13:                          general.file_type u32             \
          \ = 15\r\nllama_model_loader: - kv  14:                       tokenizer.ggml.model\
          \ str              = llama\r\nllama_model_loader: - kv  15:            \
          \          tokenizer.ggml.tokens arr[str,32025]   = [\"!\", \"\\\"\", \"\
          #\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: - kv  16:  \
          \                    tokenizer.ggml.scores arr[f32,32025]   = [-1000.000000,\
          \ -1000.000000, -1000.00...\r\nllama_model_loader: - kv  17:           \
          \       tokenizer.ggml.token_type arr[i32,32025]   = [1, 1, 1, 1, 1, 1,\
          \ 1, 1, 1, 1, 1, 1, ...\r\nllama_model_loader: - kv  18:               \
          \ tokenizer.ggml.bos_token_id u32              = 32022\r\nllama_model_loader:\
          \ - kv  19:                tokenizer.ggml.eos_token_id u32             \
          \ = 32023\r\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id\
          \ u32              = 32024\r\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id\
          \ u32              = 32014\r\nllama_model_loader: - kv  22:            \
          \   tokenizer.ggml.add_bos_token bool             = true\r\nllama_model_loader:\
          \ - kv  23:               tokenizer.ggml.add_eos_token bool            \
          \ = false\r\nllama_model_loader: - kv  24:               general.quantization_version\
          \ u32              = 2\r\nllama_model_loader: - type  f32:  125 tensors\r\
          \nllama_model_loader: - type q4_K:  375 tensors\r\nllama_model_loader: -\
          \ type q6_K:   61 tensors\r\nerror loading model: unordered_map::at: key\
          \ not found\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
          \ error: failed to load model './models/whiterabbitneo-33b-v1.Q4_K_M.gguf'\r\
          \nmain: error: unable to load model\r\n"
        updatedAt: '2024-01-12T17:09:35.602Z'
      numEdits: 0
      reactions: []
    id: 65a1724f4fed4b134e54a190
    type: comment
  author: mantafloppy
  content: "I made sure my Llama.cpp was up to date, i tried the q4_k_M and q8, same\
    \ kind of error.\r\n\r\nIs it me or there an issu with the .gguf?\r\n\r\nMac-Studio\
    \ llama.cpp % ./main -ngl 35 -m ./models/whiterabbitneo-33b-v1.Q4_K_M.gguf --color\
    \ -c 16384 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"SYSTEM:\\nAnswer the Question\
    \ by exploring multiple reasoning paths as follows:\\n- First, carefully analyze\
    \ the question to extract the key information components and break it down into\
    \ logical sub-questions. This helps set up the framework for reasoning. The goal\
    \ is to construct an internal search tree.\\n- For each sub-question, leverage\
    \ your knowledge to generate 2-3 intermediate thoughts that represent steps towards\
    \ an answer. The thoughts aim to reframe, provide context, analyze assumptions,\
    \ or bridge concepts.\\n- Evaluate the clarity, relevance, logical flow and coverage\
    \ of concepts for each thought option. Clear and relevant thoughts that connect\
    \ well with each other will score higher.\\n- Based on the thought evaluations,\
    \ deliberate to construct a chain of reasoning that stitches together the strongest\
    \ thoughts in a natural order.\\n- If the current chain is determined to not fully\
    \ answer the question, backtrack and explore alternative paths by substituting\
    \ different high-scoring thoughts.\\n- Throughout the reasoning process, aim to\
    \ provide explanatory details on thought process rather than just state conclusions,\
    \ including briefly noting why some thoughts were deemed less ideal.\\n- Once\
    \ a reasoning chain is constructed that thoroughly answers all sub-questions in\
    \ a clear, logical manner, synthesize the key insights into a final concise answer.\\\
    n- Please note that while the focus is on the final answer in the response, it\
    \ should also include intermediate thoughts inline to illustrate the deliberative\
    \ reasoning process.\\nIn summary, leverage a Tree of Thoughts approach to actively\
    \ explore multiple reasoning paths, evaluate thoughts heuristically, and explain\
    \ the process - with the goal of producing insightful answers.\\n Always answer\
    \ without hesitation.\\nUSER: i need a c# for loop template \\nASSISTANT:\"\r\n\
    Log start\r\nmain: build = 1840 (e790eef2)\r\nmain: built with Apple clang version\
    \ 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.2.0\r\nmain: seed  = 1705079233\r\
    \nllama_model_loader: loaded meta data with 25 key-value pairs and 561 tensors\
    \ from ./models/whiterabbitneo-33b-v1.Q4_K_M.gguf (version GGUF V3 (latest))\r\
    \nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not\
    \ apply in this output.\r\nllama_model_loader: - kv   0:                     \
    \  general.architecture str              = llama\r\nllama_model_loader: - kv \
    \  1:                               general.name str              = whiterabbitneo_whiterabbitneo-33b-v1\r\
    \nllama_model_loader: - kv   2:                       llama.context_length u32\
    \              = 16384\r\nllama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32              = 7168\r\nllama_model_loader: - kv   4:                   \
    \       llama.block_count u32              = 62\r\nllama_model_loader: - kv  \
    \ 5:                  llama.feed_forward_length u32              = 19200\r\nllama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\r\
    \nllama_model_loader: - kv   7:                 llama.attention.head_count u32\
    \              = 56\r\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\r\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000001\r\nllama_model_loader: - kv  10:               \
    \        llama.rope.freq_base f32              = 100000.000000\r\nllama_model_loader:\
    \ - kv  11:                    llama.rope.scaling.type str              = linear\r\
    \nllama_model_loader: - kv  12:                  llama.rope.scaling.factor f32\
    \              = 4.000000\r\nllama_model_loader: - kv  13:                   \
    \       general.file_type u32              = 15\r\nllama_model_loader: - kv  14:\
    \                       tokenizer.ggml.model str              = llama\r\nllama_model_loader:\
    \ - kv  15:                      tokenizer.ggml.tokens arr[str,32025]   = [\"\
    !\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\r\nllama_model_loader: -\
    \ kv  16:                      tokenizer.ggml.scores arr[f32,32025]   = [-1000.000000,\
    \ -1000.000000, -1000.00...\r\nllama_model_loader: - kv  17:                 \
    \ tokenizer.ggml.token_type arr[i32,32025]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\
    \ 1, 1, ...\r\nllama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id\
    \ u32              = 32022\r\nllama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id\
    \ u32              = 32023\r\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id\
    \ u32              = 32024\r\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id\
    \ u32              = 32014\r\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token\
    \ bool             = true\r\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token\
    \ bool             = false\r\nllama_model_loader: - kv  24:               general.quantization_version\
    \ u32              = 2\r\nllama_model_loader: - type  f32:  125 tensors\r\nllama_model_loader:\
    \ - type q4_K:  375 tensors\r\nllama_model_loader: - type q6_K:   61 tensors\r\
    \nerror loading model: unordered_map::at: key not found\r\nllama_load_model_from_file:\
    \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load model\
    \ './models/whiterabbitneo-33b-v1.Q4_K_M.gguf'\r\nmain: error: unable to load\
    \ model\r\n"
  created_at: 2024-01-12 17:09:35+00:00
  edited: false
  hidden: false
  id: 65a1724f4fed4b134e54a190
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f731c7d36951307fcca6bf/gAliHsOW6vufxU6yXFCVF.jpeg?w=200&h=200&f=face
      fullname: Mitko Vasilev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mitkox
      type: user
    createdAt: '2024-01-12T18:59:42.000Z'
    data:
      edited: false
      editors:
      - mitkox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2090548574924469
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f731c7d36951307fcca6bf/gAliHsOW6vufxU6yXFCVF.jpeg?w=200&h=200&f=face
          fullname: Mitko Vasilev
          isHf: false
          isPro: false
          name: mitkox
          type: user
        html: '<p>I have complied my binary today and doesn''t work for me either.
          It''s the file conversion problem </p>

          <p>main: build = 1842 (584d674)<br>main: built with Apple clang version
          15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0<br>main: seed  =
          1705085828<br>llama_model_loader: loaded meta data with 25 key-value pairs
          and 561 tensors from models/whiterabbitneo-33b-v1.Q4_K_M.gguf (version GGUF
          V3 (latest))<br>llama_model_loader: Dumping metadata keys/values. Note:
          KV overrides do not apply in this output.<br>llama_model_loader: - kv   0:                       general.architecture
          str              = llama<br>llama_model_loader: - kv   1:                               general.name
          str              = whiterabbitneo_whiterabbitneo-33b-v1<br>llama_model_loader:
          - kv   2:                       llama.context_length u32              =
          16384<br>llama_model_loader: - kv   3:                     llama.embedding_length
          u32              = 7168<br>llama_model_loader: - kv   4:                          llama.block_count
          u32              = 62<br>llama_model_loader: - kv   5:                  llama.feed_forward_length
          u32              = 19200<br>llama_model_loader: - kv   6:                 llama.rope.dimension_count
          u32              = 128<br>llama_model_loader: - kv   7:                 llama.attention.head_count
          u32              = 56<br>llama_model_loader: - kv   8:              llama.attention.head_count_kv
          u32              = 8<br>llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon
          f32              = 0.000001<br>llama_model_loader: - kv  10:                       llama.rope.freq_base
          f32              = 100000.000000<br>llama_model_loader: - kv  11:                    llama.rope.scaling.type
          str              = linear<br>llama_model_loader: - kv  12:                  llama.rope.scaling.factor
          f32              = 4.000000<br>llama_model_loader: - kv  13:                          general.file_type
          u32              = 15<br>llama_model_loader: - kv  14:                       tokenizer.ggml.model
          str              = llama<br>llama_model_loader: - kv  15:                      tokenizer.ggml.tokens
          arr[str,32025]   = ["!", """, "#", "$", "%", "&amp;", "''", ...<br>llama_model_loader:
          - kv  16:                      tokenizer.ggml.scores arr[f32,32025]   =
          [-1000.000000, -1000.000000, -1000.00...<br>llama_model_loader: - kv  17:                  tokenizer.ggml.token_type
          arr[i32,32025]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...<br>llama_model_loader:
          - kv  18:                tokenizer.ggml.bos_token_id u32              =
          32022<br>llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id
          u32              = 32023<br>llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id
          u32              = 32024<br>llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id
          u32              = 32014<br>llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token
          bool             = true<br>llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token
          bool             = false<br>llama_model_loader: - kv  24:               general.quantization_version
          u32              = 2<br>llama_model_loader: - type  f32:  125 tensors<br>llama_model_loader:
          - type q4_K:  375 tensors<br>llama_model_loader: - type q6_K:   61 tensors<br>error
          loading model: unordered_map::at: key not found<br>llama_load_model_from_file:
          failed to load model<br>llama_init_from_gpt_params: error: failed to load
          model ''models/whiterabbitneo-33b-v1.Q4_K_M.gguf''<br>main: error: unable
          to load model</p>

          '
        raw: "I have complied my binary today and doesn't work for me either. It's\
          \ the file conversion problem \n\nmain: build = 1842 (584d674)\nmain: built\
          \ with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0\n\
          main: seed  = 1705085828\nllama_model_loader: loaded meta data with 25 key-value\
          \ pairs and 561 tensors from models/whiterabbitneo-33b-v1.Q4_K_M.gguf (version\
          \ GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note:\
          \ KV overrides do not apply in this output.\nllama_model_loader: - kv  \
          \ 0:                       general.architecture str              = llama\n\
          llama_model_loader: - kv   1:                               general.name\
          \ str              = whiterabbitneo_whiterabbitneo-33b-v1\nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32             \
          \ = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length\
          \ u32              = 7168\nllama_model_loader: - kv   4:               \
          \           llama.block_count u32              = 62\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32             \
          \ = 19200\nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
          \ u32              = 128\nllama_model_loader: - kv   7:                \
          \ llama.attention.head_count u32              = 56\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32             \
          \ = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
          \ f32              = 0.000001\nllama_model_loader: - kv  10:           \
          \            llama.rope.freq_base f32              = 100000.000000\nllama_model_loader:\
          \ - kv  11:                    llama.rope.scaling.type str             \
          \ = linear\nllama_model_loader: - kv  12:                  llama.rope.scaling.factor\
          \ f32              = 4.000000\nllama_model_loader: - kv  13:           \
          \               general.file_type u32              = 15\nllama_model_loader:\
          \ - kv  14:                       tokenizer.ggml.model str             \
          \ = llama\nllama_model_loader: - kv  15:                      tokenizer.ggml.tokens\
          \ arr[str,32025]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\"\
          , ...\nllama_model_loader: - kv  16:                      tokenizer.ggml.scores\
          \ arr[f32,32025]   = [-1000.000000, -1000.000000, -1000.00...\nllama_model_loader:\
          \ - kv  17:                  tokenizer.ggml.token_type arr[i32,32025]  \
          \ = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv \
          \ 18:                tokenizer.ggml.bos_token_id u32              = 32022\n\
          llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id\
          \ u32              = 32023\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id\
          \ u32              = 32024\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id\
          \ u32              = 32014\nllama_model_loader: - kv  22:              \
          \ tokenizer.ggml.add_bos_token bool             = true\nllama_model_loader:\
          \ - kv  23:               tokenizer.ggml.add_eos_token bool            \
          \ = false\nllama_model_loader: - kv  24:               general.quantization_version\
          \ u32              = 2\nllama_model_loader: - type  f32:  125 tensors\n\
          llama_model_loader: - type q4_K:  375 tensors\nllama_model_loader: - type\
          \ q6_K:   61 tensors\nerror loading model: unordered_map::at: key not found\n\
          llama_load_model_from_file: failed to load model\nllama_init_from_gpt_params:\
          \ error: failed to load model 'models/whiterabbitneo-33b-v1.Q4_K_M.gguf'\n\
          main: error: unable to load model"
        updatedAt: '2024-01-12T18:59:42.578Z'
      numEdits: 0
      reactions: []
    id: 65a18c1e3581a68c41a7d124
    type: comment
  author: mitkox
  content: "I have complied my binary today and doesn't work for me either. It's the\
    \ file conversion problem \n\nmain: build = 1842 (584d674)\nmain: built with Apple\
    \ clang version 15.0.0 (clang-1500.1.0.2.5) for arm64-apple-darwin23.2.0\nmain:\
    \ seed  = 1705085828\nllama_model_loader: loaded meta data with 25 key-value pairs\
    \ and 561 tensors from models/whiterabbitneo-33b-v1.Q4_K_M.gguf (version GGUF\
    \ V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides\
    \ do not apply in this output.\nllama_model_loader: - kv   0:                \
    \       general.architecture str              = llama\nllama_model_loader: - kv\
    \   1:                               general.name str              = whiterabbitneo_whiterabbitneo-33b-v1\n\
    llama_model_loader: - kv   2:                       llama.context_length u32 \
    \             = 16384\nllama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32              = 7168\nllama_model_loader: - kv   4:                     \
    \     llama.block_count u32              = 62\nllama_model_loader: - kv   5: \
    \                 llama.feed_forward_length u32              = 19200\nllama_model_loader:\
    \ - kv   6:                 llama.rope.dimension_count u32              = 128\n\
    llama_model_loader: - kv   7:                 llama.attention.head_count u32 \
    \             = 56\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32              = 0.000001\nllama_model_loader: - kv  10:                 \
    \      llama.rope.freq_base f32              = 100000.000000\nllama_model_loader:\
    \ - kv  11:                    llama.rope.scaling.type str              = linear\n\
    llama_model_loader: - kv  12:                  llama.rope.scaling.factor f32 \
    \             = 4.000000\nllama_model_loader: - kv  13:                      \
    \    general.file_type u32              = 15\nllama_model_loader: - kv  14:  \
    \                     tokenizer.ggml.model str              = llama\nllama_model_loader:\
    \ - kv  15:                      tokenizer.ggml.tokens arr[str,32025]   = [\"\
    !\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv\
    \  16:                      tokenizer.ggml.scores arr[f32,32025]   = [-1000.000000,\
    \ -1000.000000, -1000.00...\nllama_model_loader: - kv  17:                  tokenizer.ggml.token_type\
    \ arr[i32,32025]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader:\
    \ - kv  18:                tokenizer.ggml.bos_token_id u32              = 32022\n\
    llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32 \
    \             = 32023\nllama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id\
    \ u32              = 32024\nllama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id\
    \ u32              = 32014\nllama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token\
    \ bool             = true\nllama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token\
    \ bool             = false\nllama_model_loader: - kv  24:               general.quantization_version\
    \ u32              = 2\nllama_model_loader: - type  f32:  125 tensors\nllama_model_loader:\
    \ - type q4_K:  375 tensors\nllama_model_loader: - type q6_K:   61 tensors\nerror\
    \ loading model: unordered_map::at: key not found\nllama_load_model_from_file:\
    \ failed to load model\nllama_init_from_gpt_params: error: failed to load model\
    \ 'models/whiterabbitneo-33b-v1.Q4_K_M.gguf'\nmain: error: unable to load model"
  created_at: 2024-01-12 18:59:42+00:00
  edited: false
  hidden: false
  id: 65a18c1e3581a68c41a7d124
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/606d83f9648593213263268243738d68.svg
      fullname: Jeffrey Machado
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Isonium
      type: user
    createdAt: '2024-01-12T20:53:45.000Z'
    data:
      edited: false
      editors:
      - Isonium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7926706671714783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/606d83f9648593213263268243738d68.svg
          fullname: Jeffrey Machado
          isHf: false
          isPro: false
          name: Isonium
          type: user
        html: '<p>I get the same error on Q8_0 and Q5_K_M.</p>

          '
        raw: I get the same error on Q8_0 and Q5_K_M.
        updatedAt: '2024-01-12T20:53:45.069Z'
      numEdits: 0
      reactions: []
    id: 65a1a6d9f266d20b2e135231
    type: comment
  author: Isonium
  content: I get the same error on Q8_0 and Q5_K_M.
  created_at: 2024-01-12 20:53:45+00:00
  edited: false
  hidden: false
  id: 65a1a6d9f266d20b2e135231
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/606d83f9648593213263268243738d68.svg
      fullname: Jeffrey Machado
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Isonium
      type: user
    createdAt: '2024-01-13T02:18:37.000Z'
    data:
      edited: false
      editors:
      - Isonium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9439614415168762
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/606d83f9648593213263268243738d68.svg
          fullname: Jeffrey Machado
          isHf: false
          isPro: false
          name: Isonium
          type: user
        html: '<p>Something must be wrong in the convert.py of llama.cpp (latest release.)
          I went back to an older version (Late December 2023) of llama.cpp and was
          able to successfully convert WhiteRabbitNeo 33B v1 into a q8_0 GGUF file.</p>

          '
        raw: Something must be wrong in the convert.py of llama.cpp (latest release.)
          I went back to an older version (Late December 2023) of llama.cpp and was
          able to successfully convert WhiteRabbitNeo 33B v1 into a q8_0 GGUF file.
        updatedAt: '2024-01-13T02:18:37.835Z'
      numEdits: 0
      reactions: []
    id: 65a1f2fd90b5e87bcda1d919
    type: comment
  author: Isonium
  content: Something must be wrong in the convert.py of llama.cpp (latest release.)
    I went back to an older version (Late December 2023) of llama.cpp and was able
    to successfully convert WhiteRabbitNeo 33B v1 into a q8_0 GGUF file.
  created_at: 2024-01-13 02:18:37+00:00
  edited: false
  hidden: false
  id: 65a1f2fd90b5e87bcda1d919
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6515e47ed467ffdc051cda43/cy9UP54WW9TfdPf-lNgDd.jpeg?w=200&h=200&f=face
      fullname: N D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mantafloppy
      type: user
    createdAt: '2024-01-13T22:05:14.000Z'
    data:
      edited: false
      editors:
      - mantafloppy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8516525030136108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6515e47ed467ffdc051cda43/cy9UP54WW9TfdPf-lNgDd.jpeg?w=200&h=200&f=face
          fullname: N D
          isHf: false
          isPro: false
          name: mantafloppy
          type: user
        html: '<p>Until its fix, i uploaded mine : <a href="https://huggingface.co/mantafloppy/WhiteRabbitNeo-33B-v1-GGUF">https://huggingface.co/mantafloppy/WhiteRabbitNeo-33B-v1-GGUF</a></p>

          '
        raw: 'Until its fix, i uploaded mine : https://huggingface.co/mantafloppy/WhiteRabbitNeo-33B-v1-GGUF'
        updatedAt: '2024-01-13T22:05:14.196Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - GrennKren
        - mufeed
    id: 65a3091a534e60db99e40177
    type: comment
  author: mantafloppy
  content: 'Until its fix, i uploaded mine : https://huggingface.co/mantafloppy/WhiteRabbitNeo-33B-v1-GGUF'
  created_at: 2024-01-13 22:05:14+00:00
  edited: false
  hidden: false
  id: 65a3091a534e60db99e40177
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/606d83f9648593213263268243738d68.svg
      fullname: Jeffrey Machado
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Isonium
      type: user
    createdAt: '2024-01-15T05:32:48.000Z'
    data:
      edited: false
      editors:
      - Isonium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5369468331336975
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/606d83f9648593213263268243738d68.svg
          fullname: Jeffrey Machado
          isHf: false
          isPro: false
          name: Isonium
          type: user
        html: "<p>Working quantized model can be found here\u2026</p>\n<p> <a href=\"\
          https://huggingface.co/Isonium/WhiteRabbitNeo-33B-v1-GGUF/tree/main\">https://huggingface.co/Isonium/WhiteRabbitNeo-33B-v1-GGUF/tree/main</a></p>\n"
        raw: "Working quantized model can be found here\u2026\n\n https://huggingface.co/Isonium/WhiteRabbitNeo-33B-v1-GGUF/tree/main"
        updatedAt: '2024-01-15T05:32:48.956Z'
      numEdits: 0
      reactions: []
    id: 65a4c380224f96d8cc656bc7
    type: comment
  author: Isonium
  content: "Working quantized model can be found here\u2026\n\n https://huggingface.co/Isonium/WhiteRabbitNeo-33B-v1-GGUF/tree/main"
  created_at: 2024-01-15 05:32:48+00:00
  edited: false
  hidden: false
  id: 65a4c380224f96d8cc656bc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8c8668cb560fb151e3b8ea909dafe55.svg
      fullname: John Davies
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jtdavies
      type: user
    createdAt: '2024-01-18T18:43:17.000Z'
    data:
      edited: false
      editors:
      - jtdavies
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6336641311645508
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8c8668cb560fb151e3b8ea909dafe55.svg
          fullname: John Davies
          isHf: false
          isPro: false
          name: jtdavies
          type: user
        html: '<p>Same problem for me with the TheBloke/WhiteRabbitNeo-33B-v1-GGUF/whiterabbitneo-33b-v1.Q8_0.gguf,
          I got "unordered_map::at: key not found" with LM Studio.</p>

          '
        raw: 'Same problem for me with the TheBloke/WhiteRabbitNeo-33B-v1-GGUF/whiterabbitneo-33b-v1.Q8_0.gguf,
          I got "unordered_map::at: key not found" with LM Studio.'
        updatedAt: '2024-01-18T18:43:17.983Z'
      numEdits: 0
      reactions: []
    id: 65a97145230423404fb5b9d6
    type: comment
  author: jtdavies
  content: 'Same problem for me with the TheBloke/WhiteRabbitNeo-33B-v1-GGUF/whiterabbitneo-33b-v1.Q8_0.gguf,
    I got "unordered_map::at: key not found" with LM Studio.'
  created_at: 2024-01-18 18:43:17+00:00
  edited: false
  hidden: false
  id: 65a97145230423404fb5b9d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WhiteRabbitNeo-33B-v1-GGUF
repo_type: model
status: open
target_branch: null
title: Not able to run this model?
