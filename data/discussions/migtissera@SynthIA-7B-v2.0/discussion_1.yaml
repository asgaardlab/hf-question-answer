!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dyoung
conflicting_files: null
created_at: 2023-11-16 23:11:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-11-16T23:11:14.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.970146656036377
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>Hello,</p>

          <p>I was looking through the model card and in the quick use code example
          I noticed in the generate_text function something that caught my curiosity.<br>After
          the tokenization of the input prompt intended to be going to the model in
          GPU("tokens = tokenizer.encode(instruction)"),  the tokens are recast as
          longTensors (64-bit signed interreges tensors recast at "tokens = torch.LongTensor(tokens).unsqueeze(0)").<br>I''ve
          not seen a lot of others doing this with what I''ve seen so far in my Ai
          journey. I was curious as to what the reasoning why. I can speculate several
          reasons why. I figure it wouldn''t hurt if I ask directly. I''ll also be
          looking online. If you can, could you point me at any material I can look
          at that further supports why it''s smart to recast a tensor before sending
          off to the GPU, that would be appreciated. If you can''t or do not want
          to, that is understandable.</p>

          <p>Thank you for your time.</p>

          '
        raw: "Hello,\r\n\r\nI was looking through the model card and in the quick\
          \ use code example I noticed in the generate_text function something that\
          \ caught my curiosity.\r\nAfter the tokenization of the input prompt intended\
          \ to be going to the model in GPU(\"tokens = tokenizer.encode(instruction)\"\
          ),  the tokens are recast as longTensors (64-bit signed interreges tensors\
          \ recast at \"tokens = torch.LongTensor(tokens).unsqueeze(0)\").\r\nI've\
          \ not seen a lot of others doing this with what I've seen so far in my Ai\
          \ journey. I was curious as to what the reasoning why. I can speculate several\
          \ reasons why. I figure it wouldn't hurt if I ask directly. I'll also be\
          \ looking online. If you can, could you point me at any material I can look\
          \ at that further supports why it's smart to recast a tensor before sending\
          \ off to the GPU, that would be appreciated. If you can't or do not want\
          \ to, that is understandable.\r\n\r\nThank you for your time."
        updatedAt: '2023-11-16T23:11:14.225Z'
      numEdits: 0
      reactions: []
    id: 6556a1922de0aee1a11119b7
    type: comment
  author: dyoung
  content: "Hello,\r\n\r\nI was looking through the model card and in the quick use\
    \ code example I noticed in the generate_text function something that caught my\
    \ curiosity.\r\nAfter the tokenization of the input prompt intended to be going\
    \ to the model in GPU(\"tokens = tokenizer.encode(instruction)\"),  the tokens\
    \ are recast as longTensors (64-bit signed interreges tensors recast at \"tokens\
    \ = torch.LongTensor(tokens).unsqueeze(0)\").\r\nI've not seen a lot of others\
    \ doing this with what I've seen so far in my Ai journey. I was curious as to\
    \ what the reasoning why. I can speculate several reasons why. I figure it wouldn't\
    \ hurt if I ask directly. I'll also be looking online. If you can, could you point\
    \ me at any material I can look at that further supports why it's smart to recast\
    \ a tensor before sending off to the GPU, that would be appreciated. If you can't\
    \ or do not want to, that is understandable.\r\n\r\nThank you for your time."
  created_at: 2023-11-16 23:11:14+00:00
  edited: false
  hidden: false
  id: 6556a1922de0aee1a11119b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-11-16T23:45:01.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9700952768325806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>I think I may of answered what is going on. I''m seeing that "tokens
          = tokenizer.encode(instruction)" from the example code returns a common
          built in python list object/class. Which obviously doesn''t have the ability
          to be sent to the GPU as is (No ".to("cuda") method for the list class ...).
          So the recast with a pytorch long tensor is done in order to be able to
          copy the prompts tokens over to the GPU. And which is the class/data type
          expected for inference needs.<br>What I see more common is that people are
          using ''tokens = tokenizer(sentence, return_tensors="pt").to(device)''.
          Where a single statement is doing multiple steps in one line. So it looks
          like the example code is just breaking down the parts into different statements
          rather then packing them into one line. Seeing this being done differently
          caught my attention. And I was curious as to why. I''m happy with this explanation
          I''ve found for my self.<br>This thread could be considered close and I''d
          be ok with that.</p>

          '
        raw: 'I think I may of answered what is going on. I''m seeing that "tokens
          = tokenizer.encode(instruction)" from the example code returns a common
          built in python list object/class. Which obviously doesn''t have the ability
          to be sent to the GPU as is (No ".to("cuda") method for the list class ...).
          So the recast with a pytorch long tensor is done in order to be able to
          copy the prompts tokens over to the GPU. And which is the class/data type
          expected for inference needs.

          What I see more common is that people are using ''tokens = tokenizer(sentence,
          return_tensors="pt").to(device)''. Where a single statement is doing multiple
          steps in one line. So it looks like the example code is just breaking down
          the parts into different statements rather then packing them into one line.
          Seeing this being done differently caught my attention. And I was curious
          as to why. I''m happy with this explanation I''ve found for my self.

          This thread could be considered close and I''d be ok with that.'
        updatedAt: '2023-11-16T23:45:01.629Z'
      numEdits: 0
      reactions: []
    id: 6556a97df062c53d555ce5c1
    type: comment
  author: dyoung
  content: 'I think I may of answered what is going on. I''m seeing that "tokens =
    tokenizer.encode(instruction)" from the example code returns a common built in
    python list object/class. Which obviously doesn''t have the ability to be sent
    to the GPU as is (No ".to("cuda") method for the list class ...). So the recast
    with a pytorch long tensor is done in order to be able to copy the prompts tokens
    over to the GPU. And which is the class/data type expected for inference needs.

    What I see more common is that people are using ''tokens = tokenizer(sentence,
    return_tensors="pt").to(device)''. Where a single statement is doing multiple
    steps in one line. So it looks like the example code is just breaking down the
    parts into different statements rather then packing them into one line. Seeing
    this being done differently caught my attention. And I was curious as to why.
    I''m happy with this explanation I''ve found for my self.

    This thread could be considered close and I''d be ok with that.'
  created_at: 2023-11-16 23:45:01+00:00
  edited: false
  hidden: false
  id: 6556a97df062c53d555ce5c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: migtissera/SynthIA-7B-v2.0
repo_type: model
status: open
target_branch: null
title: token recast to torch.longTensor
