!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bezale
conflicting_files: null
created_at: 2023-11-07 17:13:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bezale
      type: user
    createdAt: '2023-11-07T17:13:44.000Z'
    data:
      edited: true
      editors:
      - bezale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9297427535057068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: bezale
          type: user
        html: '<p>Hi, mate! Whatever I do I can''t run this exactly model version
          properly. It replies with nonsence everytime.<br>I''ve tried your deepseek-coder-33B-instruct
          GPTQ, tried 6.7B version AWQ - they all running fine (but slow, thats why
          I desperate looking for AWQ models). Is there something you aware of or
          I do wrong? Does it run fine on your side?</p>

          <p>Thank you in advance for all your work here! </p>

          '
        raw: "Hi, mate! Whatever I do I can't run this exactly model version properly.\
          \ It replies with nonsence everytime. \nI've tried your deepseek-coder-33B-instruct\
          \ GPTQ, tried 6.7B version AWQ - they all running fine (but slow, thats\
          \ why I desperate looking for AWQ models). Is there something you aware\
          \ of or I do wrong? Does it run fine on your side?\n\nThank you in advance\
          \ for all your work here! \n"
        updatedAt: '2023-11-07T17:14:21.643Z'
      numEdits: 1
      reactions: []
    id: 654a70488f78bd30273c8dcc
    type: comment
  author: bezale
  content: "Hi, mate! Whatever I do I can't run this exactly model version properly.\
    \ It replies with nonsence everytime. \nI've tried your deepseek-coder-33B-instruct\
    \ GPTQ, tried 6.7B version AWQ - they all running fine (but slow, thats why I\
    \ desperate looking for AWQ models). Is there something you aware of or I do wrong?\
    \ Does it run fine on your side?\n\nThank you in advance for all your work here!\
    \ \n"
  created_at: 2023-11-07 17:13:44+00:00
  edited: true
  hidden: false
  id: 654a70488f78bd30273c8dcc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-08T10:30:42.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9547932147979736
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>I guess I have the same issue. It just outputs one token in repeat
          until it runs into max_token limit. How does it look for you? Are you trying
          to run it in oobabooga webui? </p>

          '
        raw: 'I guess I have the same issue. It just outputs one token in repeat until
          it runs into max_token limit. How does it look for you? Are you trying to
          run it in oobabooga webui? '
        updatedAt: '2023-11-08T10:30:42.800Z'
      numEdits: 0
      reactions: []
    id: 654b63521a0dc245a9c4d0d7
    type: comment
  author: adamo1139
  content: 'I guess I have the same issue. It just outputs one token in repeat until
    it runs into max_token limit. How does it look for you? Are you trying to run
    it in oobabooga webui? '
  created_at: 2023-11-08 10:30:42+00:00
  edited: false
  hidden: false
  id: 654b63521a0dc245a9c4d0d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bezale
      type: user
    createdAt: '2023-11-08T11:03:47.000Z'
    data:
      edited: true
      editors:
      - bezale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957962155342102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: bezale
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;adamo1139&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/adamo1139\">@<span class=\"\
          underline\">adamo1139</span></a></span>\n\n\t</span></span> You've described\
          \ it behaviour pretty good on my side as well. Sometimes it produced something\
          \ like \"YouYouYouYouYouYouYou..\" instead of empty token but always it's\
          \ nothing meaningful and stops at max token limit. I'm using my custom solution\
          \ for inference and there was the first time I saw that kind of output among\
          \ many-many models.</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ I really hope you'll look into this as this model is considered one of\
          \ the best for coding and AWQ version is essential for some of us :) </p>\n"
        raw: '@adamo1139 You''ve described it behaviour pretty good on my side as
          well. Sometimes it produced something like "YouYouYouYouYouYouYou.." instead
          of empty token but always it''s nothing meaningful and stops at max token
          limit. I''m using my custom solution for inference and there was the first
          time I saw that kind of output among many-many models.


          @TheBloke I really hope you''ll look into this as this model is considered
          one of the best for coding and AWQ version is essential for some of us :) '
        updatedAt: '2023-11-08T11:04:42.105Z'
      numEdits: 2
      reactions: []
    id: 654b6b13a560c049e0a7d76f
    type: comment
  author: bezale
  content: '@adamo1139 You''ve described it behaviour pretty good on my side as well.
    Sometimes it produced something like "YouYouYouYouYouYouYou.." instead of empty
    token but always it''s nothing meaningful and stops at max token limit. I''m using
    my custom solution for inference and there was the first time I saw that kind
    of output among many-many models.


    @TheBloke I really hope you''ll look into this as this model is considered one
    of the best for coding and AWQ version is essential for some of us :) '
  created_at: 2023-11-08 11:03:47+00:00
  edited: true
  hidden: false
  id: 654b6b13a560c049e0a7d76f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-08T11:08:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9331877827644348
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That sounds like a rope_scaling issue. What are your sequence length
          settings? </p>

          <p>Try at sequence length 16384 and see what happens there</p>

          <p>If that doesn''t work, or you don''t have VRAM for 16384, try editing
          config.json locally to remove the <code>rope_scaling</code> section and
          set <code>max_position_embeddings</code> to 4096</p>

          <p>I''ve not tested AWQ / AutoAWQ with longer sequence lengths - possibly
          the clients lack the controls that GPTQ would have for automatically setting
          the right rope scaling value according to the chosen sequence length</p>

          <p>Try those things and let me know</p>

          '
        raw: "That sounds like a rope_scaling issue. What are your sequence length\
          \ settings? \n\nTry at sequence length 16384 and see what happens there\n\
          \nIf that doesn't work, or you don't have VRAM for 16384, try editing config.json\
          \ locally to remove the `rope_scaling` section and set `max_position_embeddings`\
          \ to 4096\n\nI've not tested AWQ / AutoAWQ with longer sequence lengths\
          \ - possibly the clients lack the controls that GPTQ would have for automatically\
          \ setting the right rope scaling value according to the chosen sequence\
          \ length\n\nTry those things and let me know"
        updatedAt: '2023-11-08T11:08:38.236Z'
      numEdits: 0
      reactions: []
    id: 654b6c368221616b2123096c
    type: comment
  author: TheBloke
  content: "That sounds like a rope_scaling issue. What are your sequence length settings?\
    \ \n\nTry at sequence length 16384 and see what happens there\n\nIf that doesn't\
    \ work, or you don't have VRAM for 16384, try editing config.json locally to remove\
    \ the `rope_scaling` section and set `max_position_embeddings` to 4096\n\nI've\
    \ not tested AWQ / AutoAWQ with longer sequence lengths - possibly the clients\
    \ lack the controls that GPTQ would have for automatically setting the right rope\
    \ scaling value according to the chosen sequence length\n\nTry those things and\
    \ let me know"
  created_at: 2023-11-08 11:08:38+00:00
  edited: false
  hidden: false
  id: 654b6c368221616b2123096c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bezale
      type: user
    createdAt: '2023-11-08T11:26:26.000Z'
    data:
      edited: false
      editors:
      - bezale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.973818302154541
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: bezale
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Thanks for reply!</p>\n\
          <p>Sure, here is more info: </p>\n<ul>\n<li>I have A6000 with 48 GB VRAM\
          \ and I wasn't able even run the model with it default rope_scaling factor\
          \ of 4.0 because of OOM (I could easily run all 70B AWQ models before).\
          \ </li>\n<li>After removing  rope_scaling (or setting it to something low\
          \ like 1.1 instead of 4.0) I was able to run the model but it doesn't solve\
          \ wrong output (last time I did the same to even run it). max_position_embeddings\
          \ to 4096 also did nothing.</li>\n</ul>\n<p>Can It be some error during\
          \ the quantization? As GPTQ (btw,  I don't need to change rope_scaling there\
          \ to avoid OOM) version and smaller AWQ work as intended. It's very strange.\
          \ I see such OOM and wrong output problems the first time of all your AWQ\
          \ models I'v tried.</p>\n"
        raw: "@TheBloke Thanks for reply!\n\nSure, here is more info: \n- I have A6000\
          \ with 48 GB VRAM and I wasn't able even run the model with it default rope_scaling\
          \ factor of 4.0 because of OOM (I could easily run all 70B AWQ models before).\
          \ \n- After removing  rope_scaling (or setting it to something low like\
          \ 1.1 instead of 4.0) I was able to run the model but it doesn't solve wrong\
          \ output (last time I did the same to even run it). max_position_embeddings\
          \ to 4096 also did nothing.\n\nCan It be some error during the quantization?\
          \ As GPTQ (btw,  I don't need to change rope_scaling there to avoid OOM)\
          \ version and smaller AWQ work as intended. It's very strange. I see such\
          \ OOM and wrong output problems the first time of all your AWQ models I'v\
          \ tried."
        updatedAt: '2023-11-08T11:26:26.463Z'
      numEdits: 0
      reactions: []
    id: 654b7062910c8d5d07f96de1
    type: comment
  author: bezale
  content: "@TheBloke Thanks for reply!\n\nSure, here is more info: \n- I have A6000\
    \ with 48 GB VRAM and I wasn't able even run the model with it default rope_scaling\
    \ factor of 4.0 because of OOM (I could easily run all 70B AWQ models before).\
    \ \n- After removing  rope_scaling (or setting it to something low like 1.1 instead\
    \ of 4.0) I was able to run the model but it doesn't solve wrong output (last\
    \ time I did the same to even run it). max_position_embeddings to 4096 also did\
    \ nothing.\n\nCan It be some error during the quantization? As GPTQ (btw,  I don't\
    \ need to change rope_scaling there to avoid OOM) version and smaller AWQ work\
    \ as intended. It's very strange. I see such OOM and wrong output problems the\
    \ first time of all your AWQ models I'v tried."
  created_at: 2023-11-08 11:26:26+00:00
  edited: false
  hidden: false
  id: 654b7062910c8d5d07f96de1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-08T11:33:29.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9383832216262817
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah that could be the case</p>

          <p>One final test: could you try setting:<br><code>max_position_embeddings=4096</code><br>and
          <code>rope_scaling</code> leave as it is set in my repo, ie factor 4.0</p>

          <p>And set inference seqlen to 16384, and see if that makes any difference
          to OOM or output</p>

          <p>By the way, are you loading with Flash Attention 2?  That will significantly
          lower VRAM requirements at longer contexts</p>

          '
        raw: 'Yeah that could be the case


          One final test: could you try setting:

          `max_position_embeddings=4096`

          and `rope_scaling` leave as it is set in my repo, ie factor 4.0


          And set inference seqlen to 16384, and see if that makes any difference
          to OOM or output


          By the way, are you loading with Flash Attention 2?  That will significantly
          lower VRAM requirements at longer contexts'
        updatedAt: '2023-11-08T11:33:47.655Z'
      numEdits: 1
      reactions: []
    id: 654b7209c5350429d61b70eb
    type: comment
  author: TheBloke
  content: 'Yeah that could be the case


    One final test: could you try setting:

    `max_position_embeddings=4096`

    and `rope_scaling` leave as it is set in my repo, ie factor 4.0


    And set inference seqlen to 16384, and see if that makes any difference to OOM
    or output


    By the way, are you loading with Flash Attention 2?  That will significantly lower
    VRAM requirements at longer contexts'
  created_at: 2023-11-08 11:33:29+00:00
  edited: true
  hidden: false
  id: 654b7209c5350429d61b70eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-08T11:42:38.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9402377009391785
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>I am out of VRAM if I run it with sequence length 16384, but it''s
          just slowing things down and moving stuff to RAM. That didn''t help, it
          still outputs "''''''''''''''''''"<br>Removing <code>rope scaling</code>,
          setting <code>max_position_embeddings</code> to 4096 and loading it in webui
          at sequence length 4096 didn''t fix it either.<br>SHA256 of both safetensors
          files matches. I redownloaded jsons from your repo but that didn''t help
          either.<br>Messing with BOS token and special tokens settings in oobabooga
          didn''t help.<br>This is the first time I am using AWQ, so there is probably
          something wrong with my setup - I will check with other versions of awq,
          my oobabooga setup is currently on 0.1.6 (latest)</p>

          '
        raw: 'I am out of VRAM if I run it with sequence length 16384, but it''s just
          slowing things down and moving stuff to RAM. That didn''t help, it still
          outputs "''''''''''''''''''"

          Removing `rope scaling`, setting `max_position_embeddings` to 4096 and loading
          it in webui at sequence length 4096 didn''t fix it either.

          SHA256 of both safetensors files matches. I redownloaded jsons from your
          repo but that didn''t help either.

          Messing with BOS token and special tokens settings in oobabooga didn''t
          help.

          This is the first time I am using AWQ, so there is probably something wrong
          with my setup - I will check with other versions of awq, my oobabooga setup
          is currently on 0.1.6 (latest)

          '
        updatedAt: '2023-11-08T11:42:38.084Z'
      numEdits: 0
      reactions: []
    id: 654b742e17e776aa11370bcd
    type: comment
  author: adamo1139
  content: 'I am out of VRAM if I run it with sequence length 16384, but it''s just
    slowing things down and moving stuff to RAM. That didn''t help, it still outputs
    "''''''''''''''''''"

    Removing `rope scaling`, setting `max_position_embeddings` to 4096 and loading
    it in webui at sequence length 4096 didn''t fix it either.

    SHA256 of both safetensors files matches. I redownloaded jsons from your repo
    but that didn''t help either.

    Messing with BOS token and special tokens settings in oobabooga didn''t help.

    This is the first time I am using AWQ, so there is probably something wrong with
    my setup - I will check with other versions of awq, my oobabooga setup is currently
    on 0.1.6 (latest)

    '
  created_at: 2023-11-08 11:42:38+00:00
  edited: false
  hidden: false
  id: 654b742e17e776aa11370bcd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bezale
      type: user
    createdAt: '2023-11-08T11:54:37.000Z'
    data:
      edited: false
      editors:
      - bezale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9232504367828369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: bezale
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span><br>Hmm, it actually\
          \ helped (max_position_embeddings=4096 and retuned default rope_scaling\
          \ )</p>\n<p>While model often produces some strange text (nonsense) at the\
          \ end of reply it start to reply as expected with code etc. But it's maybe\
          \ can be fixed with generating params/template tuning.</p>\n<p>For AWQ inference\
          \ I'm using vllm lib with default params. They use as I know PagedAttention</p>\n"
        raw: "@TheBloke \nHmm, it actually helped (max_position_embeddings=4096 and\
          \ retuned default rope_scaling )\n\nWhile model often produces some strange\
          \ text (nonsense) at the end of reply it start to reply as expected with\
          \ code etc. But it's maybe can be fixed with generating params/template\
          \ tuning.\n\nFor AWQ inference I'm using vllm lib with default params. They\
          \ use as I know PagedAttention"
        updatedAt: '2023-11-08T11:54:37.939Z'
      numEdits: 0
      reactions: []
    id: 654b76fdd983a42287e7188a
    type: comment
  author: bezale
  content: "@TheBloke \nHmm, it actually helped (max_position_embeddings=4096 and\
    \ retuned default rope_scaling )\n\nWhile model often produces some strange text\
    \ (nonsense) at the end of reply it start to reply as expected with code etc.\
    \ But it's maybe can be fixed with generating params/template tuning.\n\nFor AWQ\
    \ inference I'm using vllm lib with default params. They use as I know PagedAttention"
  created_at: 2023-11-08 11:54:37+00:00
  edited: false
  hidden: false
  id: 654b76fdd983a42287e7188a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-08T12:05:51.000Z'
    data:
      edited: false
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968097448348999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: "<p>I tried updating oobabooga, downgrading awq to 0.1.5 and doing what\
          \ helped <span data-props=\"{&quot;user&quot;:&quot;bezale&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bezale\">@<span class=\"\
          underline\">bezale</span></a></span>\n\n\t</span></span>.<br>Same issue\
          \ remains. I will try gptq quant, maybe I won't have this issue there.</p>\n"
        raw: "I tried updating oobabooga, downgrading awq to 0.1.5 and doing what\
          \ helped @bezale. \nSame issue remains. I will try gptq quant, maybe I won't\
          \ have this issue there."
        updatedAt: '2023-11-08T12:05:51.218Z'
      numEdits: 0
      reactions: []
    id: 654b799fdc1b6cf3bda89c10
    type: comment
  author: adamo1139
  content: "I tried updating oobabooga, downgrading awq to 0.1.5 and doing what helped\
    \ @bezale. \nSame issue remains. I will try gptq quant, maybe I won't have this\
    \ issue there."
  created_at: 2023-11-08 12:05:51+00:00
  edited: false
  hidden: false
  id: 654b799fdc1b6cf3bda89c10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bezale
      type: user
    createdAt: '2023-11-08T12:09:00.000Z'
    data:
      edited: false
      editors:
      - bezale
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598509073257446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/156a27b6762526718189e13ecabc6ace.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: bezale
          type: user
        html: '<p>Yeah, it start working only with vllm back-end, while with common
          huggingface transformers it still doesn''t work. Probably oobabooga using
          huggingface standard inference. So the problem remains for many people who
          doesn''t know about the vllm.</p>

          '
        raw: Yeah, it start working only with vllm back-end, while with common huggingface
          transformers it still doesn't work. Probably oobabooga using huggingface
          standard inference. So the problem remains for many people who doesn't know
          about the vllm.
        updatedAt: '2023-11-08T12:09:00.409Z'
      numEdits: 0
      reactions: []
    id: 654b7a5c3226ad682123462f
    type: comment
  author: bezale
  content: Yeah, it start working only with vllm back-end, while with common huggingface
    transformers it still doesn't work. Probably oobabooga using huggingface standard
    inference. So the problem remains for many people who doesn't know about the vllm.
  created_at: 2023-11-08 12:09:00+00:00
  edited: false
  hidden: false
  id: 654b7a5c3226ad682123462f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adamo1139
      type: user
    createdAt: '2023-11-08T13:55:16.000Z'
    data:
      edited: true
      editors:
      - adamo1139
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.833230197429657
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3f7d24c9cfa2f12d2fd42fcc0a8d820a.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adamo1139
          type: user
        html: '<p>I tried to run the awq without webui, so without webui miniconda
          env, I got basically the same result - model outputs about 50 '' characters
          and then outputs newlines until it reaches max output token limit. I tried
          6.7B gptq model and it works fine. Out of topic, but Instruct model was
          clearly trained on some data from ChatGPT/GPT APIs. That''s what I get with
          default Alpaca system prompt<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/630fdd96a119d49bc1e770d5/SCz-G6ff-onzbR9c8-Ruf.png"><img
          alt="32879348792346932.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/630fdd96a119d49bc1e770d5/SCz-G6ff-onzbR9c8-Ruf.png"></a></p>

          '
        raw: 'I tried to run the awq without webui, so without webui miniconda env,
          I got basically the same result - model outputs about 50 '' characters and
          then outputs newlines until it reaches max output token limit. I tried 6.7B
          gptq model and it works fine. Out of topic, but Instruct model was clearly
          trained on some data from ChatGPT/GPT APIs. That''s what I get with default
          Alpaca system prompt

          ![32879348792346932.PNG](https://cdn-uploads.huggingface.co/production/uploads/630fdd96a119d49bc1e770d5/SCz-G6ff-onzbR9c8-Ruf.png)

          '
        updatedAt: '2023-11-08T13:56:56.882Z'
      numEdits: 3
      reactions: []
    id: 654b93446b51714c2a4a54e6
    type: comment
  author: adamo1139
  content: 'I tried to run the awq without webui, so without webui miniconda env,
    I got basically the same result - model outputs about 50 '' characters and then
    outputs newlines until it reaches max output token limit. I tried 6.7B gptq model
    and it works fine. Out of topic, but Instruct model was clearly trained on some
    data from ChatGPT/GPT APIs. That''s what I get with default Alpaca system prompt

    ![32879348792346932.PNG](https://cdn-uploads.huggingface.co/production/uploads/630fdd96a119d49bc1e770d5/SCz-G6ff-onzbR9c8-Ruf.png)

    '
  created_at: 2023-11-08 13:55:16+00:00
  edited: true
  hidden: false
  id: 654b93446b51714c2a4a54e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bed8ddcaad9d1201c1cfdaf831efb617.svg
      fullname: Sunjin Park
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narfian
      type: user
    createdAt: '2023-12-12T03:18:04.000Z'
    data:
      edited: false
      editors:
      - Narfian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.762172520160675
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bed8ddcaad9d1201c1cfdaf831efb617.svg
          fullname: Sunjin Park
          isHf: false
          isPro: false
          name: Narfian
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> How could I apply\
          \ your solution(max_position_embeddings, rope_scaling) in vLLM? Does vLLM\
          \ also reference config.json?</p>\n"
        raw: '@TheBloke How could I apply your solution(max_position_embeddings, rope_scaling)
          in vLLM? Does vLLM also reference config.json?'
        updatedAt: '2023-12-12T03:18:04.927Z'
      numEdits: 0
      reactions: []
    id: 6577d0ecdd2996f01ae28f04
    type: comment
  author: Narfian
  content: '@TheBloke How could I apply your solution(max_position_embeddings, rope_scaling)
    in vLLM? Does vLLM also reference config.json?'
  created_at: 2023-12-12 03:18:04+00:00
  edited: false
  hidden: false
  id: 6577d0ecdd2996f01ae28f04
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/deepseek-coder-33B-instruct-AWQ
repo_type: model
status: open
target_branch: null
title: Problem Running Model
