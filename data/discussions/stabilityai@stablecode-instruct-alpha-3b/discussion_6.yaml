!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ymgenesis
conflicting_files: null
created_at: 2023-08-10 12:44:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f37ddeb99a1eb579462d5e74ebf2038d.svg
      fullname: nope
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ymgenesis
      type: user
    createdAt: '2023-08-10T13:44:35.000Z'
    data:
      edited: true
      editors:
      - ymgenesis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9320109486579895
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f37ddeb99a1eb579462d5e74ebf2038d.svg
          fullname: nope
          isHf: false
          isPro: false
          name: ymgenesis
          type: user
        html: "<p><code>torch_dtype=auto</code> doesn't seem to take mps.<br>I get\
          \ <code>AttributeError: 'GPTNeoXForCausalLM' object has no attribute 'mps'</code>\
          \ when trying to troubleshoot.<br>I installed the nightly torch with:<br><code>pip3\
          \ install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu</code></p>\n\
          <p>Here's the changes I made:</p>\n<pre><code>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"\
          stabilityai/stablecode-instruct-alpha-3b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \  \"stabilityai/stablecode-instruct-alpha-3b\",\n  trust_remote_code=True,\n\
          \  torch_dtype=torch.bfloat16,\n)\nmodel=model.to(\"mps\")\nmodel.mps()\n\
          inputs = tokenizer(\"###Instruction\\nGenerate a python function to find\
          \ number of CPU cores###Response\\n\", return_tensors=\"pt\").to(\"mps\"\
          )\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n\
          \  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
          </code></pre>\n<p>and I get:</p>\n<pre><code>TypeError: BFloat16 is not\
          \ supported on MPS\n</code></pre>\n<p>I thought float16 was made compatible\
          \ recently?</p>\n"
        raw: "`torch_dtype=auto` doesn't seem to take mps.\nI get `AttributeError:\
          \ 'GPTNeoXForCausalLM' object has no attribute 'mps'` when trying to troubleshoot.\n\
          I installed the nightly torch with:\n`pip3 install --pre torch torchvision\
          \ torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu`\n\
          \nHere's the changes I made:\n```\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\nimport torch\ntokenizer = AutoTokenizer.from_pretrained(\"\
          stabilityai/stablecode-instruct-alpha-3b\")\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \  \"stabilityai/stablecode-instruct-alpha-3b\",\n  trust_remote_code=True,\n\
          \  torch_dtype=torch.bfloat16,\n)\nmodel=model.to(\"mps\")\nmodel.mps()\n\
          inputs = tokenizer(\"###Instruction\\nGenerate a python function to find\
          \ number of CPU cores###Response\\n\", return_tensors=\"pt\").to(\"mps\"\
          )\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n\
          \  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
          ```\nand I get:\n```\nTypeError: BFloat16 is not supported on MPS\n```\n\
          \nI thought float16 was made compatible recently?"
        updatedAt: '2023-08-10T14:20:41.214Z'
      numEdits: 4
      reactions: []
    id: 64d4e9c3887f55fb6eec5168
    type: comment
  author: ymgenesis
  content: "`torch_dtype=auto` doesn't seem to take mps.\nI get `AttributeError: 'GPTNeoXForCausalLM'\
    \ object has no attribute 'mps'` when trying to troubleshoot.\nI installed the\
    \ nightly torch with:\n`pip3 install --pre torch torchvision torchaudio --index-url\
    \ https://download.pytorch.org/whl/nightly/cpu`\n\nHere's the changes I made:\n\
    ```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\
    tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablecode-instruct-alpha-3b\"\
    )\nmodel = AutoModelForCausalLM.from_pretrained(\n  \"stabilityai/stablecode-instruct-alpha-3b\"\
    ,\n  trust_remote_code=True,\n  torch_dtype=torch.bfloat16,\n)\nmodel=model.to(\"\
    mps\")\nmodel.mps()\ninputs = tokenizer(\"###Instruction\\nGenerate a python function\
    \ to find number of CPU cores###Response\\n\", return_tensors=\"pt\").to(\"mps\"\
    )\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n\
    \  do_sample=True,\n)\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
    ```\nand I get:\n```\nTypeError: BFloat16 is not supported on MPS\n```\n\nI thought\
    \ float16 was made compatible recently?"
  created_at: 2023-08-10 12:44:35+00:00
  edited: true
  hidden: false
  id: 64d4e9c3887f55fb6eec5168
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/607cf601a096493a2beaf48e19d8fc1d.svg
      fullname: Takseev Kirill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: froznfox
      type: user
    createdAt: '2023-08-10T20:24:21.000Z'
    data:
      edited: false
      editors:
      - froznfox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4493834376335144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/607cf601a096493a2beaf48e19d8fc1d.svg
          fullname: Takseev Kirill
          isHf: false
          isPro: false
          name: froznfox
          type: user
        html: "<p>This is not a solution, but you can run it using the CPU.</p>\n\
          <pre><code>model.to(\"cpu\")    &lt;---- Add this\ninputs = tokenizer(\n\
          \    \"###Instruction\\nGenerate a java function to find number of CPU cores###Response\\\
          n\", \n    return_tensors=\"pt\",\n    return_token_type_ids=False,    &lt;----\
          \ Add this\n).to(\"cpu\")    &lt;---- Add this\n\ntokens = model.generate(\n\
          \  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n  do_sample=True,\n\
          \  pad_token_id=50256    &lt;---- Add this\n)\n</code></pre>\n"
        raw: "This is not a solution, but you can run it using the CPU.\n\n```\nmodel.to(\"\
          cpu\")    <---- Add this\ninputs = tokenizer(\n    \"###Instruction\\nGenerate\
          \ a java function to find number of CPU cores###Response\\n\", \n    return_tensors=\"\
          pt\",\n    return_token_type_ids=False,    <---- Add this\n).to(\"cpu\"\
          )    <---- Add this\n\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n\
          \  temperature=0.2,\n  do_sample=True,\n  pad_token_id=50256    <---- Add\
          \ this\n)\n```"
        updatedAt: '2023-08-10T20:24:21.892Z'
      numEdits: 0
      reactions: []
    id: 64d54775aa89fb548a7be28e
    type: comment
  author: froznfox
  content: "This is not a solution, but you can run it using the CPU.\n\n```\nmodel.to(\"\
    cpu\")    <---- Add this\ninputs = tokenizer(\n    \"###Instruction\\nGenerate\
    \ a java function to find number of CPU cores###Response\\n\", \n    return_tensors=\"\
    pt\",\n    return_token_type_ids=False,    <---- Add this\n).to(\"cpu\")    <----\
    \ Add this\n\ntokens = model.generate(\n  **inputs,\n  max_new_tokens=48,\n  temperature=0.2,\n\
    \  do_sample=True,\n  pad_token_id=50256    <---- Add this\n)\n```"
  created_at: 2023-08-10 19:24:21+00:00
  edited: false
  hidden: false
  id: 64d54775aa89fb548a7be28e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f37ddeb99a1eb579462d5e74ebf2038d.svg
      fullname: nope
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ymgenesis
      type: user
    createdAt: '2023-08-11T08:41:15.000Z'
    data:
      edited: false
      editors:
      - ymgenesis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9090143442153931
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f37ddeb99a1eb579462d5e74ebf2038d.svg
          fullname: nope
          isHf: false
          isPro: false
          name: ymgenesis
          type: user
        html: '<p>Thanks that seemed to work, though using CPU it takes about 10 minutes
          to generate an answer (expectedly). </p>

          <p>Looking forward to mps compatibility with PyTorch (<a rel="nofollow"
          href="https://pytorch.org/docs/stable/notes/mps.html">https://pytorch.org/docs/stable/notes/mps.html</a>).</p>

          '
        raw: "Thanks that seemed to work, though using CPU it takes about 10 minutes\
          \ to generate an answer (expectedly). \n\nLooking forward to mps compatibility\
          \ with PyTorch (https://pytorch.org/docs/stable/notes/mps.html)."
        updatedAt: '2023-08-11T08:41:15.822Z'
      numEdits: 0
      reactions: []
    id: 64d5f42bd9d00dcb1159294d
    type: comment
  author: ymgenesis
  content: "Thanks that seemed to work, though using CPU it takes about 10 minutes\
    \ to generate an answer (expectedly). \n\nLooking forward to mps compatibility\
    \ with PyTorch (https://pytorch.org/docs/stable/notes/mps.html)."
  created_at: 2023-08-11 07:41:15+00:00
  edited: false
  hidden: false
  id: 64d5f42bd9d00dcb1159294d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f37ddeb99a1eb579462d5e74ebf2038d.svg
      fullname: nope
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ymgenesis
      type: user
    createdAt: '2023-08-11T09:34:20.000Z'
    data:
      edited: true
      editors:
      - ymgenesis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6779212951660156
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f37ddeb99a1eb579462d5e74ebf2038d.svg
          fullname: nope
          isHf: false
          isPro: false
          name: ymgenesis
          type: user
        html: "<p>Taking guidance from the link from pytorch above I seem to have\
          \ got it working with mps on my M1 Macbook Pro.</p>\n<pre><code>from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, TextStreamer\nimport torch\n\
          \nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n\
          \        print(\"MPS not available because the current PyTorch install was\
          \ not \"\n              \"built with MPS enabled.\")\n    else:\n      \
          \  print(\"MPS not available because the current MacOS version is not 12.3+\
          \ \"\n              \"and/or you do not have an MPS-enabled device on this\
          \ machine.\")\n\nelse:\n    print(\"Attempting to use MPS...\")\n    mps_device\
          \ = torch.device(\"mps\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          stabilityai/stablecode-instruct-alpha-3b\")\n    streamer = TextStreamer(tokenizer)\n\
          \    model = AutoModelForCausalLM.from_pretrained(\n      \"stabilityai/stablecode-instruct-alpha-3b\"\
          ,\n      trust_remote_code=True,\n    )\n    model.to(mps_device)\n    \n\
          \    inputs = tokenizer(\n      \"\\n###Instruction\\n\\nGenerate a python\
          \ function to find number of CPU cores\\n\\n###Response\\n\",\n      return_tensors=\"\
          pt\",\n      return_token_type_ids=False,\n    ).to(mps_device)\n    tokens\
          \ = model.generate(\n      **inputs,\n      max_new_tokens=48,\n      temperature=0.2,\n\
          \      do_sample=True,\n      pad_token_id=50256,\n      streamer=streamer\n\
          \    )\n\n    print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
          </code></pre>\n<p>Changing <code>max_new_tokens</code> to something larger\
          \ to get anything of length. Added TextStreamer to visualize the output\
          \ as generation is still slow with mps, but it's definitely using it.</p>\n"
        raw: "Taking guidance from the link from pytorch above I seem to have got\
          \ it working with mps on my M1 Macbook Pro.\n```\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, TextStreamer\nimport torch\n\nif\
          \ not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n\
          \        print(\"MPS not available because the current PyTorch install was\
          \ not \"\n              \"built with MPS enabled.\")\n    else:\n      \
          \  print(\"MPS not available because the current MacOS version is not 12.3+\
          \ \"\n              \"and/or you do not have an MPS-enabled device on this\
          \ machine.\")\n\nelse:\n\tprint(\"Attempting to use MPS...\")\n\tmps_device\
          \ = torch.device(\"mps\")\n\n\ttokenizer = AutoTokenizer.from_pretrained(\"\
          stabilityai/stablecode-instruct-alpha-3b\")\n\tstreamer = TextStreamer(tokenizer)\n\
          \tmodel = AutoModelForCausalLM.from_pretrained(\n\t  \"stabilityai/stablecode-instruct-alpha-3b\"\
          ,\n\t  trust_remote_code=True,\n\t)\n\tmodel.to(mps_device)\n\t\n\tinputs\
          \ = tokenizer(\n\t  \"\\n###Instruction\\n\\nGenerate a python function\
          \ to find number of CPU cores\\n\\n###Response\\n\",\n\t  return_tensors=\"\
          pt\",\n\t  return_token_type_ids=False,\n\t).to(mps_device)\n\ttokens =\
          \ model.generate(\n\t  **inputs,\n\t  max_new_tokens=48,\n\t  temperature=0.2,\n\
          \t  do_sample=True,\n\t  pad_token_id=50256,\n\t  streamer=streamer\n\t\
          )\n\n\tprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n```\n\
          Changing `max_new_tokens` to something larger to get anything of length.\
          \ Added TextStreamer to visualize the output as generation is still slow\
          \ with mps, but it's definitely using it."
        updatedAt: '2023-08-11T09:35:02.134Z'
      numEdits: 1
      reactions: []
    id: 64d6009c8767727dffb1b2ae
    type: comment
  author: ymgenesis
  content: "Taking guidance from the link from pytorch above I seem to have got it\
    \ working with mps on my M1 Macbook Pro.\n```\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, TextStreamer\nimport torch\n\nif not torch.backends.mps.is_available():\n\
    \    if not torch.backends.mps.is_built():\n        print(\"MPS not available\
    \ because the current PyTorch install was not \"\n              \"built with MPS\
    \ enabled.\")\n    else:\n        print(\"MPS not available because the current\
    \ MacOS version is not 12.3+ \"\n              \"and/or you do not have an MPS-enabled\
    \ device on this machine.\")\n\nelse:\n\tprint(\"Attempting to use MPS...\")\n\
    \tmps_device = torch.device(\"mps\")\n\n\ttokenizer = AutoTokenizer.from_pretrained(\"\
    stabilityai/stablecode-instruct-alpha-3b\")\n\tstreamer = TextStreamer(tokenizer)\n\
    \tmodel = AutoModelForCausalLM.from_pretrained(\n\t  \"stabilityai/stablecode-instruct-alpha-3b\"\
    ,\n\t  trust_remote_code=True,\n\t)\n\tmodel.to(mps_device)\n\t\n\tinputs = tokenizer(\n\
    \t  \"\\n###Instruction\\n\\nGenerate a python function to find number of CPU\
    \ cores\\n\\n###Response\\n\",\n\t  return_tensors=\"pt\",\n\t  return_token_type_ids=False,\n\
    \t).to(mps_device)\n\ttokens = model.generate(\n\t  **inputs,\n\t  max_new_tokens=48,\n\
    \t  temperature=0.2,\n\t  do_sample=True,\n\t  pad_token_id=50256,\n\t  streamer=streamer\n\
    \t)\n\n\tprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n```\nChanging\
    \ `max_new_tokens` to something larger to get anything of length. Added TextStreamer\
    \ to visualize the output as generation is still slow with mps, but it's definitely\
    \ using it."
  created_at: 2023-08-11 08:34:20+00:00
  edited: true
  hidden: false
  id: 64d6009c8767727dffb1b2ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cedb57325451d6704eb17f9adef132ff.svg
      fullname: Andries Jacobus du Plooy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AJDP
      type: user
    createdAt: '2023-08-14T22:05:17.000Z'
    data:
      edited: false
      editors:
      - AJDP
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6742610335350037
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cedb57325451d6704eb17f9adef132ff.svg
          fullname: Andries Jacobus du Plooy
          isHf: false
          isPro: false
          name: AJDP
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ymgenesis&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ymgenesis\">@<span class=\"\
          underline\">ymgenesis</span></a></span>\n\n\t</span></span> thanks so much\
          \ for this!<br>After using your solution I ran into another issue,</p>\n\
          <p><code>RuntimeError: MPS does not support cumsum op with int64 input</code></p>\n\
          <p>Got it working on my M1 Macbook Pro by following this solution:<br><code>pip3\
          \ install --upgrade --no-deps --force-reinstall --pre torch torchvision\
          \ torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu</code><br>Ref:\
          \ <a rel=\"nofollow\" href=\"https://github.com/pytorch/pytorch/issues/96610#issuecomment-1593230620\"\
          >https://github.com/pytorch/pytorch/issues/96610#issuecomment-1593230620</a></p>\n\
          <blockquote>\n<p>Taking guidance from the link from pytorch above I seem\
          \ to have got it working with mps on my M1 Macbook Pro.</p>\n<pre><code>from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\
          import torch\n\nif not torch.backends.mps.is_available():\n    if not torch.backends.mps.is_built():\n\
          \        print(\"MPS not available because the current PyTorch install was\
          \ not \"\n              \"built with MPS enabled.\")\n    else:\n      \
          \  print(\"MPS not available because the current MacOS version is not 12.3+\
          \ \"\n              \"and/or you do not have an MPS-enabled device on this\
          \ machine.\")\n\nelse:\n    print(\"Attempting to use MPS...\")\n    mps_device\
          \ = torch.device(\"mps\")\n\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          stabilityai/stablecode-instruct-alpha-3b\")\n    streamer = TextStreamer(tokenizer)\n\
          \    model = AutoModelForCausalLM.from_pretrained(\n      \"stabilityai/stablecode-instruct-alpha-3b\"\
          ,\n      trust_remote_code=True,\n    )\n    model.to(mps_device)\n    \n\
          \    inputs = tokenizer(\n      \"\\n###Instruction\\n\\nGenerate a python\
          \ function to find number of CPU cores\\n\\n###Response\\n\",\n      return_tensors=\"\
          pt\",\n      return_token_type_ids=False,\n    ).to(mps_device)\n    tokens\
          \ = model.generate(\n      **inputs,\n      max_new_tokens=48,\n      temperature=0.2,\n\
          \      do_sample=True,\n      pad_token_id=50256,\n      streamer=streamer\n\
          \    )\n\n    print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
          </code></pre>\n<p>Changing <code>max_new_tokens</code> to something larger\
          \ to get anything of length. Added TextStreamer to visualize the output\
          \ as generation is still slow with mps, but it's definitely using it.</p>\n\
          </blockquote>\n"
        raw: "@ymgenesis thanks so much for this!\nAfter using your solution I ran\
          \ into another issue,\n\n`RuntimeError: MPS does not support cumsum op with\
          \ int64 input`\n\nGot it working on my M1 Macbook Pro by following this\
          \ solution:\n`pip3 install --upgrade --no-deps --force-reinstall --pre torch\
          \ torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu`\n\
          Ref: https://github.com/pytorch/pytorch/issues/96610#issuecomment-1593230620\n\
          \n> Taking guidance from the link from pytorch above I seem to have got\
          \ it working with mps on my M1 Macbook Pro.\n> ```\n> from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n> import torch\n\
          > \n> if not torch.backends.mps.is_available():\n>     if not torch.backends.mps.is_built():\n\
          >         print(\"MPS not available because the current PyTorch install\
          \ was not \"\n>               \"built with MPS enabled.\")\n>     else:\n\
          >         print(\"MPS not available because the current MacOS version is\
          \ not 12.3+ \"\n>               \"and/or you do not have an MPS-enabled\
          \ device on this machine.\")\n> \n> else:\n> \tprint(\"Attempting to use\
          \ MPS...\")\n> \tmps_device = torch.device(\"mps\")\n> \n> \ttokenizer =\
          \ AutoTokenizer.from_pretrained(\"stabilityai/stablecode-instruct-alpha-3b\"\
          )\n> \tstreamer = TextStreamer(tokenizer)\n> \tmodel = AutoModelForCausalLM.from_pretrained(\n\
          > \t  \"stabilityai/stablecode-instruct-alpha-3b\",\n> \t  trust_remote_code=True,\n\
          > \t)\n> \tmodel.to(mps_device)\n> \t\n> \tinputs = tokenizer(\n> \t  \"\
          \\n###Instruction\\n\\nGenerate a python function to find number of CPU\
          \ cores\\n\\n###Response\\n\",\n> \t  return_tensors=\"pt\",\n> \t  return_token_type_ids=False,\n\
          > \t).to(mps_device)\n> \ttokens = model.generate(\n> \t  **inputs,\n> \t\
          \  max_new_tokens=48,\n> \t  temperature=0.2,\n> \t  do_sample=True,\n>\
          \ \t  pad_token_id=50256,\n> \t  streamer=streamer\n> \t)\n> \n> \tprint(tokenizer.decode(tokens[0],\
          \ skip_special_tokens=True))\n> ```\n> Changing `max_new_tokens` to something\
          \ larger to get anything of length. Added TextStreamer to visualize the\
          \ output as generation is still slow with mps, but it's definitely using\
          \ it.\n\n\n"
        updatedAt: '2023-08-14T22:05:17.412Z'
      numEdits: 0
      reactions: []
    id: 64daa51d139c3065f9391c37
    type: comment
  author: AJDP
  content: "@ymgenesis thanks so much for this!\nAfter using your solution I ran into\
    \ another issue,\n\n`RuntimeError: MPS does not support cumsum op with int64 input`\n\
    \nGot it working on my M1 Macbook Pro by following this solution:\n`pip3 install\
    \ --upgrade --no-deps --force-reinstall --pre torch torchvision torchaudio --index-url\
    \ https://download.pytorch.org/whl/nightly/cpu`\nRef: https://github.com/pytorch/pytorch/issues/96610#issuecomment-1593230620\n\
    \n> Taking guidance from the link from pytorch above I seem to have got it working\
    \ with mps on my M1 Macbook Pro.\n> ```\n> from transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, TextStreamer\n> import torch\n> \n> if not torch.backends.mps.is_available():\n\
    >     if not torch.backends.mps.is_built():\n>         print(\"MPS not available\
    \ because the current PyTorch install was not \"\n>               \"built with\
    \ MPS enabled.\")\n>     else:\n>         print(\"MPS not available because the\
    \ current MacOS version is not 12.3+ \"\n>               \"and/or you do not have\
    \ an MPS-enabled device on this machine.\")\n> \n> else:\n> \tprint(\"Attempting\
    \ to use MPS...\")\n> \tmps_device = torch.device(\"mps\")\n> \n> \ttokenizer\
    \ = AutoTokenizer.from_pretrained(\"stabilityai/stablecode-instruct-alpha-3b\"\
    )\n> \tstreamer = TextStreamer(tokenizer)\n> \tmodel = AutoModelForCausalLM.from_pretrained(\n\
    > \t  \"stabilityai/stablecode-instruct-alpha-3b\",\n> \t  trust_remote_code=True,\n\
    > \t)\n> \tmodel.to(mps_device)\n> \t\n> \tinputs = tokenizer(\n> \t  \"\\n###Instruction\\\
    n\\nGenerate a python function to find number of CPU cores\\n\\n###Response\\\
    n\",\n> \t  return_tensors=\"pt\",\n> \t  return_token_type_ids=False,\n> \t).to(mps_device)\n\
    > \ttokens = model.generate(\n> \t  **inputs,\n> \t  max_new_tokens=48,\n> \t\
    \  temperature=0.2,\n> \t  do_sample=True,\n> \t  pad_token_id=50256,\n> \t  streamer=streamer\n\
    > \t)\n> \n> \tprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n\
    > ```\n> Changing `max_new_tokens` to something larger to get anything of length.\
    \ Added TextStreamer to visualize the output as generation is still slow with\
    \ mps, but it's definitely using it.\n\n\n"
  created_at: 2023-08-14 21:05:17+00:00
  edited: false
  hidden: false
  id: 64daa51d139c3065f9391c37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6c7db6a5c73a66c294b2f52ed9806b3e.svg
      fullname: Patrick Chu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patchu
      type: user
    createdAt: '2023-08-16T14:23:58.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/6c7db6a5c73a66c294b2f52ed9806b3e.svg
          fullname: Patrick Chu
          isHf: false
          isPro: false
          name: patchu
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-16T14:24:40.314Z'
      numEdits: 0
      reactions: []
    id: 64dcdbfeb3e75f78a4ab64e0
    type: comment
  author: patchu
  content: This comment has been hidden
  created_at: 2023-08-16 13:23:58+00:00
  edited: true
  hidden: true
  id: 64dcdbfeb3e75f78a4ab64e0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: stabilityai/stablecode-instruct-alpha-3b
repo_type: model
status: open
target_branch: null
title: Compatibility with mps/Mac M1?
