!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jy395
conflicting_files: null
created_at: 2023-08-16 19:00:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bc3e3a078757e86d1cee0b3388f4db5.svg
      fullname: Junyi Ye
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jy395
      type: user
    createdAt: '2023-08-16T20:00:12.000Z'
    data:
      edited: false
      editors:
      - jy395
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7420613765716553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bc3e3a078757e86d1cee0b3388f4db5.svg
          fullname: Junyi Ye
          isHf: false
          isPro: false
          name: jy395
          type: user
        html: '<p>lib\site-packages\transformers\generation\utils.py:1270: UserWarning:
          You have modified the pretrained model configuration to control generation.
          This is a deprecated strategy to control generation and will be removed
          soon, in a future version. Please use a generation configuration file (see
          <a href="https://huggingface.co/docs/transformers/main_classes/text_generation">https://huggingface.co/docs/transformers/main_classes/text_generation</a>
          )<br>  warnings.warn(<br>Setting <code>pad_token_id</code> to <code>eos_token_id</code>:0
          for open-end generation.</p>

          <p>How to resolve the above warnings?</p>

          '
        raw: "lib\\site-packages\\transformers\\generation\\utils.py:1270: UserWarning:\
          \ You have modified the pretrained model configuration to control generation.\
          \ This is a deprecated strategy to control generation and will be removed\
          \ soon, in a future version. Please use a generation configuration file\
          \ (see https://huggingface.co/docs/transformers/main_classes/text_generation\
          \ )\r\n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:0 for\
          \ open-end generation.\r\n\r\nHow to resolve the above warnings?"
        updatedAt: '2023-08-16T20:00:12.662Z'
      numEdits: 0
      reactions: []
    id: 64dd2accc79e3245ebb4382f
    type: comment
  author: jy395
  content: "lib\\site-packages\\transformers\\generation\\utils.py:1270: UserWarning:\
    \ You have modified the pretrained model configuration to control generation.\
    \ This is a deprecated strategy to control generation and will be removed soon,\
    \ in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation\
    \ )\r\n  warnings.warn(\r\nSetting `pad_token_id` to `eos_token_id`:0 for open-end\
    \ generation.\r\n\r\nHow to resolve the above warnings?"
  created_at: 2023-08-16 19:00:12+00:00
  edited: false
  hidden: false
  id: 64dd2accc79e3245ebb4382f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29adfb553eee5783c7700d5f955d87a4.svg
      fullname: Jacob Dart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdart
      type: user
    createdAt: '2023-08-18T05:57:51.000Z'
    data:
      edited: false
      editors:
      - jdart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6554374694824219
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29adfb553eee5783c7700d5f955d87a4.svg
          fullname: Jacob Dart
          isHf: false
          isPro: false
          name: jdart
          type: user
        html: "<p>From GPT-4:</p>\n<p>The warning you're seeing is related to using\
          \ a deprecated method for controlling generation in the Hugging Face Transformers\
          \ library.</p>\n<p>Let's break down the warning and address the issues:</p>\n\
          <ol>\n<li><p><strong>Modification of the Pretrained Model Configuration</strong>:\
          \ The warning message indicates that directly modifying the model's configuration\
          \ for controlling generation is a deprecated strategy. The solution Hugging\
          \ Face recommends is to use a generation configuration file.</p>\n</li>\n\
          <li><p><strong>Setting <code>pad_token_id</code> to <code>eos_token_id</code></strong>:\
          \ This is an informational message rather than a warning. In some generation\
          \ tasks, when <code>pad_token_id</code> is not specified, the <code>eos_token_id</code>\
          \ (End-of-Sentence token ID) is used as the <code>pad_token_id</code> by\
          \ default. If you don\u2019t want this behavior, you should explicitly set\
          \ the <code>pad_token_id</code>.</p>\n</li>\n</ol>\n<h3 id=\"solutions\"\
          >Solutions:</h3>\n<ol>\n<li><p><strong>Using Generation Configuration</strong>:</p>\n\
          <p>Instead of modifying the model's configuration, you should use <code>generation</code>\
          \ methods provided by the model or the <code>generate</code> function and\
          \ pass the necessary arguments there.</p>\n<p>For example, let's say you\
          \ want to generate text using a model:<br>```python<br>from transformers\
          \ import GPT2LMHeadModel, GPT2Tokenizer</p>\n<p>model = GPT2LMHeadModel.from_pretrained(\"\
          gpt2-medium\")<br>tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\"\
          )</p>\n<p>input_text = \"Once upon a time\"<br>input_ids = tokenizer.encode(input_text,\
          \ return_tensors=\"pt\")</p>\n</li>\n</ol>\n<h1 id=\"generate-output\">Generate\
          \ output</h1>\n<p>   output = model.generate(input_ids, max_length=100,\
          \ num_return_sequences=5)</p>\n<h1 id=\"decode-the-output\">Decode the output</h1>\n\
          <p>   for sequence in output:<br>       print(tokenizer.decode(sequence,\
          \ skip_special_tokens=True))</p>\n<pre><code>\nIf you want to control generation,\
          \ you can pass various arguments to the `generate` function, like `max_length`,\
          \ `temperature`, `top_k`, etc. However, you shouldn't directly modify the\
          \ model's configuration for these parameters.\n\n2. **Explicitly Setting\
          \ `pad_token_id`**:\n\nIf you don\u2019t want the default behavior of setting\
          \ `pad_token_id` to `eos_token_id`, you should explicitly set `pad_token_id`\
          \ during the generation:\n\n```python\noutput = model.generate(input_ids,\
          \ max_length=100, pad_token_id=tokenizer.eos_token_id)\n</code></pre>\n\
          <p>In summary, always refer to the official <a href=\"https://huggingface.co/transformers/main_classes/text_generation.html\"\
          >Hugging Face documentation</a> or the linked URL in the warning for updated\
          \ best practices and guidelines.</p>\n"
        raw: "From GPT-4:\n\nThe warning you're seeing is related to using a deprecated\
          \ method for controlling generation in the Hugging Face Transformers library.\n\
          \nLet's break down the warning and address the issues:\n\n1. **Modification\
          \ of the Pretrained Model Configuration**: The warning message indicates\
          \ that directly modifying the model's configuration for controlling generation\
          \ is a deprecated strategy. The solution Hugging Face recommends is to use\
          \ a generation configuration file.\n\n2. **Setting `pad_token_id` to `eos_token_id`**:\
          \ This is an informational message rather than a warning. In some generation\
          \ tasks, when `pad_token_id` is not specified, the `eos_token_id` (End-of-Sentence\
          \ token ID) is used as the `pad_token_id` by default. If you don\u2019t\
          \ want this behavior, you should explicitly set the `pad_token_id`.\n\n\
          ### Solutions:\n\n1. **Using Generation Configuration**:\n   \n   Instead\
          \ of modifying the model's configuration, you should use `generation` methods\
          \ provided by the model or the `generate` function and pass the necessary\
          \ arguments there.\n   \n   For example, let's say you want to generate\
          \ text using a model:\n   ```python\n   from transformers import GPT2LMHeadModel,\
          \ GPT2Tokenizer\n\n   model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\"\
          )\n   tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n   \n\
          \   input_text = \"Once upon a time\"\n   input_ids = tokenizer.encode(input_text,\
          \ return_tensors=\"pt\")\n   \n   # Generate output\n   output = model.generate(input_ids,\
          \ max_length=100, num_return_sequences=5)\n   \n   # Decode the output\n\
          \   for sequence in output:\n       print(tokenizer.decode(sequence, skip_special_tokens=True))\n\
          \   ```\n\n   If you want to control generation, you can pass various arguments\
          \ to the `generate` function, like `max_length`, `temperature`, `top_k`,\
          \ etc. However, you shouldn't directly modify the model's configuration\
          \ for these parameters.\n\n2. **Explicitly Setting `pad_token_id`**:\n\n\
          \   If you don\u2019t want the default behavior of setting `pad_token_id`\
          \ to `eos_token_id`, you should explicitly set `pad_token_id` during the\
          \ generation:\n\n   ```python\n   output = model.generate(input_ids, max_length=100,\
          \ pad_token_id=tokenizer.eos_token_id)\n   ```\n\nIn summary, always refer\
          \ to the official [Hugging Face documentation](https://huggingface.co/transformers/main_classes/text_generation.html)\
          \ or the linked URL in the warning for updated best practices and guidelines."
        updatedAt: '2023-08-18T05:57:51.664Z'
      numEdits: 0
      reactions: []
    id: 64df085f2802bd66a8b37d92
    type: comment
  author: jdart
  content: "From GPT-4:\n\nThe warning you're seeing is related to using a deprecated\
    \ method for controlling generation in the Hugging Face Transformers library.\n\
    \nLet's break down the warning and address the issues:\n\n1. **Modification of\
    \ the Pretrained Model Configuration**: The warning message indicates that directly\
    \ modifying the model's configuration for controlling generation is a deprecated\
    \ strategy. The solution Hugging Face recommends is to use a generation configuration\
    \ file.\n\n2. **Setting `pad_token_id` to `eos_token_id`**: This is an informational\
    \ message rather than a warning. In some generation tasks, when `pad_token_id`\
    \ is not specified, the `eos_token_id` (End-of-Sentence token ID) is used as the\
    \ `pad_token_id` by default. If you don\u2019t want this behavior, you should\
    \ explicitly set the `pad_token_id`.\n\n### Solutions:\n\n1. **Using Generation\
    \ Configuration**:\n   \n   Instead of modifying the model's configuration, you\
    \ should use `generation` methods provided by the model or the `generate` function\
    \ and pass the necessary arguments there.\n   \n   For example, let's say you\
    \ want to generate text using a model:\n   ```python\n   from transformers import\
    \ GPT2LMHeadModel, GPT2Tokenizer\n\n   model = GPT2LMHeadModel.from_pretrained(\"\
    gpt2-medium\")\n   tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n\
    \   \n   input_text = \"Once upon a time\"\n   input_ids = tokenizer.encode(input_text,\
    \ return_tensors=\"pt\")\n   \n   # Generate output\n   output = model.generate(input_ids,\
    \ max_length=100, num_return_sequences=5)\n   \n   # Decode the output\n   for\
    \ sequence in output:\n       print(tokenizer.decode(sequence, skip_special_tokens=True))\n\
    \   ```\n\n   If you want to control generation, you can pass various arguments\
    \ to the `generate` function, like `max_length`, `temperature`, `top_k`, etc.\
    \ However, you shouldn't directly modify the model's configuration for these parameters.\n\
    \n2. **Explicitly Setting `pad_token_id`**:\n\n   If you don\u2019t want the default\
    \ behavior of setting `pad_token_id` to `eos_token_id`, you should explicitly\
    \ set `pad_token_id` during the generation:\n\n   ```python\n   output = model.generate(input_ids,\
    \ max_length=100, pad_token_id=tokenizer.eos_token_id)\n   ```\n\nIn summary,\
    \ always refer to the official [Hugging Face documentation](https://huggingface.co/transformers/main_classes/text_generation.html)\
    \ or the linked URL in the warning for updated best practices and guidelines."
  created_at: 2023-08-18 04:57:51+00:00
  edited: false
  hidden: false
  id: 64df085f2802bd66a8b37d92
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: stabilityai/stablecode-instruct-alpha-3b
repo_type: model
status: open
target_branch: null
title: 'UserWarning: You have modified the pretrained model configuration to control
  generation'
