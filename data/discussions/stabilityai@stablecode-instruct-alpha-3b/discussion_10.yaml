!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tylersuard
conflicting_files: null
created_at: 2023-08-16 14:01:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
      fullname: Tyler Suard
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tylersuard
      type: user
    createdAt: '2023-08-16T15:01:05.000Z'
    data:
      edited: false
      editors:
      - Tylersuard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5185287594795227
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e82de9400b6e283a3fe5a72179fadd1.svg
          fullname: Tyler Suard
          isHf: false
          isPro: false
          name: Tylersuard
          type: user
        html: '<p>I tried running the example code in Colab, got this error</p>

          <p>ValueError                                Traceback (most recent call
          last)</p>

          <p> in &lt;cell line: 10&gt;()<br>      8 model.cuda()<br>      9 inputs
          = tokenizer("import torch\nimport torch.nn as nn", return_tensors="pt").to("cuda")<br>---&gt;
          10 tokens = model.generate(<br>     11   **inputs,<br>     12   max_new_tokens=48,</p>

          <p>2 frames</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py
          in _validate_model_kwargs(self, model_kwargs)<br>   1153<br>   1154         if
          unused_model_args:<br>-&gt; 1155             raise ValueError(<br>   1156                 f"The
          following <code>model_kwargs</code> are not used by the model: {unused_model_args}
          (note: typos in the"<br>   1157                 " generate arguments will
          also show up in this list)"</p>

          <p>ValueError: The following <code>model_kwargs</code> are not used by the
          model: [''token_type_ids''] (note: typos in the generate arguments will
          also show up in this list)</p>

          '
        raw: "I tried running the example code in Colab, got this error\r\n\r\nValueError\
          \                                Traceback (most recent call last)\r\n\r\
          \n<ipython-input-2-a7eb3532bea0> in <cell line: 10>()\r\n      8 model.cuda()\r\
          \n      9 inputs = tokenizer(\"import torch\\nimport torch.nn as nn\", return_tensors=\"\
          pt\").to(\"cuda\")\r\n---> 10 tokens = model.generate(\r\n     11   **inputs,\r\
          \n     12   max_new_tokens=48,\r\n\r\n2 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\
          \ in _validate_model_kwargs(self, model_kwargs)\r\n   1153 \r\n   1154 \
          \        if unused_model_args:\r\n-> 1155             raise ValueError(\r\
          \n   1156                 f\"The following `model_kwargs` are not used by\
          \ the model: {unused_model_args} (note: typos in the\"\r\n   1157      \
          \           \" generate arguments will also show up in this list)\"\r\n\r\
          \nValueError: The following `model_kwargs` are not used by the model: ['token_type_ids']\
          \ (note: typos in the generate arguments will also show up in this list)"
        updatedAt: '2023-08-16T15:01:05.988Z'
      numEdits: 0
      reactions: []
    id: 64dce4b1fdef63c2216a3a87
    type: comment
  author: Tylersuard
  content: "I tried running the example code in Colab, got this error\r\n\r\nValueError\
    \                                Traceback (most recent call last)\r\n\r\n<ipython-input-2-a7eb3532bea0>\
    \ in <cell line: 10>()\r\n      8 model.cuda()\r\n      9 inputs = tokenizer(\"\
    import torch\\nimport torch.nn as nn\", return_tensors=\"pt\").to(\"cuda\")\r\n\
    ---> 10 tokens = model.generate(\r\n     11   **inputs,\r\n     12   max_new_tokens=48,\r\
    \n\r\n2 frames\r\n\r\n/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\
    \ in _validate_model_kwargs(self, model_kwargs)\r\n   1153 \r\n   1154       \
    \  if unused_model_args:\r\n-> 1155             raise ValueError(\r\n   1156 \
    \                f\"The following `model_kwargs` are not used by the model: {unused_model_args}\
    \ (note: typos in the\"\r\n   1157                 \" generate arguments will\
    \ also show up in this list)\"\r\n\r\nValueError: The following `model_kwargs`\
    \ are not used by the model: ['token_type_ids'] (note: typos in the generate arguments\
    \ will also show up in this list)"
  created_at: 2023-08-16 14:01:05+00:00
  edited: false
  hidden: false
  id: 64dce4b1fdef63c2216a3a87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29adfb553eee5783c7700d5f955d87a4.svg
      fullname: Jacob Dart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdart
      type: user
    createdAt: '2023-08-18T05:54:42.000Z'
    data:
      edited: false
      editors:
      - jdart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8794835209846497
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29adfb553eee5783c7700d5f955d87a4.svg
          fullname: Jacob Dart
          isHf: false
          isPro: false
          name: jdart
          type: user
        html: '<p>This is what chatgpt had to say. </p>

          <p>The error you''re seeing suggests that the model you are using for generation
          does not make use of the argument <code>token_type_ids</code>. This can
          happen, for example, if you are using a model architecture or configuration
          that doesn''t require or support segment embeddings, which is the purpose
          of <code>token_type_ids</code>.</p>

          <p>Here''s how to resolve this:</p>

          <ol>

          <li>When tokenizing your inputs, ensure that you''re not producing <code>token_type_ids</code>
          if they''re not used by your model. You can do this by adjusting the <code>return_tensors</code>
          argument in your tokenizer call.</li>

          </ol>

          <p>For example, if you''re using the HuggingFace Transformers library:</p>

          <pre><code class="language-python">inputs = tokenizer(<span class="hljs-string">"import
          torch\nimport torch.nn as nn"</span>, return_tensors=<span class="hljs-string">"pt"</span>,
          add_token_type_ids=<span class="hljs-literal">False</span>).to(<span class="hljs-string">"cuda"</span>)

          </code></pre>

          <ol start="2">

          <li><p>If you''re sure your model does use <code>token_type_ids</code> but
          the error persists, make sure you''re using the right model and tokenizer
          pair. Some model architectures or configurations might not use <code>token_type_ids</code>.</p>

          </li>

          <li><p>Double-check that the model you''re using is compatible with the
          <code>generate</code> method. Not all models support generation. For example,
          if you''re trying to generate with a standard BERT model, you''ll encounter
          problems as BERT isn''t designed for sequence-to-sequence tasks out of the
          box.</p>

          </li>

          <li><p>Ensure that you''re using the latest version of the library. Sometimes
          bugs or incompatibilities can be resolved by simply updating the library.
          If you''re using Google Colab:</p>

          </li>

          </ol>

          <pre><code class="language-python">!pip install transformers --upgrade

          </code></pre>

          <p>Lastly, it''s always a good idea to check the official documentation
          or GitHub issues of the library you''re using to see if others have encountered
          a similar problem and if there''s a known solution or workaround.</p>

          '
        raw: "This is what chatgpt had to say. \n\nThe error you're seeing suggests\
          \ that the model you are using for generation does not make use of the argument\
          \ `token_type_ids`. This can happen, for example, if you are using a model\
          \ architecture or configuration that doesn't require or support segment\
          \ embeddings, which is the purpose of `token_type_ids`.\n\nHere's how to\
          \ resolve this:\n\n1. When tokenizing your inputs, ensure that you're not\
          \ producing `token_type_ids` if they're not used by your model. You can\
          \ do this by adjusting the `return_tensors` argument in your tokenizer call.\n\
          \nFor example, if you're using the HuggingFace Transformers library:\n\n\
          ```python\ninputs = tokenizer(\"import torch\\nimport torch.nn as nn\",\
          \ return_tensors=\"pt\", add_token_type_ids=False).to(\"cuda\")\n```\n\n\
          2. If you're sure your model does use `token_type_ids` but the error persists,\
          \ make sure you're using the right model and tokenizer pair. Some model\
          \ architectures or configurations might not use `token_type_ids`.\n\n3.\
          \ Double-check that the model you're using is compatible with the `generate`\
          \ method. Not all models support generation. For example, if you're trying\
          \ to generate with a standard BERT model, you'll encounter problems as BERT\
          \ isn't designed for sequence-to-sequence tasks out of the box.\n\n4. Ensure\
          \ that you're using the latest version of the library. Sometimes bugs or\
          \ incompatibilities can be resolved by simply updating the library. If you're\
          \ using Google Colab:\n\n```python\n!pip install transformers --upgrade\n\
          ```\n\nLastly, it's always a good idea to check the official documentation\
          \ or GitHub issues of the library you're using to see if others have encountered\
          \ a similar problem and if there's a known solution or workaround."
        updatedAt: '2023-08-18T05:54:42.947Z'
      numEdits: 0
      reactions: []
    id: 64df07a25bd74c3c8146d052
    type: comment
  author: jdart
  content: "This is what chatgpt had to say. \n\nThe error you're seeing suggests\
    \ that the model you are using for generation does not make use of the argument\
    \ `token_type_ids`. This can happen, for example, if you are using a model architecture\
    \ or configuration that doesn't require or support segment embeddings, which is\
    \ the purpose of `token_type_ids`.\n\nHere's how to resolve this:\n\n1. When tokenizing\
    \ your inputs, ensure that you're not producing `token_type_ids` if they're not\
    \ used by your model. You can do this by adjusting the `return_tensors` argument\
    \ in your tokenizer call.\n\nFor example, if you're using the HuggingFace Transformers\
    \ library:\n\n```python\ninputs = tokenizer(\"import torch\\nimport torch.nn as\
    \ nn\", return_tensors=\"pt\", add_token_type_ids=False).to(\"cuda\")\n```\n\n\
    2. If you're sure your model does use `token_type_ids` but the error persists,\
    \ make sure you're using the right model and tokenizer pair. Some model architectures\
    \ or configurations might not use `token_type_ids`.\n\n3. Double-check that the\
    \ model you're using is compatible with the `generate` method. Not all models\
    \ support generation. For example, if you're trying to generate with a standard\
    \ BERT model, you'll encounter problems as BERT isn't designed for sequence-to-sequence\
    \ tasks out of the box.\n\n4. Ensure that you're using the latest version of the\
    \ library. Sometimes bugs or incompatibilities can be resolved by simply updating\
    \ the library. If you're using Google Colab:\n\n```python\n!pip install transformers\
    \ --upgrade\n```\n\nLastly, it's always a good idea to check the official documentation\
    \ or GitHub issues of the library you're using to see if others have encountered\
    \ a similar problem and if there's a known solution or workaround."
  created_at: 2023-08-18 04:54:42+00:00
  edited: false
  hidden: false
  id: 64df07a25bd74c3c8146d052
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: stabilityai/stablecode-instruct-alpha-3b
repo_type: model
status: open
target_branch: null
title: Example code does not work
