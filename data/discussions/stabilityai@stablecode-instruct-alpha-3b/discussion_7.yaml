!!python/object:huggingface_hub.community.DiscussionWithDetails
author: axiopaladin
conflicting_files: null
created_at: 2023-08-10 16:52:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7oyMzkxzyh3n0AYTUcLpZ.png?w=200&h=200&f=face
      fullname: axiopaladin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: axiopaladin
      type: user
    createdAt: '2023-08-10T17:52:56.000Z'
    data:
      edited: false
      editors:
      - axiopaladin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4076663553714752
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7oyMzkxzyh3n0AYTUcLpZ.png?w=200&h=200&f=face
          fullname: axiopaladin
          isHf: false
          isPro: false
          name: axiopaladin
          type: user
        html: "<p>When running the sample snippet provided on the model page, it throws\
          \ this error (after downloading the tokenizer, config, safetensors, etc):<br><code>ValueError:\
          \ The following `model_kwargs` are not used by the model: ['token_type_ids']\
          \ (note: typos in the generate arguments will also show up in this list)</code></p>\n\
          <p>This is from running the snippet copied directly from the documentation\
          \ with no alterations. Python version 3.10.12, Pytorch version 2.1.0.dev20230705+cu121,\
          \ running with CUDA on a 10GB RTX 3080.</p>\n<p>Full traceback:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[1], line 10\n      8 model.cuda()\n      9 inputs = tokenizer(\"\
          ###Instruction\\nGenerate a python function to find number of CPU cores###Response\\\
          n\", return_tensors=\"pt\").to(\"cuda\")\n---&gt; 10 tokens = model.generate(\n\
          \     11   **inputs,\n     12   max_new_tokens=48,\n     13   temperature=0.2,\n\
          \     14   do_sample=True,\n     15 )\n     16 print(tokenizer.decode(tokens[0],\
          \ skip_special_tokens=True))\n\nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator..decorate_context(*args, **kwargs)\n    112 @functools.wraps(func)\n\
          \    113 def decorate_context(*args, **kwargs):\n    114     with ctx_factory():\n\
          --&gt; 115         return func(*args, **kwargs)\n\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1282,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\n   1280 model_kwargs = generation_config.update(**kwargs)\
          \  # All unused kwargs must be model kwargs\n   1281 generation_config.validate()\n\
          -&gt; 1282 self._validate_model_kwargs(model_kwargs.copy())\n   1284 # 2.\
          \ Set generation parameters if not already defined\n   1285 logits_processor\
          \ = logits_processor if logits_processor is not None else LogitsProcessorList()\n\
          \nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1155,\
          \ in GenerationMixin._validate_model_kwargs(self, model_kwargs)\n   1152\
          \         unused_model_args.append(key)\n   1154 if unused_model_args:\n\
          -&gt; 1155     raise ValueError(\n   1156         f\"The following `model_kwargs`\
          \ are not used by the model: {unused_model_args} (note: typos in the\"\n\
          \   1157         \" generate arguments will also show up in this list)\"\
          \n   1158     )\n\nValueError: The following `model_kwargs` are not used\
          \ by the model: ['token_type_ids'] (note: typos in the generate arguments\
          \ will also show up in this list)\n</code></pre>\n"
        raw: "When running the sample snippet provided on the model page, it throws\
          \ this error (after downloading the tokenizer, config, safetensors, etc):\r\
          \n``ValueError: The following `model_kwargs` are not used by the model:\
          \ ['token_type_ids'] (note: typos in the generate arguments will also show\
          \ up in this list)``\r\n\r\nThis is from running the snippet copied directly\
          \ from the documentation with no alterations. Python version 3.10.12, Pytorch\
          \ version 2.1.0.dev20230705+cu121, running with CUDA on a 10GB RTX 3080.\r\
          \n\r\nFull traceback:\r\n```\r\n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[1], line 10\r\n      8 model.cuda()\r\n      9 inputs\
          \ = tokenizer(\"###Instruction\\nGenerate a python function to find number\
          \ of CPU cores###Response\\n\", return_tensors=\"pt\").to(\"cuda\")\r\n\
          ---> 10 tokens = model.generate(\r\n     11   **inputs,\r\n     12   max_new_tokens=48,\r\
          \n     13   temperature=0.2,\r\n     14   do_sample=True,\r\n     15 )\r\
          \n     16 print(tokenizer.decode(tokens[0], skip_special_tokens=True))\r\
          \n\r\nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
          \ in context_decorator..decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\
          \n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\
          \n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1282,\
          \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
          \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model,\
          \ streamer, **kwargs)\r\n   1280 model_kwargs = generation_config.update(**kwargs)\
          \  # All unused kwargs must be model kwargs\r\n   1281 generation_config.validate()\r\
          \n-> 1282 self._validate_model_kwargs(model_kwargs.copy())\r\n   1284 #\
          \ 2. Set generation parameters if not already defined\r\n   1285 logits_processor\
          \ = logits_processor if logits_processor is not None else LogitsProcessorList()\r\
          \n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1155,\
          \ in GenerationMixin._validate_model_kwargs(self, model_kwargs)\r\n   1152\
          \         unused_model_args.append(key)\r\n   1154 if unused_model_args:\r\
          \n-> 1155     raise ValueError(\r\n   1156         f\"The following `model_kwargs`\
          \ are not used by the model: {unused_model_args} (note: typos in the\"\r\
          \n   1157         \" generate arguments will also show up in this list)\"\
          \r\n   1158     )\r\n\r\nValueError: The following `model_kwargs` are not\
          \ used by the model: ['token_type_ids'] (note: typos in the generate arguments\
          \ will also show up in this list)\r\n```"
        updatedAt: '2023-08-10T17:52:56.522Z'
      numEdits: 0
      reactions: []
    id: 64d523f8a3c9b9276153a137
    type: comment
  author: axiopaladin
  content: "When running the sample snippet provided on the model page, it throws\
    \ this error (after downloading the tokenizer, config, safetensors, etc):\r\n\
    ``ValueError: The following `model_kwargs` are not used by the model: ['token_type_ids']\
    \ (note: typos in the generate arguments will also show up in this list)``\r\n\
    \r\nThis is from running the snippet copied directly from the documentation with\
    \ no alterations. Python version 3.10.12, Pytorch version 2.1.0.dev20230705+cu121,\
    \ running with CUDA on a 10GB RTX 3080.\r\n\r\nFull traceback:\r\n```\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[1], line 10\r\n      8 model.cuda()\r\n      9 inputs = tokenizer(\"\
    ###Instruction\\nGenerate a python function to find number of CPU cores###Response\\\
    n\", return_tensors=\"pt\").to(\"cuda\")\r\n---> 10 tokens = model.generate(\r\
    \n     11   **inputs,\r\n     12   max_new_tokens=48,\r\n     13   temperature=0.2,\r\
    \n     14   do_sample=True,\r\n     15 )\r\n     16 print(tokenizer.decode(tokens[0],\
    \ skip_special_tokens=True))\r\n\r\nFile ~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115,\
    \ in context_decorator..decorate_context(*args, **kwargs)\r\n    112 @functools.wraps(func)\r\
    \n    113 def decorate_context(*args, **kwargs):\r\n    114     with ctx_factory():\r\
    \n--> 115         return func(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1282,\
    \ in GenerationMixin.generate(self, inputs, generation_config, logits_processor,\
    \ stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer,\
    \ **kwargs)\r\n   1280 model_kwargs = generation_config.update(**kwargs)  # All\
    \ unused kwargs must be model kwargs\r\n   1281 generation_config.validate()\r\
    \n-> 1282 self._validate_model_kwargs(model_kwargs.copy())\r\n   1284 # 2. Set\
    \ generation parameters if not already defined\r\n   1285 logits_processor = logits_processor\
    \ if logits_processor is not None else LogitsProcessorList()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1155,\
    \ in GenerationMixin._validate_model_kwargs(self, model_kwargs)\r\n   1152   \
    \      unused_model_args.append(key)\r\n   1154 if unused_model_args:\r\n-> 1155\
    \     raise ValueError(\r\n   1156         f\"The following `model_kwargs` are\
    \ not used by the model: {unused_model_args} (note: typos in the\"\r\n   1157\
    \         \" generate arguments will also show up in this list)\"\r\n   1158 \
    \    )\r\n\r\nValueError: The following `model_kwargs` are not used by the model:\
    \ ['token_type_ids'] (note: typos in the generate arguments will also show up\
    \ in this list)\r\n```"
  created_at: 2023-08-10 16:52:56+00:00
  edited: false
  hidden: false
  id: 64d523f8a3c9b9276153a137
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a6f32d6ef39f23d6145b84fcf2bf85.svg
      fullname: yong-jer chuang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yongjer
      type: user
    createdAt: '2023-08-13T04:11:54.000Z'
    data:
      edited: true
      editors:
      - yongjer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9926702380180359
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a6f32d6ef39f23d6145b84fcf2bf85.svg
          fullname: yong-jer chuang
          isHf: false
          isPro: false
          name: yongjer
          type: user
        html: '<p>I have faced the same issue, using colab </p>

          '
        raw: 'I have faced the same issue, using colab '
        updatedAt: '2023-08-13T04:13:12.187Z'
      numEdits: 1
      reactions: []
    id: 64d8580ac8d03cca8fcf76d2
    type: comment
  author: yongjer
  content: 'I have faced the same issue, using colab '
  created_at: 2023-08-13 03:11:54+00:00
  edited: true
  hidden: false
  id: 64d8580ac8d03cca8fcf76d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0876f1f1150621210598fef0d2f673f7.svg
      fullname: Enthu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIML434
      type: user
    createdAt: '2023-08-14T07:53:32.000Z'
    data:
      edited: false
      editors:
      - AIML434
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9048681259155273
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0876f1f1150621210598fef0d2f673f7.svg
          fullname: Enthu
          isHf: false
          isPro: false
          name: AIML434
          type: user
        html: '<blockquote>

          <p>I have faced the same issue, using colab</p>

          </blockquote>

          <p>Tell me, can you share exisitng code and ask it to debug? How? In the
          colab there is weird behavior on changing the prompt and adding my own code.
          In oogabaoga, the same thing. Can it only write code?</p>

          '
        raw: '> I have faced the same issue, using colab


          Tell me, can you share exisitng code and ask it to debug? How? In the colab
          there is weird behavior on changing the prompt and adding my own code. In
          oogabaoga, the same thing. Can it only write code?'
        updatedAt: '2023-08-14T07:53:32.332Z'
      numEdits: 0
      reactions: []
    id: 64d9dd7c92ebeb5b61356874
    type: comment
  author: AIML434
  content: '> I have faced the same issue, using colab


    Tell me, can you share exisitng code and ask it to debug? How? In the colab there
    is weird behavior on changing the prompt and adding my own code. In oogabaoga,
    the same thing. Can it only write code?'
  created_at: 2023-08-14 06:53:32+00:00
  edited: false
  hidden: false
  id: 64d9dd7c92ebeb5b61356874
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a6f32d6ef39f23d6145b84fcf2bf85.svg
      fullname: yong-jer chuang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yongjer
      type: user
    createdAt: '2023-08-14T09:56:52.000Z'
    data:
      edited: false
      editors:
      - yongjer
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.602923572063446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a6f32d6ef39f23d6145b84fcf2bf85.svg
          fullname: yong-jer chuang
          isHf: false
          isPro: false
          name: yongjer
          type: user
        html: "<blockquote>\n<blockquote>\n<p>I have faced the same issue, using colab</p>\n\
          </blockquote>\n<p>Tell me, can you share exisitng code and ask it to debug?\
          \ How? In the colab there is weird behavior on changing the prompt and adding\
          \ my own code. In oogabaoga, the same thing. Can it only write code?</p>\n\
          </blockquote>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/lBuFQPtGrwuW2Kr8bv2e8.png\"\
          ><img alt=\"\u622A\u5716 2023-08-14 \u4E0B\u53485.48.04.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/lBuFQPtGrwuW2Kr8bv2e8.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/mqb9Dmx_JY4Nd-B1VJ_qF.png\"\
          ><img alt=\"\u622A\u5716 2023-08-14 \u4E0B\u53485.49.24.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/mqb9Dmx_JY4Nd-B1VJ_qF.png\"\
          ></a></p>\n"
        raw: "> > I have faced the same issue, using colab\n> \n> Tell me, can you\
          \ share exisitng code and ask it to debug? How? In the colab there is weird\
          \ behavior on changing the prompt and adding my own code. In oogabaoga,\
          \ the same thing. Can it only write code?\n\n![\u622A\u5716 2023-08-14 \u4E0B\
          \u53485.48.04.png](https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/lBuFQPtGrwuW2Kr8bv2e8.png)\n\
          \n![\u622A\u5716 2023-08-14 \u4E0B\u53485.49.24.png](https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/mqb9Dmx_JY4Nd-B1VJ_qF.png)\n"
        updatedAt: '2023-08-14T09:56:52.230Z'
      numEdits: 0
      reactions: []
    id: 64d9fa643233e44e46bfa9db
    type: comment
  author: yongjer
  content: "> > I have faced the same issue, using colab\n> \n> Tell me, can you share\
    \ exisitng code and ask it to debug? How? In the colab there is weird behavior\
    \ on changing the prompt and adding my own code. In oogabaoga, the same thing.\
    \ Can it only write code?\n\n![\u622A\u5716 2023-08-14 \u4E0B\u53485.48.04.png](https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/lBuFQPtGrwuW2Kr8bv2e8.png)\n\
    \n![\u622A\u5716 2023-08-14 \u4E0B\u53485.49.24.png](https://cdn-uploads.huggingface.co/production/uploads/6352348fbeef5fb9913c62f9/mqb9Dmx_JY4Nd-B1VJ_qF.png)\n"
  created_at: 2023-08-14 08:56:52+00:00
  edited: false
  hidden: false
  id: 64d9fa643233e44e46bfa9db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bc3e3a078757e86d1cee0b3388f4db5.svg
      fullname: Junyi Ye
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jy395
      type: user
    createdAt: '2023-08-15T23:10:51.000Z'
    data:
      edited: true
      editors:
      - jy395
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.28550106287002563
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bc3e3a078757e86d1cee0b3388f4db5.svg
          fullname: Junyi Ye
          isHf: false
          isPro: false
          name: jy395
          type: user
        html: "<pre><code>inputs = tokenizer(\"###Instruction\\nGenerate a python\
          \ function to find number of CPU cores###Response\\n\", return_tensors=\"\
          pt\").to(\"cuda\")\n\n# Removing 'token_type_ids' from the inputs dictionary\
          \ resolved the error\nif 'token_type_ids' in inputs:\n    del inputs['token_type_ids']\n\
          \ntokens = model.generate(\n</code></pre>\n"
        raw: "```\ninputs = tokenizer(\"###Instruction\\nGenerate a python function\
          \ to find number of CPU cores###Response\\n\", return_tensors=\"pt\").to(\"\
          cuda\")\n\n# Removing 'token_type_ids' from the inputs dictionary resolved\
          \ the error\nif 'token_type_ids' in inputs:\n    del inputs['token_type_ids']\n\
          \ntokens = model.generate(\n```"
        updatedAt: '2023-08-15T23:12:30.195Z'
      numEdits: 2
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - jy395
        - noobmaster29
        - technohead
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - jy395
        - noobmaster29
    id: 64dc05fb1855ce11cd6daf42
    type: comment
  author: jy395
  content: "```\ninputs = tokenizer(\"###Instruction\\nGenerate a python function\
    \ to find number of CPU cores###Response\\n\", return_tensors=\"pt\").to(\"cuda\"\
    )\n\n# Removing 'token_type_ids' from the inputs dictionary resolved the error\n\
    if 'token_type_ids' in inputs:\n    del inputs['token_type_ids']\n\ntokens = model.generate(\n\
    ```"
  created_at: 2023-08-15 22:10:51+00:00
  edited: true
  hidden: false
  id: 64dc05fb1855ce11cd6daf42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f28833f4c63b1bf56398ae414fa7322.svg
      fullname: Jon N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joenas
      type: user
    createdAt: '2023-08-16T07:54:46.000Z'
    data:
      edited: false
      editors:
      - joenas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5846784114837646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f28833f4c63b1bf56398ae414fa7322.svg
          fullname: Jon N
          isHf: false
          isPro: false
          name: joenas
          type: user
        html: '<p>You can also use fix provided here: <a href="https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b/discussions/2#64d30f314eb2ea6d5d8e118a">https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b/discussions/2#64d30f314eb2ea6d5d8e118a</a></p>

          '
        raw: 'You can also use fix provided here: https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b/discussions/2#64d30f314eb2ea6d5d8e118a'
        updatedAt: '2023-08-16T07:54:46.914Z'
      numEdits: 0
      reactions: []
    id: 64dc80c67f8116a6ab72c622
    type: comment
  author: joenas
  content: 'You can also use fix provided here: https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b/discussions/2#64d30f314eb2ea6d5d8e118a'
  created_at: 2023-08-16 06:54:46+00:00
  edited: false
  hidden: false
  id: 64dc80c67f8116a6ab72c622
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e306316db70422f864ca887351b89fce.svg
      fullname: san
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kephalian
      type: user
    createdAt: '2023-08-16T14:07:16.000Z'
    data:
      edited: true
      editors:
      - kephalian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9195398688316345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e306316db70422f864ca887351b89fce.svg
          fullname: san
          isHf: false
          isPro: false
          name: kephalian
          type: user
        html: '<p>Same error on Colab. Speaks volume about ease of use and user friendliness
          if their proverbial "Hello world" gives errors as output. Such a difficult
          model or program causing user frustration is bound to fail. I guess they
          are headed the Android Studio way!</p>

          '
        raw: Same error on Colab. Speaks volume about ease of use and user friendliness
          if their proverbial "Hello world" gives errors as output. Such a difficult
          model or program causing user frustration is bound to fail. I guess they
          are headed the Android Studio way!
        updatedAt: '2023-08-16T14:07:53.451Z'
      numEdits: 1
      reactions: []
    id: 64dcd8143fe31a792a4e4628
    type: comment
  author: kephalian
  content: Same error on Colab. Speaks volume about ease of use and user friendliness
    if their proverbial "Hello world" gives errors as output. Such a difficult model
    or program causing user frustration is bound to fail. I guess they are headed
    the Android Studio way!
  created_at: 2023-08-16 13:07:16+00:00
  edited: true
  hidden: false
  id: 64dcd8143fe31a792a4e4628
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8f8e6a71667884314508781710fadec.svg
      fullname: Koji Hirayama
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: technohead
      type: user
    createdAt: '2023-08-30T15:13:25.000Z'
    data:
      edited: false
      editors:
      - technohead
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8923729062080383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8f8e6a71667884314508781710fadec.svg
          fullname: Koji Hirayama
          isHf: false
          isPro: false
          name: technohead
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jy395&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jy395\">@<span class=\"\
          underline\">jy395</span></a></span>\n\n\t</span></span><br>I worked miracles.\
          \  Thanks!</p>\n"
        raw: "@jy395 \nI worked miracles.  Thanks!"
        updatedAt: '2023-08-30T15:13:25.645Z'
      numEdits: 0
      reactions: []
    id: 64ef5c9561013353365c0e1c
    type: comment
  author: technohead
  content: "@jy395 \nI worked miracles.  Thanks!"
  created_at: 2023-08-30 14:13:25+00:00
  edited: false
  hidden: false
  id: 64ef5c9561013353365c0e1c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: stabilityai/stablecode-instruct-alpha-3b
repo_type: model
status: open
target_branch: null
title: 'ValueError: `model_kwargs` are not used by the model'
