!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TiSy
conflicting_files: null
created_at: 2023-05-11 09:03:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3fdc0e114b1bc5800810ac41c2603808.svg
      fullname: Tim Debre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TiSy
      type: user
    createdAt: '2023-05-11T10:03:32.000Z'
    data:
      edited: false
      editors:
      - TiSy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3fdc0e114b1bc5800810ac41c2603808.svg
          fullname: Tim Debre
          isHf: false
          isPro: false
          name: TiSy
          type: user
        html: '<p>Hallo,</p>

          <p>I am a beginner at AI and image embeddings and I have been wanting to
          try out your model. However when I encode just a couple images my RAM quickly
          reaches it''s limit. I have 32 GB of RAM (22GB which are actually avaiable)
          but it only takes a handful of images to encode to use up all of that. Any
          ideas what I am doing wrong or how I could work around that? This problem
          isn''t very model specific as I have the same problem with vit_large_patch16_224.augreg_in21k_ft_in1k,
          vit_large_patch14_clip_224.openai_ft_in12k_in1k or convnext_xxlarge.clip_laion2b_soup_ft_in1k.</p>

          <p>Sincerely Tim</p>

          '
        raw: "Hallo,\r\n\r\nI am a beginner at AI and image embeddings and I have\
          \ been wanting to try out your model. However when I encode just a couple\
          \ images my RAM quickly reaches it's limit. I have 32 GB of RAM (22GB which\
          \ are actually avaiable) but it only takes a handful of images to encode\
          \ to use up all of that. Any ideas what I am doing wrong or how I could\
          \ work around that? This problem isn't very model specific as I have the\
          \ same problem with vit_large_patch16_224.augreg_in21k_ft_in1k, vit_large_patch14_clip_224.openai_ft_in12k_in1k\
          \ or convnext_xxlarge.clip_laion2b_soup_ft_in1k.\r\n\r\nSincerely Tim"
        updatedAt: '2023-05-11T10:03:32.240Z'
      numEdits: 0
      reactions: []
    id: 645cbd74a03f3ebb0bde55fb
    type: comment
  author: TiSy
  content: "Hallo,\r\n\r\nI am a beginner at AI and image embeddings and I have been\
    \ wanting to try out your model. However when I encode just a couple images my\
    \ RAM quickly reaches it's limit. I have 32 GB of RAM (22GB which are actually\
    \ avaiable) but it only takes a handful of images to encode to use up all of that.\
    \ Any ideas what I am doing wrong or how I could work around that? This problem\
    \ isn't very model specific as I have the same problem with vit_large_patch16_224.augreg_in21k_ft_in1k,\
    \ vit_large_patch14_clip_224.openai_ft_in12k_in1k or convnext_xxlarge.clip_laion2b_soup_ft_in1k.\r\
    \n\r\nSincerely Tim"
  created_at: 2023-05-11 09:03:32+00:00
  edited: false
  hidden: false
  id: 645cbd74a03f3ebb0bde55fb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: timm/eva02_large_patch14_448.mim_m38m_ft_in1k
repo_type: model
status: open
target_branch: null
title: not enough RAM to compute embeddings
