!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Giulianini
conflicting_files: null
created_at: 2023-09-24 09:15:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9f28bcf5014319f48e3207e9280205c.svg
      fullname: Nini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Giulianini
      type: user
    createdAt: '2023-09-24T10:15:36.000Z'
    data:
      edited: true
      editors:
      - Giulianini
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4534875750541687
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9f28bcf5014319f48e3207e9280205c.svg
          fullname: Nini
          isHf: false
          isPro: false
          name: Giulianini
          type: user
        html: "<pre><code># model_path = r\"TheBloke/Llama-2-7b-Chat-GPTQ\"\n    #\
          \ llm = Llama(model_path=model_path)\n    # llm = CTransformers(model=model_path,\
          \ model_type=\"gptq\", model_file=\"gptq_model-4bit-128g.safetensors\",\
          \ lib='avx')\n\n    model_name_or_path = \"TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ\"\
          \ # \"TheBloke/llama2_7b_chat_uncensored-GPTQ\"  # \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"\
          \  # \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n    model_basename = \"gptq_model-4bit-128g\"\
          \n\n    use_triton = False\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\n\n    # model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \    #                                            # model_basename=model_basename,\n\
          \    #                                            use_safetensors=True,\n\
          \    #                                            trust_remote_code=True,\n\
          \    #                                            device=\"cuda:0\",\n \
          \   #                                            use_triton=use_triton,\n\
          \    #                                            quantize_config=None)\n\
          \n    model = AutoAWQForCausalLM.from_quantized(model_name_or_path,\n  \
          \                                             # model_basename=model_basename,\n\
          \                                               safetensors=True,\n    \
          \                                           trust_remote_code=True,\n  \
          \                                             fuse_layers=True)\n</code></pre>\n\
          <p>Exception:</p>\n<pre><code>Traceback (most recent call last):\n  File\
          \ \"C:\\Program Files\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\\
          pydev\\pydevd.py\", line 1500, in _exec\n    pydev_imports.execfile(file,\
          \ globals, locals)  # execute the script\n  File \"C:\\Program Files\\JetBrains\\\
          PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\"\
          , line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'),\
          \ glob, loc)\n  File \"C:\\Users\\user\\Desktop\\cuda-test\\main.py\", line\
          \ 30, in &lt;module&gt;\n    model = AutoAWQForCausalLM.from_quantized(model_name_or_path,\n\
          \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
          awq\\models\\auto.py\", line 46, in from_quantized\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n\
          \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
          awq\\models\\base.py\", line 160, in from_quantized\n    load_checkpoint_in_model(\n\
          \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
          accelerate\\utils\\modeling.py\", line 1245, in load_checkpoint_in_model\n\
          \    raise ValueError(\nValueError: C:\\Users\\user\\.cache\\huggingface\\\
          hub\\models--TheBloke--Wizard-Vicuna-7B-Uncensored-AWQ\\snapshots\\74b9fa629a898caab05b0bb854d52890c9a3d62b\
          \ is not a folder containing a `.index.json` file or a pytorch_model.bin\
          \ file\npython-BaseException\n</code></pre>\n"
        raw: "```\n# model_path = r\"TheBloke/Llama-2-7b-Chat-GPTQ\"\n    # llm =\
          \ Llama(model_path=model_path)\n    # llm = CTransformers(model=model_path,\
          \ model_type=\"gptq\", model_file=\"gptq_model-4bit-128g.safetensors\",\
          \ lib='avx')\n\n    model_name_or_path = \"TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ\"\
          \ # \"TheBloke/llama2_7b_chat_uncensored-GPTQ\"  # \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"\
          \  # \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n    model_basename = \"gptq_model-4bit-128g\"\
          \n\n    use_triton = False\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\n\n    # model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \    #                                            # model_basename=model_basename,\n\
          \    #                                            use_safetensors=True,\n\
          \    #                                            trust_remote_code=True,\n\
          \    #                                            device=\"cuda:0\",\n \
          \   #                                            use_triton=use_triton,\n\
          \    #                                            quantize_config=None)\n\
          \n    model = AutoAWQForCausalLM.from_quantized(model_name_or_path,\n  \
          \                                             # model_basename=model_basename,\n\
          \                                               safetensors=True,\n    \
          \                                           trust_remote_code=True,\n  \
          \                                             fuse_layers=True)\n```\n\n\
          Exception:\n```\nTraceback (most recent call last):\n  File \"C:\\Program\
          \ Files\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\pydevd.py\"\
          , line 1500, in _exec\n    pydev_imports.execfile(file, globals, locals)\
          \  # execute the script\n  File \"C:\\Program Files\\JetBrains\\PyCharm\
          \ 2023.2\\plugins\\python\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py\"\
          , line 18, in execfile\n    exec(compile(contents+\"\\n\", file, 'exec'),\
          \ glob, loc)\n  File \"C:\\Users\\user\\Desktop\\cuda-test\\main.py\", line\
          \ 30, in <module>\n    model = AutoAWQForCausalLM.from_quantized(model_name_or_path,\n\
          \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
          awq\\models\\auto.py\", line 46, in from_quantized\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n\
          \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
          awq\\models\\base.py\", line 160, in from_quantized\n    load_checkpoint_in_model(\n\
          \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
          accelerate\\utils\\modeling.py\", line 1245, in load_checkpoint_in_model\n\
          \    raise ValueError(\nValueError: C:\\Users\\user\\.cache\\huggingface\\\
          hub\\models--TheBloke--Wizard-Vicuna-7B-Uncensored-AWQ\\snapshots\\74b9fa629a898caab05b0bb854d52890c9a3d62b\
          \ is not a folder containing a `.index.json` file or a pytorch_model.bin\
          \ file\npython-BaseException\n```"
        updatedAt: '2023-09-24T10:19:42.195Z'
      numEdits: 1
      reactions: []
    id: 65100c48f141bc34f93c2223
    type: comment
  author: Giulianini
  content: "```\n# model_path = r\"TheBloke/Llama-2-7b-Chat-GPTQ\"\n    # llm = Llama(model_path=model_path)\n\
    \    # llm = CTransformers(model=model_path, model_type=\"gptq\", model_file=\"\
    gptq_model-4bit-128g.safetensors\", lib='avx')\n\n    model_name_or_path = \"\
    TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ\" # \"TheBloke/llama2_7b_chat_uncensored-GPTQ\"\
    \  # \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"  # \"TheBloke/Llama-2-7b-Chat-GPTQ\"\
    \n    model_basename = \"gptq_model-4bit-128g\"\n\n    use_triton = False\n\n\
    \    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n\
    \n    # model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n    #\
    \                                            # model_basename=model_basename,\n\
    \    #                                            use_safetensors=True,\n    #\
    \                                            trust_remote_code=True,\n    #  \
    \                                          device=\"cuda:0\",\n    #         \
    \                                   use_triton=use_triton,\n    #            \
    \                                quantize_config=None)\n\n    model = AutoAWQForCausalLM.from_quantized(model_name_or_path,\n\
    \                                               # model_basename=model_basename,\n\
    \                                               safetensors=True,\n          \
    \                                     trust_remote_code=True,\n              \
    \                                 fuse_layers=True)\n```\n\nException:\n```\n\
    Traceback (most recent call last):\n  File \"C:\\Program Files\\JetBrains\\PyCharm\
    \ 2023.2\\plugins\\python\\helpers\\pydev\\pydevd.py\", line 1500, in _exec\n\
    \    pydev_imports.execfile(file, globals, locals)  # execute the script\n  File\
    \ \"C:\\Program Files\\JetBrains\\PyCharm 2023.2\\plugins\\python\\helpers\\pydev\\\
    _pydev_imps\\_pydev_execfile.py\", line 18, in execfile\n    exec(compile(contents+\"\
    \\n\", file, 'exec'), glob, loc)\n  File \"C:\\Users\\user\\Desktop\\cuda-test\\\
    main.py\", line 30, in <module>\n    model = AutoAWQForCausalLM.from_quantized(model_name_or_path,\n\
    \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
    awq\\models\\auto.py\", line 46, in from_quantized\n    return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\n\
    \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
    awq\\models\\base.py\", line 160, in from_quantized\n    load_checkpoint_in_model(\n\
    \  File \"C:\\Users\\user\\AppData\\Local\\anaconda3\\envs\\ai\\lib\\site-packages\\\
    accelerate\\utils\\modeling.py\", line 1245, in load_checkpoint_in_model\n   \
    \ raise ValueError(\nValueError: C:\\Users\\user\\.cache\\huggingface\\hub\\models--TheBloke--Wizard-Vicuna-7B-Uncensored-AWQ\\\
    snapshots\\74b9fa629a898caab05b0bb854d52890c9a3d62b is not a folder containing\
    \ a `.index.json` file or a pytorch_model.bin file\npython-BaseException\n```"
  created_at: 2023-09-24 09:15:36+00:00
  edited: true
  hidden: false
  id: 65100c48f141bc34f93c2223
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-24T11:55:01.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8988519310951233
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hi, I'm confused about what code you're running exactly, and for\
          \ what models.  This model is Wizard-Vicuna-7B-Uncensored-AWQ, but the code\
          \ you listed above shows Llama-2-7B-GPTQ, which is a different model in\
          \ a different quant format.  You also show you running <code>llm = CTransformers</code>\
          \ which is for GGUF models, not GPTQ or AWQ.</p>\n<p>Please refer to the\
          \ README of this model to see the example Python code to use with this AWQ\
          \ model.  I just tested it on a fresh system, and the generate example worked\
          \ fine:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> awq <span class=\"hljs-keyword\">import</span> AutoAWQForCausalLM\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer\n\nmodel_name_or_path = <span class=\"hljs-string\"\
          >\"TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ\"</span>\n\n<span class=\"hljs-comment\"\
          ># Load model</span>\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
          \ fuse_layers=<span class=\"hljs-literal\">True</span>,\n              \
          \                            trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>, safetensors=<span class=\"hljs-literal\">True</span>)\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n\nprompt = <span class=\"hljs-string\"\
          >\"Tell me about AI\"</span>\nprompt_template=<span class=\"hljs-string\"\
          >f'''A chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions. USER: <span class=\"hljs-subst\">{prompt}</span> ASSISTANT:</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">'''</span>\n\
          \n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"\\n\\n*** Generate:\"</span>)\n\ntokens = tokenizer(\n    prompt_template,\n\
          \    return_tensors=<span class=\"hljs-string\">'pt'</span>\n).input_ids.cuda()\n\
          \n<span class=\"hljs-comment\"># Generate output</span>\ngeneration_output\
          \ = model.generate(\n    tokens,\n    do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    top_k=<span class=\"\
          hljs-number\">40</span>,\n    max_new_tokens=<span class=\"hljs-number\"\
          >512</span>\n)\n\n<span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"Output: \"</span>, tokenizer.decode(generation_output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>Output:</p>\n<pre><code>***\
          \ Generate:\nOutput:  &lt;s&gt; A chat between a curious user and an artificial\
          \ intelligence assistant. The assistant gives helpful, detailed, and polite\
          \ answers to the user's questions. USER: Tell me about AI ASSISTANT:\n\n\
          Artificial Intelligence (AI) is a branch of computer science that deals\
          \ with creating intelligent machines that can perform tasks that typically\
          \ require human intelligence, such as visual perception, speech recognition,\
          \ decision-making, and language translation. AI is divided into two categories:\
          \ narrow or weak AI and general or strong AI. Narrow AI is designed to perform\
          \ specific tasks, while general AI is capable of learning and adapting to\
          \ new situations and tasks.\n\nAI uses various techniques such as machine\
          \ learning, deep learning, natural language processing, and robotics to\
          \ simulate human intelligence. AI can be applied in various fields such\
          \ as healthcare, finance, education, transportation, and entertainment.\n\
          \nOne of the most significant advantages of AI is its ability to process\
          \ large amounts of data and identify patterns and trends that humans may\
          \ miss. This can lead to better decision-making, improved efficiency, and\
          \ cost savings. AI can also improve the accuracy of diagnoses in healthcare,\
          \ improve customer service in retail, and enhance security in cybersecurity.\n\
          \nHowever, there are also concerns about the impact of AI on jobs, privacy,\
          \ and security. Some fear that AI will replace human workers, leading to\
          \ unemployment and economic inequality. Others worry about the potential\
          \ misuse of AI, such as in surveillance or cyberattacks.\n\nDespite these\
          \ concerns, the potential benefits of AI are too great to ignore. As AI\
          \ continues to evolve, it has the potential to transform the way we live\
          \ and work, making our lives easier, safer, and more efficient.&lt;/s&gt;\n\
          </code></pre>\n<p>Note: my example includes a pipeline example. It seems\
          \ this does not work with AutoAWQ.  But <code>model.generate()</code> does\
          \ work fine.  I will remove the pipeline example from my READMEs shortly,\
          \ until this is fixed in AutoAWQ.</p>\n"
        raw: "Hi, I'm confused about what code you're running exactly, and for what\
          \ models.  This model is Wizard-Vicuna-7B-Uncensored-AWQ, but the code you\
          \ listed above shows Llama-2-7B-GPTQ, which is a different model in a different\
          \ quant format.  You also show you running `llm = CTransformers` which is\
          \ for GGUF models, not GPTQ or AWQ.\n\nPlease refer to the README of this\
          \ model to see the example Python code to use with this AWQ model.  I just\
          \ tested it on a fresh system, and the generate example worked fine:\n\n\
          ```python\nfrom awq import AutoAWQForCausalLM\nfrom transformers import\
          \ AutoTokenizer\n\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ\"\
          \n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
          \ fuse_layers=True,\n                                          trust_remote_code=True,\
          \ safetensors=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A\
          \ chat between a curious user and an artificial intelligence assistant.\
          \ The assistant gives helpful, detailed, and polite answers to the user's\
          \ questions. USER: {prompt} ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ntokens = tokenizer(\n    prompt_template,\n    return_tensors='pt'\n\
          ).input_ids.cuda()\n\n# Generate output\ngeneration_output = model.generate(\n\
          \    tokens,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n\
          \    top_k=40,\n    max_new_tokens=512\n)\n\nprint(\"Output: \", tokenizer.decode(generation_output[0]))\n\
          \n```\n\nOutput:\n```\n*** Generate:\nOutput:  <s> A chat between a curious\
          \ user and an artificial intelligence assistant. The assistant gives helpful,\
          \ detailed, and polite answers to the user's questions. USER: Tell me about\
          \ AI ASSISTANT:\n\nArtificial Intelligence (AI) is a branch of computer\
          \ science that deals with creating intelligent machines that can perform\
          \ tasks that typically require human intelligence, such as visual perception,\
          \ speech recognition, decision-making, and language translation. AI is divided\
          \ into two categories: narrow or weak AI and general or strong AI. Narrow\
          \ AI is designed to perform specific tasks, while general AI is capable\
          \ of learning and adapting to new situations and tasks.\n\nAI uses various\
          \ techniques such as machine learning, deep learning, natural language processing,\
          \ and robotics to simulate human intelligence. AI can be applied in various\
          \ fields such as healthcare, finance, education, transportation, and entertainment.\n\
          \nOne of the most significant advantages of AI is its ability to process\
          \ large amounts of data and identify patterns and trends that humans may\
          \ miss. This can lead to better decision-making, improved efficiency, and\
          \ cost savings. AI can also improve the accuracy of diagnoses in healthcare,\
          \ improve customer service in retail, and enhance security in cybersecurity.\n\
          \nHowever, there are also concerns about the impact of AI on jobs, privacy,\
          \ and security. Some fear that AI will replace human workers, leading to\
          \ unemployment and economic inequality. Others worry about the potential\
          \ misuse of AI, such as in surveillance or cyberattacks.\n\nDespite these\
          \ concerns, the potential benefits of AI are too great to ignore. As AI\
          \ continues to evolve, it has the potential to transform the way we live\
          \ and work, making our lives easier, safer, and more efficient.</s>\n```\n\
          \nNote: my example includes a pipeline example. It seems this does not work\
          \ with AutoAWQ.  But `model.generate()` does work fine.  I will remove the\
          \ pipeline example from my READMEs shortly, until this is fixed in AutoAWQ."
        updatedAt: '2023-09-24T11:55:01.881Z'
      numEdits: 0
      reactions: []
    id: 6510239557343388e596ee71
    type: comment
  author: TheBloke
  content: "Hi, I'm confused about what code you're running exactly, and for what\
    \ models.  This model is Wizard-Vicuna-7B-Uncensored-AWQ, but the code you listed\
    \ above shows Llama-2-7B-GPTQ, which is a different model in a different quant\
    \ format.  You also show you running `llm = CTransformers` which is for GGUF models,\
    \ not GPTQ or AWQ.\n\nPlease refer to the README of this model to see the example\
    \ Python code to use with this AWQ model.  I just tested it on a fresh system,\
    \ and the generate example worked fine:\n\n```python\nfrom awq import AutoAWQForCausalLM\n\
    from transformers import AutoTokenizer\n\nmodel_name_or_path = \"TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ\"\
    \n\n# Load model\nmodel = AutoAWQForCausalLM.from_quantized(model_name_or_path,\
    \ fuse_layers=True,\n                                          trust_remote_code=True,\
    \ safetensors=True)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ trust_remote_code=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''A\
    \ chat between a curious user and an artificial intelligence assistant. The assistant\
    \ gives helpful, detailed, and polite answers to the user's questions. USER: {prompt}\
    \ ASSISTANT:\n\n'''\n\nprint(\"\\n\\n*** Generate:\")\n\ntokens = tokenizer(\n\
    \    prompt_template,\n    return_tensors='pt'\n).input_ids.cuda()\n\n# Generate\
    \ output\ngeneration_output = model.generate(\n    tokens,\n    do_sample=True,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    max_new_tokens=512\n\
    )\n\nprint(\"Output: \", tokenizer.decode(generation_output[0]))\n\n```\n\nOutput:\n\
    ```\n*** Generate:\nOutput:  <s> A chat between a curious user and an artificial\
    \ intelligence assistant. The assistant gives helpful, detailed, and polite answers\
    \ to the user's questions. USER: Tell me about AI ASSISTANT:\n\nArtificial Intelligence\
    \ (AI) is a branch of computer science that deals with creating intelligent machines\
    \ that can perform tasks that typically require human intelligence, such as visual\
    \ perception, speech recognition, decision-making, and language translation. AI\
    \ is divided into two categories: narrow or weak AI and general or strong AI.\
    \ Narrow AI is designed to perform specific tasks, while general AI is capable\
    \ of learning and adapting to new situations and tasks.\n\nAI uses various techniques\
    \ such as machine learning, deep learning, natural language processing, and robotics\
    \ to simulate human intelligence. AI can be applied in various fields such as\
    \ healthcare, finance, education, transportation, and entertainment.\n\nOne of\
    \ the most significant advantages of AI is its ability to process large amounts\
    \ of data and identify patterns and trends that humans may miss. This can lead\
    \ to better decision-making, improved efficiency, and cost savings. AI can also\
    \ improve the accuracy of diagnoses in healthcare, improve customer service in\
    \ retail, and enhance security in cybersecurity.\n\nHowever, there are also concerns\
    \ about the impact of AI on jobs, privacy, and security. Some fear that AI will\
    \ replace human workers, leading to unemployment and economic inequality. Others\
    \ worry about the potential misuse of AI, such as in surveillance or cyberattacks.\n\
    \nDespite these concerns, the potential benefits of AI are too great to ignore.\
    \ As AI continues to evolve, it has the potential to transform the way we live\
    \ and work, making our lives easier, safer, and more efficient.</s>\n```\n\nNote:\
    \ my example includes a pipeline example. It seems this does not work with AutoAWQ.\
    \  But `model.generate()` does work fine.  I will remove the pipeline example\
    \ from my READMEs shortly, until this is fixed in AutoAWQ."
  created_at: 2023-09-24 10:55:01+00:00
  edited: false
  hidden: false
  id: 6510239557343388e596ee71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9f28bcf5014319f48e3207e9280205c.svg
      fullname: Nini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Giulianini
      type: user
    createdAt: '2023-09-24T13:22:47.000Z'
    data:
      edited: false
      editors:
      - Giulianini
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9172269105911255
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9f28bcf5014319f48e3207e9280205c.svg
          fullname: Nini
          isHf: false
          isPro: false
          name: Giulianini
          type: user
        html: '<p>I was trying to switch from GPTQ to AWQ. Installed autoawq from
          pip but got the error posted here.<br>Then I now tried to install from source
          autoawq but everything blocked cos my 1660super is not compatible with compute
          capability 8 (it''s 7). Looking to autoawq page, they require a minimum
          compatibility of Ampere Nvidia models (compute capability = 8).<br>I close
          the issue </p>

          '
        raw: "I was trying to switch from GPTQ to AWQ. Installed autoawq from pip\
          \ but got the error posted here. \nThen I now tried to install from source\
          \ autoawq but everything blocked cos my 1660super is not compatible with\
          \ compute capability 8 (it's 7). Looking to autoawq page, they require a\
          \ minimum compatibility of Ampere Nvidia models (compute capability = 8).\
          \ \nI close the issue "
        updatedAt: '2023-09-24T13:22:47.880Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65103827e92f215ce5930ee8
    id: 65103827e92f215ce5930ee7
    type: comment
  author: Giulianini
  content: "I was trying to switch from GPTQ to AWQ. Installed autoawq from pip but\
    \ got the error posted here. \nThen I now tried to install from source autoawq\
    \ but everything blocked cos my 1660super is not compatible with compute capability\
    \ 8 (it's 7). Looking to autoawq page, they require a minimum compatibility of\
    \ Ampere Nvidia models (compute capability = 8). \nI close the issue "
  created_at: 2023-09-24 12:22:47+00:00
  edited: false
  hidden: false
  id: 65103827e92f215ce5930ee7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c9f28bcf5014319f48e3207e9280205c.svg
      fullname: Nini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Giulianini
      type: user
    createdAt: '2023-09-24T13:22:47.000Z'
    data:
      status: closed
    id: 65103827e92f215ce5930ee8
    type: status-change
  author: Giulianini
  created_at: 2023-09-24 12:22:47+00:00
  id: 65103827e92f215ce5930ee8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Wizard-Vicuna-7B-Uncensored-AWQ
repo_type: model
status: closed
target_branch: null
title: Error using AWQ model
