!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SenyorDrew
conflicting_files: null
created_at: 2022-08-31 20:41:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1196e290bbc79eeea26d8af5e04fcd6.svg
      fullname: Andrew Wollacott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SenyorDrew
      type: user
    createdAt: '2022-08-31T21:41:23.000Z'
    data:
      edited: false
      editors:
      - SenyorDrew
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1196e290bbc79eeea26d8af5e04fcd6.svg
          fullname: Andrew Wollacott
          isHf: false
          isPro: false
          name: SenyorDrew
          type: user
        html: '<p>Thanks for making such an awesome resource available.  I had a question
          about the lengths of designed proteins - it seems this question was asked
          previously, but I don''t think I understand the answer.  I''ve followed
          the instructions for generating protein sequences:<br>sequences = protgpt2("M",
          min_length=50, max_length=70, do_sample=True, top_k=950, repetition_penalty=1.2,
          num_return_sequences=10, eos_token_id=0)</p>

          <p>I trying to ask that the protein sequences be no longer than 70 amino
          acids, but most of the sequences are much longer (most are ~180 AA, with
          the smallest having 98AA''s).  They also contain multiple "\n" tokens.<br>Questions:</p>

          <ol>

          <li>How should the "\n" tokens be interpreted?  Should I just ignore them?</li>

          <li>How can I force the output designed sequences to actually be between
          50 and 70 amino acids in length?</li>

          </ol>

          <p>Thanks in advance for the help.</p>

          '
        raw: "Thanks for making such an awesome resource available.  I had a question\
          \ about the lengths of designed proteins - it seems this question was asked\
          \ previously, but I don't think I understand the answer.  I've followed\
          \ the instructions for generating protein sequences:\r\nsequences = protgpt2(\"\
          M\", min_length=50, max_length=70, do_sample=True, top_k=950, repetition_penalty=1.2,\
          \ num_return_sequences=10, eos_token_id=0)\r\n\r\nI trying to ask that the\
          \ protein sequences be no longer than 70 amino acids, but most of the sequences\
          \ are much longer (most are ~180 AA, with the smallest having 98AA's). \
          \ They also contain multiple \"\\n\" tokens.  \r\nQuestions:\r\n1) How should\
          \ the \"\\n\" tokens be interpreted?  Should I just ignore them?\r\n2) How\
          \ can I force the output designed sequences to actually be between 50 and\
          \ 70 amino acids in length?\r\n\r\nThanks in advance for the help."
        updatedAt: '2022-08-31T21:41:23.003Z'
      numEdits: 0
      reactions: []
    id: 630fd5836f75a5f478094420
    type: comment
  author: SenyorDrew
  content: "Thanks for making such an awesome resource available.  I had a question\
    \ about the lengths of designed proteins - it seems this question was asked previously,\
    \ but I don't think I understand the answer.  I've followed the instructions for\
    \ generating protein sequences:\r\nsequences = protgpt2(\"M\", min_length=50,\
    \ max_length=70, do_sample=True, top_k=950, repetition_penalty=1.2, num_return_sequences=10,\
    \ eos_token_id=0)\r\n\r\nI trying to ask that the protein sequences be no longer\
    \ than 70 amino acids, but most of the sequences are much longer (most are ~180\
    \ AA, with the smallest having 98AA's).  They also contain multiple \"\\n\" tokens.\
    \  \r\nQuestions:\r\n1) How should the \"\\n\" tokens be interpreted?  Should\
    \ I just ignore them?\r\n2) How can I force the output designed sequences to actually\
    \ be between 50 and 70 amino acids in length?\r\n\r\nThanks in advance for the\
    \ help."
  created_at: 2022-08-31 20:41:23+00:00
  edited: false
  hidden: false
  id: 630fd5836f75a5f478094420
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-09-01T12:03:18.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi! </p>

          <p>Thanks a lot for reaching out and using ProtGPT2!</p>

          <p>The min_length and max_length arguments are expressed in tokens, not
          amino acids. The dataset was tokenized before the pre-training stage. Each
          token, on average, has 3-4 amino acids (but it can vary a lot); hence you
          get around 180AA. If you want sequences shorter than 70AA, I will try with
          min_length=20 and max_length=25. But this will require trial and error because
          some sequences will be enriched with longer or shorter tokens than average.
          What I''d do if you need a specific range is generate those approximate
          numbers (min=20 and max=25) and discard those that do not fulfill your criteria.
          Inference should generally be very fast if you have a GPU.</p>

          <p>When generating de novo sequences, instead of ''M,'' I''d start with
          the end-of-sequences token: ''&lt;|endoftext|&gt;'' You will avoid lots
          of those '' \ n'' tokens. Starting with &nbsp;''&lt;|endoftext|&gt;,'' you
          should not see more than one &nbsp;''\n'' token every 60 amino acids. If
          you do, &nbsp;I''d discard those sequences and keep sampling until getting
          good ones.</p>

          <p>Another thought. When the model is passed the max_length argument, it
          generates until that length, but perhaps those sequences aren''t naturally
          that short. Hence the model is truncating them. (the model would naturally
          generate longer sequences, 300-400 amino acids long). To avoid this, I''d
          generate many sequences and only consider those whose final token is an
          ''&lt;|endoftext|&gt;'' token. This way you will avoid unnaturally truncated
          sequences.</p>

          <p>I hope this helps for now; if you have any more questions, please keep
          me posted!<br>Noelia</p>

          '
        raw: "Hi! \n\nThanks a lot for reaching out and using ProtGPT2!\n\nThe min_length\
          \ and max_length arguments are expressed in tokens, not amino acids. The\
          \ dataset was tokenized before the pre-training stage. Each token, on average,\
          \ has 3-4 amino acids (but it can vary a lot); hence you get around 180AA.\
          \ If you want sequences shorter than 70AA, I will try with min_length=20\
          \ and max_length=25. But this will require trial and error because some\
          \ sequences will be enriched with longer or shorter tokens than average.\
          \ What I'd do if you need a specific range is generate those approximate\
          \ numbers (min=20 and max=25) and discard those that do not fulfill your\
          \ criteria. Inference should generally be very fast if you have a GPU.\n\
          \nWhen generating de novo sequences, instead of 'M,' I'd start with the\
          \ end-of-sequences token: '<|endoftext|>' You will avoid lots of those '\
          \ \\ n' tokens. Starting with \_'<|endoftext|>,' you should not see more\
          \ than one \_'\\n' token every 60 amino acids. If you do, \_I'd discard\
          \ those sequences and keep sampling until getting good ones.\n\nAnother\
          \ thought. When the model is passed the max_length argument, it generates\
          \ until that length, but perhaps those sequences aren't naturally that short.\
          \ Hence the model is truncating them. (the model would naturally generate\
          \ longer sequences, 300-400 amino acids long). To avoid this, I'd generate\
          \ many sequences and only consider those whose final token is an '<|endoftext|>'\
          \ token. This way you will avoid unnaturally truncated sequences.\n\nI hope\
          \ this helps for now; if you have any more questions, please keep me posted!\n\
          Noelia"
        updatedAt: '2022-09-01T12:03:18.477Z'
      numEdits: 0
      reactions: []
    id: 63109f86a23f0327bce630d3
    type: comment
  author: nferruz
  content: "Hi! \n\nThanks a lot for reaching out and using ProtGPT2!\n\nThe min_length\
    \ and max_length arguments are expressed in tokens, not amino acids. The dataset\
    \ was tokenized before the pre-training stage. Each token, on average, has 3-4\
    \ amino acids (but it can vary a lot); hence you get around 180AA. If you want\
    \ sequences shorter than 70AA, I will try with min_length=20 and max_length=25.\
    \ But this will require trial and error because some sequences will be enriched\
    \ with longer or shorter tokens than average. What I'd do if you need a specific\
    \ range is generate those approximate numbers (min=20 and max=25) and discard\
    \ those that do not fulfill your criteria. Inference should generally be very\
    \ fast if you have a GPU.\n\nWhen generating de novo sequences, instead of 'M,'\
    \ I'd start with the end-of-sequences token: '<|endoftext|>' You will avoid lots\
    \ of those ' \\ n' tokens. Starting with \_'<|endoftext|>,' you should not see\
    \ more than one \_'\\n' token every 60 amino acids. If you do, \_I'd discard those\
    \ sequences and keep sampling until getting good ones.\n\nAnother thought. When\
    \ the model is passed the max_length argument, it generates until that length,\
    \ but perhaps those sequences aren't naturally that short. Hence the model is\
    \ truncating them. (the model would naturally generate longer sequences, 300-400\
    \ amino acids long). To avoid this, I'd generate many sequences and only consider\
    \ those whose final token is an '<|endoftext|>' token. This way you will avoid\
    \ unnaturally truncated sequences.\n\nI hope this helps for now; if you have any\
    \ more questions, please keep me posted!\nNoelia"
  created_at: 2022-09-01 11:03:18+00:00
  edited: false
  hidden: false
  id: 63109f86a23f0327bce630d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Lengths of designed proteins
