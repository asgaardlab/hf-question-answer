!!python/object:huggingface_hub.community.DiscussionWithDetails
author: emrecicekyurt
conflicting_files: null
created_at: 2023-04-19 09:49:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce9f720a3ff781cf7bd32df80f5f7937.svg
      fullname: "Emre \xC7i\xE7ekyurt"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emrecicekyurt
      type: user
    createdAt: '2023-04-19T10:49:57.000Z'
    data:
      edited: false
      editors:
      - emrecicekyurt
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce9f720a3ff781cf7bd32df80f5f7937.svg
          fullname: "Emre \xC7i\xE7ekyurt"
          isHf: false
          isPro: false
          name: emrecicekyurt
          type: user
        html: "<p>Hello,</p>\n<p>Firstly, thank you <span data-props=\"{&quot;user&quot;:&quot;nferruz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nferruz\"\
          >@<span class=\"underline\">nferruz</span></a></span>\n\n\t</span></span>\
          \ for providing a freely accessible and well-documented model.</p>\n<p>I\
          \ am wondering how the model inserts newline(\\n) characters into the generated\
          \ sequences. It seems that a newline character is inserted after every 60\
          \ characters to mimic the format of a typical text document.  However, in\
          \ some cases, the model inserts a new line before 60 characters.</p>\n<p>\
          \ I couldn't find a proper answer to these questions:</p>\n<ul>\n<li>Are\
          \ there any criteria for inserting newline characters into sequences? </li>\n\
          <li>Are  \"\\n\" only used for formatting purposes and can it be removed\
          \ to obtain the actual amino acid sequence?</li>\n</ul>\n<p>Thanks in advance,</p>\n\
          <p>Emre</p>\n"
        raw: "Hello,\r\n\r\nFirstly, thank you @nferruz for providing a freely accessible\
          \ and well-documented model.\r\n\r\nI am wondering how the model inserts\
          \ newline(\\n) characters into the generated sequences. It seems that a\
          \ newline character is inserted after every 60 characters to mimic the format\
          \ of a typical text document.  However, in some cases, the model inserts\
          \ a new line before 60 characters.\r\n\r\n\r\n I couldn't find a proper\
          \ answer to these questions:\r\n- Are there any criteria for inserting newline\
          \ characters into sequences? \r\n- Are  \"\\n\" only used for formatting\
          \ purposes and can it be removed to obtain the actual amino acid sequence?\r\
          \n\r\nThanks in advance,\r\n\r\nEmre"
        updatedAt: '2023-04-19T10:49:57.512Z'
      numEdits: 0
      reactions: []
    id: 643fc755388f31f64ccb0f5d
    type: comment
  author: emrecicekyurt
  content: "Hello,\r\n\r\nFirstly, thank you @nferruz for providing a freely accessible\
    \ and well-documented model.\r\n\r\nI am wondering how the model inserts newline(\\\
    n) characters into the generated sequences. It seems that a newline character\
    \ is inserted after every 60 characters to mimic the format of a typical text\
    \ document.  However, in some cases, the model inserts a new line before 60 characters.\r\
    \n\r\n\r\n I couldn't find a proper answer to these questions:\r\n- Are there\
    \ any criteria for inserting newline characters into sequences? \r\n- Are  \"\\\
    n\" only used for formatting purposes and can it be removed to obtain the actual\
    \ amino acid sequence?\r\n\r\nThanks in advance,\r\n\r\nEmre"
  created_at: 2023-04-19 09:49:57+00:00
  edited: false
  hidden: false
  id: 643fc755388f31f64ccb0f5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-04-19T20:06:01.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hello!</p>

          <p>Thanks for your post. Yes, those newline tokens are an artifact of the
          way I trained the model. I didn''t notice them at the time of training,
          but of course, following the fasta format, they were there after every 60
          characters. We trained several models after ProtGPT2, and I ensured they
          didn''t have newline characters as they only make generation more complicated.<br>In
          any case, for this model, I''d ignore all sequences where the model generates
          a new line character in the first 60 amino acids- those are bad sequences.
          And then, for the rest of the sequences, you can take the sequence after
          removing the newline character to get the final string - although I''d leave
          the newline character if you are computing perplexity values since the model
          expects them every 60 characters. Also, it has never happened to me, but
          if a newline character appeared at a different position than a 60 amino
          acid window, I would discard that sequence too.</p>

          <p>Let me know if questions remain.<br>Noelia</p>

          '
        raw: 'Hello!


          Thanks for your post. Yes, those newline tokens are an artifact of the way
          I trained the model. I didn''t notice them at the time of training, but
          of course, following the fasta format, they were there after every 60 characters.
          We trained several models after ProtGPT2, and I ensured they didn''t have
          newline characters as they only make generation more complicated.

          In any case, for this model, I''d ignore all sequences where the model generates
          a new line character in the first 60 amino acids- those are bad sequences.
          And then, for the rest of the sequences, you can take the sequence after
          removing the newline character to get the final string - although I''d leave
          the newline character if you are computing perplexity values since the model
          expects them every 60 characters. Also, it has never happened to me, but
          if a newline character appeared at a different position than a 60 amino
          acid window, I would discard that sequence too.


          Let me know if questions remain.

          Noelia'
        updatedAt: '2023-04-19T20:06:01.651Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - emrecicekyurt
    id: 644049a92113f7dfcb568265
    type: comment
  author: nferruz
  content: 'Hello!


    Thanks for your post. Yes, those newline tokens are an artifact of the way I trained
    the model. I didn''t notice them at the time of training, but of course, following
    the fasta format, they were there after every 60 characters. We trained several
    models after ProtGPT2, and I ensured they didn''t have newline characters as they
    only make generation more complicated.

    In any case, for this model, I''d ignore all sequences where the model generates
    a new line character in the first 60 amino acids- those are bad sequences. And
    then, for the rest of the sequences, you can take the sequence after removing
    the newline character to get the final string - although I''d leave the newline
    character if you are computing perplexity values since the model expects them
    every 60 characters. Also, it has never happened to me, but if a newline character
    appeared at a different position than a 60 amino acid window, I would discard
    that sequence too.


    Let me know if questions remain.

    Noelia'
  created_at: 2023-04-19 19:06:01+00:00
  edited: false
  hidden: false
  id: 644049a92113f7dfcb568265
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: New line characters in generated sequences
