!!python/object:huggingface_hub.community.DiscussionWithDetails
author: miraclewizard
conflicting_files: null
created_at: 2023-10-24 19:44:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b4d7478016420c17cecacfba6f8c8a28.svg
      fullname: Grant L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miraclewizard
      type: user
    createdAt: '2023-10-24T20:44:52.000Z'
    data:
      edited: false
      editors:
      - miraclewizard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8449475765228271
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b4d7478016420c17cecacfba6f8c8a28.svg
          fullname: Grant L
          isHf: false
          isPro: false
          name: miraclewizard
          type: user
        html: '<p>Hello! This is such a great resource and I''m really looking forward
          to using this. I want to train on a dataset (relatively small, &lt;677 lines
          so ~10k tokens). I followed the directions and created a training.txt file
          with &lt;|endoftext|&gt; as a header for each AA sequence. I saved ~10%
          (70 lines) as a validation.txt dataset. My command: python run_clm.py --model_name_or_path
          nferruz/ProtGPT2 --train_file training.txt --validation_file validation.txt
          --tokenizer_name nferruz/ProtGPT2 --output_dir /home/grant/test</p>

          <p>The command runs successfully without any errors and to my knowledge
          there is no error.txt output (yay!). However, my output only has a README.md
          that reads as follows (see below). Clearly my results is null ([]) so I''m
          assuming that the training dataset is too small (I saw some other posts
          where people used &gt;500 sequences). Am I right in my assumption? Is this
          the end of the road?</p>

          <p>Another way to answer this would be if there were an example training.txt
          (and accompanying validation.txt) I could download so I know what a "good"
          validation run looks like. </p>

          <p>ANY help would be appreicated. THANKS!</p>

          <h2 id="my-output-readmemd">my output: README.md </h2>

          <p>license: apache-2.0<br>base_model: nferruz/ProtGPT2<br>tags:</p>

          <ul>

          <li>generated_from_trainer<br>model-index:</li>

          <li>name: test<br>results: []</li>

          </ul>

          <hr>



          <h1 id="test">test</h1>

          <p>This model is a fine-tuned version of <a href="https://huggingface.co/nferruz/ProtGPT2">nferruz/ProtGPT2</a>
          on an unknown dataset.</p>

          <h2 id="model-description">Model description</h2>

          <p>More information needed</p>

          <h2 id="intended-uses--limitations">Intended uses &amp; limitations</h2>

          <p>More information needed</p>

          <h2 id="training-and-evaluation-data">Training and evaluation data</h2>

          <p>More information needed</p>

          <h2 id="training-procedure">Training procedure</h2>

          <h3 id="training-hyperparameters">Training hyperparameters</h3>

          <p>The following hyperparameters were used during training:</p>

          <ul>

          <li>learning_rate: 5e-05</li>

          <li>train_batch_size: 8</li>

          <li>eval_batch_size: 8</li>

          <li>seed: 42</li>

          <li>optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08</li>

          <li>lr_scheduler_type: linear</li>

          <li>num_epochs: 3.0</li>

          </ul>

          <h3 id="framework-versions">Framework versions</h3>

          <ul>

          <li>Transformers 4.35.0.dev0</li>

          <li>Pytorch 2.1.0+cu118</li>

          <li>Datasets 2.14.5</li>

          <li>Tokenizers 0.14.1</li>

          </ul>

          '
        raw: "Hello! This is such a great resource and I'm really looking forward\
          \ to using this. I want to train on a dataset (relatively small, <677 lines\
          \ so ~10k tokens). I followed the directions and created a training.txt\
          \ file with <|endoftext|> as a header for each AA sequence. I saved ~10%\
          \ (70 lines) as a validation.txt dataset. My command: python run_clm.py\
          \ --model_name_or_path nferruz/ProtGPT2 --train_file training.txt --validation_file\
          \ validation.txt --tokenizer_name nferruz/ProtGPT2 --output_dir /home/grant/test\r\
          \n\r\nThe command runs successfully without any errors and to my knowledge\
          \ there is no error.txt output (yay!). However, my output only has a README.md\
          \ that reads as follows (see below). Clearly my results is null ([]) so\
          \ I'm assuming that the training dataset is too small (I saw some other\
          \ posts where people used >500 sequences). Am I right in my assumption?\
          \ Is this the end of the road?\r\n\r\nAnother way to answer this would be\
          \ if there were an example training.txt (and accompanying validation.txt)\
          \ I could download so I know what a \"good\" validation run looks like.\
          \ \r\n\r\nANY help would be appreicated. THANKS!\r\n\r\nmy output: README.md\
          \ \r\n---\r\nlicense: apache-2.0\r\nbase_model: nferruz/ProtGPT2\r\ntags:\r\
          \n- generated_from_trainer\r\nmodel-index:\r\n- name: test\r\n  results:\
          \ []\r\n---\r\n\r\n<!-- This model card has been generated automatically\
          \ according to the information the Trainer had access to. You\r\nshould\
          \ probably proofread and complete it, then remove this comment. -->\r\n\r\
          \n# test\r\n\r\nThis model is a fine-tuned version of [nferruz/ProtGPT2](https://huggingface.co/nferruz/ProtGPT2)\
          \ on an unknown dataset.\r\n\r\n## Model description\r\n\r\nMore information\
          \ needed\r\n\r\n## Intended uses & limitations\r\n\r\nMore information needed\r\
          \n\r\n## Training and evaluation data\r\n\r\nMore information needed\r\n\
          \r\n## Training procedure\r\n\r\n### Training hyperparameters\r\n\r\nThe\
          \ following hyperparameters were used during training:\r\n- learning_rate:\
          \ 5e-05\r\n- train_batch_size: 8\r\n- eval_batch_size: 8\r\n- seed: 42\r\
          \n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\r\n- lr_scheduler_type:\
          \ linear\r\n- num_epochs: 3.0\r\n\r\n### Framework versions\r\n\r\n- Transformers\
          \ 4.35.0.dev0\r\n- Pytorch 2.1.0+cu118\r\n- Datasets 2.14.5\r\n- Tokenizers\
          \ 0.14.1"
        updatedAt: '2023-10-24T20:44:52.408Z'
      numEdits: 0
      reactions: []
    id: 65382cc4183979eafb8ace71
    type: comment
  author: miraclewizard
  content: "Hello! This is such a great resource and I'm really looking forward to\
    \ using this. I want to train on a dataset (relatively small, <677 lines so ~10k\
    \ tokens). I followed the directions and created a training.txt file with <|endoftext|>\
    \ as a header for each AA sequence. I saved ~10% (70 lines) as a validation.txt\
    \ dataset. My command: python run_clm.py --model_name_or_path nferruz/ProtGPT2\
    \ --train_file training.txt --validation_file validation.txt --tokenizer_name\
    \ nferruz/ProtGPT2 --output_dir /home/grant/test\r\n\r\nThe command runs successfully\
    \ without any errors and to my knowledge there is no error.txt output (yay!).\
    \ However, my output only has a README.md that reads as follows (see below). Clearly\
    \ my results is null ([]) so I'm assuming that the training dataset is too small\
    \ (I saw some other posts where people used >500 sequences). Am I right in my\
    \ assumption? Is this the end of the road?\r\n\r\nAnother way to answer this would\
    \ be if there were an example training.txt (and accompanying validation.txt) I\
    \ could download so I know what a \"good\" validation run looks like. \r\n\r\n\
    ANY help would be appreicated. THANKS!\r\n\r\nmy output: README.md \r\n---\r\n\
    license: apache-2.0\r\nbase_model: nferruz/ProtGPT2\r\ntags:\r\n- generated_from_trainer\r\
    \nmodel-index:\r\n- name: test\r\n  results: []\r\n---\r\n\r\n<!-- This model\
    \ card has been generated automatically according to the information the Trainer\
    \ had access to. You\r\nshould probably proofread and complete it, then remove\
    \ this comment. -->\r\n\r\n# test\r\n\r\nThis model is a fine-tuned version of\
    \ [nferruz/ProtGPT2](https://huggingface.co/nferruz/ProtGPT2) on an unknown dataset.\r\
    \n\r\n## Model description\r\n\r\nMore information needed\r\n\r\n## Intended uses\
    \ & limitations\r\n\r\nMore information needed\r\n\r\n## Training and evaluation\
    \ data\r\n\r\nMore information needed\r\n\r\n## Training procedure\r\n\r\n###\
    \ Training hyperparameters\r\n\r\nThe following hyperparameters were used during\
    \ training:\r\n- learning_rate: 5e-05\r\n- train_batch_size: 8\r\n- eval_batch_size:\
    \ 8\r\n- seed: 42\r\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\r\
    \n- lr_scheduler_type: linear\r\n- num_epochs: 3.0\r\n\r\n### Framework versions\r\
    \n\r\n- Transformers 4.35.0.dev0\r\n- Pytorch 2.1.0+cu118\r\n- Datasets 2.14.5\r\
    \n- Tokenizers 0.14.1"
  created_at: 2023-10-24 19:44:52+00:00
  edited: false
  hidden: false
  id: 65382cc4183979eafb8ace71
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Help with training dataset output and validation .txts
