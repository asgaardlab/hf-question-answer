!!python/object:huggingface_hub.community.DiscussionWithDetails
author: codev
conflicting_files: null
created_at: 2023-05-11 21:39:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-05-11T22:39:13.000Z'
    data:
      edited: false
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: '<p>Hello, </p>

          <p>I am trying to fine-tune ProtGPT2 using a training dataset of about 4000
          sequences. However, when I run <code>run_clm.py</code> it keeps saying the
          number of training samples is 605 (see screenshot below), so it doesn''t
          seem like it is using all of my training data. I have tried adjusting the
          batch size and number of epochs, and setting the <code>--max_train_samples</code>
          argument to 10000, all of which seem to have no effect. Has anyone else
          run into this? </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6425b4862a423daad528dfe0/XhJEClZ5Oa_7_4sBtaQrA.png"><img
          alt="Screenshot 2023-05-11 at 4.37.28 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/6425b4862a423daad528dfe0/XhJEClZ5Oa_7_4sBtaQrA.png"></a></p>

          <p>Thanks in advance! </p>

          <p>Kathryn  </p>

          '
        raw: "Hello, \r\n\r\nI am trying to fine-tune ProtGPT2 using a training dataset\
          \ of about 4000 sequences. However, when I run `run_clm.py` it keeps saying\
          \ the number of training samples is 605 (see screenshot below), so it doesn't\
          \ seem like it is using all of my training data. I have tried adjusting\
          \ the batch size and number of epochs, and setting the `--max_train_samples`\
          \ argument to 10000, all of which seem to have no effect. Has anyone else\
          \ run into this? \r\n\r\n![Screenshot 2023-05-11 at 4.37.28 PM.png](https://cdn-uploads.huggingface.co/production/uploads/6425b4862a423daad528dfe0/XhJEClZ5Oa_7_4sBtaQrA.png)\r\
          \n\r\nThanks in advance! \r\n\r\nKathryn  "
        updatedAt: '2023-05-11T22:39:13.706Z'
      numEdits: 0
      reactions: []
    id: 645d6e915ebf379fd6daf212
    type: comment
  author: codev
  content: "Hello, \r\n\r\nI am trying to fine-tune ProtGPT2 using a training dataset\
    \ of about 4000 sequences. However, when I run `run_clm.py` it keeps saying the\
    \ number of training samples is 605 (see screenshot below), so it doesn't seem\
    \ like it is using all of my training data. I have tried adjusting the batch size\
    \ and number of epochs, and setting the `--max_train_samples` argument to 10000,\
    \ all of which seem to have no effect. Has anyone else run into this? \r\n\r\n\
    ![Screenshot 2023-05-11 at 4.37.28 PM.png](https://cdn-uploads.huggingface.co/production/uploads/6425b4862a423daad528dfe0/XhJEClZ5Oa_7_4sBtaQrA.png)\r\
    \n\r\nThanks in advance! \r\n\r\nKathryn  "
  created_at: 2023-05-11 21:39:13+00:00
  edited: false
  hidden: false
  id: 645d6e915ebf379fd6daf212
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-05-16T12:30:50.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi codev!</p>

          <p>the run_clm.py script combines the sequences together into single batches
          to fit the window size of 512. This window size is expressed in tokens,
          where each token more or less has an average length of 4 amino acids. So
          every batch could have 2-10 sequences depending on your sequence length.
          Hence, 605 batches can perfectly comprise 4000 sequences.<br>Hope this helps,
          let me know if it doesn''t!</p>

          '
        raw: 'Hi codev!


          the run_clm.py script combines the sequences together into single batches
          to fit the window size of 512. This window size is expressed in tokens,
          where each token more or less has an average length of 4 amino acids. So
          every batch could have 2-10 sequences depending on your sequence length.
          Hence, 605 batches can perfectly comprise 4000 sequences.

          Hope this helps, let me know if it doesn''t!'
        updatedAt: '2023-05-16T12:30:50.935Z'
      numEdits: 0
      reactions: []
    id: 6463777a12814d7541796331
    type: comment
  author: nferruz
  content: 'Hi codev!


    the run_clm.py script combines the sequences together into single batches to fit
    the window size of 512. This window size is expressed in tokens, where each token
    more or less has an average length of 4 amino acids. So every batch could have
    2-10 sequences depending on your sequence length. Hence, 605 batches can perfectly
    comprise 4000 sequences.

    Hope this helps, let me know if it doesn''t!'
  created_at: 2023-05-16 11:30:50+00:00
  edited: false
  hidden: false
  id: 6463777a12814d7541796331
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-05-16T16:43:33.000Z'
    data:
      edited: false
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: "<p>Got it, thank you so much <span data-props=\"{&quot;user&quot;:&quot;nferruz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nferruz\"\
          >@<span class=\"underline\">nferruz</span></a></span>\n\n\t</span></span>!\
          \ I was coming to a similar conclusion yesterday as I was reading through\
          \ the run_clm script, but it is helpful to know that this is the intended\
          \ behavior. </p>\n<p>Best,</p>\n<p>Kathryn</p>\n"
        raw: "Got it, thank you so much @nferruz! I was coming to a similar conclusion\
          \ yesterday as I was reading through the run_clm script, but it is helpful\
          \ to know that this is the intended behavior. \n\nBest,\n\nKathryn"
        updatedAt: '2023-05-16T16:43:33.969Z'
      numEdits: 0
      reactions: []
    id: 6463b2b576155a231c814113
    type: comment
  author: codev
  content: "Got it, thank you so much @nferruz! I was coming to a similar conclusion\
    \ yesterday as I was reading through the run_clm script, but it is helpful to\
    \ know that this is the intended behavior. \n\nBest,\n\nKathryn"
  created_at: 2023-05-16 15:43:33+00:00
  edited: false
  hidden: false
  id: 6463b2b576155a231c814113
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Discrepancy between num_samples when fine tuning and the number of samples
  in my training file
