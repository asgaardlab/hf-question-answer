!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 0jj0
conflicting_files: null
created_at: 2022-11-23 12:20:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
      fullname: Elephas Maximus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0jj0
      type: user
    createdAt: '2022-11-23T12:20:17.000Z'
    data:
      edited: false
      editors:
      - 0jj0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
          fullname: Elephas Maximus
          isHf: false
          isPro: false
          name: 0jj0
          type: user
        html: '<p>Hi Noelia,<br>thanks a lot for creating ProtGTP2. It is my first
          experience with protein language models and deep learning, so I have been
          learning quite a lot when playing with ProtGTP2. One noob question maybe,
          is it possible to use ProtGTP2 to generate a set of new proteins (enzymes)
          based on one input sequence of an enzyme that was characterized before.
          For now from what I understand, you need to provide a part of the sequence
          (is that the case that it must be the beginning of the sequence?) and ProtGTP2
          will "fill in the blank" after the initial input.<br>I saw that there is
          a way to retrain the model based on a set of sequences but I guess an example
          of 1 single sequence is not enough to retrain it?<br>One small comment is
          that if possible, you could include a short instruction on how to setup
          Transformers for someone who is very inexperienced with the field. I''m
          familiar with <code>conda</code> and co so it''s not a real issue for me
          and a Colab notebook (<a rel="nofollow" href="https://colab.research.google.com/drive/15ucZMtrAeFE_YOBQ9FdrWlAngvljJ4ss?usp=sharing&amp;pli=1#scrollTo=rZjPq1f5y9Op">https://colab.research.google.com/drive/15ucZMtrAeFE_YOBQ9FdrWlAngvljJ4ss?usp=sharing&amp;pli=1#scrollTo=rZjPq1f5y9Op</a>)
          has been very useful for me to setup ProtGTP2. Also from my setup, installing
          transformers using <code>conda</code> with the channel <code>conda-forge</code>seemed
          to work better than the channel <code>huggingface</code> as suggested by
          Hugging Face installation page (<code>pytorch</code> and other dependencies
          were installed at the same time when using <code>conda-forge</code>).<br>Thanks
          again and looking forward to your feedback,<br>Best regards,<br>Mr. Curiosity</p>

          '
        raw: "Hi Noelia,\r\nthanks a lot for creating ProtGTP2. It is my first experience\
          \ with protein language models and deep learning, so I have been learning\
          \ quite a lot when playing with ProtGTP2. One noob question maybe, is it\
          \ possible to use ProtGTP2 to generate a set of new proteins (enzymes) based\
          \ on one input sequence of an enzyme that was characterized before. For\
          \ now from what I understand, you need to provide a part of the sequence\
          \ (is that the case that it must be the beginning of the sequence?) and\
          \ ProtGTP2 will \"fill in the blank\" after the initial input.\r\nI saw\
          \ that there is a way to retrain the model based on a set of sequences but\
          \ I guess an example of 1 single sequence is not enough to retrain it?\r\
          \nOne small comment is that if possible, you could include a short instruction\
          \ on how to setup Transformers for someone who is very inexperienced with\
          \ the field. I'm familiar with `conda` and co so it's not a real issue for\
          \ me and a Colab notebook (https://colab.research.google.com/drive/15ucZMtrAeFE_YOBQ9FdrWlAngvljJ4ss?usp=sharing&pli=1#scrollTo=rZjPq1f5y9Op)\
          \ has been very useful for me to setup ProtGTP2. Also from my setup, installing\
          \ transformers using `conda` with the channel `conda-forge`seemed to work\
          \ better than the channel `huggingface` as suggested by Hugging Face installation\
          \ page (`pytorch` and other dependencies were installed at the same time\
          \ when using `conda-forge`).\r\nThanks again and looking forward to your\
          \ feedback,\r\nBest regards,\r\nMr. Curiosity"
        updatedAt: '2022-11-23T12:20:17.047Z'
      numEdits: 0
      reactions: []
    id: 637e1001893d2d51989c070f
    type: comment
  author: 0jj0
  content: "Hi Noelia,\r\nthanks a lot for creating ProtGTP2. It is my first experience\
    \ with protein language models and deep learning, so I have been learning quite\
    \ a lot when playing with ProtGTP2. One noob question maybe, is it possible to\
    \ use ProtGTP2 to generate a set of new proteins (enzymes) based on one input\
    \ sequence of an enzyme that was characterized before. For now from what I understand,\
    \ you need to provide a part of the sequence (is that the case that it must be\
    \ the beginning of the sequence?) and ProtGTP2 will \"fill in the blank\" after\
    \ the initial input.\r\nI saw that there is a way to retrain the model based on\
    \ a set of sequences but I guess an example of 1 single sequence is not enough\
    \ to retrain it?\r\nOne small comment is that if possible, you could include a\
    \ short instruction on how to setup Transformers for someone who is very inexperienced\
    \ with the field. I'm familiar with `conda` and co so it's not a real issue for\
    \ me and a Colab notebook (https://colab.research.google.com/drive/15ucZMtrAeFE_YOBQ9FdrWlAngvljJ4ss?usp=sharing&pli=1#scrollTo=rZjPq1f5y9Op)\
    \ has been very useful for me to setup ProtGTP2. Also from my setup, installing\
    \ transformers using `conda` with the channel `conda-forge`seemed to work better\
    \ than the channel `huggingface` as suggested by Hugging Face installation page\
    \ (`pytorch` and other dependencies were installed at the same time when using\
    \ `conda-forge`).\r\nThanks again and looking forward to your feedback,\r\nBest\
    \ regards,\r\nMr. Curiosity"
  created_at: 2022-11-23 12:20:17+00:00
  edited: false
  hidden: false
  id: 637e1001893d2d51989c070f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-11-25T22:56:08.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi kurioscity, thanks a lot for writing!<br>Yes, as you mention\
          \ with one single sequence it will be hard to fine-tune the model (I don\u2019\
          t expect the weights to update much), and yes, the only other option to\
          \ condition the output is to provide the context (i.e., leftmost part of\
          \ the sequence), which is not helpful in your case.<br>But maybe there\u2019\
          s another option. I recently trained another model, which was trained only\
          \ on enzymes - you can find it with the name ZymCTRL. We haven\u2019t uploaded\
          \ the preprint yet, but it\u2019s about to get out. If you know your enzyme\
          \ EC number, finetuning this model might give you better results than ProtGPT2.\
          \ </p>\n<p>You\u2019re right about an intro to the Transformers module -\
          \ I will try make the docs more accessible on Monday!</p>\n<p>Let me know\
          \ if you try to finetune the models and questions arise,<br>Noelia</p>\n"
        raw: "Hi kurioscity, thanks a lot for writing!\nYes, as you mention with one\
          \ single sequence it will be hard to fine-tune the model (I don\u2019t expect\
          \ the weights to update much), and yes, the only other option to condition\
          \ the output is to provide the context (i.e., leftmost part of the sequence),\
          \ which is not helpful in your case.\nBut maybe there\u2019s another option.\
          \ I recently trained another model, which was trained only on enzymes -\
          \ you can find it with the name ZymCTRL. We haven\u2019t uploaded the preprint\
          \ yet, but it\u2019s about to get out. If you know your enzyme EC number,\
          \ finetuning this model might give you better results than ProtGPT2. \n\n\
          You\u2019re right about an intro to the Transformers module - I will try\
          \ make the docs more accessible on Monday!\n\nLet me know if you try to\
          \ finetune the models and questions arise,\nNoelia"
        updatedAt: '2022-11-25T22:56:08.940Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - 0jj0
        - Biohebb
    id: 63814808f496d57325c4b2a5
    type: comment
  author: nferruz
  content: "Hi kurioscity, thanks a lot for writing!\nYes, as you mention with one\
    \ single sequence it will be hard to fine-tune the model (I don\u2019t expect\
    \ the weights to update much), and yes, the only other option to condition the\
    \ output is to provide the context (i.e., leftmost part of the sequence), which\
    \ is not helpful in your case.\nBut maybe there\u2019s another option. I recently\
    \ trained another model, which was trained only on enzymes - you can find it with\
    \ the name ZymCTRL. We haven\u2019t uploaded the preprint yet, but it\u2019s about\
    \ to get out. If you know your enzyme EC number, finetuning this model might give\
    \ you better results than ProtGPT2. \n\nYou\u2019re right about an intro to the\
    \ Transformers module - I will try make the docs more accessible on Monday!\n\n\
    Let me know if you try to finetune the models and questions arise,\nNoelia"
  created_at: 2022-11-25 22:56:08+00:00
  edited: false
  hidden: false
  id: 63814808f496d57325c4b2a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
      fullname: Elephas Maximus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 0jj0
      type: user
    createdAt: '2022-11-27T19:18:59.000Z'
    data:
      edited: false
      editors:
      - 0jj0
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f01beafaea32920aa413e1bf1c53e771.svg
          fullname: Elephas Maximus
          isHf: false
          isPro: false
          name: 0jj0
          type: user
        html: '<p>Hi  Noeila,<br>thanks a lot for your reply. I did see the ZymCTRL
          project and wanted to try it out as it looks very interesting (I work a
          lot with enzyme discovery and characterization). I will try it out and let
          you know if I have any questions. As mentioned, I did try to use ProtGTP2
          to create new sequences of an enzyme having a pretty new sequence in terms
          of sequence identity (the enzyme class is not novel but kind of underexplored)
          using the first 25 amino acids as the starting point. The ouput sequences
          are very different from the original whole sequence but I have not really
          looked into whether they will have the same function. One thing I consider
          for using ProtGTP2 for my purpose is to let the network create, say, 100
          000 sequences, and use for example InterProScan to predict the putative
          function and select the ones that are predicted to be my enzyme classes
          of interest.<br>Thank you also for considering to improve the documentation.
          And now I''m pretty curious trying ZymCTRL. I saw also that you have a preprint
          exploring using ProtGTP2 for Biosynthetic Gene Clusters applcation (also
          one of my research interest). I think there are a lot of potentials there,
          maybe training a network of MiBIG BGCs'' genes with their corresponding
          reactions (when available) and use it to create a totally novel BGC that
          might produce a new compound.<br>Best regards,<br>Mr. Curiosity</p>

          '
        raw: 'Hi  Noeila,

          thanks a lot for your reply. I did see the ZymCTRL project and wanted to
          try it out as it looks very interesting (I work a lot with enzyme discovery
          and characterization). I will try it out and let you know if I have any
          questions. As mentioned, I did try to use ProtGTP2 to create new sequences
          of an enzyme having a pretty new sequence in terms of sequence identity
          (the enzyme class is not novel but kind of underexplored) using the first
          25 amino acids as the starting point. The ouput sequences are very different
          from the original whole sequence but I have not really looked into whether
          they will have the same function. One thing I consider for using ProtGTP2
          for my purpose is to let the network create, say, 100 000 sequences, and
          use for example InterProScan to predict the putative function and select
          the ones that are predicted to be my enzyme classes of interest.

          Thank you also for considering to improve the documentation. And now I''m
          pretty curious trying ZymCTRL. I saw also that you have a preprint exploring
          using ProtGTP2 for Biosynthetic Gene Clusters applcation (also one of my
          research interest). I think there are a lot of potentials there, maybe training
          a network of MiBIG BGCs'' genes with their corresponding reactions (when
          available) and use it to create a totally novel BGC that might produce a
          new compound.

          Best regards,

          Mr. Curiosity'
        updatedAt: '2022-11-27T19:18:59.464Z'
      numEdits: 0
      reactions: []
    id: 6383b823830bbad7b91631dc
    type: comment
  author: 0jj0
  content: 'Hi  Noeila,

    thanks a lot for your reply. I did see the ZymCTRL project and wanted to try it
    out as it looks very interesting (I work a lot with enzyme discovery and characterization).
    I will try it out and let you know if I have any questions. As mentioned, I did
    try to use ProtGTP2 to create new sequences of an enzyme having a pretty new sequence
    in terms of sequence identity (the enzyme class is not novel but kind of underexplored)
    using the first 25 amino acids as the starting point. The ouput sequences are
    very different from the original whole sequence but I have not really looked into
    whether they will have the same function. One thing I consider for using ProtGTP2
    for my purpose is to let the network create, say, 100 000 sequences, and use for
    example InterProScan to predict the putative function and select the ones that
    are predicted to be my enzyme classes of interest.

    Thank you also for considering to improve the documentation. And now I''m pretty
    curious trying ZymCTRL. I saw also that you have a preprint exploring using ProtGTP2
    for Biosynthetic Gene Clusters applcation (also one of my research interest).
    I think there are a lot of potentials there, maybe training a network of MiBIG
    BGCs'' genes with their corresponding reactions (when available) and use it to
    create a totally novel BGC that might produce a new compound.

    Best regards,

    Mr. Curiosity'
  created_at: 2022-11-27 19:18:59+00:00
  edited: false
  hidden: false
  id: 6383b823830bbad7b91631dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-12-01T21:43:15.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi Kurioscity,<br>your project sounds really interesting to me!
          Indeed with the 25 first amino acids, the generated sequences will most
          likely diverge a lot. Still, I would do as you say and pass them through
          Interproscan to see if they perform the function you want. For this purpose,
          I''d suggest you also have a look at ProteInfer (<a rel="nofollow" href="https://google-research.github.io/proteinfer/">https://google-research.github.io/proteinfer/</a>)
          since it directly outputs EC classes. You can also send a job and request
          the result with an URL request instead of using the web interface.</p>

          <p>The preprint that you mention actually followed a similar protocol. We
          generated 100,000 sequences and as an example chose sequences that could
          produce molecular glues. Now that you say it, you could use the same pipeline
          we used and search for your specific function: <a rel="nofollow" href="https://github.com/hefeda/PGP">https://github.com/hefeda/PGP</a>.<br>What
          you mention is a good idea! all these models tend to focus on a single sequence
          or reaction but we haven''t yet explored training networks that exploit
          the interconnection of many to create enzymatic cascades or BGCs</p>

          '
        raw: "Hi Kurioscity, \nyour project sounds really interesting to me! Indeed\
          \ with the 25 first amino acids, the generated sequences will most likely\
          \ diverge a lot. Still, I would do as you say and pass them through Interproscan\
          \ to see if they perform the function you want. For this purpose, I'd suggest\
          \ you also have a look at ProteInfer (https://google-research.github.io/proteinfer/)\
          \ since it directly outputs EC classes. You can also send a job and request\
          \ the result with an URL request instead of using the web interface.\n\n\
          The preprint that you mention actually followed a similar protocol. We generated\
          \ 100,000 sequences and as an example chose sequences that could produce\
          \ molecular glues. Now that you say it, you could use the same pipeline\
          \ we used and search for your specific function: https://github.com/hefeda/PGP.\
          \  \nWhat you mention is a good idea! all these models tend to focus on\
          \ a single sequence or reaction but we haven't yet explored training networks\
          \ that exploit the interconnection of many to create enzymatic cascades\
          \ or BGCs"
        updatedAt: '2022-12-01T21:43:15.237Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Biohebb
    id: 63891ff3c0b95b2c293957a9
    type: comment
  author: nferruz
  content: "Hi Kurioscity, \nyour project sounds really interesting to me! Indeed\
    \ with the 25 first amino acids, the generated sequences will most likely diverge\
    \ a lot. Still, I would do as you say and pass them through Interproscan to see\
    \ if they perform the function you want. For this purpose, I'd suggest you also\
    \ have a look at ProteInfer (https://google-research.github.io/proteinfer/) since\
    \ it directly outputs EC classes. You can also send a job and request the result\
    \ with an URL request instead of using the web interface.\n\nThe preprint that\
    \ you mention actually followed a similar protocol. We generated 100,000 sequences\
    \ and as an example chose sequences that could produce molecular glues. Now that\
    \ you say it, you could use the same pipeline we used and search for your specific\
    \ function: https://github.com/hefeda/PGP.  \nWhat you mention is a good idea!\
    \ all these models tend to focus on a single sequence or reaction but we haven't\
    \ yet explored training networks that exploit the interconnection of many to create\
    \ enzymatic cascades or BGCs"
  created_at: 2022-12-01 21:43:15+00:00
  edited: false
  hidden: false
  id: 63891ff3c0b95b2c293957a9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Generation of sequences based on one input sequence
