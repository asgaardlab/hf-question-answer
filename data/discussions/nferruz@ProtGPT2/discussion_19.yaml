!!python/object:huggingface_hub.community.DiscussionWithDetails
author: codev
conflicting_files: null
created_at: 2023-03-31 20:54:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-03-31T21:54:34.000Z'
    data:
      edited: true
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: '<p>Hello, </p>

          <p>Thank you so much for your work to make this model freely accessible,
          easy to use and well documented! </p>

          <p>I am wondering what the minimum requirements are for GPU memory when
          fine-tuning. I am currently using four NVIDIA A10G Tensor Core GPUs (24
          GB memory each) in a distributed manner, using all of the memory saving
          tricks I know (batch size = 1, setting --gradient_accumulation_steps to
          16 and enabling gradient checkpointing, and setting the optimizer to use
          adafactor (per the blog post <a href="https://huggingface.co/docs/transformers/v4.18.0/en/performance">here</a>)
          but I am still running out of memory. My command is below: </p>

          <p><code>python -m torch.distributed.launch --nproc_per_node 4 run_clm.py
          --model_name_or_path nferruz/ProtGPT2 --train_file train.txt --validation_file
          test.txt --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir
          output --learning_rate 1e-06 --per_device_train_batch_size=1 --gradient_accumulation_steps=16
          --gradient_checkpointing --adafactor</code></p>

          <p>I saw in the documentation that the model was trained using 100 NVIDIA
          A100 GPU''s, but I was thinking that fine-tuning could be done on a smaller
          system. </p>

          <p>Thanks in advance! </p>

          <p>Kathryn</p>

          '
        raw: "Hello, \n\nThank you so much for your work to make this model freely\
          \ accessible, easy to use and well documented! \n\nI am wondering what the\
          \ minimum requirements are for GPU memory when fine-tuning. I am currently\
          \ using four NVIDIA A10G Tensor Core GPUs (24 GB memory each) in a distributed\
          \ manner, using all of the memory saving tricks I know (batch size = 1,\
          \ setting --gradient_accumulation_steps to 16 and enabling gradient checkpointing,\
          \ and setting the optimizer to use adafactor (per the blog post [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance))\
          \ but I am still running out of memory. My command is below: \n\n`python\
          \ -m torch.distributed.launch --nproc_per_node 4 run_clm.py --model_name_or_path\
          \ nferruz/ProtGPT2 --train_file train.txt --validation_file test.txt --tokenizer_name\
          \ nferruz/ProtGPT2 --do_train --do_eval --output_dir output --learning_rate\
          \ 1e-06 --per_device_train_batch_size=1 --gradient_accumulation_steps=16\
          \ --gradient_checkpointing --adafactor`\n\nI saw in the documentation that\
          \ the model was trained using 100 NVIDIA A100 GPU's, but I was thinking\
          \ that fine-tuning could be done on a smaller system. \n\nThanks in advance!\
          \ \n\nKathryn"
        updatedAt: '2023-03-31T21:56:35.064Z'
      numEdits: 1
      reactions: []
    id: 6427569a39c7e60c4b37de52
    type: comment
  author: codev
  content: "Hello, \n\nThank you so much for your work to make this model freely accessible,\
    \ easy to use and well documented! \n\nI am wondering what the minimum requirements\
    \ are for GPU memory when fine-tuning. I am currently using four NVIDIA A10G Tensor\
    \ Core GPUs (24 GB memory each) in a distributed manner, using all of the memory\
    \ saving tricks I know (batch size = 1, setting --gradient_accumulation_steps\
    \ to 16 and enabling gradient checkpointing, and setting the optimizer to use\
    \ adafactor (per the blog post [here](https://huggingface.co/docs/transformers/v4.18.0/en/performance))\
    \ but I am still running out of memory. My command is below: \n\n`python -m torch.distributed.launch\
    \ --nproc_per_node 4 run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file\
    \ train.txt --validation_file test.txt --tokenizer_name nferruz/ProtGPT2 --do_train\
    \ --do_eval --output_dir output --learning_rate 1e-06 --per_device_train_batch_size=1\
    \ --gradient_accumulation_steps=16 --gradient_checkpointing --adafactor`\n\nI\
    \ saw in the documentation that the model was trained using 100 NVIDIA A100 GPU's,\
    \ but I was thinking that fine-tuning could be done on a smaller system. \n\n\
    Thanks in advance! \n\nKathryn"
  created_at: 2023-03-31 20:54:34+00:00
  edited: true
  hidden: false
  id: 6427569a39c7e60c4b37de52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-03-31T21:59:45.000Z'
    data:
      from: Recommended architecture for fine tuning
      to: Recommended GPU architecture for fine tuning
    id: 642757d1fe8a66429bbc3990
    type: title-change
  author: codev
  created_at: 2023-03-31 20:59:45+00:00
  id: 642757d1fe8a66429bbc3990
  new_title: Recommended GPU architecture for fine tuning
  old_title: Recommended architecture for fine tuning
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-04-03T07:35:31.000Z'
    data:
      edited: true
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi codec,</p>

          <p>That is unfortunate; I would have expected that it would also run in
          your system. It seems that you might need a card with more memory.<br>I
          can only think of changing the data type to --fp16 or --bf16 and seeing
          if that fits: <a href="https://huggingface.co/docs/transformers/v4.13.0/en/performance#floating-data-types">https://huggingface.co/docs/transformers/v4.13.0/en/performance#floating-data-types</a></p>

          <p>I don''t know the minimum requirements because we always fine-tune on
          a single A40. However, if I remember correctly, I tried once to fine-tune
          on a single RTX 3090, and that didn''t fit either...</p>

          <p>I still hope that changing the data type works!</p>

          <p>Best<br>Noelia</p>

          '
        raw: "Hi codec,\n\nThat is unfortunate; I would have expected that it would\
          \ also run in your system. It seems that you might need a card with more\
          \ memory. \nI can only think of changing the data type to --fp16 or --bf16\
          \ and seeing if that fits: https://huggingface.co/docs/transformers/v4.13.0/en/performance#floating-data-types\n\
          \nI don't know the minimum requirements because we always fine-tune on a\
          \ single A40. However, if I remember correctly, I tried once to fine-tune\
          \ on a single RTX 3090, and that didn't fit either...\n\nI still hope that\
          \ changing the data type works!\n\nBest\nNoelia"
        updatedAt: '2023-04-03T07:36:02.862Z'
      numEdits: 1
      reactions: []
    id: 642a81c3c6bf4ceb91a8fbf2
    type: comment
  author: nferruz
  content: "Hi codec,\n\nThat is unfortunate; I would have expected that it would\
    \ also run in your system. It seems that you might need a card with more memory.\
    \ \nI can only think of changing the data type to --fp16 or --bf16 and seeing\
    \ if that fits: https://huggingface.co/docs/transformers/v4.13.0/en/performance#floating-data-types\n\
    \nI don't know the minimum requirements because we always fine-tune on a single\
    \ A40. However, if I remember correctly, I tried once to fine-tune on a single\
    \ RTX 3090, and that didn't fit either...\n\nI still hope that changing the data\
    \ type works!\n\nBest\nNoelia"
  created_at: 2023-04-03 06:35:31+00:00
  edited: true
  hidden: false
  id: 642a81c3c6bf4ceb91a8fbf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-04-03T20:30:05.000Z'
    data:
      edited: false
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: '<p>I have tried that as well, but didn''t seem to make a difference
          - after reading the documentation it seems like converting to --fp16 is
          more to speed up training than save memory. I will try increasing my GPU
          memory. Thanks for the help! </p>

          <p>Kathryn</p>

          '
        raw: "I have tried that as well, but didn't seem to make a difference - after\
          \ reading the documentation it seems like converting to --fp16 is more to\
          \ speed up training than save memory. I will try increasing my GPU memory.\
          \ Thanks for the help! \n\nKathryn"
        updatedAt: '2023-04-03T20:30:05.693Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nferruz
    id: 642b374df3d829a868c6bb73
    type: comment
  author: codev
  content: "I have tried that as well, but didn't seem to make a difference - after\
    \ reading the documentation it seems like converting to --fp16 is more to speed\
    \ up training than save memory. I will try increasing my GPU memory. Thanks for\
    \ the help! \n\nKathryn"
  created_at: 2023-04-03 19:30:05+00:00
  edited: false
  hidden: false
  id: 642b374df3d829a868c6bb73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-04-05T16:27:52.000Z'
    data:
      edited: false
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: '<p>Just an FYI for others who might be running into similar issues
          - I was able to fine-tune the model on an NVIDIA A100 (40 GB ram) with the
          command mentioned in my first message. I ran out of memory when not using
          the memory saving strategies.</p>

          '
        raw: Just an FYI for others who might be running into similar issues - I was
          able to fine-tune the model on an NVIDIA A100 (40 GB ram) with the command
          mentioned in my first message. I ran out of memory when not using the memory
          saving strategies.
        updatedAt: '2023-04-05T16:27:52.901Z'
      numEdits: 0
      reactions: []
    id: 642da18825c08a2cb4bb760d
    type: comment
  author: codev
  content: Just an FYI for others who might be running into similar issues - I was
    able to fine-tune the model on an NVIDIA A100 (40 GB ram) with the command mentioned
    in my first message. I ran out of memory when not using the memory saving strategies.
  created_at: 2023-04-05 15:27:52+00:00
  edited: false
  hidden: false
  id: 642da18825c08a2cb4bb760d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-04T10:57:01.000Z'
    data:
      edited: true
      editors:
      - littleworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;codev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/codev\">@<span class=\"\
          underline\">codev</span></a></span>\n\n\t</span></span> Thanks for this\
          \ post. </p>\n<p>I encountered the same problem. Please see my post <a href=\"\
          https://huggingface.co/nferruz/ProtGPT2/discussions/25#64538845e045760131632448\"\
          >here</a>.<br>Since I'm using AWS, I'm not sure what's the right instances\
          \ I can use.<br>Currently, I tried even with AWS p3.16xlarge, but I got\
          \ a Cuda OOM error.</p>\n<p>AWS EC2 p3.16xlarge instance type is powered\
          \ by 8 NVIDIA Tesla V100 GPUs, each with 16 GB of GPU memory.<br>In total,\
          \ it provides 128 GB of GPU memory.</p>\n<p>Do you have any suggestions\
          \ on how to resolve this?</p>\n<p>Sincerely,<br>Littleworth</p>\n"
        raw: "@codev Thanks for this post. \n\nI encountered the same problem. Please\
          \ see my post [here](https://huggingface.co/nferruz/ProtGPT2/discussions/25#64538845e045760131632448).\n\
          Since I'm using AWS, I'm not sure what's the right instances I can use.\
          \ \nCurrently, I tried even with AWS p3.16xlarge, but I got a Cuda OOM error.\n\
          \nAWS EC2 p3.16xlarge instance type is powered by 8 NVIDIA Tesla V100 GPUs,\
          \ each with 16 GB of GPU memory.\nIn total, it provides 128 GB of GPU memory.\n\
          \nDo you have any suggestions on how to resolve this?\n\nSincerely, \nLittleworth"
        updatedAt: '2023-05-04T11:41:48.582Z'
      numEdits: 2
      reactions: []
    id: 64538f7de225c516d49d58d8
    type: comment
  author: littleworth
  content: "@codev Thanks for this post. \n\nI encountered the same problem. Please\
    \ see my post [here](https://huggingface.co/nferruz/ProtGPT2/discussions/25#64538845e045760131632448).\n\
    Since I'm using AWS, I'm not sure what's the right instances I can use. \nCurrently,\
    \ I tried even with AWS p3.16xlarge, but I got a Cuda OOM error.\n\nAWS EC2 p3.16xlarge\
    \ instance type is powered by 8 NVIDIA Tesla V100 GPUs, each with 16 GB of GPU\
    \ memory.\nIn total, it provides 128 GB of GPU memory.\n\nDo you have any suggestions\
    \ on how to resolve this?\n\nSincerely, \nLittleworth"
  created_at: 2023-05-04 09:57:01+00:00
  edited: true
  hidden: false
  id: 64538f7de225c516d49d58d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-05-04T14:19:30.000Z'
    data:
      edited: false
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;littleworth&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/littleworth\"\
          >@<span class=\"underline\">littleworth</span></a></span>\n\n\t</span></span>\
          \ The most important thing is the amount of memory per GPU. The total GPU\
          \ memory does not matter, because it parallelizes batches across the GPUs\
          \ on your instance, each GPU instance will get a single batch, so if that\
          \ batch exceeds your single GPU memory you will get an OOM error. The only\
          \ instance I am aware of that uses NVIDIA A100's is p4d.24xlarge, however\
          \ I have had trouble getting access to these which is why I switched to\
          \ using google colab for fine tuning; this should get you access to A100's\
          \ which have 40GB GPU memory per GPU.  I also use some memory saving tricks\
          \ when I run the fine tuning, see this article here for more info: <a href=\"\
          https://huggingface.co/docs/transformers/v4.18.0/en/performance\">https://huggingface.co/docs/transformers/v4.18.0/en/performance</a>.</p>\n"
        raw: '@littleworth The most important thing is the amount of memory per GPU.
          The total GPU memory does not matter, because it parallelizes batches across
          the GPUs on your instance, each GPU instance will get a single batch, so
          if that batch exceeds your single GPU memory you will get an OOM error.
          The only instance I am aware of that uses NVIDIA A100''s is p4d.24xlarge,
          however I have had trouble getting access to these which is why I switched
          to using google colab for fine tuning; this should get you access to A100''s
          which have 40GB GPU memory per GPU.  I also use some memory saving tricks
          when I run the fine tuning, see this article here for more info: https://huggingface.co/docs/transformers/v4.18.0/en/performance.'
        updatedAt: '2023-05-04T14:19:30.367Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - littleworth
    id: 6453bef272d331dec898540f
    type: comment
  author: codev
  content: '@littleworth The most important thing is the amount of memory per GPU.
    The total GPU memory does not matter, because it parallelizes batches across the
    GPUs on your instance, each GPU instance will get a single batch, so if that batch
    exceeds your single GPU memory you will get an OOM error. The only instance I
    am aware of that uses NVIDIA A100''s is p4d.24xlarge, however I have had trouble
    getting access to these which is why I switched to using google colab for fine
    tuning; this should get you access to A100''s which have 40GB GPU memory per GPU.  I
    also use some memory saving tricks when I run the fine tuning, see this article
    here for more info: https://huggingface.co/docs/transformers/v4.18.0/en/performance.'
  created_at: 2023-05-04 13:19:30+00:00
  edited: false
  hidden: false
  id: 6453bef272d331dec898540f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-04T15:42:21.000Z'
    data:
      edited: false
      editors:
      - littleworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;codev&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/codev\">@<span class=\"\
          underline\">codev</span></a></span>\n\n\t</span></span> Thanks for your\
          \ advice. I'll give it a try. Meanwhile, I managed to get my code working\
          \ with DeepSpeed.<br>See my comment <a href=\"https://huggingface.co/nferruz/ProtGPT2/discussions/25#6453bf12dd49b82d7af93623\"\
          >here</a><br>Please let me know if that approach is not good or something\
          \ :-)</p>\n"
        raw: '@codev Thanks for your advice. I''ll give it a try. Meanwhile, I managed
          to get my code working with DeepSpeed.

          See my comment [here](https://huggingface.co/nferruz/ProtGPT2/discussions/25#6453bf12dd49b82d7af93623)

          Please let me know if that approach is not good or something :-)'
        updatedAt: '2023-05-04T15:42:21.354Z'
      numEdits: 0
      reactions: []
    id: 6453d25d68cbb276cb50b8b8
    type: comment
  author: littleworth
  content: '@codev Thanks for your advice. I''ll give it a try. Meanwhile, I managed
    to get my code working with DeepSpeed.

    See my comment [here](https://huggingface.co/nferruz/ProtGPT2/discussions/25#6453bf12dd49b82d7af93623)

    Please let me know if that approach is not good or something :-)'
  created_at: 2023-05-04 14:42:21+00:00
  edited: false
  hidden: false
  id: 6453d25d68cbb276cb50b8b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
      fullname: Kathryn Klarich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codev
      type: user
    createdAt: '2023-05-04T15:44:14.000Z'
    data:
      edited: false
      editors:
      - codev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c11dca32fceeb19017c41adf1b0ad79.svg
          fullname: Kathryn Klarich
          isHf: false
          isPro: false
          name: codev
          type: user
        html: '<p>I''m not familiar with DeepSpeed, but glad you got it working!</p>

          '
        raw: I'm not familiar with DeepSpeed, but glad you got it working!
        updatedAt: '2023-05-04T15:44:14.345Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - littleworth
    id: 6453d2ce72d331dec89a8263
    type: comment
  author: codev
  content: I'm not familiar with DeepSpeed, but glad you got it working!
  created_at: 2023-05-04 14:44:14+00:00
  edited: false
  hidden: false
  id: 6453d2ce72d331dec89a8263
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Recommended GPU architecture for fine tuning
