!!python/object:huggingface_hub.community.DiscussionWithDetails
author: littleworth
conflicting_files: null
created_at: 2023-05-05 00:13:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-05T01:13:14.000Z'
    data:
      edited: true
      editors:
      - littleworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nferruz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nferruz\">@<span class=\"\
          underline\">nferruz</span></a></span>\n\n\t</span></span> </p>\n<p>Dear\
          \ Noeli,</p>\n<p>I'd like to express my appreciation for your efforts in\
          \ developing the ProtGPT2 model. It's an excellent resource for the research\
          \ community, and I'm excited to work with it.</p>\n<p>I have a few questions\
          \ regarding the correct usage of prompts and output length when working\
          \ with ProtGPT2. In your provided example, I noticed that you use an <code>&lt;|endoftext|&gt;</code>\
          \ as the prompt:</p>\n<pre><code>from transformers import pipeline\nprotgpt2\
          \ = pipeline('text-generation', model=\"nferruz/ProtGPT2\")\nsequences =\
          \ protgpt2(\"&lt;|endoftext|&gt;\", max_length=100, do_sample=True, top_k=950,\
          \ repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)\nfor\
          \ seq in sequences:\n    print(seq)\n</code></pre>\n<p>Is it necessary to\
          \ include the <code>&lt;|endoftext|&gt;</code> as a prompt? I experimented\
          \ with using \"MKK\" directly as my prompt and the model returned results\
          \ without any errors. However, I'm concerned about the accuracy of the results\
          \ when using this approach. Could you please clarify whether the <code>&lt;|endoftext|&gt;</code>\
          \  token is required for accurate output?<br>Like  is <code>&lt;|endoftext|&gt;MKK</code>\
          \ a more correct approach?</p>\n<p>Additionally, does ProtGPT2 default to\
          \ starting with \"M\" as the beginning of the generated sequence when provided\
          \ with an <code>&lt;|endoftext|&gt;</code> prompt only?</p>\n<p>My second\
          \ question pertains to the max_length parameter. In your example, you set\
          \ <code>max_length=100</code>, but the generated output can exceed this\
          \ length. Is this an expected behavior of the model?</p>\n<p>Your guidance\
          \ on these matters would be greatly appreciated. I look forward to hearing\
          \ from you and gaining a deeper understanding of the ProtGPT2 model.</p>\n\
          <p>Sincerely,<br>Littleworth</p>\n"
        raw: "@nferruz \n\nDear Noeli,\n\nI'd like to express my appreciation for\
          \ your efforts in developing the ProtGPT2 model. It's an excellent resource\
          \ for the research community, and I'm excited to work with it.\n\nI have\
          \ a few questions regarding the correct usage of prompts and output length\
          \ when working with ProtGPT2. In your provided example, I noticed that you\
          \ use an `<|endoftext|>` as the prompt:\n\n```\nfrom transformers import\
          \ pipeline\nprotgpt2 = pipeline('text-generation', model=\"nferruz/ProtGPT2\"\
          )\nsequences = protgpt2(\"<|endoftext|>\", max_length=100, do_sample=True,\
          \ top_k=950, repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)\n\
          for seq in sequences:\n    print(seq)\n```\nIs it necessary to include the\
          \ `<|endoftext|>` as a prompt? I experimented with using \"MKK\" directly\
          \ as my prompt and the model returned results without any errors. However,\
          \ I'm concerned about the accuracy of the results when using this approach.\
          \ Could you please clarify whether the `<|endoftext|>`  token is required\
          \ for accurate output?\nLike  is `<|endoftext|>MKK` a more correct approach?\n\
          \nAdditionally, does ProtGPT2 default to starting with \"M\" as the beginning\
          \ of the generated sequence when provided with an `<|endoftext|>` prompt\
          \ only?\n\nMy second question pertains to the max_length parameter. In your\
          \ example, you set `max_length=100`, but the generated output can exceed\
          \ this length. Is this an expected behavior of the model?\n\nYour guidance\
          \ on these matters would be greatly appreciated. I look forward to hearing\
          \ from you and gaining a deeper understanding of the ProtGPT2 model.\n\n\
          Sincerely,\nLittleworth"
        updatedAt: '2023-05-05T01:17:14.218Z'
      numEdits: 2
      reactions: []
    id: 6454582acd09ceba0e15ade3
    type: comment
  author: littleworth
  content: "@nferruz \n\nDear Noeli,\n\nI'd like to express my appreciation for your\
    \ efforts in developing the ProtGPT2 model. It's an excellent resource for the\
    \ research community, and I'm excited to work with it.\n\nI have a few questions\
    \ regarding the correct usage of prompts and output length when working with ProtGPT2.\
    \ In your provided example, I noticed that you use an `<|endoftext|>` as the prompt:\n\
    \n```\nfrom transformers import pipeline\nprotgpt2 = pipeline('text-generation',\
    \ model=\"nferruz/ProtGPT2\")\nsequences = protgpt2(\"<|endoftext|>\", max_length=100,\
    \ do_sample=True, top_k=950, repetition_penalty=1.2, num_return_sequences=10,\
    \ eos_token_id=0)\nfor seq in sequences:\n    print(seq)\n```\nIs it necessary\
    \ to include the `<|endoftext|>` as a prompt? I experimented with using \"MKK\"\
    \ directly as my prompt and the model returned results without any errors. However,\
    \ I'm concerned about the accuracy of the results when using this approach. Could\
    \ you please clarify whether the `<|endoftext|>`  token is required for accurate\
    \ output?\nLike  is `<|endoftext|>MKK` a more correct approach?\n\nAdditionally,\
    \ does ProtGPT2 default to starting with \"M\" as the beginning of the generated\
    \ sequence when provided with an `<|endoftext|>` prompt only?\n\nMy second question\
    \ pertains to the max_length parameter. In your example, you set `max_length=100`,\
    \ but the generated output can exceed this length. Is this an expected behavior\
    \ of the model?\n\nYour guidance on these matters would be greatly appreciated.\
    \ I look forward to hearing from you and gaining a deeper understanding of the\
    \ ProtGPT2 model.\n\nSincerely,\nLittleworth"
  created_at: 2023-05-05 00:13:14+00:00
  edited: true
  hidden: false
  id: 6454582acd09ceba0e15ade3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-05-08T04:43:16.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi Littleworth,</p>

          <p>I put <code>&lt;|endoftext|&gt;</code> in the documentation as an example
          so that the model starts with a de novo sequence. But you can start with
          any sequence seed you want, like the one you mention. Or one could also
          leave it empty.</p>

          <p>The model will often generate an ''M'' after <code>&lt;|endoftext|&gt;</code>,
          but not always. It reproduces the distribution shown in the training set,
          and since some natural sequences don''t start with ''M'', sometimes it generates
          other amino acids too.</p>

          <p>The <code>max_length</code> param refers to the number of tokens. Each
          token has an average length of 4 amino acids, so I''d expect that a <code>max_length</code>
          of 100 gives sequences from anywhere to 0 to 500 amino acids. </p>

          <p>Hope this helps<br>Noelia</p>

          '
        raw: "Hi Littleworth,\n\nI put `<|endoftext|>` in the documentation as an\
          \ example so that the model starts with a de novo sequence. But you can\
          \ start with any sequence seed you want, like the one you mention. Or one\
          \ could also leave it empty.\n\nThe model will often generate an 'M' after\
          \ `<|endoftext|>`, but not always. It reproduces the distribution shown\
          \ in the training set, and since some natural sequences don't start with\
          \ 'M', sometimes it generates other amino acids too.\n\nThe `max_length`\
          \ param refers to the number of tokens. Each token has an average length\
          \ of 4 amino acids, so I'd expect that a `max_length` of 100 gives sequences\
          \ from anywhere to 0 to 500 amino acids. \n\nHope this helps\nNoelia"
        updatedAt: '2023-05-08T04:43:16.797Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - littleworth
        - veghen
        - Biohebb
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Biohebb
    id: 64587de4116c6b3c62ea3fd9
    type: comment
  author: nferruz
  content: "Hi Littleworth,\n\nI put `<|endoftext|>` in the documentation as an example\
    \ so that the model starts with a de novo sequence. But you can start with any\
    \ sequence seed you want, like the one you mention. Or one could also leave it\
    \ empty.\n\nThe model will often generate an 'M' after `<|endoftext|>`, but not\
    \ always. It reproduces the distribution shown in the training set, and since\
    \ some natural sequences don't start with 'M', sometimes it generates other amino\
    \ acids too.\n\nThe `max_length` param refers to the number of tokens. Each token\
    \ has an average length of 4 amino acids, so I'd expect that a `max_length` of\
    \ 100 gives sequences from anywhere to 0 to 500 amino acids. \n\nHope this helps\n\
    Noelia"
  created_at: 2023-05-08 03:43:16+00:00
  edited: false
  hidden: false
  id: 64587de4116c6b3c62ea3fd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-08T05:15:55.000Z'
    data:
      edited: false
      editors:
      - littleworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nferruz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nferruz\">@<span class=\"\
          underline\">nferruz</span></a></span>\n\n\t</span></span> Thanks so much\
          \ for your clarification.</p>\n"
        raw: '@nferruz Thanks so much for your clarification.'
        updatedAt: '2023-05-08T05:15:55.551Z'
      numEdits: 0
      reactions: []
    id: 6458858b5fc3b8a21eae1ab1
    type: comment
  author: littleworth
  content: '@nferruz Thanks so much for your clarification.'
  created_at: 2023-05-08 04:15:55+00:00
  edited: false
  hidden: false
  id: 6458858b5fc3b8a21eae1ab1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Inquiry Regarding ProtGPT2 Prompt and Output Length
