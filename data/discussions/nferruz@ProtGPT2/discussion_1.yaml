!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ptynecki
conflicting_files: null
created_at: 2022-06-29 14:20:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652175872773-noauth.jpeg?w=200&h=200&f=face
      fullname: Piotr Tynecki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ptynecki
      type: user
    createdAt: '2022-06-29T15:20:10.000Z'
    data:
      edited: true
      editors:
      - ptynecki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652175872773-noauth.jpeg?w=200&h=200&f=face
          fullname: Piotr Tynecki
          isHf: false
          isPro: false
          name: ptynecki
          type: user
        html: "<p>Hello,</p>\n<p>Thank you for wonderful research.<br>Would you like\
          \ to share end-to-end example of AA sequence vectorization using ProtGPT2?</p>\n\
          <p>Including ProtGPT2 model and tokenizer loading and execution, sth like:</p>\n\
          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nmodel_name = \"nferruz/ProtGPT2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to('cuda:0')\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprotein_sequences\
          \ = [\n    \"MINDLLDISRIISGKMTLDRAEVNLTAIARQVVEEQRQAAEAKSIQLLCSTPDTNHYVFG\"\
          ,\n    (...)\n]\n\ninput_ids = tokenizer(protein_sequences, return_tensors=\"\
          pt\").to('cuda:0')\n\noutputs = model(**input_ids)\n(...)\n</code></pre>\n\
          <p>Furthermore, it would be perfect to show how to handle output to get\
          \ fixed length numeric vector for each protein.</p>\n<p>This example could\
          \ help other researchers to compare vector space of the proteins with other\
          \ embeddings like ProtTrans or ESM.</p>\n<p>Regards,<br>Piotr</p>\n"
        raw: "Hello,\n\nThank you for wonderful research.\nWould you like to share\
          \ end-to-end example of AA sequence vectorization using ProtGPT2?\n\nIncluding\
          \ ProtGPT2 model and tokenizer loading and execution, sth like:\n\n```\n\
          from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name\
          \ = \"nferruz/ProtGPT2\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to('cuda:0')\n\
          tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprotein_sequences\
          \ = [\n    \"MINDLLDISRIISGKMTLDRAEVNLTAIARQVVEEQRQAAEAKSIQLLCSTPDTNHYVFG\"\
          ,\n    (...)\n]\n\ninput_ids = tokenizer(protein_sequences, return_tensors=\"\
          pt\").to('cuda:0')\n\noutputs = model(**input_ids)\n(...)\n```\n\nFurthermore,\
          \ it would be perfect to show how to handle output to get fixed length numeric\
          \ vector for each protein.\n\nThis example could help other researchers\
          \ to compare vector space of the proteins with other embeddings like ProtTrans\
          \ or ESM.\n\nRegards,\nPiotr"
        updatedAt: '2022-06-29T15:33:02.231Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - avuhong
    id: 62bc6daa5b1ccdca01d73c09
    type: comment
  author: ptynecki
  content: "Hello,\n\nThank you for wonderful research.\nWould you like to share end-to-end\
    \ example of AA sequence vectorization using ProtGPT2?\n\nIncluding ProtGPT2 model\
    \ and tokenizer loading and execution, sth like:\n\n```\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"nferruz/ProtGPT2\"\n\n\
    model = AutoModelForCausalLM.from_pretrained(model_name).to('cuda:0')\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name)\n\nprotein_sequences = [\n    \"\
    MINDLLDISRIISGKMTLDRAEVNLTAIARQVVEEQRQAAEAKSIQLLCSTPDTNHYVFG\",\n    (...)\n]\n\
    \ninput_ids = tokenizer(protein_sequences, return_tensors=\"pt\").to('cuda:0')\n\
    \noutputs = model(**input_ids)\n(...)\n```\n\nFurthermore, it would be perfect\
    \ to show how to handle output to get fixed length numeric vector for each protein.\n\
    \nThis example could help other researchers to compare vector space of the proteins\
    \ with other embeddings like ProtTrans or ESM.\n\nRegards,\nPiotr"
  created_at: 2022-06-29 14:20:10+00:00
  edited: true
  hidden: false
  id: 62bc6daa5b1ccdca01d73c09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-06-29T21:31:35.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi Piotr,</p>

          <p>Thanks a lot for reaching out! </p>

          <p>I haven''t explored sequence embedding myself yet, since I trained ProtGTP2
          with protein design in mind, but I''d love to see how it performs.<br>But,
          from the original GPT paper (although it wasn''t explored in the GPT2 and
          GPT3 papers), it should be possible to embed the sequences by taking as
          a vector the attention heads.</p>

          <p>Following your code, you could do:</p>

          <p><code>outputs = model(input_ids, output_attentions=True)</code><br>This
          returns a dictionary with keys <code>loss, logits, past_key_values, and
          attentions</code>.</p>

          <p>The attention tensor will have a shape <code>(batch_size, num_heads,
          sequence_length, sequence_length)</code>,<br>in your case: <code>torch.Size([1,
          20, 21, 21])</code>.</p>

          <p>It is 21 because your 60 amino acid long sequence gets converted to a
          21 token-long sequence.<br>This will be a problem if you want vectors of
          the same length after mutating a single amino acid because the number of
          tokens could change.</p>

          <p>I hope this helps for now, but in the meanwhile, I''m going to try to
          read how to do this with autoregressive models in HuggingFace. I''ll get
          back to you; sorry that I do not have hands-on experience to show!</p>

          <p>Noelia</p>

          '
        raw: "Hi Piotr,\n\nThanks a lot for reaching out! \n\nI haven't explored sequence\
          \ embedding myself yet, since I trained ProtGTP2 with protein design in\
          \ mind, but I'd love to see how it performs. \nBut, from the original GPT\
          \ paper (although it wasn't explored in the GPT2 and GPT3 papers), it should\
          \ be possible to embed the sequences by taking as a vector the attention\
          \ heads.\n\nFollowing your code, you could do:\n\n`outputs = model(input_ids,\
          \ output_attentions=True)`\nThis returns a dictionary with keys `loss, logits,\
          \ past_key_values, and attentions`.\n\nThe attention tensor will have a\
          \ shape `(batch_size, num_heads, sequence_length, sequence_length)`,\nin\
          \ your case: `torch.Size([1, 20, 21, 21])`.\n\nIt is 21 because your 60\
          \ amino acid long sequence gets converted to a 21 token-long sequence.\n\
          This will be a problem if you want vectors of the same length after mutating\
          \ a single amino acid because the number of tokens could change.\n\nI hope\
          \ this helps for now, but in the meanwhile, I'm going to try to read how\
          \ to do this with autoregressive models in HuggingFace. I'll get back to\
          \ you; sorry that I do not have hands-on experience to show!\n\nNoelia"
        updatedAt: '2022-06-29T21:31:35.744Z'
      numEdits: 0
      reactions: []
    id: 62bcc4b7006042e1899f3419
    type: comment
  author: nferruz
  content: "Hi Piotr,\n\nThanks a lot for reaching out! \n\nI haven't explored sequence\
    \ embedding myself yet, since I trained ProtGTP2 with protein design in mind,\
    \ but I'd love to see how it performs. \nBut, from the original GPT paper (although\
    \ it wasn't explored in the GPT2 and GPT3 papers), it should be possible to embed\
    \ the sequences by taking as a vector the attention heads.\n\nFollowing your code,\
    \ you could do:\n\n`outputs = model(input_ids, output_attentions=True)`\nThis\
    \ returns a dictionary with keys `loss, logits, past_key_values, and attentions`.\n\
    \nThe attention tensor will have a shape `(batch_size, num_heads, sequence_length,\
    \ sequence_length)`,\nin your case: `torch.Size([1, 20, 21, 21])`.\n\nIt is 21\
    \ because your 60 amino acid long sequence gets converted to a 21 token-long sequence.\n\
    This will be a problem if you want vectors of the same length after mutating a\
    \ single amino acid because the number of tokens could change.\n\nI hope this\
    \ helps for now, but in the meanwhile, I'm going to try to read how to do this\
    \ with autoregressive models in HuggingFace. I'll get back to you; sorry that\
    \ I do not have hands-on experience to show!\n\nNoelia"
  created_at: 2022-06-29 20:31:35+00:00
  edited: false
  hidden: false
  id: 62bcc4b7006042e1899f3419
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: End-to-end example for AA sequence vectorization
