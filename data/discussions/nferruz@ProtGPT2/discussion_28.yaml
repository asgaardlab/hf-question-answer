!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rqh
conflicting_files: null
created_at: 2023-05-22 09:11:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc4a5e7deba0d04e88e6abec7a002747.svg
      fullname: Rongqi Hong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rqh
      type: user
    createdAt: '2023-05-22T10:11:15.000Z'
    data:
      edited: true
      editors:
      - rqh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc4a5e7deba0d04e88e6abec7a002747.svg
          fullname: Rongqi Hong
          isHf: false
          isPro: false
          name: rqh
          type: user
        html: '<p>Hello,</p>

          <p>I am trying to fine-tune ProtGPT2 using a training dataset of about 10000
          sequences. However, the loss of the train set and the validation set is
          always around 3. I have tried adjusting data size and the learning rate,
          but nothing seemed to work. Has anyone else run into this?<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/646b314a1da1b6d027f9a0a5/vOoYjWJMV1OARink2Gu1U.png"><img
          alt="loss.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/646b314a1da1b6d027f9a0a5/vOoYjWJMV1OARink2Gu1U.png"></a></p>

          <p>"python run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file
          /home/dell/train.txt --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval
          --output_dir /home/dell/result --learning_rate 1e-06 --num_train_epochs
          30 --gradient_accumulation_steps=4 --per_device_train_batch_size=8 --overwrite_output_dir
          --gradient_checkpointing=True --fp16=True --logging_steps 1 --evaluation_strategy
          epoch --validation_split_percentage 10"</p>

          <p>Thanks in advance!<br>rqh</p>

          '
        raw: 'Hello,


          I am trying to fine-tune ProtGPT2 using a training dataset of about 10000
          sequences. However, the loss of the train set and the validation set is
          always around 3. I have tried adjusting data size and the learning rate,
          but nothing seemed to work. Has anyone else run into this?

          ![loss.PNG](https://cdn-uploads.huggingface.co/production/uploads/646b314a1da1b6d027f9a0a5/vOoYjWJMV1OARink2Gu1U.png)


          "python run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file /home/dell/train.txt
          --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir /home/dell/result
          --learning_rate 1e-06 --num_train_epochs 30 --gradient_accumulation_steps=4
          --per_device_train_batch_size=8 --overwrite_output_dir --gradient_checkpointing=True
          --fp16=True --logging_steps 1 --evaluation_strategy epoch --validation_split_percentage
          10"


          Thanks in advance!

          rqh'
        updatedAt: '2023-05-22T10:12:26.343Z'
      numEdits: 1
      reactions: []
    id: 646b3fc3942fd7c135da6f10
    type: comment
  author: rqh
  content: 'Hello,


    I am trying to fine-tune ProtGPT2 using a training dataset of about 10000 sequences.
    However, the loss of the train set and the validation set is always around 3.
    I have tried adjusting data size and the learning rate, but nothing seemed to
    work. Has anyone else run into this?

    ![loss.PNG](https://cdn-uploads.huggingface.co/production/uploads/646b314a1da1b6d027f9a0a5/vOoYjWJMV1OARink2Gu1U.png)


    "python run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file /home/dell/train.txt
    --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir /home/dell/result
    --learning_rate 1e-06 --num_train_epochs 30 --gradient_accumulation_steps=4 --per_device_train_batch_size=8
    --overwrite_output_dir --gradient_checkpointing=True --fp16=True --logging_steps
    1 --evaluation_strategy epoch --validation_split_percentage 10"


    Thanks in advance!

    rqh'
  created_at: 2023-05-22 09:11:15+00:00
  edited: true
  hidden: false
  id: 646b3fc3942fd7c135da6f10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-05-22T12:25:21.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi rqh,</p>

          <p>The loss sounds fine to me (as long as your curves look good). The loss
          never went very low in numbers for us as well, although it reached a plateau
          for more than half of the epochs.  I assumed it was due to the large vocabulary
          size (&gt;52000). For our second model, we reached much lower losses as
          the vocabulary only has 20 tokens. I''d focus more on the quality of the
          generated sequences. We''ve seen huge differences among different trainings.
          If your protein of interest is an enzyme I highly recommend using ZymCTRL
          instead.</p>

          '
        raw: 'Hi rqh,


          The loss sounds fine to me (as long as your curves look good). The loss
          never went very low in numbers for us as well, although it reached a plateau
          for more than half of the epochs.  I assumed it was due to the large vocabulary
          size (>52000). For our second model, we reached much lower losses as the
          vocabulary only has 20 tokens. I''d focus more on the quality of the generated
          sequences. We''ve seen huge differences among different trainings. If your
          protein of interest is an enzyme I highly recommend using ZymCTRL instead.'
        updatedAt: '2023-05-22T12:25:21.985Z'
      numEdits: 0
      reactions: []
    id: 646b5f31db697c798a390239
    type: comment
  author: nferruz
  content: 'Hi rqh,


    The loss sounds fine to me (as long as your curves look good). The loss never
    went very low in numbers for us as well, although it reached a plateau for more
    than half of the epochs.  I assumed it was due to the large vocabulary size (>52000).
    For our second model, we reached much lower losses as the vocabulary only has
    20 tokens. I''d focus more on the quality of the generated sequences. We''ve seen
    huge differences among different trainings. If your protein of interest is an
    enzyme I highly recommend using ZymCTRL instead.'
  created_at: 2023-05-22 11:25:21+00:00
  edited: false
  hidden: false
  id: 646b5f31db697c798a390239
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Model fine-tuning does not work well
