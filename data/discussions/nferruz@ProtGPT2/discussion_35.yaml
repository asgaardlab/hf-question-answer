!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xenidisv
conflicting_files: null
created_at: 2023-11-10 19:20:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9916f55f9a66ad40bdaa4c0cf6ed2ef.svg
      fullname: vxenidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xenidisv
      type: user
    createdAt: '2023-11-10T19:20:46.000Z'
    data:
      edited: false
      editors:
      - xenidisv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7510651350021362
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9916f55f9a66ad40bdaa4c0cf6ed2ef.svg
          fullname: vxenidis
          isHf: false
          isPro: false
          name: xenidisv
          type: user
        html: '<p>Hi, I am a beginner in the field and I am sorry in advance for my
          naive questions.</p>

          <p> I want to fine tune protgpt2 and I have adopted the instructions like
          this. In a Jupyter notebook, I run these commands:</p>

          <p>pip install evaluate<br>pip install transformers[torch]<br>pip install
          accelerate -U<br>pip install datasets --upgrade<br>pip install --upgrade
          protobuf<br>pip install --upgrade wandb<br>pip install git+<a rel="nofollow"
          href="https://github.com/huggingface/transformers.git">https://github.com/huggingface/transformers.git</a></p>

          <p>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM,
          pipeline<br>import os<br>os.environ["WANDB_DISABLED"] = "true"<br>tokenizer
          = AutoTokenizer.from_pretrained("nferruz/ProtGPT2")<br>model = AutoModelForCausalLM.from_pretrained("nferruz/ProtGPT2")</p>

          <h2 id="i-have-created-the-filtered_training_listtxt-and-filtered_validation_listtxt">I
          have created the filtered_training_list.txt and filtered_validation_list.txt</h2>

          <p>!python run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file
          filtered_training_list.txt --validation_file filtered_validation_list.txt
          --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir output_dir_test
          --learning_rate 1e-06 --low_cpu_mem_usage  --per_device_train_batch_size
          2  --per_device_eval_batch_size 2</p>

          <p>When I run this in a jupyter notebook, my terminal crashes (probably
          due to cpu/memory usage?). When I run it in google colab though it was successful.
          Searching in the google drive, I found the output_dir_test, but its an extremely
          small file (88 bytes). I opened it with a text editor and it said this:</p>

          <p>brain.Event:2R.<br>,tensorboard.summary.writer.event_file_writer (+plus
          12 other binary characters that cannot be pasted here)</p>

          <p>My questions are:</p>

          <ol>

          <li>Do you have any suggestions to what am I doing wrong?</li>

          <li>How can I run the finetuned model once I have generated it (to generate
          new sequences like in example 1)? </li>

          <li>While searching on how to finetune the model, I came across to the button
          train, Amazon sagemaker. Is there a tutorial that I could follow in order
          to finetune protgpt2?</li>

          </ol>

          <p>Thank you in advance</p>

          <p>Kind regards,<br>Bill</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/653ac15054ae91693239331f/jUVBroQ8WWh7GKi2F-74J.png"><img
          alt="Screenshot_20231110_211950.png" src="https://cdn-uploads.huggingface.co/production/uploads/653ac15054ae91693239331f/jUVBroQ8WWh7GKi2F-74J.png"></a></p>

          '
        raw: "Hi, I am a beginner in the field and I am sorry in advance for my naive\
          \ questions.\r\n\r\n I want to fine tune protgpt2 and I have adopted the\
          \ instructions like this. In a Jupyter notebook, I run these commands:\r\
          \n\r\npip install evaluate\r\npip install transformers[torch]\r\npip install\
          \ accelerate -U\r\npip install datasets --upgrade\r\npip install --upgrade\
          \ protobuf\r\npip install --upgrade wandb\r\npip install git+https://github.com/huggingface/transformers.git\r\
          \n\r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM,\
          \ pipeline\r\nimport os\r\nos.environ[\"WANDB_DISABLED\"] = \"true\"\r\n\
          tokenizer = AutoTokenizer.from_pretrained(\"nferruz/ProtGPT2\")\r\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\"nferruz/ProtGPT2\")\r\n\r\n##\
          \ I have created the filtered_training_list.txt and filtered_validation_list.txt\r\
          \n\r\n!python run_clm.py --model_name_or_path nferruz/ProtGPT2 --train_file\
          \ filtered_training_list.txt --validation_file filtered_validation_list.txt\
          \ --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir output_dir_test\
          \ --learning_rate 1e-06 --low_cpu_mem_usage  --per_device_train_batch_size\
          \ 2  --per_device_eval_batch_size 2\r\n\r\nWhen I run this in a jupyter\
          \ notebook, my terminal crashes (probably due to cpu/memory usage?). When\
          \ I run it in google colab though it was successful. Searching in the google\
          \ drive, I found the output_dir_test, but its an extremely small file (88\
          \ bytes). I opened it with a text editor and it said this:\r\n\r\nbrain.Event:2R.\r\
          \n,tensorboard.summary.writer.event_file_writer (+plus 12 other binary characters\
          \ that cannot be pasted here)\r\n\r\nMy questions are:\r\n1) Do you have\
          \ any suggestions to what am I doing wrong?\r\n2) How can I run the finetuned\
          \ model once I have generated it (to generate new sequences like in example\
          \ 1)? \r\n3) While searching on how to finetune the model, I came across\
          \ to the button train, Amazon sagemaker. Is there a tutorial that I could\
          \ follow in order to finetune protgpt2?\r\n\r\nThank you in advance\r\n\r\
          \nKind regards,\r\nBill\r\n\r\n\r\n![Screenshot_20231110_211950.png](https://cdn-uploads.huggingface.co/production/uploads/653ac15054ae91693239331f/jUVBroQ8WWh7GKi2F-74J.png)\r\
          \n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
        updatedAt: '2023-11-10T19:20:46.719Z'
      numEdits: 0
      reactions: []
    id: 654e828ec66f41a9920f5dd0
    type: comment
  author: xenidisv
  content: "Hi, I am a beginner in the field and I am sorry in advance for my naive\
    \ questions.\r\n\r\n I want to fine tune protgpt2 and I have adopted the instructions\
    \ like this. In a Jupyter notebook, I run these commands:\r\n\r\npip install evaluate\r\
    \npip install transformers[torch]\r\npip install accelerate -U\r\npip install\
    \ datasets --upgrade\r\npip install --upgrade protobuf\r\npip install --upgrade\
    \ wandb\r\npip install git+https://github.com/huggingface/transformers.git\r\n\
    \r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM,\
    \ pipeline\r\nimport os\r\nos.environ[\"WANDB_DISABLED\"] = \"true\"\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"nferruz/ProtGPT2\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    nferruz/ProtGPT2\")\r\n\r\n## I have created the filtered_training_list.txt and\
    \ filtered_validation_list.txt\r\n\r\n!python run_clm.py --model_name_or_path\
    \ nferruz/ProtGPT2 --train_file filtered_training_list.txt --validation_file filtered_validation_list.txt\
    \ --tokenizer_name nferruz/ProtGPT2 --do_train --do_eval --output_dir output_dir_test\
    \ --learning_rate 1e-06 --low_cpu_mem_usage  --per_device_train_batch_size 2 \
    \ --per_device_eval_batch_size 2\r\n\r\nWhen I run this in a jupyter notebook,\
    \ my terminal crashes (probably due to cpu/memory usage?). When I run it in google\
    \ colab though it was successful. Searching in the google drive, I found the output_dir_test,\
    \ but its an extremely small file (88 bytes). I opened it with a text editor and\
    \ it said this:\r\n\r\nbrain.Event:2R.\r\n,tensorboard.summary.writer.event_file_writer\
    \ (+plus 12 other binary characters that cannot be pasted here)\r\n\r\nMy questions\
    \ are:\r\n1) Do you have any suggestions to what am I doing wrong?\r\n2) How can\
    \ I run the finetuned model once I have generated it (to generate new sequences\
    \ like in example 1)? \r\n3) While searching on how to finetune the model, I came\
    \ across to the button train, Amazon sagemaker. Is there a tutorial that I could\
    \ follow in order to finetune protgpt2?\r\n\r\nThank you in advance\r\n\r\nKind\
    \ regards,\r\nBill\r\n\r\n\r\n![Screenshot_20231110_211950.png](https://cdn-uploads.huggingface.co/production/uploads/653ac15054ae91693239331f/jUVBroQ8WWh7GKi2F-74J.png)\r\
    \n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
  created_at: 2023-11-10 19:20:46+00:00
  edited: false
  hidden: false
  id: 654e828ec66f41a9920f5dd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-11-14T06:21:40.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8542560338973999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi,</p>\n<ol>\n<li><p>I\u2019ve unfortunately never seen this error;\
          \ But I also never fine-tuned on Google Colab (and your PC probably crashes\
          \ most likely to a lack of memory, do you get any out-of-memory errors?)</p>\n\
          </li>\n<li><p>Once you fine-tune the model, replace nferruz/ProtGPT2 in\
          \ your command with the path to the new model and run it again.</p>\n</li>\n\
          <li><p>About Amazon Sagemaker, no, I haven\u2019t written such tutorial,\
          \ but, if you find any tutorials about how to fine-tune gpt2 or any other\
          \ autoregressive LLM, that should work for protgpt2 too (making sure you\
          \ input your formatted training and val txt files).</p>\n</li>\n</ol>\n"
        raw: "Hi,\n\n1) I\u2019ve unfortunately never seen this error; But I also\
          \ never fine-tuned on Google Colab (and your PC probably crashes most likely\
          \ to a lack of memory, do you get any out-of-memory errors?)\n\n2) Once\
          \ you fine-tune the model, replace nferruz/ProtGPT2 in your command with\
          \ the path to the new model and run it again.\n\n3) About Amazon Sagemaker,\
          \ no, I haven\u2019t written such tutorial, but, if you find any tutorials\
          \ about how to fine-tune gpt2 or any other autoregressive LLM, that should\
          \ work for protgpt2 too (making sure you input your formatted training and\
          \ val txt files)."
        updatedAt: '2023-11-14T06:21:40.643Z'
      numEdits: 0
      reactions: []
    id: 655311f4efd7d3fe2341b8e1
    type: comment
  author: nferruz
  content: "Hi,\n\n1) I\u2019ve unfortunately never seen this error; But I also never\
    \ fine-tuned on Google Colab (and your PC probably crashes most likely to a lack\
    \ of memory, do you get any out-of-memory errors?)\n\n2) Once you fine-tune the\
    \ model, replace nferruz/ProtGPT2 in your command with the path to the new model\
    \ and run it again.\n\n3) About Amazon Sagemaker, no, I haven\u2019t written such\
    \ tutorial, but, if you find any tutorials about how to fine-tune gpt2 or any\
    \ other autoregressive LLM, that should work for protgpt2 too (making sure you\
    \ input your formatted training and val txt files)."
  created_at: 2023-11-14 06:21:40+00:00
  edited: false
  hidden: false
  id: 655311f4efd7d3fe2341b8e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9916f55f9a66ad40bdaa4c0cf6ed2ef.svg
      fullname: vxenidis
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xenidisv
      type: user
    createdAt: '2023-11-14T15:51:56.000Z'
    data:
      edited: false
      editors:
      - xenidisv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8837682604789734
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9916f55f9a66ad40bdaa4c0cf6ed2ef.svg
          fullname: vxenidis
          isHf: false
          isPro: false
          name: xenidisv
          type: user
        html: '<p>Hi, thank you for your reply.</p>

          <ol>

          <li>No, the terminal in the laptop just crashes and it closes, it doesnt
          print anything. In jupyter it also does the same thing, it freezes.</li>

          </ol>

          <p>In hugging face learning tutorials it mentions preprocessing my data,
          and using tokenizers to the new preprocessed data. </p>

          <p>a) Is this required with your model? Are the steps (code-pipeline) that
          I follow in my previous answer, correct or am I missing a basic step?  I
          also used --low_cpu_mem_usage, is it correct? Should I edit the run_clm.py
          file?</p>

          <p>b) Also, regarding the training and validation files, I have downloaded
          from a database, peptide sequences (in fasta format) and because they are
          almost 3000 peptides, I have separated 80% of them in filtered_training_list.txt
          and 20% in the filtered_validation.txt. Moreover, I have replaced the header
          and the generated txt file.  It looks like this, Is it correct? :<br>&lt;|endoftext|&gt;
          /n<br>LR..... etc  /n<br>/n<br>&lt;|endoftext|&gt;  /n<br>RS..... etc  /n</p>

          <p>each peptide sequence is 59 amino acids long and then it breaks in a
          new line, eg:</p>

          <p>&lt;|endoftext|&gt;  /n<br>RS..... etc /n<br>LR..... /n<br>SR....  /n<br>
          /n<br>&lt;|endoftext|&gt;  /n<br>LRSFJT..... etc  /n<br>/n</p>

          <p>c) Last but not least, inside the generated folder (output_dir), there
          is a folder called (runs) and inside this directory there is only the previously
          mentioned (events.out.... etc) file. In case of a correctly finetuned model,
          which are the expected directories and files (how they should look like
          and how big are they)?</p>

          <p>Thank you in advance<br>Kind regards<br>Bill</p>

          '
        raw: "Hi, thank you for your reply.\n\n1. No, the terminal in the laptop just\
          \ crashes and it closes, it doesnt print anything. In jupyter it also does\
          \ the same thing, it freezes.\n\nIn hugging face learning tutorials it mentions\
          \ preprocessing my data, and using tokenizers to the new preprocessed data.\
          \ \n\na) Is this required with your model? Are the steps (code-pipeline)\
          \ that I follow in my previous answer, correct or am I missing a basic step?\
          \  I also used --low_cpu_mem_usage, is it correct? Should I edit the run_clm.py\
          \ file?\n\nb) Also, regarding the training and validation files, I have\
          \ downloaded from a database, peptide sequences (in fasta format) and because\
          \ they are almost 3000 peptides, I have separated 80% of them in filtered_training_list.txt\
          \ and 20% in the filtered_validation.txt. Moreover, I have replaced the\
          \ header and the generated txt file.  It looks like this, Is it correct?\
          \ :\n<|endoftext|> /n\nLR..... etc  /n\n/n\n<|endoftext|>  /n\nRS..... etc\
          \  /n\n\neach peptide sequence is 59 amino acids long and then it breaks\
          \ in a new line, eg:\n\n<|endoftext|>  /n\nRS..... etc /n\nLR..... /n\n\
          SR....  /n\n /n\n<|endoftext|>  /n\nLRSFJT..... etc  /n\n/n\n\nc) Last but\
          \ not least, inside the generated folder (output_dir), there is a folder\
          \ called (runs) and inside this directory there is only the previously mentioned\
          \ (events.out.... etc) file. In case of a correctly finetuned model, which\
          \ are the expected directories and files (how they should look like and\
          \ how big are they)?\n\nThank you in advance\nKind regards\nBill"
        updatedAt: '2023-11-14T15:51:56.488Z'
      numEdits: 0
      reactions: []
    id: 6553979c94195009d9050e2d
    type: comment
  author: xenidisv
  content: "Hi, thank you for your reply.\n\n1. No, the terminal in the laptop just\
    \ crashes and it closes, it doesnt print anything. In jupyter it also does the\
    \ same thing, it freezes.\n\nIn hugging face learning tutorials it mentions preprocessing\
    \ my data, and using tokenizers to the new preprocessed data. \n\na) Is this required\
    \ with your model? Are the steps (code-pipeline) that I follow in my previous\
    \ answer, correct or am I missing a basic step?  I also used --low_cpu_mem_usage,\
    \ is it correct? Should I edit the run_clm.py file?\n\nb) Also, regarding the\
    \ training and validation files, I have downloaded from a database, peptide sequences\
    \ (in fasta format) and because they are almost 3000 peptides, I have separated\
    \ 80% of them in filtered_training_list.txt and 20% in the filtered_validation.txt.\
    \ Moreover, I have replaced the header and the generated txt file.  It looks like\
    \ this, Is it correct? :\n<|endoftext|> /n\nLR..... etc  /n\n/n\n<|endoftext|>\
    \  /n\nRS..... etc  /n\n\neach peptide sequence is 59 amino acids long and then\
    \ it breaks in a new line, eg:\n\n<|endoftext|>  /n\nRS..... etc /n\nLR..... /n\n\
    SR....  /n\n /n\n<|endoftext|>  /n\nLRSFJT..... etc  /n\n/n\n\nc) Last but not\
    \ least, inside the generated folder (output_dir), there is a folder called (runs)\
    \ and inside this directory there is only the previously mentioned (events.out....\
    \ etc) file. In case of a correctly finetuned model, which are the expected directories\
    \ and files (how they should look like and how big are they)?\n\nThank you in\
    \ advance\nKind regards\nBill"
  created_at: 2023-11-14 15:51:56+00:00
  edited: false
  hidden: false
  id: 6553979c94195009d9050e2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 35
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Questions regarding fine tuning protgpt2
