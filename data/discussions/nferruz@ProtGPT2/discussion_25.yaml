!!python/object:huggingface_hub.community.DiscussionWithDetails
author: littleworth
conflicting_files: null
created_at: 2023-05-04 09:26:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-04T10:26:13.000Z'
    data:
      edited: true
      editors:
      - littleworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nferruz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nferruz\">@<span class=\"\
          underline\">nferruz</span></a></span>\n\n\t</span></span>  Hi Noelia,</p>\n\
          <p>Will consider also having ProtGPT2-medium and ProtGPT2-small, please?<br>This\
          \ will be of great help to people who want to debug or don't have large-capacity\
          \ GPU machines.</p>\n<p>Currently with this parameter running on AWS p3.16xlarge,\
          \ the program crash with a CUDA memory error.</p>\n<p>AWS EC2 p3.16xlarge\
          \ instance type is powered by 8 NVIDIA Tesla V100 GPUs, each with 16 GB\
          \ of GPU memory.<br>In total, the p3.16xlarge instance provides 128 GB of\
          \ GPU memory</p>\n<p>Do you have any suggest what parameter I can use to\
          \ avoid that?</p>\n<pre><code>TRAINING_FILE=\"data/ha_filtered_108k.train.gpt2_format.txt\"\
          \   # 80K lines\nVALIDATION_FILE=\"data/ha_filtered_108k.validation.gpt2_format.txt\"\
          \ # 20K lines\nMODEL_OUTPUT_DIR=\"gpt2_model/ha_filtered_108k\"\n\npython\
          \ run_clm.py --model_name_or_path nferruz/ProtGPT2 \\\n    --train_file\
          \ ${TRAINING_FILE} \\\n    --validation_file ${VALIDATION_FILE} \\\n   \
          \ --tokenizer_name nferruz/ProtGPT2 \\\n    --do_train \\\n    --do_eval\
          \  \\\n    --output_dir ${MODEL_OUTPUT_DIR} \\\n    --overwrite_output_dir\
          \ \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size\
          \ 1 \\\n    --gradient_accumulation_steps=16 \\\n    --fp16 \\\n    --learning_rate\
          \ 1e-06\n</code></pre>\n<p>Sincerely,<br>Littleworth</p>\n"
        raw: "@nferruz  Hi Noelia,\n\nWill consider also having ProtGPT2-medium and\
          \ ProtGPT2-small, please?\nThis will be of great help to people who want\
          \ to debug or don't have large-capacity GPU machines.\n\nCurrently with\
          \ this parameter running on AWS p3.16xlarge, the program crash with a CUDA\
          \ memory error.\n\nAWS EC2 p3.16xlarge instance type is powered by 8 NVIDIA\
          \ Tesla V100 GPUs, each with 16 GB of GPU memory. \nIn total, the p3.16xlarge\
          \ instance provides 128 GB of GPU memory\n\nDo you have any suggest what\
          \ parameter I can use to avoid that?\n\n```\nTRAINING_FILE=\"data/ha_filtered_108k.train.gpt2_format.txt\"\
          \   # 80K lines\nVALIDATION_FILE=\"data/ha_filtered_108k.validation.gpt2_format.txt\"\
          \ # 20K lines\nMODEL_OUTPUT_DIR=\"gpt2_model/ha_filtered_108k\"\n\npython\
          \ run_clm.py --model_name_or_path nferruz/ProtGPT2 \\\n    --train_file\
          \ ${TRAINING_FILE} \\\n    --validation_file ${VALIDATION_FILE} \\\n   \
          \ --tokenizer_name nferruz/ProtGPT2 \\\n    --do_train \\\n    --do_eval\
          \  \\\n    --output_dir ${MODEL_OUTPUT_DIR} \\\n    --overwrite_output_dir\
          \ \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size\
          \ 1 \\\n    --gradient_accumulation_steps=16 \\\n    --fp16 \\\n    --learning_rate\
          \ 1e-06\n```\n\nSincerely,\nLittleworth"
        updatedAt: '2023-05-04T11:37:47.122Z'
      numEdits: 4
      reactions: []
    id: 64538845e045760131632448
    type: comment
  author: littleworth
  content: "@nferruz  Hi Noelia,\n\nWill consider also having ProtGPT2-medium and\
    \ ProtGPT2-small, please?\nThis will be of great help to people who want to debug\
    \ or don't have large-capacity GPU machines.\n\nCurrently with this parameter\
    \ running on AWS p3.16xlarge, the program crash with a CUDA memory error.\n\n\
    AWS EC2 p3.16xlarge instance type is powered by 8 NVIDIA Tesla V100 GPUs, each\
    \ with 16 GB of GPU memory. \nIn total, the p3.16xlarge instance provides 128\
    \ GB of GPU memory\n\nDo you have any suggest what parameter I can use to avoid\
    \ that?\n\n```\nTRAINING_FILE=\"data/ha_filtered_108k.train.gpt2_format.txt\"\
    \   # 80K lines\nVALIDATION_FILE=\"data/ha_filtered_108k.validation.gpt2_format.txt\"\
    \ # 20K lines\nMODEL_OUTPUT_DIR=\"gpt2_model/ha_filtered_108k\"\n\npython run_clm.py\
    \ --model_name_or_path nferruz/ProtGPT2 \\\n    --train_file ${TRAINING_FILE}\
    \ \\\n    --validation_file ${VALIDATION_FILE} \\\n    --tokenizer_name nferruz/ProtGPT2\
    \ \\\n    --do_train \\\n    --do_eval  \\\n    --output_dir ${MODEL_OUTPUT_DIR}\
    \ \\\n    --overwrite_output_dir \\\n    --per_device_train_batch_size 1 \\\n\
    \    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps=16 \\\
    \n    --fp16 \\\n    --learning_rate 1e-06\n```\n\nSincerely,\nLittleworth"
  created_at: 2023-05-04 09:26:13+00:00
  edited: true
  hidden: false
  id: 64538845e045760131632448
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-04T10:26:26.000Z'
    data:
      from: ProtGPT2-medium and ProtGPT2-small
      to: Making ProtGPT2-medium and ProtGPT2-small available?
    id: 64538852c0d2e20ec0ac6b07
    type: title-change
  author: littleworth
  created_at: 2023-05-04 09:26:26+00:00
  id: 64538852c0d2e20ec0ac6b07
  new_title: Making ProtGPT2-medium and ProtGPT2-small available?
  old_title: ProtGPT2-medium and ProtGPT2-small
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-05-04T12:05:08.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi Littleworth,</p>

          <p>I never trained a small or medium version, I am afraid. I trained another
          model but it is even bigger.</p>

          <p>Sorry I don''t have better news for now!</p>

          '
        raw: 'Hi Littleworth,


          I never trained a small or medium version, I am afraid. I trained another
          model but it is even bigger.


          Sorry I don''t have better news for now!'
        updatedAt: '2023-05-04T12:05:08.077Z'
      numEdits: 0
      reactions: []
    id: 64539f74399b1b9a0a86796d
    type: comment
  author: nferruz
  content: 'Hi Littleworth,


    I never trained a small or medium version, I am afraid. I trained another model
    but it is even bigger.


    Sorry I don''t have better news for now!'
  created_at: 2023-05-04 11:05:08+00:00
  edited: false
  hidden: false
  id: 64539f74399b1b9a0a86796d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-05-04T14:20:02.000Z'
    data:
      edited: false
      editors:
      - littleworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nferruz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nferruz\">@<span class=\"\
          underline\">nferruz</span></a></span>\n\n\t</span></span> Hi Noelia, </p>\n\
          <p>Thanks. I finally managed to get it running with the help of <a href=\"\
          https://huggingface.co/blog/accelerate-deepspeed\">DeepSpeed</a>.<br>Here\
          \ is the full code:</p>\n<pre><code>#!/bin/bash\nexport LC_ALL=C\n\nTRAINING_FILE=\"\
          data/ha_filtered_108k.train.gpt2_format.txt\"\nVALIDATION_FILE=\"data/ha_filtered_108k.validation.gpt2_format.txt\"\
          \nMODEL_OUTPUT_DIR=\"gpt2_model/ha_filtered_108k\"\nDS_CONFIG_FILE=\"ds_config.json\"\
          \n\n\n/home/ubuntu/storage1/conda_envs/py38/bin/deepspeed --num_gpus=8 run_clm.py\
          \ --model_name_or_path nferruz/ProtGPT2 \\\n    --train_file ${TRAINING_FILE}\
          \ \\\n    --validation_file ${VALIDATION_FILE} \\\n    --tokenizer_name\
          \ nferruz/ProtGPT2 \\\n    --do_train \\\n    --do_eval  \\\n    --output_dir\
          \ ${MODEL_OUTPUT_DIR} \\\n    --overwrite_output_dir \\\n    --per_device_train_batch_size\
          \ 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps=16\
          \ \\\n    --fp16 \\\n    --learning_rate 1e-06 \\\n    --deepspeed ${DS_CONFIG_FILE}\n\
          </code></pre>\n<p>And <code>ds_config.json</code> file content is:</p>\n\
          <pre><code>{\n    \"fp16\": {\n        \"enabled\": true\n    },\n    \"\
          zero_optimization\": {\n        \"stage\": 2,\n        \"allgather_partitions\"\
          : true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\"\
          : true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\"\
          : 2e8,\n        \"contiguous_gradients\": true\n    },\n    \"optimizer\"\
          : {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"\
          lr\": 1e-6,\n            \"betas\": [\n                0.9,\n          \
          \      0.999\n            ],\n            \"eps\": 1e-8,\n            \"\
          weight_decay\": 0\n        }\n    },\n    \"scheduler\": {\n        \"type\"\
          : \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": 0,\n\
          \            \"warmup_max_lr\": 1e-6,\n            \"warmup_num_steps\"\
          : \"auto\"\n        }\n    },\n    \"train_batch_size\": \"auto\",\n   \
          \ \"train_micro_batch_size_per_gpu\": 1,\n    \"gradient_accumulation_steps\"\
          : 16,\n    \"gradient_clipping\": 1.0\n}\n</code></pre>\n<p>Everything is\
          \ completed in less than 10 minutes with <code>p3.16xlarge</code>.<br>Hope\
          \ this information will help others.</p>\n<p>Regards,<br>littleworth</p>\n"
        raw: "@nferruz Hi Noelia, \n\nThanks. I finally managed to get it running\
          \ with the help of [DeepSpeed](https://huggingface.co/blog/accelerate-deepspeed).\n\
          Here is the full code:\n\n```\n#!/bin/bash\nexport LC_ALL=C\n\nTRAINING_FILE=\"\
          data/ha_filtered_108k.train.gpt2_format.txt\"\nVALIDATION_FILE=\"data/ha_filtered_108k.validation.gpt2_format.txt\"\
          \nMODEL_OUTPUT_DIR=\"gpt2_model/ha_filtered_108k\"\nDS_CONFIG_FILE=\"ds_config.json\"\
          \n\n\n/home/ubuntu/storage1/conda_envs/py38/bin/deepspeed --num_gpus=8 run_clm.py\
          \ --model_name_or_path nferruz/ProtGPT2 \\\n    --train_file ${TRAINING_FILE}\
          \ \\\n    --validation_file ${VALIDATION_FILE} \\\n    --tokenizer_name\
          \ nferruz/ProtGPT2 \\\n    --do_train \\\n    --do_eval  \\\n    --output_dir\
          \ ${MODEL_OUTPUT_DIR} \\\n    --overwrite_output_dir \\\n    --per_device_train_batch_size\
          \ 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps=16\
          \ \\\n    --fp16 \\\n    --learning_rate 1e-06 \\\n    --deepspeed ${DS_CONFIG_FILE}\n\
          \n```\n\nAnd `ds_config.json` file content is:\n\n```\n{\n    \"fp16\":\
          \ {\n        \"enabled\": true\n    },\n    \"zero_optimization\": {\n \
          \       \"stage\": 2,\n        \"allgather_partitions\": true,\n       \
          \ \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n   \
          \     \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n\
          \        \"contiguous_gradients\": true\n    },\n    \"optimizer\": {\n\
          \        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\"\
          : 1e-6,\n            \"betas\": [\n                0.9,\n              \
          \  0.999\n            ],\n            \"eps\": 1e-8,\n            \"weight_decay\"\
          : 0\n        }\n    },\n    \"scheduler\": {\n        \"type\": \"WarmupLR\"\
          ,\n        \"params\": {\n            \"warmup_min_lr\": 0,\n          \
          \  \"warmup_max_lr\": 1e-6,\n            \"warmup_num_steps\": \"auto\"\n\
          \        }\n    },\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\"\
          : 1,\n    \"gradient_accumulation_steps\": 16,\n    \"gradient_clipping\"\
          : 1.0\n}\n```\n\nEverything is completed in less than 10 minutes with `p3.16xlarge`.\
          \ \nHope this information will help others.\n\nRegards,\nlittleworth"
        updatedAt: '2023-05-04T14:20:02.856Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - nferruz
    id: 6453bf12dd49b82d7af93623
    type: comment
  author: littleworth
  content: "@nferruz Hi Noelia, \n\nThanks. I finally managed to get it running with\
    \ the help of [DeepSpeed](https://huggingface.co/blog/accelerate-deepspeed).\n\
    Here is the full code:\n\n```\n#!/bin/bash\nexport LC_ALL=C\n\nTRAINING_FILE=\"\
    data/ha_filtered_108k.train.gpt2_format.txt\"\nVALIDATION_FILE=\"data/ha_filtered_108k.validation.gpt2_format.txt\"\
    \nMODEL_OUTPUT_DIR=\"gpt2_model/ha_filtered_108k\"\nDS_CONFIG_FILE=\"ds_config.json\"\
    \n\n\n/home/ubuntu/storage1/conda_envs/py38/bin/deepspeed --num_gpus=8 run_clm.py\
    \ --model_name_or_path nferruz/ProtGPT2 \\\n    --train_file ${TRAINING_FILE}\
    \ \\\n    --validation_file ${VALIDATION_FILE} \\\n    --tokenizer_name nferruz/ProtGPT2\
    \ \\\n    --do_train \\\n    --do_eval  \\\n    --output_dir ${MODEL_OUTPUT_DIR}\
    \ \\\n    --overwrite_output_dir \\\n    --per_device_train_batch_size 1 \\\n\
    \    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps=16 \\\
    \n    --fp16 \\\n    --learning_rate 1e-06 \\\n    --deepspeed ${DS_CONFIG_FILE}\n\
    \n```\n\nAnd `ds_config.json` file content is:\n\n```\n{\n    \"fp16\": {\n  \
    \      \"enabled\": true\n    },\n    \"zero_optimization\": {\n        \"stage\"\
    : 2,\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\"\
    : 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n \
    \       \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n\
    \    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\"\
    : {\n            \"lr\": 1e-6,\n            \"betas\": [\n                0.9,\n\
    \                0.999\n            ],\n            \"eps\": 1e-8,\n         \
    \   \"weight_decay\": 0\n        }\n    },\n    \"scheduler\": {\n        \"type\"\
    : \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": 0,\n  \
    \          \"warmup_max_lr\": 1e-6,\n            \"warmup_num_steps\": \"auto\"\
    \n        }\n    },\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\"\
    : 1,\n    \"gradient_accumulation_steps\": 16,\n    \"gradient_clipping\": 1.0\n\
    }\n```\n\nEverything is completed in less than 10 minutes with `p3.16xlarge`.\
    \ \nHope this information will help others.\n\nRegards,\nlittleworth"
  created_at: 2023-05-04 13:20:02+00:00
  edited: false
  hidden: false
  id: 6453bf12dd49b82d7af93623
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca6ce33088106deef70f1880011ed365.svg
      fullname: Desmond
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deskk
      type: user
    createdAt: '2023-09-29T18:56:02.000Z'
    data:
      edited: true
      editors:
      - deskk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9454253315925598
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca6ce33088106deef70f1880011ed365.svg
          fullname: Desmond
          isHf: false
          isPro: false
          name: deskk
          type: user
        html: '<p>hi, thank you for sharing all these tricks. may i ask are you still
          using the 8x v100 GPUs with 16gb in this case with DeepSpeed? Thanks!</p>

          '
        raw: hi, thank you for sharing all these tricks. may i ask are you still using
          the 8x v100 GPUs with 16gb in this case with DeepSpeed? Thanks!
        updatedAt: '2023-09-29T18:58:02.430Z'
      numEdits: 1
      reactions: []
    id: 65171dc216f0c073c5707fad
    type: comment
  author: deskk
  content: hi, thank you for sharing all these tricks. may i ask are you still using
    the 8x v100 GPUs with 16gb in this case with DeepSpeed? Thanks!
  created_at: 2023-09-29 17:56:02+00:00
  edited: true
  hidden: false
  id: 65171dc216f0c073c5707fad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Making ProtGPT2-medium and ProtGPT2-small available?
