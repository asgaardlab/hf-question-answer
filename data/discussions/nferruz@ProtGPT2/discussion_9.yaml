!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SweetGAN
conflicting_files: null
created_at: 2022-11-29 14:55:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a5cdbeb52cc6fafb118eaae8c581cfc.svg
      fullname: SweetGAN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SweetGAN
      type: user
    createdAt: '2022-11-29T14:55:06.000Z'
    data:
      edited: false
      editors:
      - SweetGAN
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a5cdbeb52cc6fafb118eaae8c581cfc.svg
          fullname: SweetGAN
          isHf: false
          isPro: false
          name: SweetGAN
          type: user
        html: '<p>Hi,<br>Many thanks for sharing ProtGPT2.<br>I want to fine-tune
          ProtGPT2 on my dataset. Based on your explanation here, I edited the sequences
          by swapping the header with "&lt;|endoftext|&gt;". After making the training
          and validation set, I ran the command: python run_clm.py --model_name_or_path
          nferruz/ProtGPT2 --train_file training.txt --validation_file validation.txt
          --tokenizer_name nferruz/ProtGPT2  --do_train --do_eval --output_dir output
          --learning_rate 1e-06<br>My first question is that by only doing this I
          got a CUDA out-of-memory error and I had to trim my train set (from 240
          sequences to 10). Could you please provide documentation on how to resolve
          this issue?<br>Second, after fine-tuning with the only sets of 10 and 4
          training and validation sequences respectively, in the output folder, there
          are several files. Can you explain what merges.txt contains? Because it
          is also including the letters from "&lt;|endoftext|&gt;".  As a next step,
          I have to use pytorch_model.bin in order to generate new sequences (something
          like a zero-shot fashion?).<br>And finally, can you maybe explain how we
          can get the optimized learning rate? Because with only this line of code
          I got confused.<br>At the very end, thank you in advance for your help,
          and sorry if my questions are naive (first-time user).</p>

          '
        raw: "Hi,\r\nMany thanks for sharing ProtGPT2.\r\nI want to fine-tune ProtGPT2\
          \ on my dataset. Based on your explanation here, I edited the sequences\
          \ by swapping the header with \"<|endoftext|>\". After making the training\
          \ and validation set, I ran the command: python run_clm.py --model_name_or_path\
          \ nferruz/ProtGPT2 --train_file training.txt --validation_file validation.txt\
          \ --tokenizer_name nferruz/ProtGPT2  --do_train --do_eval --output_dir output\
          \ --learning_rate 1e-06\r\nMy first question is that by only doing this\
          \ I got a CUDA out-of-memory error and I had to trim my train set (from\
          \ 240 sequences to 10). Could you please provide documentation on how to\
          \ resolve this issue? \r\nSecond, after fine-tuning with the only sets of\
          \ 10 and 4 training and validation sequences respectively, in the output\
          \ folder, there are several files. Can you explain what merges.txt contains?\
          \ Because it is also including the letters from \"<|endoftext|>\".  As a\
          \ next step, I have to use pytorch_model.bin in order to generate new sequences\
          \ (something like a zero-shot fashion?).\r\nAnd finally, can you maybe explain\
          \ how we can get the optimized learning rate? Because with only this line\
          \ of code I got confused. \r\nAt the very end, thank you in advance for\
          \ your help, and sorry if my questions are naive (first-time user).\r\n\r\
          \n"
        updatedAt: '2022-11-29T14:55:06.774Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - likun
    id: 63861d4ab3ef0523c63bb101
    type: comment
  author: SweetGAN
  content: "Hi,\r\nMany thanks for sharing ProtGPT2.\r\nI want to fine-tune ProtGPT2\
    \ on my dataset. Based on your explanation here, I edited the sequences by swapping\
    \ the header with \"<|endoftext|>\". After making the training and validation\
    \ set, I ran the command: python run_clm.py --model_name_or_path nferruz/ProtGPT2\
    \ --train_file training.txt --validation_file validation.txt --tokenizer_name\
    \ nferruz/ProtGPT2  --do_train --do_eval --output_dir output --learning_rate 1e-06\r\
    \nMy first question is that by only doing this I got a CUDA out-of-memory error\
    \ and I had to trim my train set (from 240 sequences to 10). Could you please\
    \ provide documentation on how to resolve this issue? \r\nSecond, after fine-tuning\
    \ with the only sets of 10 and 4 training and validation sequences respectively,\
    \ in the output folder, there are several files. Can you explain what merges.txt\
    \ contains? Because it is also including the letters from \"<|endoftext|>\". \
    \ As a next step, I have to use pytorch_model.bin in order to generate new sequences\
    \ (something like a zero-shot fashion?).\r\nAnd finally, can you maybe explain\
    \ how we can get the optimized learning rate? Because with only this line of code\
    \ I got confused. \r\nAt the very end, thank you in advance for your help, and\
    \ sorry if my questions are naive (first-time user).\r\n\r\n"
  created_at: 2022-11-29 14:55:06+00:00
  edited: false
  hidden: false
  id: 63861d4ab3ef0523c63bb101
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-12-01T22:00:16.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi SweetGAN,</p>\n<p>The CUDA OOM errors are very common when you\
          \ use large language models. What is your GPU if I may ask?<br>Usually,\
          \ during training we split the sequences into batches to avoid this, then\
          \ you feed the largest batch size that fits your card. What batch size did\
          \ you use? If you used the command above, the default batch size is 8. You\
          \ can decrease it to 4, or perhaps even 2 and 1 with the flag \"--per_device_train_batch_size\"\
          . This way you can still use the 240 sequences.</p>\n<p>The output files\
          \ you find after training correspond to the tokenizer and model weights.\
          \ The merges are part of the tokenization,  you have an explanation of this\
          \ file in this post: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/1083#issuecomment-524303077\"\
          >https://github.com/huggingface/transformers/issues/1083#issuecomment-524303077</a>.\
          \ The most important file in that folder is called pytorch_model.bin, it\
          \ contains the model weights, In practice, you don't need to do anything\
          \ in particular to that folder; pytorch_model.bin is indeed the model, but\
          \ the huggingface pipeline recognizes the folder and the files inside it\
          \ and generates from it directly. See the example below to directly load\
          \ tokenizer and model from a local folder:</p>\n<pre><code>    tokenizer\
          \ = AutoTokenizer.from_pretrained('/path/to/output')\n    model = GPT2LMHeadModel.from_pretrained('/path/to/output').to(device)\
          \ \n\n    outputs = model.generate(\n        input_ids, \n        top_k=950,\
          \ \n        repetition_penalty=1.2,\n        max_length=1024,\n        eos_token_id=0,\n\
          \           do_sample=True,\n           num_return_sequences=100)\n</code></pre>\n\
          <p>To optimize the learning rate, I would train using the command that you\
          \ posted above with different learning rates. Then, you should visualise\
          \ the training curves, i.e., during training you will get training and validation\
          \ loss in the output, extract them from the command log and plot them externally.\
          \ Both validation and training curves should descend until some point when\
          \ usually the validation loss starts increasing again.</p>\n<p>Thanks for\
          \ reaching out and let me know if you still have questions!<br>Noelia</p>\n"
        raw: "Hi SweetGAN,\n\nThe CUDA OOM errors are very common when you use large\
          \ language models. What is your GPU if I may ask?\nUsually, during training\
          \ we split the sequences into batches to avoid this, then you feed the largest\
          \ batch size that fits your card. What batch size did you use? If you used\
          \ the command above, the default batch size is 8. You can decrease it to\
          \ 4, or perhaps even 2 and 1 with the flag \"--per_device_train_batch_size\"\
          . This way you can still use the 240 sequences.\n\nThe output files you\
          \ find after training correspond to the tokenizer and model weights. The\
          \ merges are part of the tokenization,  you have an explanation of this\
          \ file in this post: https://github.com/huggingface/transformers/issues/1083#issuecomment-524303077.\
          \ The most important file in that folder is called pytorch_model.bin, it\
          \ contains the model weights, In practice, you don't need to do anything\
          \ in particular to that folder; pytorch_model.bin is indeed the model, but\
          \ the huggingface pipeline recognizes the folder and the files inside it\
          \ and generates from it directly. See the example below to directly load\
          \ tokenizer and model from a local folder:\n```\n    tokenizer = AutoTokenizer.from_pretrained('/path/to/output')\n\
          \    model = GPT2LMHeadModel.from_pretrained('/path/to/output').to(device)\
          \ \n\n    outputs = model.generate(\n        input_ids, \n    \ttop_k=950,\
          \ \n        repetition_penalty=1.2,\n        max_length=1024,\n        eos_token_id=0,\n\
          \   \t    do_sample=True,\n   \t    num_return_sequences=100)\n```\n\nTo\
          \ optimize the learning rate, I would train using the command that you posted\
          \ above with different learning rates. Then, you should visualise the training\
          \ curves, i.e., during training you will get training and validation loss\
          \ in the output, extract them from the command log and plot them externally.\
          \ Both validation and training curves should descend until some point when\
          \ usually the validation loss starts increasing again.\n\nThanks for reaching\
          \ out and let me know if you still have questions!\nNoelia"
        updatedAt: '2022-12-01T22:00:16.016Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - likun
        - SweetGAN
    id: 638923f0184615e463a96ae1
    type: comment
  author: nferruz
  content: "Hi SweetGAN,\n\nThe CUDA OOM errors are very common when you use large\
    \ language models. What is your GPU if I may ask?\nUsually, during training we\
    \ split the sequences into batches to avoid this, then you feed the largest batch\
    \ size that fits your card. What batch size did you use? If you used the command\
    \ above, the default batch size is 8. You can decrease it to 4, or perhaps even\
    \ 2 and 1 with the flag \"--per_device_train_batch_size\". This way you can still\
    \ use the 240 sequences.\n\nThe output files you find after training correspond\
    \ to the tokenizer and model weights. The merges are part of the tokenization,\
    \  you have an explanation of this file in this post: https://github.com/huggingface/transformers/issues/1083#issuecomment-524303077.\
    \ The most important file in that folder is called pytorch_model.bin, it contains\
    \ the model weights, In practice, you don't need to do anything in particular\
    \ to that folder; pytorch_model.bin is indeed the model, but the huggingface pipeline\
    \ recognizes the folder and the files inside it and generates from it directly.\
    \ See the example below to directly load tokenizer and model from a local folder:\n\
    ```\n    tokenizer = AutoTokenizer.from_pretrained('/path/to/output')\n    model\
    \ = GPT2LMHeadModel.from_pretrained('/path/to/output').to(device) \n\n    outputs\
    \ = model.generate(\n        input_ids, \n    \ttop_k=950, \n        repetition_penalty=1.2,\n\
    \        max_length=1024,\n        eos_token_id=0,\n   \t    do_sample=True,\n\
    \   \t    num_return_sequences=100)\n```\n\nTo optimize the learning rate, I would\
    \ train using the command that you posted above with different learning rates.\
    \ Then, you should visualise the training curves, i.e., during training you will\
    \ get training and validation loss in the output, extract them from the command\
    \ log and plot them externally. Both validation and training curves should descend\
    \ until some point when usually the validation loss starts increasing again.\n\
    \nThanks for reaching out and let me know if you still have questions!\nNoelia"
  created_at: 2022-12-01 22:00:16+00:00
  edited: false
  hidden: false
  id: 638923f0184615e463a96ae1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a5cdbeb52cc6fafb118eaae8c581cfc.svg
      fullname: SweetGAN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SweetGAN
      type: user
    createdAt: '2022-12-12T11:41:57.000Z'
    data:
      edited: false
      editors:
      - SweetGAN
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a5cdbeb52cc6fafb118eaae8c581cfc.svg
          fullname: SweetGAN
          isHf: false
          isPro: false
          name: SweetGAN
          type: user
        html: '<p>Dear Noelia,</p>

          <p>Many thanks for your comments. I did as you said and it worked. I found
          the best model with the optimized learning rate.<br>For sequence generation
          from scratch (zero-shot fashion), I used this block of code, and first,
          I want to be sure if this is correct: </p>

          <p>input_ids = tokenizer.encode("&lt;|endoftext|&gt;", return_tensors=''pt'').to(device)</p>

          <p>outputs = model.generate(<br>    input_ids,<br>    top_k=950,<br>    repetition_penalty=1.2,<br>    max_length=512,<br>    eos_token_id=0,<br>    do_sample=True,<br>    num_return_sequences=100)</p>

          <p>for seq in outputs:<br>    print(tokenizer.decode(seq, skip_special_tokens=True))</p>

          <hr>

          <p>Second, I have a question regarding the calculatePerplexity function
          that you provided. This function gets a tokenizer as input but is never
          used, and instead uses input_ids which is never created in the function.
          Can you please explain what should I use as the input_ids? Is it the same
          as the train data?</p>

          <p>Many thanks,<br>SweetGAN</p>

          '
        raw: "Dear Noelia,\n\nMany thanks for your comments. I did as you said and\
          \ it worked. I found the best model with the optimized learning rate.\n\
          For sequence generation from scratch (zero-shot fashion), I used this block\
          \ of code, and first, I want to be sure if this is correct: \n\ninput_ids\
          \ = tokenizer.encode(\"<|endoftext|>\", return_tensors='pt').to(device)\n\
          \noutputs = model.generate(\n    input_ids, \n    top_k=950, \n    repetition_penalty=1.2,\n\
          \    max_length=512,\n    eos_token_id=0,\n    do_sample=True,\n    num_return_sequences=100)\n\
          \nfor seq in outputs:\n    print(tokenizer.decode(seq, skip_special_tokens=True))\n\
          -------------------------\n\nSecond, I have a question regarding the calculatePerplexity\
          \ function that you provided. This function gets a tokenizer as input but\
          \ is never used, and instead uses input_ids which is never created in the\
          \ function. Can you please explain what should I use as the input_ids? Is\
          \ it the same as the train data?\n\nMany thanks,\nSweetGAN"
        updatedAt: '2022-12-12T11:41:57.555Z'
      numEdits: 0
      reactions: []
    id: 639713857dca3977ed0558de
    type: comment
  author: SweetGAN
  content: "Dear Noelia,\n\nMany thanks for your comments. I did as you said and it\
    \ worked. I found the best model with the optimized learning rate.\nFor sequence\
    \ generation from scratch (zero-shot fashion), I used this block of code, and\
    \ first, I want to be sure if this is correct: \n\ninput_ids = tokenizer.encode(\"\
    <|endoftext|>\", return_tensors='pt').to(device)\n\noutputs = model.generate(\n\
    \    input_ids, \n    top_k=950, \n    repetition_penalty=1.2,\n    max_length=512,\n\
    \    eos_token_id=0,\n    do_sample=True,\n    num_return_sequences=100)\n\nfor\
    \ seq in outputs:\n    print(tokenizer.decode(seq, skip_special_tokens=True))\n\
    -------------------------\n\nSecond, I have a question regarding the calculatePerplexity\
    \ function that you provided. This function gets a tokenizer as input but is never\
    \ used, and instead uses input_ids which is never created in the function. Can\
    \ you please explain what should I use as the input_ids? Is it the same as the\
    \ train data?\n\nMany thanks,\nSweetGAN"
  created_at: 2022-12-12 11:41:57+00:00
  edited: false
  hidden: false
  id: 639713857dca3977ed0558de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-12-12T11:49:23.000Z'
    data:
      edited: true
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi SweetGAN,</p>\n<p>Great to hear fine-tuning worked fine. The\
          \ code that you typed looks good.<br>Regarding the perplexity function,\
          \ you are right. See below this function which uses the tokenizer for a\
          \ given sequence:</p>\n<pre><code>def calculatePerplexity(sequence,model,tokenizer):\n\
          \    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0) \n\
          \    input_ids = input_ids.to(device)\n    with torch.no_grad():\n     \
          \   outputs = model(input_ids, labels=input_ids)\n    loss, logits = outputs[:2]\n\
          \    return math.exp(loss)\n\n\n\nsequence='LETHVLTCASAAYVARHGEPRHPRDLADGHRCVLIRDPVTGRPYGWEFHRGDEVVPFDATGRLTVN'\n\
          </code></pre>\n<p>Thanks for noticing. I'm updating the documentation.</p>\n"
        raw: "Hi SweetGAN,\n\nGreat to hear fine-tuning worked fine. The code that\
          \ you typed looks good.\nRegarding the perplexity function, you are right.\
          \ See below this function which uses the tokenizer for a given sequence:\n\
          \n\n```\ndef calculatePerplexity(sequence,model,tokenizer):\n    input_ids\
          \ = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0) \n    input_ids\
          \ = input_ids.to(device)\n    with torch.no_grad():\n        outputs = model(input_ids,\
          \ labels=input_ids)\n    loss, logits = outputs[:2]\n    return math.exp(loss)\n\
          \n\n\nsequence='LETHVLTCASAAYVARHGEPRHPRDLADGHRCVLIRDPVTGRPYGWEFHRGDEVVPFDATGRLTVN'\n\
          ```\n\nThanks for noticing. I'm updating the documentation."
        updatedAt: '2022-12-12T11:49:37.079Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - SweetGAN
    id: 6397154345c0ab898e5ebb29
    type: comment
  author: nferruz
  content: "Hi SweetGAN,\n\nGreat to hear fine-tuning worked fine. The code that you\
    \ typed looks good.\nRegarding the perplexity function, you are right. See below\
    \ this function which uses the tokenizer for a given sequence:\n\n\n```\ndef calculatePerplexity(sequence,model,tokenizer):\n\
    \    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0) \n    input_ids\
    \ = input_ids.to(device)\n    with torch.no_grad():\n        outputs = model(input_ids,\
    \ labels=input_ids)\n    loss, logits = outputs[:2]\n    return math.exp(loss)\n\
    \n\n\nsequence='LETHVLTCASAAYVARHGEPRHPRDLADGHRCVLIRDPVTGRPYGWEFHRGDEVVPFDATGRLTVN'\n\
    ```\n\nThanks for noticing. I'm updating the documentation."
  created_at: 2022-12-12 11:49:23+00:00
  edited: true
  hidden: false
  id: 6397154345c0ab898e5ebb29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a5cdbeb52cc6fafb118eaae8c581cfc.svg
      fullname: SweetGAN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SweetGAN
      type: user
    createdAt: '2022-12-15T09:57:21.000Z'
    data:
      edited: true
      editors:
      - SweetGAN
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a5cdbeb52cc6fafb118eaae8c581cfc.svg
          fullname: SweetGAN
          isHf: false
          isPro: false
          name: SweetGAN
          type: user
        html: '<p>Hi Noelia,<br>Thanks for updating the documentation.</p>

          '
        raw: 'Hi Noelia,

          Thanks for updating the documentation.'
        updatedAt: '2022-12-20T14:41:40.412Z'
      numEdits: 2
      reactions: []
    id: 639aef81dc88d6af3d1952be
    type: comment
  author: SweetGAN
  content: 'Hi Noelia,

    Thanks for updating the documentation.'
  created_at: 2022-12-15 09:57:21+00:00
  edited: true
  hidden: false
  id: 639aef81dc88d6af3d1952be
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: ' Finetuning ProtGPT2 on a set of user-defined sequences'
