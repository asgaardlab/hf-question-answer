!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hunarbatra
conflicting_files: null
created_at: 2022-08-15 03:21:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660273191881-noauth.jpeg?w=200&h=200&f=face
      fullname: Hunar Batra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hunarbatra
      type: user
    createdAt: '2022-08-15T04:21:17.000Z'
    data:
      edited: true
      editors:
      - hunarbatra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660273191881-noauth.jpeg?w=200&h=200&f=face
          fullname: Hunar Batra
          isHf: false
          isPro: false
          name: hunarbatra
          type: user
        html: '<p>Dear Authors,</p>

          <p>Thank you for the excellent work with ProtGPT2 and for making it available
          through HuggingFace :).</p>

          <p>I had an out-of-the-box question that I was trying to use ProtGPT2 for
          and it''ll be great to hear if you have any inputs over this:-<br>Is it
          possible to fine-tune ProtGPT2 for a mutation generation task (input-output
          pairs, more like question-answering in tasks terminology) where given a
          sequence it would return a mutated sequence. And I''ll fine-tune it on this
          kind of a dataset with input-output pairs of wild-type &amp; mutated sequence?<br>If
          yes, then could you please guide me with how this kind of input-output sequence
          mutation generation could be performed with ProtGPT2? </p>

          <p>Thank you so much :)</p>

          '
        raw: "Dear Authors,\n\nThank you for the excellent work with ProtGPT2 and\
          \ for making it available through HuggingFace :).\n\nI had an out-of-the-box\
          \ question that I was trying to use ProtGPT2 for and it'll be great to hear\
          \ if you have any inputs over this:-\nIs it possible to fine-tune ProtGPT2\
          \ for a mutation generation task (input-output pairs, more like question-answering\
          \ in tasks terminology) where given a sequence it would return a mutated\
          \ sequence. And I'll fine-tune it on this kind of a dataset with input-output\
          \ pairs of wild-type & mutated sequence?\nIf yes, then could you please\
          \ guide me with how this kind of input-output sequence mutation generation\
          \ could be performed with ProtGPT2? \n\nThank you so much :)"
        updatedAt: '2022-08-15T04:36:33.276Z'
      numEdits: 1
      reactions: []
    id: 62f9c9bdb4ba236ebf83ced1
    type: comment
  author: hunarbatra
  content: "Dear Authors,\n\nThank you for the excellent work with ProtGPT2 and for\
    \ making it available through HuggingFace :).\n\nI had an out-of-the-box question\
    \ that I was trying to use ProtGPT2 for and it'll be great to hear if you have\
    \ any inputs over this:-\nIs it possible to fine-tune ProtGPT2 for a mutation\
    \ generation task (input-output pairs, more like question-answering in tasks terminology)\
    \ where given a sequence it would return a mutated sequence. And I'll fine-tune\
    \ it on this kind of a dataset with input-output pairs of wild-type & mutated\
    \ sequence?\nIf yes, then could you please guide me with how this kind of input-output\
    \ sequence mutation generation could be performed with ProtGPT2? \n\nThank you\
    \ so much :)"
  created_at: 2022-08-15 03:21:17+00:00
  edited: true
  hidden: false
  id: 62f9c9bdb4ba236ebf83ced1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660273191881-noauth.jpeg?w=200&h=200&f=face
      fullname: Hunar Batra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hunarbatra
      type: user
    createdAt: '2022-08-15T06:38:07.000Z'
    data:
      edited: true
      editors:
      - hunarbatra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660273191881-noauth.jpeg?w=200&h=200&f=face
          fullname: Hunar Batra
          isHf: false
          isPro: false
          name: hunarbatra
          type: user
        html: '<p>I figured out how to do that (just how we do it to a regular GPT2
          model by adding [WILDTYPE]: seq..... \n [MUTATION]: seq......)<br>But I''m
          unable to do the finetuning on Google Colab hosted runtime (GPU) or  locally
          on my system (16gigs RAM / M1 mbp)<br>"RuntimeError: CUDA out of memory.
          Tried to allocate 160.00 MiB (GPU 0; 14.76 GiB total capacity; 13.64 GiB
          already allocated; 81.75 MiB free; 13.71 GiB reserved in total by PyTorch)
          If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"</p>

          <p>Is it possible to do the finetuning on Colab or with the Colab pro plan?
          Any clue?</p>

          '
        raw: 'I figured out how to do that (just how we do it to a regular GPT2 model
          by adding [WILDTYPE]: seq..... \n [MUTATION]: seq......)

          But I''m unable to do the finetuning on Google Colab hosted runtime (GPU)
          or  locally on my system (16gigs RAM / M1 mbp)

          "RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0;
          14.76 GiB total capacity; 13.64 GiB already allocated; 81.75 MiB free; 13.71
          GiB reserved in total by PyTorch) If reserved memory is >> allocated memory
          try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF"


          Is it possible to do the finetuning on Colab or with the Colab pro plan?
          Any clue?'
        updatedAt: '2022-08-15T06:41:37.422Z'
      numEdits: 1
      reactions: []
    id: 62f9e9cfefd2de85492a6f1a
    type: comment
  author: hunarbatra
  content: 'I figured out how to do that (just how we do it to a regular GPT2 model
    by adding [WILDTYPE]: seq..... \n [MUTATION]: seq......)

    But I''m unable to do the finetuning on Google Colab hosted runtime (GPU) or  locally
    on my system (16gigs RAM / M1 mbp)

    "RuntimeError: CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 14.76
    GiB total capacity; 13.64 GiB already allocated; 81.75 MiB free; 13.71 GiB reserved
    in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb
    to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"


    Is it possible to do the finetuning on Colab or with the Colab pro plan? Any clue?'
  created_at: 2022-08-15 05:38:07+00:00
  edited: true
  hidden: false
  id: 62f9e9cfefd2de85492a6f1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-08-16T14:15:45.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi hunarbatra!  </p>

          <p>Happy that you''re using ProtGPT2! It indeed seems that the model hits
          OOM on colab, I haven''t tried to load the model myself on colab, so no
          experience in this regard, but a few thoughts come to mind that will help
          with the memory issues. </p>

          <ol>

          <li><p>If you are not specifying --per_device_train_batch_size in your command,
          it possibly is using a batch size of 8. You could try a batch size of 1:  add
          the flag --per_device_train_batch_size 1 to your command. </p>

          </li>

          <li><p>If you are still getting that CUDA OOM error after this, I''d try
          gradient_checkpointing: <a href="https://huggingface.co/docs/transformers/main/en/performance#gradient-checkpointing">https://huggingface.co/docs/transformers/main/en/performance#gradient-checkpointing</a>.
          </p>

          </li>

          <li><p>If that still doesn''t fit into memory, I''d use mixed precision
          (passing the flag --fp16 to the Trainer).</p>

          </li>

          </ol>

          <p>I hope this helps, and please let me know if questions remain!<br>Noelia</p>

          '
        raw: "Hi hunarbatra!  \n\nHappy that you're using ProtGPT2! It indeed seems\
          \ that the model hits OOM on colab, I haven't tried to load the model myself\
          \ on colab, so no experience in this regard, but a few thoughts come to\
          \ mind that will help with the memory issues. \n\n1)  If you are not specifying\
          \ --per_device_train_batch_size in your command, it possibly is using a\
          \ batch size of 8. You could try a batch size of 1:  add the flag --per_device_train_batch_size\
          \ 1 to your command. \n\n2) If you are still getting that CUDA OOM error\
          \ after this, I'd try gradient_checkpointing: https://huggingface.co/docs/transformers/main/en/performance#gradient-checkpointing.\
          \ \n\n3) If that still doesn't fit into memory, I'd use mixed precision\
          \ (passing the flag --fp16 to the Trainer). \n\nI hope this helps, and please\
          \ let me know if questions remain!\nNoelia"
        updatedAt: '2022-08-16T14:15:45.777Z'
      numEdits: 0
      reactions: []
    id: 62fba6919af1d16bc0af1285
    type: comment
  author: nferruz
  content: "Hi hunarbatra!  \n\nHappy that you're using ProtGPT2! It indeed seems\
    \ that the model hits OOM on colab, I haven't tried to load the model myself on\
    \ colab, so no experience in this regard, but a few thoughts come to mind that\
    \ will help with the memory issues. \n\n1)  If you are not specifying --per_device_train_batch_size\
    \ in your command, it possibly is using a batch size of 8. You could try a batch\
    \ size of 1:  add the flag --per_device_train_batch_size 1 to your command. \n\
    \n2) If you are still getting that CUDA OOM error after this, I'd try gradient_checkpointing:\
    \ https://huggingface.co/docs/transformers/main/en/performance#gradient-checkpointing.\
    \ \n\n3) If that still doesn't fit into memory, I'd use mixed precision (passing\
    \ the flag --fp16 to the Trainer). \n\nI hope this helps, and please let me know\
    \ if questions remain!\nNoelia"
  created_at: 2022-08-16 13:15:45+00:00
  edited: false
  hidden: false
  id: 62fba6919af1d16bc0af1285
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660273191881-noauth.jpeg?w=200&h=200&f=face
      fullname: Hunar Batra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hunarbatra
      type: user
    createdAt: '2022-08-16T21:43:35.000Z'
    data:
      edited: false
      editors:
      - hunarbatra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660273191881-noauth.jpeg?w=200&h=200&f=face
          fullname: Hunar Batra
          isHf: false
          isPro: false
          name: hunarbatra
          type: user
        html: '<p>Thank you so much! :)</p>

          '
        raw: Thank you so much! :)
        updatedAt: '2022-08-16T21:43:35.306Z'
      numEdits: 0
      reactions: []
    id: 62fc0f879af1d16bc0b283ae
    type: comment
  author: hunarbatra
  content: Thank you so much! :)
  created_at: 2022-08-16 20:43:35+00:00
  edited: false
  hidden: false
  id: 62fc0f879af1d16bc0b283ae
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: ProtGPT2 QA Task for Mutation Generation
