!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flpgrz
conflicting_files: null
created_at: 2023-01-23 11:15:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e5e7776bede7e561dbee0223bf0197f.svg
      fullname: Filippo Grazioli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flpgrz
      type: user
    createdAt: '2023-01-23T11:15:12.000Z'
    data:
      edited: true
      editors:
      - flpgrz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e5e7776bede7e561dbee0223bf0197f.svg
          fullname: Filippo Grazioli
          isHf: false
          isPro: false
          name: flpgrz
          type: user
        html: '<p>Hello, </p>

          <p>Thanks for making this model available!</p>

          <p>I have been trying to embed sequences (of different lengths), by using
          the following code:</p>

          <p>inputs = tokenizer([''CASSPRAGGITDTQYF'', ''CASSLLQPFGTEAFF''], return_tensors="pt",
          padding=True)<br>outputs = model(**inputs, output_hidden_states=True)<br>embeddings
          = outputs.hidden_states[0]  #embedding before final fc layer</p>

          <p>The two exemplary sequences have a different length and give a different
          number of tokens. Hence, padding is needed (padding=True).</p>

          <p>However, I get the following error:<br>ValueError: Asking to pad but
          the tokenizer does not have a padding token.</p>

          <p>This makes me think that padding was not used at training time, as the
          tokenizer does  not have a padding token.<br>How did you concatenate proteins
          of different lengths to create a batch at training time without padding?</p>

          <p>Thanks for your help.</p>

          '
        raw: "Hello, \n\nThanks for making this model available!\n\nI have been trying\
          \ to embed sequences (of different lengths), by using the following code:\n\
          \ninputs = tokenizer(['CASSPRAGGITDTQYF', 'CASSLLQPFGTEAFF'], return_tensors=\"\
          pt\", padding=True)\noutputs = model(**inputs, output_hidden_states=True)\n\
          embeddings = outputs.hidden_states[0]  #embedding before final fc layer\n\
          \nThe two exemplary sequences have a different length and give a different\
          \ number of tokens. Hence, padding is needed (padding=True).\n\nHowever,\
          \ I get the following error:\nValueError: Asking to pad but the tokenizer\
          \ does not have a padding token.\n\nThis makes me think that padding was\
          \ not used at training time, as the tokenizer does  not have a padding token.\n\
          How did you concatenate proteins of different lengths to create a batch\
          \ at training time without padding?\n\nThanks for your help."
        updatedAt: '2023-01-23T11:21:55.989Z'
      numEdits: 3
      reactions: []
    id: 63ce6c403bdd5a9e83b9b227
    type: comment
  author: flpgrz
  content: "Hello, \n\nThanks for making this model available!\n\nI have been trying\
    \ to embed sequences (of different lengths), by using the following code:\n\n\
    inputs = tokenizer(['CASSPRAGGITDTQYF', 'CASSLLQPFGTEAFF'], return_tensors=\"\
    pt\", padding=True)\noutputs = model(**inputs, output_hidden_states=True)\nembeddings\
    \ = outputs.hidden_states[0]  #embedding before final fc layer\n\nThe two exemplary\
    \ sequences have a different length and give a different number of tokens. Hence,\
    \ padding is needed (padding=True).\n\nHowever, I get the following error:\nValueError:\
    \ Asking to pad but the tokenizer does not have a padding token.\n\nThis makes\
    \ me think that padding was not used at training time, as the tokenizer does \
    \ not have a padding token.\nHow did you concatenate proteins of different lengths\
    \ to create a batch at training time without padding?\n\nThanks for your help."
  created_at: 2023-01-23 11:15:12+00:00
  edited: true
  hidden: false
  id: 63ce6c403bdd5a9e83b9b227
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-01-23T12:38:36.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi flpgrz,</p>\n<p>I did not pad in ProtGTP2 because the sequences\
          \ were truncated across groups. This is something I did not like and modified\
          \ in ZymCTRL, which has a padding token.<br>In any case, I think you can\
          \ add a padding token on the fly; could you try this?</p>\n<pre><code>if\
          \ tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token':\
          \ '[PAD]'})\n</code></pre>\n"
        raw: "Hi flpgrz,\n\nI did not pad in ProtGTP2 because the sequences were truncated\
          \ across groups. This is something I did not like and modified in ZymCTRL,\
          \ which has a padding token.\nIn any case, I think you can add a padding\
          \ token on the fly; could you try this?\n```\nif tokenizer.pad_token is\
          \ None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n```"
        updatedAt: '2023-01-23T12:38:36.815Z'
      numEdits: 0
      reactions: []
    id: 63ce7fcc2a7f32cbc45b5847
    type: comment
  author: nferruz
  content: "Hi flpgrz,\n\nI did not pad in ProtGTP2 because the sequences were truncated\
    \ across groups. This is something I did not like and modified in ZymCTRL, which\
    \ has a padding token.\nIn any case, I think you can add a padding token on the\
    \ fly; could you try this?\n```\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token':\
    \ '[PAD]'})\n```"
  created_at: 2023-01-23 12:38:36+00:00
  edited: false
  hidden: false
  id: 63ce7fcc2a7f32cbc45b5847
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-01-23T12:39:51.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>This issue could also be useful: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/3021">https://github.com/huggingface/transformers/issues/3021</a></p>

          '
        raw: 'This issue could also be useful: https://github.com/huggingface/transformers/issues/3021'
        updatedAt: '2023-01-23T12:39:51.693Z'
      numEdits: 0
      reactions: []
    id: 63ce80178ce02f2ec8ad202e
    type: comment
  author: nferruz
  content: 'This issue could also be useful: https://github.com/huggingface/transformers/issues/3021'
  created_at: 2023-01-23 12:39:51+00:00
  edited: false
  hidden: false
  id: 63ce80178ce02f2ec8ad202e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3e5e7776bede7e561dbee0223bf0197f.svg
      fullname: Filippo Grazioli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flpgrz
      type: user
    createdAt: '2023-01-23T12:42:30.000Z'
    data:
      edited: false
      editors:
      - flpgrz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3e5e7776bede7e561dbee0223bf0197f.svg
          fullname: Filippo Grazioli
          isHf: false
          isPro: false
          name: flpgrz
          type: user
        html: '<p>I understand. Thanks for clarifying.</p>

          <p>I might be wrong, but I think adding the padding token in the tokeniser
          step might not work, because the model does not know how to process it.
          But I should first try.</p>

          <p>What I did so far is to embed one sequence at a time and do 0-padding
          afterwards to account for the different lengths</p>

          '
        raw: 'I understand. Thanks for clarifying.


          I might be wrong, but I think adding the padding token in the tokeniser
          step might not work, because the model does not know how to process it.
          But I should first try.


          What I did so far is to embed one sequence at a time and do 0-padding afterwards
          to account for the different lengths'
        updatedAt: '2023-01-23T12:42:30.144Z'
      numEdits: 0
      reactions: []
    id: 63ce80b62a7f32cbc45b6a62
    type: comment
  author: flpgrz
  content: 'I understand. Thanks for clarifying.


    I might be wrong, but I think adding the padding token in the tokeniser step might
    not work, because the model does not know how to process it. But I should first
    try.


    What I did so far is to embed one sequence at a time and do 0-padding afterwards
    to account for the different lengths'
  created_at: 2023-01-23 12:42:30+00:00
  edited: false
  hidden: false
  id: 63ce80b62a7f32cbc45b6a62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-01-23T12:43:51.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Yes, I think you are right. I think they talk about this issue in
          the GitHub issue I sent: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/3021">https://github.com/huggingface/transformers/issues/3021</a><br>But
          I haven''t tested it myself. Let me know if it works!</p>

          '
        raw: 'Yes, I think you are right. I think they talk about this issue in the
          GitHub issue I sent: https://github.com/huggingface/transformers/issues/3021

          But I haven''t tested it myself. Let me know if it works!'
        updatedAt: '2023-01-23T12:43:51.382Z'
      numEdits: 0
      reactions: []
    id: 63ce81078ce02f2ec8ad3332
    type: comment
  author: nferruz
  content: 'Yes, I think you are right. I think they talk about this issue in the
    GitHub issue I sent: https://github.com/huggingface/transformers/issues/3021

    But I haven''t tested it myself. Let me know if it works!'
  created_at: 2023-01-23 12:43:51+00:00
  edited: false
  hidden: false
  id: 63ce81078ce02f2ec8ad3332
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Embedding sequences
