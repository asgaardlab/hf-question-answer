!!python/object:huggingface_hub.community.DiscussionWithDetails
author: littleworth
conflicting_files: null
created_at: 2023-12-15 07:59:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
      fullname: E. LIttleworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: littleworth
      type: user
    createdAt: '2023-12-15T07:59:23.000Z'
    data:
      edited: true
      editors:
      - littleworth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9189077019691467
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e4676ae022d6a6697da2b4ef9b2687b.svg
          fullname: E. LIttleworth
          isHf: false
          isPro: false
          name: littleworth
          type: user
        html: '<p>Hi Noelia,</p>

          <p>I''m I''d like to thank you with your work on ProtGPT2 and its application
          in protein design.<br>It''s a notable contribution to the field.</p>

          <p>I am currently engaged in a project where I am utilizing ProtGPT2, and  I
          have a couple of queries:</p>

          <ol>

          <li><p>Could you share the accuracy and loss metrics during both the training
          and evaluation phases of ProtGPT2? This information is vital for me to benchmark
          my fine-tuning results against the baseline performance of the model.</p>

          </li>

          <li><p>Following the guidelines provided on the <a href="https://huggingface.co/nferruz/ProtGPT2">Hugging
          Face model page</a>, I have fine-tuned the ProtGPT2 model for a specific
          task. To ensure the quality of the fine-tuning, could you recommend methodologies
          to effectively evaluate the fine-tuned model, focusing on assessing for
          ''catastrophic forgetting'', ensuring it retains some of its prior knowledge?</p>

          </li>

          </ol>

          <p>Thank you for your time and consideration. I look forward to your response.</p>

          <p>Sincerely,<br>LW</p>

          '
        raw: "Hi Noelia,\n\nI'm I'd like to thank you with your work on ProtGPT2 and\
          \ its application in protein design. \nIt's a notable contribution to the\
          \ field.\n\nI am currently engaged in a project where I am utilizing ProtGPT2,\
          \ and  I have a couple of queries:\n\n1. Could you share the accuracy and\
          \ loss metrics during both the training and evaluation phases of ProtGPT2?\
          \ This information is vital for me to benchmark my fine-tuning results against\
          \ the baseline performance of the model.\n\n2. Following the guidelines\
          \ provided on the [Hugging Face model page](https://huggingface.co/nferruz/ProtGPT2),\
          \ I have fine-tuned the ProtGPT2 model for a specific task. To ensure the\
          \ quality of the fine-tuning, could you recommend methodologies to effectively\
          \ evaluate the fine-tuned model, focusing on assessing for 'catastrophic\
          \ forgetting', ensuring it retains some of its prior knowledge?\n\nThank\
          \ you for your time and consideration. I look forward to your response.\n\
          \nSincerely,\nLW"
        updatedAt: '2023-12-15T08:17:25.850Z'
      numEdits: 2
      reactions: []
    id: 657c075baa07c6e014e0b282
    type: comment
  author: littleworth
  content: "Hi Noelia,\n\nI'm I'd like to thank you with your work on ProtGPT2 and\
    \ its application in protein design. \nIt's a notable contribution to the field.\n\
    \nI am currently engaged in a project where I am utilizing ProtGPT2, and  I have\
    \ a couple of queries:\n\n1. Could you share the accuracy and loss metrics during\
    \ both the training and evaluation phases of ProtGPT2? This information is vital\
    \ for me to benchmark my fine-tuning results against the baseline performance\
    \ of the model.\n\n2. Following the guidelines provided on the [Hugging Face model\
    \ page](https://huggingface.co/nferruz/ProtGPT2), I have fine-tuned the ProtGPT2\
    \ model for a specific task. To ensure the quality of the fine-tuning, could you\
    \ recommend methodologies to effectively evaluate the fine-tuned model, focusing\
    \ on assessing for 'catastrophic forgetting', ensuring it retains some of its\
    \ prior knowledge?\n\nThank you for your time and consideration. I look forward\
    \ to your response.\n\nSincerely,\nLW"
  created_at: 2023-12-15 07:59:23+00:00
  edited: true
  hidden: false
  id: 657c075baa07c6e014e0b282
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 38
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Inquiry on ProtGPT2 Model Performance and Fine-Tuning Evaluation
