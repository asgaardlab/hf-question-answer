!!python/object:huggingface_hub.community.DiscussionWithDetails
author: avuhong
conflicting_files: null
created_at: 2022-08-16 13:00:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11c421aff0fd4175e1460a7b25f6d1ac.svg
      fullname: Ai Vu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avuhong
      type: user
    createdAt: '2022-08-16T14:00:16.000Z'
    data:
      edited: false
      editors:
      - avuhong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11c421aff0fd4175e1460a7b25f6d1ac.svg
          fullname: Ai Vu
          isHf: false
          isPro: false
          name: avuhong
          type: user
        html: '<p>Hi,</p>

          <p>Thank you for sharing your awesome work. I wonder this model can be used
          for reliably estimating the designed sequences'' perplexity like with its
          parental GPT2 model? If yes, can you please give me some hints how to do
          it? As shown in this thread, take average loss from the whole sequence seems
          suboptimal (<a href="https://huggingface.co/docs/transformers/perplexity">https://huggingface.co/docs/transformers/perplexity</a>)</p>

          <p>Bests,<br>Ai </p>

          '
        raw: "Hi,\r\n\r\nThank you for sharing your awesome work. I wonder this model\
          \ can be used for reliably estimating the designed sequences' perplexity\
          \ like with its parental GPT2 model? If yes, can you please give me some\
          \ hints how to do it? As shown in this thread, take average loss from the\
          \ whole sequence seems suboptimal (https://huggingface.co/docs/transformers/perplexity)\r\
          \n\r\nBests,\r\nAi "
        updatedAt: '2022-08-16T14:00:16.314Z'
      numEdits: 0
      reactions: []
    id: 62fba2f0fcce44435d8056db
    type: comment
  author: avuhong
  content: "Hi,\r\n\r\nThank you for sharing your awesome work. I wonder this model\
    \ can be used for reliably estimating the designed sequences' perplexity like\
    \ with its parental GPT2 model? If yes, can you please give me some hints how\
    \ to do it? As shown in this thread, take average loss from the whole sequence\
    \ seems suboptimal (https://huggingface.co/docs/transformers/perplexity)\r\n\r\
    \nBests,\r\nAi "
  created_at: 2022-08-16 13:00:16+00:00
  edited: false
  hidden: false
  id: 62fba2f0fcce44435d8056db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-08-16T14:23:17.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: "<p>Hi avuhong! </p>\n<p>Thanks for reaching out! Yes, you can compute\
          \ perplexity for each sequence separately and in fact, I'd recommend so\
          \ as a good way to evaluate them. </p>\n<p>You can easily do that with the\
          \ following HuggingFace package: <a href=\"https://huggingface.co/spaces/evaluate-measurement/perplexity\"\
          >https://huggingface.co/spaces/evaluate-measurement/perplexity</a>:</p>\n\
          <pre><code>from evaluate import load\nperplexity = load(\"perplexity\",\
          \  module_type= \"measurement\")\nresults = perplexity.compute(data=input_texts,\
          \ model_id=' nferruz/ProtGPT2')\n</code></pre>\n<p>As you will see from\
          \ the link, you can also pass several sequences to the computation with\
          \ a list (copied from the link above):</p>\n<pre><code>perplexity = evaluate.load(\"\
          perplexity\", module_type=\"measurement\")\ninput_texts = [\"lorem ipsum\"\
          , \"Happy Birthday!\", \"Bienvenue\"]\nresults = perplexity.compute(model_id='gpt2',\n\
          \                             add_start_token=False,\n                 \
          \            data=input_texts)\nprint(list(results.keys()))\n&gt;&gt;&gt;['perplexities',\
          \ 'mean_perplexity']\nprint(round(results[\"mean_perplexity\"], 2))\n&gt;&gt;&gt;646.74\n\
          print(round(results[\"perplexities\"][0], 2))\n&gt;&gt;&gt;32.25\n</code></pre>\n\
          <p>I hope this helps let me know if there are further questions!</p>\n"
        raw: "Hi avuhong! \n\nThanks for reaching out! Yes, you can compute perplexity\
          \ for each sequence separately and in fact, I'd recommend so as a good way\
          \ to evaluate them. \n\nYou can easily do that with the following HuggingFace\
          \ package: https://huggingface.co/spaces/evaluate-measurement/perplexity:\n\
          \n```\nfrom evaluate import load\nperplexity = load(\"perplexity\",  module_type=\
          \ \"measurement\")\nresults = perplexity.compute(data=input_texts, model_id='\
          \ nferruz/ProtGPT2')\n```\nAs you will see from the link, you can also pass\
          \ several sequences to the computation with a list (copied from the link\
          \ above):\n\n```\nperplexity = evaluate.load(\"perplexity\", module_type=\"\
          measurement\")\ninput_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"\
          Bienvenue\"]\nresults = perplexity.compute(model_id='gpt2',\n          \
          \                   add_start_token=False,\n                           \
          \  data=input_texts)\nprint(list(results.keys()))\n>>>['perplexities', 'mean_perplexity']\n\
          print(round(results[\"mean_perplexity\"], 2))\n>>>646.74\nprint(round(results[\"\
          perplexities\"][0], 2))\n>>>32.25\n```\n\nI hope this helps let me know\
          \ if there are further questions!"
        updatedAt: '2022-08-16T14:23:17.144Z'
      numEdits: 0
      reactions: []
    id: 62fba8559af1d16bc0af2203
    type: comment
  author: nferruz
  content: "Hi avuhong! \n\nThanks for reaching out! Yes, you can compute perplexity\
    \ for each sequence separately and in fact, I'd recommend so as a good way to\
    \ evaluate them. \n\nYou can easily do that with the following HuggingFace package:\
    \ https://huggingface.co/spaces/evaluate-measurement/perplexity:\n\n```\nfrom\
    \ evaluate import load\nperplexity = load(\"perplexity\",  module_type= \"measurement\"\
    )\nresults = perplexity.compute(data=input_texts, model_id=' nferruz/ProtGPT2')\n\
    ```\nAs you will see from the link, you can also pass several sequences to the\
    \ computation with a list (copied from the link above):\n\n```\nperplexity = evaluate.load(\"\
    perplexity\", module_type=\"measurement\")\ninput_texts = [\"lorem ipsum\", \"\
    Happy Birthday!\", \"Bienvenue\"]\nresults = perplexity.compute(model_id='gpt2',\n\
    \                             add_start_token=False,\n                       \
    \      data=input_texts)\nprint(list(results.keys()))\n>>>['perplexities', 'mean_perplexity']\n\
    print(round(results[\"mean_perplexity\"], 2))\n>>>646.74\nprint(round(results[\"\
    perplexities\"][0], 2))\n>>>32.25\n```\n\nI hope this helps let me know if there\
    \ are further questions!"
  created_at: 2022-08-16 13:23:17+00:00
  edited: false
  hidden: false
  id: 62fba8559af1d16bc0af2203
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2022-08-16T14:24:09.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Ah! You can also find the code here: <a href="https://huggingface.co/spaces/evaluate-metric/perplexity/blob/main/perplexity.py">https://huggingface.co/spaces/evaluate-metric/perplexity/blob/main/perplexity.py</a></p>

          '
        raw: 'Ah! You can also find the code here: https://huggingface.co/spaces/evaluate-metric/perplexity/blob/main/perplexity.py'
        updatedAt: '2022-08-16T14:24:09.605Z'
      numEdits: 0
      reactions: []
    id: 62fba8897a577e83cccbfb02
    type: comment
  author: nferruz
  content: 'Ah! You can also find the code here: https://huggingface.co/spaces/evaluate-metric/perplexity/blob/main/perplexity.py'
  created_at: 2022-08-16 13:24:09+00:00
  edited: false
  hidden: false
  id: 62fba8897a577e83cccbfb02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11c421aff0fd4175e1460a7b25f6d1ac.svg
      fullname: Ai Vu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avuhong
      type: user
    createdAt: '2022-08-16T14:49:13.000Z'
    data:
      edited: false
      editors:
      - avuhong
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11c421aff0fd4175e1460a7b25f6d1ac.svg
          fullname: Ai Vu
          isHf: false
          isPro: false
          name: avuhong
          type: user
        html: '<p>That''s perfect, thank you very much ;) </p>

          '
        raw: 'That''s perfect, thank you very much ;) '
        updatedAt: '2022-08-16T14:49:13.302Z'
      numEdits: 0
      reactions: []
      relatedEventId: 62fbae69610dae1bcd04a20b
    id: 62fbae69610dae1bcd04a20a
    type: comment
  author: avuhong
  content: 'That''s perfect, thank you very much ;) '
  created_at: 2022-08-16 13:49:13+00:00
  edited: false
  hidden: false
  id: 62fbae69610dae1bcd04a20a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/11c421aff0fd4175e1460a7b25f6d1ac.svg
      fullname: Ai Vu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: avuhong
      type: user
    createdAt: '2022-08-16T14:49:13.000Z'
    data:
      status: closed
    id: 62fbae69610dae1bcd04a20b
    type: status-change
  author: avuhong
  created_at: 2022-08-16 13:49:13+00:00
  id: 62fbae69610dae1bcd04a20b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: nferruz/ProtGPT2
repo_type: model
status: closed
target_branch: null
title: Use model for calculating sequence perplexity
