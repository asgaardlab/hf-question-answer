!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kevinky
conflicting_files: null
created_at: 2023-01-26 11:07:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/44a8956fb99720c963d579ec63b82db3.svg
      fullname: Kevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kevinky
      type: user
    createdAt: '2023-01-26T11:07:36.000Z'
    data:
      edited: false
      editors:
      - kevinky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/44a8956fb99720c963d579ec63b82db3.svg
          fullname: Kevin
          isHf: false
          isPro: false
          name: kevinky
          type: user
        html: '<p>Dear Noelia,</p>

          <p>Many thanks for this important work and open sourcing it.</p>

          <p>I have two simple questions about generating sequences using ProtGPT2:</p>

          <ol>

          <li>About the timing, by simply using the example case:</li>

          </ol>

          <p>protgpt2 = pipeline(''text-generation'', model=''#path to local protgpt2'')<br>sequences
          = protgpt2("&lt;|endoftext|&gt;", max_length=100, do_sample=True, top_k=950,
          repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)</p>

          <p>It takes 62 seconds for me in jupyter notebook with a GPU NVIDIA TITAN
          Xp. As you described, the generation should be very fast with a GPU. I am
          wondering if the codes above assign the GPU to do the generation.</p>

          <ol start="2">

          <li>As you described in other issue panels (<a href="https://huggingface.co/nferruz/ProtGPT2/discussions/6">https://huggingface.co/nferruz/ProtGPT2/discussions/6</a>),
          the generation process will generate sequences with &lt;|endoftext|&gt;
          as the final token. I randomly run the generation codes above several times
          but don''t see a sequence with that kind of final token yet, and want to
          confirm if this is normal.</li>

          </ol>

          <p>Many thanks,<br>Kevin</p>

          '
        raw: "Dear Noelia,\r\n\r\nMany thanks for this important work and open sourcing\
          \ it.\r\n\r\nI have two simple questions about generating sequences using\
          \ ProtGPT2:\r\n\r\n1. About the timing, by simply using the example case:\r\
          \n\r\nprotgpt2 = pipeline('text-generation', model='#path to local protgpt2')\r\
          \nsequences = protgpt2(\"<|endoftext|>\", max_length=100, do_sample=True,\
          \ top_k=950, repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)\r\
          \n\r\nIt takes 62 seconds for me in jupyter notebook with a GPU NVIDIA TITAN\
          \ Xp. As you described, the generation should be very fast with a GPU. I\
          \ am wondering if the codes above assign the GPU to do the generation.\r\
          \n\r\n2. As you described in other issue panels (https://huggingface.co/nferruz/ProtGPT2/discussions/6),\
          \ the generation process will generate sequences with <|endoftext|> as the\
          \ final token. I randomly run the generation codes above several times but\
          \ don't see a sequence with that kind of final token yet, and want to confirm\
          \ if this is normal.\r\n\r\nMany thanks,\r\nKevin"
        updatedAt: '2023-01-26T11:07:36.949Z'
      numEdits: 0
      reactions: []
    id: 63d25ef88700bc77c8e7b56a
    type: comment
  author: kevinky
  content: "Dear Noelia,\r\n\r\nMany thanks for this important work and open sourcing\
    \ it.\r\n\r\nI have two simple questions about generating sequences using ProtGPT2:\r\
    \n\r\n1. About the timing, by simply using the example case:\r\n\r\nprotgpt2 =\
    \ pipeline('text-generation', model='#path to local protgpt2')\r\nsequences =\
    \ protgpt2(\"<|endoftext|>\", max_length=100, do_sample=True, top_k=950, repetition_penalty=1.2,\
    \ num_return_sequences=10, eos_token_id=0)\r\n\r\nIt takes 62 seconds for me in\
    \ jupyter notebook with a GPU NVIDIA TITAN Xp. As you described, the generation\
    \ should be very fast with a GPU. I am wondering if the codes above assign the\
    \ GPU to do the generation.\r\n\r\n2. As you described in other issue panels (https://huggingface.co/nferruz/ProtGPT2/discussions/6),\
    \ the generation process will generate sequences with <|endoftext|> as the final\
    \ token. I randomly run the generation codes above several times but don't see\
    \ a sequence with that kind of final token yet, and want to confirm if this is\
    \ normal.\r\n\r\nMany thanks,\r\nKevin"
  created_at: 2023-01-26 11:07:36+00:00
  edited: false
  hidden: false
  id: 63d25ef88700bc77c8e7b56a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
      fullname: Noelia Ferruz
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: nferruz
      type: user
    createdAt: '2023-01-29T22:05:54.000Z'
    data:
      edited: false
      editors:
      - nferruz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d53ce612a2ca69b758fd3fe0e962d3cc.svg
          fullname: Noelia Ferruz
          isHf: false
          isPro: false
          name: nferruz
          type: user
        html: '<p>Hi Kevin,</p>

          <p>Thanks a lot for your message. It sounds like the code isn''t properly
          utilizing the GPU (maybe). You could force it to use the GPU by explicitly
          setting the model''s device.<br>For example:</p>

          <pre><code>input = "M"

          device = torch.device(''cuda'')

          tokenizer = AutoTokenizer.from_pretrained(''path-to-the-model'') #&nbsp;or
          nferruz/ProtGTP2

          model = GPT2LMHeadModel.from_pretrained(''path-to-the-model'').to(device)
          #&nbsp;here is where you define the device

          input_ids = tokenizer.encode(input,return_tensors=''pt'').to(device)

          output = model.generate(input_ids,  max_length=100, do_sample=True, top_k=950,
          repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)

          print(output)

          </code></pre>

          <p>The speed will also depend on the GPU. For example, with an A100 I can
          generate more than 100 sequences in less than a minute, but with other GPUs,
          I have to lower that threshold significantly.  Another thing that would
          speed up generation significantly would be the datatype. I haven''t explored
          this, but a few colleagues mentioned using b16 speeds up inference by several
          orders of magnitude.</p>

          <p>For your second point, yes, sequences are separated with the &lt;|endoftext|&gt;
          token. It should also be the padding token. So if you look at the generated
          outputs, their last token should be 0 (token for &lt;|endoftext|&gt;). It
          could be, however, that your threshold length is too short, and the model
          is truncating them before an &lt;|endoftext|&gt;  token is produced.<br>Also,
          during decoding, ensure that the <code>skip_special_tokens</code>parameter
          is not set to True. This way, the 0 token gets decoded as &lt;|endoftext|&gt;.
          When it is set to true, the decoder considers it special and skips it so
          you wouldn''t see it. </p>

          <p>Let me know if this helps!<br>Noelia</p>

          '
        raw: "Hi Kevin,\n\nThanks a lot for your message. It sounds like the code\
          \ isn't properly utilizing the GPU (maybe). You could force it to use the\
          \ GPU by explicitly setting the model's device.\nFor example:\n```\ninput\
          \ = \"M\"\ndevice = torch.device('cuda')\ntokenizer = AutoTokenizer.from_pretrained('path-to-the-model')\
          \ #\_or nferruz/ProtGTP2\nmodel = GPT2LMHeadModel.from_pretrained('path-to-the-model').to(device)\
          \ #\_here is where you define the device\ninput_ids = tokenizer.encode(input,return_tensors='pt').to(device)\n\
          output = model.generate(input_ids,  max_length=100, do_sample=True, top_k=950,\
          \ repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)\nprint(output)\n\
          ```\nThe speed will also depend on the GPU. For example, with an A100 I\
          \ can generate more than 100 sequences in less than a minute, but with other\
          \ GPUs, I have to lower that threshold significantly.  Another thing that\
          \ would speed up generation significantly would be the datatype. I haven't\
          \ explored this, but a few colleagues mentioned using b16 speeds up inference\
          \ by several orders of magnitude.\n\nFor your second point, yes, sequences\
          \ are separated with the <|endoftext|> token. It should also be the padding\
          \ token. So if you look at the generated outputs, their last token should\
          \ be 0 (token for <|endoftext|>). It could be, however, that your threshold\
          \ length is too short, and the model is truncating them before an <|endoftext|>\
          \  token is produced. \nAlso, during decoding, ensure that the `skip_special_tokens`parameter\
          \ is not set to True. This way, the 0 token gets decoded as <|endoftext|>.\
          \ When it is set to true, the decoder considers it special and skips it\
          \ so you wouldn't see it. \n\nLet me know if this helps!\nNoelia"
        updatedAt: '2023-01-29T22:05:54.087Z'
      numEdits: 0
      reactions: []
    id: 63d6edc20a79149d1f3edfa0
    type: comment
  author: nferruz
  content: "Hi Kevin,\n\nThanks a lot for your message. It sounds like the code isn't\
    \ properly utilizing the GPU (maybe). You could force it to use the GPU by explicitly\
    \ setting the model's device.\nFor example:\n```\ninput = \"M\"\ndevice = torch.device('cuda')\n\
    tokenizer = AutoTokenizer.from_pretrained('path-to-the-model') #\_or nferruz/ProtGTP2\n\
    model = GPT2LMHeadModel.from_pretrained('path-to-the-model').to(device) #\_here\
    \ is where you define the device\ninput_ids = tokenizer.encode(input,return_tensors='pt').to(device)\n\
    output = model.generate(input_ids,  max_length=100, do_sample=True, top_k=950,\
    \ repetition_penalty=1.2, num_return_sequences=10, eos_token_id=0)\nprint(output)\n\
    ```\nThe speed will also depend on the GPU. For example, with an A100 I can generate\
    \ more than 100 sequences in less than a minute, but with other GPUs, I have to\
    \ lower that threshold significantly.  Another thing that would speed up generation\
    \ significantly would be the datatype. I haven't explored this, but a few colleagues\
    \ mentioned using b16 speeds up inference by several orders of magnitude.\n\n\
    For your second point, yes, sequences are separated with the <|endoftext|> token.\
    \ It should also be the padding token. So if you look at the generated outputs,\
    \ their last token should be 0 (token for <|endoftext|>). It could be, however,\
    \ that your threshold length is too short, and the model is truncating them before\
    \ an <|endoftext|>  token is produced. \nAlso, during decoding, ensure that the\
    \ `skip_special_tokens`parameter is not set to True. This way, the 0 token gets\
    \ decoded as <|endoftext|>. When it is set to true, the decoder considers it special\
    \ and skips it so you wouldn't see it. \n\nLet me know if this helps!\nNoelia"
  created_at: 2023-01-29 22:05:54+00:00
  edited: false
  hidden: false
  id: 63d6edc20a79149d1f3edfa0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: nferruz/ProtGPT2
repo_type: model
status: open
target_branch: null
title: Basic sequence generation usage
