!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lskywalker
conflicting_files: null
created_at: 2023-11-02 21:49:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f2c1273771b38ae3199a47a775f3a8b.svg
      fullname: Luca Pietrobon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lskywalker
      type: user
    createdAt: '2023-11-02T22:49:47.000Z'
    data:
      edited: false
      editors:
      - lskywalker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.909263014793396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f2c1273771b38ae3199a47a775f3a8b.svg
          fullname: Luca Pietrobon
          isHf: false
          isPro: false
          name: lskywalker
          type: user
        html: '<p>Thanks for making this!<br>I was wondering if you can save the model
          into smaller shards before pushing it to the hub. This is helpful for those
          who (like me, yes ^^ ) don''t have access to large compute resources and
          want to check out things on Colab first.</p>

          <p>I recently came across abhishek''s setup here<br><a href="https://huggingface.co/abhishek/llama-2-7b-hf-small-shards/tree/main">https://huggingface.co/abhishek/llama-2-7b-hf-small-shards/tree/main</a><br>where
          there is a llama2-7b model sharded across 10 files. This means I can load
          it in Colab (using 4bit) without going out of system memory<br><a rel="nofollow"
          href="https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb">https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb</a><br>..which
          is nice :)</p>

          <p>I don''t think it''s a lot of work to make smaller shards, provided you
          can load the model in the first place. I think something like this would
          do it</p>

          <pre><code>model = AutoModelForCausalLM.from_pretrained(....)

          tokenizer = AutoTokenizer.from_pretrained(...)

          model.save_pretrained(path, max_shard_size="3GB")

          tokenizer.save_pretrained(path)

          </code></pre>

          '
        raw: "Thanks for making this!\r\nI was wondering if you can save the model\
          \ into smaller shards before pushing it to the hub. This is helpful for\
          \ those who (like me, yes ^^ ) don't have access to large compute resources\
          \ and want to check out things on Colab first.\r\n\r\nI recently came across\
          \ abhishek's setup here \r\nhttps://huggingface.co/abhishek/llama-2-7b-hf-small-shards/tree/main\
          \ \r\nwhere there is a llama2-7b model sharded across 10 files. This means\
          \ I can load it in Colab (using 4bit) without going out of system memory\r\
          \nhttps://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb\r\
          \n..which is nice :)\r\n\r\nI don't think it's a lot of work to make smaller\
          \ shards, provided you can load the model in the first place. I think something\
          \ like this would do it\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(....)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(...)\r\nmodel.save_pretrained(path,\
          \ max_shard_size=\"3GB\")\r\ntokenizer.save_pretrained(path)\r\n```"
        updatedAt: '2023-11-02T22:49:47.475Z'
      numEdits: 0
      reactions: []
    id: 6544278b0df73a1682832aac
    type: comment
  author: lskywalker
  content: "Thanks for making this!\r\nI was wondering if you can save the model into\
    \ smaller shards before pushing it to the hub. This is helpful for those who (like\
    \ me, yes ^^ ) don't have access to large compute resources and want to check\
    \ out things on Colab first.\r\n\r\nI recently came across abhishek's setup here\
    \ \r\nhttps://huggingface.co/abhishek/llama-2-7b-hf-small-shards/tree/main \r\n\
    where there is a llama2-7b model sharded across 10 files. This means I can load\
    \ it in Colab (using 4bit) without going out of system memory\r\nhttps://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_LLM.ipynb\r\
    \n..which is nice :)\r\n\r\nI don't think it's a lot of work to make smaller shards,\
    \ provided you can load the model in the first place. I think something like this\
    \ would do it\r\n```\r\nmodel = AutoModelForCausalLM.from_pretrained(....)\r\n\
    tokenizer = AutoTokenizer.from_pretrained(...)\r\nmodel.save_pretrained(path,\
    \ max_shard_size=\"3GB\")\r\ntokenizer.save_pretrained(path)\r\n```"
  created_at: 2023-11-02 21:49:47+00:00
  edited: false
  hidden: false
  id: 6544278b0df73a1682832aac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NousResearch/Yarn-Mistral-7b-128k
repo_type: model
status: open
target_branch: null
title: smaller shards, pls
