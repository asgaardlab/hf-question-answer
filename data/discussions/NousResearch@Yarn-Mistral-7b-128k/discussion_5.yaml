!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hypersniper
conflicting_files: null
created_at: 2023-11-03 14:48:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
      fullname: Hypersniper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hypersniper
      type: user
    createdAt: '2023-11-03T15:48:32.000Z'
    data:
      edited: false
      editors:
      - Hypersniper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9468884468078613
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
          fullname: Hypersniper
          isHf: false
          isPro: false
          name: Hypersniper
          type: user
        html: '<p>Any idea how much more vram you''ll need to get the full 128k if
          you load the model in 4bit? </p>

          '
        raw: 'Any idea how much more vram you''ll need to get the full 128k if you
          load the model in 4bit? '
        updatedAt: '2023-11-03T15:48:32.976Z'
      numEdits: 0
      reactions: []
    id: 654516507ed90fc4715969da
    type: comment
  author: Hypersniper
  content: 'Any idea how much more vram you''ll need to get the full 128k if you load
    the model in 4bit? '
  created_at: 2023-11-03 14:48:32+00:00
  edited: false
  hidden: false
  id: 654516507ed90fc4715969da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2023-11-03T20:41:02.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-11-03T21:39:05.788Z'
      numEdits: 1
      reactions: []
    id: 65455ade6507968374292851
    type: comment
  author: macadeliccc
  content: This comment has been hidden
  created_at: 2023-11-03 19:41:02+00:00
  edited: true
  hidden: true
  id: 65455ade6507968374292851
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
      fullname: Bowen Peng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bloc97
      type: user
    createdAt: '2023-11-03T20:45:46.000Z'
    data:
      edited: false
      editors:
      - bloc97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9797146916389465
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
          fullname: Bowen Peng
          isHf: false
          isPro: false
          name: bloc97
          type: user
        html: '<p>Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently
          working on reducing this with better optimizations...</p>

          '
        raw: Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently
          working on reducing this with better optimizations...
        updatedAt: '2023-11-03T20:45:46.226Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - msze
        - hamnaanaa
        - DemonMaike
        - TanvirOnHF
        - richardlian
    id: 65455bfa2a2a483042cd4583
    type: comment
  author: bloc97
  content: Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently working
    on reducing this with better optimizations...
  created_at: 2023-11-03 19:45:46+00:00
  edited: false
  hidden: false
  id: 65455bfa2a2a483042cd4583
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4f07bf25144aa344ed250f9ed2ce3af.svg
      fullname: benny
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 3blackbar
      type: user
    createdAt: '2023-11-04T13:12:51.000Z'
    data:
      edited: false
      editors:
      - 3blackbar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9185636043548584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4f07bf25144aa344ed250f9ed2ce3af.svg
          fullname: benny
          isHf: false
          isPro: false
          name: 3blackbar
          type: user
        html: '<p>no need, ill start my delorean to travel to 2030 and get some gpus
          to run it locally</p>

          '
        raw: no need, ill start my delorean to travel to 2030 and get some gpus to
          run it locally
        updatedAt: '2023-11-04T13:12:51.102Z'
      numEdits: 0
      reactions:
      - count: 10
        reaction: "\U0001F917"
        users:
        - maywell
        - dim
        - DemonMaike
        - mrlundmark
        - w-a-cat
        - aleposada7
        - adedayoniyi
        - pszemraj
        - Raspbfox
        - TanvirOnHF
    id: 654643535cd5692b3aa26c7b
    type: comment
  author: 3blackbar
  content: no need, ill start my delorean to travel to 2030 and get some gpus to run
    it locally
  created_at: 2023-11-04 12:12:51+00:00
  edited: false
  hidden: false
  id: 654643535cd5692b3aa26c7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/M57emMqveu7urIqjGt4Fd.png?w=200&h=200&f=face
      fullname: Dmitry Malyshev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DemonMaike
      type: user
    createdAt: '2023-11-05T22:41:45.000Z'
    data:
      edited: false
      editors:
      - DemonMaike
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9318355917930603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/M57emMqveu7urIqjGt4Fd.png?w=200&h=200&f=face
          fullname: Dmitry Malyshev
          isHf: false
          isPro: false
          name: DemonMaike
          type: user
        html: '<p>Hi and thanks for your work, this is amazing.<br>But Could you help
          me with my question?<br>So, i small work with ai and models, and i trust
          look for information about multicards output for model, but i seen only
          info about finetune using multicard.<br>Maybe, you can take me links or
          some documentation about it.</p>

          '
        raw: 'Hi and thanks for your work, this is amazing.

          But Could you help me with my question?

          So, i small work with ai and models, and i trust look for information about
          multicards output for model, but i seen only info about finetune using multicard.

          Maybe, you can take me links or some documentation about it.'
        updatedAt: '2023-11-05T22:41:45.590Z'
      numEdits: 0
      reactions: []
    id: 65481a29565e3985e8c7517d
    type: comment
  author: DemonMaike
  content: 'Hi and thanks for your work, this is amazing.

    But Could you help me with my question?

    So, i small work with ai and models, and i trust look for information about multicards
    output for model, but i seen only info about finetune using multicard.

    Maybe, you can take me links or some documentation about it.'
  created_at: 2023-11-05 22:41:45+00:00
  edited: false
  hidden: false
  id: 65481a29565e3985e8c7517d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
      fullname: Hypersniper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hypersniper
      type: user
    createdAt: '2023-11-06T14:30:28.000Z'
    data:
      edited: false
      editors:
      - Hypersniper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.972164511680603
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b229669d21227b914badbb/umr0ngvZ5L_Nv2nVlC-Zo.png?w=200&h=200&f=face
          fullname: Hypersniper
          isHf: false
          isPro: false
          name: Hypersniper
          type: user
        html: '<blockquote>

          <p>Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently
          working on reducing this with better optimizations...</p>

          </blockquote>

          <p>Perhaps using flash attention could help</p>

          '
        raw: '> Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently
          working on reducing this with better optimizations...


          Perhaps using flash attention could help'
        updatedAt: '2023-11-06T14:30:28.579Z'
      numEdits: 0
      reactions: []
    id: 6548f884b4c5d07feb8a12ae
    type: comment
  author: Hypersniper
  content: '> Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently
    working on reducing this with better optimizations...


    Perhaps using flash attention could help'
  created_at: 2023-11-06 14:30:28+00:00
  edited: false
  hidden: false
  id: 6548f884b4c5d07feb8a12ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
      fullname: Bowen Peng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bloc97
      type: user
    createdAt: '2023-11-06T19:07:05.000Z'
    data:
      edited: true
      editors:
      - bloc97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9339426159858704
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
          fullname: Bowen Peng
          isHf: false
          isPro: false
          name: bloc97
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently
          working on reducing this with better optimizations...</p>

          </blockquote>

          <p>Perhaps using flash attention could help</p>

          </blockquote>

          <p>It is already using flash attention. However if you are focused solely
          on inference use cases, dedicated inference kernels in libraries such as
          <a rel="nofollow" href="https://github.com/vllm-project/vllm">vLLM</a>,
          <a rel="nofollow" href="https://github.com/turboderp/exllamav2">ExLlama</a>
          and <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">llamacpp</a>
          would help reduce the VRAM requirements significantly. I''ve heard rumours
          that it should be possible to run 128k context with llamacpp on a single
          40GB GPU...</p>

          '
        raw: "> > Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently\
          \ working on reducing this with better optimizations...\n> \n> Perhaps using\
          \ flash attention could help\n\nIt is already using flash attention. However\
          \ if you are focused solely on inference use cases, dedicated inference\
          \ kernels in libraries such as [vLLM](https://github.com/vllm-project/vllm),\
          \ [ExLlama](https://github.com/turboderp/exllamav2) and [llamacpp](https://github.com/ggerganov/llama.cpp)\
          \ would help reduce the VRAM requirements significantly. I've heard rumours\
          \ that it should be possible to run 128k context with llamacpp on a single\
          \ 40GB GPU..."
        updatedAt: '2023-11-06T19:07:19.511Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\U0001F917"
        users:
        - DemonMaike
        - troykirin
        - conceptron
        - TanvirOnHF
        - mangologic
    id: 65493959a46359771471b128
    type: comment
  author: bloc97
  content: "> > Currently, it is about 4 80GB A100s, so 320GB VRAM, we are currently\
    \ working on reducing this with better optimizations...\n> \n> Perhaps using flash\
    \ attention could help\n\nIt is already using flash attention. However if you\
    \ are focused solely on inference use cases, dedicated inference kernels in libraries\
    \ such as [vLLM](https://github.com/vllm-project/vllm), [ExLlama](https://github.com/turboderp/exllamav2)\
    \ and [llamacpp](https://github.com/ggerganov/llama.cpp) would help reduce the\
    \ VRAM requirements significantly. I've heard rumours that it should be possible\
    \ to run 128k context with llamacpp on a single 40GB GPU..."
  created_at: 2023-11-06 19:07:05+00:00
  edited: true
  hidden: false
  id: 65493959a46359771471b128
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: NousResearch/Yarn-Mistral-7b-128k
repo_type: model
status: open
target_branch: null
title: VRAM usage for full 128k tokens
