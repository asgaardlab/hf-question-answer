!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MohamedRashad
conflicting_files: null
created_at: 2023-11-03 08:52:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
      fullname: Mohamed Rashad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedRashad
      type: user
    createdAt: '2023-11-03T09:52:06.000Z'
    data:
      edited: false
      editors:
      - MohamedRashad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9494427442550659
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
          fullname: Mohamed Rashad
          isHf: false
          isPro: false
          name: MohamedRashad
          type: user
        html: "<p>I have faced this problem with llama-2-7B-32k where it continues\
          \ producing text until the max number of tokens is reached.<br>Is there\
          \ a solution for this problem ?\n </p>\n"
        raw: "I have faced this problem with llama-2-7B-32k where it continues producing\
          \ text until the max number of tokens is reached.\r\nIs there a solution\
          \ for this problem ?\r\n "
        updatedAt: '2023-11-03T09:52:06.808Z'
      numEdits: 0
      reactions: []
    id: 6544c2c6276a59f14305c917
    type: comment
  author: MohamedRashad
  content: "I have faced this problem with llama-2-7B-32k where it continues producing\
    \ text until the max number of tokens is reached.\r\nIs there a solution for this\
    \ problem ?\r\n "
  created_at: 2023-11-03 08:52:06+00:00
  edited: false
  hidden: false
  id: 6544c2c6276a59f14305c917
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2023-11-03T15:41:58.000Z'
    data:
      edited: false
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6472037434577942
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: '<p>Set the EOS token to the corresponding value in the vocabulary</p>

          '
        raw: Set the EOS token to the corresponding value in the vocabulary
        updatedAt: '2023-11-03T15:41:58.858Z'
      numEdits: 0
      reactions: []
    id: 654514c6a156b3b1d144d966
    type: comment
  author: macadeliccc
  content: Set the EOS token to the corresponding value in the vocabulary
  created_at: 2023-11-03 14:41:58+00:00
  edited: false
  hidden: false
  id: 654514c6a156b3b1d144d966
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
      fullname: Mohamed Rashad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedRashad
      type: user
    createdAt: '2023-11-03T17:49:57.000Z'
    data:
      edited: false
      editors:
      - MohamedRashad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5279690027236938
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
          fullname: Mohamed Rashad
          isHf: false
          isPro: false
          name: MohamedRashad
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;macadeliccc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/macadeliccc\"\
          >@<span class=\"underline\">macadeliccc</span></a></span>\n\n\t</span></span><br>Can\
          \ you give me a code example ?</p>\n"
        raw: "@macadeliccc \nCan you give me a code example ?"
        updatedAt: '2023-11-03T17:49:57.051Z'
      numEdits: 0
      reactions: []
    id: 654532c5cd0a562139e4b1b6
    type: comment
  author: MohamedRashad
  content: "@macadeliccc \nCan you give me a code example ?"
  created_at: 2023-11-03 16:49:57+00:00
  edited: false
  hidden: false
  id: 654532c5cd0a562139e4b1b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
      fullname: Tim Dolan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: macadeliccc
      type: user
    createdAt: '2023-11-03T18:48:47.000Z'
    data:
      edited: false
      editors:
      - macadeliccc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6535049080848694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/NcB1yDz0ZBtXXFiApnFyl.png?w=200&h=200&f=face
          fullname: Tim Dolan
          isHf: false
          isPro: false
          name: macadeliccc
          type: user
        html: "<p><a href=\"https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.eos_token\"\
          >https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.eos_token</a>\
          \ </p>\n<p>This is the example from amazon/MistralLite: </p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          <span class=\"hljs-keyword\">import</span> transformers\n<span class=\"\
          hljs-keyword\">import</span> torch\n\nmodel_id = <span class=\"hljs-string\"\
          >\"amazon/MistralLite\"</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          model = AutoModelForCausalLM.from_pretrained(model_id,\n               \
          \                              torch_dtype=torch.bfloat16,\n           \
          \                                  use_flash_attention_2=<span class=\"\
          hljs-literal\">True</span>,\n                                          \
          \   device_map=<span class=\"hljs-string\">\"auto\"</span>,)\npipeline =\
          \ transformers.pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n)\nprompt = <span\
          \ class=\"hljs-string\">\"&lt;|prompter|&gt;What are the main challenges\
          \ to support a long context for LLM?&lt;/s&gt;&lt;|assistant|&gt;\"</span>\n\
          \nsequences = pipeline(\n    prompt,\n    max_new_tokens=<span class=\"\
          hljs-number\">400</span>,\n    do_sample=<span class=\"hljs-literal\">False</span>,\n\
          \    return_full_text=<span class=\"hljs-literal\">False</span>,\n    num_return_sequences=<span\
          \ class=\"hljs-number\">1</span>,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\n<span class=\"hljs-keyword\">for</span> seq <span class=\"hljs-keyword\"\
          >in</span> sequences:\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{seq[<span class=\"\
          hljs-string\">'generated_text'</span>]}</span>\"</span>)\n</code></pre>\n\
          <p>This line can be referenced like this:</p>\n<pre><code> eos_token_id=tokenizer.eos_token_id,\n\
          </code></pre>\n<p>Or like this: </p>\n<pre><code> eos_token_id=32101,\n\
          </code></pre>\n<p>The number i selected is arbitrary I just wanted to show\
          \ you that its referencing the index of the vocabulary.</p>\n<p>Given that\
          \ this is a mistral ft I think this should suffice. Regardless, this is\
          \ the logic that stops the sentence and prevents run-on generation and can\
          \ be found in most/all text generation models.</p>\n"
        raw: "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.eos_token\
          \ \n\nThis is the example from amazon/MistralLite: \n\n```python\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\nimport transformers\nimport\
          \ torch\n\nmodel_id = \"amazon/MistralLite\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          model = AutoModelForCausalLM.from_pretrained(model_id,\n               \
          \                              torch_dtype=torch.bfloat16,\n           \
          \                                  use_flash_attention_2=True,\n       \
          \                                      device_map=\"auto\",)\npipeline =\
          \ transformers.pipeline(\n    \"text-generation\",\n    model=model,\n \
          \   tokenizer=tokenizer,\n)\nprompt = \"<|prompter|>What are the main challenges\
          \ to support a long context for LLM?</s><|assistant|>\"\n\nsequences = pipeline(\n\
          \    prompt,\n    max_new_tokens=400,\n    do_sample=False,\n    return_full_text=False,\n\
          \    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n```\n\
          This line can be referenced like this:\n \n```\n eos_token_id=tokenizer.eos_token_id,\n\
          ```\nOr like this: \n\n```\n eos_token_id=32101,\n```\nThe number i selected\
          \ is arbitrary I just wanted to show you that its referencing the index\
          \ of the vocabulary.\n\nGiven that this is a mistral ft I think this should\
          \ suffice. Regardless, this is the logic that stops the sentence and prevents\
          \ run-on generation and can be found in most/all text generation models."
        updatedAt: '2023-11-03T18:48:47.976Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MohamedRashad
    id: 6545408f9295970f87340618
    type: comment
  author: macadeliccc
  content: "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.eos_token\
    \ \n\nThis is the example from amazon/MistralLite: \n\n```python\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\nimport transformers\nimport torch\n\
    \nmodel_id = \"amazon/MistralLite\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
    model = AutoModelForCausalLM.from_pretrained(model_id,\n                     \
    \                        torch_dtype=torch.bfloat16,\n                       \
    \                      use_flash_attention_2=True,\n                         \
    \                    device_map=\"auto\",)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\nprompt\
    \ = \"<|prompter|>What are the main challenges to support a long context for LLM?</s><|assistant|>\"\
    \n\nsequences = pipeline(\n    prompt,\n    max_new_tokens=400,\n    do_sample=False,\n\
    \    return_full_text=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n```\nThis line\
    \ can be referenced like this:\n \n```\n eos_token_id=tokenizer.eos_token_id,\n\
    ```\nOr like this: \n\n```\n eos_token_id=32101,\n```\nThe number i selected is\
    \ arbitrary I just wanted to show you that its referencing the index of the vocabulary.\n\
    \nGiven that this is a mistral ft I think this should suffice. Regardless, this\
    \ is the logic that stops the sentence and prevents run-on generation and can\
    \ be found in most/all text generation models."
  created_at: 2023-11-03 17:48:47+00:00
  edited: false
  hidden: false
  id: 6545408f9295970f87340618
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
      fullname: Mohamed Rashad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedRashad
      type: user
    createdAt: '2023-11-04T06:03:58.000Z'
    data:
      edited: false
      editors:
      - MohamedRashad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9585961103439331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
          fullname: Mohamed Rashad
          isHf: false
          isPro: false
          name: MohamedRashad
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;macadeliccc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/macadeliccc\"\
          >@<span class=\"underline\">macadeliccc</span></a></span>\n\n\t</span></span>\
          \ It worked thanks</p>\n"
        raw: '@macadeliccc It worked thanks'
        updatedAt: '2023-11-04T06:03:58.619Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6545dece565e3985e863aa72
    id: 6545dece565e3985e863aa70
    type: comment
  author: MohamedRashad
  content: '@macadeliccc It worked thanks'
  created_at: 2023-11-04 05:03:58+00:00
  edited: false
  hidden: false
  id: 6545dece565e3985e863aa70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628885133347-6116d0584ef9fdfbf45dc4d9.jpeg?w=200&h=200&f=face
      fullname: Mohamed Rashad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedRashad
      type: user
    createdAt: '2023-11-04T06:03:58.000Z'
    data:
      status: closed
    id: 6545dece565e3985e863aa72
    type: status-change
  author: MohamedRashad
  created_at: 2023-11-04 05:03:58+00:00
  id: 6545dece565e3985e863aa72
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: NousResearch/Yarn-Mistral-7b-128k
repo_type: model
status: closed
target_branch: null
title: It doesn't stop generating text.
