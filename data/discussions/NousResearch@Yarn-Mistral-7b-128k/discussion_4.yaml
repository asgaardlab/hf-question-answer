!!python/object:huggingface_hub.community.DiscussionWithDetails
author: keyishen
conflicting_files: null
created_at: 2023-11-03 11:08:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7dbe43753e546de16a44c53f65fb8be.svg
      fullname: keyi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: keyishen
      type: user
    createdAt: '2023-11-03T12:08:43.000Z'
    data:
      edited: false
      editors:
      - keyishen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7060973644256592
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7dbe43753e546de16a44c53f65fb8be.svg
          fullname: keyi
          isHf: false
          isPro: false
          name: keyishen
          type: user
        html: '<p>Does it mean yarn-mistral  version discards the sliding window attention
          used in Mistral-7B?</p>

          '
        raw: Does it mean yarn-mistral  version discards the sliding window attention
          used in Mistral-7B?
        updatedAt: '2023-11-03T12:08:43.429Z'
      numEdits: 0
      reactions: []
    id: 6544e2cb2881e19360c7ddf3
    type: comment
  author: keyishen
  content: Does it mean yarn-mistral  version discards the sliding window attention
    used in Mistral-7B?
  created_at: 2023-11-03 11:08:43+00:00
  edited: false
  hidden: false
  id: 6544e2cb2881e19360c7ddf3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/e7dbe43753e546de16a44c53f65fb8be.svg
      fullname: keyi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: keyishen
      type: user
    createdAt: '2023-11-03T17:53:24.000Z'
    data:
      from: sliding_window = 131072?
      to: sliding_window = 131072? Sliding window attention doesn't work for 128?
    id: 654533948767484a05e4c062
    type: title-change
  author: keyishen
  created_at: 2023-11-03 16:53:24+00:00
  id: 654533948767484a05e4c062
  new_title: sliding_window = 131072? Sliding window attention doesn't work for 128?
  old_title: sliding_window = 131072?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
      fullname: Bowen Peng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bloc97
      type: user
    createdAt: '2023-11-03T20:47:24.000Z'
    data:
      edited: true
      editors:
      - bloc97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8954713940620422
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cf262026c94b143173ef65/FFcC-XhG5hMjApSvfk_he.png?w=200&h=200&f=face
          fullname: Bowen Peng
          isHf: false
          isPro: false
          name: bloc97
          type: user
        html: '<p>Sliding window does work, but in order to take advantage of the
          full 128k context, you should set it to 128k. Smaller windows will lower
          VRAM requirements but will degrade PPL and reduce the context size that
          the model can retrieve information from.</p>

          '
        raw: Sliding window does work, but in order to take advantage of the full
          128k context, you should set it to 128k. Smaller windows will lower VRAM
          requirements but will degrade PPL and reduce the context size that the model
          can retrieve information from.
        updatedAt: '2023-11-03T20:54:06.725Z'
      numEdits: 3
      reactions: []
    id: 65455c5c7e0f913218dc37da
    type: comment
  author: bloc97
  content: Sliding window does work, but in order to take advantage of the full 128k
    context, you should set it to 128k. Smaller windows will lower VRAM requirements
    but will degrade PPL and reduce the context size that the model can retrieve information
    from.
  created_at: 2023-11-03 19:47:24+00:00
  edited: true
  hidden: false
  id: 65455c5c7e0f913218dc37da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: NousResearch/Yarn-Mistral-7b-128k
repo_type: model
status: open
target_branch: null
title: sliding_window = 131072? Sliding window attention doesn't work for 128?
