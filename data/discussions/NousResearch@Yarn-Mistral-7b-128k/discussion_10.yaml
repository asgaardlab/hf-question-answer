!!python/object:huggingface_hub.community.DiscussionWithDetails
author: haltux
conflicting_files: null
created_at: 2023-11-08 17:29:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2355a09bb61d5e8bc800f5c873e950e1.svg
      fullname: Julien Carme
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: haltux
      type: user
    createdAt: '2023-11-08T17:29:47.000Z'
    data:
      edited: false
      editors:
      - haltux
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7076253890991211
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2355a09bb61d5e8bc800f5c873e950e1.svg
          fullname: Julien Carme
          isHf: false
          isPro: false
          name: haltux
          type: user
        html: "<p>Hello,</p>\n<p>I could make the model work directly in Python with\
          \ the example code provided, but with VLLM it does not work, even with 80\
          \ GB of GPU memory.</p>\n<p>I try:</p>\n<pre><code>python -u -m vllm.entrypoints.openai.api_server\
          \ --host 0.0.0.0 --model NousResearch/Yarn-Mistral-7b-128k --trust-remote-code\n\
          </code></pre>\n<p>and i get:</p>\n<pre><code>  File \"/home/azureuser/.local/lib/python3.10/site-packages/vllm/model_executor/layers/attention.py\"\
          , line 266, in forward\n    self.multi_query_kv_attention(\n  File \"/home/azureuser/.local/lib/python3.10/site-packages/vllm/model_executor/layers/attention.py\"\
          , line 117, in multi_query_kv_attention\n    key = torch.repeat_interleave(key,\
          \ self.num_queries_per_kv, dim=1)\nRuntimeError: CUDA error: an illegal\
          \ memory access was encountered\nCUDA kernel errors might be asynchronously\
          \ reported at some other API call, so the stacktrace below might be incorrect.\n\
          For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA`\
          \ to enable device-side assertions.\n</code></pre>\n<p>Any idea about how\
          \ to fix that?</p>\n<p>Thanks a lot.</p>\n"
        raw: "Hello,\r\n\r\nI could make the model work directly in Python with the\
          \ example code provided, but with VLLM it does not work, even with 80 GB\
          \ of GPU memory.\r\n\r\nI try:\r\n```\r\npython -u -m vllm.entrypoints.openai.api_server\
          \ --host 0.0.0.0 --model NousResearch/Yarn-Mistral-7b-128k --trust-remote-code\r\
          \n\r\n```\r\n\r\nand i get:\r\n``` \r\n  File \"/home/azureuser/.local/lib/python3.10/site-packages/vllm/model_executor/layers/attention.py\"\
          , line 266, in forward\r\n    self.multi_query_kv_attention(\r\n  File \"\
          /home/azureuser/.local/lib/python3.10/site-packages/vllm/model_executor/layers/attention.py\"\
          , line 117, in multi_query_kv_attention\r\n    key = torch.repeat_interleave(key,\
          \ self.num_queries_per_kv, dim=1)\r\nRuntimeError: CUDA error: an illegal\
          \ memory access was encountered\r\nCUDA kernel errors might be asynchronously\
          \ reported at some other API call, so the stacktrace below might be incorrect.\r\
          \nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with\
          \ `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\n\r\n\
          Any idea about how to fix that?\r\n\r\nThanks a lot.\r\n"
        updatedAt: '2023-11-08T17:29:47.818Z'
      numEdits: 0
      reactions: []
    id: 654bc58bbac6e6e4987a04e1
    type: comment
  author: haltux
  content: "Hello,\r\n\r\nI could make the model work directly in Python with the\
    \ example code provided, but with VLLM it does not work, even with 80 GB of GPU\
    \ memory.\r\n\r\nI try:\r\n```\r\npython -u -m vllm.entrypoints.openai.api_server\
    \ --host 0.0.0.0 --model NousResearch/Yarn-Mistral-7b-128k --trust-remote-code\r\
    \n\r\n```\r\n\r\nand i get:\r\n``` \r\n  File \"/home/azureuser/.local/lib/python3.10/site-packages/vllm/model_executor/layers/attention.py\"\
    , line 266, in forward\r\n    self.multi_query_kv_attention(\r\n  File \"/home/azureuser/.local/lib/python3.10/site-packages/vllm/model_executor/layers/attention.py\"\
    , line 117, in multi_query_kv_attention\r\n    key = torch.repeat_interleave(key,\
    \ self.num_queries_per_kv, dim=1)\r\nRuntimeError: CUDA error: an illegal memory\
    \ access was encountered\r\nCUDA kernel errors might be asynchronously reported\
    \ at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging\
    \ consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA`\
    \ to enable device-side assertions.\r\n```\r\n\r\nAny idea about how to fix that?\r\
    \n\r\nThanks a lot.\r\n"
  created_at: 2023-11-08 17:29:47+00:00
  edited: false
  hidden: false
  id: 654bc58bbac6e6e4987a04e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/61512f51bdf316b15d32389fe8f14fcb.svg
      fullname: Ann-Katrin Thebille
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anione
      type: user
    createdAt: '2023-11-27T16:18:06.000Z'
    data:
      edited: false
      editors:
      - anione
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9908117651939392
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/61512f51bdf316b15d32389fe8f14fcb.svg
          fullname: Ann-Katrin Thebille
          isHf: false
          isPro: false
          name: anione
          type: user
        html: '<p>vLLM v0.2.2 was just released, which includes Yarn support, so maybe
          it will work for you now! </p>

          '
        raw: 'vLLM v0.2.2 was just released, which includes Yarn support, so maybe
          it will work for you now! '
        updatedAt: '2023-11-27T16:18:06.110Z'
      numEdits: 0
      reactions: []
    id: 6564c13e605c2f29e829823e
    type: comment
  author: anione
  content: 'vLLM v0.2.2 was just released, which includes Yarn support, so maybe it
    will work for you now! '
  created_at: 2023-11-27 16:18:06+00:00
  edited: false
  hidden: false
  id: 6564c13e605c2f29e829823e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: NousResearch/Yarn-Mistral-7b-128k
repo_type: model
status: open
target_branch: null
title: Using this model with Vllm
