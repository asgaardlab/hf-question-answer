!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ont
conflicting_files: null
created_at: 2024-01-04 06:07:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/320243a2588740e3ad886cebb082098c.svg
      fullname: Ontario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ont
      type: user
    createdAt: '2024-01-04T06:07:10.000Z'
    data:
      edited: true
      editors:
      - Ont
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8767113089561462
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/320243a2588740e3ad886cebb082098c.svg
          fullname: Ontario
          isHf: false
          isPro: false
          name: Ont
          type: user
        html: '<p>With llama.cpp "--chatml", I noticed a response intermixed with
          strings of "/******/", a list of words related to the response intermixed
          with strings of "/:******/", and finally after "###########", the response
          again as normal text.</p>

          <p>Investigating further, although the model card states that this model
          used a dataset following the ChatML template, this model lacks the ChatML
          tokens that llama.cpp supports. For comparison, OpenHermes was trained with
          an expanded "vocab_size" of 32002 and an "added_tokens.json" with<br>  {
          "&lt;|im_end|&gt;": 32000, "&lt;|im_start|&gt;": 32001 }</p>

          '
        raw: "With llama.cpp \"--chatml\", I noticed a response intermixed with strings\
          \ of \"/\\*\\*\\*\\*\\*\\*/\", a list of words related to the response intermixed\
          \ with strings of \"/:******/\", and finally after \"###########\", the\
          \ response again as normal text.\n\nInvestigating further, although the\
          \ model card states that this model used a dataset following the ChatML\
          \ template, this model lacks the ChatML tokens that llama.cpp supports.\
          \ For comparison, OpenHermes was trained with an expanded \"vocab_size\"\
          \ of 32002 and an \"added_tokens.json\" with\n  { \"<|im_end|>\": 32000,\
          \ \"<|im_start|>\": 32001 }"
        updatedAt: '2024-01-04T06:07:59.425Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoafyLemon
    id: 65964b0e099754be6053de47
    type: comment
  author: Ont
  content: "With llama.cpp \"--chatml\", I noticed a response intermixed with strings\
    \ of \"/\\*\\*\\*\\*\\*\\*/\", a list of words related to the response intermixed\
    \ with strings of \"/:******/\", and finally after \"###########\", the response\
    \ again as normal text.\n\nInvestigating further, although the model card states\
    \ that this model used a dataset following the ChatML template, this model lacks\
    \ the ChatML tokens that llama.cpp supports. For comparison, OpenHermes was trained\
    \ with an expanded \"vocab_size\" of 32002 and an \"added_tokens.json\" with\n\
    \  { \"<|im_end|>\": 32000, \"<|im_start|>\": 32001 }"
  created_at: 2024-01-04 06:07:10+00:00
  edited: true
  hidden: false
  id: 65964b0e099754be6053de47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/245ec3b183919c079f8c5023b3f7ca9f.svg
      fullname: CultriX
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: CultriX
      type: user
    createdAt: '2024-01-05T03:15:10.000Z'
    data:
      edited: false
      editors:
      - CultriX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7234906554222107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/245ec3b183919c079f8c5023b3f7ca9f.svg
          fullname: CultriX
          isHf: false
          isPro: false
          name: CultriX
          type: user
        html: "<p>OpenHermes-2.5-Mistral-7B uses a specific chat template, called\
          \ ChatML. Here is an example of a conversation formatted with this template:</p>\n\
          <p>&lt;|im_start|&gt;system<br>You are a helpful chatbot assistant.&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>Hi&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>Hi,\
          \ how can I help you?&lt;|im_end|&gt;<br>As you can see, ChatML defines\
          \ different roles (system, user, assistant) and appends special tokens (&lt;|im_start|&gt;\
          \ and &lt;|im_end|&gt;) to separate them. Moreover, DPOTrainer also requires\
          \ a specific format with three columns: prompt, chosen, and rejected.</p>\n\
          <p>Our dataset contains four columns: system, question, chatgpt, and llama2\u2013\
          13b-chat. We\u2019ll simply concatenate the system and question columns\
          \ to the prompt column. We\u2019ll also map the chatgpt column to \u201C\
          chosen\u201D and llama2\u201313b-chat to \u201Crejected\u201D. To format\
          \ the dataset in a reliable way, we\u2019ll use the tokenizer\u2019s apply_chat_template()\
          \ function, which already uses ChatML.</p>\n<p><a rel=\"nofollow\" href=\"\
          https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac\"\
          >https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac</a></p>\n"
        raw: "OpenHermes-2.5-Mistral-7B uses a specific chat template, called ChatML.\
          \ Here is an example of a conversation formatted with this template:\n\n\
          <|im_start|>system\nYou are a helpful chatbot assistant.<|im_end|>\n<|im_start|>user\n\
          Hi<|im_end|>\n<|im_start|>assistant\nHi, how can I help you?<|im_end|>\n\
          As you can see, ChatML defines different roles (system, user, assistant)\
          \ and appends special tokens (<|im_start|> and <|im_end|>) to separate them.\
          \ Moreover, DPOTrainer also requires a specific format with three columns:\
          \ prompt, chosen, and rejected.\n\nOur dataset contains four columns: system,\
          \ question, chatgpt, and llama2\u201313b-chat. We\u2019ll simply concatenate\
          \ the system and question columns to the prompt column. We\u2019ll also\
          \ map the chatgpt column to \u201Cchosen\u201D and llama2\u201313b-chat\
          \ to \u201Crejected\u201D. To format the dataset in a reliable way, we\u2019\
          ll use the tokenizer\u2019s apply_chat_template() function, which already\
          \ uses ChatML.\n\nhttps://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac\n"
        updatedAt: '2024-01-05T03:15:10.361Z'
      numEdits: 0
      reactions: []
    id: 6597743ec50abdfec4f6efb7
    type: comment
  author: CultriX
  content: "OpenHermes-2.5-Mistral-7B uses a specific chat template, called ChatML.\
    \ Here is an example of a conversation formatted with this template:\n\n<|im_start|>system\n\
    You are a helpful chatbot assistant.<|im_end|>\n<|im_start|>user\nHi<|im_end|>\n\
    <|im_start|>assistant\nHi, how can I help you?<|im_end|>\nAs you can see, ChatML\
    \ defines different roles (system, user, assistant) and appends special tokens\
    \ (<|im_start|> and <|im_end|>) to separate them. Moreover, DPOTrainer also requires\
    \ a specific format with three columns: prompt, chosen, and rejected.\n\nOur dataset\
    \ contains four columns: system, question, chatgpt, and llama2\u201313b-chat.\
    \ We\u2019ll simply concatenate the system and question columns to the prompt\
    \ column. We\u2019ll also map the chatgpt column to \u201Cchosen\u201D and llama2\u2013\
    13b-chat to \u201Crejected\u201D. To format the dataset in a reliable way, we\u2019\
    ll use the tokenizer\u2019s apply_chat_template() function, which already uses\
    \ ChatML.\n\nhttps://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac\n"
  created_at: 2024-01-05 03:15:10+00:00
  edited: false
  hidden: false
  id: 6597743ec50abdfec4f6efb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/320243a2588740e3ad886cebb082098c.svg
      fullname: Ontario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ont
      type: user
    createdAt: '2024-01-07T05:35:55.000Z'
    data:
      edited: false
      editors:
      - Ont
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8550944924354553
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/320243a2588740e3ad886cebb082098c.svg
          fullname: Ontario
          isHf: false
          isPro: false
          name: Ont
          type: user
        html: '<p>To be more specific,  given "&lt;|im_start|&gt;user" the tokenizer
          outputs 8 tokens for this model,<br>  ''&lt;'':28789, ''|'':28766, ''im'':321,
          ''_'':28730, ''start'':2521, ''|'':28766, ''&gt;'':28767, ''user'':1838<br>because
          the input and output weights of this model were not expanded to include
          those special ChatML tokens before this model was trained.</p>

          <p>In contrast, in models like OpenHermes that have those special tokens
          added to the input and output weights, given "&lt;|im_start|&gt;user" the
          tokenizer outputs 2 tokens,<br> ''&lt;|im_start|&gt;'':32001, ''user'':1838</p>

          <p>I say this because these missing tokens may have something to do with
          the unexpected behavior that I observed while using a ChatML template with
          this model.</p>

          '
        raw: "To be more specific,  given \"<|im_start|>user\" the tokenizer outputs\
          \ 8 tokens for this model,\n  '<':28789, '|':28766, 'im':321, '_':28730,\
          \ 'start':2521, '|':28766, '>':28767, 'user':1838\nbecause the input and\
          \ output weights of this model were not expanded to include those special\
          \ ChatML tokens before this model was trained.\n\nIn contrast, in models\
          \ like OpenHermes that have those special tokens added to the input and\
          \ output weights, given \"<|im_start|>user\" the tokenizer outputs 2 tokens,\n\
          \ '<|im_start|>':32001, 'user':1838\n\nI say this because these missing\
          \ tokens may have something to do with the unexpected behavior that I observed\
          \ while using a ChatML template with this model.\n\n"
        updatedAt: '2024-01-07T05:35:55.792Z'
      numEdits: 0
      reactions: []
    id: 659a383b654fe4eb0ae0065f
    type: comment
  author: Ont
  content: "To be more specific,  given \"<|im_start|>user\" the tokenizer outputs\
    \ 8 tokens for this model,\n  '<':28789, '|':28766, 'im':321, '_':28730, 'start':2521,\
    \ '|':28766, '>':28767, 'user':1838\nbecause the input and output weights of this\
    \ model were not expanded to include those special ChatML tokens before this model\
    \ was trained.\n\nIn contrast, in models like OpenHermes that have those special\
    \ tokens added to the input and output weights, given \"<|im_start|>user\" the\
    \ tokenizer outputs 2 tokens,\n '<|im_start|>':32001, 'user':1838\n\nI say this\
    \ because these missing tokens may have something to do with the unexpected behavior\
    \ that I observed while using a ChatML template with this model.\n\n"
  created_at: 2024-01-07 05:35:55+00:00
  edited: false
  hidden: false
  id: 659a383b654fe4eb0ae0065f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: CultriX/MistralTrix-v1
repo_type: model
status: open
target_branch: null
title: Missing ChatML tokens and unexpected symbols in the response
