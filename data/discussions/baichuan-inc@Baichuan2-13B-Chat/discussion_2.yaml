!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BBLL3456
conflicting_files: null
created_at: 2023-09-06 10:51:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-06T11:51:35.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8408532738685608
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>I could load Baichuan version 1 in 8bit but cannot load version
          2, has the following error:</p>

          <p>ValueError:<br>                        Some modules are dispatched on
          the CPU or the disk. Make sure you have enough GPU RAM to fit<br>                        the
          quantized model. If you want to dispatch the model on the CPU or the disk
          while keeping<br>                        these modules in 32-bit, you need
          to set <code>load_in_8bit_fp32_cpu_offload=True</code> and pass a custom<br>                        <code>device_map</code>
          to <code>from_pretrained</code>. Check<br>                        <a href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu">https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu</a><br>                        for
          more details.</p>

          '
        raw: "I could load Baichuan version 1 in 8bit but cannot load version 2, has\
          \ the following error:\r\n\r\nValueError:\r\n                        Some\
          \ modules are dispatched on the CPU or the disk. Make sure you have enough\
          \ GPU RAM to fit\r\n                        the quantized model. If you\
          \ want to dispatch the model on the CPU or the disk while keeping\r\n  \
          \                      these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
          \ and pass a custom\r\n                        `device_map` to `from_pretrained`.\
          \ Check\r\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
          \n                        for more details."
        updatedAt: '2023-09-06T11:51:35.875Z'
      numEdits: 0
      reactions: []
    id: 64f867c77565a69eb6aaacc7
    type: comment
  author: BBLL3456
  content: "I could load Baichuan version 1 in 8bit but cannot load version 2, has\
    \ the following error:\r\n\r\nValueError:\r\n                        Some modules\
    \ are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to\
    \ fit\r\n                        the quantized model. If you want to dispatch\
    \ the model on the CPU or the disk while keeping\r\n                        these\
    \ modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and\
    \ pass a custom\r\n                        `device_map` to `from_pretrained`.\
    \ Check\r\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\r\
    \n                        for more details."
  created_at: 2023-09-06 10:51:35+00:00
  edited: false
  hidden: false
  id: 64f867c77565a69eb6aaacc7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-06T12:58:35.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.867520272731781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>Can you post your code?</p>

          '
        raw: Can you post your code?
        updatedAt: '2023-09-06T12:58:35.339Z'
      numEdits: 0
      reactions: []
    id: 64f8777ba92703ef65efa945
    type: comment
  author: wuzhiying2023
  content: Can you post your code?
  created_at: 2023-09-06 11:58:35+00:00
  edited: false
  hidden: false
  id: 64f8777ba92703ef65efa945
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-06T14:07:58.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5888040661811829
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>I used the web_demo.py on github and just added the load_in_8bit.
          I can load the version 2 with load_in_4bit</p>

          <p>def init_model():<br>    model = AutoModelForCausalLM.from_pretrained(<br>        "./model/Baichuan2-13B-Chat",<br>        torch_dtype=torch.float16,<br>        device_map="auto",<br>        load_in_8bit=True,<br>        trust_remote_code=True</p>

          '
        raw: "I used the web_demo.py on github and just added the load_in_8bit. I\
          \ can load the version 2 with load_in_4bit\n\ndef init_model():\n    model\
          \ = AutoModelForCausalLM.from_pretrained(\n        \"./model/Baichuan2-13B-Chat\"\
          ,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n  \
          \      load_in_8bit=True,\n        trust_remote_code=True"
        updatedAt: '2023-09-06T14:07:58.714Z'
      numEdits: 0
      reactions: []
    id: 64f887be2f7d3d945ce5acc4
    type: comment
  author: BBLL3456
  content: "I used the web_demo.py on github and just added the load_in_8bit. I can\
    \ load the version 2 with load_in_4bit\n\ndef init_model():\n    model = AutoModelForCausalLM.from_pretrained(\n\
    \        \"./model/Baichuan2-13B-Chat\",\n        torch_dtype=torch.float16,\n\
    \        device_map=\"auto\",\n        load_in_8bit=True,\n        trust_remote_code=True"
  created_at: 2023-09-06 13:07:58+00:00
  edited: false
  hidden: false
  id: 64f887be2f7d3d945ce5acc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T02:07:52.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8138181567192078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>I cannot reproduce your error. Did you pull the latest code?</p>

          '
        raw: I cannot reproduce your error. Did you pull the latest code?
        updatedAt: '2023-09-07T02:07:52.657Z'
      numEdits: 0
      reactions: []
    id: 64f930781590553308f70253
    type: comment
  author: wuzhiying2023
  content: I cannot reproduce your error. Did you pull the latest code?
  created_at: 2023-09-07 01:07:52+00:00
  edited: false
  hidden: false
  id: 64f930781590553308f70253
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b531b68527bf9021e2b93f284835ee5.svg
      fullname: XuWave
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XuWave
      type: user
    createdAt: '2023-09-07T02:23:14.000Z'
    data:
      edited: false
      editors:
      - XuWave
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2290990650653839
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b531b68527bf9021e2b93f284835ee5.svg
          fullname: XuWave
          isHf: false
          isPro: false
          name: XuWave
          type: user
        html: "<p>\u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\
          \u6837\u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898</p>\n<p>from transformers\
          \ import AutoModelForCausalLM</p>\n<p>model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat',\
          \ load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)<br>model.save_pretrained('Baichuan2-13B-Chat-int8')</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
          \ in BaichuanForCausalLM.<strong>init</strong>(self, config, *model_args,\
          \ **model_kwargs)<br>    535 self.model = BaichuanModel(config)<br>    536\
          \ self.lm_head = NormHead(config.hidden_size, config.vocab_size, bias=False)<br>--&gt;\
          \ 537 if hasattr(config, \"quantization_config\") and config.quantization_config['load_in_4bit']:<br>\
          \    538     try:<br>    539         from .quantizer import quantize_offline,\
          \ init_model_weight_int4</p>\n<p>TypeError: 'BitsAndBytesConfig' object\
          \ is not subscriptable</p>\n"
        raw: "\u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\u6837\
          \u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\n\nfrom transformers import AutoModelForCausalLM\n\
          \nmodel = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat', load_in_8bit=True,\
          \ device_map=\"auto\", trust_remote_code=True)\nmodel.save_pretrained('Baichuan2-13B-Chat-int8')\n\
          \nFile ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
          \ in BaichuanForCausalLM.__init__(self, config, *model_args, **model_kwargs)\n\
          \    535 self.model = BaichuanModel(config)\n    536 self.lm_head = NormHead(config.hidden_size,\
          \ config.vocab_size, bias=False)\n--> 537 if hasattr(config, \"quantization_config\"\
          ) and config.quantization_config['load_in_4bit']:\n    538     try:\n  \
          \  539         from .quantizer import quantize_offline, init_model_weight_int4\n\
          \nTypeError: 'BitsAndBytesConfig' object is not subscriptable"
        updatedAt: '2023-09-07T02:23:14.262Z'
      numEdits: 0
      reactions: []
    id: 64f934124110f1806f319720
    type: comment
  author: XuWave
  content: "\u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\u6837\
    \u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\n\nfrom transformers import AutoModelForCausalLM\n\
    \nmodel = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat', load_in_8bit=True,\
    \ device_map=\"auto\", trust_remote_code=True)\nmodel.save_pretrained('Baichuan2-13B-Chat-int8')\n\
    \nFile ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
    \ in BaichuanForCausalLM.__init__(self, config, *model_args, **model_kwargs)\n\
    \    535 self.model = BaichuanModel(config)\n    536 self.lm_head = NormHead(config.hidden_size,\
    \ config.vocab_size, bias=False)\n--> 537 if hasattr(config, \"quantization_config\"\
    ) and config.quantization_config['load_in_4bit']:\n    538     try:\n    539 \
    \        from .quantizer import quantize_offline, init_model_weight_int4\n\nTypeError:\
    \ 'BitsAndBytesConfig' object is not subscriptable"
  created_at: 2023-09-07 01:23:14+00:00
  edited: false
  hidden: false
  id: 64f934124110f1806f319720
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T02:33:32.000Z'
    data:
      edited: true
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8714120984077454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: "<blockquote>\n<p>I cannot reproduce your error. Did you pull the latest\
          \ code?</p>\n</blockquote>\n<p>Yes, it is the latest, including the change\
          \ for  'BitsAndBytesConfig' object is not subscriptable.</p>\n<p>I am not\
          \ sure if it makes a difference, I downloaded the files locally and put\
          \ them in ./model folder</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;XuWave&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/XuWave\"\
          >@<span class=\"underline\">XuWave</span></a></span>\n\n\t</span></span>\
          \ you need to download the latest modeling_baichuan.py<br> But there would\
          \ still be an error for running 8bit, running 4bit is ok.</p>\n"
        raw: "> I cannot reproduce your error. Did you pull the latest code?\n\nYes,\
          \ it is the latest, including the change for  'BitsAndBytesConfig' object\
          \ is not subscriptable.\n\nI am not sure if it makes a difference, I downloaded\
          \ the files locally and put them in ./model folder\n\n@XuWave you need to\
          \ download the latest modeling_baichuan.py\n But there would still be an\
          \ error for running 8bit, running 4bit is ok."
        updatedAt: '2023-09-07T02:34:51.421Z'
      numEdits: 1
      reactions: []
    id: 64f9367ca92703ef651017d6
    type: comment
  author: BBLL3456
  content: "> I cannot reproduce your error. Did you pull the latest code?\n\nYes,\
    \ it is the latest, including the change for  'BitsAndBytesConfig' object is not\
    \ subscriptable.\n\nI am not sure if it makes a difference, I downloaded the files\
    \ locally and put them in ./model folder\n\n@XuWave you need to download the latest\
    \ modeling_baichuan.py\n But there would still be an error for running 8bit, running\
    \ 4bit is ok."
  created_at: 2023-09-07 01:33:32+00:00
  edited: true
  hidden: false
  id: 64f9367ca92703ef651017d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T02:47:04.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9673253297805786
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>I think somehow this version may be taking much more memory to load
          the 8 bit than the Baichuan version 1. If you could confirm that, then it
          could be a memory issue.</p>

          '
        raw: I think somehow this version may be taking much more memory to load the
          8 bit than the Baichuan version 1. If you could confirm that, then it could
          be a memory issue.
        updatedAt: '2023-09-07T02:47:04.247Z'
      numEdits: 0
      reactions: []
    id: 64f939a805961fa127a7d71c
    type: comment
  author: BBLL3456
  content: I think somehow this version may be taking much more memory to load the
    8 bit than the Baichuan version 1. If you could confirm that, then it could be
    a memory issue.
  created_at: 2023-09-07 01:47:04+00:00
  edited: false
  hidden: false
  id: 64f939a805961fa127a7d71c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b531b68527bf9021e2b93f284835ee5.svg
      fullname: XuWave
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XuWave
      type: user
    createdAt: '2023-09-07T03:06:36.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/3b531b68527bf9021e2b93f284835ee5.svg
          fullname: XuWave
          isHf: false
          isPro: false
          name: XuWave
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-07T03:11:01.489Z'
      numEdits: 0
      reactions: []
    id: 64f93e3c29a9aa4778fc89fb
    type: comment
  author: XuWave
  content: This comment has been hidden
  created_at: 2023-09-07 02:06:36+00:00
  edited: true
  hidden: true
  id: 64f93e3c29a9aa4778fc89fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b531b68527bf9021e2b93f284835ee5.svg
      fullname: XuWave
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XuWave
      type: user
    createdAt: '2023-09-07T03:07:38.000Z'
    data:
      edited: false
      editors:
      - XuWave
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.5160105228424072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b531b68527bf9021e2b93f284835ee5.svg
          fullname: XuWave
          isHf: false
          isPro: false
          name: XuWave
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;BBLL3456&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/BBLL3456\">@<span class=\"\
          underline\">BBLL3456</span></a></span>\n\n\t</span></span> \u5185\u5B58\
          90GB\uFF0C\u663E\u5B5832GB</p>\n"
        raw: "@BBLL3456 \u5185\u5B5890GB\uFF0C\u663E\u5B5832GB"
        updatedAt: '2023-09-07T03:07:38.244Z'
      numEdits: 0
      reactions: []
    id: 64f93e7a9433a36edf7785a7
    type: comment
  author: XuWave
  content: "@BBLL3456 \u5185\u5B5890GB\uFF0C\u663E\u5B5832GB"
  created_at: 2023-09-07 02:07:38+00:00
  edited: false
  hidden: false
  id: 64f93e7a9433a36edf7785a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T03:19:28.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463922381401062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<blockquote>

          <p>I think somehow this version may be taking much more memory to load the
          8 bit than the Baichuan version 1. If you could confirm that, then it could
          be a memory issue.</p>

          </blockquote>

          <p>For int8, 13B-Chat will cost 14.2GiB memory.</p>

          '
        raw: '> I think somehow this version may be taking much more memory to load
          the 8 bit than the Baichuan version 1. If you could confirm that, then it
          could be a memory issue.


          For int8, 13B-Chat will cost 14.2GiB memory.'
        updatedAt: '2023-09-07T03:19:28.561Z'
      numEdits: 0
      reactions: []
    id: 64f941406a71cea1c70c357d
    type: comment
  author: wuzhiying2023
  content: '> I think somehow this version may be taking much more memory to load
    the 8 bit than the Baichuan version 1. If you could confirm that, then it could
    be a memory issue.


    For int8, 13B-Chat will cost 14.2GiB memory.'
  created_at: 2023-09-07 02:19:28+00:00
  edited: false
  hidden: false
  id: 64f941406a71cea1c70c357d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T03:24:07.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2504251003265381
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: "<blockquote>\n<p>\u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\
          \u5019 \u540C\u6837\u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898</p>\n<p>from\
          \ transformers import AutoModelForCausalLM</p>\n<p>model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat',\
          \ load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)<br>model.save_pretrained('Baichuan2-13B-Chat-int8')</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
          \ in BaichuanForCausalLM.<strong>init</strong>(self, config, *model_args,\
          \ **model_kwargs)<br>    535 self.model = BaichuanModel(config)<br>    536\
          \ self.lm_head = NormHead(config.hidden_size, config.vocab_size, bias=False)<br>--&gt;\
          \ 537 if hasattr(config, \"quantization_config\") and config.quantization_config['load_in_4bit']:<br>\
          \    538     try:<br>    539         from .quantizer import quantize_offline,\
          \ init_model_weight_int4</p>\n<p>TypeError: 'BitsAndBytesConfig' object\
          \ is not subscriptable</p>\n</blockquote>\n<p>The code is not latest.<br>\"\
          </p>\n<blockquote>\n<p>--&gt; 537 if hasattr(config, \"quantization_config\"\
          ) and config.quantization_config['load_in_4bit']:<br>\"<br>is been changed\
          \ to :<br>if hasattr(config, \"quantization_config\") and isinstance(config.quantization_config,\
          \ dict) and config.quantization_config.get('load_in_4bit', False):</p>\n\
          </blockquote>\n"
        raw: "> \u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\u6837\
          \u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\n> \n> from transformers import\
          \ AutoModelForCausalLM\n> \n> model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat',\
          \ load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)\n> model.save_pretrained('Baichuan2-13B-Chat-int8')\n\
          > \n> File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
          \ in BaichuanForCausalLM.__init__(self, config, *model_args, **model_kwargs)\n\
          >     535 self.model = BaichuanModel(config)\n>     536 self.lm_head = NormHead(config.hidden_size,\
          \ config.vocab_size, bias=False)\n> --> 537 if hasattr(config, \"quantization_config\"\
          ) and config.quantization_config['load_in_4bit']:\n>     538     try:\n\
          >     539         from .quantizer import quantize_offline, init_model_weight_int4\n\
          > \n> TypeError: 'BitsAndBytesConfig' object is not subscriptable\n\nThe\
          \ code is not latest. \n\"\n> --> 537 if hasattr(config, \"quantization_config\"\
          ) and config.quantization_config['load_in_4bit']:\n\"\nis been changed to\
          \ :\nif hasattr(config, \"quantization_config\") and isinstance(config.quantization_config,\
          \ dict) and config.quantization_config.get('load_in_4bit', False):"
        updatedAt: '2023-09-07T03:24:07.558Z'
      numEdits: 0
      reactions: []
    id: 64f942570590f3db14bd3b06
    type: comment
  author: wuzhiying2023
  content: "> \u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\u6837\
    \u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\n> \n> from transformers import AutoModelForCausalLM\n\
    > \n> model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat', load_in_8bit=True,\
    \ device_map=\"auto\", trust_remote_code=True)\n> model.save_pretrained('Baichuan2-13B-Chat-int8')\n\
    > \n> File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
    \ in BaichuanForCausalLM.__init__(self, config, *model_args, **model_kwargs)\n\
    >     535 self.model = BaichuanModel(config)\n>     536 self.lm_head = NormHead(config.hidden_size,\
    \ config.vocab_size, bias=False)\n> --> 537 if hasattr(config, \"quantization_config\"\
    ) and config.quantization_config['load_in_4bit']:\n>     538     try:\n>     539\
    \         from .quantizer import quantize_offline, init_model_weight_int4\n> \n\
    > TypeError: 'BitsAndBytesConfig' object is not subscriptable\n\nThe code is not\
    \ latest. \n\"\n> --> 537 if hasattr(config, \"quantization_config\") and config.quantization_config['load_in_4bit']:\n\
    \"\nis been changed to :\nif hasattr(config, \"quantization_config\") and isinstance(config.quantization_config,\
    \ dict) and config.quantization_config.get('load_in_4bit', False):"
  created_at: 2023-09-07 02:24:07+00:00
  edited: false
  hidden: false
  id: 64f942570590f3db14bd3b06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T03:24:32.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9338337779045105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I think somehow this version may be taking much more memory to load the
          8 bit than the Baichuan version 1. If you could confirm that, then it could
          be a memory issue.</p>

          </blockquote>

          <p>For int8, 13B-Chat will cost 14.2GiB memory.</p>

          </blockquote>

          <p>I have 16GB GPU 32GB RAM - can''t load 8bit version 2. </p>

          '
        raw: "> > I think somehow this version may be taking much more memory to load\
          \ the 8 bit than the Baichuan version 1. If you could confirm that, then\
          \ it could be a memory issue.\n> \n> For int8, 13B-Chat will cost 14.2GiB\
          \ memory.\n\nI have 16GB GPU 32GB RAM - can't load 8bit version 2. "
        updatedAt: '2023-09-07T03:24:32.197Z'
      numEdits: 0
      reactions: []
    id: 64f942705515d7dccec26472
    type: comment
  author: BBLL3456
  content: "> > I think somehow this version may be taking much more memory to load\
    \ the 8 bit than the Baichuan version 1. If you could confirm that, then it could\
    \ be a memory issue.\n> \n> For int8, 13B-Chat will cost 14.2GiB memory.\n\nI\
    \ have 16GB GPU 32GB RAM - can't load 8bit version 2. "
  created_at: 2023-09-07 02:24:32+00:00
  edited: false
  hidden: false
  id: 64f942705515d7dccec26472
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T03:26:33.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44067174196243286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: "<blockquote>\n<blockquote>\n<p>\u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\
          \u7684\u65F6\u5019 \u540C\u6837\u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\
          </p>\n<p>from transformers import AutoModelForCausalLM</p>\n<p>model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat',\
          \ load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)<br>model.save_pretrained('Baichuan2-13B-Chat-int8')</p>\n\
          <p>File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
          \ in BaichuanForCausalLM.<strong>init</strong>(self, config, *model_args,\
          \ **model_kwargs)<br>    535 self.model = BaichuanModel(config)<br>    536\
          \ self.lm_head = NormHead(config.hidden_size, config.vocab_size, bias=False)<br>--&gt;\
          \ 537 if hasattr(config, \"quantization_config\") and config.quantization_config['load_in_4bit']:<br>\
          \    538     try:<br>    539         from .quantizer import quantize_offline,\
          \ init_model_weight_int4</p>\n<p>TypeError: 'BitsAndBytesConfig' object\
          \ is not subscriptable</p>\n</blockquote>\n<p>The code is not latest.<br>\"\
          </p>\n<blockquote>\n<p>--&gt; 537 if hasattr(config, \"quantization_config\"\
          ) and config.quantization_config['load_in_4bit']:<br>\"<br>is been changed\
          \ to :<br>if hasattr(config, \"quantization_config\") and isinstance(config.quantization_config,\
          \ dict) and config.quantization_config.get('load_in_4bit', False):</p>\n\
          </blockquote>\n</blockquote>\n<p>Yes I know, i was just replying to <span\
          \ data-props=\"{&quot;user&quot;:&quot;XuWave&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/XuWave\">@<span class=\"underline\"\
          >XuWave</span></a></span>\n\n\t</span></span> . I am using the latest code.\
          \ Like i said, i can run the 4bit with no issue and I am pretty sure the\
          \ 13B v2 is drawing on more GPU than the v1. Could you please look at the\
          \ codes of V1 and V2? I am using the same environment for both</p>\n"
        raw: "> > \u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\
          \u6837\u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\n> > \n> > from transformers\
          \ import AutoModelForCausalLM\n> > \n> > model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat',\
          \ load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)\n> > model.save_pretrained('Baichuan2-13B-Chat-int8')\n\
          > > \n> > File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
          \ in BaichuanForCausalLM.__init__(self, config, *model_args, **model_kwargs)\n\
          > >     535 self.model = BaichuanModel(config)\n> >     536 self.lm_head\
          \ = NormHead(config.hidden_size, config.vocab_size, bias=False)\n> > -->\
          \ 537 if hasattr(config, \"quantization_config\") and config.quantization_config['load_in_4bit']:\n\
          > >     538     try:\n> >     539         from .quantizer import quantize_offline,\
          \ init_model_weight_int4\n> > \n> > TypeError: 'BitsAndBytesConfig' object\
          \ is not subscriptable\n> \n> The code is not latest. \n> \"\n> > --> 537\
          \ if hasattr(config, \"quantization_config\") and config.quantization_config['load_in_4bit']:\n\
          > \"\n> is been changed to :\n> if hasattr(config, \"quantization_config\"\
          ) and isinstance(config.quantization_config, dict) and config.quantization_config.get('load_in_4bit',\
          \ False):\n\nYes I know, i was just replying to @XuWave . I am using the\
          \ latest code. Like i said, i can run the 4bit with no issue and I am pretty\
          \ sure the 13B v2 is drawing on more GPU than the v1. Could you please look\
          \ at the codes of V1 and V2? I am using the same environment for both"
        updatedAt: '2023-09-07T03:26:33.786Z'
      numEdits: 0
      reactions: []
    id: 64f942e9fe57e7455ac805e3
    type: comment
  author: BBLL3456
  content: "> > \u6211\u5728\u8FDB\u884Cint8\u64CD\u4F5C\u7684\u65F6\u5019 \u540C\u6837\
    \u9047\u5230\u4E86\u8FD9\u4E2A\u95EE\u9898\n> > \n> > from transformers import\
    \ AutoModelForCausalLM\n> > \n> > model = AutoModelForCausalLM.from_pretrained('Baichuan2-13B-Chat',\
    \ load_in_8bit=True, device_map=\"auto\", trust_remote_code=True)\n> > model.save_pretrained('Baichuan2-13B-Chat-int8')\n\
    > > \n> > File ~/.cache/huggingface/modules/transformers_modules/Baichuan2-13B-Chat/modeling_baichuan.py:537,\
    \ in BaichuanForCausalLM.__init__(self, config, *model_args, **model_kwargs)\n\
    > >     535 self.model = BaichuanModel(config)\n> >     536 self.lm_head = NormHead(config.hidden_size,\
    \ config.vocab_size, bias=False)\n> > --> 537 if hasattr(config, \"quantization_config\"\
    ) and config.quantization_config['load_in_4bit']:\n> >     538     try:\n> > \
    \    539         from .quantizer import quantize_offline, init_model_weight_int4\n\
    > > \n> > TypeError: 'BitsAndBytesConfig' object is not subscriptable\n> \n> The\
    \ code is not latest. \n> \"\n> > --> 537 if hasattr(config, \"quantization_config\"\
    ) and config.quantization_config['load_in_4bit']:\n> \"\n> is been changed to\
    \ :\n> if hasattr(config, \"quantization_config\") and isinstance(config.quantization_config,\
    \ dict) and config.quantization_config.get('load_in_4bit', False):\n\nYes I know,\
    \ i was just replying to @XuWave . I am using the latest code. Like i said, i\
    \ can run the 4bit with no issue and I am pretty sure the 13B v2 is drawing on\
    \ more GPU than the v1. Could you please look at the codes of V1 and V2? I am\
    \ using the same environment for both"
  created_at: 2023-09-07 02:26:33+00:00
  edited: false
  hidden: false
  id: 64f942e9fe57e7455ac805e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T05:08:43.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9246981143951416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>Yes, V2 will use more memory than V1,  The cause is  mainly the
          serval factors below:</p>

          <ol>

          <li>the vocab is 2x times than V1;</li>

          <li>quantizer is mix-precision-8bits quantization-op;<br>If we take the
          fragmentization of  gpu-memory into consideration, 15GB is not enough is
          possible.</li>

          </ol>

          '
        raw: 'Yes, V2 will use more memory than V1,  The cause is  mainly the serval
          factors below:

          1. the vocab is 2x times than V1;

          2. quantizer is mix-precision-8bits quantization-op;

          If we take the fragmentization of  gpu-memory into consideration, 15GB is
          not enough is possible.'
        updatedAt: '2023-09-07T05:08:43.640Z'
      numEdits: 0
      reactions: []
    id: 64f95adb5515d7dccec5b4c1
    type: comment
  author: wuzhiying2023
  content: 'Yes, V2 will use more memory than V1,  The cause is  mainly the serval
    factors below:

    1. the vocab is 2x times than V1;

    2. quantizer is mix-precision-8bits quantization-op;

    If we take the fragmentization of  gpu-memory into consideration, 15GB is not
    enough is possible.'
  created_at: 2023-09-07 04:08:43+00:00
  edited: false
  hidden: false
  id: 64f95adb5515d7dccec5b4c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T08:58:59.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9298050999641418
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>Sad, I can''t run Baichuan2 8bit on my machine then...</p>

          '
        raw: Sad, I can't run Baichuan2 8bit on my machine then...
        updatedAt: '2023-09-07T08:58:59.477Z'
      numEdits: 0
      reactions: []
    id: 64f990d3d0ab37df10242809
    type: comment
  author: BBLL3456
  content: Sad, I can't run Baichuan2 8bit on my machine then...
  created_at: 2023-09-07 07:58:59+00:00
  edited: false
  hidden: false
  id: 64f990d3d0ab37df10242809
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T09:02:19.000Z'
    data:
      edited: true
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8830901384353638
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6433c515a4c9c55871a47066/sdUz5wtEzFRAji3QXfKB8.png"><img
          alt="GPU Baichuan2.png" src="https://cdn-uploads.huggingface.co/production/uploads/6433c515a4c9c55871a47066/sdUz5wtEzFRAji3QXfKB8.png"></a></p>

          <p>Well according to your Github page, it is supposed to be more efficient
          than Version 1, and only requires 14.2gb as opposed to 15.8GB in Version
          1. By right i should be able to load onto my machine.</p>

          '
        raw: '

          ![GPU Baichuan2.png](https://cdn-uploads.huggingface.co/production/uploads/6433c515a4c9c55871a47066/sdUz5wtEzFRAji3QXfKB8.png)


          Well according to your Github page, it is supposed to be more efficient
          than Version 1, and only requires 14.2gb as opposed to 15.8GB in Version
          1. By right i should be able to load onto my machine.'
        updatedAt: '2023-09-07T09:03:32.169Z'
      numEdits: 1
      reactions: []
    id: 64f9919be121671c4a42ab51
    type: comment
  author: BBLL3456
  content: '

    ![GPU Baichuan2.png](https://cdn-uploads.huggingface.co/production/uploads/6433c515a4c9c55871a47066/sdUz5wtEzFRAji3QXfKB8.png)


    Well according to your Github page, it is supposed to be more efficient than Version
    1, and only requires 14.2gb as opposed to 15.8GB in Version 1. By right i should
    be able to load onto my machine.'
  created_at: 2023-09-07 08:02:19+00:00
  edited: true
  hidden: false
  id: 64f9919be121671c4a42ab51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T09:10:25.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-07T09:10:54.016Z'
      numEdits: 0
      reactions: []
    id: 64f9938174cba6ca3f730924
    type: comment
  author: wuzhiying2023
  content: This comment has been hidden
  created_at: 2023-09-07 08:10:25+00:00
  edited: true
  hidden: true
  id: 64f9938174cba6ca3f730924
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T09:13:57.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8094189167022705
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>I have no idea. On my machine, the memory usage is about 15241971712Bytes
          / 2**30 = 14.2GiB for 8bit-loading</p>

          '
        raw: I have no idea. On my machine, the memory usage is about 15241971712Bytes
          / 2**30 = 14.2GiB for 8bit-loading
        updatedAt: '2023-09-07T09:13:57.745Z'
      numEdits: 0
      reactions: []
    id: 64f994551a867f47c756c82d
    type: comment
  author: wuzhiying2023
  content: I have no idea. On my machine, the memory usage is about 15241971712Bytes
    / 2**30 = 14.2GiB for 8bit-loading
  created_at: 2023-09-07 08:13:57+00:00
  edited: false
  hidden: false
  id: 64f994551a867f47c756c82d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T10:07:33.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8106929063796997
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<blockquote>

          <p>I have no idea. On my machine, the memory usage is about 15241971712Bytes
          / 2**30 = 14.2GiB for 8bit-loading</p>

          </blockquote>

          <p>Just now, I tested 13B-int8 gpu memory usage,  nvidia-smi show 16.05GB,  while  invoking
          torch.cuda.max_allocated_memory(), we get 14.2GB. So there are other memory
          is used by the model which torch cannot get?</p>

          '
        raw: '> I have no idea. On my machine, the memory usage is about 15241971712Bytes
          / 2**30 = 14.2GiB for 8bit-loading


          Just now, I tested 13B-int8 gpu memory usage,  nvidia-smi show 16.05GB,  while  invoking
          torch.cuda.max_allocated_memory(), we get 14.2GB. So there are other memory
          is used by the model which torch cannot get?'
        updatedAt: '2023-09-07T10:07:33.247Z'
      numEdits: 0
      reactions: []
    id: 64f9a0e511e92ca5568b06ae
    type: comment
  author: wuzhiying2023
  content: '> I have no idea. On my machine, the memory usage is about 15241971712Bytes
    / 2**30 = 14.2GiB for 8bit-loading


    Just now, I tested 13B-int8 gpu memory usage,  nvidia-smi show 16.05GB,  while  invoking
    torch.cuda.max_allocated_memory(), we get 14.2GB. So there are other memory is
    used by the model which torch cannot get?'
  created_at: 2023-09-07 09:07:33+00:00
  edited: false
  hidden: false
  id: 64f9a0e511e92ca5568b06ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T10:10:49.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861140847206116
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>I guess some ops will use more additional memory. I have no idea
          on how to solve it.</p>

          '
        raw: I guess some ops will use more additional memory. I have no idea on how
          to solve it.
        updatedAt: '2023-09-07T10:10:49.402Z'
      numEdits: 0
      reactions: []
    id: 64f9a1a97786587e7fd894b9
    type: comment
  author: wuzhiying2023
  content: I guess some ops will use more additional memory. I have no idea on how
    to solve it.
  created_at: 2023-09-07 09:10:49+00:00
  edited: false
  hidden: false
  id: 64f9a1a97786587e7fd894b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T12:11:56.000Z'
    data:
      edited: true
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.917323112487793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>Is the model first loaded in fp32 instead of 16bit? It is the initial
          loading that caught the error.</p>

          <p>I also saw some discussions on this same issue raised on your Github
          page.</p>

          <p>I am pasting the entire error below:</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py",
          line 311, in _handle_cache_miss<br>    cached_result = cache.read_result(value_key)<br>  File
          "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_resource_api.py",
          line 500, in read_result<br>    raise CacheKeyNotFoundError()<br>streamlit.runtime.caching.cache_errors.CacheKeyNotFoundError</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py",
          line 552, in _run_script<br>    exec(code, module.<strong>dict</strong>)<br>  File
          "/home/user/baichuan2/web_demo.py", line 72, in <br>    main()<br>  File
          "/home/user/baichuan2/web_demo.py", line 51, in main<br>    model, tokenizer
          = init_model()<br>  File "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py",
          line 211, in wrapper<br>    return cached_func(*args, **kwargs)<br>  File
          "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py",
          line 240, in <strong>call</strong><br>    return self._get_or_create_cached_value(args,
          kwargs)<br>  File "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py",
          line 266, in _get_or_create_cached_value<br>    return self._handle_cache_miss(cache,
          value_key, func_args, func_kwargs)<br>  File "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py",
          line 320, in _handle_cache_miss<br>    computed_value = self._info.func(*func_args,
          **func_kwargs)<br>  File "/home/user/baichuan2/web_demo.py", line 13, in
          init_model<br>    model = AutoModelForCausalLM.from_pretrained(<br>  File
          "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py",
          line 558, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "/home/user/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-13B-Chat/670d17ee403f45334f53121d72feff623cc37de1/modeling_baichuan.py",
          line 669, in from_pretrained<br>    return super(BaichuanForCausalLM, cls).from_pretrained(pretrained_model_name_or_path,
          *model_args,<br>  File "/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/transformers/modeling_utils.py",
          line 3114, in from_pretrained<br>    raise ValueError(<br>ValueError:<br>                        Some
          modules are dispatched on the CPU or the disk. Make sure you have enough
          GPU RAM to fit<br>                        the quantized model. If you want
          to dispatch the model on the CPU or the disk while keeping<br>                        these
          modules in 32-bit, you need to set <code>load_in_8bit_fp32_cpu_offload=True</code>
          and pass a custom<br>                        <code>device_map</code> to
          <code>from_pretrained</code>. Check<br>                        <a href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu">https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu</a><br>                        for
          more details.</p>

          '
        raw: "Is the model first loaded in fp32 instead of 16bit? It is the initial\
          \ loading that caught the error.\n\nI also saw some discussions on this\
          \ same issue raised on your Github page.\n\nI am pasting the entire error\
          \ below:\n\nDuring handling of the above exception, another exception occurred:\n\
          \nTraceback (most recent call last):\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 311, in _handle_cache_miss\n    cached_result = cache.read_result(value_key)\n\
          \  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_resource_api.py\"\
          , line 500, in read_result\n    raise CacheKeyNotFoundError()\nstreamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\n\
          \nDuring handling of the above exception, another exception occurred:\n\n\
          Traceback (most recent call last):\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\"\
          , line 552, in _run_script\n    exec(code, module.__dict__)\n  File \"/home/user/baichuan2/web_demo.py\"\
          , line 72, in <module>\n    main()\n  File \"/home/user/baichuan2/web_demo.py\"\
          , line 51, in main\n    model, tokenizer = init_model()\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 211, in wrapper\n    return cached_func(*args, **kwargs)\n  File\
          \ \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 240, in __call__\n    return self._get_or_create_cached_value(args,\
          \ kwargs)\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 266, in _get_or_create_cached_value\n    return self._handle_cache_miss(cache,\
          \ value_key, func_args, func_kwargs)\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 320, in _handle_cache_miss\n    computed_value = self._info.func(*func_args,\
          \ **func_kwargs)\n  File \"/home/user/baichuan2/web_demo.py\", line 13,\
          \ in init_model\n    model = AutoModelForCausalLM.from_pretrained(\n  File\
          \ \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/home/user/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-13B-Chat/670d17ee403f45334f53121d72feff623cc37de1/modeling_baichuan.py\"\
          , line 669, in from_pretrained\n    return super(BaichuanForCausalLM, cls).from_pretrained(pretrained_model_name_or_path,\
          \ *model_args,\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3114, in from_pretrained\n    raise ValueError(\nValueError:\n  \
          \                      Some modules are dispatched on the CPU or the disk.\
          \ Make sure you have enough GPU RAM to fit\n                        the\
          \ quantized model. If you want to dispatch the model on the CPU or the disk\
          \ while keeping\n                        these modules in 32-bit, you need\
          \ to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n      \
          \                  `device_map` to `from_pretrained`. Check\n          \
          \              https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
          \                        for more details."
        updatedAt: '2023-09-07T12:17:18.746Z'
      numEdits: 2
      reactions: []
    id: 64f9be0c9c099d93dbc52d5b
    type: comment
  author: BBLL3456
  content: "Is the model first loaded in fp32 instead of 16bit? It is the initial\
    \ loading that caught the error.\n\nI also saw some discussions on this same issue\
    \ raised on your Github page.\n\nI am pasting the entire error below:\n\nDuring\
    \ handling of the above exception, another exception occurred:\n\nTraceback (most\
    \ recent call last):\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 311, in _handle_cache_miss\n    cached_result = cache.read_result(value_key)\n\
    \  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_resource_api.py\"\
    , line 500, in read_result\n    raise CacheKeyNotFoundError()\nstreamlit.runtime.caching.cache_errors.CacheKeyNotFoundError\n\
    \nDuring handling of the above exception, another exception occurred:\n\nTraceback\
    \ (most recent call last):\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\"\
    , line 552, in _run_script\n    exec(code, module.__dict__)\n  File \"/home/user/baichuan2/web_demo.py\"\
    , line 72, in <module>\n    main()\n  File \"/home/user/baichuan2/web_demo.py\"\
    , line 51, in main\n    model, tokenizer = init_model()\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 211, in wrapper\n    return cached_func(*args, **kwargs)\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 240, in __call__\n    return self._get_or_create_cached_value(args, kwargs)\n\
    \  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 266, in _get_or_create_cached_value\n    return self._handle_cache_miss(cache,\
    \ value_key, func_args, func_kwargs)\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 320, in _handle_cache_miss\n    computed_value = self._info.func(*func_args,\
    \ **func_kwargs)\n  File \"/home/user/baichuan2/web_demo.py\", line 13, in init_model\n\
    \    model = AutoModelForCausalLM.from_pretrained(\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 558, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"/home/user/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-13B-Chat/670d17ee403f45334f53121d72feff623cc37de1/modeling_baichuan.py\"\
    , line 669, in from_pretrained\n    return super(BaichuanForCausalLM, cls).from_pretrained(pretrained_model_name_or_path,\
    \ *model_args,\n  File \"/home/user/miniconda3/envs/baichuan2/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3114, in from_pretrained\n    raise ValueError(\nValueError:\n        \
    \                Some modules are dispatched on the CPU or the disk. Make sure\
    \ you have enough GPU RAM to fit\n                        the quantized model.\
    \ If you want to dispatch the model on the CPU or the disk while keeping\n   \
    \                     these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True`\
    \ and pass a custom\n                        `device_map` to `from_pretrained`.\
    \ Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n\
    \                        for more details."
  created_at: 2023-09-07 11:11:56+00:00
  edited: true
  hidden: false
  id: 64f9be0c9c099d93dbc52d5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-07T12:15:20.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806591868400574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<p>Not really.</p>

          '
        raw: Not really.
        updatedAt: '2023-09-07T12:15:20.509Z'
      numEdits: 0
      reactions: []
    id: 64f9bed890dbb62435b51852
    type: comment
  author: wuzhiying2023
  content: Not really.
  created_at: 2023-09-07 11:15:20+00:00
  edited: false
  hidden: false
  id: 64f9bed890dbb62435b51852
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-07T20:11:11.000Z'
    data:
      edited: false
      editors:
      - BBLL3456
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9236427545547485
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
          fullname: supersuper
          isHf: false
          isPro: false
          name: BBLL3456
          type: user
        html: '<p>Would you be able to provide an int-8 bit version?</p>

          '
        raw: Would you be able to provide an int-8 bit version?
        updatedAt: '2023-09-07T20:11:11.241Z'
      numEdits: 0
      reactions: []
    id: 64fa2e5f20a2d04cc15992db
    type: comment
  author: BBLL3456
  content: Would you be able to provide an int-8 bit version?
  created_at: 2023-09-07 19:11:11+00:00
  edited: false
  hidden: false
  id: 64fa2e5f20a2d04cc15992db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
      fullname: wuzhiying2023
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: wuzhiying2023
      type: user
    createdAt: '2023-09-08T01:33:02.000Z'
    data:
      edited: false
      editors:
      - wuzhiying2023
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.958832323551178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd9725a883f4cc4ae64a3f35697e6533.svg
          fullname: wuzhiying2023
          isHf: false
          isPro: false
          name: wuzhiying2023
          type: user
        html: '<blockquote>

          <p>Would you be able to provide an int-8 bit version?</p>

          </blockquote>

          <p>We have no plan to provide an int8 bit version by now</p>

          '
        raw: '> Would you be able to provide an int-8 bit version?


          We have no plan to provide an int8 bit version by now'
        updatedAt: '2023-09-08T01:33:02.610Z'
      numEdits: 0
      reactions: []
    id: 64fa79cedcc5ce730e43b84b
    type: comment
  author: wuzhiying2023
  content: '> Would you be able to provide an int-8 bit version?


    We have no plan to provide an int8 bit version by now'
  created_at: 2023-09-08 00:33:02+00:00
  edited: false
  hidden: false
  id: 64fa79cedcc5ce730e43b84b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/0df4268f9ce3f803235e13f95b2f21a0.svg
      fullname: supersuper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BBLL3456
      type: user
    createdAt: '2023-09-08T02:33:38.000Z'
    data:
      status: closed
    id: 64fa88025ca946a01079089f
    type: status-change
  author: BBLL3456
  created_at: 2023-09-08 01:33:38+00:00
  id: 64fa88025ca946a01079089f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: baichuan-inc/Baichuan2-13B-Chat
repo_type: model
status: closed
target_branch: null
title: load_in_8bit error
