!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brrbaral
conflicting_files: null
created_at: 2023-07-28 08:10:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64620de578b79557edde0434a34bd592.svg
      fullname: Biswaraj Baral
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brrbaral
      type: user
    createdAt: '2023-07-28T09:10:29.000Z'
    data:
      edited: false
      editors:
      - brrbaral
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7147406935691833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64620de578b79557edde0434a34bd592.svg
          fullname: Biswaraj Baral
          isHf: false
          isPro: false
          name: brrbaral
          type: user
        html: '<p>HI,<br>I am unable to use the model. I tried using the same code
          from the documentation. I am using google colab for execution. </p>

          <p>Code:<br>import torch<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>model_name
          = "GOAT-AI/GOAT-7B-Community"</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    torch_dtype=torch.bfloat16<br>)</p>

          <p>Error:<br>ValueError: Couldn''t instantiate the backend tokenizer from
          one of:<br>(1) a <code>tokenizers</code> library serialization file,<br>(2)
          a slow tokenizer instance to convert or<br>(3) an equivalent slow tokenizer
          class to instantiate and convert.<br>You need to have sentencepiece installed
          to convert a slow tokenizer to a fast one.</p>

          '
        raw: "HI, \r\nI am unable to use the model. I tried using the same code from\
          \ the documentation. I am using google colab for execution. \r\n\r\nCode:\
          \ \r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \nmodel_name = \"GOAT-AI/GOAT-7B-Community\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\n \
          \   torch_dtype=torch.bfloat16\r\n)\r\n\r\nError: \r\nValueError: Couldn't\
          \ instantiate the backend tokenizer from one of: \r\n(1) a `tokenizers`\
          \ library serialization file, \r\n(2) a slow tokenizer instance to convert\
          \ or \r\n(3) an equivalent slow tokenizer class to instantiate and convert.\
          \ \r\nYou need to have sentencepiece installed to convert a slow tokenizer\
          \ to a fast one.\r\n"
        updatedAt: '2023-07-28T09:10:29.739Z'
      numEdits: 0
      reactions: []
    id: 64c386057918ee895ba31141
    type: comment
  author: brrbaral
  content: "HI, \r\nI am unable to use the model. I tried using the same code from\
    \ the documentation. I am using google colab for execution. \r\n\r\nCode: \r\n\
    import torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \nmodel_name = \"GOAT-AI/GOAT-7B-Community\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(\r\n    model_name,\r\n    torch_dtype=torch.bfloat16\r\
    \n)\r\n\r\nError: \r\nValueError: Couldn't instantiate the backend tokenizer from\
    \ one of: \r\n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer\
    \ instance to convert or \r\n(3) an equivalent slow tokenizer class to instantiate\
    \ and convert. \r\nYou need to have sentencepiece installed to convert a slow\
    \ tokenizer to a fast one.\r\n"
  created_at: 2023-07-28 08:10:29+00:00
  edited: false
  hidden: false
  id: 64c386057918ee895ba31141
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8408052774d5ac67e31b152a22e96b4.svg
      fullname: Sungbae Chun
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bungbae
      type: user
    createdAt: '2023-07-28T09:37:19.000Z'
    data:
      edited: false
      editors:
      - bungbae
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.902353823184967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8408052774d5ac67e31b152a22e96b4.svg
          fullname: Sungbae Chun
          isHf: false
          isPro: false
          name: bungbae
          type: user
        html: '<p>Hi, you forgot to install <code>sentencepiece</code> package, see
          the last line: "You need to have sentencepiece installed to convert a slow
          tokenizer to a fast one."</p>

          '
        raw: 'Hi, you forgot to install `sentencepiece` package, see the last line:
          "You need to have sentencepiece installed to convert a slow tokenizer to
          a fast one."'
        updatedAt: '2023-07-28T09:37:19.138Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - goatai
        - flaminisd
        - yervssle
      relatedEventId: 64c38c4f2a5eaefd00100d93
    id: 64c38c4f2a5eaefd00100d91
    type: comment
  author: bungbae
  content: 'Hi, you forgot to install `sentencepiece` package, see the last line:
    "You need to have sentencepiece installed to convert a slow tokenizer to a fast
    one."'
  created_at: 2023-07-28 08:37:19+00:00
  edited: false
  hidden: false
  id: 64c38c4f2a5eaefd00100d91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c8408052774d5ac67e31b152a22e96b4.svg
      fullname: Sungbae Chun
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bungbae
      type: user
    createdAt: '2023-07-28T09:37:19.000Z'
    data:
      status: closed
    id: 64c38c4f2a5eaefd00100d93
    type: status-change
  author: bungbae
  created_at: 2023-07-28 08:37:19+00:00
  id: 64c38c4f2a5eaefd00100d93
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: GOAT-AI/GOAT-7B-Community
repo_type: model
status: closed
target_branch: null
title: 'ValueError: Couldn''t instantiate the backend tokenizer'
