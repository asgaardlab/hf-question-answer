!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brucethemoose
conflicting_files: null
created_at: 2024-01-12 00:01:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-12T00:01:35.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9792817234992981
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>This model seems awesome, and not just at financial analysis! TBH
          I have ignored your whole series of "sin" models for far too long.</p>

          <p>What context length did you train on?</p>

          <p>Was it trained as a lora? What framework?</p>

          <p>I noticed that this model in particular seems to retain its long context
          (40K+) performance. Some extensively trained Yi 200K models (like DPO Bagel)
          lost this in  their training, so I''m curious what you did to keep it.</p>

          '
        raw: 'This model seems awesome, and not just at financial analysis! TBH I
          have ignored your whole series of "sin" models for far too long.


          What context length did you train on?


          Was it trained as a lora? What framework?


          I noticed that this model in particular seems to retain its long context
          (40K+) performance. Some extensively trained Yi 200K models (like DPO Bagel)
          lost this in  their training, so I''m curious what you did to keep it.'
        updatedAt: '2024-01-12T00:03:49.469Z'
      numEdits: 4
      reactions: []
    id: 65a0815f19665f7549534a4c
    type: comment
  author: brucethemoose
  content: 'This model seems awesome, and not just at financial analysis! TBH I have
    ignored your whole series of "sin" models for far too long.


    What context length did you train on?


    Was it trained as a lora? What framework?


    I noticed that this model in particular seems to retain its long context (40K+)
    performance. Some extensively trained Yi 200K models (like DPO Bagel) lost this
    in  their training, so I''m curious what you did to keep it.'
  created_at: 2024-01-12 00:01:35+00:00
  edited: true
  hidden: false
  id: 65a0815f19665f7549534a4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661737644873-noauth.jpeg?w=200&h=200&f=face
      fullname: triad party
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TriadParty
      type: user
    createdAt: '2024-01-12T00:16:35.000Z'
    data:
      edited: false
      editors:
      - TriadParty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9159637689590454
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661737644873-noauth.jpeg?w=200&h=200&f=face
          fullname: triad party
          isHf: false
          isPro: false
          name: TriadParty
          type: user
        html: '<p>Because each piece of data I use in the pre-training stage is a
          whole research report, the length will indeed be longer than the ordinary
          model. I trained with lora at first, but somehow I always lost it in the
          middle. So I used all the parameters for continuous pre-training. I guess
          this should be the difference?</p>

          '
        raw: Because each piece of data I use in the pre-training stage is a whole
          research report, the length will indeed be longer than the ordinary model.
          I trained with lora at first, but somehow I always lost it in the middle.
          So I used all the parameters for continuous pre-training. I guess this should
          be the difference?
        updatedAt: '2024-01-12T00:16:35.470Z'
      numEdits: 0
      reactions: []
    id: 65a084e3c5770b27aea60c41
    type: comment
  author: TriadParty
  content: Because each piece of data I use in the pre-training stage is a whole research
    report, the length will indeed be longer than the ordinary model. I trained with
    lora at first, but somehow I always lost it in the middle. So I used all the parameters
    for continuous pre-training. I guess this should be the difference?
  created_at: 2024-01-12 00:16:35+00:00
  edited: false
  hidden: false
  id: 65a084e3c5770b27aea60c41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-12T00:21:09.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9462214112281799
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Full finetune, awesome.</p>

          <p>Was a max context length actually set, or was it really the default 200K?
          I just assumed 200K context would blow up unless you did the full pretraining
          on GPUs with more than 80GB.</p>

          '
        raw: 'Full finetune, awesome.


          Was a max context length actually set, or was it really the default 200K?
          I just assumed 200K context would blow up unless you did the full pretraining
          on GPUs with more than 80GB.'
        updatedAt: '2024-01-12T00:22:28.797Z'
      numEdits: 2
      reactions: []
    id: 65a085f51a2774bf62d61669
    type: comment
  author: brucethemoose
  content: 'Full finetune, awesome.


    Was a max context length actually set, or was it really the default 200K? I just
    assumed 200K context would blow up unless you did the full pretraining on GPUs
    with more than 80GB.'
  created_at: 2024-01-12 00:21:09+00:00
  edited: true
  hidden: false
  id: 65a085f51a2774bf62d61669
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661737644873-noauth.jpeg?w=200&h=200&f=face
      fullname: triad party
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TriadParty
      type: user
    createdAt: '2024-01-12T00:31:30.000Z'
    data:
      edited: false
      editors:
      - TriadParty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9236879944801331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661737644873-noauth.jpeg?w=200&h=200&f=face
          fullname: triad party
          isHf: false
          isPro: false
          name: TriadParty
          type: user
        html: "<blockquote>\n<p>Full finetune, awesome.</p>\n<p>Was a max context\
          \ length actually set, or was it really the default 200K? I just assumed\
          \ 200K context would blow up unless you did the full pretraining on GPUs\
          \ with more than 80GB.</p>\n</blockquote>\n<p>I used the default value of\
          \ 200k+, but to be honest, I don\u2019t know if the longest one in my research\
          \ report exceeds 200k+. During training, I adopted a \"truncation\" mechanism.\
          \ If a single piece of data exceeds 200K, the original text will be truncated\
          \ while retaining certain intermediate information. During the actual training,\
          \ I judged based on the Log that this mechanism was not triggered, so I\
          \ think the maximum length of 40K+ is reasonable. In addition, I did train\
          \ on 8*A800 (for well-known reasons, it is actually A100), and used deepspeed\
          \ zero3 to offload the optimizer.</p>\n"
        raw: "> Full finetune, awesome.\n> \n> Was a max context length actually set,\
          \ or was it really the default 200K? I just assumed 200K context would blow\
          \ up unless you did the full pretraining on GPUs with more than 80GB.\n\n\
          I used the default value of 200k+, but to be honest, I don\u2019t know if\
          \ the longest one in my research report exceeds 200k+. During training,\
          \ I adopted a \"truncation\" mechanism. If a single piece of data exceeds\
          \ 200K, the original text will be truncated while retaining certain intermediate\
          \ information. During the actual training, I judged based on the Log that\
          \ this mechanism was not triggered, so I think the maximum length of 40K+\
          \ is reasonable. In addition, I did train on 8*A800 (for well-known reasons,\
          \ it is actually A100), and used deepspeed zero3 to offload the optimizer."
        updatedAt: '2024-01-12T00:31:30.529Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - brucethemoose
        - ZoneOverreach
    id: 65a08862628cd4ce2f1dd77e
    type: comment
  author: TriadParty
  content: "> Full finetune, awesome.\n> \n> Was a max context length actually set,\
    \ or was it really the default 200K? I just assumed 200K context would blow up\
    \ unless you did the full pretraining on GPUs with more than 80GB.\n\nI used the\
    \ default value of 200k+, but to be honest, I don\u2019t know if the longest one\
    \ in my research report exceeds 200k+. During training, I adopted a \"truncation\"\
    \ mechanism. If a single piece of data exceeds 200K, the original text will be\
    \ truncated while retaining certain intermediate information. During the actual\
    \ training, I judged based on the Log that this mechanism was not triggered, so\
    \ I think the maximum length of 40K+ is reasonable. In addition, I did train on\
    \ 8*A800 (for well-known reasons, it is actually A100), and used deepspeed zero3\
    \ to offload the optimizer."
  created_at: 2024-01-12 00:31:30+00:00
  edited: false
  hidden: false
  id: 65a08862628cd4ce2f1dd77e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-12T00:52:13.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9505773186683655
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Awesome. There aren''t many (IMO not enough) people uploading Yi
          200K finetunes trained on long context data like that.</p>

          <p>And yeah, I''m not surprised nothing actually hits 200K. That''s like,
          a huge novel. It would have to be one monster of a document.</p>

          '
        raw: 'Awesome. There aren''t many (IMO not enough) people uploading Yi 200K
          finetunes trained on long context data like that.


          And yeah, I''m not surprised nothing actually hits 200K. That''s like, a
          huge novel. It would have to be one monster of a document.'
        updatedAt: '2024-01-12T00:52:58.096Z'
      numEdits: 3
      reactions: []
    id: 65a08d3d7d0684eef98170e1
    type: comment
  author: brucethemoose
  content: 'Awesome. There aren''t many (IMO not enough) people uploading Yi 200K
    finetunes trained on long context data like that.


    And yeah, I''m not surprised nothing actually hits 200K. That''s like, a huge
    novel. It would have to be one monster of a document.'
  created_at: 2024-01-12 00:52:13+00:00
  edited: true
  hidden: false
  id: 65a08d3d7d0684eef98170e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
      fullname: Blair Sadewitz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tachyphylaxis
      type: user
    createdAt: '2024-01-15T19:23:42.000Z'
    data:
      edited: false
      editors:
      - tachyphylaxis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9719901084899902
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8a081099e6d560e1b9016666568584e.svg
          fullname: Blair Sadewitz
          isHf: false
          isPro: false
          name: tachyphylaxis
          type: user
        html: '<p>A novel is typically 80,000 to 100,000 words long, FWIW.</p>

          '
        raw: A novel is typically 80,000 to 100,000 words long, FWIW.
        updatedAt: '2024-01-15T19:23:42.318Z'
      numEdits: 0
      reactions: []
    id: 65a5863e8a0485c12dbc3d94
    type: comment
  author: tachyphylaxis
  content: A novel is typically 80,000 to 100,000 words long, FWIW.
  created_at: 2024-01-15 19:23:42+00:00
  edited: false
  hidden: false
  id: 65a5863e8a0485c12dbc3d94
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TriadParty/deepmoney-34b-200k-base
repo_type: model
status: open
target_branch: null
title: Awesome model! Training parameters?
