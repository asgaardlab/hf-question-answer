!!python/object:huggingface_hub.community.DiscussionWithDetails
author: smjain
conflicting_files: null
created_at: 2023-08-28 14:24:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-08-28T15:24:08.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8409942388534546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>Hi,<br>Is it possible to share the compilation code,script for these
          models.<br>Thanks<br>Shashank</p>

          '
        raw: "Hi, \r\nIs it possible to share the compilation code,script for these\
          \ models.\r\nThanks\r\nShashank"
        updatedAt: '2023-08-28T15:24:08.068Z'
      numEdits: 0
      reactions: []
    id: 64ecbc18d9a41edc4dfab81a
    type: comment
  author: smjain
  content: "Hi, \r\nIs it possible to share the compilation code,script for these\
    \ models.\r\nThanks\r\nShashank"
  created_at: 2023-08-28 14:24:08+00:00
  edited: false
  hidden: false
  id: 64ecbc18d9a41edc4dfab81a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63682a2b4e7a848a188c8d45/KNkXSnBaj9l9R0rYm8ich.jpeg?w=200&h=200&f=face
      fullname: David Pissarra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: davidpissarra
      type: user
    createdAt: '2023-08-28T15:43:35.000Z'
    data:
      edited: false
      editors:
      - davidpissarra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8411890268325806
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63682a2b4e7a848a188c8d45/KNkXSnBaj9l9R0rYm8ich.jpeg?w=200&h=200&f=face
          fullname: David Pissarra
          isHf: false
          isPro: false
          name: davidpissarra
          type: user
        html: '<p>You can refer to <a rel="nofollow" href="https://mlc.ai/mlc-llm/docs/compilation/compile_models.html">https://mlc.ai/mlc-llm/docs/compilation/compile_models.html</a>
          for model compilation</p>

          '
        raw: You can refer to https://mlc.ai/mlc-llm/docs/compilation/compile_models.html
          for model compilation
        updatedAt: '2023-08-28T15:43:35.525Z'
      numEdits: 0
      reactions: []
    id: 64ecc0a7e4a7528580bff463
    type: comment
  author: davidpissarra
  content: You can refer to https://mlc.ai/mlc-llm/docs/compilation/compile_models.html
    for model compilation
  created_at: 2023-08-28 14:43:35+00:00
  edited: false
  hidden: false
  id: 64ecc0a7e4a7528580bff463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-08-29T00:34:51.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4130006730556488
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>Thanks. I was more looking into step by step python code which I
          can say use for compiling my own models like FlanT5 etc.<br>I was trying
          to do it via TVM directly, but during inference it was generating repetitive
          text.<br>Here is sample of my code<br>model = T5ForConditionalGeneration.from_pretrained("t5-small")<br>tokenizer
          = T5Tokenizer.from_pretrained("t5-small")<br>model.eval()</p>

          <h1 id="sample-input-and-onnx-export-with-padding">Sample Input and ONNX
          Export with Padding</h1>

          <p>max_sequence_length = 512  # Define a fixed sequence length for optimization<br>input_text
          = "translate English to French: Hello"<br>input_dict = tokenizer(input_text,
          return_tensors="pt", padding="max_length", max_length=max_sequence_length,
          truncation=True)<br>input_ids = input_dict["input_ids"].to(torch.int64)  #
          Explicitly convert to Long type<br>attention_mask = input_dict["attention_mask"].to(torch.int64)  #
          Explicitly convert to Long type<br>decoder_input_ids = torch.zeros((1, max_sequence_length),
          dtype=torch.int64)  # Explicitly set dtype to Long</p>

          <h1 id="onnx-export-code">ONNX export code</h1>

          <p>torch.onnx.export(model,<br>                  (input_ids, attention_mask,
          decoder_input_ids),<br>                  "t5-small.onnx",<br>                  verbose=True,<br>                  input_names=["input_ids",
          "attention_mask", "decoder_input_ids"],<br>                  output_names=["output"])</p>

          <p>optional</p>

          <p>import onnx<br>onnx_model = onnx.load("t5_qa.onnx")<br>onnx.checker.check_model(onnx_model)<br>print(onnx.helper.printable_graph(onnx_model.graph))</p>

          <p>onnx_model = onnx.load("t5-small.onnx")<br>shape_dict = {"input_ids":
          input_ids.shape, "attention_mask": attention_mask.shape, "decoder_input_ids":
          decoder_input_ids.shape}<br>mod, params = relay.frontend.from_onnx(onnx_model,
          shape_dict)</p>

          <p>target = tvm.target.Target("llvm")<br>with tvm.transform.PassContext(opt_level=3):<br>    lib
          = relay.build(mod, target=target, params=params)</p>

          <p>dev = tvm.runtime.cpu()<br>module = tvm.contrib.graph_executor.GraphModule(lib<a
          rel="nofollow" href="dev">"default"</a>)</p>

          <p>new_text = "translate English to French: Goodbye"<br>new_input_dict =
          tokenizer(new_text, return_tensors="pt", padding="max_length", max_length=max_sequence_length,
          truncation=True)<br>new_input_ids = new_input_dict["input_ids"].numpy().astype(np.int64)  #
          Explicitly convert to Long type<br>new_attention_mask = new_input_dict["attention_mask"].numpy().astype(np.int64)  #
          Explicitly convert to Long type<br>new_decoder_input_ids = np.zeros((1,
          max_sequence_length), dtype=np.int64)  # Explicitly set dtype to Long</p>

          <h1 id="set-inputs-and-run-inference-remains-the-same">Set Inputs and Run
          Inference (remains the same)</h1>

          <p>module.set_input("input_ids", tvm.nd.array(new_input_ids))<br>module.set_input("attention_mask",
          tvm.nd.array(new_attention_mask))<br>module.set_input("decoder_input_ids",
          tvm.nd.array(new_decoder_input_ids))</p>

          <p>module.run()</p>

          <p>output = module.get_output(0).asnumpy()</p>

          <p>What i was looking for is a similar kind of example, where I can start
          with a new model in code and step by step do this</p>

          '
        raw: "Thanks. I was more looking into step by step python code which I can\
          \ say use for compiling my own models like FlanT5 etc.\nI was trying to\
          \ do it via TVM directly, but during inference it was generating repetitive\
          \ text.\nHere is sample of my code\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          t5-small\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel.eval()\n\
          \n\n# Sample Input and ONNX Export with Padding\nmax_sequence_length = 512\
          \  # Define a fixed sequence length for optimization\ninput_text = \"translate\
          \ English to French: Hello\"\ninput_dict = tokenizer(input_text, return_tensors=\"\
          pt\", padding=\"max_length\", max_length=max_sequence_length, truncation=True)\n\
          input_ids = input_dict[\"input_ids\"].to(torch.int64)  # Explicitly convert\
          \ to Long type\nattention_mask = input_dict[\"attention_mask\"].to(torch.int64)\
          \  # Explicitly convert to Long type\ndecoder_input_ids = torch.zeros((1,\
          \ max_sequence_length), dtype=torch.int64)  # Explicitly set dtype to Long\n\
          \n# ONNX export code\ntorch.onnx.export(model, \n                  (input_ids,\
          \ attention_mask, decoder_input_ids), \n                  \"t5-small.onnx\"\
          , \n                  verbose=True, \n                  input_names=[\"\
          input_ids\", \"attention_mask\", \"decoder_input_ids\"], \n            \
          \      output_names=[\"output\"])\n\t\t\t\t  \n\t\t\t\t  \noptional\n\n\
          import onnx\nonnx_model = onnx.load(\"t5_qa.onnx\")\nonnx.checker.check_model(onnx_model)\n\
          print(onnx.helper.printable_graph(onnx_model.graph))\n\n\nonnx_model = onnx.load(\"\
          t5-small.onnx\")\nshape_dict = {\"input_ids\": input_ids.shape, \"attention_mask\"\
          : attention_mask.shape, \"decoder_input_ids\": decoder_input_ids.shape}\n\
          mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n\ntarget\
          \ = tvm.target.Target(\"llvm\")\nwith tvm.transform.PassContext(opt_level=3):\n\
          \    lib = relay.build(mod, target=target, params=params)\n\t\n\t\ndev =\
          \ tvm.runtime.cpu()\nmodule = tvm.contrib.graph_executor.GraphModule(lib[\"\
          default\"](dev))\n\n\nnew_text = \"translate English to French: Goodbye\"\
          \nnew_input_dict = tokenizer(new_text, return_tensors=\"pt\", padding=\"\
          max_length\", max_length=max_sequence_length, truncation=True)\nnew_input_ids\
          \ = new_input_dict[\"input_ids\"].numpy().astype(np.int64)  # Explicitly\
          \ convert to Long type\nnew_attention_mask = new_input_dict[\"attention_mask\"\
          ].numpy().astype(np.int64)  # Explicitly convert to Long type\nnew_decoder_input_ids\
          \ = np.zeros((1, max_sequence_length), dtype=np.int64)  # Explicitly set\
          \ dtype to Long\n\n# Set Inputs and Run Inference (remains the same)\n\n\
          \n\nmodule.set_input(\"input_ids\", tvm.nd.array(new_input_ids))\nmodule.set_input(\"\
          attention_mask\", tvm.nd.array(new_attention_mask))\nmodule.set_input(\"\
          decoder_input_ids\", tvm.nd.array(new_decoder_input_ids))\n\nmodule.run()\n\
          \n\noutput = module.get_output(0).asnumpy()\n\n\nWhat i was looking for\
          \ is a similar kind of example, where I can start with a new model in code\
          \ and step by step do this"
        updatedAt: '2023-08-29T00:34:51.034Z'
      numEdits: 0
      reactions: []
    id: 64ed3d2b685746556dcacb0c
    type: comment
  author: smjain
  content: "Thanks. I was more looking into step by step python code which I can say\
    \ use for compiling my own models like FlanT5 etc.\nI was trying to do it via\
    \ TVM directly, but during inference it was generating repetitive text.\nHere\
    \ is sample of my code\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
    t5-small\")\ntokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nmodel.eval()\n\
    \n\n# Sample Input and ONNX Export with Padding\nmax_sequence_length = 512  #\
    \ Define a fixed sequence length for optimization\ninput_text = \"translate English\
    \ to French: Hello\"\ninput_dict = tokenizer(input_text, return_tensors=\"pt\"\
    , padding=\"max_length\", max_length=max_sequence_length, truncation=True)\ninput_ids\
    \ = input_dict[\"input_ids\"].to(torch.int64)  # Explicitly convert to Long type\n\
    attention_mask = input_dict[\"attention_mask\"].to(torch.int64)  # Explicitly\
    \ convert to Long type\ndecoder_input_ids = torch.zeros((1, max_sequence_length),\
    \ dtype=torch.int64)  # Explicitly set dtype to Long\n\n# ONNX export code\ntorch.onnx.export(model,\
    \ \n                  (input_ids, attention_mask, decoder_input_ids), \n     \
    \             \"t5-small.onnx\", \n                  verbose=True, \n        \
    \          input_names=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\"\
    ], \n                  output_names=[\"output\"])\n\t\t\t\t  \n\t\t\t\t  \noptional\n\
    \nimport onnx\nonnx_model = onnx.load(\"t5_qa.onnx\")\nonnx.checker.check_model(onnx_model)\n\
    print(onnx.helper.printable_graph(onnx_model.graph))\n\n\nonnx_model = onnx.load(\"\
    t5-small.onnx\")\nshape_dict = {\"input_ids\": input_ids.shape, \"attention_mask\"\
    : attention_mask.shape, \"decoder_input_ids\": decoder_input_ids.shape}\nmod,\
    \ params = relay.frontend.from_onnx(onnx_model, shape_dict)\n\ntarget = tvm.target.Target(\"\
    llvm\")\nwith tvm.transform.PassContext(opt_level=3):\n    lib = relay.build(mod,\
    \ target=target, params=params)\n\t\n\t\ndev = tvm.runtime.cpu()\nmodule = tvm.contrib.graph_executor.GraphModule(lib[\"\
    default\"](dev))\n\n\nnew_text = \"translate English to French: Goodbye\"\nnew_input_dict\
    \ = tokenizer(new_text, return_tensors=\"pt\", padding=\"max_length\", max_length=max_sequence_length,\
    \ truncation=True)\nnew_input_ids = new_input_dict[\"input_ids\"].numpy().astype(np.int64)\
    \  # Explicitly convert to Long type\nnew_attention_mask = new_input_dict[\"attention_mask\"\
    ].numpy().astype(np.int64)  # Explicitly convert to Long type\nnew_decoder_input_ids\
    \ = np.zeros((1, max_sequence_length), dtype=np.int64)  # Explicitly set dtype\
    \ to Long\n\n# Set Inputs and Run Inference (remains the same)\n\n\n\nmodule.set_input(\"\
    input_ids\", tvm.nd.array(new_input_ids))\nmodule.set_input(\"attention_mask\"\
    , tvm.nd.array(new_attention_mask))\nmodule.set_input(\"decoder_input_ids\", tvm.nd.array(new_decoder_input_ids))\n\
    \nmodule.run()\n\n\noutput = module.get_output(0).asnumpy()\n\n\nWhat i was looking\
    \ for is a similar kind of example, where I can start with a new model in code\
    \ and step by step do this"
  created_at: 2023-08-28 23:34:51+00:00
  edited: false
  hidden: false
  id: 64ed3d2b685746556dcacb0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
      fullname: Charlie Ruan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CharlieFRuan
      type: user
    createdAt: '2023-08-29T22:44:29.000Z'
    data:
      edited: false
      editors:
      - CharlieFRuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6772249341011047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
          fullname: Charlie Ruan
          isHf: false
          isPro: false
          name: CharlieFRuan
          type: user
        html: '<p>Would this be what you are looking for: <a rel="nofollow" href="https://github.com/mlc-ai/notebooks/blob/main/tutorial/How_to_add_model_architeture_in_MLC_LLM.ipynb">https://github.com/mlc-ai/notebooks/blob/main/tutorial/How_to_add_model_architeture_in_MLC_LLM.ipynb</a></p>

          '
        raw: 'Would this be what you are looking for: https://github.com/mlc-ai/notebooks/blob/main/tutorial/How_to_add_model_architeture_in_MLC_LLM.ipynb'
        updatedAt: '2023-08-29T22:44:29.287Z'
      numEdits: 0
      reactions: []
    id: 64ee74cdbc6abd157a045e19
    type: comment
  author: CharlieFRuan
  content: 'Would this be what you are looking for: https://github.com/mlc-ai/notebooks/blob/main/tutorial/How_to_add_model_architeture_in_MLC_LLM.ipynb'
  created_at: 2023-08-29 21:44:29+00:00
  edited: false
  hidden: false
  id: 64ee74cdbc6abd157a045e19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-08-30T00:26:36.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9358749389648438
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>Thanks for quick replies.<br>What I am looking for is to how to
          compile a Flan T5 via mlc/tvm.<br>If there is some notebook or well documented
          steps which I need to take, it would help me immensely.</p>

          <p>So basically I get the model from HF. I convert say to onnx and how do
          I proceed from there till inferemce of compiled module.<br>Thanks again</p>

          '
        raw: 'Thanks for quick replies.

          What I am looking for is to how to compile a Flan T5 via mlc/tvm.

          If there is some notebook or well documented steps which I need to take,
          it would help me immensely.


          So basically I get the model from HF. I convert say to onnx and how do I
          proceed from there till inferemce of compiled module.

          Thanks again'
        updatedAt: '2023-08-30T00:26:36.485Z'
      numEdits: 0
      reactions: []
    id: 64ee8cbc534940d37825bf0a
    type: comment
  author: smjain
  content: 'Thanks for quick replies.

    What I am looking for is to how to compile a Flan T5 via mlc/tvm.

    If there is some notebook or well documented steps which I need to take, it would
    help me immensely.


    So basically I get the model from HF. I convert say to onnx and how do I proceed
    from there till inferemce of compiled module.

    Thanks again'
  created_at: 2023-08-29 23:26:36+00:00
  edited: false
  hidden: false
  id: 64ee8cbc534940d37825bf0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
      fullname: Charlie Ruan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CharlieFRuan
      type: user
    createdAt: '2023-08-30T00:38:07.000Z'
    data:
      edited: false
      editors:
      - CharlieFRuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9411831498146057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
          fullname: Charlie Ruan
          isHf: false
          isPro: false
          name: CharlieFRuan
          type: user
        html: '<p>Hmm I see... Perhaps you could trace through the compilation workflow
          in mlc-llm starting here: <a rel="nofollow" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/core.py#L601C5-L601C16">https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/core.py#L601C5-L601C16</a>.
          </p>

          <p>Not sure whether referring to how models like llama are compiled could
          be helpful.</p>

          <p>There is also a course on MLC: <a rel="nofollow" href="https://mlc.ai/index.html">https://mlc.ai/index.html</a>,
          which covers things like TensorIR, scheduler, etc.</p>

          <p>Sorry couldn''t provide much help.  You could post issues in the github
          repo, it would get more attention there. </p>

          '
        raw: "Hmm I see... Perhaps you could trace through the compilation workflow\
          \ in mlc-llm starting here: https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/core.py#L601C5-L601C16.\
          \ \n\nNot sure whether referring to how models like llama are compiled could\
          \ be helpful.\n\nThere is also a course on MLC: https://mlc.ai/index.html,\
          \ which covers things like TensorIR, scheduler, etc.\n\nSorry couldn't provide\
          \ much help.  You could post issues in the github repo, it would get more\
          \ attention there. "
        updatedAt: '2023-08-30T00:38:07.479Z'
      numEdits: 0
      reactions: []
    id: 64ee8f6f94b52a7a0d681d37
    type: comment
  author: CharlieFRuan
  content: "Hmm I see... Perhaps you could trace through the compilation workflow\
    \ in mlc-llm starting here: https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/core.py#L601C5-L601C16.\
    \ \n\nNot sure whether referring to how models like llama are compiled could be\
    \ helpful.\n\nThere is also a course on MLC: https://mlc.ai/index.html, which\
    \ covers things like TensorIR, scheduler, etc.\n\nSorry couldn't provide much\
    \ help.  You could post issues in the github repo, it would get more attention\
    \ there. "
  created_at: 2023-08-29 23:38:07+00:00
  edited: false
  hidden: false
  id: 64ee8f6f94b52a7a0d681d37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
      fullname: SHashank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smjain
      type: user
    createdAt: '2023-08-30T00:53:38.000Z'
    data:
      edited: false
      editors:
      - smjain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8914042711257935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94eb2907bf28e2c2dd45f96d1f1124a4.svg
          fullname: SHashank
          isHf: false
          isPro: false
          name: smjain
          type: user
        html: '<p>Ok . Sure Thanks for all your help.<br>By the way any chance if
          your team will provide compilation of T5 models anytime soon?</p>

          '
        raw: 'Ok . Sure Thanks for all your help.

          By the way any chance if your team will provide compilation of T5 models
          anytime soon?'
        updatedAt: '2023-08-30T00:53:38.680Z'
      numEdits: 0
      reactions: []
    id: 64ee931239bc2a763d6dca0e
    type: comment
  author: smjain
  content: 'Ok . Sure Thanks for all your help.

    By the way any chance if your team will provide compilation of T5 models anytime
    soon?'
  created_at: 2023-08-29 23:53:38+00:00
  edited: false
  hidden: false
  id: 64ee931239bc2a763d6dca0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
      fullname: Charlie Ruan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CharlieFRuan
      type: user
    createdAt: '2023-08-30T01:00:06.000Z'
    data:
      edited: false
      editors:
      - CharlieFRuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9720935821533203
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
          fullname: Charlie Ruan
          isHf: false
          isPro: false
          name: CharlieFRuan
          type: user
        html: '<p>Perhaps not anytime soon unfortunately...</p>

          '
        raw: Perhaps not anytime soon unfortunately...
        updatedAt: '2023-08-30T01:00:06.426Z'
      numEdits: 0
      reactions: []
    id: 64ee94969aaae2556c7b6d6b
    type: comment
  author: CharlieFRuan
  content: Perhaps not anytime soon unfortunately...
  created_at: 2023-08-30 00:00:06+00:00
  edited: false
  hidden: false
  id: 64ee94969aaae2556c7b6d6b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mlc-ai/mlc-chat-CodeLlama-34b-Instruct-hf-q4f16_1
repo_type: model
status: open
target_branch: null
title: Compilation code
