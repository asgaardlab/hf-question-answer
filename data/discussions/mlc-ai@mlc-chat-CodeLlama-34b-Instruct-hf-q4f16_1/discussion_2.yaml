!!python/object:huggingface_hub.community.DiscussionWithDetails
author: whatever1983
conflicting_files: null
created_at: 2023-09-07 09:15:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
      fullname: TS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whatever1983
      type: user
    createdAt: '2023-09-07T10:15:43.000Z'
    data:
      edited: true
      editors:
      - whatever1983
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7107963562011719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b628b91320688dcf2e954c4b22d4a630.svg
          fullname: TS
          isHf: false
          isPro: false
          name: whatever1983
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;davidpissarra&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/davidpissarra\"\
          >@<span class=\"underline\">davidpissarra</span></a></span>\n\n\t</span></span>\
          \ :</p>\n<p>Why did mlc-ai stop at CodeLlama-34B-Instruct, when the CodeLlama-34B-Python\
          \ is better?  Or even better, WizardCoder-34B-Python-hf-q4f16_1 at 73.2\
          \ HumanEval?</p>\n<p>BTW, the Vulkan path on linux is kinda buggy, Ryzen\
          \ 7840U's 8TF 780M is running no faster than Ryzen 4700U's 1TF iGPU at around\
          \ 2 tokens/s.  amdgpu.gttsize kernel parameter is set to 24000 (24GB of\
          \ the system ram can be seen by the iGPU)  It is nice that vulkan.so can\
          \ use the amdgpu.gttsize=24000 setting but the ROCm path cannot with export\
          \ hsa_override_gfx_version=11.0.0  ROCM 5.6.1</p>\n"
        raw: '@davidpissarra :


          Why did mlc-ai stop at CodeLlama-34B-Instruct, when the CodeLlama-34B-Python
          is better?  Or even better, WizardCoder-34B-Python-hf-q4f16_1 at 73.2 HumanEval?


          BTW, the Vulkan path on linux is kinda buggy, Ryzen 7840U''s 8TF 780M is
          running no faster than Ryzen 4700U''s 1TF iGPU at around 2 tokens/s.  amdgpu.gttsize
          kernel parameter is set to 24000 (24GB of the system ram can be seen by
          the iGPU)  It is nice that vulkan.so can use the amdgpu.gttsize=24000 setting
          but the ROCm path cannot with export hsa_override_gfx_version=11.0.0  ROCM
          5.6.1

          '
        updatedAt: '2023-09-07T10:21:37.041Z'
      numEdits: 2
      reactions: []
    id: 64f9a2cf3188ade79d7b47f1
    type: comment
  author: whatever1983
  content: '@davidpissarra :


    Why did mlc-ai stop at CodeLlama-34B-Instruct, when the CodeLlama-34B-Python is
    better?  Or even better, WizardCoder-34B-Python-hf-q4f16_1 at 73.2 HumanEval?


    BTW, the Vulkan path on linux is kinda buggy, Ryzen 7840U''s 8TF 780M is running
    no faster than Ryzen 4700U''s 1TF iGPU at around 2 tokens/s.  amdgpu.gttsize kernel
    parameter is set to 24000 (24GB of the system ram can be seen by the iGPU)  It
    is nice that vulkan.so can use the amdgpu.gttsize=24000 setting but the ROCm path
    cannot with export hsa_override_gfx_version=11.0.0  ROCM 5.6.1

    '
  created_at: 2023-09-07 09:15:43+00:00
  edited: true
  hidden: false
  id: 64f9a2cf3188ade79d7b47f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63682a2b4e7a848a188c8d45/KNkXSnBaj9l9R0rYm8ich.jpeg?w=200&h=200&f=face
      fullname: David Pissarra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: davidpissarra
      type: user
    createdAt: '2023-09-07T12:02:50.000Z'
    data:
      edited: false
      editors:
      - davidpissarra
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9192875623703003
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63682a2b4e7a848a188c8d45/KNkXSnBaj9l9R0rYm8ich.jpeg?w=200&h=200&f=face
          fullname: David Pissarra
          isHf: false
          isPro: false
          name: davidpissarra
          type: user
        html: '<p>Thanks for your feedback! We are currently working on a way to reuse
          the Llama2-chat lib for all model variants (<a rel="nofollow" href="https://github.com/mlc-ai/mlc-llm/issues/833">https://github.com/mlc-ai/mlc-llm/issues/833</a>).
          As soon as this is merged, it will be easier to start uploading more variants.
          </p>

          '
        raw: 'Thanks for your feedback! We are currently working on a way to reuse
          the Llama2-chat lib for all model variants (https://github.com/mlc-ai/mlc-llm/issues/833).
          As soon as this is merged, it will be easier to start uploading more variants. '
        updatedAt: '2023-09-07T12:02:50.822Z'
      numEdits: 0
      reactions: []
    id: 64f9bbea92e158f65b9c56c0
    type: comment
  author: davidpissarra
  content: 'Thanks for your feedback! We are currently working on a way to reuse the
    Llama2-chat lib for all model variants (https://github.com/mlc-ai/mlc-llm/issues/833).
    As soon as this is merged, it will be easier to start uploading more variants. '
  created_at: 2023-09-07 11:02:50+00:00
  edited: false
  hidden: false
  id: 64f9bbea92e158f65b9c56c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63682a2b4e7a848a188c8d45/KNkXSnBaj9l9R0rYm8ich.jpeg?w=200&h=200&f=face
      fullname: David Pissarra
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: davidpissarra
      type: user
    createdAt: '2023-09-27T13:19:14.000Z'
    data:
      status: closed
    id: 65142bd288af3d2946ac09af
    type: status-change
  author: davidpissarra
  created_at: 2023-09-27 12:19:14+00:00
  id: 65142bd288af3d2946ac09af
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: mlc-ai/mlc-chat-CodeLlama-34b-Instruct-hf-q4f16_1
repo_type: model
status: closed
target_branch: null
title: How about CodeLlama-34B-Python-hf-q4f16_1 and WizardCoder-34B-Python-hf-q4f16_1
