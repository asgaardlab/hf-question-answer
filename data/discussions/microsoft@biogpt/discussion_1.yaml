!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tdekelver
conflicting_files: null
created_at: 2022-11-28 14:28:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
      fullname: Thomas Dekelver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdekelver
      type: user
    createdAt: '2022-11-28T14:28:32.000Z'
    data:
      edited: false
      editors:
      - tdekelver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
          fullname: Thomas Dekelver
          isHf: false
          isPro: false
          name: tdekelver
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kamalkraj&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kamalkraj\"\
          >@<span class=\"underline\">kamalkraj</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>Thanks a lot for the contributions, however it seems like that\
          \ BioGptTokenizer and LMHeadModel are not implemented in transformers yet.\
          \ Is this normal?</p>\n<p>Tanks in advance for the help,<br>Kind regards,<br>tdekelver</p>\n"
        raw: "Hi @kamalkraj ,\r\n\r\nThanks a lot for the contributions, however it\
          \ seems like that BioGptTokenizer and LMHeadModel are not implemented in\
          \ transformers yet. Is this normal?\r\n\r\nTanks in advance for the help,\r\
          \nKind regards,\r\ntdekelver"
        updatedAt: '2022-11-28T14:28:32.726Z'
      numEdits: 0
      reactions: []
    id: 6384c590e57073c0ba18b46a
    type: comment
  author: tdekelver
  content: "Hi @kamalkraj ,\r\n\r\nThanks a lot for the contributions, however it\
    \ seems like that BioGptTokenizer and LMHeadModel are not implemented in transformers\
    \ yet. Is this normal?\r\n\r\nTanks in advance for the help,\r\nKind regards,\r\
    \ntdekelver"
  created_at: 2022-11-28 14:28:32+00:00
  edited: false
  hidden: false
  id: 6384c590e57073c0ba18b46a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626090925405-60c1e0def6cf4127c19307a0.jpeg?w=200&h=200&f=face
      fullname: Kamal Raj Kanakarajan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kamalkraj
      type: user
    createdAt: '2022-11-28T14:45:09.000Z'
    data:
      edited: false
      editors:
      - kamalkraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626090925405-60c1e0def6cf4127c19307a0.jpeg?w=200&h=200&f=face
          fullname: Kamal Raj Kanakarajan
          isHf: false
          isPro: false
          name: kamalkraj
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;tdekelver&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tdekelver\"\
          >@<span class=\"underline\">tdekelver</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>The PR is not yet merged with the main branch.  For experiments\
          \ you can install the transformers directly from- <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/pull/20420\">https://github.com/huggingface/transformers/pull/20420</a></p>\n\
          <p>Thanks,<br>Kamal</p>\n"
        raw: 'Hi @tdekelver ,


          The PR is not yet merged with the main branch.  For experiments you can
          install the transformers directly from- https://github.com/huggingface/transformers/pull/20420


          Thanks,

          Kamal'
        updatedAt: '2022-11-28T14:45:09.877Z'
      numEdits: 0
      reactions: []
    id: 6384c97516ccd1034bb149e3
    type: comment
  author: kamalkraj
  content: 'Hi @tdekelver ,


    The PR is not yet merged with the main branch.  For experiments you can install
    the transformers directly from- https://github.com/huggingface/transformers/pull/20420


    Thanks,

    Kamal'
  created_at: 2022-11-28 14:45:09+00:00
  edited: false
  hidden: false
  id: 6384c97516ccd1034bb149e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
      fullname: Thomas Dekelver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdekelver
      type: user
    createdAt: '2022-11-28T14:57:36.000Z'
    data:
      edited: false
      editors:
      - tdekelver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
          fullname: Thomas Dekelver
          isHf: false
          isPro: false
          name: tdekelver
          type: user
        html: "<p>Hi Kamal,</p>\n<p>Thanks I just tried it out and wanted to train\
          \ the model with my own dataset (2 classes) but I get an error when I try\
          \ to train it, can you help me ?<br>See below my code:</p>\n<pre><code>!\
          \ pip install git+https://github.com/kamalkraj/transformers.git@BioGPT\n\
          ! pip install sacremoses\n\nfrom transformers import BioGptTokenizer, BioGptForCausalLM,\
          \ TrainingArguments, Trainer\nimport evaluate \n\nmodel = BioGptForCausalLM.from_pretrained(\"\
          kamalkraj/biogpt\", num_labels=2)\ntokenizer = BioGptTokenizer.from_pretrained(\"\
          kamalkraj/biogpt\", use_fast=True)\nclf_metrics = evaluate.combine([\"accuracy\"\
          , \"f1\", \"precision\", \"recall\"])\n\nargs = TrainingArguments(\n   \
          \ \"biogpt-finetuned\",\n    evaluation_strategy = \"epoch\",\n    save_strategy\
          \ = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n\
          \    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n    weight_decay=0.01,\n\
          \    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n   \
          \ push_to_hub=False,\n    report_to='mlflow'\n)\n\ndef compute_metrics(eval_pred):\n\
          \    predictions, labels = eval_pred\n    predictions = predictions[:, 0]\n\
          \    return clf_metrics.compute(predictions=predictions, references=labels)\n\
          \ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"\
          train\"],\n    eval_dataset=encoded_dataset['valid'],\n    tokenizer=tokenizer,\n\
          \    compute_metrics=compute_metrics,\n  )\n\ntrainer.train()\n</code></pre>\n\
          <p>and the last line (to train the model) gives me the following error:</p>\n\
          <pre><code>The following columns in the training set don't have a corresponding\
          \ argument in `BioGptForCausalLM.forward` and have been ignored: text, abstract,\
          \ title, BERT_txt, authors, journals, keywords, sources, file. If text,\
          \ abstract, title, BERT_txt, authors, journals, keywords, sources, file\
          \ are not expected by `BioGptForCausalLM.forward`,  you can safely ignore\
          \ this message.\n/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\n\
          \  FutureWarning,\n***** Running training *****\n  Num examples = 2820\n\
          \  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train\
          \ batch size (w. parallel, distributed &amp; accumulation) = 4\n  Gradient\
          \ Accumulation steps = 1\n  Total optimization steps = 3525\n  Number of\
          \ trainable parameters = 346763264\n\n---------------------------------------------------------------------------\n\
          \nIndexError                                Traceback (most recent call\
          \ last)\n\n&lt;ipython-input-20-3435b262f1ae&gt; in &lt;module&gt;\n----&gt;\
          \ 1 trainer.train()\n\n5 frames\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n\
          \   1549             resume_from_checkpoint=resume_from_checkpoint,\n  \
          \ 1550             trial=trial,\n-&gt; 1551             ignore_keys_for_eval=ignore_keys_for_eval,\n\
          \   1552         )\n   1553 \n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in _inner_training_loop(self, batch_size, args, resume_from_checkpoint,\
          \ trial, ignore_keys_for_eval)\n   1793                         tr_loss_step\
          \ = self.training_step(model, inputs)\n   1794                 else:\n-&gt;\
          \ 1795                     tr_loss_step = self.training_step(model, inputs)\n\
          \   1796 \n   1797                 if (\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in training_step(self, model, inputs)\n   2552 \n   2553         with\
          \ self.compute_loss_context_manager():\n-&gt; 2554             loss = self.compute_loss(model,\
          \ inputs)\n   2555 \n   2556         if self.args.n_gpu &gt; 1:\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in compute_loss(self, model, inputs, return_outputs)\n   2584        \
          \ else:\n   2585             labels = None\n-&gt; 2586         outputs =\
          \ model(**inputs)\n   2587         # Save past state if it exists\n   2588\
          \         # TODO: this needs to be fixed and made cleaner later.\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *input, **kwargs)\n   1128         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\
          \   1129                 or _global_forward_hooks or _global_forward_pre_hooks):\n\
          -&gt; 1130             return forward_call(*input, **kwargs)\n   1131  \
          \       # Do not call functions when jit is used\n   1132         full_backward_hooks,\
          \ non_full_backward_hooks = [], []\n\n/usr/local/lib/python3.7/dist-packages/transformers/models/biogpt/modeling_biogpt.py\
          \ in forward(self, input_ids, attention_mask, head_mask, inputs_embeds,\
          \ past_key_values, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\n    685             # we are doing next-token prediction;\
          \ shift prediction scores and input ids by one\n    686             shifted_prediction_scores\
          \ = prediction_scores[:, :-1, :].contiguous()\n--&gt; 687             labels\
          \ = labels[:, 1:].contiguous()\n    688             loss_fct = CrossEntropyLoss()\n\
          \    689             lm_loss = loss_fct(shifted_prediction_scores.view(-1,\
          \ self.config.vocab_size), labels.view(-1))\n\nIndexError: too many indices\
          \ for tensor of dimension 1\n</code></pre>\n<p>Can you help me?</p>\n"
        raw: "Hi Kamal,\n\nThanks I just tried it out and wanted to train the model\
          \ with my own dataset (2 classes) but I get an error when I try to train\
          \ it, can you help me ?\nSee below my code:\n\n```\n! pip install git+https://github.com/kamalkraj/transformers.git@BioGPT\n\
          ! pip install sacremoses\n\nfrom transformers import BioGptTokenizer, BioGptForCausalLM,\
          \ TrainingArguments, Trainer\nimport evaluate \n\nmodel = BioGptForCausalLM.from_pretrained(\"\
          kamalkraj/biogpt\", num_labels=2)\ntokenizer = BioGptTokenizer.from_pretrained(\"\
          kamalkraj/biogpt\", use_fast=True)\nclf_metrics = evaluate.combine([\"accuracy\"\
          , \"f1\", \"precision\", \"recall\"])\n\nargs = TrainingArguments(\n   \
          \ \"biogpt-finetuned\",\n    evaluation_strategy = \"epoch\",\n    save_strategy\
          \ = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n\
          \    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n    weight_decay=0.01,\n\
          \    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n   \
          \ push_to_hub=False,\n    report_to='mlflow'\n)\n\ndef compute_metrics(eval_pred):\n\
          \    predictions, labels = eval_pred\n    predictions = predictions[:, 0]\n\
          \    return clf_metrics.compute(predictions=predictions, references=labels)\n\
          \ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"\
          train\"],\n    eval_dataset=encoded_dataset['valid'],\n    tokenizer=tokenizer,\n\
          \    compute_metrics=compute_metrics,\n  )\n\ntrainer.train()\n```\n\nand\
          \ the last line (to train the model) gives me the following error:\n\n```\n\
          The following columns in the training set don't have a corresponding argument\
          \ in `BioGptForCausalLM.forward` and have been ignored: text, abstract,\
          \ title, BERT_txt, authors, journals, keywords, sources, file. If text,\
          \ abstract, title, BERT_txt, authors, journals, keywords, sources, file\
          \ are not expected by `BioGptForCausalLM.forward`,  you can safely ignore\
          \ this message.\n/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310:\
          \ FutureWarning: This implementation of AdamW is deprecated and will be\
          \ removed in a future version. Use the PyTorch implementation torch.optim.AdamW\
          \ instead, or set `no_deprecation_warning=True` to disable this warning\n\
          \  FutureWarning,\n***** Running training *****\n  Num examples = 2820\n\
          \  Num Epochs = 5\n  Instantaneous batch size per device = 4\n  Total train\
          \ batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation\
          \ steps = 1\n  Total optimization steps = 3525\n  Number of trainable parameters\
          \ = 346763264\n\n---------------------------------------------------------------------------\n\
          \nIndexError                                Traceback (most recent call\
          \ last)\n\n<ipython-input-20-3435b262f1ae> in <module>\n----> 1 trainer.train()\n\
          \n5 frames\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n\
          \   1549             resume_from_checkpoint=resume_from_checkpoint,\n  \
          \ 1550             trial=trial,\n-> 1551             ignore_keys_for_eval=ignore_keys_for_eval,\n\
          \   1552         )\n   1553 \n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in _inner_training_loop(self, batch_size, args, resume_from_checkpoint,\
          \ trial, ignore_keys_for_eval)\n   1793                         tr_loss_step\
          \ = self.training_step(model, inputs)\n   1794                 else:\n->\
          \ 1795                     tr_loss_step = self.training_step(model, inputs)\n\
          \   1796 \n   1797                 if (\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in training_step(self, model, inputs)\n   2552 \n   2553         with\
          \ self.compute_loss_context_manager():\n-> 2554             loss = self.compute_loss(model,\
          \ inputs)\n   2555 \n   2556         if self.args.n_gpu > 1:\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
          \ in compute_loss(self, model, inputs, return_outputs)\n   2584        \
          \ else:\n   2585             labels = None\n-> 2586         outputs = model(**inputs)\n\
          \   2587         # Save past state if it exists\n   2588         # TODO:\
          \ this needs to be fixed and made cleaner later.\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\
          \ in _call_impl(self, *input, **kwargs)\n   1128         if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\
          \   1129                 or _global_forward_hooks or _global_forward_pre_hooks):\n\
          -> 1130             return forward_call(*input, **kwargs)\n   1131     \
          \    # Do not call functions when jit is used\n   1132         full_backward_hooks,\
          \ non_full_backward_hooks = [], []\n\n/usr/local/lib/python3.7/dist-packages/transformers/models/biogpt/modeling_biogpt.py\
          \ in forward(self, input_ids, attention_mask, head_mask, inputs_embeds,\
          \ past_key_values, labels, use_cache, output_attentions, output_hidden_states,\
          \ return_dict)\n    685             # we are doing next-token prediction;\
          \ shift prediction scores and input ids by one\n    686             shifted_prediction_scores\
          \ = prediction_scores[:, :-1, :].contiguous()\n--> 687             labels\
          \ = labels[:, 1:].contiguous()\n    688             loss_fct = CrossEntropyLoss()\n\
          \    689             lm_loss = loss_fct(shifted_prediction_scores.view(-1,\
          \ self.config.vocab_size), labels.view(-1))\n\nIndexError: too many indices\
          \ for tensor of dimension 1\n```\n\nCan you help me?"
        updatedAt: '2022-11-28T14:57:36.699Z'
      numEdits: 0
      reactions: []
    id: 6384cc60cafe4e4ac4059aaa
    type: comment
  author: tdekelver
  content: "Hi Kamal,\n\nThanks I just tried it out and wanted to train the model\
    \ with my own dataset (2 classes) but I get an error when I try to train it, can\
    \ you help me ?\nSee below my code:\n\n```\n! pip install git+https://github.com/kamalkraj/transformers.git@BioGPT\n\
    ! pip install sacremoses\n\nfrom transformers import BioGptTokenizer, BioGptForCausalLM,\
    \ TrainingArguments, Trainer\nimport evaluate \n\nmodel = BioGptForCausalLM.from_pretrained(\"\
    kamalkraj/biogpt\", num_labels=2)\ntokenizer = BioGptTokenizer.from_pretrained(\"\
    kamalkraj/biogpt\", use_fast=True)\nclf_metrics = evaluate.combine([\"accuracy\"\
    , \"f1\", \"precision\", \"recall\"])\n\nargs = TrainingArguments(\n    \"biogpt-finetuned\"\
    ,\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=2e-5,\n\
    \    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=5,\n\
    \    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n\
    \    push_to_hub=False,\n    report_to='mlflow'\n)\n\ndef compute_metrics(eval_pred):\n\
    \    predictions, labels = eval_pred\n    predictions = predictions[:, 0]\n  \
    \  return clf_metrics.compute(predictions=predictions, references=labels)\n\n\
    trainer = Trainer(\n    model,\n    args,\n    train_dataset=encoded_dataset[\"\
    train\"],\n    eval_dataset=encoded_dataset['valid'],\n    tokenizer=tokenizer,\n\
    \    compute_metrics=compute_metrics,\n  )\n\ntrainer.train()\n```\n\nand the\
    \ last line (to train the model) gives me the following error:\n\n```\nThe following\
    \ columns in the training set don't have a corresponding argument in `BioGptForCausalLM.forward`\
    \ and have been ignored: text, abstract, title, BERT_txt, authors, journals, keywords,\
    \ sources, file. If text, abstract, title, BERT_txt, authors, journals, keywords,\
    \ sources, file are not expected by `BioGptForCausalLM.forward`,  you can safely\
    \ ignore this message.\n/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310:\
    \ FutureWarning: This implementation of AdamW is deprecated and will be removed\
    \ in a future version. Use the PyTorch implementation torch.optim.AdamW instead,\
    \ or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n\
    ***** Running training *****\n  Num examples = 2820\n  Num Epochs = 5\n  Instantaneous\
    \ batch size per device = 4\n  Total train batch size (w. parallel, distributed\
    \ & accumulation) = 4\n  Gradient Accumulation steps = 1\n  Total optimization\
    \ steps = 3525\n  Number of trainable parameters = 346763264\n\n---------------------------------------------------------------------------\n\
    \nIndexError                                Traceback (most recent call last)\n\
    \n<ipython-input-20-3435b262f1ae> in <module>\n----> 1 trainer.train()\n\n5 frames\n\
    \n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in train(self,\
    \ resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n   1549    \
    \         resume_from_checkpoint=resume_from_checkpoint,\n   1550            \
    \ trial=trial,\n-> 1551             ignore_keys_for_eval=ignore_keys_for_eval,\n\
    \   1552         )\n   1553 \n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
    \ in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial,\
    \ ignore_keys_for_eval)\n   1793                         tr_loss_step = self.training_step(model,\
    \ inputs)\n   1794                 else:\n-> 1795                     tr_loss_step\
    \ = self.training_step(model, inputs)\n   1796 \n   1797                 if (\n\
    \n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in training_step(self,\
    \ model, inputs)\n   2552 \n   2553         with self.compute_loss_context_manager():\n\
    -> 2554             loss = self.compute_loss(model, inputs)\n   2555 \n   2556\
    \         if self.args.n_gpu > 1:\n\n/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\
    \ in compute_loss(self, model, inputs, return_outputs)\n   2584         else:\n\
    \   2585             labels = None\n-> 2586         outputs = model(**inputs)\n\
    \   2587         # Save past state if it exists\n   2588         # TODO: this\
    \ needs to be fixed and made cleaner later.\n\n/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\
    \ in _call_impl(self, *input, **kwargs)\n   1128         if not (self._backward_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\
    \   1129                 or _global_forward_hooks or _global_forward_pre_hooks):\n\
    -> 1130             return forward_call(*input, **kwargs)\n   1131         # Do\
    \ not call functions when jit is used\n   1132         full_backward_hooks, non_full_backward_hooks\
    \ = [], []\n\n/usr/local/lib/python3.7/dist-packages/transformers/models/biogpt/modeling_biogpt.py\
    \ in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, past_key_values,\
    \ labels, use_cache, output_attentions, output_hidden_states, return_dict)\n \
    \   685             # we are doing next-token prediction; shift prediction scores\
    \ and input ids by one\n    686             shifted_prediction_scores = prediction_scores[:,\
    \ :-1, :].contiguous()\n--> 687             labels = labels[:, 1:].contiguous()\n\
    \    688             loss_fct = CrossEntropyLoss()\n    689             lm_loss\
    \ = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n\
    \nIndexError: too many indices for tensor of dimension 1\n```\n\nCan you help\
    \ me?"
  created_at: 2022-11-28 14:57:36+00:00
  edited: false
  hidden: false
  id: 6384cc60cafe4e4ac4059aaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626090925405-60c1e0def6cf4127c19307a0.jpeg?w=200&h=200&f=face
      fullname: Kamal Raj Kanakarajan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kamalkraj
      type: user
    createdAt: '2022-11-28T18:28:13.000Z'
    data:
      edited: false
      editors:
      - kamalkraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1626090925405-60c1e0def6cf4127c19307a0.jpeg?w=200&h=200&f=face
          fullname: Kamal Raj Kanakarajan
          isHf: false
          isPro: false
          name: kamalkraj
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;tdekelver&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tdekelver\"\
          >@<span class=\"underline\">tdekelver</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p><code>BioGptForCausalLM</code> is not for the sequence classification\
          \ tasks. It is only for generating text. </p>\n<p>The original and the current\
          \ HF implementation don't have a sequence classification task implementation.\
          \ Once the original PR merges, I will add support for the same.</p>\n<p>Thanks.</p>\n"
        raw: "Hi @tdekelver ,\n\n`BioGptForCausalLM` is not for the sequence classification\
          \ tasks. It is only for generating text. \n\nThe original and the current\
          \ HF implementation don't have a sequence classification task implementation.\
          \ Once the original PR merges, I will add support for the same.\n\nThanks."
        updatedAt: '2022-11-28T18:28:13.658Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tdekelver
    id: 6384fdbddfffab4824015a13
    type: comment
  author: kamalkraj
  content: "Hi @tdekelver ,\n\n`BioGptForCausalLM` is not for the sequence classification\
    \ tasks. It is only for generating text. \n\nThe original and the current HF implementation\
    \ don't have a sequence classification task implementation. Once the original\
    \ PR merges, I will add support for the same.\n\nThanks."
  created_at: 2022-11-28 18:28:13+00:00
  edited: false
  hidden: false
  id: 6384fdbddfffab4824015a13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
      fullname: Thomas Dekelver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdekelver
      type: user
    createdAt: '2022-11-29T08:28:12.000Z'
    data:
      edited: false
      editors:
      - tdekelver
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
          fullname: Thomas Dekelver
          isHf: false
          isPro: false
          name: tdekelver
          type: user
        html: '<p>Ah okay thanks !</p>

          '
        raw: Ah okay thanks !
        updatedAt: '2022-11-29T08:28:12.217Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6385c29cb3ef0523c638edcf
    id: 6385c29cb3ef0523c638edce
    type: comment
  author: tdekelver
  content: Ah okay thanks !
  created_at: 2022-11-29 08:28:12+00:00
  edited: false
  hidden: false
  id: 6385c29cb3ef0523c638edce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d1bca2fa326b1d6799848357cc2201a4.svg
      fullname: Thomas Dekelver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tdekelver
      type: user
    createdAt: '2022-11-29T08:28:12.000Z'
    data:
      status: closed
    id: 6385c29cb3ef0523c638edcf
    type: status-change
  author: tdekelver
  created_at: 2022-11-29 08:28:12+00:00
  id: 6385c29cb3ef0523c638edcf
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: microsoft/biogpt
repo_type: model
status: closed
target_branch: null
title: BioGptTokenizer, BioGptLMHeadModel don't exist yet in transformers
