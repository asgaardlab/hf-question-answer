!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tekeshwarhirwani
conflicting_files: null
created_at: 2023-06-28 11:02:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43bfffe27fc1eaea014120eeabc77e34.svg
      fullname: Tekeshwar Hirwani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tekeshwarhirwani
      type: user
    createdAt: '2023-06-28T12:02:33.000Z'
    data:
      edited: false
      editors:
      - tekeshwarhirwani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.16849467158317566
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43bfffe27fc1eaea014120eeabc77e34.svg
          fullname: Tekeshwar Hirwani
          isHf: false
          isPro: false
          name: tekeshwarhirwani
          type: user
        html: "<p>Hi I was using run_ner.py script It showed an error that it doesn't\
          \ support model which have slow tokenizer and then they suggested that try\
          \ to run old run_ner.py script which is this (<a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/tree/main/examples/legacy/token-classification\"\
          >https://github.com/huggingface/transformers/tree/main/examples/legacy/token-classification</a>),\
          \ Now I am getting an error </p>\n<p>/content/transformers/examples/legacy/token-classification/run_ner.py:327\
          \ in \u2502<br>\u2502                                                  \
          \                    \u2502<br>\u2502                                  \
          \                                            \u2502<br>\u2502   324    \
          \                                                                    \u2502\
          <br>\u2502   325                                                       \
          \                 \u2502<br>\u2502   326 if <strong>name</strong> == \"\
          <strong>main</strong>\":                                             \u2502\
          <br>\u2502 \u2771 327 \u2502   main()                                  \
          \                           \u2502<br>\u2502   328                     \
          \                                                   \u2502<br>\u2502   \
          \                                                                      \
          \     \u2502<br>\u2502 /content/transformers/examples/legacy/token-classification/run_ner.py:262\
          \ in \u2502<br>\u2502 main                                             \
          \                            \u2502<br>\u2502                          \
          \                                                    \u2502<br>\u2502  \
          \ 259 \u2502                                                           \
          \           \u2502<br>\u2502   260 \u2502   # Training                 \
          \                                        \u2502<br>\u2502   261 \u2502 \
          \  if training_args.do_train:                                         \u2502\
          <br>\u2502 \u2771 262 \u2502   \u2502   trainer.train(                 \
          \                                \u2502<br>\u2502   263 \u2502   \u2502\
          \   \u2502   model_path=model_args.model_name_or_path if os.path.isdir(\
          \ \u2502<br>\u2502   264 \u2502   \u2502   )                           \
          \                                   \u2502<br>\u2502   265 \u2502   \u2502\
          \   trainer.save_model()                                           \u2502\
          <br>\u2502                                                             \
          \                 \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1536\
          \ in      \u2502<br>\u2502 train                                       \
          \                                 \u2502<br>\u2502                     \
          \                                                         \u2502<br>\u2502\
          \   1533 \u2502   \u2502   inner_training_loop = find_executable_batch_size(\
          \             \u2502<br>\u2502   1534 \u2502   \u2502   \u2502   self._inner_training_loop,\
          \ self._train_batch_size, args.a \u2502<br>\u2502   1535 \u2502   \u2502\
          \   )                                                             \u2502\
          <br>\u2502 \u2771 1536 \u2502   \u2502   return inner_training_loop(   \
          \                                \u2502<br>\u2502   1537 \u2502   \u2502\
          \   \u2502   args=args,                                                \u2502\
          <br>\u2502   1538 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
          \            \u2502<br>\u2502   1539 \u2502   \u2502   \u2502   trial=trial,\
          \                                              \u2502<br>\u2502        \
          \                                                                      \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1779\
          \ in      \u2502<br>\u2502 _inner_training_loop                        \
          \                                 \u2502<br>\u2502                     \
          \                                                         \u2502<br>\u2502\
          \   1776 \u2502   \u2502   \u2502   \u2502   rng_to_sync = True        \
          \                            \u2502<br>\u2502   1777 \u2502   \u2502   \u2502\
          \                                                             \u2502<br>\u2502\
          \   1778 \u2502   \u2502   \u2502   step = -1                          \
          \                       \u2502<br>\u2502 \u2771 1779 \u2502   \u2502   \u2502\
          \   for step, inputs in enumerate(epoch_iterator):            \u2502<br>\u2502\
          \   1780 \u2502   \u2502   \u2502   \u2502   total_batched_samples += 1\
          \                            \u2502<br>\u2502   1781 \u2502   \u2502   \u2502\
          \   \u2502   if rng_to_sync:                                       \u2502\
          <br>\u2502   1782 \u2502   \u2502   \u2502   \u2502   \u2502   self._load_rng_state(resume_from_checkpoint)\
          \      \u2502<br>\u2502                                                \
          \                              \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py:377\
          \ in     \u2502<br>\u2502 <strong>iter</strong>                        \
          \                                             \u2502<br>\u2502         \
          \                                                                     \u2502\
          <br>\u2502   374 \u2502   \u2502   dataloader_iter = super().<strong>iter</strong>()\
          \                           \u2502<br>\u2502   375 \u2502   \u2502   # We\
          \ iterate one batch ahead to check when we are at the end   \u2502<br>\u2502\
          \   376 \u2502   \u2502   try:                                         \
          \                  \u2502<br>\u2502 \u2771 377 \u2502   \u2502   \u2502\
          \   current_batch = next(dataloader_iter)                      \u2502<br>\u2502\
          \   378 \u2502   \u2502   except StopIteration:                        \
          \                  \u2502<br>\u2502   379 \u2502   \u2502   \u2502   yield\
          \                                                      \u2502<br>\u2502\
          \   380                                                                \
          \        \u2502<br>\u2502                                              \
          \                                \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633\
          \   \u2502<br>\u2502 in <strong>next</strong>                          \
          \                                        \u2502<br>\u2502              \
          \                                                                \u2502\
          <br>\u2502    630 \u2502   \u2502   \u2502   if self._sampler_iter is None:\
          \                            \u2502<br>\u2502    631 \u2502   \u2502   \u2502\
          \   \u2502   # TODO(<a rel=\"nofollow\" href=\"https://github.com/pytorch/pytorch/issues/7675\"\
          >https://github.com/pytorch/pytorch/issues/7675</a> \u2502<br>\u2502   \
          \ 632 \u2502   \u2502   \u2502   \u2502   self._reset()  # type: ignore[call-arg]\
          \               \u2502<br>\u2502 \u2771  633 \u2502   \u2502   \u2502  \
          \ data = self._next_data()                                  \u2502<br>\u2502\
          \    634 \u2502   \u2502   \u2502   self._num_yielded += 1             \
          \                       \u2502<br>\u2502    635 \u2502   \u2502   \u2502\
          \   if self._dataset_kind == _DatasetKind.Iterable and \\      \u2502<br>\u2502\
          \    636 \u2502   \u2502   \u2502   \u2502   \u2502   self._IterableDataset_len_called\
          \ is not None and  \u2502<br>\u2502                                    \
          \                                          \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:677\
          \   \u2502<br>\u2502 in _next_data                                     \
          \                           \u2502<br>\u2502                           \
          \                                                   \u2502<br>\u2502   \
          \ 674 \u2502                                                           \
          \          \u2502<br>\u2502    675 \u2502   def _next_data(self):      \
          \                                       \u2502<br>\u2502    676 \u2502 \
          \  \u2502   index = self._next_index()  # may raise StopIteration      \
          \   \u2502<br>\u2502 \u2771  677 \u2502   \u2502   data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIt \u2502<br>\u2502    678 \u2502   \u2502   if self._pin_memory:\
          \                                          \u2502<br>\u2502    679 \u2502\
          \   \u2502   \u2502   data = _utils.pin_memory.pin_memory(data, self._pin_memor\
          \ \u2502<br>\u2502    680 \u2502   \u2502   return data                \
          \                                   \u2502<br>\u2502                   \
          \                                                           \u2502<br>\u2502\
          \ /usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:54\
          \  \u2502<br>\u2502 in fetch                                           \
          \                          \u2502<br>\u2502                            \
          \                                                  \u2502<br>\u2502   51\
          \ \u2502   \u2502   \u2502   \u2502   data = [self.dataset[idx] for idx\
          \ in possibly_batched_i \u2502<br>\u2502   52 \u2502   \u2502   else:  \
          \                                                         \u2502<br>\u2502\
          \   53 \u2502   \u2502   \u2502   data = self.dataset[possibly_batched_index]\
          \                 \u2502<br>\u2502 \u2771 54 \u2502   \u2502   return self.collate_fn(data)\
          \                                    \u2502<br>\u2502   55             \
          \                                                            \u2502<br>\u2502\
          \                                                                      \
          \        \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py:698\
          \ in \u2502<br>\u2502 <strong>call</strong>                            \
          \                                         \u2502<br>\u2502             \
          \                                                                 \u2502\
          <br>\u2502   695 \u2502                                                \
          \                      \u2502<br>\u2502   696 \u2502   def <strong>call</strong>(self,\
          \ features: List[dict]):                          \u2502<br>\u2502   697\
          \ \u2502   \u2502   features = [self._remove_columns(feature) for feature\
          \ in featu \u2502<br>\u2502 \u2771 698 \u2502   \u2502   return self.data_collator(features)\
          \                            \u2502<br>\u2502   699                    \
          \                                                    \u2502<br>\u2502  \
          \                                                                      \
          \      \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:7\
          \ \u2502<br>\u2502 0 in default_data_collator                          \
          \                         \u2502<br>\u2502                             \
          \                                                 \u2502<br>\u2502     67\
          \ \u2502   # on the whole batch.                                       \
          \      \u2502<br>\u2502     68 \u2502                                  \
          \                                   \u2502<br>\u2502     69 \u2502   if\
          \ return_tensors == \"pt\":                                        \u2502\
          <br>\u2502 \u2771   70 \u2502   \u2502   return torch_default_data_collator(features)\
          \                  \u2502<br>\u2502     71 \u2502   elif return_tensors\
          \ == \"tf\":                                      \u2502<br>\u2502     72\
          \ \u2502   \u2502   return tf_default_data_collator(features)          \
          \           \u2502<br>\u2502     73 \u2502   elif return_tensors == \"np\"\
          :                                      \u2502<br>\u2502                \
          \                                                              \u2502<br>\u2502\
          \ /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1\
          \ \u2502<br>\u2502 36 in torch_default_data_collator                   \
          \                         \u2502<br>\u2502                             \
          \                                                 \u2502<br>\u2502    133\
          \ \u2502   \u2502   \u2502   elif isinstance(v, np.ndarray):           \
          \                \u2502<br>\u2502    134 \u2502   \u2502   \u2502   \u2502\
          \   batch[k] = torch.tensor(np.stack([f[k] for f in featu \u2502<br>\u2502\
          \    135 \u2502   \u2502   \u2502   else:                              \
          \                       \u2502<br>\u2502 \u2771  136 \u2502   \u2502   \u2502\
          \   \u2502   batch[k] = torch.tensor([f[k] for f in features])     \u2502\
          <br>\u2502    137 \u2502                                               \
          \                      \u2502<br>\u2502    138 \u2502   return batch   \
          \                                                   \u2502<br>\u2502   \
          \ 139                                                                  \
          \     \u2502<br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F<br>RuntimeError:\
          \ Could not infer dtype of NoneType</p>\n"
        raw: "Hi I was using run_ner.py script It showed an error that it doesn't\
          \ support model which have slow tokenizer and then they suggested that try\
          \ to run old run_ner.py script which is this (https://github.com/huggingface/transformers/tree/main/examples/legacy/token-classification),\
          \ Now I am getting an error \r\n\r\n/content/transformers/examples/legacy/token-classification/run_ner.py:327\
          \ in \u2502\r\n\u2502 <module>                                         \
          \                            \u2502\r\n\u2502                          \
          \                                                    \u2502\r\n\u2502  \
          \ 324                                                                  \
          \      \u2502\r\n\u2502   325                                          \
          \                              \u2502\r\n\u2502   326 if __name__ == \"\
          __main__\":                                             \u2502\r\n\u2502\
          \ \u2771 327 \u2502   main()                                           \
          \                  \u2502\r\n\u2502   328                              \
          \                                          \u2502\r\n\u2502            \
          \                                                                  \u2502\
          \r\n\u2502 /content/transformers/examples/legacy/token-classification/run_ner.py:262\
          \ in \u2502\r\n\u2502 main                                             \
          \                            \u2502\r\n\u2502                          \
          \                                                    \u2502\r\n\u2502  \
          \ 259 \u2502                                                           \
          \           \u2502\r\n\u2502   260 \u2502   # Training                 \
          \                                        \u2502\r\n\u2502   261 \u2502 \
          \  if training_args.do_train:                                         \u2502\
          \r\n\u2502 \u2771 262 \u2502   \u2502   trainer.train(                 \
          \                                \u2502\r\n\u2502   263 \u2502   \u2502\
          \   \u2502   model_path=model_args.model_name_or_path if os.path.isdir(\
          \ \u2502\r\n\u2502   264 \u2502   \u2502   )                           \
          \                                   \u2502\r\n\u2502   265 \u2502   \u2502\
          \   trainer.save_model()                                           \u2502\
          \r\n\u2502                                                             \
          \                 \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1536\
          \ in      \u2502\r\n\u2502 train                                       \
          \                                 \u2502\r\n\u2502                     \
          \                                                         \u2502\r\n\u2502\
          \   1533 \u2502   \u2502   inner_training_loop = find_executable_batch_size(\
          \             \u2502\r\n\u2502   1534 \u2502   \u2502   \u2502   self._inner_training_loop,\
          \ self._train_batch_size, args.a \u2502\r\n\u2502   1535 \u2502   \u2502\
          \   )                                                             \u2502\
          \r\n\u2502 \u2771 1536 \u2502   \u2502   return inner_training_loop(   \
          \                                \u2502\r\n\u2502   1537 \u2502   \u2502\
          \   \u2502   args=args,                                                \u2502\
          \r\n\u2502   1538 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
          \            \u2502\r\n\u2502   1539 \u2502   \u2502   \u2502   trial=trial,\
          \                                              \u2502\r\n\u2502        \
          \                                                                      \u2502\
          \r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1779\
          \ in      \u2502\r\n\u2502 _inner_training_loop                        \
          \                                 \u2502\r\n\u2502                     \
          \                                                         \u2502\r\n\u2502\
          \   1776 \u2502   \u2502   \u2502   \u2502   rng_to_sync = True        \
          \                            \u2502\r\n\u2502   1777 \u2502   \u2502   \u2502\
          \                                                             \u2502\r\n\
          \u2502   1778 \u2502   \u2502   \u2502   step = -1                     \
          \                            \u2502\r\n\u2502 \u2771 1779 \u2502   \u2502\
          \   \u2502   for step, inputs in enumerate(epoch_iterator):            \u2502\
          \r\n\u2502   1780 \u2502   \u2502   \u2502   \u2502   total_batched_samples\
          \ += 1                            \u2502\r\n\u2502   1781 \u2502   \u2502\
          \   \u2502   \u2502   if rng_to_sync:                                  \
          \     \u2502\r\n\u2502   1782 \u2502   \u2502   \u2502   \u2502   \u2502\
          \   self._load_rng_state(resume_from_checkpoint)      \u2502\r\n\u2502 \
          \                                                                      \
          \       \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py:377\
          \ in     \u2502\r\n\u2502 __iter__                                     \
          \                                \u2502\r\n\u2502                      \
          \                                                        \u2502\r\n\u2502\
          \   374 \u2502   \u2502   dataloader_iter = super().__iter__()         \
          \                  \u2502\r\n\u2502   375 \u2502   \u2502   # We iterate\
          \ one batch ahead to check when we are at the end   \u2502\r\n\u2502   376\
          \ \u2502   \u2502   try:                                               \
          \            \u2502\r\n\u2502 \u2771 377 \u2502   \u2502   \u2502   current_batch\
          \ = next(dataloader_iter)                      \u2502\r\n\u2502   378 \u2502\
          \   \u2502   except StopIteration:                                     \
          \     \u2502\r\n\u2502   379 \u2502   \u2502   \u2502   yield          \
          \                                            \u2502\r\n\u2502   380    \
          \                                                                    \u2502\
          \r\n\u2502                                                             \
          \                 \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633\
          \   \u2502\r\n\u2502 in __next__                                       \
          \                           \u2502\r\n\u2502                           \
          \                                                   \u2502\r\n\u2502   \
          \ 630 \u2502   \u2502   \u2502   if self._sampler_iter is None:        \
          \                    \u2502\r\n\u2502    631 \u2502   \u2502   \u2502  \
          \ \u2502   # TODO(https://github.com/pytorch/pytorch/issues/7675 \u2502\r\
          \n\u2502    632 \u2502   \u2502   \u2502   \u2502   self._reset()  # type:\
          \ ignore[call-arg]               \u2502\r\n\u2502 \u2771  633 \u2502   \u2502\
          \   \u2502   data = self._next_data()                                  \u2502\
          \r\n\u2502    634 \u2502   \u2502   \u2502   self._num_yielded += 1    \
          \                                \u2502\r\n\u2502    635 \u2502   \u2502\
          \   \u2502   if self._dataset_kind == _DatasetKind.Iterable and \\     \
          \ \u2502\r\n\u2502    636 \u2502   \u2502   \u2502   \u2502   \u2502   self._IterableDataset_len_called\
          \ is not None and  \u2502\r\n\u2502                                    \
          \                                          \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:677\
          \   \u2502\r\n\u2502 in _next_data                                     \
          \                           \u2502\r\n\u2502                           \
          \                                                   \u2502\r\n\u2502   \
          \ 674 \u2502                                                           \
          \          \u2502\r\n\u2502    675 \u2502   def _next_data(self):      \
          \                                       \u2502\r\n\u2502    676 \u2502 \
          \  \u2502   index = self._next_index()  # may raise StopIteration      \
          \   \u2502\r\n\u2502 \u2771  677 \u2502   \u2502   data = self._dataset_fetcher.fetch(index)\
          \  # may raise StopIt \u2502\r\n\u2502    678 \u2502   \u2502   if self._pin_memory:\
          \                                          \u2502\r\n\u2502    679 \u2502\
          \   \u2502   \u2502   data = _utils.pin_memory.pin_memory(data, self._pin_memor\
          \ \u2502\r\n\u2502    680 \u2502   \u2502   return data                \
          \                                   \u2502\r\n\u2502                   \
          \                                                           \u2502\r\n\u2502\
          \ /usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:54\
          \  \u2502\r\n\u2502 in fetch                                           \
          \                          \u2502\r\n\u2502                            \
          \                                                  \u2502\r\n\u2502   51\
          \ \u2502   \u2502   \u2502   \u2502   data = [self.dataset[idx] for idx\
          \ in possibly_batched_i \u2502\r\n\u2502   52 \u2502   \u2502   else:  \
          \                                                         \u2502\r\n\u2502\
          \   53 \u2502   \u2502   \u2502   data = self.dataset[possibly_batched_index]\
          \                 \u2502\r\n\u2502 \u2771 54 \u2502   \u2502   return self.collate_fn(data)\
          \                                    \u2502\r\n\u2502   55             \
          \                                                            \u2502\r\n\u2502\
          \                                                                      \
          \        \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py:698\
          \ in \u2502\r\n\u2502 __call__                                         \
          \                            \u2502\r\n\u2502                          \
          \                                                    \u2502\r\n\u2502  \
          \ 695 \u2502                                                           \
          \           \u2502\r\n\u2502   696 \u2502   def __call__(self, features:\
          \ List[dict]):                          \u2502\r\n\u2502   697 \u2502  \
          \ \u2502   features = [self._remove_columns(feature) for feature in featu\
          \ \u2502\r\n\u2502 \u2771 698 \u2502   \u2502   return self.data_collator(features)\
          \                            \u2502\r\n\u2502   699                    \
          \                                                    \u2502\r\n\u2502  \
          \                                                                      \
          \      \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:7\
          \ \u2502\r\n\u2502 0 in default_data_collator                          \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502     67\
          \ \u2502   # on the whole batch.                                       \
          \      \u2502\r\n\u2502     68 \u2502                                  \
          \                                   \u2502\r\n\u2502     69 \u2502   if\
          \ return_tensors == \"pt\":                                        \u2502\
          \r\n\u2502 \u2771   70 \u2502   \u2502   return torch_default_data_collator(features)\
          \                  \u2502\r\n\u2502     71 \u2502   elif return_tensors\
          \ == \"tf\":                                      \u2502\r\n\u2502     72\
          \ \u2502   \u2502   return tf_default_data_collator(features)          \
          \           \u2502\r\n\u2502     73 \u2502   elif return_tensors == \"np\"\
          :                                      \u2502\r\n\u2502                \
          \                                                              \u2502\r\n\
          \u2502 /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1\
          \ \u2502\r\n\u2502 36 in torch_default_data_collator                   \
          \                         \u2502\r\n\u2502                             \
          \                                                 \u2502\r\n\u2502    133\
          \ \u2502   \u2502   \u2502   elif isinstance(v, np.ndarray):           \
          \                \u2502\r\n\u2502    134 \u2502   \u2502   \u2502   \u2502\
          \   batch[k] = torch.tensor(np.stack([f[k] for f in featu \u2502\r\n\u2502\
          \    135 \u2502   \u2502   \u2502   else:                              \
          \                       \u2502\r\n\u2502 \u2771  136 \u2502   \u2502   \u2502\
          \   \u2502   batch[k] = torch.tensor([f[k] for f in features])     \u2502\
          \r\n\u2502    137 \u2502                                               \
          \                      \u2502\r\n\u2502    138 \u2502   return batch   \
          \                                                   \u2502\r\n\u2502   \
          \ 139                                                                  \
          \     \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nRuntimeError:\
          \ Could not infer dtype of NoneType"
        updatedAt: '2023-06-28T12:02:33.455Z'
      numEdits: 0
      reactions: []
    id: 649c215907bdee1b7e5d56ee
    type: comment
  author: tekeshwarhirwani
  content: "Hi I was using run_ner.py script It showed an error that it doesn't support\
    \ model which have slow tokenizer and then they suggested that try to run old\
    \ run_ner.py script which is this (https://github.com/huggingface/transformers/tree/main/examples/legacy/token-classification),\
    \ Now I am getting an error \r\n\r\n/content/transformers/examples/legacy/token-classification/run_ner.py:327\
    \ in \u2502\r\n\u2502 <module>                                               \
    \                      \u2502\r\n\u2502                                      \
    \                                        \u2502\r\n\u2502   324              \
    \                                                          \u2502\r\n\u2502  \
    \ 325                                                                        \u2502\
    \r\n\u2502   326 if __name__ == \"__main__\":                                \
    \             \u2502\r\n\u2502 \u2771 327 \u2502   main()                    \
    \                                         \u2502\r\n\u2502   328             \
    \                                                           \u2502\r\n\u2502 \
    \                                                                            \
    \ \u2502\r\n\u2502 /content/transformers/examples/legacy/token-classification/run_ner.py:262\
    \ in \u2502\r\n\u2502 main                                                   \
    \                      \u2502\r\n\u2502                                      \
    \                                        \u2502\r\n\u2502   259 \u2502       \
    \                                                               \u2502\r\n\u2502\
    \   260 \u2502   # Training                                                  \
    \       \u2502\r\n\u2502   261 \u2502   if training_args.do_train:           \
    \                              \u2502\r\n\u2502 \u2771 262 \u2502   \u2502   trainer.train(\
    \                                                 \u2502\r\n\u2502   263 \u2502\
    \   \u2502   \u2502   model_path=model_args.model_name_or_path if os.path.isdir(\
    \ \u2502\r\n\u2502   264 \u2502   \u2502   )                                 \
    \                             \u2502\r\n\u2502   265 \u2502   \u2502   trainer.save_model()\
    \                                           \u2502\r\n\u2502                 \
    \                                                             \u2502\r\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1536 in    \
    \  \u2502\r\n\u2502 train                                                    \
    \                    \u2502\r\n\u2502                                        \
    \                                      \u2502\r\n\u2502   1533 \u2502   \u2502\
    \   inner_training_loop = find_executable_batch_size(             \u2502\r\n\u2502\
    \   1534 \u2502   \u2502   \u2502   self._inner_training_loop, self._train_batch_size,\
    \ args.a \u2502\r\n\u2502   1535 \u2502   \u2502   )                         \
    \                                    \u2502\r\n\u2502 \u2771 1536 \u2502   \u2502\
    \   return inner_training_loop(                                   \u2502\r\n\u2502\
    \   1537 \u2502   \u2502   \u2502   args=args,                               \
    \                 \u2502\r\n\u2502   1538 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,\
    \            \u2502\r\n\u2502   1539 \u2502   \u2502   \u2502   trial=trial, \
    \                                             \u2502\r\n\u2502               \
    \                                                               \u2502\r\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1779 in    \
    \  \u2502\r\n\u2502 _inner_training_loop                                     \
    \                    \u2502\r\n\u2502                                        \
    \                                      \u2502\r\n\u2502   1776 \u2502   \u2502\
    \   \u2502   \u2502   rng_to_sync = True                                    \u2502\
    \r\n\u2502   1777 \u2502   \u2502   \u2502                                   \
    \                          \u2502\r\n\u2502   1778 \u2502   \u2502   \u2502  \
    \ step = -1                                                 \u2502\r\n\u2502 \u2771\
    \ 1779 \u2502   \u2502   \u2502   for step, inputs in enumerate(epoch_iterator):\
    \            \u2502\r\n\u2502   1780 \u2502   \u2502   \u2502   \u2502   total_batched_samples\
    \ += 1                            \u2502\r\n\u2502   1781 \u2502   \u2502   \u2502\
    \   \u2502   if rng_to_sync:                                       \u2502\r\n\u2502\
    \   1782 \u2502   \u2502   \u2502   \u2502   \u2502   self._load_rng_state(resume_from_checkpoint)\
    \      \u2502\r\n\u2502                                                      \
    \                        \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py:377\
    \ in     \u2502\r\n\u2502 __iter__                                           \
    \                          \u2502\r\n\u2502                                  \
    \                                            \u2502\r\n\u2502   374 \u2502   \u2502\
    \   dataloader_iter = super().__iter__()                           \u2502\r\n\u2502\
    \   375 \u2502   \u2502   # We iterate one batch ahead to check when we are at\
    \ the end   \u2502\r\n\u2502   376 \u2502   \u2502   try:                    \
    \                                       \u2502\r\n\u2502 \u2771 377 \u2502   \u2502\
    \   \u2502   current_batch = next(dataloader_iter)                      \u2502\
    \r\n\u2502   378 \u2502   \u2502   except StopIteration:                     \
    \                     \u2502\r\n\u2502   379 \u2502   \u2502   \u2502   yield\
    \                                                      \u2502\r\n\u2502   380\
    \                                                                        \u2502\
    \r\n\u2502                                                                   \
    \           \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633\
    \   \u2502\r\n\u2502 in __next__                                             \
    \                     \u2502\r\n\u2502                                       \
    \                                       \u2502\r\n\u2502    630 \u2502   \u2502\
    \   \u2502   if self._sampler_iter is None:                            \u2502\r\
    \n\u2502    631 \u2502   \u2502   \u2502   \u2502   # TODO(https://github.com/pytorch/pytorch/issues/7675\
    \ \u2502\r\n\u2502    632 \u2502   \u2502   \u2502   \u2502   self._reset()  #\
    \ type: ignore[call-arg]               \u2502\r\n\u2502 \u2771  633 \u2502   \u2502\
    \   \u2502   data = self._next_data()                                  \u2502\r\
    \n\u2502    634 \u2502   \u2502   \u2502   self._num_yielded += 1            \
    \                        \u2502\r\n\u2502    635 \u2502   \u2502   \u2502   if\
    \ self._dataset_kind == _DatasetKind.Iterable and \\      \u2502\r\n\u2502   \
    \ 636 \u2502   \u2502   \u2502   \u2502   \u2502   self._IterableDataset_len_called\
    \ is not None and  \u2502\r\n\u2502                                          \
    \                                    \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:677\
    \   \u2502\r\n\u2502 in _next_data                                           \
    \                     \u2502\r\n\u2502                                       \
    \                                       \u2502\r\n\u2502    674 \u2502       \
    \                                                              \u2502\r\n\u2502\
    \    675 \u2502   def _next_data(self):                                      \
    \       \u2502\r\n\u2502    676 \u2502   \u2502   index = self._next_index() \
    \ # may raise StopIteration         \u2502\r\n\u2502 \u2771  677 \u2502   \u2502\
    \   data = self._dataset_fetcher.fetch(index)  # may raise StopIt \u2502\r\n\u2502\
    \    678 \u2502   \u2502   if self._pin_memory:                              \
    \            \u2502\r\n\u2502    679 \u2502   \u2502   \u2502   data = _utils.pin_memory.pin_memory(data,\
    \ self._pin_memor \u2502\r\n\u2502    680 \u2502   \u2502   return data      \
    \                                             \u2502\r\n\u2502               \
    \                                                               \u2502\r\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:54\
    \  \u2502\r\n\u2502 in fetch                                                 \
    \                    \u2502\r\n\u2502                                        \
    \                                      \u2502\r\n\u2502   51 \u2502   \u2502 \
    \  \u2502   \u2502   data = [self.dataset[idx] for idx in possibly_batched_i \u2502\
    \r\n\u2502   52 \u2502   \u2502   else:                                      \
    \                     \u2502\r\n\u2502   53 \u2502   \u2502   \u2502   data =\
    \ self.dataset[possibly_batched_index]                 \u2502\r\n\u2502 \u2771\
    \ 54 \u2502   \u2502   return self.collate_fn(data)                          \
    \          \u2502\r\n\u2502   55                                             \
    \                            \u2502\r\n\u2502                                \
    \                                              \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer_utils.py:698\
    \ in \u2502\r\n\u2502 __call__                                               \
    \                      \u2502\r\n\u2502                                      \
    \                                        \u2502\r\n\u2502   695 \u2502       \
    \                                                               \u2502\r\n\u2502\
    \   696 \u2502   def __call__(self, features: List[dict]):                   \
    \       \u2502\r\n\u2502   697 \u2502   \u2502   features = [self._remove_columns(feature)\
    \ for feature in featu \u2502\r\n\u2502 \u2771 698 \u2502   \u2502   return self.data_collator(features)\
    \                            \u2502\r\n\u2502   699                          \
    \                                              \u2502\r\n\u2502              \
    \                                                                \u2502\r\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:7\
    \ \u2502\r\n\u2502 0 in default_data_collator                                \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502     67 \u2502   # on the\
    \ whole batch.                                             \u2502\r\n\u2502  \
    \   68 \u2502                                                                \
    \     \u2502\r\n\u2502     69 \u2502   if return_tensors == \"pt\":          \
    \                              \u2502\r\n\u2502 \u2771   70 \u2502   \u2502  \
    \ return torch_default_data_collator(features)                  \u2502\r\n\u2502\
    \     71 \u2502   elif return_tensors == \"tf\":                             \
    \         \u2502\r\n\u2502     72 \u2502   \u2502   return tf_default_data_collator(features)\
    \                     \u2502\r\n\u2502     73 \u2502   elif return_tensors ==\
    \ \"np\":                                      \u2502\r\n\u2502              \
    \                                                                \u2502\r\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1\
    \ \u2502\r\n\u2502 36 in torch_default_data_collator                         \
    \                   \u2502\r\n\u2502                                         \
    \                                     \u2502\r\n\u2502    133 \u2502   \u2502\
    \   \u2502   elif isinstance(v, np.ndarray):                           \u2502\r\
    \n\u2502    134 \u2502   \u2502   \u2502   \u2502   batch[k] = torch.tensor(np.stack([f[k]\
    \ for f in featu \u2502\r\n\u2502    135 \u2502   \u2502   \u2502   else:    \
    \                                                 \u2502\r\n\u2502 \u2771  136\
    \ \u2502   \u2502   \u2502   \u2502   batch[k] = torch.tensor([f[k] for f in features])\
    \     \u2502\r\n\u2502    137 \u2502                                         \
    \                            \u2502\r\n\u2502    138 \u2502   return batch   \
    \                                                   \u2502\r\n\u2502    139  \
    \                                                                     \u2502\r\
    \n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u256F\r\nRuntimeError: Could not infer dtype of NoneType"
  created_at: 2023-06-28 11:02:33+00:00
  edited: false
  hidden: false
  id: 649c215907bdee1b7e5d56ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: microsoft/biogpt
repo_type: model
status: open
target_branch: null
title: Unable to fine-tune biogpt for ner task
