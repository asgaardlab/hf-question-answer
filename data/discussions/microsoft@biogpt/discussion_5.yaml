!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Seodam
conflicting_files: null
created_at: 2023-01-27 06:55:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02a9812c76d7b8fed93f1cf729ab1632.svg
      fullname: Sungkyun Im
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Seodam
      type: user
    createdAt: '2023-01-27T06:55:41.000Z'
    data:
      edited: false
      editors:
      - Seodam
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02a9812c76d7b8fed93f1cf729ab1632.svg
          fullname: Sungkyun Im
          isHf: false
          isPro: false
          name: Seodam
          type: user
        html: "<p>Environment: Colab</p>\n<p>I want to get diagnosis string from BioGPT.<br>I\
          \ followed BioGPT official docs and tried to decode outputs.</p>\n<pre><code>from\
          \ transformers import AutoTokenizer, BioGptModel\nimport torch\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\nmodel = BioGptModel.from_pretrained(\"\
          microsoft/biogpt\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"\
          pt\")\noutputs = model(**inputs)\n\nlast_hidden_states = outputs.last_hidden_state\n\
          \nprint(tokenizer.decode(outputs[0])\n</code></pre>\n<p>But I got an error.</p>\n\
          <pre><code>  Some weights of the model checkpoint at microsoft/biogpt were\
          \ not used when initializing BioGptModel: ['output_projection.weight']\n\
          - This IS expected if you are initializing BioGptModel from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\n- This IS NOT expected if you are initializing BioGptModel from\
          \ the checkpoint of a model that you expect to be exactly identical (initializing\
          \ a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\n---------------------------------------------------------------------------\n\
          TypeError                                 Traceback (most recent call last)\n\
          &lt;ipython-input-29-e9fe28e451b8&gt; in &lt;module&gt;\n      5 outputs\
          \ = model(**inputs)\n      6 \n----&gt; 7 print(tokenizer.decode(outputs[0]))\n\
          \n2 frames\n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\
          \ in convert_ids_to_tokens(self, ids, skip_special_tokens)\n    904    \
          \     tokens = []\n    905         for index in ids:\n--&gt; 906       \
          \      index = int(index)\n    907             if skip_special_tokens and\
          \ index in self.all_special_ids:\n    908                 continue\n\nTypeError:\
          \ int() argument must be a string, a bytes-like object or a number, not\
          \ 'list'\n</code></pre>\n<p>I thought that <code>AutoTokenizer</code> is\
          \ cause of this error, then changed it to <code>BioGptTokenizer</code>.<br>But\
          \ result was same.</p>\n<p>How can i fix?</p>\n"
        raw: "Environment: Colab\r\n\r\nI want to get diagnosis string from BioGPT.\
          \ \r\nI followed BioGPT official docs and tried to decode outputs.\r\n```\r\
          \nfrom transformers import AutoTokenizer, BioGptModel\r\nimport torch\r\n\
          \r\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\r\n\
          model = BioGptModel.from_pretrained(\"microsoft/biogpt\")\r\n\r\ninputs\
          \ = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\r\noutputs\
          \ = model(**inputs)\r\n\r\nlast_hidden_states = outputs.last_hidden_state\r\
          \n\r\nprint(tokenizer.decode(outputs[0])\r\n```\r\nBut I got an error.\r\
          \n```\r\n  Some weights of the model checkpoint at microsoft/biogpt were\
          \ not used when initializing BioGptModel: ['output_projection.weight']\r\
          \n- This IS expected if you are initializing BioGptModel from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\r\n- This IS NOT expected if you are initializing BioGptModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\r\n---------------------------------------------------------------------------\r\
          \nTypeError                                 Traceback (most recent call\
          \ last)\r\n<ipython-input-29-e9fe28e451b8> in <module>\r\n      5 outputs\
          \ = model(**inputs)\r\n      6 \r\n----> 7 print(tokenizer.decode(outputs[0]))\r\
          \n\r\n2 frames\r\n/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py\
          \ in convert_ids_to_tokens(self, ids, skip_special_tokens)\r\n    904  \
          \       tokens = []\r\n    905         for index in ids:\r\n--> 906    \
          \         index = int(index)\r\n    907             if skip_special_tokens\
          \ and index in self.all_special_ids:\r\n    908                 continue\r\
          \n\r\nTypeError: int() argument must be a string, a bytes-like object or\
          \ a number, not 'list'\r\n```\r\nI thought that `AutoTokenizer` is cause\
          \ of this error, then changed it to `BioGptTokenizer`. \r\nBut result was\
          \ same.\r\n\r\nHow can i fix?"
        updatedAt: '2023-01-27T06:55:41.301Z'
      numEdits: 0
      reactions: []
    id: 63d3756dc20657d24350319f
    type: comment
  author: Seodam
  content: "Environment: Colab\r\n\r\nI want to get diagnosis string from BioGPT.\
    \ \r\nI followed BioGPT official docs and tried to decode outputs.\r\n```\r\n\
    from transformers import AutoTokenizer, BioGptModel\r\nimport torch\r\n\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\r\nmodel = BioGptModel.from_pretrained(\"\
    microsoft/biogpt\")\r\n\r\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"\
    pt\")\r\noutputs = model(**inputs)\r\n\r\nlast_hidden_states = outputs.last_hidden_state\r\
    \n\r\nprint(tokenizer.decode(outputs[0])\r\n```\r\nBut I got an error.\r\n```\r\
    \n  Some weights of the model checkpoint at microsoft/biogpt were not used when\
    \ initializing BioGptModel: ['output_projection.weight']\r\n- This IS expected\
    \ if you are initializing BioGptModel from the checkpoint of a model trained on\
    \ another task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\r\n- This IS NOT expected if you are\
    \ initializing BioGptModel from the checkpoint of a model that you expect to be\
    \ exactly identical (initializing a BertForSequenceClassification model from a\
    \ BertForSequenceClassification model).\r\n---------------------------------------------------------------------------\r\
    \nTypeError                                 Traceback (most recent call last)\r\
    \n<ipython-input-29-e9fe28e451b8> in <module>\r\n      5 outputs = model(**inputs)\r\
    \n      6 \r\n----> 7 print(tokenizer.decode(outputs[0]))\r\n\r\n2 frames\r\n\
    /usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils.py in convert_ids_to_tokens(self,\
    \ ids, skip_special_tokens)\r\n    904         tokens = []\r\n    905        \
    \ for index in ids:\r\n--> 906             index = int(index)\r\n    907     \
    \        if skip_special_tokens and index in self.all_special_ids:\r\n    908\
    \                 continue\r\n\r\nTypeError: int() argument must be a string,\
    \ a bytes-like object or a number, not 'list'\r\n```\r\nI thought that `AutoTokenizer`\
    \ is cause of this error, then changed it to `BioGptTokenizer`. \r\nBut result\
    \ was same.\r\n\r\nHow can i fix?"
  created_at: 2023-01-27 06:55:41+00:00
  edited: false
  hidden: false
  id: 63d3756dc20657d24350319f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-01-27T08:41:08.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>You can''t decode the last hidden states, as these are embeddings. You
          can only decode integers back to text.</p>

          <p>See the "beam search decoding" example in the model card.</p>

          '
        raw: 'Hi,


          You can''t decode the last hidden states, as these are embeddings. You can
          only decode integers back to text.


          See the "beam search decoding" example in the model card.'
        updatedAt: '2023-01-27T08:41:08.284Z'
      numEdits: 0
      reactions: []
    id: 63d38e2422d51714e2aafeef
    type: comment
  author: nielsr
  content: 'Hi,


    You can''t decode the last hidden states, as these are embeddings. You can only
    decode integers back to text.


    See the "beam search decoding" example in the model card.'
  created_at: 2023-01-27 08:41:08+00:00
  edited: false
  hidden: false
  id: 63d38e2422d51714e2aafeef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-03T08:59:46.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Closing this issue as it''s been resolved.</p>

          '
        raw: Closing this issue as it's been resolved.
        updatedAt: '2023-02-03T08:59:46.059Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63dccd0202895390662c08d6
    id: 63dccd0202895390662c08d5
    type: comment
  author: nielsr
  content: Closing this issue as it's been resolved.
  created_at: 2023-02-03 08:59:46+00:00
  edited: false
  hidden: false
  id: 63dccd0202895390662c08d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-03T08:59:46.000Z'
    data:
      status: closed
    id: 63dccd0202895390662c08d6
    type: status-change
  author: nielsr
  created_at: 2023-02-03 08:59:46+00:00
  id: 63dccd0202895390662c08d6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: microsoft/biogpt
repo_type: model
status: closed
target_branch: null
title: 'I can''t decode bioGPT output. '
