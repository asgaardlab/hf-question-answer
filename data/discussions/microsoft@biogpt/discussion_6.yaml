!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kentwait
conflicting_files: null
created_at: 2023-02-02 07:51:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
      fullname: Kent Kawashima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kentwait
      type: user
    createdAt: '2023-02-02T07:51:42.000Z'
    data:
      edited: false
      editors:
      - kentwait
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
          fullname: Kent Kawashima
          isHf: false
          isPro: false
          name: kentwait
          type: user
        html: '<p>In the model card, the sample code uses <code>BioGptTokenizer</code>
          instead of the usual <code>transformers.AutoTokenizer</code>.<br>Comparing
          the encoded inputs of the two tokenizers, the IDs seem to be the same for
          the texts I used.</p>

          <p>The model instantiated from <code>BioGptForCausalLM.from_pretrained("microsoft/biogpt")</code>
          takes the encoded input from either tokenize no problem but the output keys
          are different.</p>

          <p>For  <code>AutoTokenizer</code> I get <code>odict_keys([''last_hidden_state'',
          ''past_key_values''])</code>, but for  BioGptTokenizer I get <code>odict_keys([''logits'',
          ''past_key_values''])</code>.</p>

          <p>Are <code>last_hidden_state</code> and <code>logits</code> token embeddings
          or something else?<br>I get <code>torch.Size([1, n, 1024])</code> <code>torch.Size([1,
          n, 42384])</code> where n is the number of input tokens.</p>

          '
        raw: "In the model card, the sample code uses `BioGptTokenizer` instead of\
          \ the usual `transformers.AutoTokenizer`.\r\nComparing the encoded inputs\
          \ of the two tokenizers, the IDs seem to be the same for the texts I used.\r\
          \n\r\nThe model instantiated from `BioGptForCausalLM.from_pretrained(\"\
          microsoft/biogpt\")` takes the encoded input from either tokenize no problem\
          \ but the output keys are different.\r\n\r\nFor  `AutoTokenizer` I get `odict_keys(['last_hidden_state',\
          \ 'past_key_values'])`, but for  BioGptTokenizer I get `odict_keys(['logits',\
          \ 'past_key_values'])`.\r\n\r\nAre `last_hidden_state` and `logits` token\
          \ embeddings or something else? \r\nI get `torch.Size([1, n, 1024])` `torch.Size([1,\
          \ n, 42384])` where n is the number of input tokens."
        updatedAt: '2023-02-02T07:51:42.349Z'
      numEdits: 0
      reactions: []
    id: 63db6b8e9fb38bae84a94e75
    type: comment
  author: kentwait
  content: "In the model card, the sample code uses `BioGptTokenizer` instead of the\
    \ usual `transformers.AutoTokenizer`.\r\nComparing the encoded inputs of the two\
    \ tokenizers, the IDs seem to be the same for the texts I used.\r\n\r\nThe model\
    \ instantiated from `BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")`\
    \ takes the encoded input from either tokenize no problem but the output keys\
    \ are different.\r\n\r\nFor  `AutoTokenizer` I get `odict_keys(['last_hidden_state',\
    \ 'past_key_values'])`, but for  BioGptTokenizer I get `odict_keys(['logits',\
    \ 'past_key_values'])`.\r\n\r\nAre `last_hidden_state` and `logits` token embeddings\
    \ or something else? \r\nI get `torch.Size([1, n, 1024])` `torch.Size([1, n, 42384])`\
    \ where n is the number of input tokens."
  created_at: 2023-02-02 07:51:42+00:00
  edited: false
  hidden: false
  id: 63db6b8e9fb38bae84a94e75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-02T09:17:44.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>If you use </p>

          <pre><code>from transformers import AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("microsoft/biogpt")

          print(type(tokenizer))

          </code></pre>

          <p>you''ll see that it automatically maps to a <code>BioGptTokenizer</code>.
          So both are equivalent (Auto tokenizer uses the fast tokenizer by default,
          if there is one).</p>

          <p>Do you have a reproducible code snippet that shows that you get different
          keys in the output dictionary depending on the tokenizer? That shouldn''t
          be the case.</p>

          '
        raw: "Hi,\n\nIf you use \n```\nfrom transformers import AutoTokenizer\n\n\
          tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\nprint(type(tokenizer))\n\
          ```\nyou'll see that it automatically maps to a `BioGptTokenizer`. So both\
          \ are equivalent (Auto tokenizer uses the fast tokenizer by default, if\
          \ there is one).\n\nDo you have a reproducible code snippet that shows that\
          \ you get different keys in the output dictionary depending on the tokenizer?\
          \ That shouldn't be the case."
        updatedAt: '2023-02-02T09:17:44.502Z'
      numEdits: 0
      reactions: []
    id: 63db7fb829a85707bf029425
    type: comment
  author: nielsr
  content: "Hi,\n\nIf you use \n```\nfrom transformers import AutoTokenizer\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\nprint(type(tokenizer))\n\
    ```\nyou'll see that it automatically maps to a `BioGptTokenizer`. So both are\
    \ equivalent (Auto tokenizer uses the fast tokenizer by default, if there is one).\n\
    \nDo you have a reproducible code snippet that shows that you get different keys\
    \ in the output dictionary depending on the tokenizer? That shouldn't be the case."
  created_at: 2023-02-02 09:17:44+00:00
  edited: false
  hidden: false
  id: 63db7fb829a85707bf029425
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
      fullname: Kent Kawashima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kentwait
      type: user
    createdAt: '2023-02-02T14:35:32.000Z'
    data:
      edited: false
      editors:
      - kentwait
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
          fullname: Kent Kawashima
          isHf: false
          isPro: false
          name: kentwait
          type: user
        html: '<p>Yes you are right. In the case with <code>odict_keys([''last_hidden_state'',
          ''past_key_values''])</code> I was loading the bare model <code>BioGptModel.from_pretrained("microsoft/biogpt")</code>
          without the head. I mistakenly thought it was the tokenizer.</p>

          <p>The bare model is sufficient for getting the input embeddings as I don''t
          need it transformed back into the vocabulary. Is my understanding correct?</p>

          '
        raw: 'Yes you are right. In the case with `odict_keys([''last_hidden_state'',
          ''past_key_values''])` I was loading the bare model `BioGptModel.from_pretrained("microsoft/biogpt")`
          without the head. I mistakenly thought it was the tokenizer.


          The bare model is sufficient for getting the input embeddings as I don''t
          need it transformed back into the vocabulary. Is my understanding correct?'
        updatedAt: '2023-02-02T14:35:32.139Z'
      numEdits: 0
      reactions: []
    id: 63dbca348616f0acd3acc0f0
    type: comment
  author: kentwait
  content: 'Yes you are right. In the case with `odict_keys([''last_hidden_state'',
    ''past_key_values''])` I was loading the bare model `BioGptModel.from_pretrained("microsoft/biogpt")`
    without the head. I mistakenly thought it was the tokenizer.


    The bare model is sufficient for getting the input embeddings as I don''t need
    it transformed back into the vocabulary. Is my understanding correct?'
  created_at: 2023-02-02 14:35:32+00:00
  edited: false
  hidden: false
  id: 63dbca348616f0acd3acc0f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-02T14:53:06.000Z'
    data:
      edited: true
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Yes, <code>BioGptModel</code> will only output the "last hidden
          states" (also called embeddings) for each of the text tokens (<code>input_ids</code>)
          being passed through it.</p>

          <p><code>BioGptForCausalLM</code> on the other hand will add a classifier
          head on top of <code>BioGptModel</code> that maps those last hidden states
          to token indices.</p>

          '
        raw: 'Yes, `BioGptModel` will only output the "last hidden states" (also called
          embeddings) for each of the text tokens (`input_ids`) being passed through
          it.


          `BioGptForCausalLM` on the other hand will add a classifier head on top
          of `BioGptModel` that maps those last hidden states to token indices.'
        updatedAt: '2023-02-02T14:53:24.328Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - kentwait
        - molspace
    id: 63dbce52ef6ecf800ef3bf0d
    type: comment
  author: nielsr
  content: 'Yes, `BioGptModel` will only output the "last hidden states" (also called
    embeddings) for each of the text tokens (`input_ids`) being passed through it.


    `BioGptForCausalLM` on the other hand will add a classifier head on top of `BioGptModel`
    that maps those last hidden states to token indices.'
  created_at: 2023-02-02 14:53:06+00:00
  edited: true
  hidden: false
  id: 63dbce52ef6ecf800ef3bf0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
      fullname: Kent Kawashima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kentwait
      type: user
    createdAt: '2023-02-02T15:09:43.000Z'
    data:
      edited: false
      editors:
      - kentwait
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
          fullname: Kent Kawashima
          isHf: false
          isPro: false
          name: kentwait
          type: user
        html: '<p>Thanks for clearing it up!</p>

          '
        raw: Thanks for clearing it up!
        updatedAt: '2023-02-02T15:09:43.582Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63dbd23729a85707bf356513
    id: 63dbd23729a85707bf356512
    type: comment
  author: kentwait
  content: Thanks for clearing it up!
  created_at: 2023-02-02 15:09:43+00:00
  edited: false
  hidden: false
  id: 63dbd23729a85707bf356512
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1675324743810-62622a9418176506fab63c31.jpeg?w=200&h=200&f=face
      fullname: Kent Kawashima
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kentwait
      type: user
    createdAt: '2023-02-02T15:09:43.000Z'
    data:
      status: closed
    id: 63dbd23729a85707bf356513
    type: status-change
  author: kentwait
  created_at: 2023-02-02 15:09:43+00:00
  id: 63dbd23729a85707bf356513
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: microsoft/biogpt
repo_type: model
status: closed
target_branch: null
title: BioGptTokenizer vs AutoTokenizer
