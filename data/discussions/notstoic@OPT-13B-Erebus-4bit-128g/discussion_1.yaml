!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ElevenGames
conflicting_files: null
created_at: 2023-04-13 10:54:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/790fa7c2a6bf34477c947bb97d28971d.svg
      fullname: "Tobias H\xF6rl"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ElevenGames
      type: user
    createdAt: '2023-04-13T11:54:17.000Z'
    data:
      edited: false
      editors:
      - ElevenGames
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/790fa7c2a6bf34477c947bb97d28971d.svg
          fullname: "Tobias H\xF6rl"
          isHf: false
          isPro: false
          name: ElevenGames
          type: user
        html: '<p>trying to use this with windows and oobabooga gui, i get the following
          error:<br>...<br>return torch.layer_norm(input, normalized_shape, weight,
          bias, eps, torch.backends.cudnn.enabled)<br>RuntimeError: expected scalar
          type Float but found Half</p>

          <p>I''d appreciate any information of how to possibly make it work. :D</p>

          '
        raw: "trying to use this with windows and oobabooga gui, i get the following\
          \ error:\r\n...\r\nreturn torch.layer_norm(input, normalized_shape, weight,\
          \ bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: expected scalar\
          \ type Float but found Half\r\n\r\nI'd appreciate any information of how\
          \ to possibly make it work. :D"
        updatedAt: '2023-04-13T11:54:17.273Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F91D"
        users:
        - m0-ch4
        - wurm
        - cosmic-thrum
    id: 6437ed69961bb61e463adfb8
    type: comment
  author: ElevenGames
  content: "trying to use this with windows and oobabooga gui, i get the following\
    \ error:\r\n...\r\nreturn torch.layer_norm(input, normalized_shape, weight, bias,\
    \ eps, torch.backends.cudnn.enabled)\r\nRuntimeError: expected scalar type Float\
    \ but found Half\r\n\r\nI'd appreciate any information of how to possibly make\
    \ it work. :D"
  created_at: 2023-04-13 10:54:17+00:00
  edited: false
  hidden: false
  id: 6437ed69961bb61e463adfb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-04-14T12:58:03.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>I get this error using the autograd 4bit.py for inference. Using
          plain GPTQ it generates very slowly but it generates.</p>

          <p>Result is worse than 8bit <code>Output generated in 71.59 seconds (0.31
          tokens/s, 22 tokens, context 498, seed 300042613)</code></p>

          '
        raw: 'I get this error using the autograd 4bit.py for inference. Using plain
          GPTQ it generates very slowly but it generates.


          Result is worse than 8bit ```Output generated in 71.59 seconds (0.31 tokens/s,
          22 tokens, context 498, seed 300042613)```'
        updatedAt: '2023-04-14T12:58:03.542Z'
      numEdits: 0
      reactions: []
    id: 64394ddb3da4490578d541ba
    type: comment
  author: autobots
  content: 'I get this error using the autograd 4bit.py for inference. Using plain
    GPTQ it generates very slowly but it generates.


    Result is worse than 8bit ```Output generated in 71.59 seconds (0.31 tokens/s,
    22 tokens, context 498, seed 300042613)```'
  created_at: 2023-04-14 11:58:03+00:00
  edited: false
  hidden: false
  id: 64394ddb3da4490578d541ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666261512224-noauth.png?w=200&h=200&f=face
      fullname: madwurmz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wurm
      type: user
    createdAt: '2023-05-15T00:27:59.000Z'
    data:
      edited: true
      editors:
      - wurm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666261512224-noauth.png?w=200&h=200&f=face
          fullname: madwurmz
          isHf: false
          isPro: false
          name: wurm
          type: user
        html: '<p>got this error too,  windows and updated oobagobaa:<br>full error
          log:<br>Traceback (most recent call last):<br>  File "D:\AI\OOBABOOGA\text-generation-webui\modules\text_generation.py",
          line 246, in generate_reply_HF<br>    output = shared.model.generate(**generate_params)[0]<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\transformers\models\opt\modeling_opt.py",
          line 938, in forward<br>    outputs = self.model.decoder(<br>  File "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\transformers\models\opt\modeling_opt.py",
          line 704, in forward<br>    layer_outputs = decoder_layer(<br>  File "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\transformers\models\opt\modeling_opt.py",
          line 326, in forward<br>    hidden_states = self.self_attn_layer_norm(hidden_states)<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\nn\modules\normalization.py",
          line 190, in forward<br>    return F.layer_norm(<br>  File "D:\AI\OOBABOOGA\installer_files\env\lib\site-packages\torch\nn\functional.py",
          line 2515, in layer_norm<br>    return torch.layer_norm(input, normalized_shape,
          weight, bias, eps, torch.backends.cudnn.enabled)<br>RuntimeError: expected
          scalar type Float but found Half<br>Output generated in 0.08 seconds (0.00
          tokens/s, 0 tokens, context 34, seed 292170800)</p>

          <p>Bing says; this error means that there is a mismatch between the data
          types of the input and the expected input for a PyTorch function. In this
          case, the function expects a Float tensor, which is a tensor with 32-bit
          floating-point values, but it receives a Half tensor, which is a tensor
          with 16-bit floating-point values. This can happen when using mixed precision
          training, which is a technique to speed up computation and reduce memory
          usage by using lower-precision data types.</p>

          <p>idk about that..</p>

          <p>Would be fun to see this model work in OobaBooga , curious if there is
          anything I can do about this  :O</p>

          '
        raw: "got this error too,  windows and updated oobagobaa: \nfull error log:\
          \ \nTraceback (most recent call last):\n  File \"D:\\AI\\OOBABOOGA\\text-generation-webui\\\
          modules\\text_generation.py\", line 246, in generate_reply_HF\n    output\
          \ = shared.model.generate(**generate_params)[0]\n  File \"D:\\AI\\OOBABOOGA\\\
          installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1485, in generate\n    return self.sample(\n\
          \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 2524, in sample\n    outputs\
          \ = self(\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\opt\\modeling_opt.py\", line 938, in forward\n   \
          \ outputs = self.model.decoder(\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"D:\\AI\\\
          OOBABOOGA\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
          opt\\modeling_opt.py\", line 704, in forward\n    layer_outputs = decoder_layer(\n\
          \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\opt\\modeling_opt.py\", line 326, in forward\n   \
          \ hidden_states = self.self_attn_layer_norm(hidden_states)\n  File \"D:\\\
          AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
          \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\normalization.py\", line 190, in forward\n    return\
          \ F.layer_norm(\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\functional.py\", line 2515, in layer_norm\n  \
          \  return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n\
          RuntimeError: expected scalar type Float but found Half\nOutput generated\
          \ in 0.08 seconds (0.00 tokens/s, 0 tokens, context 34, seed 292170800)\n\
          \n\nBing says; this error means that there is a mismatch between the data\
          \ types of the input and the expected input for a PyTorch function. In this\
          \ case, the function expects a Float tensor, which is a tensor with 32-bit\
          \ floating-point values, but it receives a Half tensor, which is a tensor\
          \ with 16-bit floating-point values. This can happen when using mixed precision\
          \ training, which is a technique to speed up computation and reduce memory\
          \ usage by using lower-precision data types.\n\nidk about that..\n\nWould\
          \ be fun to see this model work in OobaBooga , curious if there is anything\
          \ I can do about this  :O"
        updatedAt: '2023-05-15T00:29:12.793Z'
      numEdits: 1
      reactions: []
    id: 64617c8f3cc3259ce05ce12f
    type: comment
  author: wurm
  content: "got this error too,  windows and updated oobagobaa: \nfull error log:\
    \ \nTraceback (most recent call last):\n  File \"D:\\AI\\OOBABOOGA\\text-generation-webui\\\
    modules\\text_generation.py\", line 246, in generate_reply_HF\n    output = shared.model.generate(**generate_params)[0]\n\
    \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\torch\\\
    utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
    \ **kwargs)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 1485, in generate\n    return self.sample(\n\
    \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 2524, in sample\n    outputs = self(\n  File \"D:\\\
    AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
    \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\opt\\modeling_opt.py\", line 938, in forward\n    outputs = self.model.decoder(\n\
    \  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\opt\\modeling_opt.py\", line 704, in forward\n    layer_outputs\
    \ = decoder_layer(\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\opt\\modeling_opt.py\", line 326, in forward\n    hidden_states\
    \ = self.self_attn_layer_norm(hidden_states)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
    \    return forward_call(*args, **kwargs)\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 190, in\
    \ forward\n    return F.layer_norm(\n  File \"D:\\AI\\OOBABOOGA\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\functional.py\", line 2515, in layer_norm\n\
    \    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)\n\
    RuntimeError: expected scalar type Float but found Half\nOutput generated in 0.08\
    \ seconds (0.00 tokens/s, 0 tokens, context 34, seed 292170800)\n\n\nBing says;\
    \ this error means that there is a mismatch between the data types of the input\
    \ and the expected input for a PyTorch function. In this case, the function expects\
    \ a Float tensor, which is a tensor with 32-bit floating-point values, but it\
    \ receives a Half tensor, which is a tensor with 16-bit floating-point values.\
    \ This can happen when using mixed precision training, which is a technique to\
    \ speed up computation and reduce memory usage by using lower-precision data types.\n\
    \nidk about that..\n\nWould be fun to see this model work in OobaBooga , curious\
    \ if there is anything I can do about this  :O"
  created_at: 2023-05-14 23:27:59+00:00
  edited: true
  hidden: false
  id: 64617c8f3cc3259ce05ce12f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
      fullname: Autobots
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autobots
      type: user
    createdAt: '2023-05-15T11:04:28.000Z'
    data:
      edited: false
      editors:
      - autobots
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63f26d8a7ddf724fbcc274a0/hoCyIEo1UvOJnqaNOpVAh.jpeg?w=200&h=200&f=face
          fullname: Autobots
          isHf: false
          isPro: false
          name: autobots
          type: user
        html: '<p>I think what can be done about it is encoding another model without
          group size.. but every way I try to work it, the thing generates slow. I
          will test more with a 3090 and see if it''s passable. But there is no reason
          to use this model when the int4 30b models like alpasta/alpacino exist.</p>

          '
        raw: I think what can be done about it is encoding another model without group
          size.. but every way I try to work it, the thing generates slow. I will
          test more with a 3090 and see if it's passable. But there is no reason to
          use this model when the int4 30b models like alpasta/alpacino exist.
        updatedAt: '2023-05-15T11:04:28.724Z'
      numEdits: 0
      reactions: []
    id: 646211bc5eec3c624b005452
    type: comment
  author: autobots
  content: I think what can be done about it is encoding another model without group
    size.. but every way I try to work it, the thing generates slow. I will test more
    with a 3090 and see if it's passable. But there is no reason to use this model
    when the int4 30b models like alpasta/alpacino exist.
  created_at: 2023-05-15 10:04:28+00:00
  edited: false
  hidden: false
  id: 646211bc5eec3c624b005452
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: notstoic/OPT-13B-Erebus-4bit-128g
repo_type: model
status: open
target_branch: null
title: I get expected Float found Half error
