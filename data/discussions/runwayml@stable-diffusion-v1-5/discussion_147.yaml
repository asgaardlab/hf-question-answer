!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KotomiHacker
conflicting_files: null
created_at: 2023-06-20 08:20:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ee22e3cb3798bb10ad9f143ff2503167.svg
      fullname: KotomiDu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KotomiHacker
      type: user
    createdAt: '2023-06-20T09:20:14.000Z'
    data:
      edited: false
      editors:
      - KotomiHacker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7912180423736572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ee22e3cb3798bb10ad9f143ff2503167.svg
          fullname: KotomiDu
          isHf: false
          isPro: false
          name: KotomiHacker
          type: user
        html: '<p>I saw the onnx branch having a better onnx model than I converted.
          The biggest difference is that there are more reduntant nodes around softmax
          by self converting onnx model, which leads to much more memory usage. Check
          the details in the image below.<br>Here is my solution to convert onnx model.<br>script:
          <a rel="nofollow" href="https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py">https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py</a><br>env:
          torch1.13 + onnxruntime 1.14+ diffusers 0.15.1</p>

          <p> Could you share how to convert the model shown in left side?<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/643e8d9ee08e86aec10f9ff9/mci45dThqIZbCZTS2oPup.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/643e8d9ee08e86aec10f9ff9/mci45dThqIZbCZTS2oPup.png"></a></p>

          '
        raw: "I saw the onnx branch having a better onnx model than I converted. The\
          \ biggest difference is that there are more reduntant nodes around softmax\
          \ by self converting onnx model, which leads to much more memory usage.\
          \ Check the details in the image below. \r\nHere is my solution to convert\
          \ onnx model.\r\nscript: https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py\r\
          \nenv: torch1.13 + onnxruntime 1.14+ diffusers 0.15.1\r\n\r\n Could you\
          \ share how to convert the model shown in left side?\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643e8d9ee08e86aec10f9ff9/mci45dThqIZbCZTS2oPup.png)\r\
          \n"
        updatedAt: '2023-06-20T09:20:14.779Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - seasnake
    id: 64916f4e3036b3b084617e9b
    type: comment
  author: KotomiHacker
  content: "I saw the onnx branch having a better onnx model than I converted. The\
    \ biggest difference is that there are more reduntant nodes around softmax by\
    \ self converting onnx model, which leads to much more memory usage. Check the\
    \ details in the image below. \r\nHere is my solution to convert onnx model.\r\
    \nscript: https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py\r\
    \nenv: torch1.13 + onnxruntime 1.14+ diffusers 0.15.1\r\n\r\n Could you share\
    \ how to convert the model shown in left side?\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643e8d9ee08e86aec10f9ff9/mci45dThqIZbCZTS2oPup.png)\r\
    \n"
  created_at: 2023-06-20 08:20:14+00:00
  edited: false
  hidden: false
  id: 64916f4e3036b3b084617e9b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 147
repo_id: runwayml/stable-diffusion-v1-5
repo_type: model
status: open
target_branch: null
title: How to convert an optimized onnx model
