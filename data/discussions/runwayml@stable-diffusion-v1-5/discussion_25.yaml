!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 1h2j458f84n3id
conflicting_files: null
created_at: 2022-11-14 15:29:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e525ec5f355d990e06fac491b4b94767.svg
      fullname: John Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 1h2j458f84n3id
      type: user
    createdAt: '2022-11-14T15:29:10.000Z'
    data:
      edited: true
      editors:
      - 1h2j458f84n3id
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e525ec5f355d990e06fac491b4b94767.svg
          fullname: John Smith
          isHf: false
          isPro: false
          name: 1h2j458f84n3id
          type: user
        html: '<p>I am running an sd-onnx model with a custom gradio ui on my local
          machine and observed increased consumption of RAM on each subsequent generation.<br>Is
          there a way to control how much RAM the model uses or a way to "empty" the
          pipe after each gen other than rebooting the program?</p>

          <p>I generate one picture at a time.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/1668439605801-63166cbd95b55e2621d4884b.png"><img
          alt="Figure_1.png" src="https://cdn-uploads.huggingface.co/production/uploads/1668439605801-63166cbd95b55e2621d4884b.png"></a></p>

          '
        raw: 'I am running an sd-onnx model with a custom gradio ui on my local machine
          and observed increased consumption of RAM on each subsequent generation.

          Is there a way to control how much RAM the model uses or a way to "empty"
          the pipe after each gen other than rebooting the program?


          I generate one picture at a time.


          ![Figure_1.png](https://cdn-uploads.huggingface.co/production/uploads/1668439605801-63166cbd95b55e2621d4884b.png)'
        updatedAt: '2022-11-14T15:31:28.600Z'
      numEdits: 1
      reactions: []
    id: 63725ec620a58a5e14c471ed
    type: comment
  author: 1h2j458f84n3id
  content: 'I am running an sd-onnx model with a custom gradio ui on my local machine
    and observed increased consumption of RAM on each subsequent generation.

    Is there a way to control how much RAM the model uses or a way to "empty" the
    pipe after each gen other than rebooting the program?


    I generate one picture at a time.


    ![Figure_1.png](https://cdn-uploads.huggingface.co/production/uploads/1668439605801-63166cbd95b55e2621d4884b.png)'
  created_at: 2022-11-14 15:29:10+00:00
  edited: true
  hidden: false
  id: 63725ec620a58a5e14c471ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd38c9c0c687cde5f60811d8717a4163.svg
      fullname: Shin Shi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shinshi
      type: user
    createdAt: '2022-11-21T15:51:59.000Z'
    data:
      edited: false
      editors:
      - Shinshi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd38c9c0c687cde5f60811d8717a4163.svg
          fullname: Shin Shi
          isHf: false
          isPro: false
          name: Shinshi
          type: user
        html: '<p>It''s probably the gradio ui. Definetly not the model.</p>

          '
        raw: It's probably the gradio ui. Definetly not the model.
        updatedAt: '2022-11-21T15:51:59.409Z'
      numEdits: 0
      reactions: []
    id: 637b9e9fddd85eb8ce41aa69
    type: comment
  author: Shinshi
  content: It's probably the gradio ui. Definetly not the model.
  created_at: 2022-11-21 15:51:59+00:00
  edited: false
  hidden: false
  id: 637b9e9fddd85eb8ce41aa69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba8741f080a072dfc6cde9d55590a083.svg
      fullname: '111'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vivec90
      type: user
    createdAt: '2022-11-25T21:46:43.000Z'
    data:
      edited: false
      editors:
      - vivec90
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba8741f080a072dfc6cde9d55590a083.svg
          fullname: '111'
          isHf: false
          isPro: false
          name: vivec90
          type: user
        html: '<p>Hi. i believe this would help</p>

          <p>from numba import cuda<br>import gc<br>def free_gpu_cache():<br>    torch.cuda.empty_cache()<br>    gc.collect()</p>

          <p>i would suggest put function execution right inside "Run for generating
          images" cell before the first line of code<br>it will clean some trash.<br>unfortunately
          error appears even with this code due to output picture size more then 512x512</p>

          '
        raw: "Hi. i believe this would help\n\nfrom numba import cuda\nimport gc\n\
          def free_gpu_cache():\n    torch.cuda.empty_cache()\n    gc.collect()\n\n\
          i would suggest put function execution right inside \"Run for generating\
          \ images\" cell before the first line of code\nit will clean some trash.\n\
          unfortunately error appears even with this code due to output picture size\
          \ more then 512x512"
        updatedAt: '2022-11-25T21:46:43.769Z'
      numEdits: 0
      reactions: []
    id: 638137c34491649a3a6dbf66
    type: comment
  author: vivec90
  content: "Hi. i believe this would help\n\nfrom numba import cuda\nimport gc\ndef\
    \ free_gpu_cache():\n    torch.cuda.empty_cache()\n    gc.collect()\n\ni would\
    \ suggest put function execution right inside \"Run for generating images\" cell\
    \ before the first line of code\nit will clean some trash.\nunfortunately error\
    \ appears even with this code due to output picture size more then 512x512"
  created_at: 2022-11-25 21:46:43+00:00
  edited: false
  hidden: false
  id: 638137c34491649a3a6dbf66
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: runwayml/stable-diffusion-v1-5
repo_type: model
status: open
target_branch: null
title: Progressively increasing consumption of RAM
