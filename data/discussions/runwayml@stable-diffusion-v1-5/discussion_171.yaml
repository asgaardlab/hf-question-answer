!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MAPLELEAF3659
conflicting_files: null
created_at: 2023-09-02 01:47:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633c375ed5935998f75010d9/lonIoeHSCs8BnXCxCLfXf.jpeg?w=200&h=200&f=face
      fullname: Terry Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MAPLELEAF3659
      type: user
    createdAt: '2023-09-02T02:47:03.000Z'
    data:
      edited: false
      editors:
      - MAPLELEAF3659
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.961090624332428
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633c375ed5935998f75010d9/lonIoeHSCs8BnXCxCLfXf.jpeg?w=200&h=200&f=face
          fullname: Terry Zhou
          isHf: false
          isPro: false
          name: MAPLELEAF3659
          type: user
        html: '<p>It''s anyone know how sample size work in SD''s VAE and UNet?<br>All
          I know is the SD v1.5 was trained with 512<em>512, so it can generate 512</em>512
          more properly. But when I set the pipeline like 384<em>384 or even 768</em>768,
          it seems it can generate it as well (but less correctly).<br>I wondering
          could the SD (or the LDM) have ability of generalization to different sample
          size, so it''s possible to inference in any width and height? If so, how
          its work in training and inference?</p>

          '
        raw: "It's anyone know how sample size work in SD's VAE and UNet? \r\nAll\
          \ I know is the SD v1.5 was trained with 512*512, so it can generate 512*512\
          \ more properly. But when I set the pipeline like 384*384 or even 768*768,\
          \ it seems it can generate it as well (but less correctly).\r\nI wondering\
          \ could the SD (or the LDM) have ability of generalization to different\
          \ sample size, so it's possible to inference in any width and height? If\
          \ so, how its work in training and inference?"
        updatedAt: '2023-09-02T02:47:03.261Z'
      numEdits: 0
      reactions: []
    id: 64f2a227acd455c142708d69
    type: comment
  author: MAPLELEAF3659
  content: "It's anyone know how sample size work in SD's VAE and UNet? \r\nAll I\
    \ know is the SD v1.5 was trained with 512*512, so it can generate 512*512 more\
    \ properly. But when I set the pipeline like 384*384 or even 768*768, it seems\
    \ it can generate it as well (but less correctly).\r\nI wondering could the SD\
    \ (or the LDM) have ability of generalization to different sample size, so it's\
    \ possible to inference in any width and height? If so, how its work in training\
    \ and inference?"
  created_at: 2023-09-02 01:47:03+00:00
  edited: false
  hidden: false
  id: 64f2a227acd455c142708d69
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 171
repo_id: runwayml/stable-diffusion-v1-5
repo_type: model
status: open
target_branch: null
title: Question of the sample size in VAE and UNet
