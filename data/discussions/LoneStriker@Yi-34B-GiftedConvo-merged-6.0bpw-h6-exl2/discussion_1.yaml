!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Efaarts
conflicting_files: null
created_at: 2023-11-09 21:51:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/46b2cbc6dfe0f65547782218f467066a.svg
      fullname: Ben
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Efaarts
      type: user
    createdAt: '2023-11-09T21:51:43.000Z'
    data:
      edited: false
      editors:
      - Efaarts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2562671899795532
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/46b2cbc6dfe0f65547782218f467066a.svg
          fullname: Ben
          isHf: false
          isPro: false
          name: Efaarts
          type: user
        html: "<p>Getting this on load</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64c53e475d0ab485fbff5dc3/dfpHsYEYA36QRw0gAnSAG.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64c53e475d0ab485fbff5dc3/dfpHsYEYA36QRw0gAnSAG.png\"\
          ></a></p>\n<pre><code>Traceback (most recent call last):\n\nFile \"C:\\\
          Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\modules\\ui_model_menu.py\"\
          , line 209, in load_model_wrapper\n\nshared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \nFile \"C:\\Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\\
          modules\\models.py\", line 93, in load_model\n\ntokenizer = load_tokenizer(model_name,\
          \ model)\n\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFile \"C:\\\
          Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\modules\\models.py\"\
          , line 113, in load_tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFile \"C:\\Projects\\MachineLearning\\\
          LLMWebUI\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          transformers\\models\\auto\\tokenization_auto.py\", line 751, in from_pretrained\n\
          \ntokenizer_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path,\
          \ **kwargs)\n\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \nFile \"C:\\Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
          , line 487, in get_class_from_dynamic_module\n\nfinal_module = get_cached_module_file(\n\
          \n               ^^^^^^^^^^^^^^^^^^^^^^^\n\nFile \"C:\\Projects\\MachineLearning\\\
          LLMWebUI\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          transformers\\dynamic_module_utils.py\", line 293, in get_cached_module_file\n\
          \nresolved_module_file = cached_file(\n\n                       ^^^^^^^^^^^^\n\
          \nFile \"C:\\Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\transformers\\utils\\hub.py\"\
          , line 401, in cached_file\n\nraise EnvironmentError(\n\nOSError: models\\\
          Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2 does not appear to have a file\
          \ named tokenization_yi.py. Checkout 'https://huggingface.co/models\\Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2/None'\
          \ for available files.\n</code></pre>\n"
        raw: "\r\nGetting this on load\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c53e475d0ab485fbff5dc3/dfpHsYEYA36QRw0gAnSAG.png)\r\
          \n\r\n```\r\nTraceback (most recent call last):\r\n\r\nFile \"C:\\Projects\\\
          MachineLearning\\LLMWebUI\\text-generation-webui\\modules\\ui_model_menu.py\"\
          , line 209, in load_model_wrapper\r\n\r\nshared.model, shared.tokenizer\
          \ = load_model(shared.model_name, loader)\r\n\r\n                      \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\\
          MachineLearning\\LLMWebUI\\text-generation-webui\\modules\\models.py\",\
          \ line 93, in load_model\r\n\r\ntokenizer = load_tokenizer(model_name, model)\r\
          \n\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\\
          MachineLearning\\LLMWebUI\\text-generation-webui\\modules\\models.py\",\
          \ line 113, in load_tokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\r\
          \n\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\\
          MachineLearning\\LLMWebUI\\text-generation-webui\\installer_files\\env\\\
          Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\",\
          \ line 751, in from_pretrained\r\n\r\ntokenizer_class = get_class_from_dynamic_module(class_ref,\
          \ pretrained_model_name_or_path, **kwargs)\r\n\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n\r\nFile \"C:\\Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
          , line 487, in get_class_from_dynamic_module\r\n\r\nfinal_module = get_cached_module_file(\r\
          \n\r\n               ^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\\
          MachineLearning\\LLMWebUI\\text-generation-webui\\installer_files\\env\\\
          Lib\\site-packages\\transformers\\dynamic_module_utils.py\", line 293, in\
          \ get_cached_module_file\r\n\r\nresolved_module_file = cached_file(\r\n\r\
          \n                       ^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\MachineLearning\\\
          LLMWebUI\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          transformers\\utils\\hub.py\", line 401, in cached_file\r\n\r\nraise EnvironmentError(\r\
          \n\r\nOSError: models\\Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2 does not\
          \ appear to have a file named tokenization_yi.py. Checkout 'https://huggingface.co/models\\\
          Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2/None' for available files.\r\n\
          ```\r\n"
        updatedAt: '2023-11-09T21:51:43.279Z'
      numEdits: 0
      reactions: []
    id: 654d546ff714720ae2738ded
    type: comment
  author: Efaarts
  content: "\r\nGetting this on load\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64c53e475d0ab485fbff5dc3/dfpHsYEYA36QRw0gAnSAG.png)\r\
    \n\r\n```\r\nTraceback (most recent call last):\r\n\r\nFile \"C:\\Projects\\MachineLearning\\\
    LLMWebUI\\text-generation-webui\\modules\\ui_model_menu.py\", line 209, in load_model_wrapper\r\
    \n\r\nshared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
    \n\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n\r\nFile \"C:\\Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\\
    modules\\models.py\", line 93, in load_model\r\n\r\ntokenizer = load_tokenizer(model_name,\
    \ model)\r\n\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\\
    Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\modules\\models.py\"\
    , line 113, in load_tokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\r\
    \n\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\\
    MachineLearning\\LLMWebUI\\text-generation-webui\\installer_files\\env\\Lib\\\
    site-packages\\transformers\\models\\auto\\tokenization_auto.py\", line 751, in\
    \ from_pretrained\r\n\r\ntokenizer_class = get_class_from_dynamic_module(class_ref,\
    \ pretrained_model_name_or_path, **kwargs)\r\n\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n\r\nFile \"C:\\Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\transformers\\dynamic_module_utils.py\"\
    , line 487, in get_class_from_dynamic_module\r\n\r\nfinal_module = get_cached_module_file(\r\
    \n\r\n               ^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nFile \"C:\\Projects\\MachineLearning\\\
    LLMWebUI\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\transformers\\\
    dynamic_module_utils.py\", line 293, in get_cached_module_file\r\n\r\nresolved_module_file\
    \ = cached_file(\r\n\r\n                       ^^^^^^^^^^^^\r\n\r\nFile \"C:\\\
    Projects\\MachineLearning\\LLMWebUI\\text-generation-webui\\installer_files\\\
    env\\Lib\\site-packages\\transformers\\utils\\hub.py\", line 401, in cached_file\r\
    \n\r\nraise EnvironmentError(\r\n\r\nOSError: models\\Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2\
    \ does not appear to have a file named tokenization_yi.py. Checkout 'https://huggingface.co/models\\\
    Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2/None' for available files.\r\n```\r\n"
  created_at: 2023-11-09 21:51:43+00:00
  edited: false
  hidden: false
  id: 654d546ff714720ae2738ded
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe12a9f4567bafbe9afdb1105cfdc80e.svg
      fullname: Abigail
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbiTabby
      type: user
    createdAt: '2023-11-10T03:41:27.000Z'
    data:
      edited: false
      editors:
      - AbiTabby
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.927474319934845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe12a9f4567bafbe9afdb1105cfdc80e.svg
          fullname: Abigail
          isHf: false
          isPro: false
          name: AbiTabby
          type: user
        html: '<p>I had a similar error when loading the 5.0bpw in Exllamav2_HF.<br>But
          it loaded okay in Exllamav2.</p>

          '
        raw: 'I had a similar error when loading the 5.0bpw in Exllamav2_HF.

          But it loaded okay in Exllamav2.'
        updatedAt: '2023-11-10T03:41:27.353Z'
      numEdits: 0
      reactions: []
    id: 654da66752214f778a5e5e0c
    type: comment
  author: AbiTabby
  content: 'I had a similar error when loading the 5.0bpw in Exllamav2_HF.

    But it loaded okay in Exllamav2.'
  created_at: 2023-11-10 03:41:27+00:00
  edited: false
  hidden: false
  id: 654da66752214f778a5e5e0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-11-10T17:00:23.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9678513407707214
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Also, please make sure you''re Exllamav2 is updated to the latest
          version.</p>

          <p>I never load in Exllamav2_HF as I don''t need any of the extra options.  I
          tried the 8.0bpw model on my 2x4090 system and it''s really slow for inference.
          Like 7 t/s speeds slow.  The 4.0bpw model is 23 t/s and does not seem any
          different in terms of inference quality.</p>

          '
        raw: 'Also, please make sure you''re Exllamav2 is updated to the latest version.


          I never load in Exllamav2_HF as I don''t need any of the extra options.  I
          tried the 8.0bpw model on my 2x4090 system and it''s really slow for inference.
          Like 7 t/s speeds slow.  The 4.0bpw model is 23 t/s and does not seem any
          different in terms of inference quality.'
        updatedAt: '2023-11-10T17:00:23.320Z'
      numEdits: 0
      reactions: []
    id: 654e61a79ed3f4108ff077e4
    type: comment
  author: LoneStriker
  content: 'Also, please make sure you''re Exllamav2 is updated to the latest version.


    I never load in Exllamav2_HF as I don''t need any of the extra options.  I tried
    the 8.0bpw model on my 2x4090 system and it''s really slow for inference. Like
    7 t/s speeds slow.  The 4.0bpw model is 23 t/s and does not seem any different
    in terms of inference quality.'
  created_at: 2023-11-10 17:00:23+00:00
  edited: false
  hidden: false
  id: 654e61a79ed3f4108ff077e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Yi-34B-GiftedConvo-merged-6.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Having trouble loading this in ooba
