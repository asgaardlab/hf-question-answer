!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MrDevolver
conflicting_files: null
created_at: 2023-08-04 02:21:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-08-04T03:21:14.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9809355735778809
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: '<p>Hi, thank you for your contributions!</p>

          <p>I like the idea of experimental merges of models, but it would be nice
          if you could add at least a few notes of details about the merges, like
          percentages of each model in the mix and reasoning behind such decision,
          if any at all.</p>

          <p>Also, I''ve noticed that for these 7B models you chose Q4_K_M. They are
          fairly small, it''d be nice if you could do Q6_K as well, I had good experience
          with those and they are still pretty fast. Just a suggestion.</p>

          '
        raw: "Hi, thank you for your contributions!\r\n\r\nI like the idea of experimental\
          \ merges of models, but it would be nice if you could add at least a few\
          \ notes of details about the merges, like percentages of each model in the\
          \ mix and reasoning behind such decision, if any at all.\r\n\r\nAlso, I've\
          \ noticed that for these 7B models you chose Q4_K_M. They are fairly small,\
          \ it'd be nice if you could do Q6_K as well, I had good experience with\
          \ those and they are still pretty fast. Just a suggestion."
        updatedAt: '2023-08-04T03:21:14.740Z'
      numEdits: 0
      reactions: []
    id: 64cc6eaaead94891d12103a4
    type: comment
  author: MrDevolver
  content: "Hi, thank you for your contributions!\r\n\r\nI like the idea of experimental\
    \ merges of models, but it would be nice if you could add at least a few notes\
    \ of details about the merges, like percentages of each model in the mix and reasoning\
    \ behind such decision, if any at all.\r\n\r\nAlso, I've noticed that for these\
    \ 7B models you chose Q4_K_M. They are fairly small, it'd be nice if you could\
    \ do Q6_K as well, I had good experience with those and they are still pretty\
    \ fast. Just a suggestion."
  created_at: 2023-08-04 02:21:14+00:00
  edited: false
  hidden: false
  id: 64cc6eaaead94891d12103a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7b3a0134d17e739cabd7d01cdc4c124e.svg
      fullname: Zaraki Quem Parte
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zarakiquemparte
      type: user
    createdAt: '2023-08-04T11:44:12.000Z'
    data:
      edited: false
      editors:
      - zarakiquemparte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.599345326423645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7b3a0134d17e739cabd7d01cdc4c124e.svg
          fullname: Zaraki Quem Parte
          isHf: false
          isPro: false
          name: zarakiquemparte
          type: user
        html: '<p>Thanks for the feedback, later I do quantize the q6_k version, you
          can get more information in that <a href="https://huggingface.co/zarakiquemparte/hermes-kimiko-7b">repository</a>.</p>

          <p>But basically I use nous hermes llama 2 7b with kimiko Lora with normal
          weight (1.0)</p>

          '
        raw: 'Thanks for the feedback, later I do quantize the q6_k version, you can
          get more information in that [repository](https://huggingface.co/zarakiquemparte/hermes-kimiko-7b).


          But basically I use nous hermes llama 2 7b with kimiko Lora with normal
          weight (1.0)'
        updatedAt: '2023-08-04T11:44:12.976Z'
      numEdits: 0
      reactions: []
    id: 64cce48ceed22517ae204e23
    type: comment
  author: zarakiquemparte
  content: 'Thanks for the feedback, later I do quantize the q6_k version, you can
    get more information in that [repository](https://huggingface.co/zarakiquemparte/hermes-kimiko-7b).


    But basically I use nous hermes llama 2 7b with kimiko Lora with normal weight
    (1.0)'
  created_at: 2023-08-04 10:44:12+00:00
  edited: false
  hidden: false
  id: 64cce48ceed22517ae204e23
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: zarakiquemparte/hermes-kimiko-7b-GGML
repo_type: model
status: open
target_branch: null
title: Experimental merges
