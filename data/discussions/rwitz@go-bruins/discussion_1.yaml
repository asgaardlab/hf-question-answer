!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil337
conflicting_files: null
created_at: 2023-12-09 19:42:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-09T19:42:36.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9644028544425964
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: '<p>I extensively tested both the original Q bert and this additional
          fine-tune. And while this fine-tune is a solid performer it makes a couple
          things notably worse.</p>

          <ol>

          <li><p>It increased hallucinations at the fringes of knowledge, such as
          in my pop-culture questions. This often happens when LLMs are excessively
          fine-tuned.</p>

          </li>

          <li><p>It made story telling far too stubborn to respect the user''s prompt.
          Again, this happens when LLMs are excessive fine-tuned. That is, LLMs are
          in a battle to employ pre-packed story telling elements while respecting
          the user''s prompt directives. Failing to do so results in absurd contradictions</p>

          </li>

          </ol>

          <p>Example: The pre-packaged need to build-suspense and knock on closed
          doors vs the user prompt stating he was caught stealing something. This
          played out as hearing footsteps then a knock on the door, and also the thief
          opening the door himself, yet still be caught stealing something off the
          counter. He obviously wouldn''t have stolen something after hearing footsteps
          and a knock on the door, and certainly not when opening the door himself.
          Too much fine-tuning is both taking stories away from the user prompt and
          the countless stories within the foundational model itself.</p>

          '
        raw: "I extensively tested both the original Q bert and this additional fine-tune.\
          \ And while this fine-tune is a solid performer it makes a couple things\
          \ notably worse.\r\n\r\n1) It increased hallucinations at the fringes of\
          \ knowledge, such as in my pop-culture questions. This often happens when\
          \ LLMs are excessively fine-tuned.\r\n\r\n2) It made story telling far too\
          \ stubborn to respect the user's prompt. Again, this happens when LLMs are\
          \ excessive fine-tuned. That is, LLMs are in a battle to employ pre-packed\
          \ story telling elements while respecting the user's prompt directives.\
          \ Failing to do so results in absurd contradictions\r\n\r\nExample: The\
          \ pre-packaged need to build-suspense and knock on closed doors vs the user\
          \ prompt stating he was caught stealing something. This played out as hearing\
          \ footsteps then a knock on the door, and also the thief opening the door\
          \ himself, yet still be caught stealing something off the counter. He obviously\
          \ wouldn't have stolen something after hearing footsteps and a knock on\
          \ the door, and certainly not when opening the door himself. Too much fine-tuning\
          \ is both taking stories away from the user prompt and the countless stories\
          \ within the foundational model itself."
        updatedAt: '2023-12-09T19:42:36.468Z'
      numEdits: 0
      reactions: []
    id: 6574c32cb4379e65a87e6b7a
    type: comment
  author: Phil337
  content: "I extensively tested both the original Q bert and this additional fine-tune.\
    \ And while this fine-tune is a solid performer it makes a couple things notably\
    \ worse.\r\n\r\n1) It increased hallucinations at the fringes of knowledge, such\
    \ as in my pop-culture questions. This often happens when LLMs are excessively\
    \ fine-tuned.\r\n\r\n2) It made story telling far too stubborn to respect the\
    \ user's prompt. Again, this happens when LLMs are excessive fine-tuned. That\
    \ is, LLMs are in a battle to employ pre-packed story telling elements while respecting\
    \ the user's prompt directives. Failing to do so results in absurd contradictions\r\
    \n\r\nExample: The pre-packaged need to build-suspense and knock on closed doors\
    \ vs the user prompt stating he was caught stealing something. This played out\
    \ as hearing footsteps then a knock on the door, and also the thief opening the\
    \ door himself, yet still be caught stealing something off the counter. He obviously\
    \ wouldn't have stolen something after hearing footsteps and a knock on the door,\
    \ and certainly not when opening the door himself. Too much fine-tuning is both\
    \ taking stories away from the user prompt and the countless stories within the\
    \ foundational model itself."
  created_at: 2023-12-09 19:42:36+00:00
  edited: false
  hidden: false
  id: 6574c32cb4379e65a87e6b7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a259d0f30c46422789d38d/yzFyNdV_R4aFsKfNRMw3s.jpeg?w=200&h=200&f=face
      fullname: Ryan Witzman
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rwitz
      type: user
    createdAt: '2023-12-09T20:15:26.000Z'
    data:
      edited: false
      editors:
      - rwitz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9357924461364746
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a259d0f30c46422789d38d/yzFyNdV_R4aFsKfNRMw3s.jpeg?w=200&h=200&f=face
          fullname: Ryan Witzman
          isHf: false
          isPro: false
          name: rwitz
          type: user
        html: '<p>I will attempt to reduce this in future versions. Thanks for the
          insight!</p>

          '
        raw: I will attempt to reduce this in future versions. Thanks for the insight!
        updatedAt: '2023-12-09T20:15:26.330Z'
      numEdits: 0
      reactions: []
    id: 6574cade06fdcd4ca9410698
    type: comment
  author: rwitz
  content: I will attempt to reduce this in future versions. Thanks for the insight!
  created_at: 2023-12-09 20:15:26+00:00
  edited: false
  hidden: false
  id: 6574cade06fdcd4ca9410698
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rwitz/go-bruins
repo_type: model
status: open
target_branch: null
title: Good, But A Couple Issues
