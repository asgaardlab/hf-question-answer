!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-28 09:25:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-28T09:25:25.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.19298379123210907
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p>Hi,</p>\n<p>I tried to load Yiyi2 with transformer 4.36.2, and here\
          \ is an error with tokenizer class</p>\n<pre><code>\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/server.py:241\
          \ in &lt;module&gt;                                                    \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \       \u2502\n\u2502   240         # Load the model                  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                  \u2502\n\u2502 \u2771\
          \ 241         shared.model, shared.tokenizer = load_model(model_name)  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                           \u2502\n\u2502   242         if shared.args.lora:\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/modules/models.py:98\
          \ in load_model                                                        \
          \                                                                      \
          \                                                                      \
          \                     \u2502\n\u2502                                   \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502    97         else:                                             \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502 \u2771  98           \
          \  tokenizer = load_tokenizer(model_name, model)                       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \            \u2502\n\u2502    99                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                       \u2502\n\u2502 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                            \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/modules/models.py:126\
          \ in load_tokenizer                                                    \
          \                                                                      \
          \                                                                      \
          \                    \u2502\n\u2502                                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                               \u2502\n\
          \u2502   125                                                           \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502 \u2771 126         tokenizer\
          \ = AutoTokenizer.from_pretrained(                                     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \     \u2502\n\u2502   127             path_to_model,                  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                     \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:774\
          \ in from_pretrained                                                   \
          \                                                                      \
          \                                                     \u2502\n\u2502   \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                          \u2502\n\u2502   773                 tokenizer_class.register_for_auto_class()\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                   \u2502\n\u2502 \u2771 774          \
          \   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input                                                               \
          \                                                                      \
          \                                                                      \
          \           \u2502\n\u2502   775         elif config_tokenizer_class is\
          \ not None:                                                            \
          \                                                                      \
          \                                                                      \
          \                                                      \u2502\n\u2502  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                           \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2028\
          \ in from_pretrained                                                   \
          \                                                                      \
          \                                                          \u2502\n\u2502\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                             \u2502\n\u2502   2027                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502 \u2771 2028         return cls._from_pretrained(     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                  \u2502\n\u2502   2029\
          \             resolved_vocab_files,                                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                      \u2502\n\u2502                                  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                 \u2502\
          \n\u2502                                                               \
          \                                                                      \
          \          ... 1 frames hidden ...                                     \
          \                                                                      \
          \                                    \u2502\n\u2502                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \         \u2502\n\u2502 /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:74\
          \ in __init__                                                          \
          \                                                                      \
          \                                                                      \
          \   \u2502\n\u2502                                                     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                              \u2502\n\u2502    73    \
          \     pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \    \u2502\n\u2502 \u2771  74         super().__init__(               \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                    \u2502\n\u2502    75\
          \             bos_token=bos_token,                                     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                       \u2502\n\u2502                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                  \u2502\
          \n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils.py:367\
          \ in __init__                                                          \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502    366         # the order\
          \ of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502 \u2771  367         self._add_tokens(                \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                  \u2502\n\u2502    368\
          \             [token for token in self.all_special_tokens_extended if token\
          \ not in self._a                                                       \
          \                                                                      \
          \                                                                      \
          \                  \u2502\n\u2502                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                             \u2502\n\u2502\
          \ /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils.py:467\
          \ in _add_tokens                                                       \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502    466         # TODO\
          \ this is fairly slow to improve!                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \       \u2502\n\u2502 \u2771  467         current_vocab = self.get_vocab().copy()\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                            \u2502\n\u2502    468     \
          \    new_idx = len(current_vocab)  # only call this once, len gives the\
          \ last index +                                                         \
          \                                                                      \
          \                                                                      \
          \                 \u2502\n\u2502                                       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                            \u2502\n\u2502\
          \ /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:111\
          \ in get_vocab                                                         \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                             \u2502\n\u2502   110     \
          \    \"\"\"Returns vocab as a dict\"\"\"                               \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                        \u2502\n\u2502 \u2771 111         vocab = {self.convert_ids_to_tokens(i):\
          \ i for i in range(self.vocab_size)}                                   \
          \                                                                      \
          \                                                                      \
          \                                             \u2502\n\u2502   112     \
          \    vocab.update(self.added_tokens_encoder)                           \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                  \u2502\n\u2502                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                             \u2502\n\u2502\
          \ /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:107\
          \ in vocab_size                                                        \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                             \u2502\n\u2502   106     \
          \    \"\"\"Returns vocab size\"\"\"                                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                        \u2502\n\u2502 \u2771 107         return self.sp_model.get_piece_size()\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                               \u2502\n\u2502   108   \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\nAttributeError: 'YayiTokenizer' object\
          \ has no attribute 'sp_model'\n</code></pre>\n"
        raw: "Hi,\n\nI tried to load Yiyi2 with transformer 4.36.2, and here is an\
          \ error with tokenizer class\n\n```\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/server.py:241\
          \ in <module>                                                          \
          \                                                                      \
          \                                                                      \
          \                            \u2502\n\u2502                            \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \ \u2502\n\u2502   240         # Load the model                        \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                            \u2502\n\u2502 \u2771 241 \
          \        shared.model, shared.tokenizer = load_model(model_name)       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                      \u2502\n\u2502   242         if shared.args.lora:\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/modules/models.py:98\
          \ in load_model                                                        \
          \                                                                      \
          \                                                                      \
          \                     \u2502\n\u2502                                   \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502    97         else:                                             \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502 \u2771  98           \
          \  tokenizer = load_tokenizer(model_name, model)                       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \            \u2502\n\u2502    99                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                       \u2502\n\u2502 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                            \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/modules/models.py:126\
          \ in load_tokenizer                                                    \
          \                                                                      \
          \                                                                      \
          \                    \u2502\n\u2502                                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                               \u2502\n\
          \u2502   125                                                           \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502 \u2771 126         tokenizer\
          \ = AutoTokenizer.from_pretrained(                                     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \     \u2502\n\u2502   127             path_to_model,                  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                     \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:774\
          \ in from_pretrained                                                   \
          \                                                                      \
          \                                                     \u2502\n\u2502   \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                          \u2502\n\u2502   773                 tokenizer_class.register_for_auto_class()\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                   \u2502\n\u2502 \u2771 774          \
          \   return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
          \ *input                                                               \
          \                                                                      \
          \                                                                      \
          \           \u2502\n\u2502   775         elif config_tokenizer_class is\
          \ not None:                                                            \
          \                                                                      \
          \                                                                      \
          \                                                      \u2502\n\u2502  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                           \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2028\
          \ in from_pretrained                                                   \
          \                                                                      \
          \                                                          \u2502\n\u2502\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                             \u2502\n\u2502   2027                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502 \u2771 2028         return cls._from_pretrained(     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                  \u2502\n\u2502   2029\
          \             resolved_vocab_files,                                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                      \u2502\n\u2502                                  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                 \u2502\
          \n\u2502                                                               \
          \                                                                      \
          \          ... 1 frames hidden ...                                     \
          \                                                                      \
          \                                    \u2502\n\u2502                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \         \u2502\n\u2502 /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:74\
          \ in __init__                                                          \
          \                                                                      \
          \                                                                      \
          \   \u2502\n\u2502                                                     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                              \u2502\n\u2502    73    \
          \     pad_token = AddedToken(pad_token, lstrip=False, rstrip=False) if isinstance(pad_\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \    \u2502\n\u2502 \u2771  74         super().__init__(               \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                    \u2502\n\u2502    75\
          \             bos_token=bos_token,                                     \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                       \u2502\n\u2502                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                  \u2502\
          \n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils.py:367\
          \ in __init__                                                          \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502    366         # the order\
          \ of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following  \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502 \u2771  367         self._add_tokens(                \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                  \u2502\n\u2502    368\
          \             [token for token in self.all_special_tokens_extended if token\
          \ not in self._a                                                       \
          \                                                                      \
          \                                                                      \
          \                  \u2502\n\u2502                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                             \u2502\n\u2502\
          \ /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils.py:467\
          \ in _add_tokens                                                       \
          \                                                                      \
          \                                                                \u2502\n\
          \u2502                                                                 \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                  \u2502\n\u2502    466         # TODO\
          \ this is fairly slow to improve!                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \       \u2502\n\u2502 \u2771  467         current_vocab = self.get_vocab().copy()\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                            \u2502\n\u2502    468     \
          \    new_idx = len(current_vocab)  # only call this once, len gives the\
          \ last index +                                                         \
          \                                                                      \
          \                                                                      \
          \                 \u2502\n\u2502                                       \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                            \u2502\n\u2502\
          \ /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:111\
          \ in get_vocab                                                         \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                             \u2502\n\u2502   110     \
          \    \"\"\"Returns vocab as a dict\"\"\"                               \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                        \u2502\n\u2502 \u2771 111         vocab = {self.convert_ids_to_tokens(i):\
          \ i for i in range(self.vocab_size)}                                   \
          \                                                                      \
          \                                                                      \
          \                                             \u2502\n\u2502   112     \
          \    vocab.update(self.added_tokens_encoder)                           \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                  \u2502\n\u2502                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                             \u2502\n\u2502\
          \ /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:107\
          \ in vocab_size                                                        \
          \                                                                      \
          \                                                                      \
          \  \u2502\n\u2502                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                             \u2502\n\u2502   106     \
          \    \"\"\"Returns vocab size\"\"\"                                    \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                        \u2502\n\u2502 \u2771 107         return self.sp_model.get_piece_size()\
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                               \u2502\n\u2502   108   \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                                                                      \
          \                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\nAttributeError: 'YayiTokenizer' object\
          \ has no attribute 'sp_model'\n```"
        updatedAt: '2023-12-28T09:25:47.745Z'
      numEdits: 1
      reactions: []
    id: 658d3f057ecef7270a38afd7
    type: comment
  author: Yhyu13
  content: "Hi,\n\nI tried to load Yiyi2 with transformer 4.36.2, and here is an error\
    \ with tokenizer class\n\n```\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/server.py:241\
    \ in <module>                                                                \
    \                                                                            \
    \                                                                            \
    \          \u2502\n\u2502                                                    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                             \u2502\n\u2502   240         # Load the model  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                \u2502\n\u2502 \u2771 241   \
    \      shared.model, shared.tokenizer = load_model(model_name)               \
    \                                                                            \
    \                                                                            \
    \                                                                        \u2502\
    \n\u2502   242         if shared.args.lora:                                  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \            \u2502\n\u2502                                                  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                               \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/modules/models.py:98\
    \ in load_model                                                              \
    \                                                                            \
    \                                                                            \
    \   \u2502\n\u2502                                                           \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                      \u2502\n\u2502    97         else:                    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                         \u2502\n\u2502 \u2771  98          \
    \   tokenizer = load_tokenizer(model_name, model)                            \
    \                                                                            \
    \                                                                            \
    \                                                                 \u2502\n\u2502\
    \    99                                                                      \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \     \u2502\n\u2502                                                         \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                        \u2502\n\u2502 /home/hangyu5/Documents/Git-repoMy/text-generation-webui/modules/models.py:126\
    \ in load_tokenizer                                                          \
    \                                                                            \
    \                                                                            \
    \  \u2502\n\u2502                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                     \u2502\n\u2502   125                                   \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                        \u2502\n\u2502 \u2771 126         tokenizer\
    \ = AutoTokenizer.from_pretrained(                                           \
    \                                                                            \
    \                                                                            \
    \                                                         \u2502\n\u2502   127\
    \             path_to_model,                                                 \
    \                                                                            \
    \                                                                            \
    \                                                                           \u2502\
    \n\u2502                                                                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \            \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:774\
    \ in from_pretrained                                                         \
    \                                                                            \
    \                                         \u2502\n\u2502                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                            \u2502\n\u2502  \
    \ 773                 tokenizer_class.register_for_auto_class()              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \   \u2502\n\u2502 \u2771 774             return tokenizer_class.from_pretrained(pretrained_model_name_or_path,\
    \ *input                                                                     \
    \                                                                            \
    \                                                                     \u2502\n\
    \u2502   775         elif config_tokenizer_class is not None:                \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \          \u2502\n\u2502                                                    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                             \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2028\
    \ in from_pretrained                                                         \
    \                                                                            \
    \                                              \u2502\n\u2502                \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                 \u2502\n\u2502\
    \   2027                                                                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \     \u2502\n\u2502 \u2771 2028         return cls._from_pretrained(        \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                             \u2502\n\u2502   2029             resolved_vocab_files,\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                        \u2502\n\u2502                      \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                           \u2502\n\u2502   \
    \                                                                            \
    \                                                                ... 1 frames\
    \ hidden ...                                                                 \
    \                                                                            \
    \  \u2502\n\u2502                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                     \u2502\n\u2502 /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:74\
    \ in __init__                                                                \
    \                                                                            \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \    \u2502\n\u2502    73         pad_token = AddedToken(pad_token, lstrip=False,\
    \ rstrip=False) if isinstance(pad_                                           \
    \                                                                            \
    \                                                                            \
    \                   \u2502\n\u2502 \u2771  74         super().__init__(      \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                           \u2502\n\u2502    75             bos_token=bos_token,\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                          \u2502\n\u2502                    \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                             \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils.py:367\
    \ in __init__                                                                \
    \                                                                            \
    \                                                    \u2502\n\u2502          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                       \u2502\
    \n\u2502    366         # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES\
    \ following                                                                  \
    \                                                                            \
    \                                                                        \u2502\
    \n\u2502 \u2771  367         self._add_tokens(                               \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                 \u2502\n\u2502    368             [token for token in self.all_special_tokens_extended\
    \ if token not in self._a                                                    \
    \                                                                            \
    \                                                                            \
    \         \u2502\n\u2502                                                     \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                            \u2502\n\u2502 /home/hangyu5/anaconda3/envs/textgen/lib/python3.11/site-packages/transformers/tokenization_utils.py:467\
    \ in _add_tokens                                                             \
    \                                                                            \
    \                                                    \u2502\n\u2502          \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                       \u2502\
    \n\u2502    466         # TODO this is fairly slow to improve!               \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \            \u2502\n\u2502 \u2771  467         current_vocab = self.get_vocab().copy()\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                          \u2502\n\u2502    468         new_idx = len(current_vocab)\
    \  # only call this once, len gives the last index +                         \
    \                                                                            \
    \                                                                            \
    \                                     \u2502\n\u2502                         \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                        \u2502\n\u2502 /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:111\
    \ in get_vocab                                                               \
    \                                                                            \
    \                                                            \u2502\n\u2502  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \   \u2502\n\u2502   110         \"\"\"Returns vocab as a dict\"\"\"         \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                            \u2502\n\u2502 \u2771 111         vocab = {self.convert_ids_to_tokens(i):\
    \ i for i in range(self.vocab_size)}                                         \
    \                                                                            \
    \                                                                            \
    \                           \u2502\n\u2502   112         vocab.update(self.added_tokens_encoder)\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                           \u2502\n\u2502                                   \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                              \u2502\n\u2502 /home/hangyu5/.cache/huggingface/modules/transformers_modules/yayi2-30b/tokenization_yayi.py:107\
    \ in vocab_size                                                              \
    \                                                                            \
    \                                                            \u2502\n\u2502  \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \   \u2502\n\u2502   106         \"\"\"Returns vocab size\"\"\"              \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                            \u2502\n\u2502 \u2771 107         return self.sp_model.get_piece_size()\
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                             \u2502\n\u2502   108                           \
    \                                                                            \
    \                                                                            \
    \                                                                            \
    \                                                \u2502\n\u2570\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\nAttributeError: 'YayiTokenizer' object\
    \ has no attribute 'sp_model'\n```"
  created_at: 2023-12-28 09:25:25+00:00
  edited: true
  hidden: false
  id: 658d3f057ecef7270a38afd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-28T09:26:01.000Z'
    data:
      from: Tokenizer does not have sp_model
      to: 'AttributeError: ''YayiTokenizer'' object has no attribute ''sp_model'''
    id: 658d3f29eaba17684e93b6d4
    type: title-change
  author: Yhyu13
  created_at: 2023-12-28 09:26:01+00:00
  id: 658d3f29eaba17684e93b6d4
  new_title: 'AttributeError: ''YayiTokenizer'' object has no attribute ''sp_model'''
  old_title: Tokenizer does not have sp_model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-28T09:28:26.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9337018728256226
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Also, hf flash attan2 does not support this model yet. Would you
          like to update this model to be part of hf transformer?</p>

          '
        raw: Also, hf flash attan2 does not support this model yet. Would you like
          to update this model to be part of hf transformer?
        updatedAt: '2023-12-28T09:28:26.194Z'
      numEdits: 0
      reactions: []
    id: 658d3fba74e79b9a8ecdd012
    type: comment
  author: Yhyu13
  content: Also, hf flash attan2 does not support this model yet. Would you like to
    update this model to be part of hf transformer?
  created_at: 2023-12-28 09:28:26+00:00
  edited: false
  hidden: false
  id: 658d3fba74e79b9a8ecdd012
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Dqh8qh9kIYqj0PtwOCH75.png?w=200&h=200&f=face
      fullname: wenge-research
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: wenge-research
      type: user
    createdAt: '2024-01-10T04:10:49.000Z'
    data:
      edited: false
      editors:
      - wenge-research
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8335433006286621
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Dqh8qh9kIYqj0PtwOCH75.png?w=200&h=200&f=face
          fullname: wenge-research
          isHf: false
          isPro: false
          name: wenge-research
          type: user
        html: '<p>Thank you for your attention to YAYI2! In order to avoid version
          conflicts, it is recommended to follow the transformer version in <a href="https://huggingface.co/wenge-research/yayi2-30b/blob/34e35944c46c8c1e6a64b8b06c84a6d8a01dfeb5/config.json#L26">config.json</a>.  You
          can also refer to the solutions provided by <a href="https://huggingface.co/mzbac">mzbac</a>
          in discussion <a href="https://huggingface.co/wenge-research/yayi2-30b/discussions/5"></a><a
          href="/wenge-research/yayi2-30b/discussions/5">#5</a>.</p>

          <p> We will adapt to the latest transformer version as soon as possible.</p>

          '
        raw: "Thank you for your attention to YAYI2! In order to avoid version conflicts,\
          \ it is recommended to follow the transformer version in [config.json](https://huggingface.co/wenge-research/yayi2-30b/blob/34e35944c46c8c1e6a64b8b06c84a6d8a01dfeb5/config.json#L26).\
          \  You can also refer to the solutions provided by [mzbac](https://huggingface.co/mzbac)\
          \ in discussion [#5](https://huggingface.co/wenge-research/yayi2-30b/discussions/5).\n\
          \n We will adapt to the latest transformer version as soon as possible."
        updatedAt: '2024-01-10T04:10:49.027Z'
      numEdits: 0
      reactions: []
    id: 659e18c9e5bc942b4b1c0001
    type: comment
  author: wenge-research
  content: "Thank you for your attention to YAYI2! In order to avoid version conflicts,\
    \ it is recommended to follow the transformer version in [config.json](https://huggingface.co/wenge-research/yayi2-30b/blob/34e35944c46c8c1e6a64b8b06c84a6d8a01dfeb5/config.json#L26).\
    \  You can also refer to the solutions provided by [mzbac](https://huggingface.co/mzbac)\
    \ in discussion [#5](https://huggingface.co/wenge-research/yayi2-30b/discussions/5).\n\
    \n We will adapt to the latest transformer version as soon as possible."
  created_at: 2024-01-10 04:10:49+00:00
  edited: false
  hidden: false
  id: 659e18c9e5bc942b4b1c0001
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Dqh8qh9kIYqj0PtwOCH75.png?w=200&h=200&f=face
      fullname: wenge-research
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: wenge-research
      type: user
    createdAt: '2024-01-10T04:10:55.000Z'
    data:
      status: closed
    id: 659e18cfaa29d05820cbf6a5
    type: status-change
  author: wenge-research
  created_at: 2024-01-10 04:10:55+00:00
  id: 659e18cfaa29d05820cbf6a5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: wenge-research/yayi2-30b
repo_type: model
status: closed
target_branch: null
title: 'AttributeError: ''YayiTokenizer'' object has no attribute ''sp_model'''
