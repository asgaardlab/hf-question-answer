!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lostlex
conflicting_files: []
created_at: 2023-09-29 01:17:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/96b7c542406e182885497de0d7e275ab.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lostlex
      type: user
    createdAt: '2023-09-29T02:17:05.000Z'
    data:
      oid: 488085665470cea73a549159c319ea68486b2890
      parents:
      - 36295dd80b422dc49f40052021430dae76241adc
      subject: Upload tokenizer.json
    id: 651633a10000000000000000
    type: commit
  author: lostlex
  created_at: 2023-09-29 01:17:05+00:00
  id: 651633a10000000000000000
  oid: 488085665470cea73a549159c319ea68486b2890
  summary: Upload tokenizer.json
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96b7c542406e182885497de0d7e275ab.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lostlex
      type: user
    createdAt: '2023-09-29T02:17:07.000Z'
    data:
      edited: false
      editors:
      - lostlex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6323295831680298
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96b7c542406e182885497de0d7e275ab.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: lostlex
          type: user
        html: '<p>HuggingFace repositories store tokenizers in two flavors:</p>

          <ol>

          <li>"slow tokenizer" - corresponds to a tokenizer implemented in Python
          and stored as tokenizer_config.json</li>

          <li>"fast tokenizers" - corresponds to a tokenizer implemented in Rust and
          stored as tokenizer.json</li>

          </ol>

          <p>This repository only include files for the slow tokenizer. While the
          transformers library automatically converts "slow tokenizer" to "fast tokenizer"
          whenever possible, <a rel="nofollow" href="https://github.com/elixir-nx/bumblebee/">Bumblebee</a>
          relies on the Rust bindings and therefore always requires the tokenizer.json
          file. This change adds the fast tokenizer.</p>

          <p>Generated with:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"unitary/unbiased-toxic-roberta"</span>)

          <span class="hljs-keyword">assert</span> tokenizer.is_fast

          tokenizer.save_pretrained(<span class="hljs-string">"..."</span>)

          </code></pre>

          '
        raw: 'HuggingFace repositories store tokenizers in two flavors:


          1. "slow tokenizer" - corresponds to a tokenizer implemented in Python and
          stored as tokenizer_config.json

          2. "fast tokenizers" - corresponds to a tokenizer implemented in Rust and
          stored as tokenizer.json


          This repository only include files for the slow tokenizer. While the transformers
          library automatically converts "slow tokenizer" to "fast tokenizer" whenever
          possible, [Bumblebee](https://github.com/elixir-nx/bumblebee/) relies on
          the Rust bindings and therefore always requires the tokenizer.json file.
          This change adds the fast tokenizer.


          Generated with:


          ```python

          from transformers import AutoTokenizer


          tokenizer = AutoTokenizer.from_pretrained("unitary/unbiased-toxic-roberta")

          assert tokenizer.is_fast

          tokenizer.save_pretrained("...")

          ```'
        updatedAt: '2023-09-29T02:17:07.958Z'
      numEdits: 0
      reactions: []
    id: 651633a3ff0ecf2255029c6e
    type: comment
  author: lostlex
  content: 'HuggingFace repositories store tokenizers in two flavors:


    1. "slow tokenizer" - corresponds to a tokenizer implemented in Python and stored
    as tokenizer_config.json

    2. "fast tokenizers" - corresponds to a tokenizer implemented in Rust and stored
    as tokenizer.json


    This repository only include files for the slow tokenizer. While the transformers
    library automatically converts "slow tokenizer" to "fast tokenizer" whenever possible,
    [Bumblebee](https://github.com/elixir-nx/bumblebee/) relies on the Rust bindings
    and therefore always requires the tokenizer.json file. This change adds the fast
    tokenizer.


    Generated with:


    ```python

    from transformers import AutoTokenizer


    tokenizer = AutoTokenizer.from_pretrained("unitary/unbiased-toxic-roberta")

    assert tokenizer.is_fast

    tokenizer.save_pretrained("...")

    ```'
  created_at: 2023-09-29 01:17:07+00:00
  edited: false
  hidden: false
  id: 651633a3ff0ecf2255029c6e
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 3
repo_id: unitary/unbiased-toxic-roberta
repo_type: model
status: open
target_branch: refs/heads/main
title: Upload tokenizer.json
