!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NEFlame
conflicting_files: null
created_at: 2023-05-10 05:35:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9653aaee49b4ba831eb2a6671da233ee.svg
      fullname: N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NEFlame
      type: user
    createdAt: '2023-05-10T06:35:20.000Z'
    data:
      edited: false
      editors:
      - NEFlame
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9653aaee49b4ba831eb2a6671da233ee.svg
          fullname: N
          isHf: false
          isPro: false
          name: NEFlame
          type: user
        html: '<p>I''m trying to run this with an RTX 3090 and 16GB RAM via oobabooga
          webgenui. When loading the model, it maxes out all 16GBs of RAM and freezes
          my computer. I understand that the model has to first load into the system
          RAM in order to be transferred into VRAM. My question is, how much more
          RAM will I need to load this model at 8-bit precision? (I tried this 4 bit
          version and it loads successfully: <a href="https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g">https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g</a>,
          but it gets very close using ~97-99% of my system RAM.)</p>

          '
        raw: 'I''m trying to run this with an RTX 3090 and 16GB RAM via oobabooga
          webgenui. When loading the model, it maxes out all 16GBs of RAM and freezes
          my computer. I understand that the model has to first load into the system
          RAM in order to be transferred into VRAM. My question is, how much more
          RAM will I need to load this model at 8-bit precision? (I tried this 4 bit
          version and it loads successfully: https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g,
          but it gets very close using ~97-99% of my system RAM.)'
        updatedAt: '2023-05-10T06:35:20.257Z'
      numEdits: 0
      reactions: []
    id: 645b3b28438d6cfbe1ac5332
    type: comment
  author: NEFlame
  content: 'I''m trying to run this with an RTX 3090 and 16GB RAM via oobabooga webgenui.
    When loading the model, it maxes out all 16GBs of RAM and freezes my computer.
    I understand that the model has to first load into the system RAM in order to
    be transferred into VRAM. My question is, how much more RAM will I need to load
    this model at 8-bit precision? (I tried this 4 bit version and it loads successfully:
    https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g, but it gets
    very close using ~97-99% of my system RAM.)'
  created_at: 2023-05-10 05:35:20+00:00
  edited: false
  hidden: false
  id: 645b3b28438d6cfbe1ac5332
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-05-10T06:42:43.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>you need more RAM</p>

          '
        raw: you need more RAM
        updatedAt: '2023-05-10T06:42:43.310Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - jsalsman
        - NEFlame
        - vladi
    id: 645b3ce3368b6d6f6a4247ac
    type: comment
  author: ehartford
  content: you need more RAM
  created_at: 2023-05-10 05:42:43+00:00
  edited: false
  hidden: false
  id: 645b3ce3368b6d6f6a4247ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9653aaee49b4ba831eb2a6671da233ee.svg
      fullname: N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NEFlame
      type: user
    createdAt: '2023-05-10T07:54:49.000Z'
    data:
      edited: false
      editors:
      - NEFlame
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9653aaee49b4ba831eb2a6671da233ee.svg
          fullname: N
          isHf: false
          isPro: false
          name: NEFlame
          type: user
        html: '<p>well RIP. I guess I''ll see if I can scavenge an extra 8GB stick
          somewhere and just roll with the 4bit model for now. Ty for the help.</p>

          '
        raw: well RIP. I guess I'll see if I can scavenge an extra 8GB stick somewhere
          and just roll with the 4bit model for now. Ty for the help.
        updatedAt: '2023-05-10T07:54:49.457Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645b4dc9cd032baba3563af5
    id: 645b4dc9cd032baba3563af4
    type: comment
  author: NEFlame
  content: well RIP. I guess I'll see if I can scavenge an extra 8GB stick somewhere
    and just roll with the 4bit model for now. Ty for the help.
  created_at: 2023-05-10 06:54:49+00:00
  edited: false
  hidden: false
  id: 645b4dc9cd032baba3563af4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9653aaee49b4ba831eb2a6671da233ee.svg
      fullname: N
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NEFlame
      type: user
    createdAt: '2023-05-10T07:54:49.000Z'
    data:
      status: closed
    id: 645b4dc9cd032baba3563af5
    type: status-change
  author: NEFlame
  created_at: 2023-05-10 06:54:49+00:00
  id: 645b4dc9cd032baba3563af5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: cognitivecomputations/WizardLM-13B-Uncensored
repo_type: model
status: closed
target_branch: null
title: RAM (Not VRAM) requirements?
