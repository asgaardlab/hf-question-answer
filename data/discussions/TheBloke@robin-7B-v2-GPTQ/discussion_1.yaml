!!python/object:huggingface_hub.community.DiscussionWithDetails
author: photogbill40
conflicting_files: null
created_at: 2023-06-16 21:19:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d50ebc5051befd245987e69f73eb8745.svg
      fullname: william duncan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: photogbill40
      type: user
    createdAt: '2023-06-16T22:19:56.000Z'
    data:
      edited: false
      editors:
      - photogbill40
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3849930763244629
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d50ebc5051befd245987e69f73eb8745.svg
          fullname: william duncan
          isHf: false
          isPro: false
          name: photogbill40
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/szGyd27oNqTZB-5pJleA7.jpeg"><img
          alt="Screenshot 2023-06-16 181916.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/szGyd27oNqTZB-5pJleA7.jpeg"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/OYbTz0RKDHk487-Zxffor.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/OYbTz0RKDHk487-Zxffor.png"></a></p>

          '
        raw: "\r\n\r\n![Screenshot 2023-06-16 181916.jpg](https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/szGyd27oNqTZB-5pJleA7.jpeg)\r\
          \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/OYbTz0RKDHk487-Zxffor.png)\r\
          \n"
        updatedAt: '2023-06-16T22:19:56.386Z'
      numEdits: 0
      reactions: []
    id: 648ce00c4775370f6a3d9430
    type: comment
  author: photogbill40
  content: "\r\n\r\n![Screenshot 2023-06-16 181916.jpg](https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/szGyd27oNqTZB-5pJleA7.jpeg)\r\
    \n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6304bd237424d937fa33f474/OYbTz0RKDHk487-Zxffor.png)\r\
    \n"
  created_at: 2023-06-16 21:19:56+00:00
  edited: false
  hidden: false
  id: 648ce00c4775370f6a3d9430
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d50ebc5051befd245987e69f73eb8745.svg
      fullname: william duncan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: photogbill40
      type: user
    createdAt: '2023-06-16T22:38:28.000Z'
    data:
      edited: false
      editors:
      - photogbill40
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5091570615768433
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d50ebc5051befd245987e69f73eb8745.svg
          fullname: william duncan
          isHf: false
          isPro: false
          name: photogbill40
          type: user
        html: '<p>2023-06-16 18:37:01 INFO:Loading robin-7B-v2-GPTQ...<br>2023-06-16
          18:37:01 WARNING:Auto-assiging --gpu-memory 7 for your GPU to try to prevent
          out-of-memory errors. You can manually set other values.<br>2023-06-16 18:37:01
          INFO:The AutoGPTQ params are: {''model_basename'': ''robin-7b-GPTQ-4bit-128g.no-act.order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': {0: ''7GiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
          None}<br>2023-06-16 18:37:03 WARNING:The model weights are not tied. Please
          use the <code>tie_weights</code> method before using the <code>infer_auto_device</code>
          function.<br>2023-06-16 18:37:03 WARNING:The model weights are not tied.
          Please use the <code>tie_weights</code> method before using the <code>infer_auto_device</code>
          function.<br>2023-06-16 18:37:03 WARNING:The safetensors archive passed
          at models\robin-7B-v2-GPTQ\robin-7b-GPTQ-4bit-128g.no-act.order.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.<br>2023-06-16 18:37:08 WARNING:skip
          module injection for FusedLlamaMLPForQuantizedModel not support integrate
          without triton yet.<br>2023-06-16 18:37:08 INFO:Loaded the model in 6.33
          seconds.</p>

          <p>2023-06-16 18:37:08 INFO:Loading the extension "gallery"...<br>Running
          on local URL:  <a rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.</p>

          '
        raw: '2023-06-16 18:37:01 INFO:Loading robin-7B-v2-GPTQ...

          2023-06-16 18:37:01 WARNING:Auto-assiging --gpu-memory 7 for your GPU to
          try to prevent out-of-memory errors. You can manually set other values.

          2023-06-16 18:37:01 INFO:The AutoGPTQ params are: {''model_basename'': ''robin-7b-GPTQ-4bit-128g.no-act.order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': {0: ''7GiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
          None}

          2023-06-16 18:37:03 WARNING:The model weights are not tied. Please use the
          `tie_weights` method before using the `infer_auto_device` function.

          2023-06-16 18:37:03 WARNING:The model weights are not tied. Please use the
          `tie_weights` method before using the `infer_auto_device` function.

          2023-06-16 18:37:03 WARNING:The safetensors archive passed at models\robin-7B-v2-GPTQ\robin-7b-GPTQ-4bit-128g.no-act.order.safetensors
          does not contain metadata. Make sure to save your model with the `save_pretrained`
          method. Defaulting to ''pt'' metadata.

          2023-06-16 18:37:08 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel
          not support integrate without triton yet.

          2023-06-16 18:37:08 INFO:Loaded the model in 6.33 seconds.


          2023-06-16 18:37:08 INFO:Loading the extension "gallery"...

          Running on local URL:  http://127.0.0.1:7860


          To create a public link, set `share=True` in `launch()`.

          '
        updatedAt: '2023-06-16T22:38:28.155Z'
      numEdits: 0
      reactions: []
    id: 648ce464439443b08526a2d1
    type: comment
  author: photogbill40
  content: '2023-06-16 18:37:01 INFO:Loading robin-7B-v2-GPTQ...

    2023-06-16 18:37:01 WARNING:Auto-assiging --gpu-memory 7 for your GPU to try to
    prevent out-of-memory errors. You can manually set other values.

    2023-06-16 18:37:01 INFO:The AutoGPTQ params are: {''model_basename'': ''robin-7b-GPTQ-4bit-128g.no-act.order'',
    ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'': True,
    ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
    False, ''max_memory'': {0: ''7GiB'', ''cpu'': ''99GiB''}, ''quantize_config'':
    None}

    2023-06-16 18:37:03 WARNING:The model weights are not tied. Please use the `tie_weights`
    method before using the `infer_auto_device` function.

    2023-06-16 18:37:03 WARNING:The model weights are not tied. Please use the `tie_weights`
    method before using the `infer_auto_device` function.

    2023-06-16 18:37:03 WARNING:The safetensors archive passed at models\robin-7B-v2-GPTQ\robin-7b-GPTQ-4bit-128g.no-act.order.safetensors
    does not contain metadata. Make sure to save your model with the `save_pretrained`
    method. Defaulting to ''pt'' metadata.

    2023-06-16 18:37:08 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel
    not support integrate without triton yet.

    2023-06-16 18:37:08 INFO:Loaded the model in 6.33 seconds.


    2023-06-16 18:37:08 INFO:Loading the extension "gallery"...

    Running on local URL:  http://127.0.0.1:7860


    To create a public link, set `share=True` in `launch()`.

    '
  created_at: 2023-06-16 21:38:28+00:00
  edited: false
  hidden: false
  id: 648ce464439443b08526a2d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e8a95d0b22b2fb2988fedb170e9de3c.svg
      fullname: Sim Guy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Simguy1
      type: user
    createdAt: '2023-06-19T01:00:37.000Z'
    data:
      edited: true
      editors:
      - Simguy1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608825445175171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e8a95d0b22b2fb2988fedb170e9de3c.svg
          fullname: Sim Guy
          isHf: false
          isPro: false
          name: Simguy1
          type: user
        html: '<p>I was also getting this at first, but I fixed the output by ensuring
          my Instruction template properly matched the Prompt template example that''s
          provided in the Readme.<br>I also changed my Turn template. It was using
          \\n, but I swapped it to \n as that seemed to help a bit. My template now
          looks like this :</p>

          <p>user: ''###Human:''<br>bot: ''###Assistant:''<br>turn_template: "&lt;|user|&gt;
          &lt;|user-message|&gt;\n&lt;|bot|&gt; &lt;|bot-message|&gt;&lt;/s&gt;\n"</p>

          '
        raw: "I was also getting this at first, but I fixed the output by ensuring\
          \ my Instruction template properly matched the Prompt template example that's\
          \ provided in the Readme. \nI also changed my Turn template. It was using\
          \ \\\\\\n, but I swapped it to \\n as that seemed to help a bit. My template\
          \ now looks like this :\n\nuser: '###Human:'\nbot: '###Assistant:'\nturn_template:\
          \ \"<|user|> <|user-message|>\\n<|bot|> <|bot-message|>&lt;/s&gt;\\n\"\n"
        updatedAt: '2023-06-19T14:24:58.520Z'
      numEdits: 5
      reactions: []
    id: 648fa8b5008c2e032ec88628
    type: comment
  author: Simguy1
  content: "I was also getting this at first, but I fixed the output by ensuring my\
    \ Instruction template properly matched the Prompt template example that's provided\
    \ in the Readme. \nI also changed my Turn template. It was using \\\\\\n, but\
    \ I swapped it to \\n as that seemed to help a bit. My template now looks like\
    \ this :\n\nuser: '###Human:'\nbot: '###Assistant:'\nturn_template: \"<|user|>\
    \ <|user-message|>\\n<|bot|> <|bot-message|>&lt;/s&gt;\\n\"\n"
  created_at: 2023-06-19 00:00:37+00:00
  edited: true
  hidden: false
  id: 648fa8b5008c2e032ec88628
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/robin-7B-v2-GPTQ
repo_type: model
status: open
target_branch: null
title: Doesn't work for me, gives me gibe
