!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Viewegger
conflicting_files: null
created_at: 2023-08-31 14:48:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
      fullname: Martin Viewegger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Viewegger
      type: user
    createdAt: '2023-08-31T15:48:29.000Z'
    data:
      edited: false
      editors:
      - Viewegger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9583675861358643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
          fullname: Martin Viewegger
          isHf: false
          isPro: false
          name: Viewegger
          type: user
        html: '<p>Hello, </p>

          <p>Thank you for providing these models into the public!<br>I would like
          to try to finetune the model on similar dataset structure, just for different
          language. </p>

          <p>Could I ask you what was your hardware setup and what parameters you
          used? And how long did it take to complete the training in overall?</p>

          '
        raw: "Hello, \r\n\r\nThank you for providing these models into the public!\
          \ \r\nI would like to try to finetune the model on similar dataset structure,\
          \ just for different language. \r\n\r\nCould I ask you what was your hardware\
          \ setup and what parameters you used? And how long did it take to complete\
          \ the training in overall?"
        updatedAt: '2023-08-31T15:48:29.307Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - marksverdhei
    id: 64f0b64d4e9bd3f41814c9b6
    type: comment
  author: Viewegger
  content: "Hello, \r\n\r\nThank you for providing these models into the public! \r\
    \nI would like to try to finetune the model on similar dataset structure, just\
    \ for different language. \r\n\r\nCould I ask you what was your hardware setup\
    \ and what parameters you used? And how long did it take to complete the training\
    \ in overall?"
  created_at: 2023-08-31 14:48:29+00:00
  edited: false
  hidden: false
  id: 64f0b64d4e9bd3f41814c9b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64db79387f749b6e34577a61/wCo7eZHFwHn0_YfTh-HFF.png?w=200&h=200&f=face
      fullname: Ruter
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: RuterNorway
      type: user
    createdAt: '2023-09-01T12:11:17.000Z'
    data:
      edited: false
      editors:
      - RuterNorway
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8985161185264587
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64db79387f749b6e34577a61/wCo7eZHFwHn0_YfTh-HFF.png?w=200&h=200&f=face
          fullname: Ruter
          isHf: false
          isPro: false
          name: RuterNorway
          type: user
        html: '<p>Hello,<br>Thank you for your interest in our work! We''re thrilled
          to hear that you''re considering fine-tuning the model for a different language.</p>

          <h2 id="importance-of-data">Importance of Data:</h2>

          <p>From our experience, the cornerstone of successful fine-tuning lies in
          the quality of the data. In fact, the majority of our R&amp;D efforts were
          focused on this aspect.</p>

          <h2 id="hardware-setup">Hardware Setup</h2>

          <p>We utilized an AWS EC2 instance of type g5.12xlarge, equipped with 4
          x A10G Tensor Core GPUs, for our training.</p>

          <h2 id="training-parameters">Training Parameters</h2>

          <p>Here are the specifics of the parameters we used:</p>

          <ul>

          <li>Batch Size: 32</li>

          <li>Micro Batch Size: 4</li>

          <li>Validation Set Size: 32</li>

          <li>Learning Rate: 4e-4</li>

          <li>Optimizer: adamw_torch</li>

          <li>Warmup Steps: 100</li>

          <li>LoRA Parameters:</li>

          <li><ul>

          <li>lora_r: 16</li>

          </ul>

          </li>

          <li><ul>

          <li>lora_alpha: 32</li>

          </ul>

          </li>

          <li><ul>

          <li>lora_dropout: 0.05</li>

          </ul>

          </li>

          <li><ul>

          <li>qlora: True</li>

          </ul>

          </li>

          <li><ul>

          <li>lora_target_modules: [''q_proj'',''v_proj'']</li>

          </ul>

          </li>

          <li>Load in 8-bit: False</li>

          <li>Cutoff Length: 2048</li>

          </ul>

          <h2 id="training-time">Training Time</h2>

          <p>The training process took approximately 22 hours to complete a single
          epoch.</p>

          <h2 id="additional-tips">Additional Tips:</h2>

          <ul>

          <li>Smaller Models: We recommend starting with the 7b model for quicker
          experimentation. Throughout our project, experimenting with smaller models
          helped us make informed decisions about our dataset.</li>

          <li>Translation Pairs: We''re currently investigating the impact of translation
          pairs, and our early results look promising. You might find it beneficial
          to explore this method further. Our dataset, which focuses on English-Norwegian
          translation pairs, is available here. It was created by matching IDs between
          English and Norwegian datasets, specifically from this source.</li>

          </ul>

          <p>Feel free to reach out if you have any more questions or need further
          clarification.<br>Best regards, Ruter AI team</p>

          '
        raw: 'Hello,

          Thank you for your interest in our work! We''re thrilled to hear that you''re
          considering fine-tuning the model for a different language.

          ## Importance of Data:

          From our experience, the cornerstone of successful fine-tuning lies in the
          quality of the data. In fact, the majority of our R&D efforts were focused
          on this aspect.

          ## Hardware Setup

          We utilized an AWS EC2 instance of type g5.12xlarge, equipped with 4 x A10G
          Tensor Core GPUs, for our training.

          ## Training Parameters

          Here are the specifics of the parameters we used:

          * Batch Size: 32

          * Micro Batch Size: 4

          * Validation Set Size: 32

          * Learning Rate: 4e-4

          * Optimizer: adamw_torch

          * Warmup Steps: 100

          * LoRA Parameters:

          * * lora_r: 16

          * * lora_alpha: 32

          * * lora_dropout: 0.05

          * * qlora: True

          * * lora_target_modules: [''q_proj'',''v_proj'']

          * Load in 8-bit: False

          * Cutoff Length: 2048

          ## Training Time

          The training process took approximately 22 hours to complete a single epoch.

          ## Additional Tips:

          * Smaller Models: We recommend starting with the 7b model for quicker experimentation.
          Throughout our project, experimenting with smaller models helped us make
          informed decisions about our dataset.

          * Translation Pairs: We''re currently investigating the impact of translation
          pairs, and our early results look promising. You might find it beneficial
          to explore this method further. Our dataset, which focuses on English-Norwegian
          translation pairs, is available here. It was created by matching IDs between
          English and Norwegian datasets, specifically from this source.


          Feel free to reach out if you have any more questions or need further clarification.

          Best regards, Ruter AI team'
        updatedAt: '2023-09-01T12:11:17.097Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - marksverdhei
        - Viewegger
    id: 64f1d4e501c87d8c9d235c28
    type: comment
  author: RuterNorway
  content: 'Hello,

    Thank you for your interest in our work! We''re thrilled to hear that you''re
    considering fine-tuning the model for a different language.

    ## Importance of Data:

    From our experience, the cornerstone of successful fine-tuning lies in the quality
    of the data. In fact, the majority of our R&D efforts were focused on this aspect.

    ## Hardware Setup

    We utilized an AWS EC2 instance of type g5.12xlarge, equipped with 4 x A10G Tensor
    Core GPUs, for our training.

    ## Training Parameters

    Here are the specifics of the parameters we used:

    * Batch Size: 32

    * Micro Batch Size: 4

    * Validation Set Size: 32

    * Learning Rate: 4e-4

    * Optimizer: adamw_torch

    * Warmup Steps: 100

    * LoRA Parameters:

    * * lora_r: 16

    * * lora_alpha: 32

    * * lora_dropout: 0.05

    * * qlora: True

    * * lora_target_modules: [''q_proj'',''v_proj'']

    * Load in 8-bit: False

    * Cutoff Length: 2048

    ## Training Time

    The training process took approximately 22 hours to complete a single epoch.

    ## Additional Tips:

    * Smaller Models: We recommend starting with the 7b model for quicker experimentation.
    Throughout our project, experimenting with smaller models helped us make informed
    decisions about our dataset.

    * Translation Pairs: We''re currently investigating the impact of translation
    pairs, and our early results look promising. You might find it beneficial to explore
    this method further. Our dataset, which focuses on English-Norwegian translation
    pairs, is available here. It was created by matching IDs between English and Norwegian
    datasets, specifically from this source.


    Feel free to reach out if you have any more questions or need further clarification.

    Best regards, Ruter AI team'
  created_at: 2023-09-01 11:11:17+00:00
  edited: false
  hidden: false
  id: 64f1d4e501c87d8c9d235c28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
      fullname: Martin Viewegger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Viewegger
      type: user
    createdAt: '2023-09-02T09:27:37.000Z'
    data:
      edited: false
      editors:
      - Viewegger
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9869652390480042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
          fullname: Martin Viewegger
          isHf: false
          isPro: false
          name: Viewegger
          type: user
        html: '<p>That''s actually much less demanding than I thought, I have a local
          rig with 4x3090 so with some testing it shouldn''t take more than few weeks.
          </p>

          <p>Thank you! </p>

          '
        raw: "That's actually much less demanding than I thought, I have a local rig\
          \ with 4x3090 so with some testing it shouldn't take more than few weeks.\
          \ \n\nThank you! "
        updatedAt: '2023-09-02T09:27:37.796Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f30009496a22051bca1576
    id: 64f30009496a22051bca1575
    type: comment
  author: Viewegger
  content: "That's actually much less demanding than I thought, I have a local rig\
    \ with 4x3090 so with some testing it shouldn't take more than few weeks. \n\n\
    Thank you! "
  created_at: 2023-09-02 08:27:37+00:00
  edited: false
  hidden: false
  id: 64f30009496a22051bca1575
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7ca2d750fb67cc848dc07a9161bfa9dd.svg
      fullname: Martin Viewegger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Viewegger
      type: user
    createdAt: '2023-09-02T09:27:37.000Z'
    data:
      status: closed
    id: 64f30009496a22051bca1576
    type: status-change
  author: Viewegger
  created_at: 2023-09-02 08:27:37+00:00
  id: 64f30009496a22051bca1576
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: RuterNorway/Llama-2-13b-chat-norwegian
repo_type: model
status: closed
target_branch: null
title: Training details
