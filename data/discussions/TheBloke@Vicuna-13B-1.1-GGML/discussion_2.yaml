!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nicoleds
conflicting_files: null
created_at: 2023-06-22 04:49:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
      fullname: Nicole
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicoleds
      type: user
    createdAt: '2023-06-22T05:49:02.000Z'
    data:
      edited: false
      editors:
      - nicoleds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7569864988327026
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7622c1e940db9d5cfb4ae599c608d5c8.svg
          fullname: Nicole
          isHf: false
          isPro: false
          name: nicoleds
          type: user
        html: '<p>Can the quantized models be downloaded using the transformers library?
          Tried the code as follows and it returned OSError: TheBloke/vicuna-13b-1.1-GGML
          does not appear to have a file named config.json.</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("TheBloke/vicuna-13b-1.1-GGML")<br>model
          = AutoModelForCausalLM.from_pretrained("TheBloke/vicuna-13b-1.1-GGML")</p>

          <p>How do we specify which quantized model (i.e 4bit) to be used?</p>

          <p>Thanks!</p>

          '
        raw: "Can the quantized models be downloaded using the transformers library?\
          \ Tried the code as follows and it returned OSError: TheBloke/vicuna-13b-1.1-GGML\
          \ does not appear to have a file named config.json.\r\n\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          TheBloke/vicuna-13b-1.1-GGML\")    \r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/vicuna-13b-1.1-GGML\")\r\n\r\nHow do we specify which quantized\
          \ model (i.e 4bit) to be used?\r\n\r\nThanks!"
        updatedAt: '2023-06-22T05:49:02.670Z'
      numEdits: 0
      reactions: []
    id: 6493e0cef6b85df2b730233b
    type: comment
  author: nicoleds
  content: "Can the quantized models be downloaded using the transformers library?\
    \ Tried the code as follows and it returned OSError: TheBloke/vicuna-13b-1.1-GGML\
    \ does not appear to have a file named config.json.\r\n\r\nfrom transformers import\
    \ AutoTokenizer, AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/vicuna-13b-1.1-GGML\")    \r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/vicuna-13b-1.1-GGML\")\r\n\r\nHow do we specify which quantized model\
    \ (i.e 4bit) to be used?\r\n\r\nThanks!"
  created_at: 2023-06-22 04:49:02+00:00
  edited: false
  hidden: false
  id: 6493e0cef6b85df2b730233b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-22T08:26:44.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8213678598403931
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, transformers can''t handle GGML files in any way.</p>

          <p>But ctransformers can, including downloading and loading individual GGML
          files: <a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a></p>

          '
        raw: 'No, transformers can''t handle GGML files in any way.


          But ctransformers can, including downloading and loading individual GGML
          files: https://github.com/marella/ctransformers'
        updatedAt: '2023-06-22T08:26:44.266Z'
      numEdits: 0
      reactions: []
    id: 649405c45ca54da633d1dc6a
    type: comment
  author: TheBloke
  content: 'No, transformers can''t handle GGML files in any way.


    But ctransformers can, including downloading and loading individual GGML files:
    https://github.com/marella/ctransformers'
  created_at: 2023-06-22 07:26:44+00:00
  edited: false
  hidden: false
  id: 649405c45ca54da633d1dc6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f958df6004748725b01f840584e15719.svg
      fullname: Konstantinos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ConsSSJUI
      type: user
    createdAt: '2023-08-02T21:08:04.000Z'
    data:
      edited: false
      editors:
      - ConsSSJUI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8709835410118103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f958df6004748725b01f840584e15719.svg
          fullname: Konstantinos
          isHf: false
          isPro: false
          name: ConsSSJUI
          type: user
        html: "<p>That's very useful information! <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ may I ask if it happens to know if ctransformers support chat completion?\
          \ When I use it it just autocompletes sentences.</p>\n"
        raw: That's very useful information! @TheBloke may I ask if it happens to
          know if ctransformers support chat completion? When I use it it just autocompletes
          sentences.
        updatedAt: '2023-08-02T21:08:04.616Z'
      numEdits: 0
      reactions: []
    id: 64cac5b4710645aa7be7219b
    type: comment
  author: ConsSSJUI
  content: That's very useful information! @TheBloke may I ask if it happens to know
    if ctransformers support chat completion? When I use it it just autocompletes
    sentences.
  created_at: 2023-08-02 20:08:04+00:00
  edited: false
  hidden: false
  id: 64cac5b4710645aa7be7219b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T09:52:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8928667306900024
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It can provide an OpenAI compatible API so yeah that should have
          a chat API mode. Not tried it myself</p>

          '
        raw: It can provide an OpenAI compatible API so yeah that should have a chat
          API mode. Not tried it myself
        updatedAt: '2023-08-05T09:52:16.230Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ConsSSJUI
    id: 64ce1bd03c4a1b39a0b9dc9b
    type: comment
  author: TheBloke
  content: It can provide an OpenAI compatible API so yeah that should have a chat
    API mode. Not tried it myself
  created_at: 2023-08-05 08:52:16+00:00
  edited: false
  hidden: false
  id: 64ce1bd03c4a1b39a0b9dc9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1c960b332ed816bda40f14331770d792.svg
      fullname: S M FAROOQ BASHA
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: farooq9786
      type: user
    createdAt: '2023-11-08T08:05:38.000Z'
    data:
      edited: false
      editors:
      - farooq9786
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9021884202957153
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1c960b332ed816bda40f14331770d792.svg
          fullname: S M FAROOQ BASHA
          isHf: false
          isPro: false
          name: farooq9786
          type: user
        html: '<p>I have downloaded llama-2-7b-chat.ggmlv3.q4_0.bin and when I am
          trying to load the model using ctransformers I am getting error as GLIBC_2.29
          compatibility and I am using RHEL which support GLIBC_2.17. Can I use any
          other model instead of GGML? </p>

          <p>PS: I don''t have much GPU </p>

          '
        raw: "I have downloaded llama-2-7b-chat.ggmlv3.q4_0.bin and when I am trying\
          \ to load the model using ctransformers I am getting error as GLIBC_2.29\
          \ compatibility and I am using RHEL which support GLIBC_2.17. Can I use\
          \ any other model instead of GGML? \n\nPS: I don't have much GPU \n"
        updatedAt: '2023-11-08T08:05:38.091Z'
      numEdits: 0
      reactions: []
    id: 654b4152f39b362cffd8ef55
    type: comment
  author: farooq9786
  content: "I have downloaded llama-2-7b-chat.ggmlv3.q4_0.bin and when I am trying\
    \ to load the model using ctransformers I am getting error as GLIBC_2.29 compatibility\
    \ and I am using RHEL which support GLIBC_2.17. Can I use any other model instead\
    \ of GGML? \n\nPS: I don't have much GPU \n"
  created_at: 2023-11-08 08:05:38+00:00
  edited: false
  hidden: false
  id: 654b4152f39b362cffd8ef55
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Vicuna-13B-1.1-GGML
repo_type: model
status: open
target_branch: null
title: Using transformers library to download the files
