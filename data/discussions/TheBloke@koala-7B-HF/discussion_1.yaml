!!python/object:huggingface_hub.community.DiscussionWithDetails
author: itnomad
conflicting_files: null
created_at: 2023-06-06 05:36:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e31c774fbd9c76b6534fba6134449a42.svg
      fullname: Alexander Janssen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itnomad
      type: user
    createdAt: '2023-06-06T06:36:58.000Z'
    data:
      edited: false
      editors:
      - itnomad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9719843864440918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e31c774fbd9c76b6534fba6134449a42.svg
          fullname: Alexander Janssen
          isHf: false
          isPro: false
          name: itnomad
          type: user
        html: '<p>This is the first text-generation model which is not utterly dumb,
          but also which works with my setup (Radeon Instinct MI25 with ROCm &amp;
          CUDA-wrapper). All these other models either don''t really work with transformers,
          or rely on libraries such as xformers, which also needs "native NVIDIA CUDA".
          Another challenge is that my MI25 only has 16 GB of RAM, so 7B is the largest
          model which works at all...</p>

          <p>I''m now gonna look at your Patreon; looking forward for other models
          which fit into my GPU :-)</p>

          <p>All the best from DE,<br>Alex.</p>

          '
        raw: "This is the first text-generation model which is not utterly dumb, but\
          \ also which works with my setup (Radeon Instinct MI25 with ROCm & CUDA-wrapper).\
          \ All these other models either don't really work with transformers, or\
          \ rely on libraries such as xformers, which also needs \"native NVIDIA CUDA\"\
          . Another challenge is that my MI25 only has 16 GB of RAM, so 7B is the\
          \ largest model which works at all...\r\n\r\nI'm now gonna look at your\
          \ Patreon; looking forward for other models which fit into my GPU :-)\r\n\
          \r\nAll the best from DE,\r\nAlex."
        updatedAt: '2023-06-06T06:36:58.353Z'
      numEdits: 0
      reactions: []
    id: 647ed40a0633ff4de45916f6
    type: comment
  author: itnomad
  content: "This is the first text-generation model which is not utterly dumb, but\
    \ also which works with my setup (Radeon Instinct MI25 with ROCm & CUDA-wrapper).\
    \ All these other models either don't really work with transformers, or rely on\
    \ libraries such as xformers, which also needs \"native NVIDIA CUDA\". Another\
    \ challenge is that my MI25 only has 16 GB of RAM, so 7B is the largest model\
    \ which works at all...\r\n\r\nI'm now gonna look at your Patreon; looking forward\
    \ for other models which fit into my GPU :-)\r\n\r\nAll the best from DE,\r\n\
    Alex."
  created_at: 2023-06-06 05:36:58+00:00
  edited: false
  hidden: false
  id: 647ed40a0633ff4de45916f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/koala-7B-HF
repo_type: model
status: open
target_branch: null
title: Thank you so much!
