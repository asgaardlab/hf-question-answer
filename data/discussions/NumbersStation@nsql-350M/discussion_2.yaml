!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eramlak
conflicting_files: null
created_at: 2023-07-06 20:09:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1c462d401af1219d52cdd739d99c4d0.svg
      fullname: raml
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramlak
      type: user
    createdAt: '2023-07-06T21:09:45.000Z'
    data:
      edited: false
      editors:
      - eramlak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8967386484146118
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1c462d401af1219d52cdd739d99c4d0.svg
          fullname: raml
          isHf: false
          isPro: false
          name: eramlak
          type: user
        html: '<p>hi, thanks for the contribution. Does the training include samples
          from biquery sql and other variants of sql?. Also can you eloborate on your
          2 step approach for pretrain and instruct fine tune?. What is the dataset
          for pretrain is it just sql statements without questions and you do next
          token prediction? </p>

          '
        raw: 'hi, thanks for the contribution. Does the training include samples from
          biquery sql and other variants of sql?. Also can you eloborate on your 2
          step approach for pretrain and instruct fine tune?. What is the dataset
          for pretrain is it just sql statements without questions and you do next
          token prediction? '
        updatedAt: '2023-07-06T21:09:45.872Z'
      numEdits: 0
      reactions: []
    id: 64a72d99b5e5c56860aa78bc
    type: comment
  author: eramlak
  content: 'hi, thanks for the contribution. Does the training include samples from
    biquery sql and other variants of sql?. Also can you eloborate on your 2 step
    approach for pretrain and instruct fine tune?. What is the dataset for pretrain
    is it just sql statements without questions and you do next token prediction? '
  created_at: 2023-07-06 20:09:45+00:00
  edited: false
  hidden: false
  id: 64a72d99b5e5c56860aa78bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-07-06T21:26:01.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8214701414108276
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>Thanks for your interest in our work!</p>

          <p>For the pertaining step, we use the SQL subset from The Stack (<a href="https://huggingface.co/datasets/bigcode/the-stack">https://huggingface.co/datasets/bigcode/the-stack</a>),
          containing around 1M training samples. We use the raw SQL data with the
          next token prediction for continuous pertaining.<br>For the instruct fine-tuning
          step, we collect text-to-SQL pairs from more than 20 different public sources
          across the web from standard datasets such as WikiSQL to medical datasets
          such as MIMIC_III, containing around 300,000 samples of text-to-SQL pairs.</p>

          <p>You can find more information from our blog (<a rel="nofollow" href="https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models">https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models</a>).</p>

          '
        raw: 'Thanks for your interest in our work!


          For the pertaining step, we use the SQL subset from The Stack (https://huggingface.co/datasets/bigcode/the-stack),
          containing around 1M training samples. We use the raw SQL data with the
          next token prediction for continuous pertaining.

          For the instruct fine-tuning step, we collect text-to-SQL pairs from more
          than 20 different public sources across the web from standard datasets such
          as WikiSQL to medical datasets such as MIMIC_III, containing around 300,000
          samples of text-to-SQL pairs.


          You can find more information from our blog (https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models).'
        updatedAt: '2023-07-06T21:26:01.474Z'
      numEdits: 0
      reactions: []
    id: 64a73169dc911aa42e3f4d11
    type: comment
  author: senwu
  content: 'Thanks for your interest in our work!


    For the pertaining step, we use the SQL subset from The Stack (https://huggingface.co/datasets/bigcode/the-stack),
    containing around 1M training samples. We use the raw SQL data with the next token
    prediction for continuous pertaining.

    For the instruct fine-tuning step, we collect text-to-SQL pairs from more than
    20 different public sources across the web from standard datasets such as WikiSQL
    to medical datasets such as MIMIC_III, containing around 300,000 samples of text-to-SQL
    pairs.


    You can find more information from our blog (https://www.numbersstation.ai/post/introducing-nsql-open-source-sql-copilot-foundation-models).'
  created_at: 2023-07-06 20:26:01+00:00
  edited: false
  hidden: false
  id: 64a73169dc911aa42e3f4d11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c1c462d401af1219d52cdd739d99c4d0.svg
      fullname: raml
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eramlak
      type: user
    createdAt: '2023-07-06T21:44:14.000Z'
    data:
      edited: false
      editors:
      - eramlak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9026824831962585
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c1c462d401af1219d52cdd739d99c4d0.svg
          fullname: raml
          isHf: false
          isPro: false
          name: eramlak
          type: user
        html: '<p>Have you seen any catastrophic interference with the pretraining
          step? Was it only pretrained on the SQL data or do you mix other dataset
          which was used in the salesforce codegen?. Do you plan to opensource the
          training code?</p>

          '
        raw: Have you seen any catastrophic interference with the pretraining step?
          Was it only pretrained on the SQL data or do you mix other dataset which
          was used in the salesforce codegen?. Do you plan to opensource the training
          code?
        updatedAt: '2023-07-06T21:44:14.401Z'
      numEdits: 0
      reactions: []
    id: 64a735ae0c36529d75f993ea
    type: comment
  author: eramlak
  content: Have you seen any catastrophic interference with the pretraining step?
    Was it only pretrained on the SQL data or do you mix other dataset which was used
    in the salesforce codegen?. Do you plan to opensource the training code?
  created_at: 2023-07-06 20:44:14+00:00
  edited: false
  hidden: false
  id: 64a735ae0c36529d75f993ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
      fullname: Sen Wu
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: senwu
      type: user
    createdAt: '2023-07-06T22:20:00.000Z'
    data:
      edited: false
      editors:
      - senwu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9035049080848694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c4682fe1fe9d9219f46b73bbe2e7b08.svg
          fullname: Sen Wu
          isHf: false
          isPro: false
          name: senwu
          type: user
        html: '<p>With the pretraining step, we intend to let the model understand
          more about SQL and it did improve the text-to-SQL capability (You can find
          the analysis in our blog). We only pre-trained on the SQL data w/o mixing
          any other data used in salesforce codegen pertaining. We''ll release the
          instruct fine-tuning data soon. Stay tuned!</p>

          '
        raw: With the pretraining step, we intend to let the model understand more
          about SQL and it did improve the text-to-SQL capability (You can find the
          analysis in our blog). We only pre-trained on the SQL data w/o mixing any
          other data used in salesforce codegen pertaining. We'll release the instruct
          fine-tuning data soon. Stay tuned!
        updatedAt: '2023-07-06T22:20:00.743Z'
      numEdits: 0
      reactions: []
    id: 64a73e10d738314d685e72e1
    type: comment
  author: senwu
  content: With the pretraining step, we intend to let the model understand more about
    SQL and it did improve the text-to-SQL capability (You can find the analysis in
    our blog). We only pre-trained on the SQL data w/o mixing any other data used
    in salesforce codegen pertaining. We'll release the instruct fine-tuning data
    soon. Stay tuned!
  created_at: 2023-07-06 21:20:00+00:00
  edited: false
  hidden: false
  id: 64a73e10d738314d685e72e1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: NumbersStation/nsql-350M
repo_type: model
status: open
target_branch: null
title: questions on pretrain and sql formats
