!!python/object:huggingface_hub.community.DiscussionWithDetails
author: leojames
conflicting_files: null
created_at: 2023-10-31 08:24:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a8c44891f3e8243c8cfe76286adbd4a.svg
      fullname: James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leojames
      type: user
    createdAt: '2023-10-31T09:24:08.000Z'
    data:
      edited: false
      editors:
      - leojames
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47128763794898987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a8c44891f3e8243c8cfe76286adbd4a.svg
          fullname: James
          isHf: false
          isPro: false
          name: leojames
          type: user
        html: '<p> When I use the above method for inference with Codellama, I encounter
          CUDA kernel errors. Please help me understand why?</p>

          <p>WARNING:  WatchFiles detected changes in ''fastapi_vllm_codellama.py''.
          Reloading...<br>INFO 10-31 16:58:55 llm_engine.py:72] Initializing an LLM
          engine with config: model=''./CodeLlama-13B-AWQ'', tokenizer=''./CodeLlama-13B-AWQ'',
          tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16,
          max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1,
          quantization=awq, seed=0)<br>INFO 10-31 16:58:55 tokenizer.py:30] For some
          LLaMA V1 models, initializing the fast tokenizer may take a long time. To
          reduce the initialization time, consider using ''hf-internal-testing/llama-tokenizer''
          instead of the original tokenizer.<br>Process SpawnProcess-46:<br>Traceback
          (most recent call last):<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/multiprocessing/process.py",
          line 315, in _bootstrap<br>    self.run()<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/multiprocessing/process.py",
          line 108, in run<br>    self._target(*self._args, **self._kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/_subprocess.py",
          line 76, in subprocess_started<br>    target(sockets=sockets)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/server.py",
          line 61, in run<br>    return asyncio.run(self.serve(sockets=sockets))<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/asyncio/runners.py", line
          44, in run<br>    return loop.run_until_complete(main)<br>  File "uvloop/loop.pyx",
          line 1517, in uvloop.loop.Loop.run_until_complete<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/server.py",
          line 68, in serve<br>    config.load()<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/config.py",
          line 473, in load<br>    self.loaded_app = import_from_string(self.app)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/importer.py",
          line 21, in import_from_string<br>    module = importlib.import_module(module_str)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/importlib/<strong>init</strong>.py",
          line 126, in import_module<br>    return _bootstrap._gcd_import(name[level:],
          package, level)<br>  File "", line 1050, in _gcd_import<br>  File "", line
          1027, in _find_and_load<br>  File "", line 1006, in _find_and_load_unlocked<br>  File
          "", line 688, in _load_unlocked<br>  File "", line 883, in exec_module<br>  File
          "", line 241, in _call_with_frames_removed<br>  File "/mnt/gpu/code/fastapi_vllm_codellama.py",
          line 22, in <br>    llm = LLM(model="./CodeLlama-13B-AWQ", quantization="awq")<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/entrypoints/llm.py",
          line 89, in <strong>init</strong><br>    self.llm_engine = LLMEngine.from_engine_args(engine_args)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 229, in from_engine_args<br>    engine = cls(*engine_configs,<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 111, in <strong>init</strong><br>    self._init_cache()<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 191, in _init_cache<br>    num_blocks = self._run_workers(<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py",
          line 692, in _run_workers<br>    output = executor(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/worker/worker.py",
          line 109, in profile_num_available_blocks<br>    self.model(<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py",
          line 297, in forward<br>    hidden_states = self.model(input_ids, positions,
          kv_caches,<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py",
          line 257, in forward<br>    hidden_states = layer(<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py",
          line 216, in forward<br>    hidden_states = self.mlp(hidden_states)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py",
          line 81, in forward<br>    gate_up, _ = self.gate_up_proj(x)<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/parallel_utils/tensor_parallel/layers.py",
          line 238, in forward<br>    output_parallel = self.apply_weights(input_parallel,
          bias)<br>  File "/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/layers/quantized_linear/awq.py",
          line 55, in apply_weights<br>    out = quantization_ops.awq_gemm(reshaped_x,
          self.qweight, self.scales,<br>RuntimeError: CUDA error: an illegal memory
          access was encountered<br>Compile with <code>TORCH_USE_CUDA_DSA</code> to
          enable device-side assertions.</p>

          '
        raw: " When I use the above method for inference with Codellama, I encounter\
          \ CUDA kernel errors. Please help me understand why?\r\n\r\n\r\nWARNING:\
          \  WatchFiles detected changes in 'fastapi_vllm_codellama.py'. Reloading...\r\
          \nINFO 10-31 16:58:55 llm_engine.py:72] Initializing an LLM engine with\
          \ config: model='./CodeLlama-13B-AWQ', tokenizer='./CodeLlama-13B-AWQ',\
          \ tokenizer_mode=auto, revision=None, trust_remote_code=False, dtype=torch.float16,\
          \ max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1,\
          \ quantization=awq, seed=0)\r\nINFO 10-31 16:58:55 tokenizer.py:30] For\
          \ some LLaMA V1 models, initializing the fast tokenizer may take a long\
          \ time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer'\
          \ instead of the original tokenizer.\r\nProcess SpawnProcess-46:\r\nTraceback\
          \ (most recent call last):\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/multiprocessing/process.py\"\
          , line 315, in _bootstrap\r\n    self.run()\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/multiprocessing/process.py\"\
          , line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n\
          \  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/_subprocess.py\"\
          , line 76, in subprocess_started\r\n    target(sockets=sockets)\r\n  File\
          \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/server.py\"\
          , line 61, in run\r\n    return asyncio.run(self.serve(sockets=sockets))\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/asyncio/runners.py\"\
          , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File\
          \ \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/server.py\"\
          , line 68, in serve\r\n    config.load()\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/config.py\"\
          , line 473, in load\r\n    self.loaded_app = import_from_string(self.app)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/importer.py\"\
          , line 21, in import_from_string\r\n    module = importlib.import_module(module_str)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/importlib/__init__.py\"\
          , line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:],\
          \ package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050,\
          \ in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1027,\
          \ in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006,\
          \ in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\"\
          , line 688, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\"\
          , line 883, in exec_module\r\n  File \"<frozen importlib._bootstrap>\",\
          \ line 241, in _call_with_frames_removed\r\n  File \"/mnt/gpu/code/fastapi_vllm_codellama.py\"\
          , line 22, in <module>\r\n    llm = LLM(model=\"./CodeLlama-13B-AWQ\", quantization=\"\
          awq\")\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/entrypoints/llm.py\"\
          , line 89, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 229, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n\
          \  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 111, in __init__\r\n    self._init_cache()\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 191, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File\
          \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
          , line 692, in _run_workers\r\n    output = executor(*args, **kwargs)\r\n\
          \  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/worker/worker.py\"\
          , line 109, in profile_num_available_blocks\r\n    self.model(\r\n  File\
          \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
          , line 297, in forward\r\n    hidden_states = self.model(input_ids, positions,\
          \ kv_caches,\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
          , line 257, in forward\r\n    hidden_states = layer(\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
          , line 216, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n\
          \  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
          , line 81, in forward\r\n    gate_up, _ = self.gate_up_proj(x)\r\n  File\
          \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/parallel_utils/tensor_parallel/layers.py\"\
          , line 238, in forward\r\n    output_parallel = self.apply_weights(input_parallel,\
          \ bias)\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/layers/quantized_linear/awq.py\"\
          , line 55, in apply_weights\r\n    out = quantization_ops.awq_gemm(reshaped_x,\
          \ self.qweight, self.scales,\r\nRuntimeError: CUDA error: an illegal memory\
          \ access was encountered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable\
          \ device-side assertions."
        updatedAt: '2023-10-31T09:24:08.123Z'
      numEdits: 0
      reactions: []
    id: 6540c7b8231ce22e2a9ca572
    type: comment
  author: leojames
  content: " When I use the above method for inference with Codellama, I encounter\
    \ CUDA kernel errors. Please help me understand why?\r\n\r\n\r\nWARNING:  WatchFiles\
    \ detected changes in 'fastapi_vllm_codellama.py'. Reloading...\r\nINFO 10-31\
    \ 16:58:55 llm_engine.py:72] Initializing an LLM engine with config: model='./CodeLlama-13B-AWQ',\
    \ tokenizer='./CodeLlama-13B-AWQ', tokenizer_mode=auto, revision=None, trust_remote_code=False,\
    \ dtype=torch.float16, max_seq_len=16384, download_dir=None, load_format=auto,\
    \ tensor_parallel_size=1, quantization=awq, seed=0)\r\nINFO 10-31 16:58:55 tokenizer.py:30]\
    \ For some LLaMA V1 models, initializing the fast tokenizer may take a long time.\
    \ To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer'\
    \ instead of the original tokenizer.\r\nProcess SpawnProcess-46:\r\nTraceback\
    \ (most recent call last):\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/multiprocessing/process.py\"\
    , line 315, in _bootstrap\r\n    self.run()\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/multiprocessing/process.py\"\
    , line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/_subprocess.py\"\
    , line 76, in subprocess_started\r\n    target(sockets=sockets)\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/server.py\"\
    , line 61, in run\r\n    return asyncio.run(self.serve(sockets=sockets))\r\n \
    \ File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/asyncio/runners.py\"\
    , line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"uvloop/loop.pyx\"\
    , line 1517, in uvloop.loop.Loop.run_until_complete\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/server.py\"\
    , line 68, in serve\r\n    config.load()\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/config.py\"\
    , line 473, in load\r\n    self.loaded_app = import_from_string(self.app)\r\n\
    \  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/uvicorn/importer.py\"\
    , line 21, in import_from_string\r\n    module = importlib.import_module(module_str)\r\
    \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/importlib/__init__.py\"\
    , line 126, in import_module\r\n    return _bootstrap._gcd_import(name[level:],\
    \ package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\r\
    \n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n \
    \ File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\
    \n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File\
    \ \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\r\n  File\
    \ \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\r\n\
    \  File \"/mnt/gpu/code/fastapi_vllm_codellama.py\", line 22, in <module>\r\n\
    \    llm = LLM(model=\"./CodeLlama-13B-AWQ\", quantization=\"awq\")\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/entrypoints/llm.py\"\
    , line 89, in __init__\r\n    self.llm_engine = LLMEngine.from_engine_args(engine_args)\r\
    \n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 229, in from_engine_args\r\n    engine = cls(*engine_configs,\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 111, in __init__\r\n    self._init_cache()\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 191, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File \"\
    /mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/engine/llm_engine.py\"\
    , line 692, in _run_workers\r\n    output = executor(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/worker/worker.py\"\
    , line 109, in profile_num_available_blocks\r\n    self.model(\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
    , line 297, in forward\r\n    hidden_states = self.model(input_ids, positions,\
    \ kv_caches,\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
    , line 257, in forward\r\n    hidden_states = layer(\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
    , line 216, in forward\r\n    hidden_states = self.mlp(hidden_states)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\"\
    , line 81, in forward\r\n    gate_up, _ = self.gate_up_proj(x)\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/parallel_utils/tensor_parallel/layers.py\"\
    , line 238, in forward\r\n    output_parallel = self.apply_weights(input_parallel,\
    \ bias)\r\n  File \"/mnt/gpu/code/miniconda/envs/code/lib/python3.10/site-packages/vllm/model_executor/layers/quantized_linear/awq.py\"\
    , line 55, in apply_weights\r\n    out = quantization_ops.awq_gemm(reshaped_x,\
    \ self.qweight, self.scales,\r\nRuntimeError: CUDA error: an illegal memory access\
    \ was encountered\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions."
  created_at: 2023-10-31 08:24:08+00:00
  edited: false
  hidden: false
  id: 6540c7b8231ce22e2a9ca572
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/CodeLlama-13B-AWQ
repo_type: model
status: open
target_branch: null
title: 'codellama-13b-awq RuntimeError: CUDA error: an illegal memory access was encountered'
