!!python/object:huggingface_hub.community.DiscussionWithDetails
author: woshinibaba
conflicting_files: null
created_at: 2023-11-12 19:40:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e8cd225cac80fb91583d1a59da58aea.svg
      fullname: aaa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: woshinibaba
      type: user
    createdAt: '2023-11-12T19:40:13.000Z'
    data:
      edited: true
      editors:
      - woshinibaba
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9169438481330872
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e8cd225cac80fb91583d1a59da58aea.svg
          fullname: aaa
          isHf: false
          isPro: false
          name: woshinibaba
          type: user
        html: '<p>Hi, greatly thanks for your nice work on the contextual ASR decoder!</p>

          <p>I am very interested in this biasing decoding technique illustrated in
          your paper(<a rel="nofollow" href="https://arxiv.org/abs/2109.00627">https://arxiv.org/abs/2109.00627</a>).</p>

          <p>Have you tried to implement the TCPGen in the Transformer-based ASR model?</p>

          <p>As illustrated in the paper, the query combines the context vector \boldsymbol{c}_{i}
          and the previously decoded token \boldsymbol{y}_{i-1}. However, for the
          transformer-based ASR model, I am a little bit confused about which context
          vector is used since the decoder usually has 6 attention blocks, and each
          attention block will attend the encoded input to a context matrix. </p>

          <p>Could you give me more clues about it? Thanks!</p>

          '
        raw: "Hi, greatly thanks for your nice work on the contextual ASR decoder!\n\
          \nI am very interested in this biasing decoding technique illustrated in\
          \ your paper(https://arxiv.org/abs/2109.00627).\n\nHave you tried to implement\
          \ the TCPGen in the Transformer-based ASR model?\n\nAs illustrated in the\
          \ paper, the query combines the context vector \\boldsymbol{c}\\_{i} and\
          \ the previously decoded token \\boldsymbol{y}\\_{i-1}. However, for the\
          \ transformer-based ASR model, I am a little bit confused about which context\
          \ vector is used since the decoder usually has 6 attention blocks, and each\
          \ attention block will attend the encoded input to a context matrix. \n\n\
          Could you give me more clues about it? Thanks!"
        updatedAt: '2023-11-12T19:40:40.215Z'
      numEdits: 1
      reactions: []
    id: 65512a1deac892781896128e
    type: comment
  author: woshinibaba
  content: "Hi, greatly thanks for your nice work on the contextual ASR decoder!\n\
    \nI am very interested in this biasing decoding technique illustrated in your\
    \ paper(https://arxiv.org/abs/2109.00627).\n\nHave you tried to implement the\
    \ TCPGen in the Transformer-based ASR model?\n\nAs illustrated in the paper, the\
    \ query combines the context vector \\boldsymbol{c}\\_{i} and the previously decoded\
    \ token \\boldsymbol{y}\\_{i-1}. However, for the transformer-based ASR model,\
    \ I am a little bit confused about which context vector is used since the decoder\
    \ usually has 6 attention blocks, and each attention block will attend the encoded\
    \ input to a context matrix. \n\nCould you give me more clues about it? Thanks!"
  created_at: 2023-11-12 19:40:13+00:00
  edited: true
  hidden: false
  id: 65512a1deac892781896128e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dacd40e553912924645fbb6051d5b062.svg
      fullname: yuwy-thu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuwy-thu
      type: user
    createdAt: '2023-11-13T11:48:34.000Z'
    data:
      edited: false
      editors:
      - yuwy-thu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.82486492395401
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dacd40e553912924645fbb6051d5b062.svg
          fullname: yuwy-thu
          isHf: false
          isPro: false
          name: yuwy-thu
          type: user
        html: '<p>Hi. Thanks for your interest! Please check our latest work on integrating
          TCPGen into Whisper: <a rel="nofollow" href="https://arxiv.org/abs/2306.01942">https://arxiv.org/abs/2306.01942</a>.
          This is an example of applying it to a Transformer-based ASR decoder and
          how it performs without changing any original model parameter. So here,
          TCPGen works as a distribution-level adaptor. </p>

          '
        raw: 'Hi. Thanks for your interest! Please check our latest work on integrating
          TCPGen into Whisper: https://arxiv.org/abs/2306.01942. This is an example
          of applying it to a Transformer-based ASR decoder and how it performs without
          changing any original model parameter. So here, TCPGen works as a distribution-level
          adaptor. '
        updatedAt: '2023-11-13T11:48:34.734Z'
      numEdits: 0
      reactions: []
    id: 65520d1204d4294fa01c0d78
    type: comment
  author: yuwy-thu
  content: 'Hi. Thanks for your interest! Please check our latest work on integrating
    TCPGen into Whisper: https://arxiv.org/abs/2306.01942. This is an example of applying
    it to a Transformer-based ASR decoder and how it performs without changing any
    original model parameter. So here, TCPGen works as a distribution-level adaptor. '
  created_at: 2023-11-13 11:48:34+00:00
  edited: false
  hidden: false
  id: 65520d1204d4294fa01c0d78
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: espnet/guangzhisun_librispeech100_asr_train_conformer_transducer_tcpgen500_deep_sche30_GCN6L_rep_suffix
repo_type: model
status: open
target_branch: null
title: Inquiry about the implementation of TCPGen in Transformer-based ASR model
