!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Carlosky
conflicting_files: null
created_at: 2023-07-25 07:37:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
      fullname: Carlos Zambrana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Carlosky
      type: user
    createdAt: '2023-07-25T08:37:52.000Z'
    data:
      edited: true
      editors:
      - Carlosky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4596504271030426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
          fullname: Carlos Zambrana
          isHf: false
          isPro: false
          name: Carlosky
          type: user
        html: '<p>Hi, I am very new to llms. I''m having problems initializing LlamaCpp
          embedding using this model. I''ve used the 7B model and the 13B-chat model
          with the same quantization type without problems. I''m using the "llama-2-70b.ggmlv3.q4_0.bin"
          and I''m getting this error:</p>

          <p>Traceback (most recent call last):<br>  File "/home/carlosky/llama/llama.py",
          line 15, in <br>    embeddings = LlamaCppEmbeddings(model_path=''/home/carlosky/llama/models/llama-2-70b.ggmlv3.q4_0.bin'')<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "pydantic/main.py", line 341, in pydantic.main.BaseModel.<strong>init</strong><br>pydantic.error_wrappers.ValidationError:
          1 validation error for LlamaCppEmbeddings<br><strong>root</strong><br>  Could
          not load Llama model from path: /home/carlosky/llama/models/llama-2-70b.ggmlv3.q4_0.bin.
          Received error  (type=value_error)<br>Exception ignored in: &lt;function
          Llama.__del__ at 0x7f142d7c4680&gt;<br>Traceback (most recent call last):<br>  File
          "/home/carlosky/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py",
          line 1510, in <strong>del</strong><br>    if self.ctx is not None:<br>       ^^^^^^^^<br>AttributeError:
          ''Llama'' object has no attribute ''ctx''</p>

          <p>I''m using Python version 3.11 and llama-cpp-python version 0.1.77 with
          langchain.</p>

          '
        raw: "Hi, I am very new to llms. I'm having problems initializing LlamaCpp\
          \ embedding using this model. I've used the 7B model and the 13B-chat model\
          \ with the same quantization type without problems. I'm using the \"llama-2-70b.ggmlv3.q4_0.bin\"\
          \ and I'm getting this error:\n \nTraceback (most recent call last):\n \
          \ File \"/home/carlosky/llama/llama.py\", line 15, in <module>\n    embeddings\
          \ = LlamaCppEmbeddings(model_path='/home/carlosky/llama/models/llama-2-70b.ggmlv3.q4_0.bin')\n\
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n\
          pydantic.error_wrappers.ValidationError: 1 validation error for LlamaCppEmbeddings\n\
          __root__\n  Could not load Llama model from path: /home/carlosky/llama/models/llama-2-70b.ggmlv3.q4_0.bin.\
          \ Received error  (type=value_error)\nException ignored in: <function Llama.__del__\
          \ at 0x7f142d7c4680>\nTraceback (most recent call last):\n  File \"/home/carlosky/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py\"\
          , line 1510, in __del__\n    if self.ctx is not None:\n       ^^^^^^^^\n\
          AttributeError: 'Llama' object has no attribute 'ctx'\n\nI'm using Python\
          \ version 3.11 and llama-cpp-python version 0.1.77 with langchain."
        updatedAt: '2023-07-25T08:53:37.408Z'
      numEdits: 2
      reactions: []
    id: 64bf89e0891751ce9b5befd0
    type: comment
  author: Carlosky
  content: "Hi, I am very new to llms. I'm having problems initializing LlamaCpp embedding\
    \ using this model. I've used the 7B model and the 13B-chat model with the same\
    \ quantization type without problems. I'm using the \"llama-2-70b.ggmlv3.q4_0.bin\"\
    \ and I'm getting this error:\n \nTraceback (most recent call last):\n  File \"\
    /home/carlosky/llama/llama.py\", line 15, in <module>\n    embeddings = LlamaCppEmbeddings(model_path='/home/carlosky/llama/models/llama-2-70b.ggmlv3.q4_0.bin')\n\
    \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n\
    pydantic.error_wrappers.ValidationError: 1 validation error for LlamaCppEmbeddings\n\
    __root__\n  Could not load Llama model from path: /home/carlosky/llama/models/llama-2-70b.ggmlv3.q4_0.bin.\
    \ Received error  (type=value_error)\nException ignored in: <function Llama.__del__\
    \ at 0x7f142d7c4680>\nTraceback (most recent call last):\n  File \"/home/carlosky/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py\"\
    , line 1510, in __del__\n    if self.ctx is not None:\n       ^^^^^^^^\nAttributeError:\
    \ 'Llama' object has no attribute 'ctx'\n\nI'm using Python version 3.11 and llama-cpp-python\
    \ version 0.1.77 with langchain."
  created_at: 2023-07-25 07:37:52+00:00
  edited: true
  hidden: false
  id: 64bf89e0891751ce9b5befd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
      fullname: Carlos Zambrana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Carlosky
      type: user
    createdAt: '2023-07-25T09:03:39.000Z'
    data:
      status: closed
    id: 64bf8feb5cf7ae507b62b751
    type: status-change
  author: Carlosky
  created_at: 2023-07-25 08:03:39+00:00
  id: 64bf8feb5cf7ae507b62b751
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
      fullname: Carlos Zambrana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Carlosky
      type: user
    createdAt: '2023-07-25T09:05:29.000Z'
    data:
      edited: false
      editors:
      - Carlosky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9176223278045654
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
          fullname: Carlos Zambrana
          isHf: false
          isPro: false
          name: Carlosky
          type: user
        html: '<p>I just read in the README that Python libraries are not supported
          yet, sorry for the confusion!</p>

          '
        raw: I just read in the README that Python libraries are not supported yet,
          sorry for the confusion!
        updatedAt: '2023-07-25T09:05:29.881Z'
      numEdits: 0
      reactions: []
    id: 64bf905930a1f0f0f0cd6d22
    type: comment
  author: Carlosky
  content: I just read in the README that Python libraries are not supported yet,
    sorry for the confusion!
  created_at: 2023-07-25 08:05:29+00:00
  edited: false
  hidden: false
  id: 64bf905930a1f0f0f0cd6d22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-25T09:11:12.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9112679362297058
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Actually that''s now slightly out of date - llama-cpp-python updated
          to version 0.1.77 yesterday which should have Llama 70B support.  So that
          should work now I believe, if you update it.  Note that a new parameter
          is required in llama.cpp - <code>-gqa 8</code>; I don''t know how you set
          that with llama-cpp-python but I assume it does need to set, so check their
          docs or their code changes.</p>

          <p>Though I don''t know about LlamaCppEmbeddings, does that use llama-cpp-python
          or is it a different thing?</p>

          <p>I will update my READMEs to mention the llama-cpp-python update later
          today</p>

          '
        raw: 'Actually that''s now slightly out of date - llama-cpp-python updated
          to version 0.1.77 yesterday which should have Llama 70B support.  So that
          should work now I believe, if you update it.  Note that a new parameter
          is required in llama.cpp - `-gqa 8`; I don''t know how you set that with
          llama-cpp-python but I assume it does need to set, so check their docs or
          their code changes.


          Though I don''t know about LlamaCppEmbeddings, does that use llama-cpp-python
          or is it a different thing?


          I will update my READMEs to mention the llama-cpp-python update later today'
        updatedAt: '2023-07-25T09:11:12.120Z'
      numEdits: 0
      reactions: []
    id: 64bf91b0f671da974e76d317
    type: comment
  author: TheBloke
  content: 'Actually that''s now slightly out of date - llama-cpp-python updated to
    version 0.1.77 yesterday which should have Llama 70B support.  So that should
    work now I believe, if you update it.  Note that a new parameter is required in
    llama.cpp - `-gqa 8`; I don''t know how you set that with llama-cpp-python but
    I assume it does need to set, so check their docs or their code changes.


    Though I don''t know about LlamaCppEmbeddings, does that use llama-cpp-python
    or is it a different thing?


    I will update my READMEs to mention the llama-cpp-python update later today'
  created_at: 2023-07-25 08:11:12+00:00
  edited: false
  hidden: false
  id: 64bf91b0f671da974e76d317
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
      fullname: Carlos Zambrana
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Carlosky
      type: user
    createdAt: '2023-07-25T09:39:23.000Z'
    data:
      edited: true
      editors:
      - Carlosky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8295112252235413
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3129c41bbd9fafb8e235c8e49b63e8fc.svg
          fullname: Carlos Zambrana
          isHf: false
          isPro: false
          name: Carlosky
          type: user
        html: '<p>Okay, I will check then the new parameter as you mentioned. LlamaCppEmbedding
          is a Langchain class which integrates llama-cpp-python with the tool.<br>Thank
          you for the quick response and for all the work you do!<br>P.S. Just in
          case anyone wants to know, Langchain has not yet updated the new llama-cpp-python
          parameter.</p>

          '
        raw: 'Okay, I will check then the new parameter as you mentioned. LlamaCppEmbedding
          is a Langchain class which integrates llama-cpp-python with the tool.

          Thank you for the quick response and for all the work you do!

          P.S. Just in case anyone wants to know, Langchain has not yet updated the
          new llama-cpp-python parameter.'
        updatedAt: '2023-07-25T10:02:19.707Z'
      numEdits: 5
      reactions: []
    id: 64bf984b8e051085ba44ca83
    type: comment
  author: Carlosky
  content: 'Okay, I will check then the new parameter as you mentioned. LlamaCppEmbedding
    is a Langchain class which integrates llama-cpp-python with the tool.

    Thank you for the quick response and for all the work you do!

    P.S. Just in case anyone wants to know, Langchain has not yet updated the new
    llama-cpp-python parameter.'
  created_at: 2023-07-25 08:39:23+00:00
  edited: true
  hidden: false
  id: 64bf984b8e051085ba44ca83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2627a074cf6af3fbc5dd2f61e5eb8518.svg
      fullname: Malcolm Yeoman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mal-Y
      type: user
    createdAt: '2023-07-26T02:56:39.000Z'
    data:
      edited: true
      editors:
      - Mal-Y
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6154050827026367
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2627a074cf6af3fbc5dd2f61e5eb8518.svg
          fullname: Malcolm Yeoman
          isHf: false
          isPro: false
          name: Mal-Y
          type: user
        html: '<p>Regarding Langchain and llama-cpp-python - as a temporary fix to
          try out this model, I just added code in 2 places within my langchain/llms/llamacpp.py.
          </p>

          <p>Insert just after the line starting with "n_gpu_layers: Optional" :<br>n_gqa:
          Optional[int] = Field(None, alias="n_gqa")</p>

          <p>Then insert just after the comment "# For backwards compatibility, only
          include if non-null."<br>        if values["n_gqa"] is not None:<br>            model_params["n_gqa"]
          = values["n_gqa"]</p>

          <p>You then add a parameter n_gqa=8 when initialising this 70B model for
          use in langchain e.g:</p>

          <p>llm = LlamaCpp(model_path=''...'', n_gqa=8, n_gpu_layers=20, n_threads=14,
          n_ctx=2048, ...)</p>

          <p>To try out LlamaCppEmbeddings you would need to apply the edits to a
          similar file at langchain/embeddings/llamacpp.py</p>

          '
        raw: "Regarding Langchain and llama-cpp-python - as a temporary fix to try\
          \ out this model, I just added code in 2 places within my langchain/llms/llamacpp.py.\
          \ \n\nInsert just after the line starting with \"n_gpu_layers: Optional\"\
          \ :\nn_gqa: Optional[int] = Field(None, alias=\"n_gqa\")\n\nThen insert\
          \ just after the comment \"# For backwards compatibility, only include if\
          \ non-null.\"\n        if values[\"n_gqa\"] is not None:\n            model_params[\"\
          n_gqa\"] = values[\"n_gqa\"]\n\nYou then add a parameter n_gqa=8 when initialising\
          \ this 70B model for use in langchain e.g:\n\nllm = LlamaCpp(model_path='...',\
          \ n_gqa=8, n_gpu_layers=20, n_threads=14, n_ctx=2048, ...)\n\nTo try out\
          \ LlamaCppEmbeddings you would need to apply the edits to a similar file\
          \ at langchain/embeddings/llamacpp.py\n"
        updatedAt: '2023-07-26T03:18:35.162Z'
      numEdits: 3
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - Carlosky
        - Billbokay
        - yigitbey
        - Footwerk-Fly
        - aljiffy
        - czd358121692
        - datamarinier
        - nahuel89p
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - czd358121692
        - datamarinier
      - count: 1
        reaction: "\U0001F92F"
        users:
        - czd358121692
    id: 64c08b6784191336fa024551
    type: comment
  author: Mal-Y
  content: "Regarding Langchain and llama-cpp-python - as a temporary fix to try out\
    \ this model, I just added code in 2 places within my langchain/llms/llamacpp.py.\
    \ \n\nInsert just after the line starting with \"n_gpu_layers: Optional\" :\n\
    n_gqa: Optional[int] = Field(None, alias=\"n_gqa\")\n\nThen insert just after\
    \ the comment \"# For backwards compatibility, only include if non-null.\"\n \
    \       if values[\"n_gqa\"] is not None:\n            model_params[\"n_gqa\"\
    ] = values[\"n_gqa\"]\n\nYou then add a parameter n_gqa=8 when initialising this\
    \ 70B model for use in langchain e.g:\n\nllm = LlamaCpp(model_path='...', n_gqa=8,\
    \ n_gpu_layers=20, n_threads=14, n_ctx=2048, ...)\n\nTo try out LlamaCppEmbeddings\
    \ you would need to apply the edits to a similar file at langchain/embeddings/llamacpp.py\n"
  created_at: 2023-07-26 01:56:39+00:00
  edited: true
  hidden: false
  id: 64c08b6784191336fa024551
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0006334ddac07b437d4d7f267b571fb9.svg
      fullname: AddinCui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: czd358121692
      type: user
    createdAt: '2023-08-04T23:19:35.000Z'
    data:
      edited: false
      editors:
      - czd358121692
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6757078170776367
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0006334ddac07b437d4d7f267b571fb9.svg
          fullname: AddinCui
          isHf: false
          isPro: false
          name: czd358121692
          type: user
        html: '<blockquote>

          <p>Regarding Langchain and llama-cpp-python - as a temporary fix to try
          out this model, I just added code in 2 places within my langchain/llms/llamacpp.py.
          </p>

          <p>Insert just after the line starting with "n_gpu_layers: Optional" :<br>n_gqa:
          Optional[int] = Field(None, alias="n_gqa")</p>

          <p>Then insert just after the comment "# For backwards compatibility, only
          include if non-null."<br>        if values["n_gqa"] is not None:<br>            model_params["n_gqa"]
          = values["n_gqa"]</p>

          <p>You then add a parameter n_gqa=8 when initialising this 70B model for
          use in langchain e.g:</p>

          <p>llm = LlamaCpp(model_path=''...'', n_gqa=8, n_gpu_layers=20, n_threads=14,
          n_ctx=2048, ...)</p>

          <p>To try out LlamaCppEmbeddings you would need to apply the edits to a
          similar file at langchain/embeddings/llamacpp.py</p>

          </blockquote>

          <p>hope langchain get this update soon</p>

          '
        raw: "> Regarding Langchain and llama-cpp-python - as a temporary fix to try\
          \ out this model, I just added code in 2 places within my langchain/llms/llamacpp.py.\
          \ \n> \n> Insert just after the line starting with \"n_gpu_layers: Optional\"\
          \ :\n> n_gqa: Optional[int] = Field(None, alias=\"n_gqa\")\n> \n> Then insert\
          \ just after the comment \"# For backwards compatibility, only include if\
          \ non-null.\"\n>         if values[\"n_gqa\"] is not None:\n>          \
          \   model_params[\"n_gqa\"] = values[\"n_gqa\"]\n> \n> You then add a parameter\
          \ n_gqa=8 when initialising this 70B model for use in langchain e.g:\n>\
          \ \n> llm = LlamaCpp(model_path='...', n_gqa=8, n_gpu_layers=20, n_threads=14,\
          \ n_ctx=2048, ...)\n> \n> To try out LlamaCppEmbeddings you would need to\
          \ apply the edits to a similar file at langchain/embeddings/llamacpp.py\n\
          \nhope langchain get this update soon"
        updatedAt: '2023-08-04T23:19:35.542Z'
      numEdits: 0
      reactions: []
    id: 64cd8787995a0b5d591ec975
    type: comment
  author: czd358121692
  content: "> Regarding Langchain and llama-cpp-python - as a temporary fix to try\
    \ out this model, I just added code in 2 places within my langchain/llms/llamacpp.py.\
    \ \n> \n> Insert just after the line starting with \"n_gpu_layers: Optional\"\
    \ :\n> n_gqa: Optional[int] = Field(None, alias=\"n_gqa\")\n> \n> Then insert\
    \ just after the comment \"# For backwards compatibility, only include if non-null.\"\
    \n>         if values[\"n_gqa\"] is not None:\n>             model_params[\"n_gqa\"\
    ] = values[\"n_gqa\"]\n> \n> You then add a parameter n_gqa=8 when initialising\
    \ this 70B model for use in langchain e.g:\n> \n> llm = LlamaCpp(model_path='...',\
    \ n_gqa=8, n_gpu_layers=20, n_threads=14, n_ctx=2048, ...)\n> \n> To try out LlamaCppEmbeddings\
    \ you would need to apply the edits to a similar file at langchain/embeddings/llamacpp.py\n\
    \nhope langchain get this update soon"
  created_at: 2023-08-04 22:19:35+00:00
  edited: false
  hidden: false
  id: 64cd8787995a0b5d591ec975
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Llama-2-70B-Chat-GGML
repo_type: model
status: closed
target_branch: null
title: Problem initializing LlamaCpp embeddings
