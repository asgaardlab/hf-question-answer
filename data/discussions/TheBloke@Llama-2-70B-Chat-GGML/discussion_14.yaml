!!python/object:huggingface_hub.community.DiscussionWithDetails
author: barha
conflicting_files: null
created_at: 2023-09-03 11:28:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9342ff868ec9099def4cc760967f87c9.svg
      fullname: bar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: barha
      type: user
    createdAt: '2023-09-03T12:28:03.000Z'
    data:
      edited: false
      editors:
      - barha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3707732856273651
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9342ff868ec9099def4cc760967f87c9.svg
          fullname: bar
          isHf: false
          isPro: false
          name: barha
          type: user
        html: '<p>I am able to convert GGML to GGUF but the model won''t load</p>

          <p>llm_load_print_meta: format         = GGUF V2 (latest)<br>llm_load_print_meta:
          arch           = llama<br>llm_load_print_meta: vocab type     = SPM<br>llm_load_print_meta:
          n_vocab        = 32000<br>llm_load_print_meta: n_merges       = 0<br>llm_load_print_meta:
          n_ctx_train    = 2048<br>llm_load_print_meta: n_ctx          = 3072<br>llm_load_print_meta:
          n_embd         = 8192<br>llm_load_print_meta: n_head         = 64<br>llm_load_print_meta:
          n_head_kv      = 64<br>llm_load_print_meta: n_layer        = 80<br>llm_load_print_meta:
          n_rot          = 128<br>llm_load_print_meta: n_gqa          = 1<br>llm_load_print_meta:
          f_norm_eps     = 1.0e-05<br>llm_load_print_meta: f_norm_rms_eps = 5.0e-06<br>llm_load_print_meta:
          n_ff           = 28672<br>llm_load_print_meta: freq_base      = 10000.0<br>llm_load_print_meta:
          freq_scale     = 1<br>llm_load_print_meta: model type     = 65B<br>llm_load_print_meta:
          model ftype    = mostly Q4_K - Medium (guessed)<br>llm_load_print_meta:
          model size     = 68.98 B<br>llm_load_print_meta: general.name   = llama-2-70b-chat.ggmlv3.q4_K_M.bin<br>llm_load_print_meta:
          BOS token = 1 ''<s>''<br>llm_load_print_meta: EOS token = 2 ''</s>''<br>llm_load_print_meta:
          UNK token = 0 ''''<br>llm_load_print_meta: LF token  = 13 ''&lt;0x0A&gt;''<br>llm_load_tensors:
          ggml ctx size =    0.23 MB<br>llm_load_tensors: using CUDA for GPU acceleration<br>ggml_cuda_set_main_device:
          using device 0 (Tesla P100-PCIE-16GB) as main device<br>error loading model:
          create_tensor: tensor ''blk.0.attn_k.weight'' has wrong shape; expected  8192,  8192,
          got  8192,  1024,     1,     1<br>llama_load_model_from_file: failed to
          load model</p>

          '
        raw: "I am able to convert GGML to GGUF but the model won't load\r\n\r\nllm_load_print_meta:\
          \ format         = GGUF V2 (latest)\r\nllm_load_print_meta: arch       \
          \    = llama\r\nllm_load_print_meta: vocab type     = SPM\r\nllm_load_print_meta:\
          \ n_vocab        = 32000\r\nllm_load_print_meta: n_merges       = 0\r\n\
          llm_load_print_meta: n_ctx_train    = 2048\r\nllm_load_print_meta: n_ctx\
          \          = 3072\r\nllm_load_print_meta: n_embd         = 8192\r\nllm_load_print_meta:\
          \ n_head         = 64\r\nllm_load_print_meta: n_head_kv      = 64\r\nllm_load_print_meta:\
          \ n_layer        = 80\r\nllm_load_print_meta: n_rot          = 128\r\nllm_load_print_meta:\
          \ n_gqa          = 1\r\nllm_load_print_meta: f_norm_eps     = 1.0e-05\r\n\
          llm_load_print_meta: f_norm_rms_eps = 5.0e-06\r\nllm_load_print_meta: n_ff\
          \           = 28672\r\nllm_load_print_meta: freq_base      = 10000.0\r\n\
          llm_load_print_meta: freq_scale     = 1\r\nllm_load_print_meta: model type\
          \     = 65B\r\nllm_load_print_meta: model ftype    = mostly Q4_K - Medium\
          \ (guessed)\r\nllm_load_print_meta: model size     = 68.98 B\r\nllm_load_print_meta:\
          \ general.name   = llama-2-70b-chat.ggmlv3.q4_K_M.bin\r\nllm_load_print_meta:\
          \ BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta:\
          \ UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\
          \nllm_load_tensors: ggml ctx size =    0.23 MB\r\nllm_load_tensors: using\
          \ CUDA for GPU acceleration\r\nggml_cuda_set_main_device: using device 0\
          \ (Tesla P100-PCIE-16GB) as main device\r\nerror loading model: create_tensor:\
          \ tensor 'blk.0.attn_k.weight' has wrong shape; expected  8192,  8192, got\
          \  8192,  1024,     1,     1\r\nllama_load_model_from_file: failed to load\
          \ model"
        updatedAt: '2023-09-03T12:28:03.985Z'
      numEdits: 0
      reactions: []
    id: 64f47bd3fe151940257bea10
    type: comment
  author: barha
  content: "I am able to convert GGML to GGUF but the model won't load\r\n\r\nllm_load_print_meta:\
    \ format         = GGUF V2 (latest)\r\nllm_load_print_meta: arch           = llama\r\
    \nllm_load_print_meta: vocab type     = SPM\r\nllm_load_print_meta: n_vocab  \
    \      = 32000\r\nllm_load_print_meta: n_merges       = 0\r\nllm_load_print_meta:\
    \ n_ctx_train    = 2048\r\nllm_load_print_meta: n_ctx          = 3072\r\nllm_load_print_meta:\
    \ n_embd         = 8192\r\nllm_load_print_meta: n_head         = 64\r\nllm_load_print_meta:\
    \ n_head_kv      = 64\r\nllm_load_print_meta: n_layer        = 80\r\nllm_load_print_meta:\
    \ n_rot          = 128\r\nllm_load_print_meta: n_gqa          = 1\r\nllm_load_print_meta:\
    \ f_norm_eps     = 1.0e-05\r\nllm_load_print_meta: f_norm_rms_eps = 5.0e-06\r\n\
    llm_load_print_meta: n_ff           = 28672\r\nllm_load_print_meta: freq_base\
    \      = 10000.0\r\nllm_load_print_meta: freq_scale     = 1\r\nllm_load_print_meta:\
    \ model type     = 65B\r\nllm_load_print_meta: model ftype    = mostly Q4_K -\
    \ Medium (guessed)\r\nllm_load_print_meta: model size     = 68.98 B\r\nllm_load_print_meta:\
    \ general.name   = llama-2-70b-chat.ggmlv3.q4_K_M.bin\r\nllm_load_print_meta:\
    \ BOS token = 1 '<s>'\r\nllm_load_print_meta: EOS token = 2 '</s>'\r\nllm_load_print_meta:\
    \ UNK token = 0 '<unk>'\r\nllm_load_print_meta: LF token  = 13 '<0x0A>'\r\nllm_load_tensors:\
    \ ggml ctx size =    0.23 MB\r\nllm_load_tensors: using CUDA for GPU acceleration\r\
    \nggml_cuda_set_main_device: using device 0 (Tesla P100-PCIE-16GB) as main device\r\
    \nerror loading model: create_tensor: tensor 'blk.0.attn_k.weight' has wrong shape;\
    \ expected  8192,  8192, got  8192,  1024,     1,     1\r\nllama_load_model_from_file:\
    \ failed to load model"
  created_at: 2023-09-03 11:28:03+00:00
  edited: false
  hidden: false
  id: 64f47bd3fe151940257bea10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
      fullname: Akarshan Biswas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akarshanbiswas
      type: user
    createdAt: '2023-09-03T12:46:25.000Z'
    data:
      edited: true
      editors:
      - akarshanbiswas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9726635217666626
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1662098610747-noauth.jpeg?w=200&h=200&f=face
          fullname: Akarshan Biswas
          isHf: false
          isPro: false
          name: akarshanbiswas
          type: user
        html: '<p>Grouped Query Attention should have been 8, since you converted
          from ggml, it took as 1 which is the default. You should reconvert by correctly
          passing the -gqa 8 parameter.</p>

          '
        raw: Grouped Query Attention should have been 8, since you converted from
          ggml, it took as 1 which is the default. You should reconvert by correctly
          passing the -gqa 8 parameter.
        updatedAt: '2023-09-03T12:46:46.833Z'
      numEdits: 1
      reactions: []
    id: 64f480211ab02a71e19f379c
    type: comment
  author: akarshanbiswas
  content: Grouped Query Attention should have been 8, since you converted from ggml,
    it took as 1 which is the default. You should reconvert by correctly passing the
    -gqa 8 parameter.
  created_at: 2023-09-03 11:46:25+00:00
  edited: true
  hidden: false
  id: 64f480211ab02a71e19f379c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/Llama-2-70B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Looking for GGUF format for this model
