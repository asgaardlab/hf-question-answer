!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NitanKasat
conflicting_files: null
created_at: 2023-08-29 21:36:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e0da267fb10371af329ebd9fd83674f3.svg
      fullname: Nitan Kasat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NitanKasat
      type: user
    createdAt: '2023-08-29T22:36:24.000Z'
    data:
      edited: true
      editors:
      - NitanKasat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6256992816925049
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e0da267fb10371af329ebd9fd83674f3.svg
          fullname: Nitan Kasat
          isHf: false
          isPro: false
          name: NitanKasat
          type: user
        html: '<p>I am receiving an error while trying to load TheBloke/Llama-2-70B-Chat-GGML.
          Can anyone provide me with error clarification or assistance?<br>Here is
          the link to the GitHub repository that can help you gain a better understanding
          of errors.<br>The code I have is<br>llm = LlamaCpp(<br>    model_path=model_path,<br>    max_tokens=256,<br>    n_gpu_layers=n_gpu_layers,<br>    n_batch=n_batch,<br>    callback_manager=callback_manager,<br>    n_ctx=1024,<br>    verbose=False,<br>)</p>

          <ul>

          <li>and the error I am getting is<br>AssertionError                            Traceback
          (most recent call last)<br><a rel="nofollow" href="https://localhost:8080/#"></a>
          in &lt;cell line: 1&gt;()<br>----&gt; 1 lcpp_llm = Llama(<br>2     model_path=model_path,<br>3     n_threads=2,
          # CPU cores<br>4     n_batch=512, # Consider amount of VRAM on system<br>5     n_gpu_layers=32
          # Dependent on model and GPU RAM</li>

          </ul>

          <p><a rel="nofollow" href="https://localhost:8080/#">/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py</a>
          in <strong>init</strong>(self, model_path, n_ctx, n_parts, n_gpu_layers,
          seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads,
          n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split,
          rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)<br>    321                     self.model_path.encode("utf-8"),
          self.params<br>    322                 )<br>--&gt; 323         assert self.model
          is not None<br>    324<br>    325         if verbose:</p>

          <p>AssertionError:<br>```</p>

          '
        raw: "I am receiving an error while trying to load TheBloke/Llama-2-70B-Chat-GGML.\
          \ Can anyone provide me with error clarification or assistance?\nHere is\
          \ the link to the GitHub repository that can help you gain a better understanding\
          \ of errors.\nThe code I have is \nllm = LlamaCpp(\n    model_path=model_path,\n\
          \    max_tokens=256,\n    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n\
          \    callback_manager=callback_manager,\n    n_ctx=1024,\n    verbose=False,\n\
          )\n* and the error I am getting is \nAssertionError                    \
          \        Traceback (most recent call last)\n[<ipython-input-11-b81fe2153712>](https://localhost:8080/#)\
          \ in <cell line: 1>()\n----> 1 lcpp_llm = Llama(\n      2     model_path=model_path,\n\
          \      3     n_threads=2, # CPU cores\n      4     n_batch=512, # Consider\
          \ amount of VRAM on system\n      5     n_gpu_layers=32 # Dependent on model\
          \ and GPU RAM\n\n[/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py](https://localhost:8080/#)\
          \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv,\
          \ logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch,\
          \ last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base,\
          \ rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)\n    321   \
          \                  self.model_path.encode(\"utf-8\"), self.params\n    322\
          \                 )\n--> 323         assert self.model is not None\n   \
          \ 324 \n    325         if verbose:\n\nAssertionError:\n```"
        updatedAt: '2023-08-29T22:36:51.325Z'
      numEdits: 1
      reactions: []
    id: 64ee72e8edc1c8e6d3fcd220
    type: comment
  author: NitanKasat
  content: "I am receiving an error while trying to load TheBloke/Llama-2-70B-Chat-GGML.\
    \ Can anyone provide me with error clarification or assistance?\nHere is the link\
    \ to the GitHub repository that can help you gain a better understanding of errors.\n\
    The code I have is \nllm = LlamaCpp(\n    model_path=model_path,\n    max_tokens=256,\n\
    \    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    callback_manager=callback_manager,\n\
    \    n_ctx=1024,\n    verbose=False,\n)\n* and the error I am getting is \nAssertionError\
    \                            Traceback (most recent call last)\n[<ipython-input-11-b81fe2153712>](https://localhost:8080/#)\
    \ in <cell line: 1>()\n----> 1 lcpp_llm = Llama(\n      2     model_path=model_path,\n\
    \      3     n_threads=2, # CPU cores\n      4     n_batch=512, # Consider amount\
    \ of VRAM on system\n      5     n_gpu_layers=32 # Dependent on model and GPU\
    \ RAM\n\n[/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py](https://localhost:8080/#)\
    \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all,\
    \ vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size,\
    \ lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale,\
    \ n_gqa, rms_norm_eps, mul_mat_q, verbose)\n    321                     self.model_path.encode(\"\
    utf-8\"), self.params\n    322                 )\n--> 323         assert self.model\
    \ is not None\n    324 \n    325         if verbose:\n\nAssertionError:\n```"
  created_at: 2023-08-29 21:36:24+00:00
  edited: true
  hidden: false
  id: 64ee72e8edc1c8e6d3fcd220
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
      fullname: Richard Scott
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RichardScottOZ
      type: user
    createdAt: '2023-09-09T02:49:39.000Z'
    data:
      edited: false
      editors:
      - RichardScottOZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8930320739746094
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ab8336cfbe073b42c4b3bcfc81fc20c.svg
          fullname: Richard Scott
          isHf: false
          isPro: false
          name: RichardScottOZ
          type: user
        html: '<p>you using newest llama cpp?  won''t work with GGML?</p>

          '
        raw: you using newest llama cpp?  won't work with GGML?
        updatedAt: '2023-09-09T02:49:39.554Z'
      numEdits: 0
      reactions: []
    id: 64fbdd434010eccccc26fabb
    type: comment
  author: RichardScottOZ
  content: you using newest llama cpp?  won't work with GGML?
  created_at: 2023-09-09 01:49:39+00:00
  edited: false
  hidden: false
  id: 64fbdd434010eccccc26fabb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/Llama-2-70B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: 'Unable to Load model TheBloke/Llama-2-70B-Chat-GGML . Name : llama-2-70b-chat.ggmlv3.q5_0.bin'
