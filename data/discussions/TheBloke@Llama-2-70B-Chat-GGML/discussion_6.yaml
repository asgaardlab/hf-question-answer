!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Laurab
conflicting_files: null
created_at: 2023-07-26 21:17:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7ba0fa6d1fd11b2bc101ad42681f609.svg
      fullname: LB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Laurab
      type: user
    createdAt: '2023-07-26T22:17:29.000Z'
    data:
      edited: false
      editors:
      - Laurab
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46803611516952515
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7ba0fa6d1fd11b2bc101ad42681f609.svg
          fullname: LB
          isHf: false
          isPro: false
          name: Laurab
          type: user
        html: '<p>I''m using a recent build of llama.cpp, I downloaded <code>llama-2-70b-chat.ggmlv3.q4_1.bin</code>.
          When I try to load the model like so:</p>

          <p><code>./main -m models/llama-2-70b/llama-2-70b-chat.ggmlv3.q4_1.bin -n
          2048 --color -i -r "User:" -f prompts/coder.txt</code></p>

          <p>I get this error</p>

          <pre><code>error loading model: llama.cpp: tensor ''layers.0.attention.wk.weight''
          has wrong shape; expected  8192 x  8192, got  8192 x  1024

          llama_load_model_from_file: failed to load model

          llama_init_from_gpt_params: error: failed to load model ''models/llama-2-70b/llama-2-70b-chat.ggmlv3.q4_1.bin''

          main: error: unable to load model

          </code></pre>

          <p>Did I do something wrong?</p>

          '
        raw: "I'm using a recent build of llama.cpp, I downloaded `llama-2-70b-chat.ggmlv3.q4_1.bin`.\
          \ When I try to load the model like so:\r\n\r\n`./main -m models/llama-2-70b/llama-2-70b-chat.ggmlv3.q4_1.bin\
          \ -n 2048 --color -i -r \"User:\" -f prompts/coder.txt`\r\n\r\nI get this\
          \ error\r\n\r\n```\r\nerror loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
          \ has wrong shape; expected  8192 x  8192, got  8192 x  1024\r\nllama_load_model_from_file:\
          \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load\
          \ model 'models/llama-2-70b/llama-2-70b-chat.ggmlv3.q4_1.bin'\r\nmain: error:\
          \ unable to load model\r\n```\r\n\r\nDid I do something wrong?"
        updatedAt: '2023-07-26T22:17:29.295Z'
      numEdits: 0
      reactions: []
    id: 64c19b796489a1a4cf86e50e
    type: comment
  author: Laurab
  content: "I'm using a recent build of llama.cpp, I downloaded `llama-2-70b-chat.ggmlv3.q4_1.bin`.\
    \ When I try to load the model like so:\r\n\r\n`./main -m models/llama-2-70b/llama-2-70b-chat.ggmlv3.q4_1.bin\
    \ -n 2048 --color -i -r \"User:\" -f prompts/coder.txt`\r\n\r\nI get this error\r\
    \n\r\n```\r\nerror loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
    \ has wrong shape; expected  8192 x  8192, got  8192 x  1024\r\nllama_load_model_from_file:\
    \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load model\
    \ 'models/llama-2-70b/llama-2-70b-chat.ggmlv3.q4_1.bin'\r\nmain: error: unable\
    \ to load model\r\n```\r\n\r\nDid I do something wrong?"
  created_at: 2023-07-26 21:17:29+00:00
  edited: false
  hidden: false
  id: 64c19b796489a1a4cf86e50e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-26T22:18:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: uk
        probability: 0.11070254445075989
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to add <code>-gqa 8</code> parameter</p>

          '
        raw: You need to add `-gqa 8` parameter
        updatedAt: '2023-07-26T22:18:14.423Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - Laurab
        - fluxthedev
        - dewijones92
        - tkosht
        - rreed-pha
        - rekin
        - Xiaozhou0
        - Raspbfox
        - RyoF369
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - dewijones92
        - rekin
        - RyoF369
      - count: 2
        reaction: "\U0001F92F"
        users:
        - rekin
        - rankern
    id: 64c19ba6129617dbab967378
    type: comment
  author: TheBloke
  content: You need to add `-gqa 8` parameter
  created_at: 2023-07-26 21:18:14+00:00
  edited: false
  hidden: false
  id: 64c19ba6129617dbab967378
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7ba0fa6d1fd11b2bc101ad42681f609.svg
      fullname: LB
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Laurab
      type: user
    createdAt: '2023-07-26T22:19:07.000Z'
    data:
      edited: false
      editors:
      - Laurab
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9850247502326965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7ba0fa6d1fd11b2bc101ad42681f609.svg
          fullname: LB
          isHf: false
          isPro: false
          name: Laurab
          type: user
        html: '<p>Wow you were faster than my realization that I failed to RTFM.</p>

          <p>But yes, that is what''s missing. I recall a conversation on the llama.cpp
          github that the qga param is temporary and will be added to the ggml file
          itself at a later time.</p>

          '
        raw: 'Wow you were faster than my realization that I failed to RTFM.


          But yes, that is what''s missing. I recall a conversation on the llama.cpp
          github that the qga param is temporary and will be added to the ggml file
          itself at a later time.'
        updatedAt: '2023-07-26T22:19:07.340Z'
      numEdits: 0
      reactions: []
    id: 64c19bdbd1ca220e30319795
    type: comment
  author: Laurab
  content: 'Wow you were faster than my realization that I failed to RTFM.


    But yes, that is what''s missing. I recall a conversation on the llama.cpp github
    that the qga param is temporary and will be added to the ggml file itself at a
    later time.'
  created_at: 2023-07-26 21:19:07+00:00
  edited: false
  hidden: false
  id: 64c19bdbd1ca220e30319795
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/539e9200fe27a64fd783b1132641bd44.svg
      fullname: John D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jd4ever
      type: user
    createdAt: '2023-07-27T22:13:47.000Z'
    data:
      edited: false
      editors:
      - jd4ever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8959642052650452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/539e9200fe27a64fd783b1132641bd44.svg
          fullname: John D
          isHf: false
          isPro: false
          name: jd4ever
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Thanks so much\
          \ for uploading the 70b GGML files.<br>I am still having trouble running\
          \ it on my M1 mac with Metal, and it is giving me the GGML assertion error\
          \ <code>GGML_ASSERT: ggml-metal.m:721: ne02 == ne12</code>.</p>\n<p>I opened\
          \ an issue on the llama.cpp project page, but I want to check with you in\
          \ case I did something silly and is a user error such as omitting some command\
          \ line flag: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/2429\"\
          >https://github.com/ggerganov/llama.cpp/issues/2429</a></p>\n"
        raw: '@TheBloke Thanks so much for uploading the 70b GGML files.

          I am still having trouble running it on my M1 mac with Metal, and it is
          giving me the GGML assertion error `GGML_ASSERT: ggml-metal.m:721: ne02
          == ne12`.


          I opened an issue on the llama.cpp project page, but I want to check with
          you in case I did something silly and is a user error such as omitting some
          command line flag: https://github.com/ggerganov/llama.cpp/issues/2429


          '
        updatedAt: '2023-07-27T22:13:47.161Z'
      numEdits: 0
      reactions: []
    id: 64c2ec1b1ddb38927346c7be
    type: comment
  author: jd4ever
  content: '@TheBloke Thanks so much for uploading the 70b GGML files.

    I am still having trouble running it on my M1 mac with Metal, and it is giving
    me the GGML assertion error `GGML_ASSERT: ggml-metal.m:721: ne02 == ne12`.


    I opened an issue on the llama.cpp project page, but I want to check with you
    in case I did something silly and is a user error such as omitting some command
    line flag: https://github.com/ggerganov/llama.cpp/issues/2429


    '
  created_at: 2023-07-27 21:13:47+00:00
  edited: false
  hidden: false
  id: 64c2ec1b1ddb38927346c7be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T22:27:59.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824815988540649
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It looks like you''re trying to use Metal acceleration which isn''t
          supported at the moment. It''s CPU only for now. I''m sure GPU acceleration
          will come soon though.</p>

          <p>Also remember to use the <code>-gqa 8</code> flag, if you weren''t already.</p>

          '
        raw: 'It looks like you''re trying to use Metal acceleration which isn''t
          supported at the moment. It''s CPU only for now. I''m sure GPU acceleration
          will come soon though.


          Also remember to use the `-gqa 8` flag, if you weren''t already.'
        updatedAt: '2023-07-27T22:27:59.336Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jd4ever
    id: 64c2ef6f0e75a24a26c25ed6
    type: comment
  author: TheBloke
  content: 'It looks like you''re trying to use Metal acceleration which isn''t supported
    at the moment. It''s CPU only for now. I''m sure GPU acceleration will come soon
    though.


    Also remember to use the `-gqa 8` flag, if you weren''t already.'
  created_at: 2023-07-27 21:27:59+00:00
  edited: false
  hidden: false
  id: 64c2ef6f0e75a24a26c25ed6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd6eb94e9bef729022a944770c00de4f.svg
      fullname: Javier Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: JavierRCam
      type: user
    createdAt: '2023-07-27T22:51:40.000Z'
    data:
      edited: true
      editors:
      - JavierRCam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7177326083183289
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd6eb94e9bef729022a944770c00de4f.svg
          fullname: Javier Rojas
          isHf: false
          isPro: true
          name: JavierRCam
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , I have the same problem when trying to load it from Python like this:<br>llm\
          \ = CTransformers(model='/llama-2-70b-chat.ggmlv3.q8_0.bin', # Location\
          \ of downloaded GGML model<br>                    model_type='llama',<br>\
          \                    config={'max_new_tokens': 256,<br>                \
          \            'temperature': 0.01})</p>\n<p>Can you provide me with a guide\
          \ so that it runs successfully please?</p>\n"
        raw: "Hello @TheBloke , I have the same problem when trying to load it from\
          \ Python like this:\nllm = CTransformers(model='/llama-2-70b-chat.ggmlv3.q8_0.bin',\
          \ # Location of downloaded GGML model\n                    model_type='llama',\n\
          \                    config={'max_new_tokens': 256,\n                  \
          \          'temperature': 0.01})\n\nCan you provide me with a guide so that\
          \ it runs successfully please?"
        updatedAt: '2023-07-27T22:52:04.995Z'
      numEdits: 1
      reactions: []
    id: 64c2f4fcd2027dcbea28fe0c
    type: comment
  author: JavierRCam
  content: "Hello @TheBloke , I have the same problem when trying to load it from\
    \ Python like this:\nllm = CTransformers(model='/llama-2-70b-chat.ggmlv3.q8_0.bin',\
    \ # Location of downloaded GGML model\n                    model_type='llama',\n\
    \                    config={'max_new_tokens': 256,\n                        \
    \    'temperature': 0.01})\n\nCan you provide me with a guide so that it runs\
    \ successfully please?"
  created_at: 2023-07-27 21:51:40+00:00
  edited: true
  hidden: false
  id: 64c2f4fcd2027dcbea28fe0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-27T22:53:45.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9703606367111206
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>ctransformers has not been updated for Llama 70B yet.  llama-cpp-python
          has been though, since version 0.1.77, so you could try that</p>

          '
        raw: ctransformers has not been updated for Llama 70B yet.  llama-cpp-python
          has been though, since version 0.1.77, so you could try that
        updatedAt: '2023-07-27T22:53:45.597Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - JavierRCam
    id: 64c2f579f33d2bbd26dee38b
    type: comment
  author: TheBloke
  content: ctransformers has not been updated for Llama 70B yet.  llama-cpp-python
    has been though, since version 0.1.77, so you could try that
  created_at: 2023-07-27 21:53:45+00:00
  edited: false
  hidden: false
  id: 64c2f579f33d2bbd26dee38b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2398719e9a584448cf38664e4ae25651.svg
      fullname: John Ore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fluxthedev
      type: user
    createdAt: '2023-07-27T23:44:30.000Z'
    data:
      edited: false
      editors:
      - fluxthedev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.805918276309967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2398719e9a584448cf38664e4ae25651.svg
          fullname: John Ore
          isHf: false
          isPro: false
          name: fluxthedev
          type: user
        html: "<blockquote>\n<p>You need to add <code>-gqa 8</code> parameter</p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ Thank you! I didn't realize that parameter was for the interactive mode.\
          \ Saved me from going down a deep rabbit hole.</p>\n"
        raw: '> You need to add `-gqa 8` parameter


          @TheBloke Thank you! I didn''t realize that parameter was for the interactive
          mode. Saved me from going down a deep rabbit hole.'
        updatedAt: '2023-07-27T23:44:30.326Z'
      numEdits: 0
      reactions: []
    id: 64c3015e75e077907824d362
    type: comment
  author: fluxthedev
  content: '> You need to add `-gqa 8` parameter


    @TheBloke Thank you! I didn''t realize that parameter was for the interactive
    mode. Saved me from going down a deep rabbit hole.'
  created_at: 2023-07-27 22:44:30+00:00
  edited: false
  hidden: false
  id: 64c3015e75e077907824d362
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/539e9200fe27a64fd783b1132641bd44.svg
      fullname: John D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jd4ever
      type: user
    createdAt: '2023-08-06T00:10:30.000Z'
    data:
      edited: false
      editors:
      - jd4ever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8812616467475891
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/539e9200fe27a64fd783b1132641bd44.svg
          fullname: John D
          isHf: false
          isPro: false
          name: jd4ever
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ Thanks so much for uploading the 70b GGML files.<br>I am still having\
          \ trouble running it on my M1 mac with Metal, and it is giving me the GGML\
          \ assertion error <code>GGML_ASSERT: ggml-metal.m:721: ne02 == ne12</code>.</p>\n\
          <p>I opened an issue on the llama.cpp project page, but I want to check\
          \ with you in case I did something silly and is a user error such as omitting\
          \ some command line flag: <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/issues/2429\"\
          >https://github.com/ggerganov/llama.cpp/issues/2429</a></p>\n</blockquote>\n\
          <p>Hi all, just wanted to update this thread to say that the 70b GGML files\
          \ work with the newest llama.cpp now</p>\n"
        raw: "> @TheBloke Thanks so much for uploading the 70b GGML files.\n> I am\
          \ still having trouble running it on my M1 mac with Metal, and it is giving\
          \ me the GGML assertion error `GGML_ASSERT: ggml-metal.m:721: ne02 == ne12`.\n\
          > \n> I opened an issue on the llama.cpp project page, but I want to check\
          \ with you in case I did something silly and is a user error such as omitting\
          \ some command line flag: https://github.com/ggerganov/llama.cpp/issues/2429\n\
          \nHi all, just wanted to update this thread to say that the 70b GGML files\
          \ work with the newest llama.cpp now"
        updatedAt: '2023-08-06T00:10:30.162Z'
      numEdits: 0
      reactions: []
    id: 64cee4f6e8df1f66dd603d8e
    type: comment
  author: jd4ever
  content: "> @TheBloke Thanks so much for uploading the 70b GGML files.\n> I am still\
    \ having trouble running it on my M1 mac with Metal, and it is giving me the GGML\
    \ assertion error `GGML_ASSERT: ggml-metal.m:721: ne02 == ne12`.\n> \n> I opened\
    \ an issue on the llama.cpp project page, but I want to check with you in case\
    \ I did something silly and is a user error such as omitting some command line\
    \ flag: https://github.com/ggerganov/llama.cpp/issues/2429\n\nHi all, just wanted\
    \ to update this thread to say that the 70b GGML files work with the newest llama.cpp\
    \ now"
  created_at: 2023-08-05 23:10:30+00:00
  edited: false
  hidden: false
  id: 64cee4f6e8df1f66dd603d8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-08-06T18:31:24.000Z'
    data:
      edited: false
      editors:
      - robert1968
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7245094180107117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: '<p>Hi,<br>Can someone guide how to add -gqa 8 parameter to oobabooga_linux
          ?<br>I used to start  text-generation-webui with /home/oobabooga_linux/start_linux.sh.
          but last line "python webui.py" does not accept this parameter.<br>I also
          tried to add -gqa 8 to CMD_FLAGS.txt but also complaining.</p>

          '
        raw: "Hi,\nCan someone guide how to add -gqa 8 parameter to oobabooga_linux\
          \ ? \nI used to start  text-generation-webui with /home/oobabooga_linux/start_linux.sh.\
          \ but last line \"python webui.py\" does not accept this parameter.\nI also\
          \ tried to add -gqa 8 to CMD_FLAGS.txt but also complaining.\n"
        updatedAt: '2023-08-06T18:31:24.294Z'
      numEdits: 0
      reactions: []
    id: 64cfe6fc1720c7a483f34b0c
    type: comment
  author: robert1968
  content: "Hi,\nCan someone guide how to add -gqa 8 parameter to oobabooga_linux\
    \ ? \nI used to start  text-generation-webui with /home/oobabooga_linux/start_linux.sh.\
    \ but last line \"python webui.py\" does not accept this parameter.\nI also tried\
    \ to add -gqa 8 to CMD_FLAGS.txt but also complaining.\n"
  created_at: 2023-08-06 17:31:24+00:00
  edited: false
  hidden: false
  id: 64cfe6fc1720c7a483f34b0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d066ce749587dbe035b3aa/XJ7wN98DMvRMIrF5ctOvh.png?w=200&h=200&f=face
      fullname: Hoang Cuong Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HoangCuongNguyen
      type: user
    createdAt: '2023-08-11T01:42:06.000Z'
    data:
      edited: false
      editors:
      - HoangCuongNguyen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6250102519989014
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d066ce749587dbe035b3aa/XJ7wN98DMvRMIrF5ctOvh.png?w=200&h=200&f=face
          fullname: Hoang Cuong Nguyen
          isHf: false
          isPro: false
          name: HoangCuongNguyen
          type: user
        html: "<p>I am trying to run <code>llama-2-70b-chat.ggmlv3.q4_0.bin</code>\
          \ in my Google Colab now, and I load <code>llama_cpp</code> library like\
          \ this:</p>\n<pre><code>!pip install huggingface_hub\nmodel_name_or_path\
          \ = \"TheBloke/Llama-2-70B-Chat-GGML\"\nmodel_basename = \"llama-2-70b-chat.ggmlv3.q4_0.bin\"\
          \n\nfrom huggingface_hub import hf_hub_download\nfrom llama_cpp import Llama\n\
          \nmodel_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n\
          \n# GPU\nlcpp_llm = None\nlcpp_llm = Llama(\n    model_path=model_path,\n\
          \    n_threads=2, # CPU cores\n    n_batch=512, # Should be between 1 and\
          \ n_ctx, consider the amount of VRAM in your GPU.\n    n_gpu_layers=32 #\
          \ Change this value based on your model and your GPU VRAM pool.\n    )\n\
          </code></pre>\n<p>However, what I got is an AssertionError:</p>\n<pre><code\
          \ class=\"language-AssertionError\">&lt;ipython-input-51-da96b2fa6a04&gt;\
          \ in &lt;cell line: 3&gt;()\n      1 # GPU\n      2 lcpp_llm = None\n----&gt;\
          \ 3 lcpp_llm = Llama(\n      4     model_path=model_path,\n      5     n_threads=2,\
          \ # CPU cores\n\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\
          \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv,\
          \ logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch,\
          \ last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base,\
          \ rope_freq_scale, n_gqa, rms_norm_eps, verbose)\n    311             self.model_path.encode(\"\
          utf-8\"), self.params\n    312         )\n--&gt; 313         assert self.model\
          \ is not None\n    314 \n    315         self.ctx = llama_cpp.llama_new_context_with_model(self.model,\
          \ self.params)\n\nAssertionError: \n</code></pre>\n<p>How can I solve this\
          \ error, to run LLaMa 2 70B in my Google Colab?</p>\n"
        raw: "I am trying to run ```llama-2-70b-chat.ggmlv3.q4_0.bin``` in my Google\
          \ Colab now, and I load ```llama_cpp``` library like this:\n```\n!pip install\
          \ huggingface_hub\nmodel_name_or_path = \"TheBloke/Llama-2-70B-Chat-GGML\"\
          \nmodel_basename = \"llama-2-70b-chat.ggmlv3.q4_0.bin\"\n\nfrom huggingface_hub\
          \ import hf_hub_download\nfrom llama_cpp import Llama\n\nmodel_path = hf_hub_download(repo_id=model_name_or_path,\
          \ filename=model_basename)\n\n# GPU\nlcpp_llm = None\nlcpp_llm = Llama(\n\
          \    model_path=model_path,\n    n_threads=2, # CPU cores\n    n_batch=512,\
          \ # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n\
          \    n_gpu_layers=32 # Change this value based on your model and your GPU\
          \ VRAM pool.\n    )\n```\nHowever, what I got is an AssertionError:\n```AssertionError\
          \                            Traceback (most recent call last)\n<ipython-input-51-da96b2fa6a04>\
          \ in <cell line: 3>()\n      1 # GPU\n      2 lcpp_llm = None\n----> 3 lcpp_llm\
          \ = Llama(\n      4     model_path=model_path,\n      5     n_threads=2,\
          \ # CPU cores\n\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\
          \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv,\
          \ logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch,\
          \ last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base,\
          \ rope_freq_scale, n_gqa, rms_norm_eps, verbose)\n    311             self.model_path.encode(\"\
          utf-8\"), self.params\n    312         )\n--> 313         assert self.model\
          \ is not None\n    314 \n    315         self.ctx = llama_cpp.llama_new_context_with_model(self.model,\
          \ self.params)\n\nAssertionError: \n```\nHow can I solve this error, to\
          \ run LLaMa 2 70B in my Google Colab?"
        updatedAt: '2023-08-11T01:42:06.852Z'
      numEdits: 0
      reactions: []
    id: 64d591eea146b1c0a652b7b3
    type: comment
  author: HoangCuongNguyen
  content: "I am trying to run ```llama-2-70b-chat.ggmlv3.q4_0.bin``` in my Google\
    \ Colab now, and I load ```llama_cpp``` library like this:\n```\n!pip install\
    \ huggingface_hub\nmodel_name_or_path = \"TheBloke/Llama-2-70B-Chat-GGML\"\nmodel_basename\
    \ = \"llama-2-70b-chat.ggmlv3.q4_0.bin\"\n\nfrom huggingface_hub import hf_hub_download\n\
    from llama_cpp import Llama\n\nmodel_path = hf_hub_download(repo_id=model_name_or_path,\
    \ filename=model_basename)\n\n# GPU\nlcpp_llm = None\nlcpp_llm = Llama(\n    model_path=model_path,\n\
    \    n_threads=2, # CPU cores\n    n_batch=512, # Should be between 1 and n_ctx,\
    \ consider the amount of VRAM in your GPU.\n    n_gpu_layers=32 # Change this\
    \ value based on your model and your GPU VRAM pool.\n    )\n```\nHowever, what\
    \ I got is an AssertionError:\n```AssertionError                            Traceback\
    \ (most recent call last)\n<ipython-input-51-da96b2fa6a04> in <cell line: 3>()\n\
    \      1 # GPU\n      2 lcpp_llm = None\n----> 3 lcpp_llm = Llama(\n      4  \
    \   model_path=model_path,\n      5     n_threads=2, # CPU cores\n\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\
    \ in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all,\
    \ vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size,\
    \ lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale,\
    \ n_gqa, rms_norm_eps, verbose)\n    311             self.model_path.encode(\"\
    utf-8\"), self.params\n    312         )\n--> 313         assert self.model is\
    \ not None\n    314 \n    315         self.ctx = llama_cpp.llama_new_context_with_model(self.model,\
    \ self.params)\n\nAssertionError: \n```\nHow can I solve this error, to run LLaMa\
    \ 2 70B in my Google Colab?"
  created_at: 2023-08-11 00:42:06+00:00
  edited: false
  hidden: false
  id: 64d591eea146b1c0a652b7b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4728ef86ddb96d0c170fd966af472513.svg
      fullname: Dan Prisacaru
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielp345
      type: user
    createdAt: '2023-08-15T01:17:42.000Z'
    data:
      edited: false
      editors:
      - danielp345
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3891390264034271
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4728ef86ddb96d0c170fd966af472513.svg
          fullname: Dan Prisacaru
          isHf: false
          isPro: false
          name: danielp345
          type: user
        html: '<p>Add this parameter to the constructor: n_gqa=8</p>

          <p>Llama(<br>       model_path=model_path,<br>      n_gqa=8,                                    #
          add this<br>      n_threads=2,</p>

          '
        raw: "Add this parameter to the constructor: n_gqa=8\n\nLlama(\n       model_path=model_path,\n\
          \      n_gqa=8,                                    # add this\n      n_threads=2,"
        updatedAt: '2023-08-15T01:17:42.431Z'
      numEdits: 0
      reactions: []
    id: 64dad23669d21c567bdb780b
    type: comment
  author: danielp345
  content: "Add this parameter to the constructor: n_gqa=8\n\nLlama(\n       model_path=model_path,\n\
    \      n_gqa=8,                                    # add this\n      n_threads=2,"
  created_at: 2023-08-15 00:17:42+00:00
  edited: false
  hidden: false
  id: 64dad23669d21c567bdb780b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/06d2feec32fc432ef061bf6feec9af7f.svg
      fullname: Mert Sevinc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xtrecoolx
      type: user
    createdAt: '2023-08-17T12:58:24.000Z'
    data:
      edited: false
      editors:
      - xtrecoolx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8638842701911926
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/06d2feec32fc432ef061bf6feec9af7f.svg
          fullname: Mert Sevinc
          isHf: false
          isPro: false
          name: xtrecoolx
          type: user
        html: '<p>Is there really a python library called llama_cpp ? If yes, how
          come it is not available on PIP.</p>

          '
        raw: Is there really a python library called llama_cpp ? If yes, how come
          it is not available on PIP.
        updatedAt: '2023-08-17T12:58:24.475Z'
      numEdits: 0
      reactions: []
    id: 64de1970f08b064990cc4546
    type: comment
  author: xtrecoolx
  content: Is there really a python library called llama_cpp ? If yes, how come it
    is not available on PIP.
  created_at: 2023-08-17 11:58:24+00:00
  edited: false
  hidden: false
  id: 64de1970f08b064990cc4546
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-17T13:00:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.686148464679718
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, llama.cpp is a C++ application run from command line <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></p>

          <p>For using these models from Python, use either llama-cpp-python (<a rel="nofollow"
          href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a>)
          or ctransformers (<a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a>)</p>

          '
        raw: 'No, llama.cpp is a C++ application run from command line https://github.com/ggerganov/llama.cpp


          For using these models from Python, use either llama-cpp-python (https://github.com/abetlen/llama-cpp-python)
          or ctransformers (https://github.com/marella/ctransformers)'
        updatedAt: '2023-08-17T13:00:29.293Z'
      numEdits: 0
      reactions: []
    id: 64de19ed842e705658752608
    type: comment
  author: TheBloke
  content: 'No, llama.cpp is a C++ application run from command line https://github.com/ggerganov/llama.cpp


    For using these models from Python, use either llama-cpp-python (https://github.com/abetlen/llama-cpp-python)
    or ctransformers (https://github.com/marella/ctransformers)'
  created_at: 2023-08-17 12:00:29+00:00
  edited: false
  hidden: false
  id: 64de19ed842e705658752608
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/06d2feec32fc432ef061bf6feec9af7f.svg
      fullname: Mert Sevinc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xtrecoolx
      type: user
    createdAt: '2023-08-17T13:09:35.000Z'
    data:
      edited: false
      editors:
      - xtrecoolx
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774531126022339
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/06d2feec32fc432ef061bf6feec9af7f.svg
          fullname: Mert Sevinc
          isHf: false
          isPro: false
          name: xtrecoolx
          type: user
        html: '<p>Yes I use the command line, but I saw a sample Python code above
          in this thread that''s why I was suprised :-)</p>

          '
        raw: Yes I use the command line, but I saw a sample Python code above in this
          thread that's why I was suprised :-)
        updatedAt: '2023-08-17T13:09:35.875Z'
      numEdits: 0
      reactions: []
    id: 64de1c0f2be7a57fa240daef
    type: comment
  author: xtrecoolx
  content: Yes I use the command line, but I saw a sample Python code above in this
    thread that's why I was suprised :-)
  created_at: 2023-08-17 12:09:35+00:00
  edited: false
  hidden: false
  id: 64de1c0f2be7a57fa240daef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c10d17c972493566380d6cd39eaeb05a.svg
      fullname: Sanjay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanjay-dev-ds-28
      type: user
    createdAt: '2023-08-18T17:34:23.000Z'
    data:
      edited: false
      editors:
      - sanjay-dev-ds-28
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6216872334480286
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c10d17c972493566380d6cd39eaeb05a.svg
          fullname: Sanjay
          isHf: false
          isPro: false
          name: sanjay-dev-ds-28
          type: user
        html: "<p>model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"<br>model_basename\
          \ = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format</p>\n\
          <p>model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)</p>\n\
          <p>n_gpu_layers = 40  # Change this value based on your model and your GPU\
          \ VRAM pool.<br>n_batch = 256  # Should be between 1 and n_ctx, consider\
          \ the amount of VRAM in your GPU.</p>\n<h1 id=\"loading-model\">Loading\
          \ model,</h1>\n<p>llm = LlamaCpp(<br>    model_path=model_path,<br>    max_tokens=256,<br>\
          \    n_gpu_layers=n_gpu_layers,<br>    n_batch=n_batch,<br>    callback_manager=callback_manager,<br>\
          \    n_ctx=1024,<br>    verbose=False,<br>    n_gqa=8<br>)</p>\n<p>I am\
          \ getting<br>ValidationError: 1 validation error for LlamaCpp<br><strong>root</strong><br>\
          \  Could not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_0.bin.\
          \ Received error fileno (type=value_error)  Please help me !!  <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> </p>\n"
        raw: "\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\nmodel_basename\
          \ = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n\n\
          model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n\
          \nn_gpu_layers = 40  # Change this value based on your model and your GPU\
          \ VRAM pool.\nn_batch = 256  # Should be between 1 and n_ctx, consider the\
          \ amount of VRAM in your GPU.\n\n# Loading model,\nllm = LlamaCpp(\n   \
          \ model_path=model_path,\n    max_tokens=256,\n    n_gpu_layers=n_gpu_layers,\n\
          \    n_batch=n_batch,\n    callback_manager=callback_manager,\n    n_ctx=1024,\n\
          \    verbose=False,\n    n_gqa=8\n)\n\n\nI am getting \nValidationError:\
          \ 1 validation error for LlamaCpp\n__root__\n  Could not load Llama model\
          \ from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_0.bin.\
          \ Received error fileno (type=value_error)  Please help me !!  @TheBloke\
          \ \n"
        updatedAt: '2023-08-18T17:34:23.351Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sanjay-dev-ds-28
    id: 64dfab9ff6c2311e7ece534e
    type: comment
  author: sanjay-dev-ds-28
  content: "\nmodel_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\nmodel_basename\
    \ = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n\nmodel_path\
    \ = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n\nn_gpu_layers\
    \ = 40  # Change this value based on your model and your GPU VRAM pool.\nn_batch\
    \ = 256  # Should be between 1 and n_ctx, consider the amount of VRAM in your\
    \ GPU.\n\n# Loading model,\nllm = LlamaCpp(\n    model_path=model_path,\n    max_tokens=256,\n\
    \    n_gpu_layers=n_gpu_layers,\n    n_batch=n_batch,\n    callback_manager=callback_manager,\n\
    \    n_ctx=1024,\n    verbose=False,\n    n_gqa=8\n)\n\n\nI am getting \nValidationError:\
    \ 1 validation error for LlamaCpp\n__root__\n  Could not load Llama model from\
    \ path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_0.bin.\
    \ Received error fileno (type=value_error)  Please help me !!  @TheBloke \n"
  created_at: 2023-08-18 16:34:23+00:00
  edited: false
  hidden: false
  id: 64dfab9ff6c2311e7ece534e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d2d06abf4c2192aabc34a05fef1a428b.svg
      fullname: Aatm Prakash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luvkush-ap
      type: user
    createdAt: '2023-08-22T16:04:35.000Z'
    data:
      edited: true
      editors:
      - luvkush-ap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7121151685714722
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d2d06abf4c2192aabc34a05fef1a428b.svg
          fullname: Aatm Prakash
          isHf: false
          isPro: false
          name: luvkush-ap
          type: user
        html: "<p>i'm aslo having same kind of error when i'm loading the models </p>\n\
          <p>model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"<br>model_basename\
          \ = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format</p>\n\
          <p>model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)</p>\n\
          <p>n_gpu_layers = 40 # Change this value based on your model and your GPU\
          \ VRAM pool.<br>n_batch = 256 # Should be between 1 and n_ctx, consider\
          \ the amount of VRAM in your GPU.</p>\n<p>Loading model,<br>llm = LlamaCpp(<br>model_path=model_path,<br>max_tokens=256,<br>n_gpu_layers=n_gpu_layers,<br>n_batch=n_batch,<br>callback_manager=callback_manager,<br>n_ctx=1024,<br>verbose=False,<br>n_gqa=8<br>)</p>\n\
          <p>I am getting<br>ValidationError: 1 validation error for LlamaCpp<br>root<br>Could\
          \ not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_0.bin.\
          \ Received error fileno (type=value_error) Please help me !! <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span><br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/64dcb3a4a437abbfbe80daf7/5KP3_3ZiYs9E-EpdzAgFv.png\"\
          ><img alt=\"Screenshot from 2023-08-22 21-35-15.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/64dcb3a4a437abbfbe80daf7/5KP3_3ZiYs9E-EpdzAgFv.png\"\
          ></a></p>\n"
        raw: "i'm aslo having same kind of error when i'm loading the models \n\n\
          model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\nmodel_basename\
          \ = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n\n\
          model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n\
          \nn_gpu_layers = 40 # Change this value based on your model and your GPU\
          \ VRAM pool.\nn_batch = 256 # Should be between 1 and n_ctx, consider the\
          \ amount of VRAM in your GPU.\n\nLoading model,\nllm = LlamaCpp(\nmodel_path=model_path,\n\
          max_tokens=256,\nn_gpu_layers=n_gpu_layers,\nn_batch=n_batch,\ncallback_manager=callback_manager,\n\
          n_ctx=1024,\nverbose=False,\nn_gqa=8\n)\n\n\nI am getting\nValidationError:\
          \ 1 validation error for LlamaCpp\nroot\nCould not load Llama model from\
          \ path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_0.bin.\
          \ Received error fileno (type=value_error) Please help me !! @TheBloke\n\
          ![Screenshot from 2023-08-22 21-35-15.png](https://cdn-uploads.huggingface.co/production/uploads/64dcb3a4a437abbfbe80daf7/5KP3_3ZiYs9E-EpdzAgFv.png)\n"
        updatedAt: '2023-08-22T16:05:50.628Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - sanjay-dev-ds-28
        - borkori
    id: 64e4dc93ae2516de4157930a
    type: comment
  author: luvkush-ap
  content: "i'm aslo having same kind of error when i'm loading the models \n\nmodel_name_or_path\
    \ = \"TheBloke/Llama-2-13B-chat-GGML\"\nmodel_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\
    \ # the model is in bin format\n\nmodel_path = hf_hub_download(repo_id=model_name_or_path,\
    \ filename=model_basename)\n\nn_gpu_layers = 40 # Change this value based on your\
    \ model and your GPU VRAM pool.\nn_batch = 256 # Should be between 1 and n_ctx,\
    \ consider the amount of VRAM in your GPU.\n\nLoading model,\nllm = LlamaCpp(\n\
    model_path=model_path,\nmax_tokens=256,\nn_gpu_layers=n_gpu_layers,\nn_batch=n_batch,\n\
    callback_manager=callback_manager,\nn_ctx=1024,\nverbose=False,\nn_gqa=8\n)\n\n\
    \nI am getting\nValidationError: 1 validation error for LlamaCpp\nroot\nCould\
    \ not load Llama model from path: /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGML/snapshots/47d28ef5de4f3de523c421f325a2e4e039035bab/llama-2-13b-chat.ggmlv3.q5_0.bin.\
    \ Received error fileno (type=value_error) Please help me !! @TheBloke\n![Screenshot\
    \ from 2023-08-22 21-35-15.png](https://cdn-uploads.huggingface.co/production/uploads/64dcb3a4a437abbfbe80daf7/5KP3_3ZiYs9E-EpdzAgFv.png)\n"
  created_at: 2023-08-22 15:04:35+00:00
  edited: true
  hidden: false
  id: 64e4dc93ae2516de4157930a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-22T16:09:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9181246757507324
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Latest llama.cpp is no longer compatible with GGML models.  The
          new model format, GGUF, was merged last night.  As far as llama.cpp is concerned,
          GGML is now dead - though of course many third-party clients/libraries are
          likely to continue to support it for a lot longer.</p>

          <p>I will be providing GGUF models for all my repos in the next 2-3 days.
          I''m waiting for another PR to merge, which will add improved k-quant quantisation
          formats.</p>

          <p>For now, if you want to use llama.cpp you will need to downgrade it back
          to commit <code>dadbed99e65252d79f81101a392d0d6497b86caa</code> or earlier.  Or
          use one of the llama.cpp binary releases from before GGUF was merged.  Or
          use a third party client like KoboldCpp, LM Studio, text-generation-webui,
          etc.</p>

          <p>Look out for new <code>-GGUF</code> repos from me in the coming days.</p>

          '
        raw: 'Latest llama.cpp is no longer compatible with GGML models.  The new
          model format, GGUF, was merged last night.  As far as llama.cpp is concerned,
          GGML is now dead - though of course many third-party clients/libraries are
          likely to continue to support it for a lot longer.


          I will be providing GGUF models for all my repos in the next 2-3 days. I''m
          waiting for another PR to merge, which will add improved k-quant quantisation
          formats.


          For now, if you want to use llama.cpp you will need to downgrade it back
          to commit `dadbed99e65252d79f81101a392d0d6497b86caa` or earlier.  Or use
          one of the llama.cpp binary releases from before GGUF was merged.  Or use
          a third party client like KoboldCpp, LM Studio, text-generation-webui, etc.


          Look out for new `-GGUF` repos from me in the coming days.'
        updatedAt: '2023-08-22T16:09:54.490Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - luvkush-ap
        - numleh
        - jd4ever
        - boeroboy
        - pa-mc
        - 1ucky1uke
        - dillfrescott
      - count: 1
        reaction: "\U0001F917"
        users:
        - pa-mc
    id: 64e4ddd2e39849fe47093be3
    type: comment
  author: TheBloke
  content: 'Latest llama.cpp is no longer compatible with GGML models.  The new model
    format, GGUF, was merged last night.  As far as llama.cpp is concerned, GGML is
    now dead - though of course many third-party clients/libraries are likely to continue
    to support it for a lot longer.


    I will be providing GGUF models for all my repos in the next 2-3 days. I''m waiting
    for another PR to merge, which will add improved k-quant quantisation formats.


    For now, if you want to use llama.cpp you will need to downgrade it back to commit
    `dadbed99e65252d79f81101a392d0d6497b86caa` or earlier.  Or use one of the llama.cpp
    binary releases from before GGUF was merged.  Or use a third party client like
    KoboldCpp, LM Studio, text-generation-webui, etc.


    Look out for new `-GGUF` repos from me in the coming days.'
  created_at: 2023-08-22 15:09:54+00:00
  edited: false
  hidden: false
  id: 64e4ddd2e39849fe47093be3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-08-22T16:13:02.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5475391149520874
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>sigh.  gotta love a moving target. </p>

          '
        raw: 'sigh.  gotta love a moving target. '
        updatedAt: '2023-08-22T16:13:02.051Z'
      numEdits: 0
      reactions: []
    id: 64e4de8ef25907c3240af960
    type: comment
  author: Nurb432
  content: 'sigh.  gotta love a moving target. '
  created_at: 2023-08-22 15:13:02+00:00
  edited: false
  hidden: false
  id: 64e4de8ef25907c3240af960
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d631838e201b55a8820e5e7e865b646.svg
      fullname: Fred Rogers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fred12321
      type: user
    createdAt: '2023-08-31T20:28:57.000Z'
    data:
      edited: false
      editors:
      - Fred12321
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4167722761631012
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d631838e201b55a8820e5e7e865b646.svg
          fullname: Fred Rogers
          isHf: false
          isPro: false
          name: Fred12321
          type: user
        html: '<p>You can use the "convert-llama-ggmlv3-to-gguf.py" in the llama.cpp
          github to convert.</p>

          '
        raw: You can use the "convert-llama-ggmlv3-to-gguf.py" in the llama.cpp github
          to convert.
        updatedAt: '2023-08-31T20:28:57.687Z'
      numEdits: 0
      reactions: []
    id: 64f0f80914dd32cb8aa7b316
    type: comment
  author: Fred12321
  content: You can use the "convert-llama-ggmlv3-to-gguf.py" in the llama.cpp github
    to convert.
  created_at: 2023-08-31 19:28:57+00:00
  edited: false
  hidden: false
  id: 64f0f80914dd32cb8aa7b316
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tWYaSDhKsaiP7NvebE2iM.png?w=200&h=200&f=face
      fullname: John Boero
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: boeroboy
      type: user
    createdAt: '2023-09-03T10:26:19.000Z'
    data:
      edited: true
      editors:
      - boeroboy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8198735117912292
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/tWYaSDhKsaiP7NvebE2iM.png?w=200&h=200&f=face
          fullname: John Boero
          isHf: false
          isPro: false
          name: boeroboy
          type: user
        html: '<p>Both sides of this standard are moving, apparently.  Even the convert
          script has no guarantees when GGML versions mismatch.<br><code>ValueError:
          Only GGJTv3 supported</code><br>Sounds like a fresh download is needed...</p>

          <p>BTW I started packaging Llama.cpp in a COPR DNF repo. As it doesn''t
          use versioning, I''ve been using build date.<br><a rel="nofollow" href="https://copr.fedorainfracloud.org/coprs/boeroboy/brynzai/package/llama.cpp/">https://copr.fedorainfracloud.org/coprs/boeroboy/brynzai/package/llama.cpp/</a></p>

          <p>Ignore build failures as not every platform/distro works but the important
          ones do.  Packaged for CPU and OpenCL. CUDA violates COPR license.</p>

          '
        raw: 'Both sides of this standard are moving, apparently.  Even the convert
          script has no guarantees when GGML versions mismatch.

          `ValueError: Only GGJTv3 supported`

          Sounds like a fresh download is needed...


          BTW I started packaging Llama.cpp in a COPR DNF repo. As it doesn''t use
          versioning, I''ve been using build date.

          https://copr.fedorainfracloud.org/coprs/boeroboy/brynzai/package/llama.cpp/


          Ignore build failures as not every platform/distro works but the important
          ones do.  Packaged for CPU and OpenCL. CUDA violates COPR license.'
        updatedAt: '2023-09-03T10:28:43.526Z'
      numEdits: 1
      reactions: []
    id: 64f45f4b74243e51b0ac4292
    type: comment
  author: boeroboy
  content: 'Both sides of this standard are moving, apparently.  Even the convert
    script has no guarantees when GGML versions mismatch.

    `ValueError: Only GGJTv3 supported`

    Sounds like a fresh download is needed...


    BTW I started packaging Llama.cpp in a COPR DNF repo. As it doesn''t use versioning,
    I''ve been using build date.

    https://copr.fedorainfracloud.org/coprs/boeroboy/brynzai/package/llama.cpp/


    Ignore build failures as not every platform/distro works but the important ones
    do.  Packaged for CPU and OpenCL. CUDA violates COPR license.'
  created_at: 2023-09-03 09:26:19+00:00
  edited: true
  hidden: false
  id: 64f45f4b74243e51b0ac4292
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2528a8d86b62dea436d8efe84cc38278.svg
      fullname: Sanjana K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjanaKannan
      type: user
    createdAt: '2023-09-22T09:37:43.000Z'
    data:
      edited: false
      editors:
      - SanjanaKannan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5586934089660645
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2528a8d86b62dea436d8efe84cc38278.svg
          fullname: Sanjana K
          isHf: false
          isPro: false
          name: SanjanaKannan
          type: user
        html: "<blockquote>\n<p>Add this parameter to the constructor: n_gqa=8</p>\n\
          <p>Llama(<br>       model_path=model_path,<br>      n_gqa=8,           \
          \                         # add this<br>      n_threads=2,</p>\n</blockquote>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;danielp345&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/danielp345\">@<span class=\"\
          underline\">danielp345</span></a></span>\n\n\t</span></span> tried it still\
          \ am getting the assertion error</p>\n"
        raw: "> Add this parameter to the constructor: n_gqa=8\n> \n> Llama(\n>  \
          \      model_path=model_path,\n>       n_gqa=8,                        \
          \            # add this\n>       n_threads=2,\n\n@danielp345 tried it still\
          \ am getting the assertion error"
        updatedAt: '2023-09-22T09:37:43.927Z'
      numEdits: 0
      reactions: []
    id: 650d606729ff71d4d6d0b973
    type: comment
  author: SanjanaKannan
  content: "> Add this parameter to the constructor: n_gqa=8\n> \n> Llama(\n>    \
    \    model_path=model_path,\n>       n_gqa=8,                                \
    \    # add this\n>       n_threads=2,\n\n@danielp345 tried it still am getting\
    \ the assertion error"
  created_at: 2023-09-22 08:37:43+00:00
  edited: false
  hidden: false
  id: 650d606729ff71d4d6d0b973
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2528a8d86b62dea436d8efe84cc38278.svg
      fullname: Sanjana K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SanjanaKannan
      type: user
    createdAt: '2023-09-22T09:38:14.000Z'
    data:
      edited: false
      editors:
      - SanjanaKannan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9878426790237427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2528a8d86b62dea436d8efe84cc38278.svg
          fullname: Sanjana K
          isHf: false
          isPro: false
          name: SanjanaKannan
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;HoangCuongNguyen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/HoangCuongNguyen\"\
          >@<span class=\"underline\">HoangCuongNguyen</span></a></span>\n\n\t</span></span>\
          \ were you able to fix this error?</p>\n"
        raw: '@HoangCuongNguyen were you able to fix this error?'
        updatedAt: '2023-09-22T09:38:14.532Z'
      numEdits: 0
      reactions: []
    id: 650d608606cbce96ea5eafae
    type: comment
  author: SanjanaKannan
  content: '@HoangCuongNguyen were you able to fix this error?'
  created_at: 2023-09-22 08:38:14+00:00
  edited: false
  hidden: false
  id: 650d608606cbce96ea5eafae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9ebffea48400ec2dcc5ab0ef3df892b.svg
      fullname: Karan Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qoit
      type: user
    createdAt: '2023-10-11T07:26:59.000Z'
    data:
      edited: false
      editors:
      - qoit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6481155157089233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9ebffea48400ec2dcc5ab0ef3df892b.svg
          fullname: Karan Sharma
          isHf: false
          isPro: false
          name: qoit
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> , How do I increase\
          \ the context length for this model? I am getting this error: \"Number of\
          \ tokens exceed maximum context length 512.<br>I am using CTransformers.</p>\n"
        raw: '@TheBloke , How do I increase the context length for this model? I am
          getting this error: "Number of tokens exceed maximum context length 512.

          I am using CTransformers.'
        updatedAt: '2023-10-11T07:26:59.363Z'
      numEdits: 0
      reactions: []
    id: 65264e4379706914d411a125
    type: comment
  author: qoit
  content: '@TheBloke , How do I increase the context length for this model? I am
    getting this error: "Number of tokens exceed maximum context length 512.

    I am using CTransformers.'
  created_at: 2023-10-11 06:26:59+00:00
  edited: false
  hidden: false
  id: 65264e4379706914d411a125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RPlGT5NAVFcOiFpxnjFrl.jpeg?w=200&h=200&f=face
      fullname: Imam Cholissodin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imamcs
      type: user
    createdAt: '2023-11-17T09:21:49.000Z'
    data:
      edited: false
      editors:
      - imamcs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36838284134864807
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RPlGT5NAVFcOiFpxnjFrl.jpeg?w=200&h=200&f=face
          fullname: Imam Cholissodin
          isHf: false
          isPro: false
          name: imamcs
          type: user
        html: '<p>I have error below on MBP M1:<br>(base) imamcs@Imams-MacBook-Pro
          llama.cpp % echo "Prompt: " \</p>

          <blockquote>

          <pre><code>  &amp;&amp; read PROMPT \

          </code></pre>

          <p>cmdand&gt;         &amp;&amp; ./main <br>cmdand cmdand&gt;  -t 8 <br>cmdand
          cmdand&gt;  -ngl 1 <br>cmdand cmdand&gt;  -m ${MODEL} <br>cmdand cmdand&gt;  --color
          <br>cmdand cmdand&gt;  -c 2048 <br>cmdand cmdand&gt;  --temp 0.7 <br>cmdand
          cmdand&gt;  --repeat_penalty 1.1 <br>cmdand cmdand&gt;  -n -1 <br>cmdand
          cmdand&gt;  -p "[INST] ${PROMPT} [/INST] "<br>Prompt:<br>hello Llama!<br>Log
          start<br>main: build = 1520 (91f6499)<br>main: built with Apple clang version
          14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.5.0<br>main: seed  =
          1700212461<br>error loading model: failed to open --color: No such file
          or directory<br>llama_load_model_from_file: failed to load model<br>llama_init_from_gpt_params:
          error: failed to load model ''--color''<br>main: error: unable to load model</p>

          </blockquote>

          <p>Any solution? </p>

          '
        raw: "I have error below on MBP M1:\n(base) imamcs@Imams-MacBook-Pro llama.cpp\
          \ % echo \"Prompt: \" \\\n>       && read PROMPT \\\ncmdand>         &&\
          \ ./main \\\ncmdand cmdand>  -t 8 \\\ncmdand cmdand>  -ngl 1 \\\ncmdand\
          \ cmdand>  -m ${MODEL} \\\ncmdand cmdand>  --color \\\ncmdand cmdand>  -c\
          \ 2048 \\\ncmdand cmdand>  --temp 0.7 \\\ncmdand cmdand>  --repeat_penalty\
          \ 1.1 \\\ncmdand cmdand>  -n -1 \\\ncmdand cmdand>  -p \"[INST] ${PROMPT}\
          \ [/INST] \"\nPrompt: \nhello Llama!\nLog start\nmain: build = 1520 (91f6499)\n\
          main: built with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.5.0\n\
          main: seed  = 1700212461\nerror loading model: failed to open --color: No\
          \ such file or directory\nllama_load_model_from_file: failed to load model\n\
          llama_init_from_gpt_params: error: failed to load model '--color'\nmain:\
          \ error: unable to load model\n\nAny solution? \n"
        updatedAt: '2023-11-17T09:21:49.442Z'
      numEdits: 0
      reactions: []
    id: 655730adf7457c98b20abbd3
    type: comment
  author: imamcs
  content: "I have error below on MBP M1:\n(base) imamcs@Imams-MacBook-Pro llama.cpp\
    \ % echo \"Prompt: \" \\\n>       && read PROMPT \\\ncmdand>         && ./main\
    \ \\\ncmdand cmdand>  -t 8 \\\ncmdand cmdand>  -ngl 1 \\\ncmdand cmdand>  -m ${MODEL}\
    \ \\\ncmdand cmdand>  --color \\\ncmdand cmdand>  -c 2048 \\\ncmdand cmdand> \
    \ --temp 0.7 \\\ncmdand cmdand>  --repeat_penalty 1.1 \\\ncmdand cmdand>  -n -1\
    \ \\\ncmdand cmdand>  -p \"[INST] ${PROMPT} [/INST] \"\nPrompt: \nhello Llama!\n\
    Log start\nmain: build = 1520 (91f6499)\nmain: built with Apple clang version\
    \ 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.5.0\nmain: seed  = 1700212461\n\
    error loading model: failed to open --color: No such file or directory\nllama_load_model_from_file:\
    \ failed to load model\nllama_init_from_gpt_params: error: failed to load model\
    \ '--color'\nmain: error: unable to load model\n\nAny solution? \n"
  created_at: 2023-11-17 09:21:49+00:00
  edited: false
  hidden: false
  id: 655730adf7457c98b20abbd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-17T10:28:17.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8937471508979797
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''ve got a problem with your command line prompt - look at the
          error message: </p>

          <pre><code>error loading model: failed to open --color: No such file or
          directory

          </code></pre>

          <p>Presumably "${MODEL}" is not set correctly so it''s not seeing the name
          of the model file to load.</p>

          '
        raw: "You've got a problem with your command line prompt - look at the error\
          \ message: \n```\nerror loading model: failed to open --color: No such file\
          \ or directory\n```\n\nPresumably \"${MODEL}\" is not set correctly so it's\
          \ not seeing the name of the model file to load."
        updatedAt: '2023-11-17T10:28:17.586Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - imamcs
    id: 655740410f44935297930f5f
    type: comment
  author: TheBloke
  content: "You've got a problem with your command line prompt - look at the error\
    \ message: \n```\nerror loading model: failed to open --color: No such file or\
    \ directory\n```\n\nPresumably \"${MODEL}\" is not set correctly so it's not seeing\
    \ the name of the model file to load."
  created_at: 2023-11-17 10:28:17+00:00
  edited: false
  hidden: false
  id: 655740410f44935297930f5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RPlGT5NAVFcOiFpxnjFrl.jpeg?w=200&h=200&f=face
      fullname: Imam Cholissodin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imamcs
      type: user
    createdAt: '2023-11-24T01:35:21.000Z'
    data:
      edited: false
      editors:
      - imamcs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2100815773010254
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/RPlGT5NAVFcOiFpxnjFrl.jpeg?w=200&h=200&f=face
          fullname: Imam Cholissodin
          isHf: false
          isPro: false
          name: imamcs
          type: user
        html: '<h2 id="big-thanks-to-thebloke-done-on-mbp-m1-ram-8gb-its-very-fast-in-local-amazing-d">Big
          Thanks to TheBloke, done on MBP M1, RAM 8GB, it''s very fast in local, amazing
          :D</h2>

          <p>(base) imamcs@Imams-MacBook-Pro llama.cpp % ./main -ngl 32 -m mistral-7b-instruct-v0.1.Q4_0.gguf
          --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "{create code python
          to count circle area}"</p>

          <p>Log start<br>main: build = 1520 (91f6499)<br>main: built with Apple clang
          version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.5.0<br>main:
          seed  = 1700624603<br>llama_model_loader: loaded meta data with 20 key-value
          pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_0.gguf (version GGUF
          V2)<br>llama_model_loader: - tensor    0:                token_embd.weight
          q4_0     [  4096, 32000,     1,     1 ]<br>llama_model_loader: - tensor    1:              blk.0.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor    2:              blk.0.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor    3:              blk.0.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor    4:         blk.0.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor    6:              blk.0.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor    7:            blk.0.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor    8:           blk.0.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   10:              blk.1.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   11:              blk.1.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   12:              blk.1.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   13:         blk.1.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   15:              blk.1.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   16:            blk.1.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   17:           blk.1.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   19:              blk.2.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   20:              blk.2.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   21:              blk.2.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   22:         blk.2.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   24:              blk.2.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   25:            blk.2.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   26:           blk.2.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   28:              blk.3.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   29:              blk.3.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   30:              blk.3.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   31:         blk.3.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   33:              blk.3.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   34:            blk.3.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   35:           blk.3.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   37:              blk.4.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   38:              blk.4.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   39:              blk.4.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   40:         blk.4.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   42:              blk.4.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   43:            blk.4.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   44:           blk.4.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   46:              blk.5.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   47:              blk.5.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   48:              blk.5.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   49:         blk.5.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   51:              blk.5.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   52:            blk.5.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   53:           blk.5.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   55:              blk.6.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   56:              blk.6.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   57:              blk.6.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   58:         blk.6.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   60:              blk.6.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   61:            blk.6.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   62:           blk.6.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   64:              blk.7.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   65:              blk.7.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   66:              blk.7.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   67:         blk.7.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   69:              blk.7.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   70:            blk.7.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   71:           blk.7.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   73:              blk.8.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   74:              blk.8.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   75:              blk.8.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   76:         blk.8.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   78:              blk.8.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   79:            blk.8.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   80:           blk.8.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   82:              blk.9.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   83:              blk.9.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   84:              blk.9.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   85:         blk.9.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   87:              blk.9.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   88:            blk.9.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   89:           blk.9.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   91:             blk.10.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   92:             blk.10.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   93:             blk.10.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor   94:        blk.10.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   96:             blk.10.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor   97:           blk.10.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor   98:          blk.10.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  100:             blk.11.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  101:             blk.11.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  102:             blk.11.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  103:        blk.11.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  105:             blk.11.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  106:           blk.11.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  107:          blk.11.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  109:             blk.12.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  110:             blk.12.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  111:             blk.12.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  112:        blk.12.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  114:             blk.12.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  115:           blk.12.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  116:          blk.12.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  118:             blk.13.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  119:             blk.13.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  120:             blk.13.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  121:        blk.13.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  123:             blk.13.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  124:           blk.13.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  125:          blk.13.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  127:             blk.14.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  128:             blk.14.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  129:             blk.14.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  130:        blk.14.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  132:             blk.14.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  133:           blk.14.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  134:          blk.14.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  136:             blk.15.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  137:             blk.15.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  138:             blk.15.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  139:        blk.15.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  141:             blk.15.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  142:           blk.15.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  143:          blk.15.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  145:             blk.16.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  146:             blk.16.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  147:             blk.16.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  148:        blk.16.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  150:             blk.16.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  151:           blk.16.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  152:          blk.16.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  154:             blk.17.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  155:             blk.17.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  156:             blk.17.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  157:        blk.17.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  159:             blk.17.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  160:           blk.17.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  161:          blk.17.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  163:             blk.18.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  164:             blk.18.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  165:             blk.18.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  166:        blk.18.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  168:             blk.18.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  169:           blk.18.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  170:          blk.18.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  172:             blk.19.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  173:             blk.19.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  174:             blk.19.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  175:        blk.19.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  177:             blk.19.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  178:           blk.19.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  179:          blk.19.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  181:             blk.20.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  182:             blk.20.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  183:             blk.20.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  184:        blk.20.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  186:             blk.20.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  187:           blk.20.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  188:          blk.20.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  190:             blk.21.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  191:             blk.21.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  192:             blk.21.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  193:        blk.21.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  195:             blk.21.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  196:           blk.21.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  197:          blk.21.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  199:             blk.22.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  200:             blk.22.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  201:             blk.22.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  202:        blk.22.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  204:             blk.22.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  205:           blk.22.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  206:          blk.22.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  208:             blk.23.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  209:             blk.23.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  210:             blk.23.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  211:        blk.23.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  213:             blk.23.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  214:           blk.23.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  215:          blk.23.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  217:             blk.24.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  218:             blk.24.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  219:             blk.24.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  220:        blk.24.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  222:             blk.24.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  223:           blk.24.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  224:          blk.24.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  226:             blk.25.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  227:             blk.25.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  228:             blk.25.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  229:        blk.25.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  231:             blk.25.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  232:           blk.25.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  233:          blk.25.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  235:             blk.26.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  236:             blk.26.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  237:             blk.26.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  238:        blk.26.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  240:             blk.26.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  241:           blk.26.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  242:          blk.26.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  244:             blk.27.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  245:             blk.27.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  246:             blk.27.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  247:        blk.27.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  249:             blk.27.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  250:           blk.27.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  251:          blk.27.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  253:             blk.28.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  254:             blk.28.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  255:             blk.28.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  256:        blk.28.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  258:             blk.28.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  259:           blk.28.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  260:          blk.28.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  262:             blk.29.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  263:             blk.29.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  264:             blk.29.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  265:        blk.29.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  267:             blk.29.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  268:           blk.29.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  269:          blk.29.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  271:             blk.30.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  272:             blk.30.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  273:             blk.30.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  274:        blk.30.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  276:             blk.30.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  277:           blk.30.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  278:          blk.30.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  280:             blk.31.attn_q.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  281:             blk.31.attn_k.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  282:             blk.31.attn_v.weight
          q4_0     [  4096,  1024,     1,     1 ]<br>llama_model_loader: - tensor  283:        blk.31.attn_output.weight
          q4_0     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  285:             blk.31.ffn_up.weight
          q4_0     [  4096, 14336,     1,     1 ]<br>llama_model_loader: - tensor  286:           blk.31.ffn_down.weight
          q4_0     [ 14336,  4096,     1,     1 ]<br>llama_model_loader: - tensor  287:          blk.31.attn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  289:               output_norm.weight
          f32      [  4096,     1,     1,     1 ]<br>llama_model_loader: - tensor  290:                    output.weight
          q6_K     [  4096, 32000,     1,     1 ]<br>llama_model_loader: - kv   0:                       general.architecture
          str<br>llama_model_loader: - kv   1:                               general.name
          str<br>llama_model_loader: - kv   2:                       llama.context_length
          u32<br>llama_model_loader: - kv   3:                     llama.embedding_length
          u32<br>llama_model_loader: - kv   4:                          llama.block_count
          u32<br>llama_model_loader: - kv   5:                  llama.feed_forward_length
          u32<br>llama_model_loader: - kv   6:                 llama.rope.dimension_count
          u32<br>llama_model_loader: - kv   7:                 llama.attention.head_count
          u32<br>llama_model_loader: - kv   8:              llama.attention.head_count_kv
          u32<br>llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon
          f32<br>llama_model_loader: - kv  10:                       llama.rope.freq_base
          f32<br>llama_model_loader: - kv  11:                          general.file_type
          u32<br>llama_model_loader: - kv  12:                       tokenizer.ggml.model
          str<br>llama_model_loader: - kv  13:                      tokenizer.ggml.tokens
          arr<br>llama_model_loader: - kv  14:                      tokenizer.ggml.scores
          arr<br>llama_model_loader: - kv  15:                  tokenizer.ggml.token_type
          arr<br>llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id
          u32<br>llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id
          u32<br>llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id
          u32<br>llama_model_loader: - kv  19:               general.quantization_version
          u32<br>llama_model_loader: - type  f32:   65 tensors<br>llama_model_loader:
          - type q4_0:  225 tensors<br>llama_model_loader: - type q6_K:    1 tensors<br>llm_load_vocab:
          special tokens definition check successful ( 259/32000 ).<br>llm_load_print_meta:
          format           = GGUF V2<br>llm_load_print_meta: arch             = llama<br>llm_load_print_meta:
          vocab type       = SPM<br>llm_load_print_meta: n_vocab          = 32000<br>llm_load_print_meta:
          n_merges         = 0<br>llm_load_print_meta: n_ctx_train      = 32768<br>llm_load_print_meta:
          n_embd           = 4096<br>llm_load_print_meta: n_head           = 32<br>llm_load_print_meta:
          n_head_kv        = 8<br>llm_load_print_meta: n_layer          = 32<br>llm_load_print_meta:
          n_rot            = 128<br>llm_load_print_meta: n_gqa            = 4<br>llm_load_print_meta:
          f_norm_eps       = 0.0e+00<br>llm_load_print_meta: f_norm_rms_eps   = 1.0e-05<br>llm_load_print_meta:
          f_clamp_kqv      = 0.0e+00<br>llm_load_print_meta: f_max_alibi_bias = 0.0e+00<br>llm_load_print_meta:
          n_ff             = 14336<br>llm_load_print_meta: rope scaling     = linear<br>llm_load_print_meta:
          freq_base_train  = 10000.0<br>llm_load_print_meta: freq_scale_train = 1<br>llm_load_print_meta:
          n_yarn_orig_ctx  = 32768<br>llm_load_print_meta: rope_finetuned   = unknown<br>llm_load_print_meta:
          model type       = 7B<br>llm_load_print_meta: model ftype      = mostly
          Q4_0<br>llm_load_print_meta: model params     = 7.24 B<br>llm_load_print_meta:
          model size       = 3.83 GiB (4.54 BPW)<br>llm_load_print_meta: general.name   =
          mistralai_mistral-7b-instruct-v0.1<br>llm_load_print_meta: BOS token = 1
          ''<s>''<br>llm_load_print_meta: EOS token = 2 ''</s>''<br>llm_load_print_meta:
          UNK token = 0 ''''<br>llm_load_print_meta: LF token  = 13 ''&lt;0x0A&gt;''<br>llm_load_tensors:
          ggml ctx size =    0.11 MB<br>llm_load_tensors: mem required  = 3917.97
          MB<br>..................................................................................................<br>llama_new_context_with_model:
          n_ctx      = 4096<br>llama_new_context_with_model: freq_base  = 10000.0<br>llama_new_context_with_model:
          freq_scale = 1<br>llama_new_context_with_model: kv self size  =  512.00
          MB<br>llama_build_graph: non-view tensors processed: 740/740<br>ggml_metal_init:
          allocating<br>ggml_metal_init: found device: Apple M1<br>ggml_metal_init:
          picking default device: Apple M1<br>ggml_metal_init: default.metallib not
          found, loading from source<br>ggml_metal_init: loading ''/Users/imamcs/Documents/llama.cpp/ggml-metal.metal''<br>ggml_metal_init:
          GPU name:   Apple M1<br>ggml_metal_init: GPU family: MTLGPUFamilyApple7
          (1007)<br>ggml_metal_init: hasUnifiedMemory              = true<br>ggml_metal_init:
          recommendedMaxWorkingSetSize  =  5461.34 MB<br>ggml_metal_init: maxTransferRate               =
          built-in GPU<br>llama_new_context_with_model: compute buffer total size
          = 289.57 MB<br>llama_new_context_with_model: max tensor size =   102.54
          MB<br>ggml_metal_add_buffer: allocated ''data            '' buffer, size
          =  3918.58 MB, ( 3919.08 /  5461.34)<br>ggml_metal_add_buffer: allocated
          ''kv              '' buffer, size =   512.02 MB, ( 4431.09 /  5461.34)<br>ggml_metal_add_buffer:
          allocated ''alloc           '' buffer, size =   288.02 MB, ( 4719.11 /  5461.34)</p>

          <p>system_info: n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI
          = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA
          = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |<br>sampling:<br>    repeat_last_n
          = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty
          = 0.000<br>    top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050,
          typical_p = 1.000, temp = 0.700<br>    mirostat = 0, mirostat_lr = 0.100,
          mirostat_ent = 5.000<br>generate: n_ctx = 4096, n_batch = 512, n_predict
          = -1, n_keep = 0</p>

          <p> {create code python to count circle area}</p>

          <p>```<br>import math<br>r = float(input("Enter the radius of the circle:
          "))<br>area = math.pi * r ** 2<br>print("The area of the circle is:", area)</p>

          <p>``` [end of text]</p>

          <p>llama_print_timings:        load time =    2516.22 ms<br>llama_print_timings:      sample
          time =      28.71 ms /    50 runs   (    0.57 ms per token,  1741.86 tokens
          per second)<br>llama_print_timings: prompt eval time =     262.45 ms /    10
          tokens (   26.24 ms per token,    38.10 tokens per second)<br>llama_print_timings:        eval
          time =    3836.81 ms /    49 runs   (   78.30 ms per token,    12.77 tokens
          per second)<br>llama_print_timings:       total time =    4171.65 ms<br>ggml_metal_free:
          deallocating</p>

          <hr>

          <p>Now, how to call it or include the prompt script from Flask Framework
          as UI Web App?</p>

          '
        raw: "Big Thanks to TheBloke, done on MBP M1, RAM 8GB, it's very fast in local,\
          \ amazing :D\n------------------------------\n(base) imamcs@Imams-MacBook-Pro\
          \ llama.cpp % ./main -ngl 32 -m mistral-7b-instruct-v0.1.Q4_0.gguf --color\
          \ -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"{create code python\
          \ to count circle area}\"\n\nLog start\nmain: build = 1520 (91f6499)\nmain:\
          \ built with Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.5.0\n\
          main: seed  = 1700624603\nllama_model_loader: loaded meta data with 20 key-value\
          \ pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_0.gguf (version\
          \ GGUF V2)\nllama_model_loader: - tensor    0:                token_embd.weight\
          \ q4_0     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor\
          \    1:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \    3:              blk.0.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    5:            blk.0.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \    7:            blk.0.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   11:              blk.1.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   13:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   15:              blk.1.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   19:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   21:              blk.2.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   23:            blk.2.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   25:            blk.2.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   29:              blk.3.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   31:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   33:              blk.3.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   37:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   39:              blk.4.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   41:            blk.4.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   43:            blk.4.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   47:              blk.5.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   49:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   51:              blk.5.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   55:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   57:              blk.6.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   59:            blk.6.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   61:            blk.6.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   65:              blk.7.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   67:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   69:              blk.7.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   73:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   75:              blk.8.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   77:            blk.8.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   79:            blk.8.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   83:              blk.9.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   85:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   87:              blk.9.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   91:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \   93:             blk.10.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \   95:           blk.10.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \   97:           blk.10.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  101:             blk.11.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  103:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  105:             blk.11.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  109:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  111:             blk.12.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  113:           blk.12.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  115:           blk.12.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  119:             blk.13.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  121:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  123:             blk.13.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  127:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  129:             blk.14.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  131:           blk.14.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  133:           blk.14.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  137:             blk.15.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  139:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  141:             blk.15.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  145:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  147:             blk.16.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  149:           blk.16.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  151:           blk.16.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  155:             blk.17.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  157:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  159:             blk.17.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  163:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  165:             blk.18.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  167:           blk.18.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  169:           blk.18.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  173:             blk.19.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  175:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  177:             blk.19.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  181:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  183:             blk.20.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  185:           blk.20.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  187:           blk.20.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  191:             blk.21.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  193:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  195:             blk.21.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  199:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  201:             blk.22.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  203:           blk.22.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  205:           blk.22.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  209:             blk.23.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  211:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  213:             blk.23.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  217:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  219:             blk.24.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  221:           blk.24.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  223:           blk.24.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  227:             blk.25.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  229:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  231:             blk.25.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  235:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  237:             blk.26.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  239:           blk.26.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  241:           blk.26.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  245:             blk.27.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  247:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  249:             blk.27.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  253:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  255:             blk.28.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  257:           blk.28.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  259:           blk.28.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  263:             blk.29.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  265:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  267:             blk.29.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  271:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  273:             blk.30.attn_v.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  275:           blk.30.ffn_gate.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  277:           blk.30.ffn_down.weight q4_0     [ 14336,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight\
          \ q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  281:             blk.31.attn_k.weight q4_0     [  4096,  1024,     1,\
          \     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight\
          \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor\
          \  283:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight\
          \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor\
          \  285:             blk.31.ffn_up.weight q4_0     [  4096, 14336,     1,\
          \     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight\
          \ q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor\
          \  289:               output_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor  290:                    output.weight\
          \ q6_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - kv   0:\
          \                       general.architecture str     \nllama_model_loader:\
          \ - kv   1:                               general.name str     \nllama_model_loader:\
          \ - kv   2:                       llama.context_length u32     \nllama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32     \nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32     \nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32     \nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32     \nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32     \nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32     \nllama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \nllama_model_loader:\
          \ - kv  10:                       llama.rope.freq_base f32     \nllama_model_loader:\
          \ - kv  11:                          general.file_type u32     \nllama_model_loader:\
          \ - kv  12:                       tokenizer.ggml.model str     \nllama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.tokens arr     \nllama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr     \nllama_model_loader:\
          \ - kv  15:                  tokenizer.ggml.token_type arr     \nllama_model_loader:\
          \ - kv  16:                tokenizer.ggml.bos_token_id u32     \nllama_model_loader:\
          \ - kv  17:                tokenizer.ggml.eos_token_id u32     \nllama_model_loader:\
          \ - kv  18:            tokenizer.ggml.unknown_token_id u32     \nllama_model_loader:\
          \ - kv  19:               general.quantization_version u32     \nllama_model_loader:\
          \ - type  f32:   65 tensors\nllama_model_loader: - type q4_0:  225 tensors\n\
          llama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab: special tokens\
          \ definition check successful ( 259/32000 ).\nllm_load_print_meta: format\
          \           = GGUF V2\nllm_load_print_meta: arch             = llama\nllm_load_print_meta:\
          \ vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\n\
          llm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train\
          \      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta:\
          \ n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta:\
          \ n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta:\
          \ n_gqa            = 4\nllm_load_print_meta: f_norm_eps       = 0.0e+00\n\
          llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv\
          \      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta:\
          \ n_ff             = 14336\nllm_load_print_meta: rope scaling     = linear\n\
          llm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train\
          \ = 1\nllm_load_print_meta: n_yarn_orig_ctx  = 32768\nllm_load_print_meta:\
          \ rope_finetuned   = unknown\nllm_load_print_meta: model type       = 7B\n\
          llm_load_print_meta: model ftype      = mostly Q4_0\nllm_load_print_meta:\
          \ model params     = 7.24 B\nllm_load_print_meta: model size       = 3.83\
          \ GiB (4.54 BPW) \nllm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.1\n\
          llm_load_print_meta: BOS token = 1 '<s>'\nllm_load_print_meta: EOS token\
          \ = 2 '</s>'\nllm_load_print_meta: UNK token = 0 '<unk>'\nllm_load_print_meta:\
          \ LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml ctx size =    0.11 MB\n\
          llm_load_tensors: mem required  = 3917.97 MB\n..................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
          \ kv self size  =  512.00 MB\nllama_build_graph: non-view tensors processed:\
          \ 740/740\nggml_metal_init: allocating\nggml_metal_init: found device: Apple\
          \ M1\nggml_metal_init: picking default device: Apple M1\nggml_metal_init:\
          \ default.metallib not found, loading from source\nggml_metal_init: loading\
          \ '/Users/imamcs/Documents/llama.cpp/ggml-metal.metal'\nggml_metal_init:\
          \ GPU name:   Apple M1\nggml_metal_init: GPU family: MTLGPUFamilyApple7\
          \ (1007)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
          \ recommendedMaxWorkingSetSize  =  5461.34 MB\nggml_metal_init: maxTransferRate\
          \               = built-in GPU\nllama_new_context_with_model: compute buffer\
          \ total size = 289.57 MB\nllama_new_context_with_model: max tensor size\
          \ =   102.54 MB\nggml_metal_add_buffer: allocated 'data            ' buffer,\
          \ size =  3918.58 MB, ( 3919.08 /  5461.34)\nggml_metal_add_buffer: allocated\
          \ 'kv              ' buffer, size =   512.02 MB, ( 4431.09 /  5461.34)\n\
          ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   288.02\
          \ MB, ( 4719.11 /  5461.34)\n\nsystem_info: n_threads = 4 / 8 | AVX = 0\
          \ | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0\
          \ | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS\
          \ = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \nsampling: \n\trepeat_last_n =\
          \ 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty\
          \ = 0.000\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
          \ = 1.000, temp = 0.700\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000\ngenerate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n {create code python to count circle area}\n\n```\nimport math\n\
          r = float(input(\"Enter the radius of the circle: \"))\narea = math.pi *\
          \ r ** 2\nprint(\"The area of the circle is:\", area)\n``` [end of text]\n\
          \nllama_print_timings:        load time =    2516.22 ms\nllama_print_timings:\
          \      sample time =      28.71 ms /    50 runs   (    0.57 ms per token,\
          \  1741.86 tokens per second)\nllama_print_timings: prompt eval time = \
          \    262.45 ms /    10 tokens (   26.24 ms per token,    38.10 tokens per\
          \ second)\nllama_print_timings:        eval time =    3836.81 ms /    49\
          \ runs   (   78.30 ms per token,    12.77 tokens per second)\nllama_print_timings:\
          \       total time =    4171.65 ms\nggml_metal_free: deallocating\n\n-------------------------\n\
          \nNow, how to call it or include the prompt script from Flask Framework\
          \ as UI Web App?"
        updatedAt: '2023-11-24T01:35:21.664Z'
      numEdits: 0
      reactions: []
    id: 655ffdd9e23401f820018c2a
    type: comment
  author: imamcs
  content: "Big Thanks to TheBloke, done on MBP M1, RAM 8GB, it's very fast in local,\
    \ amazing :D\n------------------------------\n(base) imamcs@Imams-MacBook-Pro\
    \ llama.cpp % ./main -ngl 32 -m mistral-7b-instruct-v0.1.Q4_0.gguf --color -c\
    \ 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"{create code python to count\
    \ circle area}\"\n\nLog start\nmain: build = 1520 (91f6499)\nmain: built with\
    \ Apple clang version 14.0.3 (clang-1403.0.22.14.1) for arm64-apple-darwin22.5.0\n\
    main: seed  = 1700624603\nllama_model_loader: loaded meta data with 20 key-value\
    \ pairs and 291 tensors from mistral-7b-instruct-v0.1.Q4_0.gguf (version GGUF\
    \ V2)\nllama_model_loader: - tensor    0:                token_embd.weight q4_0\
    \     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:     \
    \         blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    2:              blk.0.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor    4:\
    \         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    5:            blk.0.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor    7:\
    \            blk.0.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   10:\
    \              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   11:              blk.1.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   13:\
    \         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   14:            blk.1.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   16:\
    \            blk.1.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   19:\
    \              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   20:              blk.2.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   22:\
    \         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   23:            blk.2.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   25:\
    \            blk.2.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   28:\
    \              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   29:              blk.3.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   31:\
    \         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   32:            blk.3.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   34:\
    \            blk.3.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   37:\
    \              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   38:              blk.4.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   40:\
    \         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   41:            blk.4.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   43:\
    \            blk.4.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   46:\
    \              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   47:              blk.5.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   49:\
    \         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   50:            blk.5.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   52:\
    \            blk.5.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   55:\
    \              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   56:              blk.6.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   58:\
    \         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   59:            blk.6.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   61:\
    \            blk.6.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   64:\
    \              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   65:              blk.7.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   67:\
    \         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   68:            blk.7.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   70:\
    \            blk.7.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   73:\
    \              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   74:              blk.8.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   76:\
    \         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   77:            blk.8.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   79:\
    \            blk.8.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   82:\
    \              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   83:              blk.9.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   85:\
    \         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   86:            blk.9.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   88:\
    \            blk.9.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   91:\
    \             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   92:             blk.10.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   94:\
    \        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   95:           blk.10.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   97:\
    \           blk.10.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  100:\
    \             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  101:             blk.11.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  103:\
    \        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  104:           blk.11.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  106:\
    \           blk.11.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  109:\
    \             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  110:             blk.12.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  112:\
    \        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  113:           blk.12.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  115:\
    \           blk.12.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  118:\
    \             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  119:             blk.13.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  121:\
    \        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  122:           blk.13.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  124:\
    \           blk.13.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  127:\
    \             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  128:             blk.14.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  130:\
    \        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  131:           blk.14.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  133:\
    \           blk.14.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  136:\
    \             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  137:             blk.15.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  139:\
    \        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  140:           blk.15.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  142:\
    \           blk.15.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  145:\
    \             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  146:             blk.16.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  148:\
    \        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  149:           blk.16.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  151:\
    \           blk.16.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  154:\
    \             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  155:             blk.17.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  157:\
    \        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  158:           blk.17.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  160:\
    \           blk.17.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  163:\
    \             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  164:             blk.18.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  166:\
    \        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  167:           blk.18.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  169:\
    \           blk.18.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  172:\
    \             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  173:             blk.19.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  175:\
    \        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  176:           blk.19.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  178:\
    \           blk.19.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  181:\
    \             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  182:             blk.20.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  184:\
    \        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  185:           blk.20.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  187:\
    \           blk.20.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  190:\
    \             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  191:             blk.21.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  193:\
    \        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  194:           blk.21.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  196:\
    \           blk.21.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  199:\
    \             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  200:             blk.22.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  202:\
    \        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  203:           blk.22.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  205:\
    \           blk.22.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  208:\
    \             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  209:             blk.23.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  211:\
    \        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  212:           blk.23.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  214:\
    \           blk.23.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  217:\
    \             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  218:             blk.24.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  220:\
    \        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  221:           blk.24.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  223:\
    \           blk.24.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  226:\
    \             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  227:             blk.25.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  229:\
    \        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  230:           blk.25.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  232:\
    \           blk.25.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  235:\
    \             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  236:             blk.26.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  238:\
    \        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  239:           blk.26.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  241:\
    \           blk.26.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  244:\
    \             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  245:             blk.27.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  247:\
    \        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  248:           blk.27.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  250:\
    \           blk.27.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  253:\
    \             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  254:             blk.28.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  256:\
    \        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  257:           blk.28.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  259:\
    \           blk.28.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  262:\
    \             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  263:             blk.29.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  265:\
    \        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  266:           blk.29.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  268:\
    \           blk.29.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  271:\
    \             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  272:             blk.30.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  274:\
    \        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  275:           blk.30.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  277:\
    \           blk.30.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  280:\
    \             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  281:             blk.31.attn_k.weight q4_0     [  4096,  1024,   \
    \  1,     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight\
    \ q4_0     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  283:\
    \        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  284:           blk.31.ffn_gate.weight q4_0     [  4096, 14336,   \
    \  1,     1 ]\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight\
    \ q4_0     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  286:\
    \           blk.31.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  289:\
    \               output_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader:\
    \ - tensor  290:                    output.weight q6_K     [  4096, 32000,   \
    \  1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture\
    \ str     \nllama_model_loader: - kv   1:                               general.name\
    \ str     \nllama_model_loader: - kv   2:                       llama.context_length\
    \ u32     \nllama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32     \nllama_model_loader: - kv   4:                          llama.block_count\
    \ u32     \nllama_model_loader: - kv   5:                  llama.feed_forward_length\
    \ u32     \nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
    \ u32     \nllama_model_loader: - kv   7:                 llama.attention.head_count\
    \ u32     \nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32     \nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32     \nllama_model_loader: - kv  10:                       llama.rope.freq_base\
    \ f32     \nllama_model_loader: - kv  11:                          general.file_type\
    \ u32     \nllama_model_loader: - kv  12:                       tokenizer.ggml.model\
    \ str     \nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr     \nllama_model_loader: - kv  14:                      tokenizer.ggml.scores\
    \ arr     \nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type\
    \ arr     \nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id\
    \ u32     \nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id\
    \ u32     \nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id\
    \ u32     \nllama_model_loader: - kv  19:               general.quantization_version\
    \ u32     \nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader:\
    \ - type q4_0:  225 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab:\
    \ special tokens definition check successful ( 259/32000 ).\nllm_load_print_meta:\
    \ format           = GGUF V2\nllm_load_print_meta: arch             = llama\n\
    llm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab    \
    \      = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta:\
    \ n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta:\
    \ n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta:\
    \ n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta:\
    \ n_gqa            = 4\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta:\
    \ f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\n\
    llm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff   \
    \          = 14336\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta:\
    \ freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta:\
    \ n_yarn_orig_ctx  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\n\
    llm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype \
    \     = mostly Q4_0\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta:\
    \ model size       = 3.83 GiB (4.54 BPW) \nllm_load_print_meta: general.name \
    \  = mistralai_mistral-7b-instruct-v0.1\nllm_load_print_meta: BOS token = 1 '<s>'\n\
    llm_load_print_meta: EOS token = 2 '</s>'\nllm_load_print_meta: UNK token = 0\
    \ '<unk>'\nllm_load_print_meta: LF token  = 13 '<0x0A>'\nllm_load_tensors: ggml\
    \ ctx size =    0.11 MB\nllm_load_tensors: mem required  = 3917.97 MB\n..................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 4096\nllama_new_context_with_model:\
    \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
    \ kv self size  =  512.00 MB\nllama_build_graph: non-view tensors processed: 740/740\n\
    ggml_metal_init: allocating\nggml_metal_init: found device: Apple M1\nggml_metal_init:\
    \ picking default device: Apple M1\nggml_metal_init: default.metallib not found,\
    \ loading from source\nggml_metal_init: loading '/Users/imamcs/Documents/llama.cpp/ggml-metal.metal'\n\
    ggml_metal_init: GPU name:   Apple M1\nggml_metal_init: GPU family: MTLGPUFamilyApple7\
    \ (1007)\nggml_metal_init: hasUnifiedMemory              = true\nggml_metal_init:\
    \ recommendedMaxWorkingSetSize  =  5461.34 MB\nggml_metal_init: maxTransferRate\
    \               = built-in GPU\nllama_new_context_with_model: compute buffer total\
    \ size = 289.57 MB\nllama_new_context_with_model: max tensor size =   102.54 MB\n\
    ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3918.58 MB,\
    \ ( 3919.08 /  5461.34)\nggml_metal_add_buffer: allocated 'kv              ' buffer,\
    \ size =   512.02 MB, ( 4431.09 /  5461.34)\nggml_metal_add_buffer: allocated\
    \ 'alloc           ' buffer, size =   288.02 MB, ( 4719.11 /  5461.34)\n\nsystem_info:\
    \ n_threads = 4 / 8 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
    \ = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
    \ = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \nsampling: \n\trepeat_last_n\
    \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty =\
    \ 0.000\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
    \ = 1.000, temp = 0.700\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n\
    generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n {create\
    \ code python to count circle area}\n\n```\nimport math\nr = float(input(\"Enter\
    \ the radius of the circle: \"))\narea = math.pi * r ** 2\nprint(\"The area of\
    \ the circle is:\", area)\n``` [end of text]\n\nllama_print_timings:        load\
    \ time =    2516.22 ms\nllama_print_timings:      sample time =      28.71 ms\
    \ /    50 runs   (    0.57 ms per token,  1741.86 tokens per second)\nllama_print_timings:\
    \ prompt eval time =     262.45 ms /    10 tokens (   26.24 ms per token,    38.10\
    \ tokens per second)\nllama_print_timings:        eval time =    3836.81 ms /\
    \    49 runs   (   78.30 ms per token,    12.77 tokens per second)\nllama_print_timings:\
    \       total time =    4171.65 ms\nggml_metal_free: deallocating\n\n-------------------------\n\
    \nNow, how to call it or include the prompt script from Flask Framework as UI\
    \ Web App?"
  created_at: 2023-11-24 01:35:21+00:00
  edited: false
  hidden: false
  id: 655ffdd9e23401f820018c2a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Llama-2-70B-Chat-GGML
repo_type: model
status: open
target_branch: null
title: Unable to load model in latest llama.cpp build
