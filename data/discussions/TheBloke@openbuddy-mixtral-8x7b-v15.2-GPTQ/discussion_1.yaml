!!python/object:huggingface_hub.community.DiscussionWithDetails
author: akshaaayyy
conflicting_files: null
created_at: 2023-12-20 19:37:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2ec3cd1dc46b9b2975328a024cba705.svg
      fullname: Akshay Kumar Satheesh Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akshaaayyy
      type: user
    createdAt: '2023-12-20T19:37:28.000Z'
    data:
      edited: false
      editors:
      - akshaaayyy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7439944744110107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2ec3cd1dc46b9b2975328a024cba705.svg
          fullname: Akshay Kumar Satheesh Kumar
          isHf: false
          isPro: false
          name: akshaaayyy
          type: user
        html: '<p>Hi,<br>could you help me with the RAM and VRAM requirements to run
          the model branch  : ''gptq-3bit--1g-actorder_True'' </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/655630f00dec9774117e496c/ChWhcECbQfqP52RPxP6It.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/655630f00dec9774117e496c/ChWhcECbQfqP52RPxP6It.png"></a></p>

          <p>I have a RTX 3070 with 7GB usable VRAM and 20GB usable RAM. </p>

          '
        raw: "Hi, \r\ncould you help me with the RAM and VRAM requirements to run\
          \ the model branch  : 'gptq-3bit--1g-actorder_True' \r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/655630f00dec9774117e496c/ChWhcECbQfqP52RPxP6It.png)\r\
          \n\r\n\r\nI have a RTX 3070 with 7GB usable VRAM and 20GB usable RAM. "
        updatedAt: '2023-12-20T19:37:28.339Z'
      numEdits: 0
      reactions: []
    id: 65834278b02f38ef346a1e1a
    type: comment
  author: akshaaayyy
  content: "Hi, \r\ncould you help me with the RAM and VRAM requirements to run the\
    \ model branch  : 'gptq-3bit--1g-actorder_True' \r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/655630f00dec9774117e496c/ChWhcECbQfqP52RPxP6It.png)\r\
    \n\r\n\r\nI have a RTX 3070 with 7GB usable VRAM and 20GB usable RAM. "
  created_at: 2023-12-20 19:37:28+00:00
  edited: false
  hidden: false
  id: 65834278b02f38ef346a1e1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a18e8af96fd468318fa5bb00f6d3b13d.svg
      fullname: kevin dd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kevindd
      type: user
    createdAt: '2023-12-20T20:01:06.000Z'
    data:
      edited: false
      editors:
      - Kevindd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9204694032669067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a18e8af96fd468318fa5bb00f6d3b13d.svg
          fullname: kevin dd
          isHf: false
          isPro: false
          name: Kevindd
          type: user
        html: '<p>Unless you want to wait for a very long time for the AI to answer
          I would recommend you to rent a powerful GPU instead.    Vast.ai  is the
          cheapest I found so far. Go for 2X RTX 3090 for a total of 48 gig. It will
          cost around 50-60 cents per hour. (Price fluctuate wildly...)</p>

          '
        raw: Unless you want to wait for a very long time for the AI to answer I would
          recommend you to rent a powerful GPU instead.    Vast.ai  is the cheapest
          I found so far. Go for 2X RTX 3090 for a total of 48 gig. It will cost around
          50-60 cents per hour. (Price fluctuate wildly...)
        updatedAt: '2023-12-20T20:01:06.137Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - akshaaayyy
    id: 6583480264188479683f8fc9
    type: comment
  author: Kevindd
  content: Unless you want to wait for a very long time for the AI to answer I would
    recommend you to rent a powerful GPU instead.    Vast.ai  is the cheapest I found
    so far. Go for 2X RTX 3090 for a total of 48 gig. It will cost around 50-60 cents
    per hour. (Price fluctuate wildly...)
  created_at: 2023-12-20 20:01:06+00:00
  edited: false
  hidden: false
  id: 6583480264188479683f8fc9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/openbuddy-mixtral-8x7b-v15.2-GPTQ
repo_type: model
status: open
target_branch: null
title: Help with RAM and VRAM requirements
