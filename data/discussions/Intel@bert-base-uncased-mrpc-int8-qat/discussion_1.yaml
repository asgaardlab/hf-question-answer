!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Amanda95
conflicting_files: null
created_at: 2022-10-25 15:42:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf891cdcc8579d9c11d9e8bd4f3b5c3.svg
      fullname: Shi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Amanda95
      type: user
    createdAt: '2022-10-25T16:42:04.000Z'
    data:
      edited: false
      editors:
      - Amanda95
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf891cdcc8579d9c11d9e8bd4f3b5c3.svg
          fullname: Shi
          isHf: false
          isPro: false
          name: Amanda95
          type: user
        html: '<p>Hi,<br>I am trying to evaluate the quantized int8 model on the MRPC
          task but failed to reproduce the reported f1 score.  The following code
          is how I manage to tokenize/load model/evaluate model:</p>

          <p> self.tokenizer =  AutoTokenizer.from_pretrained("Intel/bert-base-uncased-mrpc-int8-qat")<br>int8_model
          = AutoModelForSequenceClassification.from_pretrained("Intel/bert-base-uncased-mrpc-int8-qat")<br>logits=int8_model(input_ids=batch_data["input_ids"],
          token_type_ids=batch_data["token_type_ids"],attention_mask=batch_data["attention_mask"]).logits</p>

          <p>Could you help to check if there is something wrong? And the weirdest
          thing is the f1 score is zero always with eval() on, however f1 score becomes
          0.81 when eval() is off. </p>

          '
        raw: "Hi,\r\nI am trying to evaluate the quantized int8 model on the MRPC\
          \ task but failed to reproduce the reported f1 score.  The following code\
          \ is how I manage to tokenize/load model/evaluate model:\r\n\r\n self.tokenizer\
          \ =  AutoTokenizer.from_pretrained(\"Intel/bert-base-uncased-mrpc-int8-qat\"\
          )\r\nint8_model = AutoModelForSequenceClassification.from_pretrained(\"\
          Intel/bert-base-uncased-mrpc-int8-qat\")\r\nlogits=int8_model(input_ids=batch_data[\"\
          input_ids\"], token_type_ids=batch_data[\"token_type_ids\"],attention_mask=batch_data[\"\
          attention_mask\"]).logits\r\n\r\nCould you help to check if there is something\
          \ wrong? And the weirdest thing is the f1 score is zero always with eval()\
          \ on, however f1 score becomes 0.81 when eval() is off. "
        updatedAt: '2022-10-25T16:42:04.287Z'
      numEdits: 0
      reactions: []
    id: 635811dc9734bea912ca53f7
    type: comment
  author: Amanda95
  content: "Hi,\r\nI am trying to evaluate the quantized int8 model on the MRPC task\
    \ but failed to reproduce the reported f1 score.  The following code is how I\
    \ manage to tokenize/load model/evaluate model:\r\n\r\n self.tokenizer =  AutoTokenizer.from_pretrained(\"\
    Intel/bert-base-uncased-mrpc-int8-qat\")\r\nint8_model = AutoModelForSequenceClassification.from_pretrained(\"\
    Intel/bert-base-uncased-mrpc-int8-qat\")\r\nlogits=int8_model(input_ids=batch_data[\"\
    input_ids\"], token_type_ids=batch_data[\"token_type_ids\"],attention_mask=batch_data[\"\
    attention_mask\"]).logits\r\n\r\nCould you help to check if there is something\
    \ wrong? And the weirdest thing is the f1 score is zero always with eval() on,\
    \ however f1 score becomes 0.81 when eval() is off. "
  created_at: 2022-10-25 15:42:04+00:00
  edited: false
  hidden: false
  id: 635811dc9734bea912ca53f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5849f8d98755070cbee148a344c899ca.svg
      fullname: He, Xin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: xinhe
      type: user
    createdAt: '2022-10-26T01:12:52.000Z'
    data:
      edited: false
      editors:
      - xinhe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5849f8d98755070cbee148a344c899ca.svg
          fullname: He, Xin
          isHf: false
          isPro: false
          name: xinhe
          type: user
        html: "<p>Please follow model card to load the int8 model.</p>\n<pre><code>from\
          \ neural_compressor.utils.load_huggingface import OptimizedModel\nint8_model\
          \ = OptimizedModel.from_pretrained(\n    'Intel/bert-base-uncased-mrpc-int8-qat',\n\
          )\n</code></pre>\n<p>or </p>\n<pre><code>from optimum.intel.neural_compressor.quantization\
          \ import IncQuantizedModelForSequenceClassification\nint8_model = IncQuantizedModelForSequenceClassification.from_pretrained(\n\
          \    'Intel/bert-base-uncased-mrpc-int8-qat',\n)\n</code></pre>\n"
        raw: "Please follow model card to load the int8 model.\n```\nfrom neural_compressor.utils.load_huggingface\
          \ import OptimizedModel\nint8_model = OptimizedModel.from_pretrained(\n\
          \    'Intel/bert-base-uncased-mrpc-int8-qat',\n)\n```\nor \n```\nfrom optimum.intel.neural_compressor.quantization\
          \ import IncQuantizedModelForSequenceClassification\nint8_model = IncQuantizedModelForSequenceClassification.from_pretrained(\n\
          \    'Intel/bert-base-uncased-mrpc-int8-qat',\n)\n```"
        updatedAt: '2022-10-26T01:12:52.353Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hengyu
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hengyu
    id: 635889949734bea912cef768
    type: comment
  author: xinhe
  content: "Please follow model card to load the int8 model.\n```\nfrom neural_compressor.utils.load_huggingface\
    \ import OptimizedModel\nint8_model = OptimizedModel.from_pretrained(\n    'Intel/bert-base-uncased-mrpc-int8-qat',\n\
    )\n```\nor \n```\nfrom optimum.intel.neural_compressor.quantization import IncQuantizedModelForSequenceClassification\n\
    int8_model = IncQuantizedModelForSequenceClassification.from_pretrained(\n   \
    \ 'Intel/bert-base-uncased-mrpc-int8-qat',\n)\n```"
  created_at: 2022-10-26 00:12:52+00:00
  edited: false
  hidden: false
  id: 635889949734bea912cef768
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0bf891cdcc8579d9c11d9e8bd4f3b5c3.svg
      fullname: Shi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Amanda95
      type: user
    createdAt: '2022-10-26T11:03:14.000Z'
    data:
      edited: true
      editors:
      - Amanda95
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0bf891cdcc8579d9c11d9e8bd4f3b5c3.svg
          fullname: Shi
          isHf: false
          isPro: false
          name: Amanda95
          type: user
        html: "<p>Thank you for your reply. I used the way you suggested at the very\
          \ beginning but still could not reproduce the reported f1 score (the f1\
          \ score I got is 0.51, reported one is 0.91). I am attaching all the code\
          \ that I manage to evaluate the model. Could you help to see if there are\
          \ something wrong?:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">import</span>\
          \ json\n<span class=\"hljs-keyword\">import</span> random\n<span class=\"\
          hljs-keyword\">import</span> numpy <span class=\"hljs-keyword\">as</span>\
          \ np\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\"\
          >as</span> pd\n<span class=\"hljs-keyword\">import</span> os\n<span class=\"\
          hljs-keyword\">import</span> re\n\n<span class=\"hljs-keyword\">from</span>\
          \ tqdm <span class=\"hljs-keyword\">import</span> tqdm\n<span class=\"hljs-keyword\"\
          >from</span> torch.utils <span class=\"hljs-keyword\">import</span> data\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> BertTokenizer \n<span class=\"hljs-keyword\">from</span>\
          \ neural_compressor.utils.load_huggingface <span class=\"hljs-keyword\"\
          >import</span> OptimizedModel\n<span class=\"hljs-keyword\">from</span>\
          \ sklearn.metrics <span class=\"hljs-keyword\">import</span> f1_score\n\
          <span class=\"hljs-keyword\">from</span> ipdb <span class=\"hljs-keyword\"\
          >import</span> set_trace\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, AutoModelForSequenceClassification\n\
          <span class=\"hljs-keyword\">from</span> optimum.intel.neural_compressor.quantization\
          \ <span class=\"hljs-keyword\">import</span> IncQuantizedModelForSequenceClassification\n\
          \n<span class=\"hljs-keyword\">class</span> <span class=\"hljs-title class_\"\
          >MRPCset</span>(data.Dataset):\n   <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">__init__</span>(<span class=\"hljs-params\"\
          >self,path1, path2</span>):\n       self.convert_tsv_to_json(path1,path2)\n\
          \       self.read_json_data(path2) \n       self.tokenizer =  AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"Intel/bert-base-uncased-mrpc-int8-qat\"</span>)\n\
          \   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >read_data</span>(<span class=\"hljs-params\">self, path</span>):\n    \
          \   self.data = pd.read_csv(path, sep=<span class=\"hljs-string\">'delimiter'</span>,\
          \ header=<span class=\"hljs-literal\">None</span>)\n       \n   <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">read_json_data</span>(<span\
          \ class=\"hljs-params\">self, path</span>):\n       <span class=\"hljs-keyword\"\
          >with</span> <span class=\"hljs-built_in\">open</span>(path) <span class=\"\
          hljs-keyword\">as</span> f:\n           self.data = json.load(f)[<span class=\"\
          hljs-string\">\"0\"</span>] \n       self.data_dict = []   \n       <span\
          \ class=\"hljs-keyword\">for</span> key, value <span class=\"hljs-keyword\"\
          >in</span> self.data.items():\n           <span class=\"hljs-keyword\">if</span>\
          \ key == <span class=\"hljs-string\">'0'</span>:\n               <span class=\"\
          hljs-keyword\">continue</span>\n           label, id_1,id_2, sent_1, sent_2\
          \ = re.split(<span class=\"hljs-string\">r\"\\t+\"</span>,value)\n     \
          \      self.data_dict.append({\n           <span class=\"hljs-string\">\"\
          label\"</span>: label,\n           <span class=\"hljs-string\">\"id_1\"\
          </span>: id_1,\n           <span class=\"hljs-string\">\"id_2\"</span>:\
          \ id_2,\n           <span class=\"hljs-string\">\"sent_1\"</span>: sent_1,\n\
          \           <span class=\"hljs-string\">\"sent_2\"</span>: sent_2,\n   \
          \        })\n   \n   <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">convert_tsv_to_json</span>(<span class=\"hljs-params\"\
          >self,tsv_path,json_path</span>):\n       <span class=\"hljs-keyword\">if</span>\
          \ os.path.exists(json_path):\n           <span class=\"hljs-keyword\">return</span>\n\
          \       data = pd.read_csv(path, sep=<span class=\"hljs-string\">'delimiter'</span>,\
          \ header=<span class=\"hljs-literal\">None</span>)\n       data.to_json(json_path)\n\
          \       \n   <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">preprocess_text</span>(<span class=\"hljs-params\">self,sent1,\
          \ sent2</span>):\n       <span class=\"hljs-keyword\">return</span> self.tokenizer(sent1,sent2,return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\n       \n   <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">__len__</span>(<span class=\"\
          hljs-params\">self</span>):\n       <span class=\"hljs-keyword\">return</span>\
          \ <span class=\"hljs-built_in\">len</span>(self.data_dict)\n   \n   <span\
          \ class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >__getitem__</span>(<span class=\"hljs-params\">self, index</span>):\n \
          \      current_data = self.data_dict[index]\n       data_dict = self.preprocess_text(current_data[<span\
          \ class=\"hljs-string\">\"sent_1\"</span>],current_data[<span class=\"hljs-string\"\
          >\"sent_2\"</span>])\n       label = torch.tensor(<span class=\"hljs-built_in\"\
          >int</span>(current_data[<span class=\"hljs-string\">\"label\"</span>]))\n\
          \n       batch = {<span class=\"hljs-string\">\"input_ids\"</span>: data_dict[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>].squeeze(<span class=\"hljs-number\"\
          >0</span>),\n                <span class=\"hljs-string\">\"token_type_ids\"\
          </span>: data_dict[<span class=\"hljs-string\">\"token_type_ids\"</span>].squeeze(<span\
          \ class=\"hljs-number\">0</span>),\n                <span class=\"hljs-string\"\
          >\"attention_mask\"</span>: data_dict[<span class=\"hljs-string\">\"attention_mask\"\
          </span>].squeeze(<span class=\"hljs-number\">0</span>),\n              \
          \  <span class=\"hljs-string\">\"label\"</span>:label}\n       <span class=\"\
          hljs-keyword\">return</span> batch   \n           \n<span class=\"hljs-keyword\"\
          >if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n\
          \   batch_size = <span class=\"hljs-number\">1</span>\n   <span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">_main</span>():\n\
          \       path_root = <span class=\"hljs-string\">'./glue_data/MRPC/'</span>\n\
          \       json_path = path_root+ <span class=\"hljs-string\">'dev_data.json'</span>\n\
          \       tsv_path = path_root + <span class=\"hljs-string\">'dev.tsv'</span>\n\
          \       test = MRPCset(tsv_path, json_path)\n       \n       data_loader\
          \ = data.DataLoader(\n       test,\n       batch_size = batch_size,\n  \
          \     shuffle=<span class=\"hljs-literal\">False</span>,\n       num_workers=<span\
          \ class=\"hljs-number\">0</span>,\n       drop_last=<span class=\"hljs-literal\"\
          >False</span>,\n       pin_memory=<span class=\"hljs-literal\">False</span>,\n\
          \       )\n   \n       <span class=\"hljs-comment\">#fp_model = OptimizedModel.from_pretrained('Intel/bert-base-uncased-mrpc',)\
          \ # can reproduce the reported f1 score of floating point mode</span>\n\
          \       int8_model = OptimizedModel.from_pretrained(<span class=\"hljs-string\"\
          >'Intel/bert-base-uncased-mrpc-int8-qat'</span>,)\n       int8_model.<span\
          \ class=\"hljs-built_in\">eval</span>()\n       \n       total_labels =\
          \ []\n       pred_labels = []\n       <span class=\"hljs-keyword\">for</span>\
          \ i, batch_data <span class=\"hljs-keyword\">in</span> tqdm(<span class=\"\
          hljs-built_in\">enumerate</span>(data_loader)):\n           <span class=\"\
          hljs-built_in\">print</span>(i)\n           label = batch_data.pop(<span\
          \ class=\"hljs-string\">\"label\"</span>)\n           logits=int8_model(input_ids=batch_data[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>], token_type_ids=batch_data[<span\
          \ class=\"hljs-string\">\"token_type_ids\"</span>],attention_mask=batch_data[<span\
          \ class=\"hljs-string\">\"attention_mask\"</span>]).logits\n           pred_label\
          \ = torch.<span class=\"hljs-built_in\">max</span>(logits,dim=<span class=\"\
          hljs-number\">1</span>).indices\n           total_labels.append(label)\n\
          \           pred_labels.append(pred_label)\n       \n       total_gt_labels=\
          \ np.array(torch.cat(total_labels))\n       total_pred_labels = np.array(torch.cat(pred_labels))\n\
          \       <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"f1 score\"</span>, f1_score(total_gt_labels, total_pred_labels))\n  \
          \     set_trace()\n       <span class=\"hljs-built_in\">print</span>(test.data)\n\
          \       \n   _main()  \n</code></pre>\n"
        raw: "Thank you for your reply. I used the way you suggested at the very beginning\
          \ but still could not reproduce the reported f1 score (the f1 score I got\
          \ is 0.51, reported one is 0.91). I am attaching all the code that I manage\
          \ to evaluate the model. Could you help to see if there are something wrong?:\n\
          \n ```python\nimport torch\nimport json\nimport random\nimport numpy as\
          \ np\nimport pandas as pd\nimport os\nimport re\n\nfrom tqdm import tqdm\n\
          from torch.utils import data\nfrom transformers import BertTokenizer \n\
          from neural_compressor.utils.load_huggingface import OptimizedModel\nfrom\
          \ sklearn.metrics import f1_score\nfrom ipdb import set_trace\nfrom transformers\
          \ import AutoTokenizer, AutoModelForSequenceClassification\nfrom optimum.intel.neural_compressor.quantization\
          \ import IncQuantizedModelForSequenceClassification\n\nclass MRPCset(data.Dataset):\n\
          \    def __init__(self,path1, path2):\n        self.convert_tsv_to_json(path1,path2)\n\
          \        self.read_json_data(path2) \n        self.tokenizer =  AutoTokenizer.from_pretrained(\"\
          Intel/bert-base-uncased-mrpc-int8-qat\")\n    def read_data(self, path):\n\
          \        self.data = pd.read_csv(path, sep='delimiter', header=None)\n \
          \       \n    def read_json_data(self, path):\n        with open(path) as\
          \ f:\n            self.data = json.load(f)[\"0\"] \n        self.data_dict\
          \ = []   \n        for key, value in self.data.items():\n            if\
          \ key == '0':\n                continue\n            label, id_1,id_2, sent_1,\
          \ sent_2 = re.split(r\"\\t+\",value)\n            self.data_dict.append({\n\
          \            \"label\": label,\n            \"id_1\": id_1,\n          \
          \  \"id_2\": id_2,\n            \"sent_1\": sent_1,\n            \"sent_2\"\
          : sent_2,\n            })\n    \n    def convert_tsv_to_json(self,tsv_path,json_path):\n\
          \        if os.path.exists(json_path):\n            return\n        data\
          \ = pd.read_csv(path, sep='delimiter', header=None)\n        data.to_json(json_path)\n\
          \        \n    def preprocess_text(self,sent1, sent2):\n        return self.tokenizer(sent1,sent2,return_tensors=\"\
          pt\")\n        \n    def __len__(self):\n        return len(self.data_dict)\n\
          \    \n    def __getitem__(self, index):\n        current_data = self.data_dict[index]\n\
          \        data_dict = self.preprocess_text(current_data[\"sent_1\"],current_data[\"\
          sent_2\"])\n        label = torch.tensor(int(current_data[\"label\"]))\n\
          \n        batch = {\"input_ids\": data_dict[\"input_ids\"].squeeze(0),\n\
          \                 \"token_type_ids\": data_dict[\"token_type_ids\"].squeeze(0),\n\
          \                 \"attention_mask\": data_dict[\"attention_mask\"].squeeze(0),\n\
          \                 \"label\":label}\n        return batch   \n          \
          \  \nif __name__ == \"__main__\":\n    batch_size = 1\n    def _main():\n\
          \        path_root = './glue_data/MRPC/'\n        json_path = path_root+\
          \ 'dev_data.json'\n        tsv_path = path_root + 'dev.tsv'\n        test\
          \ = MRPCset(tsv_path, json_path)\n        \n        data_loader = data.DataLoader(\n\
          \        test,\n        batch_size = batch_size,\n        shuffle=False,\n\
          \        num_workers=0,\n        drop_last=False,\n        pin_memory=False,\n\
          \        )\n    \n        #fp_model = OptimizedModel.from_pretrained('Intel/bert-base-uncased-mrpc',)\
          \ # can reproduce the reported f1 score of floating point mode\n       \
          \ int8_model = OptimizedModel.from_pretrained('Intel/bert-base-uncased-mrpc-int8-qat',)\n\
          \        int8_model.eval()\n        \n        total_labels = []\n      \
          \  pred_labels = []\n        for i, batch_data in tqdm(enumerate(data_loader)):\n\
          \            print(i)\n            label = batch_data.pop(\"label\")\n \
          \           logits=int8_model(input_ids=batch_data[\"input_ids\"], token_type_ids=batch_data[\"\
          token_type_ids\"],attention_mask=batch_data[\"attention_mask\"]).logits\n\
          \            pred_label = torch.max(logits,dim=1).indices\n            total_labels.append(label)\n\
          \            pred_labels.append(pred_label)\n        \n        total_gt_labels=\
          \ np.array(torch.cat(total_labels))\n        total_pred_labels = np.array(torch.cat(pred_labels))\n\
          \        print(\"f1 score\", f1_score(total_gt_labels, total_pred_labels))\n\
          \        set_trace()\n        print(test.data)\n        \n    _main()  \n\
          ```"
        updatedAt: '2022-10-26T11:57:55.273Z'
      numEdits: 2
      reactions: []
    id: 635913f28502aa9594f76d4a
    type: comment
  author: Amanda95
  content: "Thank you for your reply. I used the way you suggested at the very beginning\
    \ but still could not reproduce the reported f1 score (the f1 score I got is 0.51,\
    \ reported one is 0.91). I am attaching all the code that I manage to evaluate\
    \ the model. Could you help to see if there are something wrong?:\n\n ```python\n\
    import torch\nimport json\nimport random\nimport numpy as np\nimport pandas as\
    \ pd\nimport os\nimport re\n\nfrom tqdm import tqdm\nfrom torch.utils import data\n\
    from transformers import BertTokenizer \nfrom neural_compressor.utils.load_huggingface\
    \ import OptimizedModel\nfrom sklearn.metrics import f1_score\nfrom ipdb import\
    \ set_trace\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\
    from optimum.intel.neural_compressor.quantization import IncQuantizedModelForSequenceClassification\n\
    \nclass MRPCset(data.Dataset):\n    def __init__(self,path1, path2):\n       \
    \ self.convert_tsv_to_json(path1,path2)\n        self.read_json_data(path2) \n\
    \        self.tokenizer =  AutoTokenizer.from_pretrained(\"Intel/bert-base-uncased-mrpc-int8-qat\"\
    )\n    def read_data(self, path):\n        self.data = pd.read_csv(path, sep='delimiter',\
    \ header=None)\n        \n    def read_json_data(self, path):\n        with open(path)\
    \ as f:\n            self.data = json.load(f)[\"0\"] \n        self.data_dict\
    \ = []   \n        for key, value in self.data.items():\n            if key ==\
    \ '0':\n                continue\n            label, id_1,id_2, sent_1, sent_2\
    \ = re.split(r\"\\t+\",value)\n            self.data_dict.append({\n         \
    \   \"label\": label,\n            \"id_1\": id_1,\n            \"id_2\": id_2,\n\
    \            \"sent_1\": sent_1,\n            \"sent_2\": sent_2,\n          \
    \  })\n    \n    def convert_tsv_to_json(self,tsv_path,json_path):\n        if\
    \ os.path.exists(json_path):\n            return\n        data = pd.read_csv(path,\
    \ sep='delimiter', header=None)\n        data.to_json(json_path)\n        \n \
    \   def preprocess_text(self,sent1, sent2):\n        return self.tokenizer(sent1,sent2,return_tensors=\"\
    pt\")\n        \n    def __len__(self):\n        return len(self.data_dict)\n\
    \    \n    def __getitem__(self, index):\n        current_data = self.data_dict[index]\n\
    \        data_dict = self.preprocess_text(current_data[\"sent_1\"],current_data[\"\
    sent_2\"])\n        label = torch.tensor(int(current_data[\"label\"]))\n\n   \
    \     batch = {\"input_ids\": data_dict[\"input_ids\"].squeeze(0),\n         \
    \        \"token_type_ids\": data_dict[\"token_type_ids\"].squeeze(0),\n     \
    \            \"attention_mask\": data_dict[\"attention_mask\"].squeeze(0),\n \
    \                \"label\":label}\n        return batch   \n            \nif __name__\
    \ == \"__main__\":\n    batch_size = 1\n    def _main():\n        path_root =\
    \ './glue_data/MRPC/'\n        json_path = path_root+ 'dev_data.json'\n      \
    \  tsv_path = path_root + 'dev.tsv'\n        test = MRPCset(tsv_path, json_path)\n\
    \        \n        data_loader = data.DataLoader(\n        test,\n        batch_size\
    \ = batch_size,\n        shuffle=False,\n        num_workers=0,\n        drop_last=False,\n\
    \        pin_memory=False,\n        )\n    \n        #fp_model = OptimizedModel.from_pretrained('Intel/bert-base-uncased-mrpc',)\
    \ # can reproduce the reported f1 score of floating point mode\n        int8_model\
    \ = OptimizedModel.from_pretrained('Intel/bert-base-uncased-mrpc-int8-qat',)\n\
    \        int8_model.eval()\n        \n        total_labels = []\n        pred_labels\
    \ = []\n        for i, batch_data in tqdm(enumerate(data_loader)):\n         \
    \   print(i)\n            label = batch_data.pop(\"label\")\n            logits=int8_model(input_ids=batch_data[\"\
    input_ids\"], token_type_ids=batch_data[\"token_type_ids\"],attention_mask=batch_data[\"\
    attention_mask\"]).logits\n            pred_label = torch.max(logits,dim=1).indices\n\
    \            total_labels.append(label)\n            pred_labels.append(pred_label)\n\
    \        \n        total_gt_labels= np.array(torch.cat(total_labels))\n      \
    \  total_pred_labels = np.array(torch.cat(pred_labels))\n        print(\"f1 score\"\
    , f1_score(total_gt_labels, total_pred_labels))\n        set_trace()\n       \
    \ print(test.data)\n        \n    _main()  \n```"
  created_at: 2022-10-26 10:03:14+00:00
  edited: true
  hidden: false
  id: 635913f28502aa9594f76d4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5849f8d98755070cbee148a344c899ca.svg
      fullname: He, Xin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: xinhe
      type: user
    createdAt: '2022-10-28T05:38:48.000Z'
    data:
      edited: false
      editors:
      - xinhe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5849f8d98755070cbee148a344c899ca.svg
          fullname: He, Xin
          isHf: false
          isPro: false
          name: xinhe
          type: user
        html: '<p>Hi Amanda95, please check whether your machine support INT8 ISA,
          for example, AVX512_VNNI.<br>I run your code on cascade lake and got F1=0.91068.
          you can try script in <a rel="nofollow" href="https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor/text-classification">huggingface/optimum-intel</a>
          to avoid mistakes.</p>

          '
        raw: 'Hi Amanda95, please check whether your machine support INT8 ISA, for
          example, AVX512_VNNI.

          I run your code on cascade lake and got F1=0.91068. you can try script in
          [huggingface/optimum-intel](https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor/text-classification)
          to avoid mistakes.'
        updatedAt: '2022-10-28T05:38:48.888Z'
      numEdits: 0
      reactions: []
    id: 635b6ae851744cd564662d32
    type: comment
  author: xinhe
  content: 'Hi Amanda95, please check whether your machine support INT8 ISA, for example,
    AVX512_VNNI.

    I run your code on cascade lake and got F1=0.91068. you can try script in [huggingface/optimum-intel](https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor/text-classification)
    to avoid mistakes.'
  created_at: 2022-10-28 04:38:48+00:00
  edited: false
  hidden: false
  id: 635b6ae851744cd564662d32
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Intel/bert-base-uncased-mrpc-int8-qat
repo_type: model
status: open
target_branch: null
title: 'Quantized model inference '
