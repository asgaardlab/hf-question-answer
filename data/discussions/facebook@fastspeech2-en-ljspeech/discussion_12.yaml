!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nunyabees
conflicting_files: null
created_at: 2023-02-03 01:59:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3c258c5f29bba5a5ea1c5744a30ccbc8.svg
      fullname: nunya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nunyabees
      type: user
    createdAt: '2023-02-03T01:59:13.000Z'
    data:
      edited: false
      editors:
      - nunyabees
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3c258c5f29bba5a5ea1c5744a30ccbc8.svg
          fullname: nunya
          isHf: false
          isPro: false
          name: nunyabees
          type: user
        html: "<p>RuntimeError: Expected all tensors to be on the same device, but\
          \ found at least two devices, cpu and cuda:0! (when checking argument for\
          \ argument weight in method wrapper___slow_conv2d_forward)</p>\n<pre><code>\
          \ 11 generator = task.build_generator(models, cfg)\n 12 sample = TTSHubInterface.get_model_input(task,\
          \ \"testing one two three\")\n</code></pre>\n<p>---&gt; 13 wav, rate = TTSHubInterface.get_prediction(task,\
          \ model, generator, sample)<br>     14<br>     15 ipd.Audio(wav, rate=rate)</p>\n\
          <p>\\lib\\site-packages\\fairseq\\models\\text_to_speech\\hub_interface.py\
          \ in get_prediction(cls, task, model, generator, sample)<br>    130    \
          \ @classmethod<br>    131     def get_prediction(cls, task, model, generator,\
          \ sample) -&gt; Tuple[torch.Tensor, int]:<br>--&gt; 132         prediction\
          \ = generator.generate(model, sample)<br>    133         return prediction[0][\"\
          waveform\"], task.sr<br>    134 </p>\n<p>\\lib\\site-packages\\torch\\autograd\\\
          grad_mode.py in decorate_context(*args, **kwargs)<br>     25         def\
          \ decorate_context(*args, **kwargs):<br>     26             with self.clone():<br>---&gt;\
          \ 27                 return func(*args, **kwargs)<br>     28         return\
          \ cast(F, decorate_context)<br>     29 </p>\n<p>\\lib\\site-packages\\fairseq\\\
          speech_generator.py in generate(self, model, sample, has_targ, **kwargs)<br>\
          \    160<br>...<br>--&gt; 309         return F.conv1d(input, weight, bias,\
          \ self.stride,<br>    310                         self.padding, self.dilation,\
          \ self.groups)<br>    311 </p>\n"
        raw: "RuntimeError: Expected all tensors to be on the same device, but found\
          \ at least two devices, cpu and cuda:0! (when checking argument for argument\
          \ weight in method wrapper___slow_conv2d_forward)\r\n\r\n     11 generator\
          \ = task.build_generator(models, cfg)\r\n     12 sample = TTSHubInterface.get_model_input(task,\
          \ \"testing one two three\")\r\n---> 13 wav, rate = TTSHubInterface.get_prediction(task,\
          \ model, generator, sample)\r\n     14 \r\n     15 ipd.Audio(wav, rate=rate)\r\
          \n\r\n\\lib\\site-packages\\fairseq\\models\\text_to_speech\\hub_interface.py\
          \ in get_prediction(cls, task, model, generator, sample)\r\n    130    \
          \ @classmethod\r\n    131     def get_prediction(cls, task, model, generator,\
          \ sample) -> Tuple[torch.Tensor, int]:\r\n--> 132         prediction = generator.generate(model,\
          \ sample)\r\n    133         return prediction[0][\"waveform\"], task.sr\r\
          \n    134 \r\n\r\n\\lib\\site-packages\\torch\\autograd\\grad_mode.py in\
          \ decorate_context(*args, **kwargs)\r\n     25         def decorate_context(*args,\
          \ **kwargs):\r\n     26             with self.clone():\r\n---> 27      \
          \           return func(*args, **kwargs)\r\n     28         return cast(F,\
          \ decorate_context)\r\n     29 \r\n\r\n\\lib\\site-packages\\fairseq\\speech_generator.py\
          \ in generate(self, model, sample, has_targ, **kwargs)\r\n    160 \r\n...\r\
          \n--> 309         return F.conv1d(input, weight, bias, self.stride,\r\n\
          \    310                         self.padding, self.dilation, self.groups)\r\
          \n    311 "
        updatedAt: '2023-02-03T01:59:13.204Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - SunshineOMM
        - nunyabees
        - liranux
    id: 63dc6a71a99b2c8a7c210038
    type: comment
  author: nunyabees
  content: "RuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cpu and cuda:0! (when checking argument for argument weight\
    \ in method wrapper___slow_conv2d_forward)\r\n\r\n     11 generator = task.build_generator(models,\
    \ cfg)\r\n     12 sample = TTSHubInterface.get_model_input(task, \"testing one\
    \ two three\")\r\n---> 13 wav, rate = TTSHubInterface.get_prediction(task, model,\
    \ generator, sample)\r\n     14 \r\n     15 ipd.Audio(wav, rate=rate)\r\n\r\n\\\
    lib\\site-packages\\fairseq\\models\\text_to_speech\\hub_interface.py in get_prediction(cls,\
    \ task, model, generator, sample)\r\n    130     @classmethod\r\n    131     def\
    \ get_prediction(cls, task, model, generator, sample) -> Tuple[torch.Tensor, int]:\r\
    \n--> 132         prediction = generator.generate(model, sample)\r\n    133  \
    \       return prediction[0][\"waveform\"], task.sr\r\n    134 \r\n\r\n\\lib\\\
    site-packages\\torch\\autograd\\grad_mode.py in decorate_context(*args, **kwargs)\r\
    \n     25         def decorate_context(*args, **kwargs):\r\n     26          \
    \   with self.clone():\r\n---> 27                 return func(*args, **kwargs)\r\
    \n     28         return cast(F, decorate_context)\r\n     29 \r\n\r\n\\lib\\\
    site-packages\\fairseq\\speech_generator.py in generate(self, model, sample, has_targ,\
    \ **kwargs)\r\n    160 \r\n...\r\n--> 309         return F.conv1d(input, weight,\
    \ bias, self.stride,\r\n    310                         self.padding, self.dilation,\
    \ self.groups)\r\n    311 "
  created_at: 2023-02-03 01:59:13+00:00
  edited: false
  hidden: false
  id: 63dc6a71a99b2c8a7c210038
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
      fullname: Dave Young
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dyoung
      type: user
    createdAt: '2023-03-20T08:04:12.000Z'
    data:
      edited: false
      editors:
      - dyoung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679299766133-63e7f060db40d9e67ff2a2ba.jpeg?w=200&h=200&f=face
          fullname: Dave Young
          isHf: false
          isPro: false
          name: dyoung
          type: user
        html: '<p>I''m getting a feel for this myself.  But something that worked
          for me was to use the CPU. This model seems to be small from what I see
          from the files on hugging face. So it seems reasonable to try on a CPU when
          getting a feel for using it.</p>

          <p>My quick temp solution:<br>   line:              ''arg_overrides={"vocoder":
          "hifigan", "fp16": False''<br>   change to: ''arg_overrides={"vocoder":
          "hifigan", "fp16": False, "cpu": True}''</p>

          <p>The line relates to setting up and configuring the model for later inference/generation.<br>I
          found this from the documentation at: <a rel="nofollow" href="https://fairseq.readthedocs.io/en/latest/">https://fairseq.readthedocs.io/en/latest/</a><br>I
          noticed that the ''arg_overrides'' variable dictionary was similar to what
          was being used for the fairseq command lines arguments from running fairseq
          generations from shell/command line.<br>   (see: <a rel="nofollow" href="https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-generate">https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-generate</a>
          for a bit more specifics.)</p>

          <p>This is likely the best I can help with this at the moment.</p>

          '
        raw: "I'm getting a feel for this myself.  But something that worked for me\
          \ was to use the CPU. This model seems to be small from what I see from\
          \ the files on hugging face. So it seems reasonable to try on a CPU when\
          \ getting a feel for using it.\n\nMy quick temp solution: \n   line:   \
          \           'arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False'\n\
          \   change to: 'arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False,\
          \ \"cpu\": True}'\n\nThe line relates to setting up and configuring the\
          \ model for later inference/generation.\nI found this from the documentation\
          \ at: https://fairseq.readthedocs.io/en/latest/\nI noticed that the 'arg_overrides'\
          \ variable dictionary was similar to what was being used for the fairseq\
          \ command lines arguments from running fairseq generations from shell/command\
          \ line.\n   (see: https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-generate\
          \ for a bit more specifics.)\n\nThis is likely the best I can help with\
          \ this at the moment."
        updatedAt: '2023-03-20T08:04:12.130Z'
      numEdits: 0
      reactions: []
    id: 6418137cc40218f71dca2531
    type: comment
  author: dyoung
  content: "I'm getting a feel for this myself.  But something that worked for me\
    \ was to use the CPU. This model seems to be small from what I see from the files\
    \ on hugging face. So it seems reasonable to try on a CPU when getting a feel\
    \ for using it.\n\nMy quick temp solution: \n   line:              'arg_overrides={\"\
    vocoder\": \"hifigan\", \"fp16\": False'\n   change to: 'arg_overrides={\"vocoder\"\
    : \"hifigan\", \"fp16\": False, \"cpu\": True}'\n\nThe line relates to setting\
    \ up and configuring the model for later inference/generation.\nI found this from\
    \ the documentation at: https://fairseq.readthedocs.io/en/latest/\nI noticed that\
    \ the 'arg_overrides' variable dictionary was similar to what was being used for\
    \ the fairseq command lines arguments from running fairseq generations from shell/command\
    \ line.\n   (see: https://fairseq.readthedocs.io/en/latest/command_line_tools.html#fairseq-generate\
    \ for a bit more specifics.)\n\nThis is likely the best I can help with this at\
    \ the moment."
  created_at: 2023-03-20 07:04:12+00:00
  edited: false
  hidden: false
  id: 6418137cc40218f71dca2531
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: facebook/fastspeech2-en-ljspeech
repo_type: model
status: open
target_branch: null
title: Example Error
