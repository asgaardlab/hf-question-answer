!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RoversX
conflicting_files: null
created_at: 2023-08-02 16:35:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a17b6f14b6a9f13aef3207e4e70bb93.svg
      fullname: Pro One
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RoversX
      type: user
    createdAt: '2023-08-02T17:35:35.000Z'
    data:
      edited: false
      editors:
      - RoversX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711393713951111
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a17b6f14b6a9f13aef3207e4e70bb93.svg
          fullname: Pro One
          isHf: false
          isPro: false
          name: RoversX
          type: user
        html: '<p>Hello, this model looks great. Thanks. I was wondering how you convert
          the original model to ggml. I am trying to use Qlora to fine tune a model
          based on that and run it on my Mac. Is this possible? Thanks</p>

          '
        raw: Hello, this model looks great. Thanks. I was wondering how you convert
          the original model to ggml. I am trying to use Qlora to fine tune a model
          based on that and run it on my Mac. Is this possible? Thanks
        updatedAt: '2023-08-02T17:35:35.200Z'
      numEdits: 0
      reactions: []
    id: 64ca93e7ead94891d1de2ee8
    type: comment
  author: RoversX
  content: Hello, this model looks great. Thanks. I was wondering how you convert
    the original model to ggml. I am trying to use Qlora to fine tune a model based
    on that and run it on my Mac. Is this possible? Thanks
  created_at: 2023-08-02 16:35:35+00:00
  edited: false
  hidden: false
  id: 64ca93e7ead94891d1de2ee8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-08-02T18:08:58.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9631338119506836
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>You have to merge base model with fine tuned lora version, than
          based on that structure you can convert it to ggml format</p>

          '
        raw: You have to merge base model with fine tuned lora version, than based
          on that structure you can convert it to ggml format
        updatedAt: '2023-08-02T18:08:58.193Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64ca9bba79d99f9e7a1e5e44
    id: 64ca9bba79d99f9e7a1e5e43
    type: comment
  author: s3nh
  content: You have to merge base model with fine tuned lora version, than based on
    that structure you can convert it to ggml format
  created_at: 2023-08-02 17:08:58+00:00
  edited: false
  hidden: false
  id: 64ca9bba79d99f9e7a1e5e43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-08-02T18:08:58.000Z'
    data:
      status: closed
    id: 64ca9bba79d99f9e7a1e5e44
    type: status-change
  author: s3nh
  created_at: 2023-08-02 17:08:58+00:00
  id: 64ca9bba79d99f9e7a1e5e44
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-08-02T18:09:03.000Z'
    data:
      status: open
    id: 64ca9bbf38837b12d5f77d7a
    type: status-change
  author: s3nh
  created_at: 2023-08-02 17:09:03+00:00
  id: 64ca9bbf38837b12d5f77d7a
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a17b6f14b6a9f13aef3207e4e70bb93.svg
      fullname: Pro One
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RoversX
      type: user
    createdAt: '2023-08-04T03:51:39.000Z'
    data:
      edited: false
      editors:
      - RoversX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9203242659568787
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a17b6f14b6a9f13aef3207e4e70bb93.svg
          fullname: Pro One
          isHf: false
          isPro: false
          name: RoversX
          type: user
        html: '<blockquote>

          <p>You have to merge base model with fine tuned lora version, than based
          on that structure you can convert it to ggml format</p>

          </blockquote>

          <p>Thank you for providing a useful description of the combination of basic
          models with fine -tuning LoRa version and converting it to GGML format.
          I have solved the problem</p>

          '
        raw: '> You have to merge base model with fine tuned lora version, than based
          on that structure you can convert it to ggml format


          Thank you for providing a useful description of the combination of basic
          models with fine -tuning LoRa version and converting it to GGML format.
          I have solved the problem'
        updatedAt: '2023-08-04T03:51:39.893Z'
      numEdits: 0
      reactions: []
    id: 64cc75cb8256a8efea70f22f
    type: comment
  author: RoversX
  content: '> You have to merge base model with fine tuned lora version, than based
    on that structure you can convert it to ggml format


    Thank you for providing a useful description of the combination of basic models
    with fine -tuning LoRa version and converting it to GGML format. I have solved
    the problem'
  created_at: 2023-08-04 02:51:39+00:00
  edited: false
  hidden: false
  id: 64cc75cb8256a8efea70f22f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-08-04T06:57:48.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7790756225585938
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: "<p>sorry, I just was in a rush, so the answer is not really detailed\
          \ at all.  </p>\n<p>To merge and unload: </p>\n<pre><code class=\"language-python\"\
          >\n    model = PeftModel.from_pretrained(base_model, args.peft_model_path,\
          \ **device_arg)\n    model = model.merge_and_unload()\n</code></pre>\n<p>where\
          \ base_model is path to original implementation and peft_model path is a\
          \ directory to your Lora tuned weights.<br> Then, you have to save it (together\
          \ with tokenizer)</p>\n<p>  model.save_pretrained(f\"{args.output_dir}\"\
          )</p>\n<p>and you are ready to convert it to ggml. </p>\n<p>to convert it\
          \ I am using llama.cpp<br><a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp\"\
          >https://github.com/ggerganov/llama.cpp</a></p>\n<p>In simplest case you\
          \ can use lora_to_ggml.py with properly defined arguments </p>\n<pre><code\
          \ class=\"language-python\">python  lora_to_ggml.py -m cached_model_path\
          \ -l llama.cpp_path -i output_path\n</code></pre>\n"
        raw: "sorry, I just was in a rush, so the answer is not really detailed at\
          \ all.  \n\nTo merge and unload: \n\n\n```python\n\n    model = PeftModel.from_pretrained(base_model,\
          \ args.peft_model_path, **device_arg)\n    model = model.merge_and_unload()\n\
          ```\n\nwhere base_model is path to original implementation and peft_model\
          \ path is a directory to your Lora tuned weights. \n Then, you have to save\
          \ it (together with tokenizer)\n\n  model.save_pretrained(f\"{args.output_dir}\"\
          )\n\nand you are ready to convert it to ggml. \n\nto convert it I am using\
          \ llama.cpp \nhttps://github.com/ggerganov/llama.cpp\n\nIn simplest case\
          \ you can use lora_to_ggml.py with properly defined arguments \n```python\n\
          python  lora_to_ggml.py -m cached_model_path -l llama.cpp_path -i output_path\n\
          ```"
        updatedAt: '2023-08-04T06:57:48.727Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - RoversX
    id: 64cca16c91726743364f4f42
    type: comment
  author: s3nh
  content: "sorry, I just was in a rush, so the answer is not really detailed at all.\
    \  \n\nTo merge and unload: \n\n\n```python\n\n    model = PeftModel.from_pretrained(base_model,\
    \ args.peft_model_path, **device_arg)\n    model = model.merge_and_unload()\n\
    ```\n\nwhere base_model is path to original implementation and peft_model path\
    \ is a directory to your Lora tuned weights. \n Then, you have to save it (together\
    \ with tokenizer)\n\n  model.save_pretrained(f\"{args.output_dir}\")\n\nand you\
    \ are ready to convert it to ggml. \n\nto convert it I am using llama.cpp \nhttps://github.com/ggerganov/llama.cpp\n\
    \nIn simplest case you can use lora_to_ggml.py with properly defined arguments\
    \ \n```python\npython  lora_to_ggml.py -m cached_model_path -l llama.cpp_path\
    \ -i output_path\n```"
  created_at: 2023-08-04 05:57:48+00:00
  edited: false
  hidden: false
  id: 64cca16c91726743364f4f42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a17b6f14b6a9f13aef3207e4e70bb93.svg
      fullname: Pro One
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RoversX
      type: user
    createdAt: '2023-08-08T11:32:48.000Z'
    data:
      edited: false
      editors:
      - RoversX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.561145007610321
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a17b6f14b6a9f13aef3207e4e70bb93.svg
          fullname: Pro One
          isHf: false
          isPro: false
          name: RoversX
          type: user
        html: "<p>Thank you. I basically use the same method. I use the convert-pth-to-ggml.py\
          \ to convert the model to ggml-model-f32.bin and then quantize it. Basically\
          \ follow the instructions provided in llama.cpp. Thanks. \U0001F44D</p>\n\
          <p><b>Using Colab</b></p>\n<ol>\n<li><p><strong>Convert the model to ggml-model-f32.bin:</strong>\
          \  </p>\n<pre><code class=\"language-bash\">!python3 convert-pth-to-ggml.py\
          \ models/model-path/ 0\n</code></pre>\n</li>\n<li><p><strong>Quantize the\
          \ model:</strong>  </p>\n<pre><code class=\"language-bash\">!./quantize\
          \ ./models/model-path/ggml-model-f32.bin ./models/model-path/ggml-model-q4_0.bin\
          \ q4_0\n</code></pre>\n</li>\n</ol>\n"
        raw: "Thank you. I basically use the same method. I use the convert-pth-to-ggml.py\
          \ to convert the model to ggml-model-f32.bin and then quantize it. Basically\
          \ follow the instructions provided in llama.cpp. Thanks. \U0001F44D\n\n\
          <b>Using Colab</b>\n\n1. **Convert the model to ggml-model-f32.bin:**  \n\
          \n   ```bash\n   !python3 convert-pth-to-ggml.py models/model-path/ 0\n\
          \   ```\n\n2. **Quantize the model:**  \n\n\n   ```bash\n   !./quantize\
          \ ./models/model-path/ggml-model-f32.bin ./models/model-path/ggml-model-q4_0.bin\
          \ q4_0\n   ```"
        updatedAt: '2023-08-08T11:32:48.918Z'
      numEdits: 0
      reactions: []
    id: 64d227e092474b17cb21abce
    type: comment
  author: RoversX
  content: "Thank you. I basically use the same method. I use the convert-pth-to-ggml.py\
    \ to convert the model to ggml-model-f32.bin and then quantize it. Basically follow\
    \ the instructions provided in llama.cpp. Thanks. \U0001F44D\n\n<b>Using Colab</b>\n\
    \n1. **Convert the model to ggml-model-f32.bin:**  \n\n   ```bash\n   !python3\
    \ convert-pth-to-ggml.py models/model-path/ 0\n   ```\n\n2. **Quantize the model:**\
    \  \n\n\n   ```bash\n   !./quantize ./models/model-path/ggml-model-f32.bin ./models/model-path/ggml-model-q4_0.bin\
    \ q4_0\n   ```"
  created_at: 2023-08-08 10:32:48+00:00
  edited: false
  hidden: false
  id: 64d227e092474b17cb21abce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
      fullname: s3nh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: s3nh
      type: user
    createdAt: '2023-08-08T11:51:54.000Z'
    data:
      edited: false
      editors:
      - s3nh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3695532977581024
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61caeda441f9432649f03ab6/q7BrCmKdQkvVZ7mBdKdbQ.jpeg?w=200&h=200&f=face
          fullname: s3nh
          isHf: false
          isPro: false
          name: s3nh
          type: user
        html: '<p>Thats great ti hear! Cool!</p>

          '
        raw: Thats great ti hear! Cool!
        updatedAt: '2023-08-08T11:51:54.725Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - RoversX
    id: 64d22c5aad5294e29371a112
    type: comment
  author: s3nh
  content: Thats great ti hear! Cool!
  created_at: 2023-08-08 10:51:54+00:00
  edited: false
  hidden: false
  id: 64d22c5aad5294e29371a112
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: s3nh/StableBeluga-7B-GGML
repo_type: model
status: open
target_branch: null
title: How to convert that to ggml?
