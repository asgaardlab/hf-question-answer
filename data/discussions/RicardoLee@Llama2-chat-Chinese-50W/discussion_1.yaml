!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yjianchun
conflicting_files: null
created_at: 2023-07-21 08:41:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ae367379c86c042e2731db7efed22e7.svg
      fullname: jianchun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yjianchun
      type: user
    createdAt: '2023-07-21T09:41:25.000Z'
    data:
      edited: false
      editors:
      - yjianchun
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9775572419166565
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ae367379c86c042e2731db7efed22e7.svg
          fullname: jianchun
          isHf: false
          isPro: false
          name: yjianchun
          type: user
        html: "<p>\u5982\u4F55\u4F7F\u7528\u5462\uFF1F\u6709\u754C\u9762\u7684\u4F7F\
          \u7528\u65B9\u5F0F\u5417\uFF1F\u63A8\u7406\u4EE3\u7801\u662F\u600E\u4E48\
          \u6837\u7684\uFF0C\u80FD\u7ED9\u4E2A\u6837\u4F8B\u5417</p>\n"
        raw: "\u5982\u4F55\u4F7F\u7528\u5462\uFF1F\u6709\u754C\u9762\u7684\u4F7F\u7528\
          \u65B9\u5F0F\u5417\uFF1F\u63A8\u7406\u4EE3\u7801\u662F\u600E\u4E48\u6837\
          \u7684\uFF0C\u80FD\u7ED9\u4E2A\u6837\u4F8B\u5417"
        updatedAt: '2023-07-21T09:41:25.994Z'
      numEdits: 0
      reactions: []
    id: 64ba52c5bc787364968a5257
    type: comment
  author: yjianchun
  content: "\u5982\u4F55\u4F7F\u7528\u5462\uFF1F\u6709\u754C\u9762\u7684\u4F7F\u7528\
    \u65B9\u5F0F\u5417\uFF1F\u63A8\u7406\u4EE3\u7801\u662F\u600E\u4E48\u6837\u7684\
    \uFF0C\u80FD\u7ED9\u4E2A\u6837\u4F8B\u5417"
  created_at: 2023-07-21 08:41:25+00:00
  edited: false
  hidden: false
  id: 64ba52c5bc787364968a5257
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4075d6e82df7f69e04d9910404719b86.svg
      fullname: Tian Li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: RicardoLee
      type: user
    createdAt: '2023-07-21T09:49:05.000Z'
    data:
      edited: true
      editors:
      - RicardoLee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3334992527961731
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4075d6e82df7f69e04d9910404719b86.svg
          fullname: Tian Li
          isHf: false
          isPro: false
          name: RicardoLee
          type: user
        html: "<p>\u672C\u9879\u76EE\u53EF\u5728\u5404\u4E2A\u573A\u666F\u8FDB\u884C\
          \u4F7F\u7528\uFF0C\u90E8\u7F72\u65B9\u6848\u89C6\u4F60\u7684\u5177\u4F53\
          \u573A\u666F\u800C\u5B9A\u3002<br>\u5982\u679C\u4F60\u6709\u663E\u5361\u7684\
          \u8BDD\uFF0C\u53EF\u4EE5\u5C1D\u8BD5\u4EE5\u4E0B\u65B9\u5F0F\u8FDB\u884C\
          \u6A21\u578B\u52A0\u8F7D</p>\n<pre><code>import os\nimport torch\nfrom transformers\
          \ import LlamaForCausalLM, LlamaTokenizer\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
          ]=\"0\"\n\ngeneration_config = dict(\n    temperature=0.2,\n    top_k=40,\n\
          \    top_p=0.9,\n    do_sample=True,\n    num_beams=1,\n    repetition_penalty=1.3,\n\
          \    max_new_tokens=2048\n    )\n\n # The prompt template below is taken\
          \ from llama.cpp\n # and is slightly different from the one used in training.\n\
          \ # But we find it gives better results\nprompt_input = (\n    \"Below is\
          \ an instruction that describes a task. \"\n    \"Write a response that\
          \ appropriately completes the request.\\n\\n\"\n    \"### Instruction:\\\
          n\\n{instruction}\\n\\n### Response:\\n\\n\"\n)\n\ndef generate_prompt(instruction,\
          \ input=None):\n    if input:\n        instruction = instruction + '\\n'\
          \ + input\n    return prompt_input.format_map({'instruction': instruction})\n\
          \nload_type = torch.float16\n\ndevice = torch.device(0)\n\ntokenizer = LlamaTokenizer.from_pretrained(\"\
          ${Model_Path}\")\n\nbase_model = LlamaForCausalLM.from_pretrained(\n   \
          \ \"${Model_Path}\",\n    load_in_8bit=False,\n    torch_dtype=load_type,\n\
          \    low_cpu_mem_usage=True,\n    device_map='auto',\n    )\n\nmodel = base_model\n\
          \nmodel.eval()\n\nwith torch.no_grad(), torch.autocast(\"cuda\"):\n    raw_input_text\
          \ = \"\u4EC0\u4E48\u662F\u5F3A\u5316\u5B66\u4E60\uFF1F\"\n    input_text\
          \ = generate_prompt(instruction=raw_input_text)\n    inputs = tokenizer(input_text,return_tensors=\"\
          pt\")\n    generation_output = model.generate(\n        input_ids = inputs[\"\
          input_ids\"].to(device),\n        attention_mask = inputs['attention_mask'].to(device),\n\
          \        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n\
          \        **generation_config\n    )\n    s = generation_output[0]\n    output\
          \ = tokenizer.decode(s,skip_special_tokens=True)\n    response = output.split(\"\
          ### Response:\")[1].strip()\n    print(response)\n</code></pre>\n"
        raw: "\u672C\u9879\u76EE\u53EF\u5728\u5404\u4E2A\u573A\u666F\u8FDB\u884C\u4F7F\
          \u7528\uFF0C\u90E8\u7F72\u65B9\u6848\u89C6\u4F60\u7684\u5177\u4F53\u573A\
          \u666F\u800C\u5B9A\u3002\n\u5982\u679C\u4F60\u6709\u663E\u5361\u7684\u8BDD\
          \uFF0C\u53EF\u4EE5\u5C1D\u8BD5\u4EE5\u4E0B\u65B9\u5F0F\u8FDB\u884C\u6A21\
          \u578B\u52A0\u8F7D\n\n```\nimport os\nimport torch\nfrom transformers import\
          \ LlamaForCausalLM, LlamaTokenizer\nos.environ[\"CUDA_VISIBLE_DEVICES\"\
          ]=\"0\"\n\ngeneration_config = dict(\n    temperature=0.2,\n    top_k=40,\n\
          \    top_p=0.9,\n    do_sample=True,\n    num_beams=1,\n    repetition_penalty=1.3,\n\
          \    max_new_tokens=2048\n    )\n\n # The prompt template below is taken\
          \ from llama.cpp\n # and is slightly different from the one used in training.\n\
          \ # But we find it gives better results\nprompt_input = (\n    \"Below is\
          \ an instruction that describes a task. \"\n    \"Write a response that\
          \ appropriately completes the request.\\n\\n\"\n    \"### Instruction:\\\
          n\\n{instruction}\\n\\n### Response:\\n\\n\"\n)\n\ndef generate_prompt(instruction,\
          \ input=None):\n    if input:\n        instruction = instruction + '\\n'\
          \ + input\n    return prompt_input.format_map({'instruction': instruction})\n\
          \nload_type = torch.float16\n\ndevice = torch.device(0)\n\ntokenizer = LlamaTokenizer.from_pretrained(\"\
          ${Model_Path}\")\n\nbase_model = LlamaForCausalLM.from_pretrained(\n   \
          \ \"${Model_Path}\",\n    load_in_8bit=False,\n    torch_dtype=load_type,\n\
          \    low_cpu_mem_usage=True,\n    device_map='auto',\n    )\n\nmodel = base_model\n\
          \nmodel.eval()\n\nwith torch.no_grad(), torch.autocast(\"cuda\"):\n    raw_input_text\
          \ = \"\u4EC0\u4E48\u662F\u5F3A\u5316\u5B66\u4E60\uFF1F\"\n    input_text\
          \ = generate_prompt(instruction=raw_input_text)\n    inputs = tokenizer(input_text,return_tensors=\"\
          pt\")\n    generation_output = model.generate(\n        input_ids = inputs[\"\
          input_ids\"].to(device),\n        attention_mask = inputs['attention_mask'].to(device),\n\
          \        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n\
          \        **generation_config\n    )\n    s = generation_output[0]\n    output\
          \ = tokenizer.decode(s,skip_special_tokens=True)\n    response = output.split(\"\
          ### Response:\")[1].strip()\n    print(response)\n```"
        updatedAt: '2023-07-21T09:50:15.722Z'
      numEdits: 1
      reactions: []
    id: 64ba54917789c0a80d70f423
    type: comment
  author: RicardoLee
  content: "\u672C\u9879\u76EE\u53EF\u5728\u5404\u4E2A\u573A\u666F\u8FDB\u884C\u4F7F\
    \u7528\uFF0C\u90E8\u7F72\u65B9\u6848\u89C6\u4F60\u7684\u5177\u4F53\u573A\u666F\
    \u800C\u5B9A\u3002\n\u5982\u679C\u4F60\u6709\u663E\u5361\u7684\u8BDD\uFF0C\u53EF\
    \u4EE5\u5C1D\u8BD5\u4EE5\u4E0B\u65B9\u5F0F\u8FDB\u884C\u6A21\u578B\u52A0\u8F7D\
    \n\n```\nimport os\nimport torch\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\
    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\ngeneration_config = dict(\n    temperature=0.2,\n\
    \    top_k=40,\n    top_p=0.9,\n    do_sample=True,\n    num_beams=1,\n    repetition_penalty=1.3,\n\
    \    max_new_tokens=2048\n    )\n\n # The prompt template below is taken from\
    \ llama.cpp\n # and is slightly different from the one used in training.\n # But\
    \ we find it gives better results\nprompt_input = (\n    \"Below is an instruction\
    \ that describes a task. \"\n    \"Write a response that appropriately completes\
    \ the request.\\n\\n\"\n    \"### Instruction:\\n\\n{instruction}\\n\\n### Response:\\\
    n\\n\"\n)\n\ndef generate_prompt(instruction, input=None):\n    if input:\n  \
    \      instruction = instruction + '\\n' + input\n    return prompt_input.format_map({'instruction':\
    \ instruction})\n\nload_type = torch.float16\n\ndevice = torch.device(0)\n\ntokenizer\
    \ = LlamaTokenizer.from_pretrained(\"${Model_Path}\")\n\nbase_model = LlamaForCausalLM.from_pretrained(\n\
    \    \"${Model_Path}\",\n    load_in_8bit=False,\n    torch_dtype=load_type,\n\
    \    low_cpu_mem_usage=True,\n    device_map='auto',\n    )\n\nmodel = base_model\n\
    \nmodel.eval()\n\nwith torch.no_grad(), torch.autocast(\"cuda\"):\n    raw_input_text\
    \ = \"\u4EC0\u4E48\u662F\u5F3A\u5316\u5B66\u4E60\uFF1F\"\n    input_text = generate_prompt(instruction=raw_input_text)\n\
    \    inputs = tokenizer(input_text,return_tensors=\"pt\")\n    generation_output\
    \ = model.generate(\n        input_ids = inputs[\"input_ids\"].to(device),\n \
    \       attention_mask = inputs['attention_mask'].to(device),\n        eos_token_id=tokenizer.eos_token_id,\n\
    \        pad_token_id=tokenizer.pad_token_id,\n        **generation_config\n \
    \   )\n    s = generation_output[0]\n    output = tokenizer.decode(s,skip_special_tokens=True)\n\
    \    response = output.split(\"### Response:\")[1].strip()\n    print(response)\n\
    ```"
  created_at: 2023-07-21 08:49:05+00:00
  edited: true
  hidden: false
  id: 64ba54917789c0a80d70f423
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ae367379c86c042e2731db7efed22e7.svg
      fullname: jianchun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yjianchun
      type: user
    createdAt: '2023-07-21T10:04:24.000Z'
    data:
      edited: false
      editors:
      - yjianchun
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9751722812652588
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ae367379c86c042e2731db7efed22e7.svg
          fullname: jianchun
          isHf: false
          isPro: false
          name: yjianchun
          type: user
        html: "<p>\u80FD\u5E2E\u5F04\u4E2Agradio\u65B9\u5F0F\u5417 \u7B80\u5355\u7684\
          \u5C31\u884C \u521D\u5B66\u8005\u60F3\u7528\u7528\u4F60\u7684\u8FD9\u4E2A\
          \u6A21\u578B</p>\n"
        raw: "\u80FD\u5E2E\u5F04\u4E2Agradio\u65B9\u5F0F\u5417 \u7B80\u5355\u7684\u5C31\
          \u884C \u521D\u5B66\u8005\u60F3\u7528\u7528\u4F60\u7684\u8FD9\u4E2A\u6A21\
          \u578B"
        updatedAt: '2023-07-21T10:04:24.113Z'
      numEdits: 0
      reactions: []
    id: 64ba5828997dd1af6d4e9041
    type: comment
  author: yjianchun
  content: "\u80FD\u5E2E\u5F04\u4E2Agradio\u65B9\u5F0F\u5417 \u7B80\u5355\u7684\u5C31\
    \u884C \u521D\u5B66\u8005\u60F3\u7528\u7528\u4F60\u7684\u8FD9\u4E2A\u6A21\u578B"
  created_at: 2023-07-21 09:04:24+00:00
  edited: false
  hidden: false
  id: 64ba5828997dd1af6d4e9041
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4075d6e82df7f69e04d9910404719b86.svg
      fullname: Tian Li
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: RicardoLee
      type: user
    createdAt: '2023-07-21T12:09:24.000Z'
    data:
      edited: false
      editors:
      - RicardoLee
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.4581524431705475
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4075d6e82df7f69e04d9910404719b86.svg
          fullname: Tian Li
          isHf: false
          isPro: false
          name: RicardoLee
          type: user
        html: "<p>\u8BE5\u6A21\u578B\u53EF\u76F4\u63A5\u7528<a rel=\"nofollow\" href=\"\
          https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/gradio_demo.py\"\
          >https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/gradio_demo.py</a>\
          \ \u52A0\u8F7D\u6A21\u578B\u3002<br>\u6A21\u578Bclone\u5230\u672C\u5730\u540E\
          \uFF0C\u53EF\u8FD0\u884C\uFF1A<br>python gradio_demo.py --base_model ${Model_Path}\
          \ --tokenizer_path ${Model_Path} --gpus 0<br>\u542F\u52A8\u670D\u52A1</p>\n"
        raw: "\u8BE5\u6A21\u578B\u53EF\u76F4\u63A5\u7528https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/gradio_demo.py\
          \ \u52A0\u8F7D\u6A21\u578B\u3002\n\u6A21\u578Bclone\u5230\u672C\u5730\u540E\
          \uFF0C\u53EF\u8FD0\u884C\uFF1A\npython gradio_demo.py --base_model ${Model_Path}\
          \ --tokenizer_path ${Model_Path} --gpus 0 \n\u542F\u52A8\u670D\u52A1"
        updatedAt: '2023-07-21T12:09:24.094Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wxllyf
    id: 64ba757406fefa2be480b519
    type: comment
  author: RicardoLee
  content: "\u8BE5\u6A21\u578B\u53EF\u76F4\u63A5\u7528https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference/gradio_demo.py\
    \ \u52A0\u8F7D\u6A21\u578B\u3002\n\u6A21\u578Bclone\u5230\u672C\u5730\u540E\uFF0C\
    \u53EF\u8FD0\u884C\uFF1A\npython gradio_demo.py --base_model ${Model_Path} --tokenizer_path\
    \ ${Model_Path} --gpus 0 \n\u542F\u52A8\u670D\u52A1"
  created_at: 2023-07-21 11:09:24+00:00
  edited: false
  hidden: false
  id: 64ba757406fefa2be480b519
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/53e88f374504825a717e1d2478240c6c.svg
      fullname: BatmanBill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BatmanBill
      type: user
    createdAt: '2023-07-26T07:04:04.000Z'
    data:
      edited: false
      editors:
      - BatmanBill
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.9546336531639099
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/53e88f374504825a717e1d2478240c6c.svg
          fullname: BatmanBill
          isHf: false
          isPro: false
          name: BatmanBill
          type: user
        html: "<p>\u60F3\u95EE\u4E0Btrain_sft.py\u4E2D\u7684coati\u5305\u5728\u54EA\
          \u91CC\uFF1F\u663E\u793A\u627E\u4E0D\u5230\u8FD9\u4E2A\u5305</p>\n"
        raw: "\u60F3\u95EE\u4E0Btrain_sft.py\u4E2D\u7684coati\u5305\u5728\u54EA\u91CC\
          \uFF1F\u663E\u793A\u627E\u4E0D\u5230\u8FD9\u4E2A\u5305"
        updatedAt: '2023-07-26T07:04:04.641Z'
      numEdits: 0
      reactions: []
    id: 64c0c564e175dd56a57234a5
    type: comment
  author: BatmanBill
  content: "\u60F3\u95EE\u4E0Btrain_sft.py\u4E2D\u7684coati\u5305\u5728\u54EA\u91CC\
    \uFF1F\u663E\u793A\u627E\u4E0D\u5230\u8FD9\u4E2A\u5305"
  created_at: 2023-07-26 06:04:04+00:00
  edited: false
  hidden: false
  id: 64c0c564e175dd56a57234a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: RicardoLee/Llama2-chat-Chinese-50W
repo_type: model
status: open
target_branch: null
title: "\u90A3\u8FD9\u4E2A\u600E\u4E48\u8C03\u7528\u5462"
