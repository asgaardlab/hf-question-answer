!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yiouyou
conflicting_files: null
created_at: 2023-11-19 09:08:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-11-19T09:08:53.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9790146350860596
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>I''d like to buy. What''s the format of this model? And what''s
          the training loss of this model?</p>

          <p>Thanks,</p>

          '
        raw: "I'd like to buy. What's the format of this model? And what's the training\
          \ loss of this model?\r\n\r\nThanks,"
        updatedAt: '2023-11-19T09:08:53.354Z'
      numEdits: 0
      reactions: []
    id: 6559d0a5c68499a1b96755f9
    type: comment
  author: yiouyou
  content: "I'd like to buy. What's the format of this model? And what's the training\
    \ loss of this model?\r\n\r\nThanks,"
  created_at: 2023-11-19 09:08:53+00:00
  edited: false
  hidden: false
  id: 6559d0a5c68499a1b96755f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-19T14:15:31.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9389023780822754
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>This is a full precision bf16 model. It is not AWQ or GPTQ.</p>

          <p>This model was trained with quantized LoRA (bnb nf4) followed by merging
          of the adapter to the base model, so I expect the perplexity to be a tiny
          bit higher than the base model. For example, if base perplexity were 1.00,
          there might be a perplexity increase of about 0.05 . However, if you load
          the base model larryvrh/Yi-34B-200K-Llamafied and then apply the adapter,
          that should eliminate that perplexity increase. (the perplexity increase
          has to do with imperfect merging of adapters when using quantization).</p>

          <p>The main drawback of this current model is that it is fine-tuned from
          a base model that is not SFT fine-tuned to a chat format. So, I expect this
          model to do well when function calls are required, but it will only be as
          good at conversations as the base model. For this reason, I''m deprecating
          this model in favour of:</p>

          <p>"Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2"</p>

          <p>That is a function calling fine-tuned version of a chat fine-tuned model
          (Trelis/Yi-34B-200K-Llamafied-chat-SFT). Furthermore, all fine-tunes were
          done in bf16 (not quantized) so there is no perplexity loss as described
          above. Additionally, I''m making an AWQ version: Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ.
          I don''t plan to make a GPTQ because perplexity of GPTQ is bad compared
          to AWQ, but let me know if you need GPTQ.</p>

          '
        raw: 'This is a full precision bf16 model. It is not AWQ or GPTQ.


          This model was trained with quantized LoRA (bnb nf4) followed by merging
          of the adapter to the base model, so I expect the perplexity to be a tiny
          bit higher than the base model. For example, if base perplexity were 1.00,
          there might be a perplexity increase of about 0.05 . However, if you load
          the base model larryvrh/Yi-34B-200K-Llamafied and then apply the adapter,
          that should eliminate that perplexity increase. (the perplexity increase
          has to do with imperfect merging of adapters when using quantization).


          The main drawback of this current model is that it is fine-tuned from a
          base model that is not SFT fine-tuned to a chat format. So, I expect this
          model to do well when function calls are required, but it will only be as
          good at conversations as the base model. For this reason, I''m deprecating
          this model in favour of:


          "Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2"


          That is a function calling fine-tuned version of a chat fine-tuned model
          (Trelis/Yi-34B-200K-Llamafied-chat-SFT). Furthermore, all fine-tunes were
          done in bf16 (not quantized) so there is no perplexity loss as described
          above. Additionally, I''m making an AWQ version: Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ.
          I don''t plan to make a GPTQ because perplexity of GPTQ is bad compared
          to AWQ, but let me know if you need GPTQ.'
        updatedAt: '2023-11-19T14:15:31.076Z'
      numEdits: 0
      reactions: []
    id: 655a1883c68499a1b97280e5
    type: comment
  author: RonanMcGovern
  content: 'This is a full precision bf16 model. It is not AWQ or GPTQ.


    This model was trained with quantized LoRA (bnb nf4) followed by merging of the
    adapter to the base model, so I expect the perplexity to be a tiny bit higher
    than the base model. For example, if base perplexity were 1.00, there might be
    a perplexity increase of about 0.05 . However, if you load the base model larryvrh/Yi-34B-200K-Llamafied
    and then apply the adapter, that should eliminate that perplexity increase. (the
    perplexity increase has to do with imperfect merging of adapters when using quantization).


    The main drawback of this current model is that it is fine-tuned from a base model
    that is not SFT fine-tuned to a chat format. So, I expect this model to do well
    when function calls are required, but it will only be as good at conversations
    as the base model. For this reason, I''m deprecating this model in favour of:


    "Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2"


    That is a function calling fine-tuned version of a chat fine-tuned model (Trelis/Yi-34B-200K-Llamafied-chat-SFT).
    Furthermore, all fine-tunes were done in bf16 (not quantized) so there is no perplexity
    loss as described above. Additionally, I''m making an AWQ version: Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2-AWQ.
    I don''t plan to make a GPTQ because perplexity of GPTQ is bad compared to AWQ,
    but let me know if you need GPTQ.'
  created_at: 2023-11-19 14:15:31+00:00
  edited: false
  hidden: false
  id: 655a1883c68499a1b97280e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-11-20T01:47:14.000Z'
    data:
      edited: true
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9241958856582642
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>"Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2" came
          out, well done! One question, does this one has the adapter merging issue?
          Do you plan to generate the adapter version of "chat+function calling"?
          Based on your suggestion, larryvrh/Yi-34B-200K-Llamafied + adpater seems
          better than AWQ?</p>

          <p>Thanks,</p>

          '
        raw: '"Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2" came out,
          well done! One question, does this one has the adapter merging issue? Do
          you plan to generate the adapter version of "chat+function calling"? Based
          on your suggestion, larryvrh/Yi-34B-200K-Llamafied + adpater seems better
          than AWQ?


          Thanks,'
        updatedAt: '2023-11-20T01:47:35.226Z'
      numEdits: 1
      reactions: []
    id: 655abaa2e0a6202d368274df
    type: comment
  author: yiouyou
  content: '"Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2" came out,
    well done! One question, does this one has the adapter merging issue? Do you plan
    to generate the adapter version of "chat+function calling"? Based on your suggestion,
    larryvrh/Yi-34B-200K-Llamafied + adpater seems better than AWQ?


    Thanks,'
  created_at: 2023-11-20 01:47:14+00:00
  edited: true
  hidden: false
  id: 655abaa2e0a6202d368274df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-11-20T05:24:16.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9739995002746582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>I''m using text-generation-webui to load LLM, it seems that AWQ
          takes more GPU memory than GPTQ, and AWQ can''t be pointed to a specific
          GPU, so even I know AWQ seems better than GPTQ, I still need GPTQ to run
          LLM. And Ollama seems only support GGUF custom LLM, so I guess GPTQ is what
          I need at the moment. Thanks~</p>

          '
        raw: I'm using text-generation-webui to load LLM, it seems that AWQ takes
          more GPU memory than GPTQ, and AWQ can't be pointed to a specific GPU, so
          even I know AWQ seems better than GPTQ, I still need GPTQ to run LLM. And
          Ollama seems only support GGUF custom LLM, so I guess GPTQ is what I need
          at the moment. Thanks~
        updatedAt: '2023-11-20T05:24:16.600Z'
      numEdits: 0
      reactions: []
    id: 655aed8057f3e9ac07d88444
    type: comment
  author: yiouyou
  content: I'm using text-generation-webui to load LLM, it seems that AWQ takes more
    GPU memory than GPTQ, and AWQ can't be pointed to a specific GPU, so even I know
    AWQ seems better than GPTQ, I still need GPTQ to run LLM. And Ollama seems only
    support GGUF custom LLM, so I guess GPTQ is what I need at the moment. Thanks~
  created_at: 2023-11-20 05:24:16+00:00
  edited: false
  hidden: false
  id: 655aed8057f3e9ac07d88444
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
      fullname: Zhuo Song
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yiouyou
      type: user
    createdAt: '2023-11-20T08:27:05.000Z'
    data:
      edited: false
      editors:
      - yiouyou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.938312828540802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bfb828df0c992a11e1b362a1a064e71d.svg
          fullname: Zhuo Song
          isHf: false
          isPro: false
          name: yiouyou
          type: user
        html: '<p>One more question, "chat-SFT-function-calling-v2" supports both
          chat and function-calling, doesn''t it?</p>

          '
        raw: One more question, "chat-SFT-function-calling-v2" supports both chat
          and function-calling, doesn't it?
        updatedAt: '2023-11-20T08:27:05.922Z'
      numEdits: 0
      reactions: []
    id: 655b1859ab0644b531bdbf34
    type: comment
  author: yiouyou
  content: One more question, "chat-SFT-function-calling-v2" supports both chat and
    function-calling, doesn't it?
  created_at: 2023-11-20 08:27:05+00:00
  edited: false
  hidden: false
  id: 655b1859ab0644b531bdbf34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-11-20T09:29:27.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.944794774055481
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy <span data-props=\"{&quot;user&quot;:&quot;yiouyou&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yiouyou\"\
          >@<span class=\"underline\">yiouyou</span></a></span>\n\n\t</span></span>\
          \ :</p>\n<ol>\n<li>\"Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2\"\
          \ came out, well done! One question, does this one has the adapter merging\
          \ issue?</li>\n</ol>\n<p>Answer: No. This model was trained in bf16 precision\
          \ (no quantization), so there is no adapter merging issue. BTW, the model\
          \ card is now up allowing purchase access.</p>\n<ol start=\"2\">\n<li>Do\
          \ you plan to generate the adapter version of \"chat+function calling\"\
          ? Based on your suggestion, larryvrh/Yi-34B-200K-Llamafied + adpater seems\
          \ better than AWQ?<br>\"Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-adapters-v2\"\
          </li>\n</ol>\n<p>Answer: The adapter version is up. However, since the model\
          \ is fine-tuned in bf16 (no quantization), there is no advantage to loading\
          \ the base model and applying the adapter. The result will be the same.\
          \ Loading the base model is probably easier.</p>\n<ol start=\"3\">\n<li>I'm\
          \ using text-generation-webui to load LLM, it seems that AWQ takes more\
          \ GPU memory than GPTQ, and AWQ can't be pointed to a specific GPU, so even\
          \ I know AWQ seems better than GPTQ, I still need GPTQ to run LLM. And Ollama\
          \ seems only support GGUF custom LLM, so I guess GPTQ is what I need at\
          \ the moment. Thanks~</li>\n</ol>\n<p>Answer: Understood, I'll work to put\
          \ up a GPTQ for the chat-SFT-function-calling-v2 model. BTW, maybe be worth\
          \ trying vLLM.</p>\n<ol start=\"4\">\n<li>One more question, \"chat-SFT-function-calling-v2\"\
          \ supports both chat and function-calling, doesn't it?</li>\n</ol>\n<p>Correct.</p>\n"
        raw: 'Howdy @yiouyou :


          1. "Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2" came out,
          well done! One question, does this one has the adapter merging issue?


          Answer: No. This model was trained in bf16 precision (no quantization),
          so there is no adapter merging issue. BTW, the model card is now up allowing
          purchase access.


          2. Do you plan to generate the adapter version of "chat+function calling"?
          Based on your suggestion, larryvrh/Yi-34B-200K-Llamafied + adpater seems
          better than AWQ?

          "Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-adapters-v2"


          Answer: The adapter version is up. However, since the model is fine-tuned
          in bf16 (no quantization), there is no advantage to loading the base model
          and applying the adapter. The result will be the same. Loading the base
          model is probably easier.


          3. I''m using text-generation-webui to load LLM, it seems that AWQ takes
          more GPU memory than GPTQ, and AWQ can''t be pointed to a specific GPU,
          so even I know AWQ seems better than GPTQ, I still need GPTQ to run LLM.
          And Ollama seems only support GGUF custom LLM, so I guess GPTQ is what I
          need at the moment. Thanks~


          Answer: Understood, I''ll work to put up a GPTQ for the chat-SFT-function-calling-v2
          model. BTW, maybe be worth trying vLLM.


          4. One more question, "chat-SFT-function-calling-v2" supports both chat
          and function-calling, doesn''t it?


          Correct.

          '
        updatedAt: '2023-11-20T09:29:27.965Z'
      numEdits: 0
      reactions: []
    id: 655b26f716de0c93aed7c035
    type: comment
  author: RonanMcGovern
  content: 'Howdy @yiouyou :


    1. "Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-v2" came out, well
    done! One question, does this one has the adapter merging issue?


    Answer: No. This model was trained in bf16 precision (no quantization), so there
    is no adapter merging issue. BTW, the model card is now up allowing purchase access.


    2. Do you plan to generate the adapter version of "chat+function calling"? Based
    on your suggestion, larryvrh/Yi-34B-200K-Llamafied + adpater seems better than
    AWQ?

    "Trelis/Yi-34B-200K-Llamafied-chat-SFT-function-calling-adapters-v2"


    Answer: The adapter version is up. However, since the model is fine-tuned in bf16
    (no quantization), there is no advantage to loading the base model and applying
    the adapter. The result will be the same. Loading the base model is probably easier.


    3. I''m using text-generation-webui to load LLM, it seems that AWQ takes more
    GPU memory than GPTQ, and AWQ can''t be pointed to a specific GPU, so even I know
    AWQ seems better than GPTQ, I still need GPTQ to run LLM. And Ollama seems only
    support GGUF custom LLM, so I guess GPTQ is what I need at the moment. Thanks~


    Answer: Understood, I''ll work to put up a GPTQ for the chat-SFT-function-calling-v2
    model. BTW, maybe be worth trying vLLM.


    4. One more question, "chat-SFT-function-calling-v2" supports both chat and function-calling,
    doesn''t it?


    Correct.

    '
  created_at: 2023-11-20 09:29:27+00:00
  edited: false
  hidden: false
  id: 655b26f716de0c93aed7c035
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-05T15:03:36.000Z'
    data:
      status: closed
    id: 656f3bc8bb6a8fc221e24ce8
    type: status-change
  author: RonanMcGovern
  created_at: 2023-12-05 15:03:36+00:00
  id: 656f3bc8bb6a8fc221e24ce8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Trelis/Yi-34B-200K-Llamafied-function-calling-v2
repo_type: model
status: closed
target_branch: null
title: Is it GPTQ/AWQ?
