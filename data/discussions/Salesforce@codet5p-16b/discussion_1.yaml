!!python/object:huggingface_hub.community.DiscussionWithDetails
author: michaelfeil
conflicting_files: null
created_at: 2023-05-19 23:46:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-05-20T00:46:43.000Z'
    data:
      edited: true
      editors:
      - michaelfeil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: '<p>Looking forward to convert this model to a faster version for accelerated
          inference. (2B, 6B, 16B)<br>Options:</p>

          <ul>

          <li>Ctranslate2: Support for all architectures such as T5, mT5, GPT-J, GPT-2,..
          As with codet5p-770m-py, this runs now at high speed and 1320MiB cuda footprint,
          batch inference which I think is awesome. <a href="https://huggingface.co/michaelfeil/ct2fast-codet5p-770m-py">https://huggingface.co/michaelfeil/ct2fast-codet5p-770m-py</a>
          -&gt; Any way to convert this to a T5 architecture?</li>

          <li>Onnx -&gt; ORT or Nvidia TensorRT -&gt; CodeT5pModuleConfig has no Onnx
          implementation, e.g. see Codegen2</li>

          </ul>

          <p>Any advice?</p>

          '
        raw: 'Looking forward to convert this model to a faster version for accelerated
          inference. (2B, 6B, 16B)

          Options:

          - Ctranslate2: Support for all architectures such as T5, mT5, GPT-J, GPT-2,..
          As with codet5p-770m-py, this runs now at high speed and 1320MiB cuda footprint,
          batch inference which I think is awesome. https://huggingface.co/michaelfeil/ct2fast-codet5p-770m-py
          -> Any way to convert this to a T5 architecture?

          - Onnx -> ORT or Nvidia TensorRT -> CodeT5pModuleConfig has no Onnx implementation,
          e.g. see Codegen2


          Any advice?'
        updatedAt: '2023-05-20T01:12:52.322Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - albertd
    id: 64681873fc6f6da8b11ebc40
    type: comment
  author: michaelfeil
  content: 'Looking forward to convert this model to a faster version for accelerated
    inference. (2B, 6B, 16B)

    Options:

    - Ctranslate2: Support for all architectures such as T5, mT5, GPT-J, GPT-2,..
    As with codet5p-770m-py, this runs now at high speed and 1320MiB cuda footprint,
    batch inference which I think is awesome. https://huggingface.co/michaelfeil/ct2fast-codet5p-770m-py
    -> Any way to convert this to a T5 architecture?

    - Onnx -> ORT or Nvidia TensorRT -> CodeT5pModuleConfig has no Onnx implementation,
    e.g. see Codegen2


    Any advice?'
  created_at: 2023-05-19 23:46:43+00:00
  edited: true
  hidden: false
  id: 64681873fc6f6da8b11ebc40
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Salesforce/codet5p-16b
repo_type: model
status: open
target_branch: null
title: Onnx-version or Compatability to T5forConditionalGeneration
