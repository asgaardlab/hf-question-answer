!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zappa2005
conflicting_files: null
created_at: 2024-01-18 11:53:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-18T11:53:46.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9461456537246704
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Would it be possible to do a smaller quant for 16GB 4080 owners?
          Maybe 2.4bpw? I''d like to test it, too.<br>Btw, did you base on instruct-v0.1
          or instruct-v0.2?</p>

          <p>Thank you!</p>

          '
        raw: "Would it be possible to do a smaller quant for 16GB 4080 owners? Maybe\
          \ 2.4bpw? I'd like to test it, too. \r\nBtw, did you base on instruct-v0.1\
          \ or instruct-v0.2?\r\n\r\nThank you!"
        updatedAt: '2024-01-18T11:53:46.952Z'
      numEdits: 0
      reactions: []
    id: 65a9114a3212568def30ba32
    type: comment
  author: zappa2005
  content: "Would it be possible to do a smaller quant for 16GB 4080 owners? Maybe\
    \ 2.4bpw? I'd like to test it, too. \r\nBtw, did you base on instruct-v0.1 or\
    \ instruct-v0.2?\r\n\r\nThank you!"
  created_at: 2024-01-18 11:53:46+00:00
  edited: false
  hidden: false
  id: 65a9114a3212568def30ba32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-18T11:53:53.000Z'
    data:
      from: Thanks!
      to: smaller quant
    id: 65a91151feb3859352b0461b
    type: title-change
  author: zappa2005
  created_at: 2024-01-18 11:53:53+00:00
  id: 65a91151feb3859352b0461b
  new_title: smaller quant
  old_title: Thanks!
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
      fullname: Zack Stone
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zaq-hack
      type: user
    createdAt: '2024-01-18T16:30:15.000Z'
    data:
      edited: true
      editors:
      - zaq-hack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9776476621627808
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
          fullname: Zack Stone
          isHf: false
          isPro: false
          name: zaq-hack
          type: user
        html: '<p>Why not ... I''ll do a 2.40 for ya. :-)</p>

          <p>The original model is here, and I''m not sure about the specifics of
          the recipe. I just know it (1) isn'' t dumber than a bag of hair, and (2)
          when it works, it puts out some hot stuff. <a href="https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss">https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss</a></p>

          <p>On the downside, it can be prone to getting stuck in a loop once you
          are past the context - even at 32k. I''m not sure how to avoid it, but it''s
          very annoying. Especially since one of my chats went over 800 messages,
          and I was really loving it. Then it fell into a loop, yesterday, and I couldn''t
          get it unstuck. I''m sure it is my fault, but I''m not sure what to do to
          move it on other than starting a fresh chat. Even so, 800 messages feels
          like a huge win compared to most others.</p>

          '
        raw: 'Why not ... I''ll do a 2.40 for ya. :-)


          The original model is here, and I''m not sure about the specifics of the
          recipe. I just know it (1) isn'' t dumber than a bag of hair, and (2) when
          it works, it puts out some hot stuff. https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss


          On the downside, it can be prone to getting stuck in a loop once you are
          past the context - even at 32k. I''m not sure how to avoid it, but it''s
          very annoying. Especially since one of my chats went over 800 messages,
          and I was really loving it. Then it fell into a loop, yesterday, and I couldn''t
          get it unstuck. I''m sure it is my fault, but I''m not sure what to do to
          move it on other than starting a fresh chat. Even so, 800 messages feels
          like a huge win compared to most others.'
        updatedAt: '2024-01-18T16:30:59.467Z'
      numEdits: 2
      reactions: []
    id: 65a95217848a78d4dadf1d3d
    type: comment
  author: zaq-hack
  content: 'Why not ... I''ll do a 2.40 for ya. :-)


    The original model is here, and I''m not sure about the specifics of the recipe.
    I just know it (1) isn'' t dumber than a bag of hair, and (2) when it works, it
    puts out some hot stuff. https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss


    On the downside, it can be prone to getting stuck in a loop once you are past
    the context - even at 32k. I''m not sure how to avoid it, but it''s very annoying.
    Especially since one of my chats went over 800 messages, and I was really loving
    it. Then it fell into a loop, yesterday, and I couldn''t get it unstuck. I''m
    sure it is my fault, but I''m not sure what to do to move it on other than starting
    a fresh chat. Even so, 800 messages feels like a huge win compared to most others.'
  created_at: 2024-01-18 16:30:15+00:00
  edited: true
  hidden: false
  id: 65a95217848a78d4dadf1d3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
      fullname: Zack Stone
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zaq-hack
      type: user
    createdAt: '2024-01-18T23:13:33.000Z'
    data:
      edited: false
      editors:
      - zaq-hack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9362331032752991
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
          fullname: Zack Stone
          isHf: false
          isPro: false
          name: zaq-hack
          type: user
        html: '<p><a href="https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw240-h6-exl2">https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw240-h6-exl2</a></p>

          <p>I think it might still be a touch too large for a 16GB card while using
          longer context. The problem is, we are getting further down the accuracy
          curve, so I don''t know how far you want to go. I''m not a big fan of 2.0
          ... but I''ll try a 2.25 to see if that might still have enough of the original
          flavor in it to get you by.</p>

          '
        raw: 'https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw240-h6-exl2


          I think it might still be a touch too large for a 16GB card while using
          longer context. The problem is, we are getting further down the accuracy
          curve, so I don''t know how far you want to go. I''m not a big fan of 2.0
          ... but I''ll try a 2.25 to see if that might still have enough of the original
          flavor in it to get you by.'
        updatedAt: '2024-01-18T23:13:33.311Z'
      numEdits: 0
      reactions: []
    id: 65a9b09dcb5b4fb08e82d998
    type: comment
  author: zaq-hack
  content: 'https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw240-h6-exl2


    I think it might still be a touch too large for a 16GB card while using longer
    context. The problem is, we are getting further down the accuracy curve, so I
    don''t know how far you want to go. I''m not a big fan of 2.0 ... but I''ll try
    a 2.25 to see if that might still have enough of the original flavor in it to
    get you by.'
  created_at: 2024-01-18 23:13:33+00:00
  edited: false
  hidden: false
  id: 65a9b09dcb5b4fb08e82d998
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
      fullname: Zack Stone
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zaq-hack
      type: user
    createdAt: '2024-01-19T03:52:34.000Z'
    data:
      edited: false
      editors:
      - zaq-hack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8060060143470764
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
          fullname: Zack Stone
          isHf: false
          isPro: false
          name: zaq-hack
          type: user
        html: '<p><a href="https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw225-h6-exl2">https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw225-h6-exl2</a></p>

          <p>Okay: I haven''t tried this one, but I hope it works great for ya!</p>

          '
        raw: 'https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw225-h6-exl2


          Okay: I haven''t tried this one, but I hope it works great for ya!'
        updatedAt: '2024-01-19T03:52:34.821Z'
      numEdits: 0
      reactions: []
    id: 65a9f202cb5b4fb08e96d3bc
    type: comment
  author: zaq-hack
  content: 'https://huggingface.co/zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw225-h6-exl2


    Okay: I haven''t tried this one, but I hope it works great for ya!'
  created_at: 2024-01-19 03:52:34+00:00
  edited: false
  hidden: false
  id: 65a9f202cb5b4fb08e96d3bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-19T10:32:23.000Z'
    data:
      edited: true
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9641277194023132
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>I will check and report back! Thanks for the quants and your time,
          appreciated.<br>Btw, did you base on instruct-v0.1 or instruct-v0.2?</p>

          '
        raw: 'I will check and report back! Thanks for the quants and your time, appreciated.

          Btw, did you base on instruct-v0.1 or instruct-v0.2?'
        updatedAt: '2024-01-19T10:32:45.270Z'
      numEdits: 1
      reactions: []
    id: 65aa4fb73b9e1f0f30abcf2e
    type: comment
  author: zappa2005
  content: 'I will check and report back! Thanks for the quants and your time, appreciated.

    Btw, did you base on instruct-v0.1 or instruct-v0.2?'
  created_at: 2024-01-19 10:32:23+00:00
  edited: true
  hidden: false
  id: 65aa4fb73b9e1f0f30abcf2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
      fullname: Zack Stone
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zaq-hack
      type: user
    createdAt: '2024-01-19T11:19:05.000Z'
    data:
      edited: false
      editors:
      - zaq-hack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806776642799377
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
          fullname: Zack Stone
          isHf: false
          isPro: false
          name: zaq-hack
          type: user
        html: '<p>Again, I''m not sure what''s under the hood. You''d have to ask
          Undi and Ikaridev.</p>

          '
        raw: Again, I'm not sure what's under the hood. You'd have to ask Undi and
          Ikaridev.
        updatedAt: '2024-01-19T11:19:05.307Z'
      numEdits: 0
      reactions: []
    id: 65aa5aa92bf3e0cbbf07219e
    type: comment
  author: zaq-hack
  content: Again, I'm not sure what's under the hood. You'd have to ask Undi and Ikaridev.
  created_at: 2024-01-19 11:19:05+00:00
  edited: false
  hidden: false
  id: 65aa5aa92bf3e0cbbf07219e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-19T13:05:55.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9517125487327576
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>The 2.25bpw loads with 16k context on a 4080, it even still surpasses
          the one of my standard reasoning tests (only a models get that right).</p>

          <p>You: If I have 7 apples today, and ate 3 last week, how many do I have
          now?<br>AI:  You currently have 7 apples. Your consumption of 3 apples from
          last week does not affect your current apple count.</p>

          <p>Looks promising, I''ll check the RP stuff later. Thanks again for your
          time!</p>

          '
        raw: 'The 2.25bpw loads with 16k context on a 4080, it even still surpasses
          the one of my standard reasoning tests (only a models get that right).


          You: If I have 7 apples today, and ate 3 last week, how many do I have now?

          AI:  You currently have 7 apples. Your consumption of 3 apples from last
          week does not affect your current apple count.


          Looks promising, I''ll check the RP stuff later. Thanks again for your time!'
        updatedAt: '2024-01-19T13:05:55.695Z'
      numEdits: 0
      reactions: []
    id: 65aa73b346d2f7fe54c1fa60
    type: comment
  author: zappa2005
  content: 'The 2.25bpw loads with 16k context on a 4080, it even still surpasses
    the one of my standard reasoning tests (only a models get that right).


    You: If I have 7 apples today, and ate 3 last week, how many do I have now?

    AI:  You currently have 7 apples. Your consumption of 3 apples from last week
    does not affect your current apple count.


    Looks promising, I''ll check the RP stuff later. Thanks again for your time!'
  created_at: 2024-01-19 13:05:55+00:00
  edited: false
  hidden: false
  id: 65aa73b346d2f7fe54c1fa60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-19T22:55:20.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413579702377319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>I had a bit of time on my hands and can say that the 2.25bpw works
          very well on my end, no obvious shortcomings, and it ranks pretty high on
          my personal favorite list! </p>

          <p>On a side note, do you know of  the exl2 quants already support this
          new 2-bit SOTA stuff that was merged recently in llama-cpp?</p>

          <p><a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/19anqbc/llamacpp_now_supports_quip_2bit_quant_mixtral_in/">https://www.reddit.com/r/LocalLLaMA/comments/19anqbc/llamacpp_now_supports_quip_2bit_quant_mixtral_in/</a></p>

          '
        raw: "I had a bit of time on my hands and can say that the 2.25bpw works very\
          \ well on my end, no obvious shortcomings, and it ranks pretty high on my\
          \ personal favorite list! \n\nOn a side note, do you know of  the exl2 quants\
          \ already support this new 2-bit SOTA stuff that was merged recently in\
          \ llama-cpp?\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/19anqbc/llamacpp_now_supports_quip_2bit_quant_mixtral_in/"
        updatedAt: '2024-01-19T22:55:20.862Z'
      numEdits: 0
      reactions: []
    id: 65aafdd8e0ee7990a66d753a
    type: comment
  author: zappa2005
  content: "I had a bit of time on my hands and can say that the 2.25bpw works very\
    \ well on my end, no obvious shortcomings, and it ranks pretty high on my personal\
    \ favorite list! \n\nOn a side note, do you know of  the exl2 quants already support\
    \ this new 2-bit SOTA stuff that was merged recently in llama-cpp?\n\nhttps://www.reddit.com/r/LocalLLaMA/comments/19anqbc/llamacpp_now_supports_quip_2bit_quant_mixtral_in/"
  created_at: 2024-01-19 22:55:20+00:00
  edited: false
  hidden: false
  id: 65aafdd8e0ee7990a66d753a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
      fullname: Zack Stone
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: zaq-hack
      type: user
    createdAt: '2024-01-20T20:00:49.000Z'
    data:
      edited: false
      editors:
      - zaq-hack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9819818735122681
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
          fullname: Zack Stone
          isHf: false
          isPro: false
          name: zaq-hack
          type: user
        html: '<p>I actually don''t know. I started playing with Aphrodite-engine
          on Thursday, and it doesn''t even support EXL2. I''ve had to use GPTQ, and
          this model doesn''t work because I can''t split it across cards. That said,
          the inference is INSANELY fast. 8k context response in 4 seconds. 32k context
          in like 9-12 seconds. It definitively changes the experience, but I''ve
          had to drop model size down to MistralTrix. <a href="https://huggingface.co/zaq-hack/MistralTrix-v1-GPTQ">https://huggingface.co/zaq-hack/MistralTrix-v1-GPTQ</a></p>

          '
        raw: I actually don't know. I started playing with Aphrodite-engine on Thursday,
          and it doesn't even support EXL2. I've had to use GPTQ, and this model doesn't
          work because I can't split it across cards. That said, the inference is
          INSANELY fast. 8k context response in 4 seconds. 32k context in like 9-12
          seconds. It definitively changes the experience, but I've had to drop model
          size down to MistralTrix. https://huggingface.co/zaq-hack/MistralTrix-v1-GPTQ
        updatedAt: '2024-01-20T20:00:49.222Z'
      numEdits: 0
      reactions: []
    id: 65ac2671b0b0876790ed412e
    type: comment
  author: zaq-hack
  content: I actually don't know. I started playing with Aphrodite-engine on Thursday,
    and it doesn't even support EXL2. I've had to use GPTQ, and this model doesn't
    work because I can't split it across cards. That said, the inference is INSANELY
    fast. 8k context response in 4 seconds. 32k context in like 9-12 seconds. It definitively
    changes the experience, but I've had to drop model size down to MistralTrix. https://huggingface.co/zaq-hack/MistralTrix-v1-GPTQ
  created_at: 2024-01-20 20:00:49+00:00
  edited: false
  hidden: false
  id: 65ac2671b0b0876790ed412e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: zaq-hack/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-bpw300-h6-exl2
repo_type: model
status: open
target_branch: null
title: smaller quant
