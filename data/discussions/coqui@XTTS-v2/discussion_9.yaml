!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gqd
conflicting_files: null
created_at: 2023-11-17 14:03:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2023-11-17T14:03:45.000Z'
    data:
      edited: false
      editors:
      - gqd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.863458514213562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
          fullname: God
          isHf: false
          isPro: false
          name: gqd
          type: user
        html: '<p>Hey</p>

          <ul>

          <li>All the examples show how to produce output with a speaker voice</li>

          <li>Wondering if it''s possible to do fine-tuning on a speaker voice and
          then inference without passing a reference sample to reduce latency?</li>

          </ul>

          <p>Thx</p>

          '
        raw: "Hey\r\n\r\n- All the examples show how to produce output with a speaker\
          \ voice\r\n- Wondering if it's possible to do fine-tuning on a speaker voice\
          \ and then inference without passing a reference sample to reduce latency?\r\
          \n\r\nThx"
        updatedAt: '2023-11-17T14:03:45.504Z'
      numEdits: 0
      reactions: []
    id: 655772c1dd1c09639225417c
    type: comment
  author: gqd
  content: "Hey\r\n\r\n- All the examples show how to produce output with a speaker\
    \ voice\r\n- Wondering if it's possible to do fine-tuning on a speaker voice and\
    \ then inference without passing a reference sample to reduce latency?\r\n\r\n\
    Thx"
  created_at: 2023-11-17 14:03:45+00:00
  edited: false
  hidden: false
  id: 655772c1dd1c09639225417c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2023-11-17T14:04:25.000Z'
    data:
      from: usage without voice cloning
      to: inference without voice cloning
    id: 655772e98616d8d740e21a0a
    type: title-change
  author: gqd
  created_at: 2023-11-17 14:04:25+00:00
  id: 655772e98616d8d740e21a0a
  new_title: inference without voice cloning
  old_title: usage without voice cloning
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
      fullname: Gorkem Goknar
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gorkemgoknar
      type: user
    createdAt: '2023-11-18T07:41:45.000Z'
    data:
      edited: false
      editors:
      - gorkemgoknar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6498833298683167
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30fb3453a8777f7d40ed311b613f03a.svg
          fullname: Gorkem Goknar
          isHf: false
          isPro: false
          name: gorkemgoknar
          type: user
        html: '<p>Once you calculated latents , you can pass same latents to inference
          there after, that reduces inference time.<br>Please check code on <a href="https://huggingface.co/spaces/coqui/xtts/blob/main/app.py#L233">https://huggingface.co/spaces/coqui/xtts/blob/main/app.py#L233</a><br>gpt_cond_latent,speaker_embedding
          = model.get_conditioning_latents(audio_path=speaker_wav, gpt_cond_len=30,
          max_ref_length=60)</p>

          '
        raw: 'Once you calculated latents , you can pass same latents to inference
          there after, that reduces inference time.

          Please check code on https://huggingface.co/spaces/coqui/xtts/blob/main/app.py#L233

          gpt_cond_latent,speaker_embedding = model.get_conditioning_latents(audio_path=speaker_wav,
          gpt_cond_len=30, max_ref_length=60)'
        updatedAt: '2023-11-18T07:41:45.995Z'
      numEdits: 0
      reactions: []
    id: 65586ab96412aaeed62d23a1
    type: comment
  author: gorkemgoknar
  content: 'Once you calculated latents , you can pass same latents to inference there
    after, that reduces inference time.

    Please check code on https://huggingface.co/spaces/coqui/xtts/blob/main/app.py#L233

    gpt_cond_latent,speaker_embedding = model.get_conditioning_latents(audio_path=speaker_wav,
    gpt_cond_len=30, max_ref_length=60)'
  created_at: 2023-11-18 07:41:45+00:00
  edited: false
  hidden: false
  id: 65586ab96412aaeed62d23a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
      fullname: God
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gqd
      type: user
    createdAt: '2023-11-18T12:11:21.000Z'
    data:
      edited: false
      editors:
      - gqd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9368894696235657
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63baa260d90985e7acd88183/0Ddjw1YjTAEhcQCm-cH-p.jpeg?w=200&h=200&f=face
          fullname: God
          isHf: false
          isPro: false
          name: gqd
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;gorkemgoknar&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gorkemgoknar\"\
          >@<span class=\"underline\">gorkemgoknar</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Is it possible to fine-tune and inference coqui/XTTS-v2 as a\
          \ single speaker model entirely, to remove the additional latency of using\
          \ the latents?</p>\n<p>Or wouldn't that make much of a difference, when\
          \ using the precomputed latents as you suggested?</p>\n<p>Thx</p>\n"
        raw: "Hey @gorkemgoknar \n\nIs it possible to fine-tune and inference coqui/XTTS-v2\
          \ as a single speaker model entirely, to remove the additional latency of\
          \ using the latents?\n\nOr wouldn't that make much of a difference, when\
          \ using the precomputed latents as you suggested?\n\nThx"
        updatedAt: '2023-11-18T12:11:21.206Z'
      numEdits: 0
      reactions: []
    id: 6558a9e957f3e9ac0785c611
    type: comment
  author: gqd
  content: "Hey @gorkemgoknar \n\nIs it possible to fine-tune and inference coqui/XTTS-v2\
    \ as a single speaker model entirely, to remove the additional latency of using\
    \ the latents?\n\nOr wouldn't that make much of a difference, when using the precomputed\
    \ latents as you suggested?\n\nThx"
  created_at: 2023-11-18 12:11:21+00:00
  edited: false
  hidden: false
  id: 6558a9e957f3e9ac0785c611
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: coqui/XTTS-v2
repo_type: model
status: open
target_branch: null
title: inference without voice cloning
