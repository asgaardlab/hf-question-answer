!!python/object:huggingface_hub.community.DiscussionWithDetails
author: worldsapart
conflicting_files: null
created_at: 2023-12-25 00:20:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69b394d85683532d68cea57cf001bbca.svg
      fullname: Jake Wiley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: worldsapart
      type: user
    createdAt: '2023-12-25T00:20:31.000Z'
    data:
      edited: false
      editors:
      - worldsapart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9233625531196594
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69b394d85683532d68cea57cf001bbca.svg
          fullname: Jake Wiley
          isHf: false
          isPro: false
          name: worldsapart
          type: user
        html: '<p>Consistently getting this issue on Windows with RTX 3070 Ti. I noticed
          GPU memory utilization shoots up to 100% quickly until the process is killed.
          I played around with PYTORCH_CUDA_ALLOC_CONF without success.</p>

          <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00
          MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of
          the allocated memory 6.11 GiB is allocated by PyTorch, and 451.52 MiB is
          reserved by PyTorch but unallocated. If reserved but unallocated memory
          is large try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>Any suggestions?</p>

          '
        raw: "Consistently getting this issue on Windows with RTX 3070 Ti. I noticed\
          \ GPU memory utilization shoots up to 100% quickly until the process is\
          \ killed. I played around with PYTORCH_CUDA_ALLOC_CONF without success.\r\
          \n\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
          \ 1024.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is\
          \ free. Of the allocated memory 6.11 GiB is allocated by PyTorch, and 451.52\
          \ MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
          \ memory is large try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n\r\nAny suggestions?"
        updatedAt: '2023-12-25T00:20:31.583Z'
      numEdits: 0
      reactions: []
    id: 6588cacf0100bf3373ce83a5
    type: comment
  author: worldsapart
  content: "Consistently getting this issue on Windows with RTX 3070 Ti. I noticed\
    \ GPU memory utilization shoots up to 100% quickly until the process is killed.\
    \ I played around with PYTORCH_CUDA_ALLOC_CONF without success.\r\n\r\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty\
    \ of 8.00 GiB of which 0 bytes is free. Of the allocated memory 6.11 GiB is allocated\
    \ by PyTorch, and 451.52 MiB is reserved by PyTorch but unallocated. If reserved\
    \ but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\n\
    Any suggestions?"
  created_at: 2023-12-25 00:20:31+00:00
  edited: false
  hidden: false
  id: 6588cacf0100bf3373ce83a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/F2k343TPD7KVfW3P26IRs.jpeg?w=200&h=200&f=face
      fullname: Yatharth Gupta
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Warlord-K
      type: user
    createdAt: '2023-12-25T11:06:45.000Z'
    data:
      edited: false
      editors:
      - Warlord-K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7279865145683289
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/F2k343TPD7KVfW3P26IRs.jpeg?w=200&h=200&f=face
          fullname: Yatharth Gupta
          isHf: false
          isPro: false
          name: Warlord-K
          type: user
        html: '<p>Try enabling cpu offload, </p>

          <pre><code class="language-python">

          pipe.enable_sequential_cpu_offload()

          </code></pre>

          <p>This should allow you to run it</p>

          '
        raw: "Try enabling cpu offload, \n\n```python\n\npipe.enable_sequential_cpu_offload()\n\
          ```\n\nThis should allow you to run it"
        updatedAt: '2023-12-25T11:06:45.773Z'
      numEdits: 0
      reactions: []
    id: 65896245304552ba0cd0cb44
    type: comment
  author: Warlord-K
  content: "Try enabling cpu offload, \n\n```python\n\npipe.enable_sequential_cpu_offload()\n\
    ```\n\nThis should allow you to run it"
  created_at: 2023-12-25 11:06:45+00:00
  edited: false
  hidden: false
  id: 65896245304552ba0cd0cb44
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: segmind/SSD-1B
repo_type: model
status: open
target_branch: null
title: CUDA out of memory
