!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfredplpl
conflicting_files: null
created_at: 2023-10-30 07:03:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-10-30T08:03:40.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579219222068787
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: '<p>I''m currently exploring various techniques to optimize and improve
          our model''s performance, and one question arose in our discussions: If
          we take a model that has already undergone distillation and then distill
          it again, would this lead to better results compared to just fine-tuning
          the original model?</p>

          <p>I understand that distillation is a method to transfer knowledge from
          a larger model (teacher) to a smaller model (student), aiming to retain
          the generalization capabilities of the larger model while benefiting from
          the efficiency of the smaller one. However, it''s unclear to me how this
          would work when applied repeatedly, especially when compared to fine-tuning.</p>

          <p>Has anyone here tried this approach before? If so, could you share your
          findings? Are there any studies or papers that discuss this topic in detail?</p>

          <p>Thank you in advance for your insights!</p>

          '
        raw: "I'm currently exploring various techniques to optimize and improve our\
          \ model's performance, and one question arose in our discussions: If we\
          \ take a model that has already undergone distillation and then distill\
          \ it again, would this lead to better results compared to just fine-tuning\
          \ the original model?\r\n\r\nI understand that distillation is a method\
          \ to transfer knowledge from a larger model (teacher) to a smaller model\
          \ (student), aiming to retain the generalization capabilities of the larger\
          \ model while benefiting from the efficiency of the smaller one. However,\
          \ it's unclear to me how this would work when applied repeatedly, especially\
          \ when compared to fine-tuning.\r\n\r\nHas anyone here tried this approach\
          \ before? If so, could you share your findings? Are there any studies or\
          \ papers that discuss this topic in detail?\r\n\r\nThank you in advance\
          \ for your insights!"
        updatedAt: '2023-10-30T08:03:40.137Z'
      numEdits: 0
      reactions: []
    id: 653f635c81f52ceb4d58ded7
    type: comment
  author: alfredplpl
  content: "I'm currently exploring various techniques to optimize and improve our\
    \ model's performance, and one question arose in our discussions: If we take a\
    \ model that has already undergone distillation and then distill it again, would\
    \ this lead to better results compared to just fine-tuning the original model?\r\
    \n\r\nI understand that distillation is a method to transfer knowledge from a\
    \ larger model (teacher) to a smaller model (student), aiming to retain the generalization\
    \ capabilities of the larger model while benefiting from the efficiency of the\
    \ smaller one. However, it's unclear to me how this would work when applied repeatedly,\
    \ especially when compared to fine-tuning.\r\n\r\nHas anyone here tried this approach\
    \ before? If so, could you share your findings? Are there any studies or papers\
    \ that discuss this topic in detail?\r\n\r\nThank you in advance for your insights!"
  created_at: 2023-10-30 07:03:40+00:00
  edited: false
  hidden: false
  id: 653f635c81f52ceb4d58ded7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/F2k343TPD7KVfW3P26IRs.jpeg?w=200&h=200&f=face
      fullname: Yatharth Gupta
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Warlord-K
      type: user
    createdAt: '2023-10-30T09:47:43.000Z'
    data:
      edited: false
      editors:
      - Warlord-K
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9549113512039185
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8ca074588fe31f4361dae/F2k343TPD7KVfW3P26IRs.jpeg?w=200&h=200&f=face
          fullname: Yatharth Gupta
          isHf: false
          isPro: false
          name: Warlord-K
          type: user
        html: '<p>Repeated distillation is probably not a good approach to take, As
          their might be a lot of hidden loss that is not apparent very quickly. Repeated
          or Progressive Distillation when explored via the lens of faster sampling
          however seems to be promising, as shown by <a rel="nofollow" href="https://arxiv.org/abs/2202.00512">this
          paper</a>. </p>

          <p>Also, Distillation is much more expensive in terms of compute. Finetuning,
          or even lora training, can instill powerful concepts into text to image
          models while requiring a fraction of the cost, so if increasing the quality
          is your motto then these methods are probably more suitable. </p>

          '
        raw: "Repeated distillation is probably not a good approach to take, As their\
          \ might be a lot of hidden loss that is not apparent very quickly. Repeated\
          \ or Progressive Distillation when explored via the lens of faster sampling\
          \ however seems to be promising, as shown by [this paper](https://arxiv.org/abs/2202.00512).\
          \ \n\nAlso, Distillation is much more expensive in terms of compute. Finetuning,\
          \ or even lora training, can instill powerful concepts into text to image\
          \ models while requiring a fraction of the cost, so if increasing the quality\
          \ is your motto then these methods are probably more suitable. "
        updatedAt: '2023-10-30T09:47:43.213Z'
      numEdits: 0
      reactions: []
    id: 653f7bbf4a52f10eaf8d5618
    type: comment
  author: Warlord-K
  content: "Repeated distillation is probably not a good approach to take, As their\
    \ might be a lot of hidden loss that is not apparent very quickly. Repeated or\
    \ Progressive Distillation when explored via the lens of faster sampling however\
    \ seems to be promising, as shown by [this paper](https://arxiv.org/abs/2202.00512).\
    \ \n\nAlso, Distillation is much more expensive in terms of compute. Finetuning,\
    \ or even lora training, can instill powerful concepts into text to image models\
    \ while requiring a fraction of the cost, so if increasing the quality is your\
    \ motto then these methods are probably more suitable. "
  created_at: 2023-10-30 08:47:43+00:00
  edited: false
  hidden: false
  id: 653f7bbf4a52f10eaf8d5618
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-10-30T09:57:25.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9600779414176941
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: '<p>Thank you for your response. Fine-tuning the distilled model is
          quite challenging, but instead of distillation, I will try to come up with
          some workaround.</p>

          '
        raw: Thank you for your response. Fine-tuning the distilled model is quite
          challenging, but instead of distillation, I will try to come up with some
          workaround.
        updatedAt: '2023-10-30T09:57:25.667Z'
      numEdits: 0
      reactions: []
      relatedEventId: 653f7e056d28265c85f7f4a3
    id: 653f7e056d28265c85f7f49f
    type: comment
  author: alfredplpl
  content: Thank you for your response. Fine-tuning the distilled model is quite challenging,
    but instead of distillation, I will try to come up with some workaround.
  created_at: 2023-10-30 08:57:25+00:00
  edited: false
  hidden: false
  id: 653f7e056d28265c85f7f49f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-10-30T09:57:25.000Z'
    data:
      status: closed
    id: 653f7e056d28265c85f7f4a3
    type: status-change
  author: alfredplpl
  created_at: 2023-10-30 08:57:25+00:00
  id: 653f7e056d28265c85f7f4a3
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: segmind/SSD-1B
repo_type: model
status: closed
target_branch: null
title: Is distilling an already distilled model superior to fine-tuning?
