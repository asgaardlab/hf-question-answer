!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wolfram
conflicting_files: null
created_at: 2023-08-04 11:06:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-08-04T12:06:19.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8987634778022766
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>The examples you give have <code>&lt;|end_of_turn|&gt;</code> as
          the only separator between user and assistant message, without any newlines:</p>

          <p><code>User: Hello&lt;|end_of_turn|&gt;Assistant: Hi&lt;|end_of_turn|&gt;User:
          How are you today?&lt;|end_of_turn|&gt;Assistant:</code></p>

          <p>But further down the page, your ooba prompt template has a newline after
          the seperator and also after the assistant message:</p>

          <p><code>&lt;|user|&gt; &lt;|user-message|&gt;&lt;|end_of_turn|&gt;\n&lt;|bot|&gt;
          &lt;|bot-message|&gt;\n</code></p>

          <p>So which is correct? And where, and how, is the "context" you give for
          ooba supposed to go in the prompt (I assume it''s the system message)?</p>

          '
        raw: "The examples you give have `<|end_of_turn|>` as the only separator between\
          \ user and assistant message, without any newlines:\r\n\r\n`User: Hello<|end_of_turn|>Assistant:\
          \ Hi<|end_of_turn|>User: How are you today?<|end_of_turn|>Assistant:`\r\n\
          \r\nBut further down the page, your ooba prompt template has a newline after\
          \ the seperator and also after the assistant message:\r\n\r\n`<|user|> <|user-message|><|end_of_turn|>\\\
          n<|bot|> <|bot-message|>\\n`\r\n\r\nSo which is correct? And where, and\
          \ how, is the \"context\" you give for ooba supposed to go in the prompt\
          \ (I assume it's the system message)?"
        updatedAt: '2023-08-04T12:06:19.812Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jphme
    id: 64cce9bb71a7bbb60c57c486
    type: comment
  author: wolfram
  content: "The examples you give have `<|end_of_turn|>` as the only separator between\
    \ user and assistant message, without any newlines:\r\n\r\n`User: Hello<|end_of_turn|>Assistant:\
    \ Hi<|end_of_turn|>User: How are you today?<|end_of_turn|>Assistant:`\r\n\r\n\
    But further down the page, your ooba prompt template has a newline after the seperator\
    \ and also after the assistant message:\r\n\r\n`<|user|> <|user-message|><|end_of_turn|>\\\
    n<|bot|> <|bot-message|>\\n`\r\n\r\nSo which is correct? And where, and how, is\
    \ the \"context\" you give for ooba supposed to go in the prompt (I assume it's\
    \ the system message)?"
  created_at: 2023-08-04 11:06:19+00:00
  edited: false
  hidden: false
  id: 64cce9bb71a7bbb60c57c486
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
      fullname: Bleys
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: bleysg
      type: user
    createdAt: '2023-08-04T15:27:21.000Z'
    data:
      edited: false
      editors:
      - bleysg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619441628456116
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
          fullname: Bleys
          isHf: false
          isPro: true
          name: bleysg
          type: user
        html: '<p>There are still some bugs in the way different inference engines
          process tokens. The way the model was trained, the newline characters shouldn''t
          be necessary, but they also shouldn''t hurt.<br>We''ve found that in some
          cases including them can reduce the chances of unexpected behavior.<br>So
          try without newlines if you''re token-budget-conscious. If you find any
          unusual behavior (e.g. not stopping inference when done outputting, or the
          model starting to have a conversation with itself), try inserting them.</p>

          <p>Note: The space after "User:" (so this "User: " not this "User:") and
          "Assistant: " will also help avoid inference bugs, and are also part of
          the training regimen, so should in all circumstances be included.</p>

          '
        raw: 'There are still some bugs in the way different inference engines process
          tokens. The way the model was trained, the newline characters shouldn''t
          be necessary, but they also shouldn''t hurt.

          We''ve found that in some cases including them can reduce the chances of
          unexpected behavior.

          So try without newlines if you''re token-budget-conscious. If you find any
          unusual behavior (e.g. not stopping inference when done outputting, or the
          model starting to have a conversation with itself), try inserting them.


          Note: The space after "User:" (so this "User: " not this "User:") and "Assistant:
          " will also help avoid inference bugs, and are also part of the training
          regimen, so should in all circumstances be included.'
        updatedAt: '2023-08-04T15:27:21.531Z'
      numEdits: 0
      reactions: []
    id: 64cd18d94fc11756c92eb163
    type: comment
  author: bleysg
  content: 'There are still some bugs in the way different inference engines process
    tokens. The way the model was trained, the newline characters shouldn''t be necessary,
    but they also shouldn''t hurt.

    We''ve found that in some cases including them can reduce the chances of unexpected
    behavior.

    So try without newlines if you''re token-budget-conscious. If you find any unusual
    behavior (e.g. not stopping inference when done outputting, or the model starting
    to have a conversation with itself), try inserting them.


    Note: The space after "User:" (so this "User: " not this "User:") and "Assistant:
    " will also help avoid inference bugs, and are also part of the training regimen,
    so should in all circumstances be included.'
  created_at: 2023-08-04 14:27:21+00:00
  edited: false
  hidden: false
  id: 64cd18d94fc11756c92eb163
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-08-04T16:00:30.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8964850306510925
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>Thanks for the explanation. Just noticed another discrepancy:</p>

          <p>What about the &lt;|end_of_turn|&gt; itself? In the first example there''s
          one after every message, both user and assistant, whereas in the ooba example,
          there''s only one after the user message, but none after the bot message,
          just a linebreak.</p>

          '
        raw: 'Thanks for the explanation. Just noticed another discrepancy:


          What about the <|end_of_turn|> itself? In the first example there''s one
          after every message, both user and assistant, whereas in the ooba example,
          there''s only one after the user message, but none after the bot message,
          just a linebreak.'
        updatedAt: '2023-08-04T16:00:30.016Z'
      numEdits: 0
      reactions: []
    id: 64cd209e57ff08e3d16744ed
    type: comment
  author: wolfram
  content: 'Thanks for the explanation. Just noticed another discrepancy:


    What about the <|end_of_turn|> itself? In the first example there''s one after
    every message, both user and assistant, whereas in the ooba example, there''s
    only one after the user message, but none after the bot message, just a linebreak.'
  created_at: 2023-08-04 15:00:30+00:00
  edited: false
  hidden: false
  id: 64cd209e57ff08e3d16744ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
      fullname: Bleys
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: bleysg
      type: user
    createdAt: '2023-08-04T16:06:36.000Z'
    data:
      edited: false
      editors:
      - bleysg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8171699047088623
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9441d895e3308a15a50c5dab942454ff.svg
          fullname: Bleys
          isHf: false
          isPro: true
          name: bleysg
          type: user
        html: '<p>The model should generate the end of turn token when it is done
          responding to the prompt.</p>

          '
        raw: The model should generate the end of turn token when it is done responding
          to the prompt.
        updatedAt: '2023-08-04T16:06:36.409Z'
      numEdits: 0
      reactions: []
    id: 64cd220c0b83533e22e95b0c
    type: comment
  author: bleysg
  content: The model should generate the end of turn token when it is done responding
    to the prompt.
  created_at: 2023-08-04 15:06:36+00:00
  edited: false
  hidden: false
  id: 64cd220c0b83533e22e95b0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-08-04T18:32:48.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9073222279548645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>OK. So when inference software catches that as a stop token and
          removes it before returning the output, it makes sense that we add it into
          the prompt for the next round of inference.</p>

          '
        raw: OK. So when inference software catches that as a stop token and removes
          it before returning the output, it makes sense that we add it into the prompt
          for the next round of inference.
        updatedAt: '2023-08-04T18:32:48.918Z'
      numEdits: 0
      reactions: []
    id: 64cd44509617774ce4a17802
    type: comment
  author: wolfram
  content: OK. So when inference software catches that as a stop token and removes
    it before returning the output, it makes sense that we add it into the prompt
    for the next round of inference.
  created_at: 2023-08-04 17:32:48+00:00
  edited: false
  hidden: false
  id: 64cd44509617774ce4a17802
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Open-Orca/OpenOrcaxOpenChat-Preview2-13B
repo_type: model
status: open
target_branch: null
title: Prompt template discrepancy
