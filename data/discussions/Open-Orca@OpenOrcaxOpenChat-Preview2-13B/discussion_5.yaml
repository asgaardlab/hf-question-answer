!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anon7463435254
conflicting_files: null
created_at: 2023-08-04 16:12:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-08-04T17:12:30.000Z'
    data:
      edited: true
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8831825256347656
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>These benchmarks declaring better results than those obtained by
          Microsoft itself are unreliable, considering that the model is not even
          able to answer simple reasoning questions, which other models like LLaMA2,
          Nous Hermes, WizardLM handle correctly. Below one of the many questions
          I''m referring to:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/837wZ-uhIENsYb1M_eirD.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/837wZ-uhIENsYb1M_eirD.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/thr7lzwcaZo53eS0raUXv.png"><img
          alt="cap2.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/thr7lzwcaZo53eS0raUXv.png"></a></p>

          <p>According to your benchmarks, should we expect that the original Orca
          will fail to answer such questions correctly? I don''t think it likely.<br>In
          conclusion, either these benchmarks are untrue or the original Microsoft
          Orca model will be incredibly disappointing.</p>

          <p>Let''s wait for the 100% version to see if it will answer properly to
          the same question.</p>

          '
        raw: 'These benchmarks declaring better results than those obtained by Microsoft
          itself are unreliable, considering that the model is not even able to answer
          simple reasoning questions, which other models like LLaMA2, Nous Hermes,
          WizardLM handle correctly. Below one of the many questions I''m referring
          to:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/837wZ-uhIENsYb1M_eirD.png)


          ![cap2.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/thr7lzwcaZo53eS0raUXv.png)


          According to your benchmarks, should we expect that the original Orca will
          fail to answer such questions correctly? I don''t think it likely.

          In conclusion, either these benchmarks are untrue or the original Microsoft
          Orca model will be incredibly disappointing.


          Let''s wait for the 100% version to see if it will answer properly to the
          same question.'
        updatedAt: '2023-08-04T17:15:37.605Z'
      numEdits: 1
      reactions: []
    id: 64cd317e749587dbe0d00255
    type: comment
  author: anon7463435254
  content: 'These benchmarks declaring better results than those obtained by Microsoft
    itself are unreliable, considering that the model is not even able to answer simple
    reasoning questions, which other models like LLaMA2, Nous Hermes, WizardLM handle
    correctly. Below one of the many questions I''m referring to:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/837wZ-uhIENsYb1M_eirD.png)


    ![cap2.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/thr7lzwcaZo53eS0raUXv.png)


    According to your benchmarks, should we expect that the original Orca will fail
    to answer such questions correctly? I don''t think it likely.

    In conclusion, either these benchmarks are untrue or the original Microsoft Orca
    model will be incredibly disappointing.


    Let''s wait for the 100% version to see if it will answer properly to the same
    question.'
  created_at: 2023-08-04 16:12:30+00:00
  edited: true
  hidden: false
  id: 64cd317e749587dbe0d00255
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df091910678851bb0cd0e0/FUXFt0C-rUFSppIAu5ZDN.png?w=200&h=200&f=face
      fullname: Alvaro Somoza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: OzzyGT
      type: user
    createdAt: '2023-08-04T21:07:50.000Z'
    data:
      edited: true
      editors:
      - OzzyGT
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8957911133766174
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63df091910678851bb0cd0e0/FUXFt0C-rUFSppIAu5ZDN.png?w=200&h=200&f=face
          fullname: Alvaro Somoza
          isHf: false
          isPro: false
          name: OzzyGT
          type: user
        html: '<p>I don''t know what parameters you set up on the space, locally I
          get this answer:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63df091910678851bb0cd0e0/WtR4PLR6ouYm-MTW3alCe.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63df091910678851bb0cd0e0/WtR4PLR6ouYm-MTW3alCe.png"></a><br>also
          I guess you know it, but you should dial down the temperature to 0.01 if
          you want more precise answers, and even with that you can still sometimes
          get a different answer if you don''t enable the cache.</p>

          <p>Partially its their fault for providing a default gradio space which
          enables to get bad answers from the model.</p>

          '
        raw: 'I don''t know what parameters you set up on the space, locally I get
          this answer:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63df091910678851bb0cd0e0/WtR4PLR6ouYm-MTW3alCe.png)

          also I guess you know it, but you should dial down the temperature to 0.01
          if you want more precise answers, and even with that you can still sometimes
          get a different answer if you don''t enable the cache.


          Partially its their fault for providing a default gradio space which enables
          to get bad answers from the model.'
        updatedAt: '2023-08-04T21:08:34.441Z'
      numEdits: 1
      reactions: []
    id: 64cd68a67a7305c589e376ca
    type: comment
  author: OzzyGT
  content: 'I don''t know what parameters you set up on the space, locally I get this
    answer:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63df091910678851bb0cd0e0/WtR4PLR6ouYm-MTW3alCe.png)

    also I guess you know it, but you should dial down the temperature to 0.01 if
    you want more precise answers, and even with that you can still sometimes get
    a different answer if you don''t enable the cache.


    Partially its their fault for providing a default gradio space which enables to
    get bad answers from the model.'
  created_at: 2023-08-04 20:07:50+00:00
  edited: true
  hidden: false
  id: 64cd68a67a7305c589e376ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-08-04T22:03:53.000Z'
    data:
      edited: true
      editors:
      - winglian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6313979625701904
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>Yeah. I get the correct answer too for this style of common sense
          question (with the default gradio settings in the space)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/USaDoYbKOLwTO6ZcIMIsq.png"><img
          alt="Screenshot 2023-08-04 at 6.02.44 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/USaDoYbKOLwTO6ZcIMIsq.png"></a></p>

          '
        raw: 'Yeah. I get the correct answer too for this style of common sense question
          (with the default gradio settings in the space)


          ![Screenshot 2023-08-04 at 6.02.44 PM.png](https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/USaDoYbKOLwTO6ZcIMIsq.png)

          '
        updatedAt: '2023-08-04T22:04:18.976Z'
      numEdits: 1
      reactions: []
    id: 64cd75c90b2ff2e916e8ddb6
    type: comment
  author: winglian
  content: 'Yeah. I get the correct answer too for this style of common sense question
    (with the default gradio settings in the space)


    ![Screenshot 2023-08-04 at 6.02.44 PM.png](https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/USaDoYbKOLwTO6ZcIMIsq.png)

    '
  created_at: 2023-08-04 21:03:53+00:00
  edited: true
  hidden: false
  id: 64cd75c90b2ff2e916e8ddb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-08-04T22:08:42.000Z'
    data:
      status: closed
    id: 64cd76ea4dfd5df707dd33f9
    type: status-change
  author: winglian
  created_at: 2023-08-04 21:08:42+00:00
  id: 64cd76ea4dfd5df707dd33f9
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0495e2cca93eab7412ab00700e9cf12d.svg
      fullname: micheal65536
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: micheal65536
      type: user
    createdAt: '2023-08-05T03:39:30.000Z'
    data:
      edited: true
      editors:
      - micheal65536
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9254661202430725
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0495e2cca93eab7412ab00700e9cf12d.svg
          fullname: micheal65536
          isHf: false
          isPro: false
          name: micheal65536
          type: user
        html: "<p>I am getting the same/similar results here as <span data-props=\"\
          {&quot;user&quot;:&quot;anon7463435254&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/anon7463435254\">@<span class=\"underline\"\
          >anon7463435254</span></a></span>\n\n\t</span></span> . Additionally, the\
          \ model seems to perform somewhat worse at coding tasks compared to other\
          \ models that I have tried (LLaMa 2 13B chat, WizardLM 13B v1.2).</p>\n\
          <p>I have tried context length of both 2048 and 4096 (not 100% sure which\
          \ is correct, changing this had no effect although ofc the prompt was less\
          \ than 2048 tokens anyway). Temperature 0.8, top_p 0.95, top_k 40. Changing\
          \ temperature to 0.3 had no effect.</p>\n<p>Using prompt template <code>&lt;|user|&gt;\
          \ &lt;|user-message|&gt;&lt;|end_of_turn|&gt;&lt;|bot|&gt; &lt;|bot-message|&gt;&lt;|end_of_turn|&gt;</code>.\
          \ I also tried putting newlines between the two sides etc. (as the documentation\
          \ has a contradiction as to which exact format is correct), there was no\
          \ change. I am using the system prompt <code>You are a helpful assistant.\
          \ Please answer all questions as truthfully as possible to the best of your\
          \ knowledge and ability.</code>. Using the system prompt from the documentation\
          \ causes the model to provide a very verbose reasoning but it arrives at\
          \ the same (incorrect) answer.</p>\n<p>In a few cases it said things such\
          \ as \"2 apples (which equals 2 oranges)\" so I suspect that it is incorrectly\
          \ inferring that apples and oranges are equivalent for the purposes of the\
          \ discussion, instead of realising that the question is misleading.</p>\n\
          <p>Probably unrelated but in text-generation-webui it doesn't always correctly\
          \ end generation at the end of its turn. I am using the prompt template\
          \ as above and I have also tried adding <code>&lt;|end_of_turn|&gt;</code>\
          \ to the stop tokens on the parameters page. The model does not appear to\
          \ always output the <code>&lt;|end_of_turn|&gt;</code> text before it starts\
          \ writing as the user. Or sometimes, \"&lt;|end_of_turn|&gt;\" appears in\
          \ the actual message in the conversation and then the model continues writing\
          \ as the user instead of generation being stopped. So I suspect that either\
          \ text-generation-webui is not tokenising the <code>&lt;|end_of_turn|&gt;</code>\
          \ marker correctly or the model is actually writing <code>&lt; | end _ of\
          \ _ turn | &gt;</code> as separate characters/tokens instead of generating\
          \ a single special token.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63fd3954ed9eead590fa61a2/4koBzuUXWIjfZ81QZmlpH.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63fd3954ed9eead590fa61a2/4koBzuUXWIjfZ81QZmlpH.png\"\
          ></a></p>\n"
        raw: 'I am getting the same/similar results here as @anon7463435254 . Additionally,
          the model seems to perform somewhat worse at coding tasks compared to other
          models that I have tried (LLaMa 2 13B chat, WizardLM 13B v1.2).


          I have tried context length of both 2048 and 4096 (not 100% sure which is
          correct, changing this had no effect although ofc the prompt was less than
          2048 tokens anyway). Temperature 0.8, top_p 0.95, top_k 40. Changing temperature
          to 0.3 had no effect.


          Using prompt template `<|user|> <|user-message|><|end_of_turn|><|bot|> <|bot-message|><|end_of_turn|>`.
          I also tried putting newlines between the two sides etc. (as the documentation
          has a contradiction as to which exact format is correct), there was no change.
          I am using the system prompt `You are a helpful assistant. Please answer
          all questions as truthfully as possible to the best of your knowledge and
          ability.`. Using the system prompt from the documentation causes the model
          to provide a very verbose reasoning but it arrives at the same (incorrect)
          answer.


          In a few cases it said things such as "2 apples (which equals 2 oranges)"
          so I suspect that it is incorrectly inferring that apples and oranges are
          equivalent for the purposes of the discussion, instead of realising that
          the question is misleading.


          Probably unrelated but in text-generation-webui it doesn''t always correctly
          end generation at the end of its turn. I am using the prompt template as
          above and I have also tried adding `<|end_of_turn|>` to the stop tokens
          on the parameters page. The model does not appear to always output the `<|end_of_turn|>`
          text before it starts writing as the user. Or sometimes, "<|end_of_turn|>"
          appears in the actual message in the conversation and then the model continues
          writing as the user instead of generation being stopped. So I suspect that
          either text-generation-webui is not tokenising the `<|end_of_turn|>` marker
          correctly or the model is actually writing `< | end _ of _ turn | >` as
          separate characters/tokens instead of generating a single special token.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63fd3954ed9eead590fa61a2/4koBzuUXWIjfZ81QZmlpH.png)

          '
        updatedAt: '2023-08-05T03:39:58.375Z'
      numEdits: 1
      reactions: []
    id: 64cdc4727d8cc9f070df727d
    type: comment
  author: micheal65536
  content: 'I am getting the same/similar results here as @anon7463435254 . Additionally,
    the model seems to perform somewhat worse at coding tasks compared to other models
    that I have tried (LLaMa 2 13B chat, WizardLM 13B v1.2).


    I have tried context length of both 2048 and 4096 (not 100% sure which is correct,
    changing this had no effect although ofc the prompt was less than 2048 tokens
    anyway). Temperature 0.8, top_p 0.95, top_k 40. Changing temperature to 0.3 had
    no effect.


    Using prompt template `<|user|> <|user-message|><|end_of_turn|><|bot|> <|bot-message|><|end_of_turn|>`.
    I also tried putting newlines between the two sides etc. (as the documentation
    has a contradiction as to which exact format is correct), there was no change.
    I am using the system prompt `You are a helpful assistant. Please answer all questions
    as truthfully as possible to the best of your knowledge and ability.`. Using the
    system prompt from the documentation causes the model to provide a very verbose
    reasoning but it arrives at the same (incorrect) answer.


    In a few cases it said things such as "2 apples (which equals 2 oranges)" so I
    suspect that it is incorrectly inferring that apples and oranges are equivalent
    for the purposes of the discussion, instead of realising that the question is
    misleading.


    Probably unrelated but in text-generation-webui it doesn''t always correctly end
    generation at the end of its turn. I am using the prompt template as above and
    I have also tried adding `<|end_of_turn|>` to the stop tokens on the parameters
    page. The model does not appear to always output the `<|end_of_turn|>` text before
    it starts writing as the user. Or sometimes, "<|end_of_turn|>" appears in the
    actual message in the conversation and then the model continues writing as the
    user instead of generation being stopped. So I suspect that either text-generation-webui
    is not tokenising the `<|end_of_turn|>` marker correctly or the model is actually
    writing `< | end _ of _ turn | >` as separate characters/tokens instead of generating
    a single special token.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63fd3954ed9eead590fa61a2/4koBzuUXWIjfZ81QZmlpH.png)

    '
  created_at: 2023-08-05 02:39:30+00:00
  edited: true
  hidden: false
  id: 64cdc4727d8cc9f070df727d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Open-Orca/OpenOrcaxOpenChat-Preview2-13B
repo_type: model
status: closed
target_branch: null
title: Unreliable Benchmarks. Definitely worse than LLaMA2-13b
