!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wiccanmind
conflicting_files: null
created_at: 2023-08-08 09:45:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
      fullname: "D\u0169ng Ph\xF9ng "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiccanmind
      type: user
    createdAt: '2023-08-08T10:45:17.000Z'
    data:
      edited: false
      editors:
      - wiccanmind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8250626921653748
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
          fullname: "D\u0169ng Ph\xF9ng "
          isHf: false
          isPro: false
          name: wiccanmind
          type: user
        html: '<p>First of all, thank you for your valuable contribution with this
          amazing model.<br>I''m currently experiencing an issue regarding the inference
          latency between two types of data. I''ve attempted to load the model using
          the from_pretrained method as shown below on 2 Titan RTX 24G GPUs:<br>model
          = AutoModelForCausalLM.from_pretrained(<br>            BASE_MODEL,<br>            torch_dtype=torch.bfloat16,<br>            #load_in_8bit
          = True,<br>            device_map="auto",<br>            )<br>The problem
          arises when I utilize the load_in_8bit parameter on just 1 GPU; the inference
          time with the same configuration seems to be roughly twice as long compared
          to loading with bf16. To be more specific:</p>

          <ul>

          <li>Original bf16 Time Cost: 10.947212934494019 s</li>

          <li>load_in_8bit Time Cost: 26.15874981880188 s<br>Of course, in both scenarios,
          the performance appears to be similar. Could you please help me understand
          where I might be mistaken?</li>

          </ul>

          '
        raw: "First of all, thank you for your valuable contribution with this amazing\
          \ model. \r\nI'm currently experiencing an issue regarding the inference\
          \ latency between two types of data. I've attempted to load the model using\
          \ the from_pretrained method as shown below on 2 Titan RTX 24G GPUs:\r\n\
          model = AutoModelForCausalLM.from_pretrained(\r\n            BASE_MODEL,\r\
          \n            torch_dtype=torch.bfloat16,\r\n            #load_in_8bit =\
          \ True,\r\n            device_map=\"auto\",\r\n            )\r\nThe problem\
          \ arises when I utilize the load_in_8bit parameter on just 1 GPU; the inference\
          \ time with the same configuration seems to be roughly twice as long compared\
          \ to loading with bf16. To be more specific:\r\n- Original bf16 Time Cost:\
          \ 10.947212934494019 s\r\n- load_in_8bit Time Cost: 26.15874981880188 s\r\
          \nOf course, in both scenarios, the performance appears to be similar. Could\
          \ you please help me understand where I might be mistaken?"
        updatedAt: '2023-08-08T10:45:17.122Z'
      numEdits: 0
      reactions: []
    id: 64d21cbdd64dade6702d65a8
    type: comment
  author: wiccanmind
  content: "First of all, thank you for your valuable contribution with this amazing\
    \ model. \r\nI'm currently experiencing an issue regarding the inference latency\
    \ between two types of data. I've attempted to load the model using the from_pretrained\
    \ method as shown below on 2 Titan RTX 24G GPUs:\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n            BASE_MODEL,\r\n            torch_dtype=torch.bfloat16,\r\n     \
    \       #load_in_8bit = True,\r\n            device_map=\"auto\",\r\n        \
    \    )\r\nThe problem arises when I utilize the load_in_8bit parameter on just\
    \ 1 GPU; the inference time with the same configuration seems to be roughly twice\
    \ as long compared to loading with bf16. To be more specific:\r\n- Original bf16\
    \ Time Cost: 10.947212934494019 s\r\n- load_in_8bit Time Cost: 26.15874981880188\
    \ s\r\nOf course, in both scenarios, the performance appears to be similar. Could\
    \ you please help me understand where I might be mistaken?"
  created_at: 2023-08-08 09:45:17+00:00
  edited: false
  hidden: false
  id: 64d21cbdd64dade6702d65a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
      fullname: Thorold Tronrud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ttronrud
      type: user
    createdAt: '2023-08-08T22:10:17.000Z'
    data:
      edited: false
      editors:
      - ttronrud
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8998145461082458
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64399cb9d45e8db3e28877c4/qtmSVmgH86jDkPBYVZ7B-.jpeg?w=200&h=200&f=face
          fullname: Thorold Tronrud
          isHf: false
          isPro: false
          name: ttronrud
          type: user
        html: '<p><a href="https://huggingface.co/blog/hf-bitsandbytes-integration#is-it-faster-than-native-models">LLM.Int8
          is substantially slower than bf16 at this model scale.</a> If you really
          want to prioritize inference speed with low VRAM usage, you could try 4-bit,
          which lies somewhere in between in terms of inference speed.</p>

          '
        raw: <a href="https://huggingface.co/blog/hf-bitsandbytes-integration#is-it-faster-than-native-models">LLM.Int8
          is substantially slower than bf16 at this model scale.</a> If you really
          want to prioritize inference speed with low VRAM usage, you could try 4-bit,
          which lies somewhere in between in terms of inference speed.
        updatedAt: '2023-08-08T22:10:17.339Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - wiccanmind
    id: 64d2bd4991aa218ee2fca1cf
    type: comment
  author: ttronrud
  content: <a href="https://huggingface.co/blog/hf-bitsandbytes-integration#is-it-faster-than-native-models">LLM.Int8
    is substantially slower than bf16 at this model scale.</a> If you really want
    to prioritize inference speed with low VRAM usage, you could try 4-bit, which
    lies somewhere in between in terms of inference speed.
  created_at: 2023-08-08 21:10:17+00:00
  edited: false
  hidden: false
  id: 64d2bd4991aa218ee2fca1cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
      fullname: "D\u0169ng Ph\xF9ng "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wiccanmind
      type: user
    createdAt: '2023-08-09T03:32:29.000Z'
    data:
      edited: false
      editors:
      - wiccanmind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7926148772239685
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64250ae54191f56e07339dd7/KVYP9MlmVIiH61jg9e2_r.jpeg?w=200&h=200&f=face
          fullname: "D\u0169ng Ph\xF9ng "
          isHf: false
          isPro: false
          name: wiccanmind
          type: user
        html: '<p>I have tried running inference with 4-bit Quantization (<a href="https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ">
          this model</a>), but the time cost is even higher (75s) with the same generated
          configuration and prompt. The model only utilizes around 11GB of GPU memory.
          I''m using AutoGPTQForCausalLM as shown below, following the instructions
          from <a href="https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ#how-to-use-this-gptq-model-from-python-code">this</a><br>from
          auto_gptq import AutoGPTQForCausalLM<br>model = AutoGPTQForCausalLM.from_quantized(<br>                            "TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ",<br>                            use_safetensors=True,<br>                            trust_remote_code=False,<br>                            device=''cuda:0'',<br>                            use_triton=False,<br>                            quantize_config=None)<br>Could
          this be an issue with the library, the model itself, or the GPU architecture?
          I haven''t found any similar questions, could you possibly suggest a solution?
          Thank you so much.</p>

          '
        raw: "I have tried running inference with 4-bit Quantization ([ this model](https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ)),\
          \ but the time cost is even higher (75s) with the same generated configuration\
          \ and prompt. The model only utilizes around 11GB of GPU memory. I'm using\
          \ AutoGPTQForCausalLM as shown below, following the instructions from [this](https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ#how-to-use-this-gptq-model-from-python-code)\n\
          from auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
          \                            \"TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ\"\
          ,\n                            use_safetensors=True,\n                 \
          \           trust_remote_code=False,\n                            device='cuda:0',\n\
          \                            use_triton=False,\n                       \
          \     quantize_config=None)\nCould this be an issue with the library, the\
          \ model itself, or the GPU architecture? I haven't found any similar questions,\
          \ could you possibly suggest a solution? Thank you so much."
        updatedAt: '2023-08-09T03:32:29.439Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - clemence-mw
    id: 64d308cd7f387061b8b0ee75
    type: comment
  author: wiccanmind
  content: "I have tried running inference with 4-bit Quantization ([ this model](https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ)),\
    \ but the time cost is even higher (75s) with the same generated configuration\
    \ and prompt. The model only utilizes around 11GB of GPU memory. I'm using AutoGPTQForCausalLM\
    \ as shown below, following the instructions from [this](https://huggingface.co/TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ#how-to-use-this-gptq-model-from-python-code)\n\
    from auto_gptq import AutoGPTQForCausalLM\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
    \                            \"TheBloke/OpenOrcaxOpenChat-Preview2-13B-GPTQ\"\
    ,\n                            use_safetensors=True,\n                       \
    \     trust_remote_code=False,\n                            device='cuda:0',\n\
    \                            use_triton=False,\n                            quantize_config=None)\n\
    Could this be an issue with the library, the model itself, or the GPU architecture?\
    \ I haven't found any similar questions, could you possibly suggest a solution?\
    \ Thank you so much."
  created_at: 2023-08-09 02:32:29+00:00
  edited: false
  hidden: false
  id: 64d308cd7f387061b8b0ee75
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Open-Orca/OpenOrcaxOpenChat-Preview2-13B
repo_type: model
status: open
target_branch: null
title: Time complexity
