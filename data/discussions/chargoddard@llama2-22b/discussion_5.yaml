!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aria256
conflicting_files: null
created_at: 2023-07-29 04:20:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/107702f00549d67cb46946a5b7861609.svg
      fullname: kawakami shinji
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aria256
      type: user
    createdAt: '2023-07-29T05:20:00.000Z'
    data:
      edited: false
      editors:
      - aria256
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5332019925117493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/107702f00549d67cb46946a5b7861609.svg
          fullname: kawakami shinji
          isHf: false
          isPro: false
          name: aria256
          type: user
        html: "<p>Is this correct in the source code?</p>\n<p>#!/usr/bin/env python3</p>\n\
          <h1 id=\"charles-o-goddard\">Charles O. Goddard</h1>\n<h1 id=\"7202023\"\
          >7/20/2023</h1>\n<p>\"\"\"Script used to generate the base frankenmerge.\
          \ Output will need fine-tuning to be useful.\"\"\"</p>\n<p>import copy<br>import\
          \ torch<br>from torch import Tensor, nn<br>import transformers</p>\n<p>from\
          \ transformers.models.gpt_neox.modeling_gpt_neox import (<br>    GPTNeoXForCausalLM,<br>\
          \    GPTNeoXLayer,<br>)<br>from transformers import GPTNeoXForCausalLM,GPTNeoXConfig</p>\n\
          <p>import torch<br>import transformers<br>import numpy as np</p>\n<p>MODEL_NAME_13B\
          \ = \"./japanese-gpt-neox-3.6b\"  # primary model<br>MODEL_NAME_33B = \"\
          ./gpt-neox-20b\"  # donor<br>BLOCK_DIAGONAL = True</p>\n<h1 id=\"if-block_diagonal-is-set-to-true-each-tensor-in-the-resultant-model-will-form-a\"\
          >If BLOCK_DIAGONAL is set to True, each tensor in the resultant model will\
          \ form a</h1>\n<h1 id=\"block-diagonal-matrix-as-illustrated-below\">block\
          \ diagonal matrix, as illustrated below:</h1>\n<h1 id=\"a-a-a-0-0\">a a\
          \ a 0 0</h1>\n<h1 id=\"a-a-a-0-0-1\">a a a 0 0</h1>\n<h1 id=\"a-a-a-0-0-2\"\
          >a a a 0 0</h1>\n<h1 id=\"0-0-0-b-b\">0 0 0 b b</h1>\n<h1 id=\"0-0-0-b-b-1\"\
          >0 0 0 b b</h1>\n<h1 id=\"in-this-configuration-the-states-hidden-and-intermediate-from-the-original\"\
          >In this configuration, the states (hidden and intermediate) from the original</h1>\n\
          <h1 id=\"and-donor-models-are-completely-decoupled-that-is-the-hidden-states\"\
          >and donor models are completely decoupled. That is, the hidden states</h1>\n\
          <h1 id=\"corresponding-to-the-original-model-remain-unchanged-and-the-new-dimensions\"\
          >corresponding to the original model remain unchanged, and the new dimensions</h1>\n\
          <h1 id=\"added-from-the-donor-model-do-not-depend-on-the-hidden-states-of-the-original-model\"\
          >added from the donor model do not depend on the hidden states of the original\
          \ model.</h1>\n<h1 id=\"if-block_diagonal-is-set-to-false-the-tensors-will-instead-have-the-following-form\"\
          >If BLOCK_DIAGONAL is set to False, the tensors will instead have the following\
          \ form:</h1>\n<h1 id=\"a-a-a-0-0-3\">a a a 0 0</h1>\n<h1 id=\"a-a-a-0-0-4\"\
          >a a a 0 0</h1>\n<h1 id=\"a-a-a-0-0-5\">a a a 0 0</h1>\n<h1 id=\"b-b-b-b-b\"\
          >b b b b b</h1>\n<h1 id=\"b-b-b-b-b-1\">b b b b b</h1>\n<h1 id=\"in-this-case-the-output-of-the-newly-added-attention-heads-depends-on-the-hidden\"\
          >In this case, the output of the newly added attention heads depends on\
          \ the hidden</h1>\n<h1 id=\"state-values-as-if-they-were-part-of-the-donor-model-although-the-original-models\"\
          >state values as if they were part of the donor model. Although the original\
          \ model's</h1>\n<h1 id=\"hidden-states-remain-unchanged-in-either-case-interaction-between-the-new-and-old\"\
          >hidden states remain unchanged in either case, interaction between the\
          \ new and old</h1>\n<h1 id=\"features-will-result-in-features-of-varying-usefulness\"\
          >features will result in features of varying usefulness.</h1>\n<p>class\
          \ NoInit:<br>    def <strong>enter</strong>(self):<br>        def noop(*args,\
          \ **kwargs):<br>            pass</p>\n<pre><code>    (k, u, n) = (\n   \
          \     torch.nn.init.kaiming_uniform_,\n        torch.nn.init.uniform_,\n\
          \        torch.nn.init.normal_,\n    )\n    torch.nn.init.kaiming_uniform_\
          \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ =\
          \ noop\n\n    transformers.modeling_utils._init_weights = False\n    self.funcs\
          \ = (k, u, n)\n\ndef __exit__(self, *args):\n    (k, u, n) = self.funcs\n\
          \    (\n        torch.nn.init.kaiming_uniform_,\n        torch.nn.init.uniform_,\n\
          \        torch.nn.init.normal_,\n    ) = (\n        k,\n        u,\n   \
          \     n,\n    )\n    transformers.modeling_utils._init_weights = True\n\
          </code></pre>\n<p>def format_kmb(n, digits=None):<br>    n = int(n)<br>\
          \    if n &lt; 1000:<br>        return str(n)<br>    elif n &lt; 1000_000:<br>\
          \        return f\"{round(n/1000, digits)}k\"<br>    elif n &lt; 1000 *\
          \ 1000 * 1000:<br>        return f\"{round(n/(1000<em>1000), digits)}m\"\
          <br>    else:<br>        return f\"{round(n/(1000</em>1000*1000), digits)}b\"\
          </p>\n<p>def convert_size(size, unit=\"B\"):<br>    units = (\"B\", \"KB\"\
          , \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\")<br>    size = int(size)<br>\
          \    i = units.index(unit.upper())<br>    size = round(size / 1024 ** i,\
          \ 2)</p>\n<pre><code>return f\"{size} {units[i]}\"\n</code></pre>\n<p>def\
          \ count_params(model):<br>    model_parameters = filter(lambda p: p.requires_grad,\
          \ model.parameters())<br>    params = sum([np.prod(p.size()) for p in model_parameters])<br>\
          \    return int(params)</p>\n<p>torch.set_default_dtype(torch.float16)</p>\n\
          <p>config_13b: GPTNeoXConfig = GPTNeoXConfig.from_pretrained(MODEL_NAME_13B)<br>config_33b:\
          \ GPTNeoXConfig = GPTNeoXConfig.from_pretrained(MODEL_NAME_33B)<br>config_more\
          \ = copy.deepcopy(config_13b)<br>config_more.intermediate_size = config_33b.intermediate_size<br>config_more.hidden_size\
          \ = config_33b.hidden_size<br>#config_more.num_key_value_heads = config_33b.num_key_value_heads<br>#config_more.num_attention_heads\
          \ = config_33b.num_key_value_heads<br>config_more.num_attention_heads =\
          \ config_33b.num_attention_heads</p>\n<p>print(config_more)</p>\n<p>with\
          \ NoInit():<br>    model = GPTNeoXForCausalLM(config_more)</p>\n<pre><code>print(f\"\
          {format_kmb(count_params(model), 3)} parameters\")\n</code></pre>\n<p>def\
          \ merge_tensors_inplace(dest: Tensor, s0: Tensor, s1: Tensor, block_diagonal:\
          \ bool):<br>    dest.zero_()<br>    if block_diagonal:<br>        dest[s0.shape[0]\
          \ :, s0.shape[1] :] = s1[<br>            s0.shape[0]  : dest.shape[0],<br>\
          \            s0.shape[1]  : dest.shape[1],<br>        ]<br>    else:<br>\
          \        dest[s0.shape[0] :, :]= s1[<br>            s0.shape[0] : dest.shape[0],<br>\
          \            : dest.shape[1],<br>        ]<br>    dest[: s0.shape[0], :\
          \ s0.shape[1]] = s0</p>\n<p>with NoInit():<br>    donor_13b = (<br>    \
          \    GPTNeoXForCausalLM.from_pretrained(MODEL_NAME_13B).to(torch.float16).eval()<br>\
          \    )<br>    print(donor_13b)<br>    donor_33b = (<br>        GPTNeoXForCausalLM.from_pretrained(MODEL_NAME_33B).to(torch.float16).eval()<br>\
          \    )</p>\n<p>with torch.no_grad():<br>    for layer_idx in range(len(model.gpt_neox.layers)):<br>\
          \        layer: GPTNeoXLayer = model.gpt_neox.layers[layer_idx]<br>    \
          \    l13: GPTNeoXLayer = donor_13b.gpt_neox.layers[layer_idx]<br>      \
          \  l33: GPTNeoXLayer = donor_33b.gpt_neox.layers[layer_idx]</p>\n<pre><code>\
          \    dest: nn.Linear = getattr(layer.attention, 'query_key_value')\n   \
          \ s13: nn.Linear = getattr(l13.attention, 'query_key_value')\n    s33: nn.Linear\
          \ = getattr(l33.attention, 'query_key_value')\n    dest: nn.Linear = getattr(layer.attention,\
          \ 'dense')\n    s13: nn.Linear = getattr(l13.attention, 'dense')\n    s33:\
          \ nn.Linear = getattr(l33.attention, 'dense')\n    merge_tensors_inplace(dest.weight,\
          \ s13.weight, s33.weight, BLOCK_DIAGONAL)\n\n\n    dest: nn.Linear = getattr(layer.mlp,\
          \ 'dense_h_to_4h')\n    s13: nn.Linear = getattr(l13.mlp, 'dense_h_to_4h')\n\
          \    s33: nn.Linear = getattr(l33.mlp, 'dense_h_to_4h')\n    dest: nn.Linear\
          \ = getattr(layer.mlp, 'dense_4h_to_h')\n    s13: nn.Linear = getattr(l13.mlp,\
          \ 'dense_4h_to_h')\n    s33: nn.Linear = getattr(l33.mlp, 'dense_4h_to_h')\n\
          \    merge_tensors_inplace(dest.weight, s13.weight, s33.weight, BLOCK_DIAGONAL)\n\
          \n    layer.input_layernorm.weight[:] = l33.input_layernorm.weight[\n  \
          \      : layer.input_layernorm.weight.shape[0]\n    ]\n    layer.input_layernorm.weight[\n\
          \        : l13.input_layernorm.weight.shape[0]\n    ] = l13.input_layernorm.weight\n\
          \    layer.post_attention_layernorm.weight[:] = l33.post_attention_layernorm.weight[\n\
          \        : layer.post_attention_layernorm.weight.shape[0]\n    ]\n    layer.post_attention_layernorm.weight[\n\
          \        : l13.post_attention_layernorm.weight.shape[0]\n    ] = l13.post_attention_layernorm.weight\n\
          \n# have initial output depend on only original llama2-13b features, so\
          \ model\n# starts unimpaired and can learn to incorporate the new features\
          \ as well\n\nmodel.embed_out.weight.zero_()\nmodel.embed_out.weight[\n \
          \   : donor_13b.embed_out.weight.shape[0], : donor_13b.embed_out.weight.shape[1]\n\
          ] = donor_13b.embed_out.weight\n\nmerge_tensors_inplace(\n    model.gpt_neox.embed_in.weight,\n\
          \    donor_13b.gpt_neox.embed_in.weight,\n    donor_33b.gpt_neox.embed_in.weight,\n\
          \    BLOCK_DIAGONAL,\n)\n</code></pre>\n<p>model.save_pretrained(\"D:\\\
          llm_model\\gpt-neox-20b_\", safe_serialization=True)</p>\n"
        raw: "Is this correct in the source code?\r\n\r\n#!/usr/bin/env python3\r\n\
          # Charles O. Goddard\r\n# 7/20/2023\r\n\"\"\"Script used to generate the\
          \ base frankenmerge. Output will need fine-tuning to be useful.\"\"\"\r\n\
          \r\nimport copy\r\nimport torch\r\nfrom torch import Tensor, nn\r\nimport\
          \ transformers\r\n\r\nfrom transformers.models.gpt_neox.modeling_gpt_neox\
          \ import (\r\n    GPTNeoXForCausalLM,\r\n    GPTNeoXLayer,\r\n)\r\nfrom\
          \ transformers import GPTNeoXForCausalLM,GPTNeoXConfig\r\n\r\nimport torch\r\
          \nimport transformers\r\nimport numpy as np\r\n\r\n\r\nMODEL_NAME_13B =\
          \ \"./japanese-gpt-neox-3.6b\"  # primary model\r\nMODEL_NAME_33B = \"./gpt-neox-20b\"\
          \  # donor\r\nBLOCK_DIAGONAL = True\r\n# If BLOCK_DIAGONAL is set to True,\
          \ each tensor in the resultant model will form a\r\n# block diagonal matrix,\
          \ as illustrated below:\r\n# a a a 0 0\r\n# a a a 0 0\r\n# a a a 0 0\r\n\
          # 0 0 0 b b\r\n# 0 0 0 b b\r\n\r\n# In this configuration, the states (hidden\
          \ and intermediate) from the original \r\n# and donor models are completely\
          \ decoupled. That is, the hidden states\r\n# corresponding to the original\
          \ model remain unchanged, and the new dimensions \r\n# added from the donor\
          \ model do not depend on the hidden states of the original model.\r\n\r\n\
          # If BLOCK_DIAGONAL is set to False, the tensors will instead have the following\
          \ form:\r\n\r\n# a a a 0 0\r\n# a a a 0 0\r\n# a a a 0 0\r\n# b b b b b\r\
          \n# b b b b b\r\n\r\n# In this case, the output of the newly added attention\
          \ heads depends on the hidden \r\n# state values as if they were part of\
          \ the donor model. Although the original model's\r\n# hidden states remain\
          \ unchanged in either case, interaction between the new and old\r\n# features\
          \ will result in features of varying usefulness.\r\n\r\nclass NoInit:\r\n\
          \    def __enter__(self):\r\n        def noop(*args, **kwargs):\r\n    \
          \        pass\r\n\r\n        (k, u, n) = (\r\n            torch.nn.init.kaiming_uniform_,\r\
          \n            torch.nn.init.uniform_,\r\n            torch.nn.init.normal_,\r\
          \n        )\r\n        torch.nn.init.kaiming_uniform_ = noop\r\n       \
          \ torch.nn.init.uniform_ = noop\r\n        torch.nn.init.normal_ = noop\r\
          \n\r\n        transformers.modeling_utils._init_weights = False\r\n    \
          \    self.funcs = (k, u, n)\r\n\r\n    def __exit__(self, *args):\r\n  \
          \      (k, u, n) = self.funcs\r\n        (\r\n            torch.nn.init.kaiming_uniform_,\r\
          \n            torch.nn.init.uniform_,\r\n            torch.nn.init.normal_,\r\
          \n        ) = (\r\n            k,\r\n            u,\r\n            n,\r\n\
          \        )\r\n        transformers.modeling_utils._init_weights = True\r\
          \n\r\ndef format_kmb(n, digits=None):\r\n    n = int(n)\r\n    if n < 1000:\r\
          \n        return str(n)\r\n    elif n < 1000_000:\r\n        return f\"\
          {round(n/1000, digits)}k\"\r\n    elif n < 1000 * 1000 * 1000:\r\n     \
          \   return f\"{round(n/(1000*1000), digits)}m\"\r\n    else:\r\n       \
          \ return f\"{round(n/(1000*1000*1000), digits)}b\"\r\n    \r\ndef convert_size(size,\
          \ unit=\"B\"):\r\n    units = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"\
          PB\", \"EB\", \"ZB\")\r\n    size = int(size)\r\n    i = units.index(unit.upper())\r\
          \n    size = round(size / 1024 ** i, 2)\r\n\r\n    return f\"{size} {units[i]}\"\
          \r\n\r\ndef count_params(model):\r\n    model_parameters = filter(lambda\
          \ p: p.requires_grad, model.parameters())\r\n    params = sum([np.prod(p.size())\
          \ for p in model_parameters])\r\n    return int(params)\r\n\r\n\r\ntorch.set_default_dtype(torch.float16)\r\
          \n\r\nconfig_13b: GPTNeoXConfig = GPTNeoXConfig.from_pretrained(MODEL_NAME_13B)\r\
          \nconfig_33b: GPTNeoXConfig = GPTNeoXConfig.from_pretrained(MODEL_NAME_33B)\r\
          \nconfig_more = copy.deepcopy(config_13b)\r\nconfig_more.intermediate_size\
          \ = config_33b.intermediate_size\r\nconfig_more.hidden_size = config_33b.hidden_size\r\
          \n#config_more.num_key_value_heads = config_33b.num_key_value_heads\r\n\
          #config_more.num_attention_heads = config_33b.num_key_value_heads\r\nconfig_more.num_attention_heads\
          \ = config_33b.num_attention_heads\r\n\r\nprint(config_more)\r\n\r\nwith\
          \ NoInit():\r\n    model = GPTNeoXForCausalLM(config_more)\r\n\r\n    print(f\"\
          {format_kmb(count_params(model), 3)} parameters\")\r\n\r\n\r\n\r\ndef merge_tensors_inplace(dest:\
          \ Tensor, s0: Tensor, s1: Tensor, block_diagonal: bool):\r\n    dest.zero_()\r\
          \n    if block_diagonal:\r\n        dest[s0.shape[0] :, s0.shape[1] :] =\
          \ s1[\r\n            s0.shape[0]  : dest.shape[0],\r\n            s0.shape[1]\
          \  : dest.shape[1],\r\n        ]\r\n    else:\r\n        dest[s0.shape[0]\
          \ :, :]= s1[\r\n            s0.shape[0] : dest.shape[0],\r\n           \
          \ : dest.shape[1],\r\n        ]\r\n    dest[: s0.shape[0], : s0.shape[1]]\
          \ = s0\r\n\r\n\r\nwith NoInit():\r\n    donor_13b = (\r\n        GPTNeoXForCausalLM.from_pretrained(MODEL_NAME_13B).to(torch.float16).eval()\r\
          \n    )\r\n    print(donor_13b)\r\n    donor_33b = (\r\n        GPTNeoXForCausalLM.from_pretrained(MODEL_NAME_33B).to(torch.float16).eval()\r\
          \n    )\r\n\r\nwith torch.no_grad():\r\n    for layer_idx in range(len(model.gpt_neox.layers)):\r\
          \n        layer: GPTNeoXLayer = model.gpt_neox.layers[layer_idx]\r\n   \
          \     l13: GPTNeoXLayer = donor_13b.gpt_neox.layers[layer_idx]\r\n     \
          \   l33: GPTNeoXLayer = donor_33b.gpt_neox.layers[layer_idx]\r\n\r\n   \
          \     dest: nn.Linear = getattr(layer.attention, 'query_key_value')\r\n\
          \        s13: nn.Linear = getattr(l13.attention, 'query_key_value')\r\n\
          \        s33: nn.Linear = getattr(l33.attention, 'query_key_value')\r\n\
          \        dest: nn.Linear = getattr(layer.attention, 'dense')\r\n       \
          \ s13: nn.Linear = getattr(l13.attention, 'dense')\r\n        s33: nn.Linear\
          \ = getattr(l33.attention, 'dense')\r\n        merge_tensors_inplace(dest.weight,\
          \ s13.weight, s33.weight, BLOCK_DIAGONAL)\r\n\r\n\r\n        dest: nn.Linear\
          \ = getattr(layer.mlp, 'dense_h_to_4h')\r\n        s13: nn.Linear = getattr(l13.mlp,\
          \ 'dense_h_to_4h')\r\n        s33: nn.Linear = getattr(l33.mlp, 'dense_h_to_4h')\r\
          \n        dest: nn.Linear = getattr(layer.mlp, 'dense_4h_to_h')\r\n    \
          \    s13: nn.Linear = getattr(l13.mlp, 'dense_4h_to_h')\r\n        s33:\
          \ nn.Linear = getattr(l33.mlp, 'dense_4h_to_h')\r\n        merge_tensors_inplace(dest.weight,\
          \ s13.weight, s33.weight, BLOCK_DIAGONAL)\r\n\r\n        layer.input_layernorm.weight[:]\
          \ = l33.input_layernorm.weight[\r\n            : layer.input_layernorm.weight.shape[0]\r\
          \n        ]\r\n        layer.input_layernorm.weight[\r\n            : l13.input_layernorm.weight.shape[0]\r\
          \n        ] = l13.input_layernorm.weight\r\n        layer.post_attention_layernorm.weight[:]\
          \ = l33.post_attention_layernorm.weight[\r\n            : layer.post_attention_layernorm.weight.shape[0]\r\
          \n        ]\r\n        layer.post_attention_layernorm.weight[\r\n      \
          \      : l13.post_attention_layernorm.weight.shape[0]\r\n        ] = l13.post_attention_layernorm.weight\r\
          \n\r\n    # have initial output depend on only original llama2-13b features,\
          \ so model\r\n    # starts unimpaired and can learn to incorporate the new\
          \ features as well\r\n    \r\n    model.embed_out.weight.zero_()\r\n   \
          \ model.embed_out.weight[\r\n        : donor_13b.embed_out.weight.shape[0],\
          \ : donor_13b.embed_out.weight.shape[1]\r\n    ] = donor_13b.embed_out.weight\r\
          \n\r\n    merge_tensors_inplace(\r\n        model.gpt_neox.embed_in.weight,\r\
          \n        donor_13b.gpt_neox.embed_in.weight,\r\n        donor_33b.gpt_neox.embed_in.weight,\r\
          \n        BLOCK_DIAGONAL,\r\n    )\r\nmodel.save_pretrained(\"D:\\llm_model\\\
          gpt-neox-20b_\", safe_serialization=True)\r\n"
        updatedAt: '2023-07-29T05:20:00.007Z'
      numEdits: 0
      reactions: []
    id: 64c4a180e2e5c94bd030fc65
    type: comment
  author: aria256
  content: "Is this correct in the source code?\r\n\r\n#!/usr/bin/env python3\r\n\
    # Charles O. Goddard\r\n# 7/20/2023\r\n\"\"\"Script used to generate the base\
    \ frankenmerge. Output will need fine-tuning to be useful.\"\"\"\r\n\r\nimport\
    \ copy\r\nimport torch\r\nfrom torch import Tensor, nn\r\nimport transformers\r\
    \n\r\nfrom transformers.models.gpt_neox.modeling_gpt_neox import (\r\n    GPTNeoXForCausalLM,\r\
    \n    GPTNeoXLayer,\r\n)\r\nfrom transformers import GPTNeoXForCausalLM,GPTNeoXConfig\r\
    \n\r\nimport torch\r\nimport transformers\r\nimport numpy as np\r\n\r\n\r\nMODEL_NAME_13B\
    \ = \"./japanese-gpt-neox-3.6b\"  # primary model\r\nMODEL_NAME_33B = \"./gpt-neox-20b\"\
    \  # donor\r\nBLOCK_DIAGONAL = True\r\n# If BLOCK_DIAGONAL is set to True, each\
    \ tensor in the resultant model will form a\r\n# block diagonal matrix, as illustrated\
    \ below:\r\n# a a a 0 0\r\n# a a a 0 0\r\n# a a a 0 0\r\n# 0 0 0 b b\r\n# 0 0\
    \ 0 b b\r\n\r\n# In this configuration, the states (hidden and intermediate) from\
    \ the original \r\n# and donor models are completely decoupled. That is, the hidden\
    \ states\r\n# corresponding to the original model remain unchanged, and the new\
    \ dimensions \r\n# added from the donor model do not depend on the hidden states\
    \ of the original model.\r\n\r\n# If BLOCK_DIAGONAL is set to False, the tensors\
    \ will instead have the following form:\r\n\r\n# a a a 0 0\r\n# a a a 0 0\r\n\
    # a a a 0 0\r\n# b b b b b\r\n# b b b b b\r\n\r\n# In this case, the output of\
    \ the newly added attention heads depends on the hidden \r\n# state values as\
    \ if they were part of the donor model. Although the original model's\r\n# hidden\
    \ states remain unchanged in either case, interaction between the new and old\r\
    \n# features will result in features of varying usefulness.\r\n\r\nclass NoInit:\r\
    \n    def __enter__(self):\r\n        def noop(*args, **kwargs):\r\n         \
    \   pass\r\n\r\n        (k, u, n) = (\r\n            torch.nn.init.kaiming_uniform_,\r\
    \n            torch.nn.init.uniform_,\r\n            torch.nn.init.normal_,\r\n\
    \        )\r\n        torch.nn.init.kaiming_uniform_ = noop\r\n        torch.nn.init.uniform_\
    \ = noop\r\n        torch.nn.init.normal_ = noop\r\n\r\n        transformers.modeling_utils._init_weights\
    \ = False\r\n        self.funcs = (k, u, n)\r\n\r\n    def __exit__(self, *args):\r\
    \n        (k, u, n) = self.funcs\r\n        (\r\n            torch.nn.init.kaiming_uniform_,\r\
    \n            torch.nn.init.uniform_,\r\n            torch.nn.init.normal_,\r\n\
    \        ) = (\r\n            k,\r\n            u,\r\n            n,\r\n     \
    \   )\r\n        transformers.modeling_utils._init_weights = True\r\n\r\ndef format_kmb(n,\
    \ digits=None):\r\n    n = int(n)\r\n    if n < 1000:\r\n        return str(n)\r\
    \n    elif n < 1000_000:\r\n        return f\"{round(n/1000, digits)}k\"\r\n \
    \   elif n < 1000 * 1000 * 1000:\r\n        return f\"{round(n/(1000*1000), digits)}m\"\
    \r\n    else:\r\n        return f\"{round(n/(1000*1000*1000), digits)}b\"\r\n\
    \    \r\ndef convert_size(size, unit=\"B\"):\r\n    units = (\"B\", \"KB\", \"\
    MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\")\r\n    size = int(size)\r\n    i\
    \ = units.index(unit.upper())\r\n    size = round(size / 1024 ** i, 2)\r\n\r\n\
    \    return f\"{size} {units[i]}\"\r\n\r\ndef count_params(model):\r\n    model_parameters\
    \ = filter(lambda p: p.requires_grad, model.parameters())\r\n    params = sum([np.prod(p.size())\
    \ for p in model_parameters])\r\n    return int(params)\r\n\r\n\r\ntorch.set_default_dtype(torch.float16)\r\
    \n\r\nconfig_13b: GPTNeoXConfig = GPTNeoXConfig.from_pretrained(MODEL_NAME_13B)\r\
    \nconfig_33b: GPTNeoXConfig = GPTNeoXConfig.from_pretrained(MODEL_NAME_33B)\r\n\
    config_more = copy.deepcopy(config_13b)\r\nconfig_more.intermediate_size = config_33b.intermediate_size\r\
    \nconfig_more.hidden_size = config_33b.hidden_size\r\n#config_more.num_key_value_heads\
    \ = config_33b.num_key_value_heads\r\n#config_more.num_attention_heads = config_33b.num_key_value_heads\r\
    \nconfig_more.num_attention_heads = config_33b.num_attention_heads\r\n\r\nprint(config_more)\r\
    \n\r\nwith NoInit():\r\n    model = GPTNeoXForCausalLM(config_more)\r\n\r\n  \
    \  print(f\"{format_kmb(count_params(model), 3)} parameters\")\r\n\r\n\r\n\r\n\
    def merge_tensors_inplace(dest: Tensor, s0: Tensor, s1: Tensor, block_diagonal:\
    \ bool):\r\n    dest.zero_()\r\n    if block_diagonal:\r\n        dest[s0.shape[0]\
    \ :, s0.shape[1] :] = s1[\r\n            s0.shape[0]  : dest.shape[0],\r\n   \
    \         s0.shape[1]  : dest.shape[1],\r\n        ]\r\n    else:\r\n        dest[s0.shape[0]\
    \ :, :]= s1[\r\n            s0.shape[0] : dest.shape[0],\r\n            : dest.shape[1],\r\
    \n        ]\r\n    dest[: s0.shape[0], : s0.shape[1]] = s0\r\n\r\n\r\nwith NoInit():\r\
    \n    donor_13b = (\r\n        GPTNeoXForCausalLM.from_pretrained(MODEL_NAME_13B).to(torch.float16).eval()\r\
    \n    )\r\n    print(donor_13b)\r\n    donor_33b = (\r\n        GPTNeoXForCausalLM.from_pretrained(MODEL_NAME_33B).to(torch.float16).eval()\r\
    \n    )\r\n\r\nwith torch.no_grad():\r\n    for layer_idx in range(len(model.gpt_neox.layers)):\r\
    \n        layer: GPTNeoXLayer = model.gpt_neox.layers[layer_idx]\r\n        l13:\
    \ GPTNeoXLayer = donor_13b.gpt_neox.layers[layer_idx]\r\n        l33: GPTNeoXLayer\
    \ = donor_33b.gpt_neox.layers[layer_idx]\r\n\r\n        dest: nn.Linear = getattr(layer.attention,\
    \ 'query_key_value')\r\n        s13: nn.Linear = getattr(l13.attention, 'query_key_value')\r\
    \n        s33: nn.Linear = getattr(l33.attention, 'query_key_value')\r\n     \
    \   dest: nn.Linear = getattr(layer.attention, 'dense')\r\n        s13: nn.Linear\
    \ = getattr(l13.attention, 'dense')\r\n        s33: nn.Linear = getattr(l33.attention,\
    \ 'dense')\r\n        merge_tensors_inplace(dest.weight, s13.weight, s33.weight,\
    \ BLOCK_DIAGONAL)\r\n\r\n\r\n        dest: nn.Linear = getattr(layer.mlp, 'dense_h_to_4h')\r\
    \n        s13: nn.Linear = getattr(l13.mlp, 'dense_h_to_4h')\r\n        s33: nn.Linear\
    \ = getattr(l33.mlp, 'dense_h_to_4h')\r\n        dest: nn.Linear = getattr(layer.mlp,\
    \ 'dense_4h_to_h')\r\n        s13: nn.Linear = getattr(l13.mlp, 'dense_4h_to_h')\r\
    \n        s33: nn.Linear = getattr(l33.mlp, 'dense_4h_to_h')\r\n        merge_tensors_inplace(dest.weight,\
    \ s13.weight, s33.weight, BLOCK_DIAGONAL)\r\n\r\n        layer.input_layernorm.weight[:]\
    \ = l33.input_layernorm.weight[\r\n            : layer.input_layernorm.weight.shape[0]\r\
    \n        ]\r\n        layer.input_layernorm.weight[\r\n            : l13.input_layernorm.weight.shape[0]\r\
    \n        ] = l13.input_layernorm.weight\r\n        layer.post_attention_layernorm.weight[:]\
    \ = l33.post_attention_layernorm.weight[\r\n            : layer.post_attention_layernorm.weight.shape[0]\r\
    \n        ]\r\n        layer.post_attention_layernorm.weight[\r\n            :\
    \ l13.post_attention_layernorm.weight.shape[0]\r\n        ] = l13.post_attention_layernorm.weight\r\
    \n\r\n    # have initial output depend on only original llama2-13b features, so\
    \ model\r\n    # starts unimpaired and can learn to incorporate the new features\
    \ as well\r\n    \r\n    model.embed_out.weight.zero_()\r\n    model.embed_out.weight[\r\
    \n        : donor_13b.embed_out.weight.shape[0], : donor_13b.embed_out.weight.shape[1]\r\
    \n    ] = donor_13b.embed_out.weight\r\n\r\n    merge_tensors_inplace(\r\n   \
    \     model.gpt_neox.embed_in.weight,\r\n        donor_13b.gpt_neox.embed_in.weight,\r\
    \n        donor_33b.gpt_neox.embed_in.weight,\r\n        BLOCK_DIAGONAL,\r\n \
    \   )\r\nmodel.save_pretrained(\"D:\\llm_model\\gpt-neox-20b_\", safe_serialization=True)\r\
    \n"
  created_at: 2023-07-29 04:20:00+00:00
  edited: false
  hidden: false
  id: 64c4a180e2e5c94bd030fc65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
      fullname: Lawliet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aslawliet
      type: user
    createdAt: '2023-11-26T11:30:15.000Z'
    data:
      edited: false
      editors:
      - aslawliet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9639571905136108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6551e98b7490049d62631325/MVLPiAaRD4MB4d0rNXsw1.jpeg?w=200&h=200&f=face
          fullname: Lawliet
          isHf: false
          isPro: false
          name: aslawliet
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aria256&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aria256\">@<span class=\"\
          underline\">aria256</span></a></span>\n\n\t</span></span> did you try it\
          \ out? Does the output model respond with garbage, it is for my case, I\
          \ guess this is because of no fine-tuning to get calibration </p>\n"
        raw: '@aria256 did you try it out? Does the output model respond with garbage,
          it is for my case, I guess this is because of no fine-tuning to get calibration '
        updatedAt: '2023-11-26T11:30:15.282Z'
      numEdits: 0
      reactions: []
    id: 65632c47ed2bca599ecdce6a
    type: comment
  author: aslawliet
  content: '@aria256 did you try it out? Does the output model respond with garbage,
    it is for my case, I guess this is because of no fine-tuning to get calibration '
  created_at: 2023-11-26 11:30:15+00:00
  edited: false
  hidden: false
  id: 65632c47ed2bca599ecdce6a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: chargoddard/llama2-22b
repo_type: model
status: open
target_branch: null
title: gpt-neox frankensteined code
