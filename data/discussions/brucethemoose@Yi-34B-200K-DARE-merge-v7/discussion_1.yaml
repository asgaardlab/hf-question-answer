!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wyxwangmed
conflicting_files: null
created_at: 2024-01-09 10:07:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af4fb3f341140ee620c04d3d790434b1.svg
      fullname: wyxwangmed
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wyxwangmed
      type: user
    createdAt: '2024-01-09T10:07:17.000Z'
    data:
      edited: false
      editors:
      - wyxwangmed
      hidden: false
      identifiedLanguage:
        language: fr
        probability: 0.32275810837745667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af4fb3f341140ee620c04d3d790434b1.svg
          fullname: wyxwangmed
          isHf: false
          isPro: false
          name: wyxwangmed
          type: user
        html: '<p>.</p>

          '
        raw: .
        updatedAt: '2024-01-09T10:07:17.714Z'
      numEdits: 0
      reactions: []
    id: 659d1ad588ea37eccf93ad99
    type: comment
  author: wyxwangmed
  content: .
  created_at: 2024-01-09 10:07:17+00:00
  edited: false
  hidden: false
  id: 659d1ad588ea37eccf93ad99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-09T16:47:01.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7345110177993774
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>It should be here: <a href="https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-merge-v7/blob/main/tokenizer.json">https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-merge-v7/blob/main/tokenizer.json</a></p>

          <p>What backend are you using? Do you mean that some require the .model
          file instead?</p>

          '
        raw: 'It should be here: https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-merge-v7/blob/main/tokenizer.json


          What backend are you using? Do you mean that some require the .model file
          instead?'
        updatedAt: '2024-01-09T16:47:16.659Z'
      numEdits: 1
      reactions: []
    id: 659d788556b112cdf2f5833b
    type: comment
  author: brucethemoose
  content: 'It should be here: https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-merge-v7/blob/main/tokenizer.json


    What backend are you using? Do you mean that some require the .model file instead?'
  created_at: 2024-01-09 16:47:01+00:00
  edited: true
  hidden: false
  id: 659d788556b112cdf2f5833b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-09T18:54:57.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8619363903999329
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Oobabooga needs it, at least under Exllama 2.<br>I just used the
          tokenizer.model of the Dare Merge v5.</p>

          '
        raw: 'Oobabooga needs it, at least under Exllama 2.

          I just used the tokenizer.model of the Dare Merge v5.'
        updatedAt: '2024-01-09T18:54:57.133Z'
      numEdits: 0
      reactions: []
    id: 659d96813752e021f6164302
    type: comment
  author: Nexesenex
  content: 'Oobabooga needs it, at least under Exllama 2.

    I just used the tokenizer.model of the Dare Merge v5.'
  created_at: 2024-01-09 18:54:57+00:00
  edited: false
  hidden: false
  id: 659d96813752e021f6164302
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-09T20:52:39.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9483690857887268
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Interesting. I believe tokenizer.model is a legacy format, not sure
          why ooba wants it.</p>

          <p>I converted it with llama.cpp and uploaded it, lemme know if it works.</p>

          <p>Also is there any reason you are not using an exl2 in ooba? Do y''all
          need an 8bpw upload?</p>

          '
        raw: 'Interesting. I believe tokenizer.model is a legacy format, not sure
          why ooba wants it.


          I converted it with llama.cpp and uploaded it, lemme know if it works.


          Also is there any reason you are not using an exl2 in ooba? Do y''all need
          an 8bpw upload?'
        updatedAt: '2024-01-09T20:53:37.808Z'
      numEdits: 2
      reactions: []
    id: 659db2173752e021f61cbd9b
    type: comment
  author: brucethemoose
  content: 'Interesting. I believe tokenizer.model is a legacy format, not sure why
    ooba wants it.


    I converted it with llama.cpp and uploaded it, lemme know if it works.


    Also is there any reason you are not using an exl2 in ooba? Do y''all need an
    8bpw upload?'
  created_at: 2024-01-09 20:52:39+00:00
  edited: true
  hidden: false
  id: 659db2173752e021f61cbd9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6520281ff28ce435b43c5fd9/4TxAmlp60z3tJ1PCy6vcs.png?w=200&h=200&f=face
      fullname: waldie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waldie
      type: user
    createdAt: '2024-01-10T08:38:49.000Z'
    data:
      edited: true
      editors:
      - waldie
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5079452991485596
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6520281ff28ce435b43c5fd9/4TxAmlp60z3tJ1PCy6vcs.png?w=200&h=200&f=face
          fullname: waldie
          isHf: false
          isPro: false
          name: waldie
          type: user
        html: "<p>i think the tokenizer.model has some issue</p>\n<p>exllamav2 0.0.11\
          \ convert.py fails with </p>\n<pre><code>Traceback (most recent call last):\n\
          \  File \"/exllamav2/convert.py\", line 69, in &lt;module&gt;\n    tokenizer\
          \ = ExLlamaV2Tokenizer(config)\n  File \"/exllamav2/exllamav2/tokenizer.py\"\
          , line 65, in __init__\n    if os.path.exists(path_spm) and not force_json:\
          \ self.tokenizer = ExLlamaV2TokenizerSPM(path_spm)\n  File \"/exllamav2/exllamav2/tokenizers/spm.py\"\
          , line 9, in __init__\n    self.spm = SentencePieceProcessor(model_file\
          \ = tokenizer_model)\n  File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 447, in Init\n    self.Load(model_file=model_file, model_proto=model_proto)\n\
          \  File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          /venv/lib/python3.10/site-packages/sentencepiece/__init__.py\", line 310,\
          \ in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto-&gt;ParseFromArray(serialized.data(),\
          \ serialized.size())]\n</code></pre>\n<p>quanting without the model file\
          \ seems to work</p>\n"
        raw: "i think the tokenizer.model has some issue\n\nexllamav2 0.0.11 convert.py\
          \ fails with \n\n```\nTraceback (most recent call last):\n  File \"/exllamav2/convert.py\"\
          , line 69, in <module>\n    tokenizer = ExLlamaV2Tokenizer(config)\n  File\
          \ \"/exllamav2/exllamav2/tokenizer.py\", line 65, in __init__\n    if os.path.exists(path_spm)\
          \ and not force_json: self.tokenizer = ExLlamaV2TokenizerSPM(path_spm)\n\
          \  File \"/exllamav2/exllamav2/tokenizers/spm.py\", line 9, in __init__\n\
          \    self.spm = SentencePieceProcessor(model_file = tokenizer_model)\n \
          \ File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 447, in Init\n    self.Load(model_file=model_file, model_proto=model_proto)\n\
          \  File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          /venv/lib/python3.10/site-packages/sentencepiece/__init__.py\", line 310,\
          \ in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
          \ serialized.size())]\n```\n\nquanting without the model file seems to work"
        updatedAt: '2024-01-10T08:39:44.502Z'
      numEdits: 1
      reactions: []
    id: 659e5799e7ed0764b40f052c
    type: comment
  author: waldie
  content: "i think the tokenizer.model has some issue\n\nexllamav2 0.0.11 convert.py\
    \ fails with \n\n```\nTraceback (most recent call last):\n  File \"/exllamav2/convert.py\"\
    , line 69, in <module>\n    tokenizer = ExLlamaV2Tokenizer(config)\n  File \"\
    /exllamav2/exllamav2/tokenizer.py\", line 65, in __init__\n    if os.path.exists(path_spm)\
    \ and not force_json: self.tokenizer = ExLlamaV2TokenizerSPM(path_spm)\n  File\
    \ \"/exllamav2/exllamav2/tokenizers/spm.py\", line 9, in __init__\n    self.spm\
    \ = SentencePieceProcessor(model_file = tokenizer_model)\n  File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 447, in Init\n    self.Load(model_file=model_file, model_proto=model_proto)\n\
    \  File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\", line\
    \ 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"/venv/lib/python3.10/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\nRuntimeError: Internal: src/sentencepiece_processor.cc(1101) [model_proto->ParseFromArray(serialized.data(),\
    \ serialized.size())]\n```\n\nquanting without the model file seems to work"
  created_at: 2024-01-10 08:38:49+00:00
  edited: true
  hidden: false
  id: 659e5799e7ed0764b40f052c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-10T09:49:18.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8756632804870605
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah I just got the same error actually.</p>

          '
        raw: Yeah I just got the same error actually.
        updatedAt: '2024-01-10T09:49:18.550Z'
      numEdits: 0
      reactions: []
    id: 659e681ea37139dc67ca38ac
    type: comment
  author: brucethemoose
  content: Yeah I just got the same error actually.
  created_at: 2024-01-10 09:49:18+00:00
  edited: false
  hidden: false
  id: 659e681ea37139dc67ca38ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-10T21:29:08.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9245325326919556
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Does anyone know how to even make an old tokenizer.model file?</p>

          <p>Technically another tokenizer could be used but some tokens from the
          union tokenizer merge may be missing.</p>

          '
        raw: 'Does anyone know how to even make an old tokenizer.model file?


          Technically another tokenizer could be used but some tokens from the union
          tokenizer merge may be missing.'
        updatedAt: '2024-01-10T21:29:08.237Z'
      numEdits: 0
      reactions: []
    id: 659f0c24c11c64921c1513c0
    type: comment
  author: brucethemoose
  content: 'Does anyone know how to even make an old tokenizer.model file?


    Technically another tokenizer could be used but some tokens from the union tokenizer
    merge may be missing.'
  created_at: 2024-01-10 21:29:08+00:00
  edited: false
  hidden: false
  id: 659f0c24c11c64921c1513c0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: brucethemoose/Yi-34B-200K-DARE-merge-v7
repo_type: model
status: open
target_branch: null
title: No tokenizer.model???
