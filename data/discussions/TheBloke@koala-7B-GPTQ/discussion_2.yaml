!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cmh
conflicting_files: null
created_at: 2023-04-07 12:15:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-07T13:15:12.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: "<p>Updated text-generation-webui (so GPTQ's repo is on the cuda branch\
          \ and probably still on an older commit).<br>Here's the errors with both\
          \ checkpoints koala-7B-4bit-128g.olderFormat.pt and koala-7B-4bit-128g:</p>\n\
          <p>(D:\\AI\\textgen-webui\\installer_files\\env) D:\\AI\\textgen-webui\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa&gt; python llama_inference.py\
          \ \"D:\\AI\\textgen-webui\\text-generation-webui\\models\\koala-7b-4bit-128g\"\
          \ --wbits 4 --groupsize 128 --load \"D:\\AI\\textgen-webui\\text-generation-webui\\\
          models\\koala-7b-4bit-128g\\koala-7B-4bit-128g.pt\" --max_length 300 --text\
          \ \"your text\"<br>Loading model ...<br>Traceback (most recent call last):<br>\
          \  File \"D:\\AI\\textgen-webui\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference.py\", line 112, in <br>    model = load_quant(args.model,\
          \ args.load, args.wbits, args.groupsize)<br>  File \"D:\\AI\\textgen-webui\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference.py\"\
          , line 52, in load_quant<br>    model.load_state_dict(torch.load(checkpoint))<br>\
          \  File \"D:\\AI\\textgen-webui\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 2041, in load_state_dict<br>    raise\
          \ RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(<br>RuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:<br>        Missing\
          \ key(s) in state_dict: \"model.layers.0.self_attn.k_proj.bias\", \"model.layers.0.self_attn.o_proj.bias\"\
          , \"model.layers.0.self_attn.q_proj.bias\", \"model.layers.0.self_attn.v_proj.bias\"\
          ,[...]</p>\n<p>I can clone the latest GPTQ cuda branch if needed, just let\
          \ me know.</p>\n<p>edit: I'm using the one click installer of texgen-webui\
          \ on Windows natively (no WSL) and ozcur_alpaca-native-4bit\\alpaca7b-4bit.pt\
          \ works fine:<br>(D:\\AI\\textgen-webui\\installer_files\\env) D:\\AI\\\
          textgen-webui\\text-generation-webui\\repositories\\GPTQ-for-LLaMa&gt;python\
          \ llama_inference.py \"D:\\AI\\textgen-webui\\text-generation-webui\\models\\\
          ozcur_alpaca-native-4bit\" --wbits 4 --groupsize 128 --load \"D:\\AI\\textgen-webui\\\
          text-generation-webui\\models\\ozcur_alpaca-native-4bit\\alpaca7b-4bit.pt\"\
          \ --max_length 300 --text \"What is an alpaca\"<br>Loading model ...<br>Done.<br>\
          \ What is an alpaca?<br>An alpaca is a species of South American camelid,\
          \ belonging to the family Camelidae. It is native to the Andes Mountains\
          \ of Ecuador, Peru, and Bolivia. Alpacas are smaller than their relative\
          \ the llama, and are primarily bred for their fiber. Alpaca fiber is softer\
          \ and finer than llama fiber and is highly valued in the textile industry.\
          \ Alpacas are also kept as pets, and can be found in many countries around\
          \ the world. What is the scientific name for the alpaca? The scientific\
          \ name for the alpaca is V...<br>What is the scientific name for the alpaca?\
          \ The scientific name for the alpaca is Vicugna vicugna. What is the average\
          \ size of an alpaca? The average size of an alpaca is 1.5 to 2.0 meters\
          \ in height and weighs up to 250 kilograms. What type of fiber does an alpaca\
          \ produce? An alpaca produces a fine, soft and luxurious fiber called vicu\xF1\
          a fiber. What type of color can the alpaca produce? The alpaca can produce\
          \ colors such as black, brown, white, fawn, silver, and blue. What is the\
          \ lifespan of an alpaca? The average lif</p>\n"
        raw: "Updated text-generation-webui (so GPTQ's repo is on the cuda branch\
          \ and probably still on an older commit).\nHere's the errors with both checkpoints\
          \ koala-7B-4bit-128g.olderFormat.pt and koala-7B-4bit-128g:\n\n(D:\\AI\\\
          textgen-webui\\installer_files\\env) D:\\AI\\textgen-webui\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa> python llama_inference.py \"D:\\AI\\textgen-webui\\\
          text-generation-webui\\models\\koala-7b-4bit-128g\" --wbits 4 --groupsize\
          \ 128 --load \"D:\\AI\\textgen-webui\\text-generation-webui\\models\\koala-7b-4bit-128g\\\
          koala-7B-4bit-128g.pt\" --max_length 300 --text \"your text\"\nLoading model\
          \ ...\nTraceback (most recent call last):\n  File \"D:\\AI\\textgen-webui\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference.py\"\
          , line 112, in <module>\n    model = load_quant(args.model, args.load, args.wbits,\
          \ args.groupsize)\n  File \"D:\\AI\\textgen-webui\\text-generation-webui\\\
          repositories\\GPTQ-for-LLaMa\\llama_inference.py\", line 52, in load_quant\n\
          \    model.load_state_dict(torch.load(checkpoint))\n  File \"D:\\AI\\textgen-webui\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for LlamaForCausalLM:\n        Missing key(s) in state_dict:\
          \ \"model.layers.0.self_attn.k_proj.bias\", \"model.layers.0.self_attn.o_proj.bias\"\
          , \"model.layers.0.self_attn.q_proj.bias\", \"model.layers.0.self_attn.v_proj.bias\"\
          ,[...]\n\nI can clone the latest GPTQ cuda branch if needed, just let me\
          \ know.\n\nedit: I'm using the one click installer of texgen-webui on Windows\
          \ natively (no WSL) and ozcur_alpaca-native-4bit\\alpaca7b-4bit.pt works\
          \ fine:\n(D:\\AI\\textgen-webui\\installer_files\\env) D:\\AI\\textgen-webui\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa>python llama_inference.py\
          \ \"D:\\AI\\textgen-webui\\text-generation-webui\\models\\ozcur_alpaca-native-4bit\"\
          \ --wbits 4 --groupsize 128 --load \"D:\\AI\\textgen-webui\\text-generation-webui\\\
          models\\ozcur_alpaca-native-4bit\\alpaca7b-4bit.pt\" --max_length 300 --text\
          \ \"What is an alpaca\"\nLoading model ...\nDone.\n What is an alpaca?\n\
          An alpaca is a species of South American camelid, belonging to the family\
          \ Camelidae. It is native to the Andes Mountains of Ecuador, Peru, and Bolivia.\
          \ Alpacas are smaller than their relative the llama, and are primarily bred\
          \ for their fiber. Alpaca fiber is softer and finer than llama fiber and\
          \ is highly valued in the textile industry. Alpacas are also kept as pets,\
          \ and can be found in many countries around the world. What is the scientific\
          \ name for the alpaca? The scientific name for the alpaca is V...\nWhat\
          \ is the scientific name for the alpaca? The scientific name for the alpaca\
          \ is Vicugna vicugna. What is the average size of an alpaca? The average\
          \ size of an alpaca is 1.5 to 2.0 meters in height and weighs up to 250\
          \ kilograms. What type of fiber does an alpaca produce? An alpaca produces\
          \ a fine, soft and luxurious fiber called vicu\xF1a fiber. What type of\
          \ color can the alpaca produce? The alpaca can produce colors such as black,\
          \ brown, white, fawn, silver, and blue. What is the lifespan of an alpaca?\
          \ The average lif"
        updatedAt: '2023-04-07T13:20:41.588Z'
      numEdits: 1
      reactions: []
    id: 64301760b009240418db5817
    type: comment
  author: cmh
  content: "Updated text-generation-webui (so GPTQ's repo is on the cuda branch and\
    \ probably still on an older commit).\nHere's the errors with both checkpoints\
    \ koala-7B-4bit-128g.olderFormat.pt and koala-7B-4bit-128g:\n\n(D:\\AI\\textgen-webui\\\
    installer_files\\env) D:\\AI\\textgen-webui\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa> python llama_inference.py \"D:\\AI\\textgen-webui\\text-generation-webui\\\
    models\\koala-7b-4bit-128g\" --wbits 4 --groupsize 128 --load \"D:\\AI\\textgen-webui\\\
    text-generation-webui\\models\\koala-7b-4bit-128g\\koala-7B-4bit-128g.pt\" --max_length\
    \ 300 --text \"your text\"\nLoading model ...\nTraceback (most recent call last):\n\
    \  File \"D:\\AI\\textgen-webui\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
    llama_inference.py\", line 112, in <module>\n    model = load_quant(args.model,\
    \ args.load, args.wbits, args.groupsize)\n  File \"D:\\AI\\textgen-webui\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\llama_inference.py\", line 52, in load_quant\n \
    \   model.load_state_dict(torch.load(checkpoint))\n  File \"D:\\AI\\textgen-webui\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\n\
    \        Missing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.bias\"\
    , \"model.layers.0.self_attn.o_proj.bias\", \"model.layers.0.self_attn.q_proj.bias\"\
    , \"model.layers.0.self_attn.v_proj.bias\",[...]\n\nI can clone the latest GPTQ\
    \ cuda branch if needed, just let me know.\n\nedit: I'm using the one click installer\
    \ of texgen-webui on Windows natively (no WSL) and ozcur_alpaca-native-4bit\\\
    alpaca7b-4bit.pt works fine:\n(D:\\AI\\textgen-webui\\installer_files\\env) D:\\\
    AI\\textgen-webui\\text-generation-webui\\repositories\\GPTQ-for-LLaMa>python\
    \ llama_inference.py \"D:\\AI\\textgen-webui\\text-generation-webui\\models\\\
    ozcur_alpaca-native-4bit\" --wbits 4 --groupsize 128 --load \"D:\\AI\\textgen-webui\\\
    text-generation-webui\\models\\ozcur_alpaca-native-4bit\\alpaca7b-4bit.pt\" --max_length\
    \ 300 --text \"What is an alpaca\"\nLoading model ...\nDone.\n What is an alpaca?\n\
    An alpaca is a species of South American camelid, belonging to the family Camelidae.\
    \ It is native to the Andes Mountains of Ecuador, Peru, and Bolivia. Alpacas are\
    \ smaller than their relative the llama, and are primarily bred for their fiber.\
    \ Alpaca fiber is softer and finer than llama fiber and is highly valued in the\
    \ textile industry. Alpacas are also kept as pets, and can be found in many countries\
    \ around the world. What is the scientific name for the alpaca? The scientific\
    \ name for the alpaca is V...\nWhat is the scientific name for the alpaca? The\
    \ scientific name for the alpaca is Vicugna vicugna. What is the average size\
    \ of an alpaca? The average size of an alpaca is 1.5 to 2.0 meters in height and\
    \ weighs up to 250 kilograms. What type of fiber does an alpaca produce? An alpaca\
    \ produces a fine, soft and luxurious fiber called vicu\xF1a fiber. What type\
    \ of color can the alpaca produce? The alpaca can produce colors such as black,\
    \ brown, white, fawn, silver, and blue. What is the lifespan of an alpaca? The\
    \ average lif"
  created_at: 2023-04-07 12:15:12+00:00
  edited: true
  hidden: false
  id: 64301760b009240418db5817
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-07T13:28:23.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>Well it worked on textgen-webui with those parameters but outputed
          garbage:<br>python server.py --auto-devices --gpu-memory 4725MiB --wbits
          4 --groupsize 128 --model koala-7b-4bit-128g --model_type LLaMA </p>

          <p>I''ll try both models and report back.<br>edit:<br>koala-7B-4bit-128g.olderFormat.pt
          doesn''t load (same error as GPTQ).<br>koala-7B-4bit-128g.pt output:<br>Common
          sense questions and answers</p>

          <p>Question: What is an alpaca ?<br>Factual answer: rf df df dfdfrfffdf
          visit dfdfdf df/ dfdf df dfdf dfdfdfFF df dfdf /OdfDF df df /ch df /df /df
          /dfdf dfdfdf /rf dfdf /rf /dfdfdfdfdfdf dfdf /dfdf df df / df df /df / df
          dfdf / / / df dfdf / /</p>

          '
        raw: "Well it worked on textgen-webui with those parameters but outputed garbage:\n\
          python server.py --auto-devices --gpu-memory 4725MiB --wbits 4 --groupsize\
          \ 128 --model koala-7b-4bit-128g --model_type LLaMA \n\nI'll try both models\
          \ and report back.\nedit: \nkoala-7B-4bit-128g.olderFormat.pt doesn't load\
          \ (same error as GPTQ).\nkoala-7B-4bit-128g.pt output:\nCommon sense questions\
          \ and answers\n\nQuestion: What is an alpaca ?\nFactual answer: rf df df\
          \ dfdfrfffdf visit dfdfdf df/ dfdf df dfdf dfdfdfFF df dfdf /OdfDF df df\
          \ /ch df /df /df /dfdf dfdfdf /rf dfdf /rf /dfdfdfdfdfdf dfdf /dfdf df df\
          \ / df df /df / df dfdf / / / df dfdf / /"
        updatedAt: '2023-04-07T13:36:13.802Z'
      numEdits: 2
      reactions: []
    id: 64301a77d08b9afcdeb1fd6e
    type: comment
  author: cmh
  content: "Well it worked on textgen-webui with those parameters but outputed garbage:\n\
    python server.py --auto-devices --gpu-memory 4725MiB --wbits 4 --groupsize 128\
    \ --model koala-7b-4bit-128g --model_type LLaMA \n\nI'll try both models and report\
    \ back.\nedit: \nkoala-7B-4bit-128g.olderFormat.pt doesn't load (same error as\
    \ GPTQ).\nkoala-7B-4bit-128g.pt output:\nCommon sense questions and answers\n\n\
    Question: What is an alpaca ?\nFactual answer: rf df df dfdfrfffdf visit dfdfdf\
    \ df/ dfdf df dfdf dfdfdfFF df dfdf /OdfDF df df /ch df /df /df /dfdf dfdfdf /rf\
    \ dfdf /rf /dfdfdfdfdfdf dfdf /dfdf df df / df df /df / df dfdf / / / df dfdf\
    \ / /"
  created_at: 2023-04-07 12:28:23+00:00
  edited: true
  hidden: false
  id: 64301a77d08b9afcdeb1fd6e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T13:29:18.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-04-07T13:52:52.399Z'
      numEdits: 0
      reactions: []
    id: 64301aaea981899400816ee4
    type: comment
  author: TheBloke
  content: This comment has been hidden
  created_at: 2023-04-07 12:29:18+00:00
  edited: true
  hidden: true
  id: 64301aaea981899400816ee4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T13:33:37.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-04-07T13:53:09.636Z'
      numEdits: 0
      reactions: []
    id: 64301bb1ade84444a5548ad8
    type: comment
  author: TheBloke
  content: This comment has been hidden
  created_at: 2023-04-07 12:33:37+00:00
  edited: true
  hidden: true
  id: 64301bb1ade84444a5548ad8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T14:00:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK short answer is I don''t understand what''s going on here. My
          knowledge is not good enough yet to be able to diagnose what''s happening.</p>

          <p>What I know for sure:</p>

          <ol>

          <li>The GPTQ models I have produced here always produce garbage output in
          text-generation-webui, and if I try to convert the <code>olderFormat</code>
          version to GGML using the llama.cpp convert script (the latest version,
          <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/545">found
          in this PR</a>), it seems to convert fine but similarly produces garbage
          when run in llama.cpp.</li>

          <li>If I try to load the unquantized Koala 7B model in HF format in text-generation-webui,
          it similarly produces garbage output. That model data can be found here:  <a
          href="https://huggingface.co/TheBloke/koala-7B-HF">koala-7b-HF</a> . The
          README explains how I converted the model from EasyML.</li>

          <li>However, if I convert koala-7B-Hf to unquantized GGML format, it loads
          and runs fine in llama.cpp, producing good output. That model can be found
          here: <a href="https://huggingface.co/TheBloke/koala-7b-ggml-unquantized">https://huggingface.co/TheBloke/koala-7b-ggml-unquantized</a></li>

          </ol>

          <p>So I am rather confused and stuck now.</p>

          <p>Given I can''t even get the unquantized model to load in text-generation-webui,
          I am thinking I won''t spend any more time diagnosing the GPTQ issue and
          will instead look again at the EasyML conversion process. Maybe something
          is going wrong there. Or I have a problem in the JSON files or something
          like that.</p>

          <p>Any help or suggestions would be much appreciated!</p>

          '
        raw: 'OK short answer is I don''t understand what''s going on here. My knowledge
          is not good enough yet to be able to diagnose what''s happening.


          What I know for sure:

          1. The GPTQ models I have produced here always produce garbage output in
          text-generation-webui, and if I try to convert the `olderFormat` version
          to GGML using the llama.cpp convert script (the latest version, [found in
          this PR](https://github.com/ggerganov/llama.cpp/pull/545)), it seems to
          convert fine but similarly produces garbage when run in llama.cpp.

          2. If I try to load the unquantized Koala 7B model in HF format in text-generation-webui,
          it similarly produces garbage output. That model data can be found here:  [koala-7b-HF](https://huggingface.co/TheBloke/koala-7B-HF)
          . The README explains how I converted the model from EasyML.

          3. However, if I convert koala-7B-Hf to unquantized GGML format, it loads
          and runs fine in llama.cpp, producing good output. That model can be found
          here: https://huggingface.co/TheBloke/koala-7b-ggml-unquantized


          So I am rather confused and stuck now.


          Given I can''t even get the unquantized model to load in text-generation-webui,
          I am thinking I won''t spend any more time diagnosing the GPTQ issue and
          will instead look again at the EasyML conversion process. Maybe something
          is going wrong there. Or I have a problem in the JSON files or something
          like that.


          Any help or suggestions would be much appreciated!'
        updatedAt: '2023-04-07T14:00:35.732Z'
      numEdits: 0
      reactions: []
    id: 64302203550e13b8327adbd9
    type: comment
  author: TheBloke
  content: 'OK short answer is I don''t understand what''s going on here. My knowledge
    is not good enough yet to be able to diagnose what''s happening.


    What I know for sure:

    1. The GPTQ models I have produced here always produce garbage output in text-generation-webui,
    and if I try to convert the `olderFormat` version to GGML using the llama.cpp
    convert script (the latest version, [found in this PR](https://github.com/ggerganov/llama.cpp/pull/545)),
    it seems to convert fine but similarly produces garbage when run in llama.cpp.

    2. If I try to load the unquantized Koala 7B model in HF format in text-generation-webui,
    it similarly produces garbage output. That model data can be found here:  [koala-7b-HF](https://huggingface.co/TheBloke/koala-7B-HF)
    . The README explains how I converted the model from EasyML.

    3. However, if I convert koala-7B-Hf to unquantized GGML format, it loads and
    runs fine in llama.cpp, producing good output. That model can be found here: https://huggingface.co/TheBloke/koala-7b-ggml-unquantized


    So I am rather confused and stuck now.


    Given I can''t even get the unquantized model to load in text-generation-webui,
    I am thinking I won''t spend any more time diagnosing the GPTQ issue and will
    instead look again at the EasyML conversion process. Maybe something is going
    wrong there. Or I have a problem in the JSON files or something like that.


    Any help or suggestions would be much appreciated!'
  created_at: 2023-04-07 13:00:35+00:00
  edited: false
  hidden: false
  id: 64302203550e13b8327adbd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-07T15:13:01.000Z'
    data:
      edited: false
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>I''m definitely not knowledgeable either, I''d love to reproduce
          the workflow but I have a very modest machine (core i5 without avx2 16 gb
          of ram and a 1060 6gb). I''ll still try/investigate and report back if I
          get any progress.</p>

          '
        raw: I'm definitely not knowledgeable either, I'd love to reproduce the workflow
          but I have a very modest machine (core i5 without avx2 16 gb of ram and
          a 1060 6gb). I'll still try/investigate and report back if I get any progress.
        updatedAt: '2023-04-07T15:13:01.897Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 643032fd12c9cad0dc063b54
    type: comment
  author: cmh
  content: I'm definitely not knowledgeable either, I'd love to reproduce the workflow
    but I have a very modest machine (core i5 without avx2 16 gb of ram and a 1060
    6gb). I'll still try/investigate and report back if I get any progress.
  created_at: 2023-04-07 14:13:01+00:00
  edited: false
  hidden: false
  id: 643032fd12c9cad0dc063b54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ecc4d502278ae77354564beb97bdff10.svg
      fullname: knoopx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: knoopx
      type: user
    createdAt: '2023-04-07T18:19:14.000Z'
    data:
      edited: false
      editors:
      - knoopx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ecc4d502278ae77354564beb97bdff10.svg
          fullname: knoopx
          isHf: false
          isPro: false
          name: knoopx
          type: user
        html: '<p>pulling the latest changes from GPTQ-for-LLaMa cuda branch and re-running
          python setup_cuda.py install made them work for me.</p>

          '
        raw: pulling the latest changes from GPTQ-for-LLaMa cuda branch and re-running
          python setup_cuda.py install made them work for me.
        updatedAt: '2023-04-07T18:19:14.285Z'
      numEdits: 0
      reactions: []
    id: 64305ea207cc7582b08e75dd
    type: comment
  author: knoopx
  content: pulling the latest changes from GPTQ-for-LLaMa cuda branch and re-running
    python setup_cuda.py install made them work for me.
  created_at: 2023-04-07 17:19:14+00:00
  edited: false
  hidden: false
  id: 64305ea207cc7582b08e75dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T18:47:36.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>pulling the latest changes from GPTQ-for-LLaMa cuda branch and re-running
          python setup_cuda.py install made them work for me.</p>

          </blockquote>

          <p>OK and it''s definitely working reliably for you? I tried that earlier
          and the first time I tried it it seemed like it worked, but then when I
          did more tests I started getting garbage again.</p>

          <p>Just so I''m clear: you''re using text-generation-webui and instead of
          cloning the oogabooga fork of GPTQ-for-LLaMa you cloned the original GPTQ
          repo, built it, and that was enough?</p>

          <p>I will go try it again myself now.</p>

          '
        raw: '> pulling the latest changes from GPTQ-for-LLaMa cuda branch and re-running
          python setup_cuda.py install made them work for me.


          OK and it''s definitely working reliably for you? I tried that earlier and
          the first time I tried it it seemed like it worked, but then when I did
          more tests I started getting garbage again.


          Just so I''m clear: you''re using text-generation-webui and instead of cloning
          the oogabooga fork of GPTQ-for-LLaMa you cloned the original GPTQ repo,
          built it, and that was enough?


          I will go try it again myself now.'
        updatedAt: '2023-04-07T18:48:04.622Z'
      numEdits: 1
      reactions: []
    id: 64306548895df85fb3ca2bb9
    type: comment
  author: TheBloke
  content: '> pulling the latest changes from GPTQ-for-LLaMa cuda branch and re-running
    python setup_cuda.py install made them work for me.


    OK and it''s definitely working reliably for you? I tried that earlier and the
    first time I tried it it seemed like it worked, but then when I did more tests
    I started getting garbage again.


    Just so I''m clear: you''re using text-generation-webui and instead of cloning
    the oogabooga fork of GPTQ-for-LLaMa you cloned the original GPTQ repo, built
    it, and that was enough?


    I will go try it again myself now.'
  created_at: 2023-04-07 17:47:36+00:00
  edited: true
  hidden: false
  id: 64306548895df85fb3ca2bb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T18:51:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>These are the commands I ran earlier when I tried that:</p>\n<pre><code>cd\
          \ text-generation-webui\nmkdir -p repositories\ncd repositories\ngit clone\
          \ https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda GPTQ-for-LLaMa\n\
          cd GPTQ-for-LLaMa\npython setup_cuda.py install --force\n</code></pre>\n\
          <p>Does that match what you did, <span data-props=\"{&quot;user&quot;:&quot;knoopx&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/knoopx\"\
          >@<span class=\"underline\">knoopx</span></a></span>\n\n\t</span></span>\
          \  ?</p>\n"
        raw: 'These are the commands I ran earlier when I tried that:

          ```

          cd text-generation-webui

          mkdir -p repositories

          cd repositories

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda GPTQ-for-LLaMa

          cd GPTQ-for-LLaMa

          python setup_cuda.py install --force

          ```

          Does that match what you did, @knoopx  ?'
        updatedAt: '2023-04-07T18:51:49.550Z'
      numEdits: 0
      reactions: []
    id: 64306645b0fa8e794edff36c
    type: comment
  author: TheBloke
  content: 'These are the commands I ran earlier when I tried that:

    ```

    cd text-generation-webui

    mkdir -p repositories

    cd repositories

    git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda GPTQ-for-LLaMa

    cd GPTQ-for-LLaMa

    python setup_cuda.py install --force

    ```

    Does that match what you did, @knoopx  ?'
  created_at: 2023-04-07 17:51:49+00:00
  edited: false
  hidden: false
  id: 64306645b0fa8e794edff36c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T19:22:18.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Yeah it's still not working.  I'm currently trying the <code>koala-7B-4bit-128g.pt</code>\
          \ file, and running the UI with this command:</p>\n<pre><code>cd /content/text-generation-webui\n\
          python server.py --model koala-7b-4bit --wbits 4 --groupsize 128 --model_type\
          \ LLaMA\n</code></pre>\n<p>In the UI I'm selecting the Llama_Precise parameters\
          \ preset (temp = 0.7, top_p = 0.1, top_k = 40, rep_penalty = 1.18, etc)</p>\n\
          <p>And here's two examples of output. In one I try just entering the query\
          \ directly, and in the other I try using the Instruction/Response format</p>\n\
          <p>Example 1:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tLm7KOunwUdt-J7jnIm1P.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tLm7KOunwUdt-J7jnIm1P.png\"\
          ></a></p>\n<p>Example 2:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zSE_pfPCY5xJCPq_ffdiW.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zSE_pfPCY5xJCPq_ffdiW.png\"\
          ></a></p>\n<p>It's mostly just garbage?</p>\n<p>I have tried some other\
          \ parameters, like the NovelAI-Pleasing, and sometimes I get something that's\
          \ at least intelligible. But then the next response will just be 'spanspanspan'\
          \ over and over..</p>\n<p>I'd love to know exactly how you're running it\
          \ <span data-props=\"{&quot;user&quot;:&quot;knoopx&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/knoopx\">@<span class=\"\
          underline\">knoopx</span></a></span>\n\n\t</span></span> and see some example\
          \ output?</p>\n"
        raw: 'Yeah it''s still not working.  I''m currently trying the `koala-7B-4bit-128g.pt`
          file, and running the UI with this command:


          ```

          cd /content/text-generation-webui

          python server.py --model koala-7b-4bit --wbits 4 --groupsize 128 --model_type
          LLaMA

          ```

          In the UI I''m selecting the Llama_Precise parameters preset (temp = 0.7,
          top_p = 0.1, top_k = 40, rep_penalty = 1.18, etc)


          And here''s two examples of output. In one I try just entering the query
          directly, and in the other I try using the Instruction/Response format


          Example 1:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tLm7KOunwUdt-J7jnIm1P.png)


          Example 2:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zSE_pfPCY5xJCPq_ffdiW.png)


          It''s mostly just garbage?


          I have tried some other parameters, like the NovelAI-Pleasing, and sometimes
          I get something that''s at least intelligible. But then the next response
          will just be ''spanspanspan'' over and over..


          I''d love to know exactly how you''re running it @knoopx and see some example
          output?'
        updatedAt: '2023-04-07T19:22:18.095Z'
      numEdits: 0
      reactions: []
    id: 64306d6a624fb3979932c5e0
    type: comment
  author: TheBloke
  content: 'Yeah it''s still not working.  I''m currently trying the `koala-7B-4bit-128g.pt`
    file, and running the UI with this command:


    ```

    cd /content/text-generation-webui

    python server.py --model koala-7b-4bit --wbits 4 --groupsize 128 --model_type
    LLaMA

    ```

    In the UI I''m selecting the Llama_Precise parameters preset (temp = 0.7, top_p
    = 0.1, top_k = 40, rep_penalty = 1.18, etc)


    And here''s two examples of output. In one I try just entering the query directly,
    and in the other I try using the Instruction/Response format


    Example 1:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tLm7KOunwUdt-J7jnIm1P.png)


    Example 2:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/zSE_pfPCY5xJCPq_ffdiW.png)


    It''s mostly just garbage?


    I have tried some other parameters, like the NovelAI-Pleasing, and sometimes I
    get something that''s at least intelligible. But then the next response will just
    be ''spanspanspan'' over and over..


    I''d love to know exactly how you''re running it @knoopx and see some example
    output?'
  created_at: 2023-04-07 18:22:18+00:00
  edited: false
  hidden: false
  id: 64306d6a624fb3979932c5e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T19:42:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>What really confuses me is that GPTQ-for-LLaMa itself can definitely
          do inference using these files:</p>

          <pre><code>cd /content/gptq-llama

          CUDA_VISIBLE_DEVICES=0 python llama_inference.py  /content/text-generation-webui/models/koala-7b-4bit
          --wbits 4 --groupsize 128 --load /content/text-generation-webui/models/koala-7b-4bit/koala-7B-4bit-128g.pt  --max_length=200
          --min_length=100 --text "write a story about Kevin"

          </code></pre>

          <p>Produces output:</p>

          <pre><code>&lt;s&gt; write a story about Kevin and Daryl trying to recapture
          Kevin''s dragon


          One day, Kevin and Daryl decided to try to recapture Kevin''s dragon. They
          knew it would be a challenging task, but they were determined to do it.


          They gathered all their supplies and set off on their journey. They traveled
          for hours, but they couldn''t find any sign of the dragon. They were starting
          to lose hope, when they finally spotted it in the distance.


          Kevin and Daryl approached the dragon, and Daryl started to feed it. Kevin''s
          dragon was hungry and eager to eat, and it didn''t take long before it was
          finished.


          Kevin was relieved that his dragon was safe and healthy, but he knew that
          it was still a wild and powerful animal. He decided to keep it as a pet
          and train it

          </code></pre>

          <p>So I''m really confused as to what''s going on with <code>text-generation-webui</code>.</p>

          '
        raw: 'What really confuses me is that GPTQ-for-LLaMa itself can definitely
          do inference using these files:


          ```

          cd /content/gptq-llama

          CUDA_VISIBLE_DEVICES=0 python llama_inference.py  /content/text-generation-webui/models/koala-7b-4bit
          --wbits 4 --groupsize 128 --load /content/text-generation-webui/models/koala-7b-4bit/koala-7B-4bit-128g.pt  --max_length=200
          --min_length=100 --text "write a story about Kevin"

          ```


          Produces output:

          ```

          <s> write a story about Kevin and Daryl trying to recapture Kevin''s dragon


          One day, Kevin and Daryl decided to try to recapture Kevin''s dragon. They
          knew it would be a challenging task, but they were determined to do it.


          They gathered all their supplies and set off on their journey. They traveled
          for hours, but they couldn''t find any sign of the dragon. They were starting
          to lose hope, when they finally spotted it in the distance.


          Kevin and Daryl approached the dragon, and Daryl started to feed it. Kevin''s
          dragon was hungry and eager to eat, and it didn''t take long before it was
          finished.


          Kevin was relieved that his dragon was safe and healthy, but he knew that
          it was still a wild and powerful animal. He decided to keep it as a pet
          and train it

          ```


          So I''m really confused as to what''s going on with `text-generation-webui`.'
        updatedAt: '2023-04-07T19:42:32.015Z'
      numEdits: 0
      reactions: []
    id: 64307228624fb3979932ed09
    type: comment
  author: TheBloke
  content: 'What really confuses me is that GPTQ-for-LLaMa itself can definitely do
    inference using these files:


    ```

    cd /content/gptq-llama

    CUDA_VISIBLE_DEVICES=0 python llama_inference.py  /content/text-generation-webui/models/koala-7b-4bit
    --wbits 4 --groupsize 128 --load /content/text-generation-webui/models/koala-7b-4bit/koala-7B-4bit-128g.pt  --max_length=200
    --min_length=100 --text "write a story about Kevin"

    ```


    Produces output:

    ```

    <s> write a story about Kevin and Daryl trying to recapture Kevin''s dragon


    One day, Kevin and Daryl decided to try to recapture Kevin''s dragon. They knew
    it would be a challenging task, but they were determined to do it.


    They gathered all their supplies and set off on their journey. They traveled for
    hours, but they couldn''t find any sign of the dragon. They were starting to lose
    hope, when they finally spotted it in the distance.


    Kevin and Daryl approached the dragon, and Daryl started to feed it. Kevin''s
    dragon was hungry and eager to eat, and it didn''t take long before it was finished.


    Kevin was relieved that his dragon was safe and healthy, but he knew that it was
    still a wild and powerful animal. He decided to keep it as a pet and train it

    ```


    So I''m really confused as to what''s going on with `text-generation-webui`.'
  created_at: 2023-04-07 18:42:32+00:00
  edited: false
  hidden: false
  id: 64307228624fb3979932ed09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-07T19:57:37.000Z'
    data:
      edited: false
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>Just updating GPTQ''s repo and reinstalling the kernel totally broke
          it for me. No errors but it wouldn''t output anything .<br>I''ve just installed
          VS2019 build tools and miniconda. I''ll try with the latest GPTQ''s repo.</p>

          '
        raw: 'Just updating GPTQ''s repo and reinstalling the kernel totally broke
          it for me. No errors but it wouldn''t output anything .

          I''ve just installed VS2019 build tools and miniconda. I''ll try with the
          latest GPTQ''s repo.'
        updatedAt: '2023-04-07T19:57:37.723Z'
      numEdits: 0
      reactions: []
    id: 643075b1624fb39799330914
    type: comment
  author: cmh
  content: 'Just updating GPTQ''s repo and reinstalling the kernel totally broke it
    for me. No errors but it wouldn''t output anything .

    I''ve just installed VS2019 build tools and miniconda. I''ll try with the latest
    GPTQ''s repo.'
  created_at: 2023-04-07 18:57:37+00:00
  edited: false
  hidden: false
  id: 643075b1624fb39799330914
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-07T20:47:59.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: "<p>Just to be thorough:<br>I deleted the one click installer's folder\
          \ then installed the \"Desktop Environment with C++\" from Visual Studio\
          \ 2019 Build Tools and miniconda<br>Finally, I started the anaconda prompt,\
          \ created an environment, yadi yada:</p>\n<p>conda create -n textgen python=3.10.9<br>conda\
          \ activate textgen<br>conda install git ninja<br>conda install cuda cudatoolkit\
          \ pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia/label/cuda-11.7.0<br>pip\
          \ install cchardet chardet bitsandbytes-windows<br>conda install -c conda-forge\
          \ cudatoolkit-dev<br>git clone <a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa.git</a> -b cuda<br>cd text-generation-webui<br>pip\
          \ install -r requirements.txt<br>md repositories<br>cd repositories<br>git\
          \ clone <a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa</a> -b cuda GPTQ-for-LLaMa<br>cd\
          \ GPTQ-for-LLaMa<br>\"C:\\Program Files (x86)\\Microsoft Visual Studio\\\
          2019\\BuildTools\\VC\\Auxiliary\\Build\\vcvars64.bat\"<br>set DISTUTILS_USE_SDK=1<br>python\
          \ setup_cuda.py install</p>\n<p>I placed the model, jsons etc. where it\
          \ belongs and in a new command prompt:<br>conda activate textgen<br>cd text-generation-webui<br>python\
          \ server.py --auto-devices --gpu-memory 4725MiB --wbits 4 --groupsize 128\
          \ --model koala-7b-4bit-128g --model_type LLaMA --no-stream --chat\t</p>\n\
          <p>It's working, kinda.<br>I deleted everything caractere related and left\
          \ the generation parameters at debug-deterministic to compare to ozcur_alpaca-native-4bit</p>\n\
          <p>You<br>What's the difference between a bird and a dinosaure ?</p>\n<p>Alpaca:<br>The\
          \ difference between a bird and a dinosaur is that a bird is alive and a\
          \ dinosaur is extinct.</p>\n<p>Koala:<br>A bird is a type of dinosaur.</p>\n\
          <p>You<br>Who's Buzz Aldrin</p>\n<p>Alpaca:<br>What's the name of the first\
          \ man to walk on the moon?</p>\n<p>Koala:<br>A person who is a type of person\
          \ who is a person who is a person who is a person who is a person who is\
          \ a person who is a person who is (I stopped).<br>Who's a jotle jotle jotle\
          \ jotle jotle jotle jotle jotle jotle j (I stopped)</p>\n"
        raw: "Just to be thorough:\nI deleted the one click installer's folder then\
          \ installed the \"Desktop Environment with C++\" from Visual Studio 2019\
          \ Build Tools and miniconda\nFinally, I started the anaconda prompt, created\
          \ an environment, yadi yada:\n\nconda create -n textgen python=3.10.9\n\
          conda activate textgen\nconda install git ninja \nconda install cuda cudatoolkit\
          \ pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia/label/cuda-11.7.0\n\
          pip install cchardet chardet bitsandbytes-windows\nconda install -c conda-forge\
          \ cudatoolkit-dev\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\
          \ -b cuda\ncd text-generation-webui\npip install -r requirements.txt\nmd\
          \ repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ -b cuda GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\n\"C:\\Program Files (x86)\\\
          Microsoft Visual Studio\\2019\\BuildTools\\VC\\Auxiliary\\Build\\vcvars64.bat\"\
          \nset DISTUTILS_USE_SDK=1\npython setup_cuda.py install\n\nI placed the\
          \ model, jsons etc. where it belongs and in a new command prompt:\nconda\
          \ activate textgen\ncd text-generation-webui\npython server.py --auto-devices\
          \ --gpu-memory 4725MiB --wbits 4 --groupsize 128 --model koala-7b-4bit-128g\
          \ --model_type LLaMA --no-stream --chat\t\n\nIt's working, kinda.\nI deleted\
          \ everything caractere related and left the generation parameters at debug-deterministic\
          \ to compare to ozcur_alpaca-native-4bit\n\n\nYou\nWhat's the difference\
          \ between a bird and a dinosaure ?\n\nAlpaca:\nThe difference between a\
          \ bird and a dinosaur is that a bird is alive and a dinosaur is extinct.\n\
          \nKoala:\nA bird is a type of dinosaur.\n\n\nYou\nWho's Buzz Aldrin\n\n\
          Alpaca:\nWhat's the name of the first man to walk on the moon?\n\nKoala:\n\
          A person who is a type of person who is a person who is a person who is\
          \ a person who is a person who is a person who is a person who is (I stopped).\n\
          Who's a jotle jotle jotle jotle jotle jotle jotle jotle jotle j (I stopped)"
        updatedAt: '2023-04-07T21:53:17.111Z'
      numEdits: 8
      reactions: []
    id: 6430817fe936438c4a243cd3
    type: comment
  author: cmh
  content: "Just to be thorough:\nI deleted the one click installer's folder then\
    \ installed the \"Desktop Environment with C++\" from Visual Studio 2019 Build\
    \ Tools and miniconda\nFinally, I started the anaconda prompt, created an environment,\
    \ yadi yada:\n\nconda create -n textgen python=3.10.9\nconda activate textgen\n\
    conda install git ninja \nconda install cuda cudatoolkit pytorch torchvision torchaudio\
    \ pytorch-cuda=11.7 -c pytorch -c nvidia/label/cuda-11.7.0\npip install cchardet\
    \ chardet bitsandbytes-windows\nconda install -c conda-forge cudatoolkit-dev\n\
    git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git -b cuda\ncd text-generation-webui\n\
    pip install -r requirements.txt\nmd repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
    \ -b cuda GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\n\"C:\\Program Files (x86)\\Microsoft\
    \ Visual Studio\\2019\\BuildTools\\VC\\Auxiliary\\Build\\vcvars64.bat\"\nset DISTUTILS_USE_SDK=1\n\
    python setup_cuda.py install\n\nI placed the model, jsons etc. where it belongs\
    \ and in a new command prompt:\nconda activate textgen\ncd text-generation-webui\n\
    python server.py --auto-devices --gpu-memory 4725MiB --wbits 4 --groupsize 128\
    \ --model koala-7b-4bit-128g --model_type LLaMA --no-stream --chat\t\n\nIt's working,\
    \ kinda.\nI deleted everything caractere related and left the generation parameters\
    \ at debug-deterministic to compare to ozcur_alpaca-native-4bit\n\n\nYou\nWhat's\
    \ the difference between a bird and a dinosaure ?\n\nAlpaca:\nThe difference between\
    \ a bird and a dinosaur is that a bird is alive and a dinosaur is extinct.\n\n\
    Koala:\nA bird is a type of dinosaur.\n\n\nYou\nWho's Buzz Aldrin\n\nAlpaca:\n\
    What's the name of the first man to walk on the moon?\n\nKoala:\nA person who\
    \ is a type of person who is a person who is a person who is a person who is a\
    \ person who is a person who is a person who is (I stopped).\nWho's a jotle jotle\
    \ jotle jotle jotle jotle jotle jotle jotle j (I stopped)"
  created_at: 2023-04-07 19:47:59+00:00
  edited: true
  hidden: false
  id: 6430817fe936438c4a243cd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-07T21:01:42.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: "<p>I'm still not very experienced with this myself, but this is what\
          \ I have found. (I haven't been able to test in Ooga because I'm still at\
          \ work and using SSH to connect to my PC &lt;.&lt;)</p>\n<p><strong>When\
          \ I 4-bit quantize using the latest (oobabooga/GPTQ-for-LLaMa) and try to\
          \ inference it all I get is gibberish:</strong></p>\n<p><strong>(textgen)\
          \ nap@wintermute:~/Documents/text-generation-webui/repositories/GPTQ-for-LLaMa$</strong>\
          \ python llama_inference.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF/\
          \ --wbits 4 --groupsize 128 --load /home/nap/Documents/text-generation-webui/models/koala-13B-HF/koala13B-4bit-128g_OOGA-GPTQ.safetensors\
          \ --text \"Tell me about Koalas\" --max_length=200 --min_length=100</p>\n\
          <pre><code> \u2047  Tell me about Koalasskorou husagent StarMDb Stockrn\
          \ Au\xDF Burgrn hus Burg TournilerFD Reserve t\xE9matuoinbourg MatrixbourgFD\u043B\
          \u0438\u044FrutMDbrnrou\u043B\u0438\u044F t\xE9maturou stick Matrix Sud\
          \ Beau MatrixSort Burg Blarn stickoin husbourg substitution BourSortrutrnoinEventListener\
          \ Beau BurgMDbrou Beau StarMDb Stock husrut t\xE9matu Burg Wall frameskorut\
          \ titles titles t\xE9matu Wall hus substitutionSort Beaurou Burgoin\u043B\
          \u0438\u044FMatrix Bruno Bourilerrut Wall hus Fourier Stockbourg HyMDb Bla\
          \ Bla Au\xDF t\xE9matuFDMDb Star Burg Sud Bouragent Bour Tournrn Tourn Bla\
          \ frame Sud Bruno Bruno Sudagent t\xE9matu hus Au\xDF Bour Stock Bruno Burg\
          \ BeauoinbourgMatrix respectrnrn Stock titles Stockagent loobrernrn stick\
          \ BourMDb Burg BourMatrix MatrixMDb respect stick t\xE9matu titlesFDMatrixagent\
          \ stickMDb lo Reserve Sud Bour titles Starrut hus MatrixMDb lorut stickrou\
          \ consprn Boursko Bour StarMDbbourgrou Matrix Reserve Hy MatrixSort Bruno\u043B\
          \u0438\u044F Bour Fourier Beau t\xE9matu Bla Fourier BlaMDbrn hus Burg\n\
          </code></pre>\n<p><strong>When I 4-bit quantize using the latest (qwopqwop200/GPTQ-for-LLaMa)\
          \ and try to inference it the output is correct:</strong></p>\n<p><strong>(gptq)\
          \ nap@wintermute:~/Documents/GPTQ-for-LLaMa$</strong> python llama_inference.py\
          \ /home/nap/Documents/text-generation-webui/models/koala-13B-HF/ --wbits\
          \ 4 --groupsize 128 --load /home/nap/Documents/text-generation-webui/models/koala-13B-HF/koala13B-4bit-128g_NEW-GPTQ.safetensors\
          \ --text \"Tell me about Koalas\" --max_length=200 --min_length=100 --device=0</p>\n\
          <pre><code>\u2047  Tell me about Koalas. Koalas are marsupials that live\
          \ in Australia and are known for their distinctive black and white fur and\
          \ habit of sleeping in trees. They have a slow rate of reproduction and\
          \ may live up to 10 years in the wild.\n\nD) That's interesting. Can you\
          \ tell me more about Koalas? Koalas are marsupials that live in Australia\
          \ and are known for their distinctive black and white fur and habit of sleeping\
          \ in trees. They have a slow rate of reproduction and may live up to 10\
          \ years in the wild.\n\nE) Let's talk about Koalas. Koalas are marsupials\
          \ that live in Australia and are known for their distinctive black and white\
          \ fur and habit of sleeping in trees. They have a slow rate of reproduction\
          \ and may live up to 10 years in the wild.\n</code></pre>\n<p><strong>HOWEVER</strong>\
          \ The model quantized with (qwopqwop200/GPTQ-for-LLaMa) cannot be inferenced\
          \ by the Ooga-webui version (oobabooga/GPTQ-for-LLaMa)</p>\n<p>I wasn't\
          \ using the 4bit file from this repo but 2 versions that I quantized for\
          \ 13b for testing. (one with each version of GPTQ-for-LLaMa)</p>\n<p><strong>EDIT:</strong>\
          \ It appears If I clone the latest (qwopqwop200/GPTQ-for-LLaMa) that I used\
          \ to quantize the model into Ooba's repositories I am at least able to load\
          \ the model (Where I used to have to use Ooba's fork)<br><strong>BUT</strong>\
          \ I still don't know if it actually works because I am not home^^; will\
          \ report back!</p>\n<p>Not sure if that helps, but that's where I'm at!</p>\n"
        raw: "I'm still not very experienced with this myself, but this is what I\
          \ have found. (I haven't been able to test in Ooga because I'm still at\
          \ work and using SSH to connect to my PC <.<)\n\n**When I 4-bit quantize\
          \ using the latest (oobabooga/GPTQ-for-LLaMa) and try to inference it all\
          \ I get is gibberish:**\n\n**(textgen) nap@wintermute:~/Documents/text-generation-webui/repositories/GPTQ-for-LLaMa$**\
          \ python llama_inference.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF/\
          \ --wbits 4 --groupsize 128 --load /home/nap/Documents/text-generation-webui/models/koala-13B-HF/koala13B-4bit-128g_OOGA-GPTQ.safetensors\
          \ --text \"Tell me about Koalas\" --max_length=200 --min_length=100\n\n\
          ```\n \u2047  Tell me about Koalasskorou husagent StarMDb Stockrn Au\xDF\
          \ Burgrn hus Burg TournilerFD Reserve t\xE9matuoinbourg MatrixbourgFD\u043B\
          \u0438\u044FrutMDbrnrou\u043B\u0438\u044F t\xE9maturou stick Matrix Sud\
          \ Beau MatrixSort Burg Blarn stickoin husbourg substitution BourSortrutrnoinEventListener\
          \ Beau BurgMDbrou Beau StarMDb Stock husrut t\xE9matu Burg Wall frameskorut\
          \ titles titles t\xE9matu Wall hus substitutionSort Beaurou Burgoin\u043B\
          \u0438\u044FMatrix Bruno Bourilerrut Wall hus Fourier Stockbourg HyMDb Bla\
          \ Bla Au\xDF t\xE9matuFDMDb Star Burg Sud Bouragent Bour Tournrn Tourn Bla\
          \ frame Sud Bruno Bruno Sudagent t\xE9matu hus Au\xDF Bour Stock Bruno Burg\
          \ BeauoinbourgMatrix respectrnrn Stock titles Stockagent loobrernrn stick\
          \ BourMDb Burg BourMatrix MatrixMDb respect stick t\xE9matu titlesFDMatrixagent\
          \ stickMDb lo Reserve Sud Bour titles Starrut hus MatrixMDb lorut stickrou\
          \ consprn Boursko Bour StarMDbbourgrou Matrix Reserve Hy MatrixSort Bruno\u043B\
          \u0438\u044F Bour Fourier Beau t\xE9matu Bla Fourier BlaMDbrn hus Burg\n\
          ```\n\n**When I 4-bit quantize using the latest (qwopqwop200/GPTQ-for-LLaMa)\
          \ and try to inference it the output is correct:**\n\n**(gptq) nap@wintermute:~/Documents/GPTQ-for-LLaMa$**\
          \ python llama_inference.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF/\
          \ --wbits 4 --groupsize 128 --load /home/nap/Documents/text-generation-webui/models/koala-13B-HF/koala13B-4bit-128g_NEW-GPTQ.safetensors\
          \ --text \"Tell me about Koalas\" --max_length=200 --min_length=100 --device=0\n\
          ```\n\u2047  Tell me about Koalas. Koalas are marsupials that live in Australia\
          \ and are known for their distinctive black and white fur and habit of sleeping\
          \ in trees. They have a slow rate of reproduction and may live up to 10\
          \ years in the wild.\n\nD) That's interesting. Can you tell me more about\
          \ Koalas? Koalas are marsupials that live in Australia and are known for\
          \ their distinctive black and white fur and habit of sleeping in trees.\
          \ They have a slow rate of reproduction and may live up to 10 years in the\
          \ wild.\n\nE) Let's talk about Koalas. Koalas are marsupials that live in\
          \ Australia and are known for their distinctive black and white fur and\
          \ habit of sleeping in trees. They have a slow rate of reproduction and\
          \ may live up to 10 years in the wild.\n```\n\n**HOWEVER** The model quantized\
          \ with (qwopqwop200/GPTQ-for-LLaMa) cannot be inferenced by the Ooga-webui\
          \ version (oobabooga/GPTQ-for-LLaMa)\n\nI wasn't using the 4bit file from\
          \ this repo but 2 versions that I quantized for 13b for testing. (one with\
          \ each version of GPTQ-for-LLaMa)\n\n**EDIT:** It appears If I clone the\
          \ latest (qwopqwop200/GPTQ-for-LLaMa) that I used to quantize the model\
          \ into Ooba's repositories I am at least able to load the model (Where I\
          \ used to have to use Ooba's fork)\n**BUT** I still don't know if it actually\
          \ works because I am not home^^; will report back!\n \nNot sure if that\
          \ helps, but that's where I'm at!"
        updatedAt: '2023-04-07T21:27:05.234Z'
      numEdits: 4
      reactions: []
    id: 643084b6dbf96004473fd5d5
    type: comment
  author: disarmyouwitha
  content: "I'm still not very experienced with this myself, but this is what I have\
    \ found. (I haven't been able to test in Ooga because I'm still at work and using\
    \ SSH to connect to my PC <.<)\n\n**When I 4-bit quantize using the latest (oobabooga/GPTQ-for-LLaMa)\
    \ and try to inference it all I get is gibberish:**\n\n**(textgen) nap@wintermute:~/Documents/text-generation-webui/repositories/GPTQ-for-LLaMa$**\
    \ python llama_inference.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF/\
    \ --wbits 4 --groupsize 128 --load /home/nap/Documents/text-generation-webui/models/koala-13B-HF/koala13B-4bit-128g_OOGA-GPTQ.safetensors\
    \ --text \"Tell me about Koalas\" --max_length=200 --min_length=100\n\n```\n \u2047\
    \  Tell me about Koalasskorou husagent StarMDb Stockrn Au\xDF Burgrn hus Burg\
    \ TournilerFD Reserve t\xE9matuoinbourg MatrixbourgFD\u043B\u0438\u044FrutMDbrnrou\u043B\
    \u0438\u044F t\xE9maturou stick Matrix Sud Beau MatrixSort Burg Blarn stickoin\
    \ husbourg substitution BourSortrutrnoinEventListener Beau BurgMDbrou Beau StarMDb\
    \ Stock husrut t\xE9matu Burg Wall frameskorut titles titles t\xE9matu Wall hus\
    \ substitutionSort Beaurou Burgoin\u043B\u0438\u044FMatrix Bruno Bourilerrut Wall\
    \ hus Fourier Stockbourg HyMDb Bla Bla Au\xDF t\xE9matuFDMDb Star Burg Sud Bouragent\
    \ Bour Tournrn Tourn Bla frame Sud Bruno Bruno Sudagent t\xE9matu hus Au\xDF Bour\
    \ Stock Bruno Burg BeauoinbourgMatrix respectrnrn Stock titles Stockagent loobrernrn\
    \ stick BourMDb Burg BourMatrix MatrixMDb respect stick t\xE9matu titlesFDMatrixagent\
    \ stickMDb lo Reserve Sud Bour titles Starrut hus MatrixMDb lorut stickrou consprn\
    \ Boursko Bour StarMDbbourgrou Matrix Reserve Hy MatrixSort Bruno\u043B\u0438\u044F\
    \ Bour Fourier Beau t\xE9matu Bla Fourier BlaMDbrn hus Burg\n```\n\n**When I 4-bit\
    \ quantize using the latest (qwopqwop200/GPTQ-for-LLaMa) and try to inference\
    \ it the output is correct:**\n\n**(gptq) nap@wintermute:~/Documents/GPTQ-for-LLaMa$**\
    \ python llama_inference.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF/\
    \ --wbits 4 --groupsize 128 --load /home/nap/Documents/text-generation-webui/models/koala-13B-HF/koala13B-4bit-128g_NEW-GPTQ.safetensors\
    \ --text \"Tell me about Koalas\" --max_length=200 --min_length=100 --device=0\n\
    ```\n\u2047  Tell me about Koalas. Koalas are marsupials that live in Australia\
    \ and are known for their distinctive black and white fur and habit of sleeping\
    \ in trees. They have a slow rate of reproduction and may live up to 10 years\
    \ in the wild.\n\nD) That's interesting. Can you tell me more about Koalas? Koalas\
    \ are marsupials that live in Australia and are known for their distinctive black\
    \ and white fur and habit of sleeping in trees. They have a slow rate of reproduction\
    \ and may live up to 10 years in the wild.\n\nE) Let's talk about Koalas. Koalas\
    \ are marsupials that live in Australia and are known for their distinctive black\
    \ and white fur and habit of sleeping in trees. They have a slow rate of reproduction\
    \ and may live up to 10 years in the wild.\n```\n\n**HOWEVER** The model quantized\
    \ with (qwopqwop200/GPTQ-for-LLaMa) cannot be inferenced by the Ooga-webui version\
    \ (oobabooga/GPTQ-for-LLaMa)\n\nI wasn't using the 4bit file from this repo but\
    \ 2 versions that I quantized for 13b for testing. (one with each version of GPTQ-for-LLaMa)\n\
    \n**EDIT:** It appears If I clone the latest (qwopqwop200/GPTQ-for-LLaMa) that\
    \ I used to quantize the model into Ooba's repositories I am at least able to\
    \ load the model (Where I used to have to use Ooba's fork)\n**BUT** I still don't\
    \ know if it actually works because I am not home^^; will report back!\n \nNot\
    \ sure if that helps, but that's where I'm at!"
  created_at: 2023-04-07 20:01:42+00:00
  edited: true
  hidden: false
  id: 643084b6dbf96004473fd5d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-07T21:25:02.000Z'
    data:
      edited: false
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>Here''s the difference that I know of: </p>

          <ul>

          <li>The default qwopqwop200''s GPTQ branch is using triton (triton isn''t
          available on Windows). </li>

          <li>Oobabooga''s textgen wiki is asking to install the cuda branch regardless
          of the operating system.</li>

          <li>Oobabooga''s one click installer for Windows is also using the cuda
          branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.</li>

          </ul>

          '
        raw: "Here's the difference that I know of: \n\n- The default qwopqwop200's\
          \ GPTQ branch is using triton (triton isn't available on Windows). \n- Oobabooga's\
          \ textgen wiki is asking to install the cuda branch regardless of the operating\
          \ system.\n- Oobabooga's one click installer for Windows is also using the\
          \ cuda branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773."
        updatedAt: '2023-04-07T21:25:02.377Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - disarmyouwitha
    id: 64308a2ea04a488be6373122
    type: comment
  author: cmh
  content: "Here's the difference that I know of: \n\n- The default qwopqwop200's\
    \ GPTQ branch is using triton (triton isn't available on Windows). \n- Oobabooga's\
    \ textgen wiki is asking to install the cuda branch regardless of the operating\
    \ system.\n- Oobabooga's one click installer for Windows is also using the cuda\
    \ branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773."
  created_at: 2023-04-07 20:25:02+00:00
  edited: false
  hidden: false
  id: 64308a2ea04a488be6373122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-07T21:30:41.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<blockquote>

          <p>Here''s the difference that I know of: </p>

          <ul>

          <li>The default qwopqwop200''s GPTQ branch is using triton (triton isn''t
          available on Windows). </li>

          <li>Oobabooga''s textgen wiki is asking to install the cuda branch regardless
          of the operating system.</li>

          <li>Oobabooga''s one click installer for Windows is also using the cuda
          branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.</li>

          </ul>

          </blockquote>

          <p>I do think this is the reason for my error when trying to run the (triton)
          quantized version on (cuda).</p>

          <p>Installing the triton version into <code>text-generation-webui/repositories</code>
          did allow me to inference, and to start the webui. (but I cant test the
          results until i get home)</p>

          <p>Still not sure why the version I quantized with (oobabooga/GPTQ-for-LLaMa)
          just produced gibberish though~</p>

          <p>Maybe I shouldn''t have done a git pull, lol.</p>

          '
        raw: "> Here's the difference that I know of: \n> \n> - The default qwopqwop200's\
          \ GPTQ branch is using triton (triton isn't available on Windows). \n> -\
          \ Oobabooga's textgen wiki is asking to install the cuda branch regardless\
          \ of the operating system.\n> - Oobabooga's one click installer for Windows\
          \ is also using the cuda branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.\n\
          \nI do think this is the reason for my error when trying to run the (triton)\
          \ quantized version on (cuda).\n\nInstalling the triton version into `text-generation-webui/repositories`\
          \ did allow me to inference, and to start the webui. (but I cant test the\
          \ results until i get home)\n\nStill not sure why the version I quantized\
          \ with (oobabooga/GPTQ-for-LLaMa) just produced gibberish though~\n\nMaybe\
          \ I shouldn't have done a git pull, lol."
        updatedAt: '2023-04-07T21:31:50.309Z'
      numEdits: 2
      reactions: []
    id: 64308b81036373277e859282
    type: comment
  author: disarmyouwitha
  content: "> Here's the difference that I know of: \n> \n> - The default qwopqwop200's\
    \ GPTQ branch is using triton (triton isn't available on Windows). \n> - Oobabooga's\
    \ textgen wiki is asking to install the cuda branch regardless of the operating\
    \ system.\n> - Oobabooga's one click installer for Windows is also using the cuda\
    \ branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.\n\nI do think\
    \ this is the reason for my error when trying to run the (triton) quantized version\
    \ on (cuda).\n\nInstalling the triton version into `text-generation-webui/repositories`\
    \ did allow me to inference, and to start the webui. (but I cant test the results\
    \ until i get home)\n\nStill not sure why the version I quantized with (oobabooga/GPTQ-for-LLaMa)\
    \ just produced gibberish though~\n\nMaybe I shouldn't have done a git pull, lol."
  created_at: 2023-04-07 20:30:41+00:00
  edited: true
  hidden: false
  id: 64308b81036373277e859282
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T21:33:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p><strong>EDIT:</strong> It appears If I clone the latest (qwopqwop200/GPTQ-for-LLaMa)
          that I used to quantize the model into Ooba''s repositories I am at least
          able to load the model (Where I used to have to use Ooba''s fork) <strong>BUT</strong>
          I still don''t know if it actually works because I am not home^^; will report
          back!</p>

          </blockquote>

          <p>Thanks for the findings! All your experiences match mine. I''ve already
          done this last step you mention above, and then it still doesn''t work properly.  I
          showed some screenshots of that above.  Sometimes the result seems partially
          intelligible, but equally as often it''s just gibberish and unusable.</p>

          <p>My guess is you''ll find the same when you try GPTQ-for-LLaMa inside
          the textgen UI, but would be good to know for sure.</p>

          <blockquote>

          <p>Here''s the difference that I know of: </p>

          <ul>

          <li>The default qwopqwop200''s GPTQ branch is using triton (triton isn''t
          available on Windows). </li>

          <li>Oobabooga''s textgen wiki is asking to install the cuda branch regardless
          of the operating system.</li>

          <li>Oobabooga''s one click installer for Windows is also using the cuda
          branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.</li>

          </ul>

          </blockquote>

          <p>Yes this is all true. So far we''ve been cloning the cuda branch when
          we try GPTQ-for-LLaMa inside textgen UI.  In fact I believe you have to,
          because if you clone the Triton branch there''s no setup.py so it can''t
          be built anywhere.  So far as I can see, there''s no way to use the Triton
          branch as a module of textgen UI.  It can only be used for the quantization
          process itself.  </p>

          <p>Eg these are the commands I used to try latest GPTQ-for-LLaMa inside
          textgen UI:</p>

          <pre><code>cd text-generation-webui

          mkdir -p repositories

          cd repositories

          git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda GPTQ-for-LLaMa

          cd GPTQ-for-LLaMa

          python setup_cuda.py install --force

          </code></pre>

          <p>If I''d left out the <code>-b cuda</code> and cloned the main repo, then
          the last step - <code>python setup_cuda.py install --force</code> would
          fail, as there''s no setup*.py in the Triton branch.</p>

          <p>With regards to quantizing itself, I tried both the Triton and the CUDA
          versions and both worked, and produced 100% identical files; exactly the
          same SHA256SUM.  So I don''t know what the actual difference is between
          using those methods. Maybe just performance (I didn''t time how long they
          took.)</p>

          <p>I''m starting to wonder if this is all just down to some bug in inference
          in textgen UI.  We know the inference code of GPTQ-for-LLaMa works fine,
          so the files are presumably OK.  Although when I tried llama.cpp''s convert
          script to convert the quantized file to GGML, that also produced gibberish
          - though that was also using an older version of the GPTQ-for-LLaMa code,
          because the latest code uses a new structure that GGML does not yet support.</p>

          '
        raw: "> **EDIT:** It appears If I clone the latest (qwopqwop200/GPTQ-for-LLaMa)\
          \ that I used to quantize the model into Ooba's repositories I am at least\
          \ able to load the model (Where I used to have to use Ooba's fork) **BUT**\
          \ I still don't know if it actually works because I am not home^^; will\
          \ report back!\n\nThanks for the findings! All your experiences match mine.\
          \ I've already done this last step you mention above, and then it still\
          \ doesn't work properly.  I showed some screenshots of that above.  Sometimes\
          \ the result seems partially intelligible, but equally as often it's just\
          \ gibberish and unusable.\n\nMy guess is you'll find the same when you try\
          \ GPTQ-for-LLaMa inside the textgen UI, but would be good to know for sure.\n\
          \n> Here's the difference that I know of: \n> \n> - The default qwopqwop200's\
          \ GPTQ branch is using triton (triton isn't available on Windows). \n> -\
          \ Oobabooga's textgen wiki is asking to install the cuda branch regardless\
          \ of the operating system.\n> - Oobabooga's one click installer for Windows\
          \ is also using the cuda branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.\n\
          \nYes this is all true. So far we've been cloning the cuda branch when we\
          \ try GPTQ-for-LLaMa inside textgen UI.  In fact I believe you have to,\
          \ because if you clone the Triton branch there's no setup.py so it can't\
          \ be built anywhere.  So far as I can see, there's no way to use the Triton\
          \ branch as a module of textgen UI.  It can only be used for the quantization\
          \ process itself.  \n\nEg these are the commands I used to try latest GPTQ-for-LLaMa\
          \ inside textgen UI:\n```\ncd text-generation-webui\nmkdir -p repositories\n\
          cd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ -b cuda GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\npython setup_cuda.py install\
          \ --force\n```\n\nIf I'd left out the `-b cuda` and cloned the main repo,\
          \ then the last step - `python setup_cuda.py install --force` would fail,\
          \ as there's no setup*.py in the Triton branch.\n\nWith regards to quantizing\
          \ itself, I tried both the Triton and the CUDA versions and both worked,\
          \ and produced 100% identical files; exactly the same SHA256SUM.  So I don't\
          \ know what the actual difference is between using those methods. Maybe\
          \ just performance (I didn't time how long they took.)\n\nI'm starting to\
          \ wonder if this is all just down to some bug in inference in textgen UI.\
          \  We know the inference code of GPTQ-for-LLaMa works fine, so the files\
          \ are presumably OK.  Although when I tried llama.cpp's convert script to\
          \ convert the quantized file to GGML, that also produced gibberish - though\
          \ that was also using an older version of the GPTQ-for-LLaMa code, because\
          \ the latest code uses a new structure that GGML does not yet support."
        updatedAt: '2023-04-07T21:33:09.339Z'
      numEdits: 0
      reactions: []
    id: 64308c15dbf96004474007d8
    type: comment
  author: TheBloke
  content: "> **EDIT:** It appears If I clone the latest (qwopqwop200/GPTQ-for-LLaMa)\
    \ that I used to quantize the model into Ooba's repositories I am at least able\
    \ to load the model (Where I used to have to use Ooba's fork) **BUT** I still\
    \ don't know if it actually works because I am not home^^; will report back!\n\
    \nThanks for the findings! All your experiences match mine. I've already done\
    \ this last step you mention above, and then it still doesn't work properly. \
    \ I showed some screenshots of that above.  Sometimes the result seems partially\
    \ intelligible, but equally as often it's just gibberish and unusable.\n\nMy guess\
    \ is you'll find the same when you try GPTQ-for-LLaMa inside the textgen UI, but\
    \ would be good to know for sure.\n\n> Here's the difference that I know of: \n\
    > \n> - The default qwopqwop200's GPTQ branch is using triton (triton isn't available\
    \ on Windows). \n> - Oobabooga's textgen wiki is asking to install the cuda branch\
    \ regardless of the operating system.\n> - Oobabooga's one click installer for\
    \ Windows is also using the cuda branch but the commit a6f363e3f93b9fb5c26064b5ac7ed58d22e3f773.\n\
    \nYes this is all true. So far we've been cloning the cuda branch when we try\
    \ GPTQ-for-LLaMa inside textgen UI.  In fact I believe you have to, because if\
    \ you clone the Triton branch there's no setup.py so it can't be built anywhere.\
    \  So far as I can see, there's no way to use the Triton branch as a module of\
    \ textgen UI.  It can only be used for the quantization process itself.  \n\n\
    Eg these are the commands I used to try latest GPTQ-for-LLaMa inside textgen UI:\n\
    ```\ncd text-generation-webui\nmkdir -p repositories\ncd repositories\ngit clone\
    \ https://github.com/qwopqwop200/GPTQ-for-LLaMa -b cuda GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\n\
    python setup_cuda.py install --force\n```\n\nIf I'd left out the `-b cuda` and\
    \ cloned the main repo, then the last step - `python setup_cuda.py install --force`\
    \ would fail, as there's no setup*.py in the Triton branch.\n\nWith regards to\
    \ quantizing itself, I tried both the Triton and the CUDA versions and both worked,\
    \ and produced 100% identical files; exactly the same SHA256SUM.  So I don't know\
    \ what the actual difference is between using those methods. Maybe just performance\
    \ (I didn't time how long they took.)\n\nI'm starting to wonder if this is all\
    \ just down to some bug in inference in textgen UI.  We know the inference code\
    \ of GPTQ-for-LLaMa works fine, so the files are presumably OK.  Although when\
    \ I tried llama.cpp's convert script to convert the quantized file to GGML, that\
    \ also produced gibberish - though that was also using an older version of the\
    \ GPTQ-for-LLaMa code, because the latest code uses a new structure that GGML\
    \ does not yet support."
  created_at: 2023-04-07 20:33:09+00:00
  edited: false
  hidden: false
  id: 64308c15dbf96004474007d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-07T21:37:38.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>If we''ve made no more progress by tomorrow I will raise an issue
          with oogabooga on the textgen UI Github. Maybe he can shed some light on
          what is going on!</p>

          '
        raw: If we've made no more progress by tomorrow I will raise an issue with
          oogabooga on the textgen UI Github. Maybe he can shed some light on what
          is going on!
        updatedAt: '2023-04-07T21:37:38.182Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - cmh
        - Yuuru
    id: 64308d22036373277e859ccc
    type: comment
  author: TheBloke
  content: If we've made no more progress by tomorrow I will raise an issue with oogabooga
    on the textgen UI Github. Maybe he can shed some light on what is going on!
  created_at: 2023-04-07 20:37:38+00:00
  edited: false
  hidden: false
  id: 64308d22036373277e859ccc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-07T22:38:45.000Z'
    data:
      edited: false
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/dgO-Nmh3vs-jWbmr81lK5.png"><img
          alt="Screenshot 2023-04-07 at 5.35.23 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/dgO-Nmh3vs-jWbmr81lK5.png"></a></p>

          <p>Hm, looks alright. I did the quantization with latest (qwopqwop200/GPTQ-for-LLaMa)(triton)
          and also inference through (triton) version in the Ooba-webui. </p>

          <p>I will keep poking and report back.</p>

          '
        raw: "![Screenshot 2023-04-07 at 5.35.23 PM.png](https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/dgO-Nmh3vs-jWbmr81lK5.png)\n\
          \nHm, looks alright. I did the quantization with latest (qwopqwop200/GPTQ-for-LLaMa)(triton)\
          \ and also inference through (triton) version in the Ooba-webui. \n\nI will\
          \ keep poking and report back."
        updatedAt: '2023-04-07T22:38:45.757Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 64309b754228b8a0d3f39917
    type: comment
  author: disarmyouwitha
  content: "![Screenshot 2023-04-07 at 5.35.23 PM.png](https://cdn-uploads.huggingface.co/production/uploads/64123681ac08ffb707922dfb/dgO-Nmh3vs-jWbmr81lK5.png)\n\
    \nHm, looks alright. I did the quantization with latest (qwopqwop200/GPTQ-for-LLaMa)(triton)\
    \ and also inference through (triton) version in the Ooba-webui. \n\nI will keep\
    \ poking and report back."
  created_at: 2023-04-07 21:38:45+00:00
  edited: false
  hidden: false
  id: 64309b754228b8a0d3f39917
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T09:05:34.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You guys definitely seem to be getting better output than me. I\
          \ am so damned confused! :( No matter what I do I cannot get good output\
          \ out of the webui.</p>\n<p>Here's everything I'm doing, start to finish,\
          \ in Google Colab. Can anyone spot anything wrong, or different to what\
          \ you're doing?</p>\n<h2 id=\"python-dependencies-referenced-to-requirements-of-webui-and-gptq\"\
          >Python dependencies (referenced to requirements of webui and GPTQ):</h2>\n\
          <pre><code>pip3 uninstall -y torch torchvision torchaudio transformers peft\
          \ datasets loralib sentencepiece safetensors accelerate triton bitsandbytes\
          \ huggingface_hub flexgen rwkv\npip3 install torch torchvision torchaudio\
          \ --index-url https://download.pytorch.org/whl/cu118\npip3 install datasets==2.10.1\
          \ loralib sentencepiece safetensors==0.3.0 accelerate==0.18.0 triton==2.0.0\
          \ huggingface_hub\npip3 install git+https://github.com/huggingface/transformers\
          \ # need to install from github\npip3 install peft==0.2.0 #git+https://github.com/huggingface/peft.git\n\
          pip3 install bitsandbytes==0.37.2\npip3 install markdown pyyaml tqdm requests\
          \ gradio==3.24.1 flexgen==0.1.7 rwkv==0.7.3 ninja\n</code></pre>\n<h2 id=\"\
          download-the-hf-format-koala7b-i-previously-uploaded\">Download the HF format\
          \ Koala7B I previously uploaded:</h2>\n<pre><code>git clone https://huggingface.co/TheBloke/koala-7B-HF\n\
          </code></pre>\n<h2 id=\"download-latest-gptq-code-triton-branch\">Download\
          \ latest GPTQ code, Triton branch</h2>\n<pre><code>git clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ gptq-llama\n</code></pre>\n<h2 id=\"create-gptq-file-using-triton-branch\"\
          >Create GPTQ file using Triton branch</h2>\n<pre><code>cd gptq-llama\nCUDA_VISIBLE_DEVICES=0\
          \ python3 llama.py /content/koala-7B-HF c4 --wbits 4 --true-sequential --act-order\
          \ --groupsize 128 --save /content/triton.koala-7B-4bit-128g.pt\n</code></pre>\n\
          <h2 id=\"test-the-gptq-file-using-triton-branch---it-works\">Test the GPTQ\
          \ file using Triton branch - it works</h2>\n<pre><code>cd gptq-llama\nTOKENIZERS_PARALLELISM=false\
          \ CUDA_VISIBLE_DEVICES=0 python llama_inference.py  /content/koala-7B-HF\
          \ --wbits 4 --groupsize 128 --device 0 --load /content/triton.koala-7B-4bit-128g.pt\
          \ --max_length=200 --min_length=100 --text \"write a story about Kevin\"\
          \n</code></pre>\n<p>Output:</p>\n<pre><code>2023-04-08 08:37:09.656957:\
          \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning:\
          \ Could not find TensorRT\nLoading model ...\nDone.\n \u2047  write a story\
          \ about Kevin, a young boy who is struggling to come to terms with his father's\
          \ divorce. Kevin feels confused and angry about his father's actions and\
          \ doesn't understand why he can't just be happy again.\n\nOne day, while\
          \ he's playing with his best friend, they come across a time capsule that\
          \ they found in the park. Inside, they discover a letter from Kevin's father,\
          \ who is now living with another family. Kevin's father writes that he left\
          \ Kevin's mother for a younger woman because he couldn't handle the pain\
          \ and sadness of their divorce. He also apologizes for not being there for\
          \ Kevin and his mother.\n\nReading the letter, Kevin starts to understand\
          \ his father's perspective and the reasons why he left. He also starts to\
          \ see that his father's actions were not a reflection of his own love for\
          \ him, but rather a result of his own struggles\n</code></pre>\n<h2 id=\"\
          clone-webui-set-up-the-model-in-models-link-in-the-already-downloaded-gptq-triton-code-into-repositories\"\
          >Clone webui, set up the model in <code>models</code>, link in the already\
          \ downloaded GPTQ Triton code into <code>repositories</code></h2>\n<pre><code>rm\
          \ -rf /content/text-generation-webui\ngit clone https://github.com/oobabooga/text-generation-webui\n\
          cd text-generation-webui\nmkdir models/koala-7b-4bit\ncp /content/koala-7B-HF/*.{json,model}\
          \ models/koala-7b-4bit\ncp /content/triton.koala-7B-4bit-128g.pt models/koala-7b-4bit\n\
          mkdir repositories\ncd repositories\nln -s /content/gptq-llama ./GPTQ-for-LLaMa\n\
          </code></pre>\n<h2 id=\"run-webui\">Run webui</h2>\n<pre><code>cd /content/text-generation-webui\n\
          python server.py --model koala-7b-4bit --wbits 4 --groupsize 128 --model_type\
          \ LLaMA --auto-devices # --gpu-memory 20000MiB --bf16 --extensions llama_prompts\
          \  #\n</code></pre>\n<p>Output:</p>\n<pre><code>2023-04-08 08:51:31.724603:\
          \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning:\
          \ Could not find TensorRT\n\n===================================BUG REPORT===================================\n\
          Welcome to bitsandbytes. For bug reports, please submit your error trace\
          \ to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\
          /usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected!\
          \ Searching further paths...\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n\
          \  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'),\
          \ PosixPath('8013')}\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12\
          \ --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-1adw4b0e7lfcl\
          \ --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s\
          \ --enable_output_coalescing=true --output_coalescing_required=true')}\n\
          \  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/env/python')}\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n\
          \  warn(msg)\nCUDA_SETUP: WARNING! libcudart.so not found in any environmental\
          \ path. Searching /usr/local/cuda/lib64...\nCUDA SETUP: CUDA runtime path\
          \ found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute\
          \ capability among GPUs detected: 8.0\nCUDA SETUP: Detected CUDA version\
          \ 118\nCUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\
          Loading koala-7b-4bit...\nLoading model ...\nDone.\nUsing the following\
          \ device map for the 4-bit model: {'': 0}\nLoaded the model in 7.69 seconds.\n\
          Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link,\
          \ set `share=True` in `launch()`.\nOutput generated in 27.50 seconds (7.24\
          \ tokens/s, 199 tokens, context 6)\nOutput generated in 17.71 seconds (11.24\
          \ tokens/s, 199 tokens, context 6)\n</code></pre>\n<h2 id=\"try-something-fail-horribly\"\
          >Try something, fail horribly:</h2>\n<p>Example 1:<br><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_mDwEwZxn8hnvSOmdiD-U.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_mDwEwZxn8hnvSOmdiD-U.png\"\
          ></a></p>\n<p>Example 2:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/t2PKvIEqs3tGGaZ-YQeXA.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/t2PKvIEqs3tGGaZ-YQeXA.png\"\
          ></a></p>\n<p>Example 3 (Debug-deterministic parameter set):<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/z0xkNwXEtTNTlo32gVOmt.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/z0xkNwXEtTNTlo32gVOmt.png\"\
          ></a></p>\n<p>What am I doing wrong/differently?! :(</p>\n"
        raw: "You guys definitely seem to be getting better output than me. I am so\
          \ damned confused! :( No matter what I do I cannot get good output out of\
          \ the webui.\n\nHere's everything I'm doing, start to finish, in Google\
          \ Colab. Can anyone spot anything wrong, or different to what you're doing?\n\
          \n## Python dependencies (referenced to requirements of webui and GPTQ):\n\
          ```\npip3 uninstall -y torch torchvision torchaudio transformers peft datasets\
          \ loralib sentencepiece safetensors accelerate triton bitsandbytes huggingface_hub\
          \ flexgen rwkv\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\
          pip3 install datasets==2.10.1 loralib sentencepiece safetensors==0.3.0 accelerate==0.18.0\
          \ triton==2.0.0 huggingface_hub\npip3 install git+https://github.com/huggingface/transformers\
          \ # need to install from github\npip3 install peft==0.2.0 #git+https://github.com/huggingface/peft.git\n\
          pip3 install bitsandbytes==0.37.2\npip3 install markdown pyyaml tqdm requests\
          \ gradio==3.24.1 flexgen==0.1.7 rwkv==0.7.3 ninja\n```\n## Download the\
          \ HF format Koala7B I previously uploaded:\n```\ngit clone https://huggingface.co/TheBloke/koala-7B-HF\n\
          ```\n## Download latest GPTQ code, Triton branch\n```\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ gptq-llama\n```\n## Create GPTQ file using Triton branch\n```\ncd gptq-llama\n\
          CUDA_VISIBLE_DEVICES=0 python3 llama.py /content/koala-7B-HF c4 --wbits\
          \ 4 --true-sequential --act-order --groupsize 128 --save /content/triton.koala-7B-4bit-128g.pt\n\
          ```\n## Test the GPTQ file using Triton branch - it works\n```\ncd gptq-llama\n\
          TOKENIZERS_PARALLELISM=false CUDA_VISIBLE_DEVICES=0 python llama_inference.py\
          \  /content/koala-7B-HF --wbits 4 --groupsize 128 --device 0 --load /content/triton.koala-7B-4bit-128g.pt\
          \ --max_length=200 --min_length=100 --text \"write a story about Kevin\"\
          \n```\nOutput:\n```\n2023-04-08 08:37:09.656957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\nLoading model ...\nDone.\n \u2047\
          \  write a story about Kevin, a young boy who is struggling to come to terms\
          \ with his father's divorce. Kevin feels confused and angry about his father's\
          \ actions and doesn't understand why he can't just be happy again.\n\nOne\
          \ day, while he's playing with his best friend, they come across a time\
          \ capsule that they found in the park. Inside, they discover a letter from\
          \ Kevin's father, who is now living with another family. Kevin's father\
          \ writes that he left Kevin's mother for a younger woman because he couldn't\
          \ handle the pain and sadness of their divorce. He also apologizes for not\
          \ being there for Kevin and his mother.\n\nReading the letter, Kevin starts\
          \ to understand his father's perspective and the reasons why he left. He\
          \ also starts to see that his father's actions were not a reflection of\
          \ his own love for him, but rather a result of his own struggles\n```\n\
          ## Clone webui, set up the model in `models`, link in the already downloaded\
          \ GPTQ Triton code into `repositories`\n```\nrm -rf /content/text-generation-webui\n\
          git clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n\
          mkdir models/koala-7b-4bit\ncp /content/koala-7B-HF/*.{json,model} models/koala-7b-4bit\n\
          cp /content/triton.koala-7B-4bit-128g.pt models/koala-7b-4bit\nmkdir repositories\n\
          cd repositories\nln -s /content/gptq-llama ./GPTQ-for-LLaMa\n```\n## Run\
          \ webui\n```\ncd /content/text-generation-webui\npython server.py --model\
          \ koala-7b-4bit --wbits 4 --groupsize 128 --model_type LLaMA --auto-devices\
          \ # --gpu-memory 20000MiB --bf16 --extensions llama_prompts  #\n```\nOutput:\n\
          ```\n2023-04-08 08:51:31.724603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\n\n===================================BUG\
          \ REPORT===================================\nWelcome to bitsandbytes. For\
          \ bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
          ================================================================================\n\
          /usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected!\
          \ Searching further paths...\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n\
          \  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'),\
          \ PosixPath('8013')}\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12\
          \ --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-1adw4b0e7lfcl\
          \ --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s\
          \ --enable_output_coalescing=true --output_coalescing_required=true')}\n\
          \  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/env/python')}\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n\
          \  warn(msg)\nCUDA_SETUP: WARNING! libcudart.so not found in any environmental\
          \ path. Searching /usr/local/cuda/lib64...\nCUDA SETUP: CUDA runtime path\
          \ found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute\
          \ capability among GPUs detected: 8.0\nCUDA SETUP: Detected CUDA version\
          \ 118\nCUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\
          Loading koala-7b-4bit...\nLoading model ...\nDone.\nUsing the following\
          \ device map for the 4-bit model: {'': 0}\nLoaded the model in 7.69 seconds.\n\
          Running on local URL:  http://127.0.0.1:7860\n\nTo create a public link,\
          \ set `share=True` in `launch()`.\nOutput generated in 27.50 seconds (7.24\
          \ tokens/s, 199 tokens, context 6)\nOutput generated in 17.71 seconds (11.24\
          \ tokens/s, 199 tokens, context 6)\n```\n## Try something, fail horribly:\n\
          \nExample 1:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_mDwEwZxn8hnvSOmdiD-U.png)\n\
          \nExample 2:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/t2PKvIEqs3tGGaZ-YQeXA.png)\n\
          \nExample 3 (Debug-deterministic parameter set):\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/z0xkNwXEtTNTlo32gVOmt.png)\n\
          \n\nWhat am I doing wrong/differently?! :("
        updatedAt: '2023-04-08T09:14:42.483Z'
      numEdits: 3
      reactions: []
    id: 64312e5e9da01699250f4bcf
    type: comment
  author: TheBloke
  content: "You guys definitely seem to be getting better output than me. I am so\
    \ damned confused! :( No matter what I do I cannot get good output out of the\
    \ webui.\n\nHere's everything I'm doing, start to finish, in Google Colab. Can\
    \ anyone spot anything wrong, or different to what you're doing?\n\n## Python\
    \ dependencies (referenced to requirements of webui and GPTQ):\n```\npip3 uninstall\
    \ -y torch torchvision torchaudio transformers peft datasets loralib sentencepiece\
    \ safetensors accelerate triton bitsandbytes huggingface_hub flexgen rwkv\npip3\
    \ install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\
    pip3 install datasets==2.10.1 loralib sentencepiece safetensors==0.3.0 accelerate==0.18.0\
    \ triton==2.0.0 huggingface_hub\npip3 install git+https://github.com/huggingface/transformers\
    \ # need to install from github\npip3 install peft==0.2.0 #git+https://github.com/huggingface/peft.git\n\
    pip3 install bitsandbytes==0.37.2\npip3 install markdown pyyaml tqdm requests\
    \ gradio==3.24.1 flexgen==0.1.7 rwkv==0.7.3 ninja\n```\n## Download the HF format\
    \ Koala7B I previously uploaded:\n```\ngit clone https://huggingface.co/TheBloke/koala-7B-HF\n\
    ```\n## Download latest GPTQ code, Triton branch\n```\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
    \ gptq-llama\n```\n## Create GPTQ file using Triton branch\n```\ncd gptq-llama\n\
    CUDA_VISIBLE_DEVICES=0 python3 llama.py /content/koala-7B-HF c4 --wbits 4 --true-sequential\
    \ --act-order --groupsize 128 --save /content/triton.koala-7B-4bit-128g.pt\n```\n\
    ## Test the GPTQ file using Triton branch - it works\n```\ncd gptq-llama\nTOKENIZERS_PARALLELISM=false\
    \ CUDA_VISIBLE_DEVICES=0 python llama_inference.py  /content/koala-7B-HF --wbits\
    \ 4 --groupsize 128 --device 0 --load /content/triton.koala-7B-4bit-128g.pt --max_length=200\
    \ --min_length=100 --text \"write a story about Kevin\"\n```\nOutput:\n```\n2023-04-08\
    \ 08:37:09.656957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT\
    \ Warning: Could not find TensorRT\nLoading model ...\nDone.\n \u2047  write a\
    \ story about Kevin, a young boy who is struggling to come to terms with his father's\
    \ divorce. Kevin feels confused and angry about his father's actions and doesn't\
    \ understand why he can't just be happy again.\n\nOne day, while he's playing\
    \ with his best friend, they come across a time capsule that they found in the\
    \ park. Inside, they discover a letter from Kevin's father, who is now living\
    \ with another family. Kevin's father writes that he left Kevin's mother for a\
    \ younger woman because he couldn't handle the pain and sadness of their divorce.\
    \ He also apologizes for not being there for Kevin and his mother.\n\nReading\
    \ the letter, Kevin starts to understand his father's perspective and the reasons\
    \ why he left. He also starts to see that his father's actions were not a reflection\
    \ of his own love for him, but rather a result of his own struggles\n```\n## Clone\
    \ webui, set up the model in `models`, link in the already downloaded GPTQ Triton\
    \ code into `repositories`\n```\nrm -rf /content/text-generation-webui\ngit clone\
    \ https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n\
    mkdir models/koala-7b-4bit\ncp /content/koala-7B-HF/*.{json,model} models/koala-7b-4bit\n\
    cp /content/triton.koala-7B-4bit-128g.pt models/koala-7b-4bit\nmkdir repositories\n\
    cd repositories\nln -s /content/gptq-llama ./GPTQ-for-LLaMa\n```\n## Run webui\n\
    ```\ncd /content/text-generation-webui\npython server.py --model koala-7b-4bit\
    \ --wbits 4 --groupsize 128 --model_type LLaMA --auto-devices # --gpu-memory 20000MiB\
    \ --bf16 --extensions llama_prompts  #\n```\nOutput:\n```\n2023-04-08 08:51:31.724603:\
    \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could\
    \ not find TensorRT\n\n===================================BUG REPORT===================================\n\
    Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    /usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning:\
    \ /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further\
    \ paths...\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
    \ UserWarning: WARNING: The following directories listed in your path were found\
    \ to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n\
    \  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
    \ UserWarning: WARNING: The following directories listed in your path were found\
    \ to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n\
    \  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
    \ UserWarning: WARNING: The following directories listed in your path were found\
    \ to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12\
    \ --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-1adw4b0e7lfcl\
    \ --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s\
    \ --enable_output_coalescing=true --output_coalescing_required=true')}\n  warn(msg)\n\
    /usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning:\
    \ WARNING: The following directories listed in your path were found to be non-existent:\
    \ {PosixPath('/env/python')}\n  warn(msg)\n/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136:\
    \ UserWarning: WARNING: The following directories listed in your path were found\
    \ to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n\
    \  warn(msg)\nCUDA_SETUP: WARNING! libcudart.so not found in any environmental\
    \ path. Searching /usr/local/cuda/lib64...\nCUDA SETUP: CUDA runtime path found:\
    \ /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among\
    \ GPUs detected: 8.0\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading\
    \ binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\
    Loading koala-7b-4bit...\nLoading model ...\nDone.\nUsing the following device\
    \ map for the 4-bit model: {'': 0}\nLoaded the model in 7.69 seconds.\nRunning\
    \ on local URL:  http://127.0.0.1:7860\n\nTo create a public link, set `share=True`\
    \ in `launch()`.\nOutput generated in 27.50 seconds (7.24 tokens/s, 199 tokens,\
    \ context 6)\nOutput generated in 17.71 seconds (11.24 tokens/s, 199 tokens, context\
    \ 6)\n```\n## Try something, fail horribly:\n\nExample 1:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/_mDwEwZxn8hnvSOmdiD-U.png)\n\
    \nExample 2:\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/t2PKvIEqs3tGGaZ-YQeXA.png)\n\
    \nExample 3 (Debug-deterministic parameter set):\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/z0xkNwXEtTNTlo32gVOmt.png)\n\
    \n\nWhat am I doing wrong/differently?! :("
  created_at: 2023-04-08 08:05:34+00:00
  edited: true
  hidden: false
  id: 64312e5e9da01699250f4bcf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
      fullname: diarmyouwitha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: disarmyouwitha
      type: user
    createdAt: '2023-04-08T15:09:02.000Z'
    data:
      edited: true
      editors:
      - disarmyouwitha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5381d6a8cdb30363827bf6936a0ff287.svg
          fullname: diarmyouwitha
          isHf: false
          isPro: false
          name: disarmyouwitha
          type: user
        html: "<p>Hmm.. it all looks correct to me. Here are the steps I took to set\
          \ up the environment:</p>\n<pre><code>**#create conda env: (I used conda\
          \ to keep my environment separate, but you don't have to)**\nconda create\
          \ -n textgen python=3.10.9\nconda activate textgen\n\npip install torch\
          \ torchvision torchaudio\n\n**# (oobabooga/text-generation-webui):**\ngit\
          \ clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n\
          pip install -r requirements.txt\n\n**# (qwopqwop200/GPTQ-for-LLaMa):**\n\
          mkdir repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\npip install -r requirements.txt\n\n\
          **# start server:**\npython server.py --model_type llama --wbits 4 --groupsize\
          \ 128 \n</code></pre>\n<p>Here is the command I used to quantize:</p>\n\
          <pre><code>CUDA_VISIBLE_DEVICES=0 python llama.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF\
          \ c4 --wbits 4 --true-sequential --act-order --groupsize 128 --save_safetensors\
          \ /home/nap/Documents/text-generation-webui/models/koala13B-4bit-128g.safetensors\n\
          </code></pre>\n<p><strong>EDIT:</strong><br>I did notice we are using different\
          \ versions of CUDA but I don't think this should matter?:</p>\n<pre><code>(textgen)\
          \ nap@wintermute:~/Documents/text-generation-webui$ ./start.sh \n\n===================================BUG\
          \ REPORT===================================\nWelcome to bitsandbytes. For\
          \ bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
          ================================================================================\n\
          CUDA SETUP: CUDA runtime path found: /home/nap/miniconda3/envs/textgen/lib/libcudart.so\n\
          CUDA SETUP: Highest compute capability among GPUs detected: 8.9\nCUDA SETUP:\
          \ Detected CUDA version 117\nCUDA SETUP: Loading binary /home/nap/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
          </code></pre>\n"
        raw: "Hmm.. it all looks correct to me. Here are the steps I took to set up\
          \ the environment:\n```\n**#create conda env: (I used conda to keep my environment\
          \ separate, but you don't have to)**\nconda create -n textgen python=3.10.9\n\
          conda activate textgen\n\npip install torch torchvision torchaudio\n\n**#\
          \ (oobabooga/text-generation-webui):**\ngit clone https://github.com/oobabooga/text-generation-webui\n\
          cd text-generation-webui\npip install -r requirements.txt\n\n**# (qwopqwop200/GPTQ-for-LLaMa):**\n\
          mkdir repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
          \ GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\npip install -r requirements.txt\n\n\
          **# start server:**\npython server.py --model_type llama --wbits 4 --groupsize\
          \ 128 \n```\n\nHere is the command I used to quantize:\n```\nCUDA_VISIBLE_DEVICES=0\
          \ python llama.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF\
          \ c4 --wbits 4 --true-sequential --act-order --groupsize 128 --save_safetensors\
          \ /home/nap/Documents/text-generation-webui/models/koala13B-4bit-128g.safetensors\n\
          ```\n\n**EDIT:**\nI did notice we are using different versions of CUDA but\
          \ I don't think this should matter?:\n```\n(textgen) nap@wintermute:~/Documents/text-generation-webui$\
          \ ./start.sh \n\n===================================BUG REPORT===================================\n\
          Welcome to bitsandbytes. For bug reports, please submit your error trace\
          \ to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\
          CUDA SETUP: CUDA runtime path found: /home/nap/miniconda3/envs/textgen/lib/libcudart.so\n\
          CUDA SETUP: Highest compute capability among GPUs detected: 8.9\nCUDA SETUP:\
          \ Detected CUDA version 117\nCUDA SETUP: Loading binary /home/nap/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
          ```"
        updatedAt: '2023-04-08T15:42:32.978Z'
      numEdits: 2
      reactions: []
    id: 6431838e1cbbdbac56a6190b
    type: comment
  author: disarmyouwitha
  content: "Hmm.. it all looks correct to me. Here are the steps I took to set up\
    \ the environment:\n```\n**#create conda env: (I used conda to keep my environment\
    \ separate, but you don't have to)**\nconda create -n textgen python=3.10.9\n\
    conda activate textgen\n\npip install torch torchvision torchaudio\n\n**# (oobabooga/text-generation-webui):**\n\
    git clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n\
    pip install -r requirements.txt\n\n**# (qwopqwop200/GPTQ-for-LLaMa):**\nmkdir\
    \ repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa\
    \ GPTQ-for-LLaMa\ncd GPTQ-for-LLaMa\npip install -r requirements.txt\n\n**# start\
    \ server:**\npython server.py --model_type llama --wbits 4 --groupsize 128 \n\
    ```\n\nHere is the command I used to quantize:\n```\nCUDA_VISIBLE_DEVICES=0 python\
    \ llama.py /home/nap/Documents/text-generation-webui/models/koala-13B-HF c4 --wbits\
    \ 4 --true-sequential --act-order --groupsize 128 --save_safetensors /home/nap/Documents/text-generation-webui/models/koala13B-4bit-128g.safetensors\n\
    ```\n\n**EDIT:**\nI did notice we are using different versions of CUDA but I don't\
    \ think this should matter?:\n```\n(textgen) nap@wintermute:~/Documents/text-generation-webui$\
    \ ./start.sh \n\n===================================BUG REPORT===================================\n\
    Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    CUDA SETUP: CUDA runtime path found: /home/nap/miniconda3/envs/textgen/lib/libcudart.so\n\
    CUDA SETUP: Highest compute capability among GPUs detected: 8.9\nCUDA SETUP: Detected\
    \ CUDA version 117\nCUDA SETUP: Loading binary /home/nap/miniconda3/envs/textgen/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\
    ```"
  created_at: 2023-04-08 14:09:02+00:00
  edited: true
  hidden: false
  id: 6431838e1cbbdbac56a6190b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T16:29:49.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks for the details.  I am completely mystified.</p>

          <p>My problems aren''t even specific to GPTQ. Today I''ve been testing the
          web UI with the unquantized Koala 7B model. I tested both my repo, at <a
          href="https://huggingface.co/TheBloke/koala-7B-HF">https://huggingface.co/TheBloke/koala-7B-HF</a>,
          and one made independently by a YouTuber called Sam Witeveen at <a href="https://huggingface.co/samwit/koala-7b">https://huggingface.co/samwit/koala-7b</a></p>

          <p>Here''s the output I get using Sam''s repo loaded into the web UI. No
          GPTQ at all:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/jDs2Y-xPyod8p95qhaJyz.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/jDs2Y-xPyod8p95qhaJyz.png"></a></p>

          <p>Total garbage.  But the web UI does work in general, I tested an unrelated
          model (Galactica 6.7b) and that worked fine.</p>

          <p>I was wondering if maybe I had some issue related to still being on Python
          3.9, so just now I upgraded the Colab to 3.10.6 and the results are the
          same.</p>

          <p>I guess I will now try with CUDA 117 just in case that is causing any
          problems. Seems unlikely though.</p>

          <p>Otherwise I will definitely need to raise an issue with oogabooga, because
          I am so confused.</p>

          <p>I guess the good news is that it seems like the GPTQ files I uploaded
          in this repo definitely are fine? The user just needs to know how to use
          them correctly.</p>

          '
        raw: 'Thanks for the details.  I am completely mystified.


          My problems aren''t even specific to GPTQ. Today I''ve been testing the
          web UI with the unquantized Koala 7B model. I tested both my repo, at https://huggingface.co/TheBloke/koala-7B-HF,
          and one made independently by a YouTuber called Sam Witeveen at https://huggingface.co/samwit/koala-7b


          Here''s the output I get using Sam''s repo loaded into the web UI. No GPTQ
          at all:


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/jDs2Y-xPyod8p95qhaJyz.png)


          Total garbage.  But the web UI does work in general, I tested an unrelated
          model (Galactica 6.7b) and that worked fine.


          I was wondering if maybe I had some issue related to still being on Python
          3.9, so just now I upgraded the Colab to 3.10.6 and the results are the
          same.


          I guess I will now try with CUDA 117 just in case that is causing any problems.
          Seems unlikely though.


          Otherwise I will definitely need to raise an issue with oogabooga, because
          I am so confused.


          I guess the good news is that it seems like the GPTQ files I uploaded in
          this repo definitely are fine? The user just needs to know how to use them
          correctly.'
        updatedAt: '2023-04-08T16:31:13.308Z'
      numEdits: 1
      reactions: []
    id: 6431967d1cbbdbac56a68f96
    type: comment
  author: TheBloke
  content: 'Thanks for the details.  I am completely mystified.


    My problems aren''t even specific to GPTQ. Today I''ve been testing the web UI
    with the unquantized Koala 7B model. I tested both my repo, at https://huggingface.co/TheBloke/koala-7B-HF,
    and one made independently by a YouTuber called Sam Witeveen at https://huggingface.co/samwit/koala-7b


    Here''s the output I get using Sam''s repo loaded into the web UI. No GPTQ at
    all:


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/jDs2Y-xPyod8p95qhaJyz.png)


    Total garbage.  But the web UI does work in general, I tested an unrelated model
    (Galactica 6.7b) and that worked fine.


    I was wondering if maybe I had some issue related to still being on Python 3.9,
    so just now I upgraded the Colab to 3.10.6 and the results are the same.


    I guess I will now try with CUDA 117 just in case that is causing any problems.
    Seems unlikely though.


    Otherwise I will definitely need to raise an issue with oogabooga, because I am
    so confused.


    I guess the good news is that it seems like the GPTQ files I uploaded in this
    repo definitely are fine? The user just needs to know how to use them correctly.'
  created_at: 2023-04-08 15:29:49+00:00
  edited: true
  hidden: false
  id: 6431967d1cbbdbac56a68f96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632e2ab7183321090a5ff116/NFRJKDs_PVKwqGZblzDEk.jpeg?w=200&h=200&f=face
      fullname: Michael Martinez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monkmartinez
      type: user
    createdAt: '2023-04-08T16:49:23.000Z'
    data:
      edited: false
      editors:
      - monkmartinez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632e2ab7183321090a5ff116/NFRJKDs_PVKwqGZblzDEk.jpeg?w=200&h=200&f=face
          fullname: Michael Martinez
          isHf: false
          isPro: false
          name: monkmartinez
          type: user
        html: "<p>I haven't tested this particular version, but I did use your \"\
          regular\" 7B version. Thank you for conversion and uploading. I was unable\
          \ to complete the conversion on my machine for some reason.</p>\n<p>I say\
          \ that to say this: *I have found the koala model is VERY sensitive to prompt.\
          \ *</p>\n<pre><code>BEGINNING OF CONVERSATION: \nUSER: &lt;user_questions/input_goes_here&gt;\n\
          \nGPT:\n</code></pre>\n<p>As an idea, maybe have a go at trying with this\
          \ prompt?</p>\n"
        raw: "I haven't tested this particular version, but I did use your \"regular\"\
          \ 7B version. Thank you for conversion and uploading. I was unable to complete\
          \ the conversion on my machine for some reason.\n\nI say that to say this:\
          \ *I have found the koala model is VERY sensitive to prompt. *\n\n```\n\
          BEGINNING OF CONVERSATION: \nUSER: <user_questions/input_goes_here>\n\n\
          GPT:\n```\nAs an idea, maybe have a go at trying with this prompt?"
        updatedAt: '2023-04-08T16:49:23.852Z'
      numEdits: 0
      reactions: []
    id: 64319b13b62bdfa25c0bd0a8
    type: comment
  author: monkmartinez
  content: "I haven't tested this particular version, but I did use your \"regular\"\
    \ 7B version. Thank you for conversion and uploading. I was unable to complete\
    \ the conversion on my machine for some reason.\n\nI say that to say this: *I\
    \ have found the koala model is VERY sensitive to prompt. *\n\n```\nBEGINNING\
    \ OF CONVERSATION: \nUSER: <user_questions/input_goes_here>\n\nGPT:\n```\nAs an\
    \ idea, maybe have a go at trying with this prompt?"
  created_at: 2023-04-08 15:49:23+00:00
  edited: false
  hidden: false
  id: 64319b13b62bdfa25c0bd0a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T17:09:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>You're welcome! Glad it helped. I've put up <code>koala-13B-HF</code>\
          \ now as well if that's of any interest to you.</p>\n<p>Thanks for the info.\
          \  Yeah I saw that prompt method earlier today, in Sam's video, and tried\
          \ it a couple of times. I've just tried it again now. It definitely does\
          \ get better output in that it tends to produce valid English sentences.\
          \ But it very quickly goes into constant repetition or starts stringing\
          \ words together without spaces.</p>\n<p>Eg loading <code>koala-7B-HF</code>,\
          \ no quantisation<br>Prompt:</p>\n<pre><code>BEGINNING OF CONVERSATION:\
          \ \nUSER:  write a story about llamas\nGPT:\n</code></pre>\n<p>I get this\
          \ output:</p>\n<pre><code>BEGINNING OF CONVERSATION: \nUSER:  write a story\
          \ about llamas\nGPT:User'sstory is not written by the user.user'sstory is\
          \ not written by the user'suser'sstory is not written by the user'suser'sstory\
          \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
          \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
          \ is not written by the user'\n... and on and on\n</code></pre>\n<p>Or,\
          \ with slightly different parameters:</p>\n<pre><code>BEGINNING OF CONVERSATION:\
          \ \nUSER:  write a story about llamas\nGPT:User has written a story about\
          \ the sea, which is called \"Makes me right.\"\nUser has written a story\
          \ about the sea, which is called \"Makes me right\".\nUser has also written\
          \ a poem about the sea, which is called \"Makes me right\".\nUser has also\
          \ written a poem about the sea, which is called \"Makes me right\".\nUser\
          \ has also written a poem about the sea, which is called \"Little Makes\
          \ me Right\", which is called \"Makes me right\".\n... more garbage\n</code></pre>\n\
          <p>I've tried various different parameters, and some produce less garbage\
          \ or different garbage, but it's rare to even get valid English sentences,\
          \ let alone anything remotely useful.</p>\n<p>Whereas if I try other inference\
          \ methods, like using <code>llama.cpp</code> on the unquantised GGML file\
          \ (which was converted from the koala-7B-HF model data), or use the <code>llama_inference.py</code>\
          \ provided with GPTQ to infer on the GPTQ versions, it seems I can use any\
          \ prompt at all and get something readable, even if the prompt wasn't ideal.</p>\n\
          <p>Example with llama.cpp:<br>Command:</p>\n<pre><code>./main -t 18 -m ~/src/ggml.koala.7b.bin\
          \ --color  -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1  -p \"BEGINNING\
          \ OF CONVERSATION:\nUSER: Write a story about llamas.\n\nGPT:\"\n</code></pre>\n\
          <p>Output:</p>\n<pre><code> BEGINNING OF CONVERSATION:\nUSER: Write a story\
          \ about llamas.\n\nGPT: Once upon a time, in the midst of a vast and verdant\
          \ forest, there lived a herd of magnificent llamas. These gentle creatures\
          \ roamed the lush green grass, their thick woolly coats keeping them warm\
          \ in the cool mountain air. They were known for their kindness and intelligence,\
          \ always willing to share their grazing with other animals in need.\n\n\
          One day, a young llama named Luna wandered far from her herd, exploring\
          \ the forest on her own. As she made her way through the trees, she came\
          \ across a beautiful meadow filled with blooming flowers. Luna was struck\
          \ by the beauty of the scene and decided to stay for a while and take in\
          \ all the sights .. etc\n</code></pre>\n<p>The fact that I can't seem to\
          \ ever get even one good output from Koala under web UI, regardless of prompt\
          \ or parameters, makes me think something must be broken somewhere. Maybe\
          \ something in my Colab environment that doesn't play nice with webUI?</p>\n\
          <p>It's so weird!</p>\n"
        raw: "You're welcome! Glad it helped. I've put up `koala-13B-HF` now as well\
          \ if that's of any interest to you.\n\nThanks for the info.  Yeah I saw\
          \ that prompt method earlier today, in Sam's video, and tried it a couple\
          \ of times. I've just tried it again now. It definitely does get better\
          \ output in that it tends to produce valid English sentences. But it very\
          \ quickly goes into constant repetition or starts stringing words together\
          \ without spaces.\n\nEg loading `koala-7B-HF`, no quantisation\nPrompt:\n\
          ```\nBEGINNING OF CONVERSATION: \nUSER:  write a story about llamas\nGPT:\n\
          ```\nI get this output:\n```\nBEGINNING OF CONVERSATION: \nUSER:  write\
          \ a story about llamas\nGPT:User'sstory is not written by the user.user'sstory\
          \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
          \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
          \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
          \ is not written by the user'\n... and on and on\n```\nOr, with slightly\
          \ different parameters:\n```\nBEGINNING OF CONVERSATION: \nUSER:  write\
          \ a story about llamas\nGPT:User has written a story about the sea, which\
          \ is called \"Makes me right.\"\nUser has written a story about the sea,\
          \ which is called \"Makes me right\".\nUser has also written a poem about\
          \ the sea, which is called \"Makes me right\".\nUser has also written a\
          \ poem about the sea, which is called \"Makes me right\".\nUser has also\
          \ written a poem about the sea, which is called \"Little Makes me Right\"\
          , which is called \"Makes me right\".\n... more garbage\n```\nI've tried\
          \ various different parameters, and some produce less garbage or different\
          \ garbage, but it's rare to even get valid English sentences, let alone\
          \ anything remotely useful.\n\nWhereas if I try other inference methods,\
          \ like using `llama.cpp` on the unquantised GGML file (which was converted\
          \ from the koala-7B-HF model data), or use the `llama_inference.py` provided\
          \ with GPTQ to infer on the GPTQ versions, it seems I can use any prompt\
          \ at all and get something readable, even if the prompt wasn't ideal.\n\n\
          Example with llama.cpp:\nCommand:\n```\n./main -t 18 -m ~/src/ggml.koala.7b.bin\
          \ --color  -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1  -p \"BEGINNING\
          \ OF CONVERSATION:\nUSER: Write a story about llamas.\n\nGPT:\"\n```\nOutput:\n\
          ```\n BEGINNING OF CONVERSATION:\nUSER: Write a story about llamas.\n\n\
          GPT: Once upon a time, in the midst of a vast and verdant forest, there\
          \ lived a herd of magnificent llamas. These gentle creatures roamed the\
          \ lush green grass, their thick woolly coats keeping them warm in the cool\
          \ mountain air. They were known for their kindness and intelligence, always\
          \ willing to share their grazing with other animals in need.\n\nOne day,\
          \ a young llama named Luna wandered far from her herd, exploring the forest\
          \ on her own. As she made her way through the trees, she came across a beautiful\
          \ meadow filled with blooming flowers. Luna was struck by the beauty of\
          \ the scene and decided to stay for a while and take in all the sights ..\
          \ etc\n```\nThe fact that I can't seem to ever get even one good output\
          \ from Koala under web UI, regardless of prompt or parameters, makes me\
          \ think something must be broken somewhere. Maybe something in my Colab\
          \ environment that doesn't play nice with webUI?\n\nIt's so weird!"
        updatedAt: '2023-04-08T17:09:49.642Z'
      numEdits: 0
      reactions: []
    id: 64319fddac73fd88eb36edff
    type: comment
  author: TheBloke
  content: "You're welcome! Glad it helped. I've put up `koala-13B-HF` now as well\
    \ if that's of any interest to you.\n\nThanks for the info.  Yeah I saw that prompt\
    \ method earlier today, in Sam's video, and tried it a couple of times. I've just\
    \ tried it again now. It definitely does get better output in that it tends to\
    \ produce valid English sentences. But it very quickly goes into constant repetition\
    \ or starts stringing words together without spaces.\n\nEg loading `koala-7B-HF`,\
    \ no quantisation\nPrompt:\n```\nBEGINNING OF CONVERSATION: \nUSER:  write a story\
    \ about llamas\nGPT:\n```\nI get this output:\n```\nBEGINNING OF CONVERSATION:\
    \ \nUSER:  write a story about llamas\nGPT:User'sstory is not written by the user.user'sstory\
    \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
    \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
    \ is not written by the user'suser'sstory is not written by the user'suser'sstory\
    \ is not written by the user'\n... and on and on\n```\nOr, with slightly different\
    \ parameters:\n```\nBEGINNING OF CONVERSATION: \nUSER:  write a story about llamas\n\
    GPT:User has written a story about the sea, which is called \"Makes me right.\"\
    \nUser has written a story about the sea, which is called \"Makes me right\".\n\
    User has also written a poem about the sea, which is called \"Makes me right\"\
    .\nUser has also written a poem about the sea, which is called \"Makes me right\"\
    .\nUser has also written a poem about the sea, which is called \"Little Makes\
    \ me Right\", which is called \"Makes me right\".\n... more garbage\n```\nI've\
    \ tried various different parameters, and some produce less garbage or different\
    \ garbage, but it's rare to even get valid English sentences, let alone anything\
    \ remotely useful.\n\nWhereas if I try other inference methods, like using `llama.cpp`\
    \ on the unquantised GGML file (which was converted from the koala-7B-HF model\
    \ data), or use the `llama_inference.py` provided with GPTQ to infer on the GPTQ\
    \ versions, it seems I can use any prompt at all and get something readable, even\
    \ if the prompt wasn't ideal.\n\nExample with llama.cpp:\nCommand:\n```\n./main\
    \ -t 18 -m ~/src/ggml.koala.7b.bin --color  -c 2048 --temp 0.7 --repeat_penalty\
    \ 1.1 -n -1  -p \"BEGINNING OF CONVERSATION:\nUSER: Write a story about llamas.\n\
    \nGPT:\"\n```\nOutput:\n```\n BEGINNING OF CONVERSATION:\nUSER: Write a story\
    \ about llamas.\n\nGPT: Once upon a time, in the midst of a vast and verdant forest,\
    \ there lived a herd of magnificent llamas. These gentle creatures roamed the\
    \ lush green grass, their thick woolly coats keeping them warm in the cool mountain\
    \ air. They were known for their kindness and intelligence, always willing to\
    \ share their grazing with other animals in need.\n\nOne day, a young llama named\
    \ Luna wandered far from her herd, exploring the forest on her own. As she made\
    \ her way through the trees, she came across a beautiful meadow filled with blooming\
    \ flowers. Luna was struck by the beauty of the scene and decided to stay for\
    \ a while and take in all the sights .. etc\n```\nThe fact that I can't seem to\
    \ ever get even one good output from Koala under web UI, regardless of prompt\
    \ or parameters, makes me think something must be broken somewhere. Maybe something\
    \ in my Colab environment that doesn't play nice with webUI?\n\nIt's so weird!"
  created_at: 2023-04-08 16:09:49+00:00
  edited: false
  hidden: false
  id: 64319fddac73fd88eb36edff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632e2ab7183321090a5ff116/NFRJKDs_PVKwqGZblzDEk.jpeg?w=200&h=200&f=face
      fullname: Michael Martinez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: monkmartinez
      type: user
    createdAt: '2023-04-08T20:06:45.000Z'
    data:
      edited: false
      editors:
      - monkmartinez
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632e2ab7183321090a5ff116/NFRJKDs_PVKwqGZblzDEk.jpeg?w=200&h=200&f=face
          fullname: Michael Martinez
          isHf: false
          isPro: false
          name: monkmartinez
          type: user
        html: '<p>That is super strange behavior! I haven''t been able to even load
          this 7B - 4bit model in webUI... still investigating why. But your other
          models have been running flawlessly if a bit constrained by the "as an AI
          model..." stuff.</p>

          <p>For additional data points... I am running this on a Windows Computer
          with the One-click installer. I can run almost everything with enough effort
          into how its loaded with flags and whatnot.</p>

          '
        raw: 'That is super strange behavior! I haven''t been able to even load this
          7B - 4bit model in webUI... still investigating why. But your other models
          have been running flawlessly if a bit constrained by the "as an AI model..."
          stuff.


          For additional data points... I am running this on a Windows Computer with
          the One-click installer. I can run almost everything with enough effort
          into how its loaded with flags and whatnot.'
        updatedAt: '2023-04-08T20:06:45.407Z'
      numEdits: 0
      reactions: []
    id: 6431c955387088721b33ef4c
    type: comment
  author: monkmartinez
  content: 'That is super strange behavior! I haven''t been able to even load this
    7B - 4bit model in webUI... still investigating why. But your other models have
    been running flawlessly if a bit constrained by the "as an AI model..." stuff.


    For additional data points... I am running this on a Windows Computer with the
    One-click installer. I can run almost everything with enough effort into how its
    loaded with flags and whatnot.'
  created_at: 2023-04-08 19:06:45+00:00
  edited: false
  hidden: false
  id: 6431c955387088721b33ef4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-08T22:21:03.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>All right I made it work consistently on Windows (without WSL) with
          textgen-webui using opoobabooga''s new-qwop branch and qwopqwop200''s GPTQ-for-LLaMa
          (it will complain about triton at GPTQ-for-LLaMa''s kernel compilation but
          it''s fine):</p>

          <h3 id="install">Install:</h3>

          <p>conda install python=3.10.9 pytorch[version=2,build=py3.10_cuda11.7*]
          torchvision torchaudio pytorch-cuda=11.7 cuda-toolkit ninja git -c pytorch
          -c nvidia/label/cuda-11.7.0 -c nvidia -c conda-forge<br>git clone <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui.git">https://github.com/oobabooga/text-generation-webui.git</a>
          -b new-qwop<br>pip install <a rel="nofollow" href="https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.37.2-py3-none-any.whl">https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.37.2-py3-none-any.whl</a><br>cd
          text-generation-webui<br>python -m pip install -r requirements.txt --upgrade<br>mkdir
          repositories<br>cd repositories<br>git clone <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa.git">https://github.com/qwopqwop200/GPTQ-for-LLaMa.git</a>
          -b cuda<br>cd GPTQ-for-LLaMa<br>call python -m pip install -r requirements.txt<br>python
          setup_cuda.py install</p>

          <p>Output goes around 2 tokens/s on a 1060 6gb. </p>

          <h3 id="heres-a-test">Here''s a test:</h3>

          <p>Below is an instruction that describes a task. Write a response that
          appropriately completes the request.<br>Instruction:<br>Write a poem about
          the transformers Python library.<br>Mention the word "large language models"
          in that poem.<br>Response:</p>

          <h3 id="koala-7b-q4-thebloke_koala-7b-4bit-128g-naive-preset">KOALA 7B Q4
          (TheBloke_koala-7b-4bit-128g), naive preset</h3>

          <p>A poem about the transformers Python library:</p>

          <p>The transformers library, oh how grand,<br>It''s a tool for large language
          models.<br>It''s a tool for NLP tasks galore,<br>It''s a tool that''s here
          to make you grow.</p>

          <p>With the transformers library, you can train,<br>And fine-tune large
          language models,<br>And make them perform tasks with ease,<br>And make them
          do what you want them to please.</p>

          <p>So don''t wait any longer,<br>And give the transformers library a try,<br>It''s
          a tool that''s here to help,<br>And make your NLP tasks more efficient</p>

          <h3 id="alpaca-7b-q4-ozcur_alpaca-native-4bit-naive-preset-third-inference">ALPACA
          7B Q4 (ozcur_alpaca-native-4bit), naive preset, third inference:</h3>

          <p>The transformers Python library is here to stay<br>It comes with many
          powerful tools each day<br>With large language models, it''s a breeze<br>To
          process data and create your views</p>

          <p>This library is quite impressive<br>It''s got the power to make you bliss<br>With
          its capabilities, it''s a joy<br>To use and will always be a toy</p>

          <p>It''s got many features that are great<br>It can even make your coding
          great<br>It''s a powerful tool, don''t you see<br>It''s sure to make you
          feel like me</p>

          <p>So use this library and be sure<br>It''ll make your coding dreams come
          true<br>With its capabilities, you''ll be glad<br>You chose to use the transformers
          Python library.</p>

          <h3 id="guanaco-7b-q4-josephuscheung_guanacoonconsumerhardware-naive-preset-second-inference">GUANACO
          7B Q4 (JosephusCheung_GuanacoOnConsumerHardware), naive preset, second inference:</h3>

          <p>The transformers Python library, it is quite the beast<br>A large language
          models it is based on the text</p>

          <p>Its power is unparalleled, and its size can''t be beat<br>A model for
          the ages, a feat that can''t be repeat</p>

          <p>It has a wide range of use, from natural language to text<br>It can do
          wonders, it''s a model of great worth</p>

          <p>It''s a model for the ages, and it can''t be beaten<br>A model of great
          worth, and it''s here to stay</p>

          '
        raw: "All right I made it work consistently on Windows (without WSL) with\
          \ textgen-webui using opoobabooga's new-qwop branch and qwopqwop200's GPTQ-for-LLaMa\
          \ (it will complain about triton at GPTQ-for-LLaMa's kernel compilation\
          \ but it's fine):\n\n### Install:\nconda install python=3.10.9 pytorch[version=2,build=py3.10_cuda11.7*]\
          \ torchvision torchaudio pytorch-cuda=11.7 cuda-toolkit ninja git -c pytorch\
          \ -c nvidia/label/cuda-11.7.0 -c nvidia -c conda-forge\ngit clone https://github.com/oobabooga/text-generation-webui.git\
          \ -b new-qwop\npip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.37.2-py3-none-any.whl\n\
          cd text-generation-webui\npython -m pip install -r requirements.txt --upgrade\n\
          mkdir repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\
          \ -b cuda\ncd GPTQ-for-LLaMa\ncall python -m pip install -r requirements.txt\
          \ \npython setup_cuda.py install\n\nOutput goes around 2 tokens/s on a 1060\
          \ 6gb. \n### Here's a test:\nBelow is an instruction that describes a task.\
          \ Write a response that appropriately completes the request.\nInstruction:\n\
          Write a poem about the transformers Python library. \nMention the word \"\
          large language models\" in that poem.\nResponse:\n\n### KOALA 7B Q4 (TheBloke_koala-7b-4bit-128g),\
          \ naive preset\nA poem about the transformers Python library:\n\nThe transformers\
          \ library, oh how grand,\nIt's a tool for large language models.\nIt's a\
          \ tool for NLP tasks galore,\nIt's a tool that's here to make you grow.\n\
          \nWith the transformers library, you can train,\nAnd fine-tune large language\
          \ models,\nAnd make them perform tasks with ease,\nAnd make them do what\
          \ you want them to please.\n\nSo don't wait any longer,\nAnd give the transformers\
          \ library a try,\nIt's a tool that's here to help,\nAnd make your NLP tasks\
          \ more efficient\n\n\n### ALPACA 7B Q4 (ozcur_alpaca-native-4bit), naive\
          \ preset, third inference:\nThe transformers Python library is here to stay\n\
          It comes with many powerful tools each day\nWith large language models,\
          \ it's a breeze\nTo process data and create your views\n\nThis library is\
          \ quite impressive\nIt's got the power to make you bliss\nWith its capabilities,\
          \ it's a joy\nTo use and will always be a toy\n\nIt's got many features\
          \ that are great\nIt can even make your coding great\nIt's a powerful tool,\
          \ don't you see\nIt's sure to make you feel like me\n\nSo use this library\
          \ and be sure\nIt'll make your coding dreams come true\nWith its capabilities,\
          \ you'll be glad\nYou chose to use the transformers Python library.\n\n\n\
          ### GUANACO 7B Q4 (JosephusCheung_GuanacoOnConsumerHardware), naive preset,\
          \ second inference:\nThe transformers Python library, it is quite the beast\n\
          A large language models it is based on the text\n\nIts power is unparalleled,\
          \ and its size can't be beat\nA model for the ages, a feat that can't be\
          \ repeat\n\nIt has a wide range of use, from natural language to text\n\
          It can do wonders, it's a model of great worth\n\nIt's a model for the ages,\
          \ and it can't be beaten\nA model of great worth, and it's here to stay"
        updatedAt: '2023-04-08T22:23:53.448Z'
      numEdits: 3
      reactions: []
    id: 6431e8cf7d795d8e7033a75e
    type: comment
  author: cmh
  content: "All right I made it work consistently on Windows (without WSL) with textgen-webui\
    \ using opoobabooga's new-qwop branch and qwopqwop200's GPTQ-for-LLaMa (it will\
    \ complain about triton at GPTQ-for-LLaMa's kernel compilation but it's fine):\n\
    \n### Install:\nconda install python=3.10.9 pytorch[version=2,build=py3.10_cuda11.7*]\
    \ torchvision torchaudio pytorch-cuda=11.7 cuda-toolkit ninja git -c pytorch -c\
    \ nvidia/label/cuda-11.7.0 -c nvidia -c conda-forge\ngit clone https://github.com/oobabooga/text-generation-webui.git\
    \ -b new-qwop\npip install https://github.com/jllllll/bitsandbytes-windows-webui/raw/main/bitsandbytes-0.37.2-py3-none-any.whl\n\
    cd text-generation-webui\npython -m pip install -r requirements.txt --upgrade\n\
    mkdir repositories\ncd repositories\ngit clone https://github.com/qwopqwop200/GPTQ-for-LLaMa.git\
    \ -b cuda\ncd GPTQ-for-LLaMa\ncall python -m pip install -r requirements.txt \n\
    python setup_cuda.py install\n\nOutput goes around 2 tokens/s on a 1060 6gb. \n\
    ### Here's a test:\nBelow is an instruction that describes a task. Write a response\
    \ that appropriately completes the request.\nInstruction:\nWrite a poem about\
    \ the transformers Python library. \nMention the word \"large language models\"\
    \ in that poem.\nResponse:\n\n### KOALA 7B Q4 (TheBloke_koala-7b-4bit-128g), naive\
    \ preset\nA poem about the transformers Python library:\n\nThe transformers library,\
    \ oh how grand,\nIt's a tool for large language models.\nIt's a tool for NLP tasks\
    \ galore,\nIt's a tool that's here to make you grow.\n\nWith the transformers\
    \ library, you can train,\nAnd fine-tune large language models,\nAnd make them\
    \ perform tasks with ease,\nAnd make them do what you want them to please.\n\n\
    So don't wait any longer,\nAnd give the transformers library a try,\nIt's a tool\
    \ that's here to help,\nAnd make your NLP tasks more efficient\n\n\n### ALPACA\
    \ 7B Q4 (ozcur_alpaca-native-4bit), naive preset, third inference:\nThe transformers\
    \ Python library is here to stay\nIt comes with many powerful tools each day\n\
    With large language models, it's a breeze\nTo process data and create your views\n\
    \nThis library is quite impressive\nIt's got the power to make you bliss\nWith\
    \ its capabilities, it's a joy\nTo use and will always be a toy\n\nIt's got many\
    \ features that are great\nIt can even make your coding great\nIt's a powerful\
    \ tool, don't you see\nIt's sure to make you feel like me\n\nSo use this library\
    \ and be sure\nIt'll make your coding dreams come true\nWith its capabilities,\
    \ you'll be glad\nYou chose to use the transformers Python library.\n\n\n### GUANACO\
    \ 7B Q4 (JosephusCheung_GuanacoOnConsumerHardware), naive preset, second inference:\n\
    The transformers Python library, it is quite the beast\nA large language models\
    \ it is based on the text\n\nIts power is unparalleled, and its size can't be\
    \ beat\nA model for the ages, a feat that can't be repeat\n\nIt has a wide range\
    \ of use, from natural language to text\nIt can do wonders, it's a model of great\
    \ worth\n\nIt's a model for the ages, and it can't be beaten\nA model of great\
    \ worth, and it's here to stay"
  created_at: 2023-04-08 21:21:03+00:00
  edited: true
  hidden: false
  id: 6431e8cf7d795d8e7033a75e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T22:21:26.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Update: I just finished GPTQing the 13B model. And it works perfectly
          in textweb UI!</p>

          <p>Here''s the model: <a href="https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g">https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g</a></p>

          <p>This makes me even more confused as to my total failure to get the 7B
          working. But given I can get 13B working - which is the model I wanted anyway
          - I guess that solves my problem.</p>

          <p>I have updated the model card for this 7B model, and the 13B model, with
          instructions on how to get these GPTQ models working with textweb UI.</p>

          '
        raw: 'Update: I just finished GPTQing the 13B model. And it works perfectly
          in textweb UI!


          Here''s the model: https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g


          This makes me even more confused as to my total failure to get the 7B working.
          But given I can get 13B working - which is the model I wanted anyway - I
          guess that solves my problem.


          I have updated the model card for this 7B model, and the 13B model, with
          instructions on how to get these GPTQ models working with textweb UI.'
        updatedAt: '2023-04-08T22:21:26.933Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cmh
    id: 6431e8e61cbbdbac56a86a00
    type: comment
  author: TheBloke
  content: 'Update: I just finished GPTQing the 13B model. And it works perfectly
    in textweb UI!


    Here''s the model: https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g


    This makes me even more confused as to my total failure to get the 7B working.
    But given I can get 13B working - which is the model I wanted anyway - I guess
    that solves my problem.


    I have updated the model card for this 7B model, and the 13B model, with instructions
    on how to get these GPTQ models working with textweb UI.'
  created_at: 2023-04-08 21:21:26+00:00
  edited: false
  hidden: false
  id: 6431e8e61cbbdbac56a86a00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T22:22:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Great, glad it's working for you <span data-props=\"{&quot;user&quot;:&quot;cmh&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/cmh\"\
          >@<span class=\"underline\">cmh</span></a></span>\n\n\t</span></span> !\
          \  thanks for the details</p>\n<p>Check out the 13B model as well if you\
          \ have enough VRAM. 13B requires 8.6GB VRAM it seems, so as long as the\
          \ GPU has &gt;8GB it should be fine.</p>\n"
        raw: 'Great, glad it''s working for you @cmh !  thanks for the details


          Check out the 13B model as well if you have enough VRAM. 13B requires 8.6GB
          VRAM it seems, so as long as the GPU has >8GB it should be fine.'
        updatedAt: '2023-04-08T22:22:51.657Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - cmh
    id: 6431e93b1cbbdbac56a86bfb
    type: comment
  author: TheBloke
  content: 'Great, glad it''s working for you @cmh !  thanks for the details


    Check out the 13B model as well if you have enough VRAM. 13B requires 8.6GB VRAM
    it seems, so as long as the GPU has >8GB it should be fine.'
  created_at: 2023-04-08 21:22:51+00:00
  edited: false
  hidden: false
  id: 6431e93b1cbbdbac56a86bfb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-08T22:26:50.000Z'
    data:
      edited: false
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>It should (slowly) work with --gpu-memory 4832MiB --no-cache --pre_layer
          24 on my 1060 6gb.<br>I''ll test real quick.</p>

          '
        raw: 'It should (slowly) work with --gpu-memory 4832MiB --no-cache --pre_layer
          24 on my 1060 6gb.

          I''ll test real quick.'
        updatedAt: '2023-04-08T22:26:50.781Z'
      numEdits: 0
      reactions: []
    id: 6431ea2a46d9d4f9d89affa8
    type: comment
  author: cmh
  content: 'It should (slowly) work with --gpu-memory 4832MiB --no-cache --pre_layer
    24 on my 1060 6gb.

    I''ll test real quick.'
  created_at: 2023-04-08 21:26:50+00:00
  edited: false
  hidden: false
  id: 6431ea2a46d9d4f9d89affa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-08T22:28:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah interesting, I didn''t realise you could still load it without
          enough VRAM! Yeah let me know if it works!</p>

          <p>On the 13B repo the pt file is already uploaded but the safetensors is
          still pushing. Should be finished in a few minutes.</p>

          '
        raw: 'Ah interesting, I didn''t realise you could still load it without enough
          VRAM! Yeah let me know if it works!


          On the 13B repo the pt file is already uploaded but the safetensors is still
          pushing. Should be finished in a few minutes.'
        updatedAt: '2023-04-08T22:28:16.351Z'
      numEdits: 0
      reactions: []
    id: 6431ea8017b2e5055bb3bfc2
    type: comment
  author: TheBloke
  content: 'Ah interesting, I didn''t realise you could still load it without enough
    VRAM! Yeah let me know if it works!


    On the 13B repo the pt file is already uploaded but the safetensors is still pushing.
    Should be finished in a few minutes.'
  created_at: 2023-04-08 21:28:16+00:00
  edited: false
  hidden: false
  id: 6431ea8017b2e5055bb3bfc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
      fullname: cmh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cmh
      type: user
    createdAt: '2023-04-08T22:48:23.000Z'
    data:
      edited: true
      editors:
      - cmh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/AIJwL4LEsMely6FvzdSSV.png?w=200&h=200&f=face
          fullname: cmh
          isHf: false
          isPro: false
          name: cmh
          type: user
        html: '<p>Yeah, it splits the model between CPU and GPU and it''s slow.<br>On
          my system 13B q4 models are on par with llama.cpp since my 3rd gen i5 can''t
          even provide AVX2 instructions. Like 0.15 tokens/s instead of 2 tokens/s
          for 7B q4 models. It took less time to download the model than to run the
          inference, lol.<br>Anyway thanks a lot for providing the models, great work.<br>Here''s
          the results, still with the naive parameter:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/1i1DQ3-Z_R_2ZECU0X6sj.png"><img
          alt="Sans titre.png" src="https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/1i1DQ3-Z_R_2ZECU0X6sj.png"></a></p>

          '
        raw: "Yeah, it splits the model between CPU and GPU and it's slow. \nOn my\
          \ system 13B q4 models are on par with llama.cpp since my 3rd gen i5 can't\
          \ even provide AVX2 instructions. Like 0.15 tokens/s instead of 2 tokens/s\
          \ for 7B q4 models. It took less time to download the model than to run\
          \ the inference, lol.\nAnyway thanks a lot for providing the models, great\
          \ work. \nHere's the results, still with the naive parameter:\n![Sans titre.png](https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/1i1DQ3-Z_R_2ZECU0X6sj.png)"
        updatedAt: '2023-04-08T22:54:09.154Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 6431ef37f2355217ea47a09f
    type: comment
  author: cmh
  content: "Yeah, it splits the model between CPU and GPU and it's slow. \nOn my system\
    \ 13B q4 models are on par with llama.cpp since my 3rd gen i5 can't even provide\
    \ AVX2 instructions. Like 0.15 tokens/s instead of 2 tokens/s for 7B q4 models.\
    \ It took less time to download the model than to run the inference, lol.\nAnyway\
    \ thanks a lot for providing the models, great work. \nHere's the results, still\
    \ with the naive parameter:\n![Sans titre.png](https://cdn-uploads.huggingface.co/production/uploads/631f1e015ba8c02634093d59/1i1DQ3-Z_R_2ZECU0X6sj.png)"
  created_at: 2023-04-08 21:48:23+00:00
  edited: true
  hidden: false
  id: 6431ef37f2355217ea47a09f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-09T15:18:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;cmh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cmh\">@<span class=\"\
          underline\">cmh</span></a></span>\n\n\t</span></span> I don't know if this\
          \ will help you with your limited CPU, but I've now got GPTQ 4bit GGML files\
          \ available for both 7B and 13B models, if you'd like to try them entirely\
          \ on CPU with llama.cpp:<br><a href=\"https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g-GGML\"\
          >https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g-GGML</a><br><a\
          \ href=\"https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g-GGML\"\
          >https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g-GGML</a></p>\n\
          <p>If you try them I'd recommend doing a <code>git pull</code> in llama.cpp\
          \ and building from source again, as they keep pushing new performance enhancements.\
          \ </p>\n<p>Worth a try anyway!</p>\n<p>I'm going to close this thread now\
          \ as I think all issues are resolved. Well, I never did actually manage\
          \ to get any 7B model working in textgen UI, which still completely baffles\
          \ me. But you guys all reported it working, and 13B models work fine for\
          \ me, so I guess that will just have to remain a mystery!</p>\n<p>Thanks\
          \ everyone for your help and advice.</p>\n"
        raw: "Hey @cmh I don't know if this will help you with your limited CPU, but\
          \ I've now got GPTQ 4bit GGML files available for both 7B and 13B models,\
          \ if you'd like to try them entirely on CPU with llama.cpp:\nhttps://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g-GGML\n\
          https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g-GGML\n\nIf you try\
          \ them I'd recommend doing a `git pull` in llama.cpp and building from source\
          \ again, as they keep pushing new performance enhancements. \n\nWorth a\
          \ try anyway!\n\nI'm going to close this thread now as I think all issues\
          \ are resolved. Well, I never did actually manage to get any 7B model working\
          \ in textgen UI, which still completely baffles me. But you guys all reported\
          \ it working, and 13B models work fine for me, so I guess that will just\
          \ have to remain a mystery!\n\nThanks everyone for your help and advice."
        updatedAt: '2023-04-09T15:18:25.375Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F91D"
        users:
        - cmh
        - disarmyouwitha
        - knoopx
      relatedEventId: 6432d7410cdd0c3686f1263a
    id: 6432d7410cdd0c3686f12639
    type: comment
  author: TheBloke
  content: "Hey @cmh I don't know if this will help you with your limited CPU, but\
    \ I've now got GPTQ 4bit GGML files available for both 7B and 13B models, if you'd\
    \ like to try them entirely on CPU with llama.cpp:\nhttps://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g-GGML\n\
    https://huggingface.co/TheBloke/koala-7B-GPTQ-4bit-128g-GGML\n\nIf you try them\
    \ I'd recommend doing a `git pull` in llama.cpp and building from source again,\
    \ as they keep pushing new performance enhancements. \n\nWorth a try anyway!\n\
    \nI'm going to close this thread now as I think all issues are resolved. Well,\
    \ I never did actually manage to get any 7B model working in textgen UI, which\
    \ still completely baffles me. But you guys all reported it working, and 13B models\
    \ work fine for me, so I guess that will just have to remain a mystery!\n\nThanks\
    \ everyone for your help and advice."
  created_at: 2023-04-09 14:18:25+00:00
  edited: false
  hidden: false
  id: 6432d7410cdd0c3686f12639
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-09T15:18:25.000Z'
    data:
      status: closed
    id: 6432d7410cdd0c3686f1263a
    type: status-change
  author: TheBloke
  created_at: 2023-04-09 14:18:25+00:00
  id: 6432d7410cdd0c3686f1263a
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/koala-7B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Won't work with GPTQ
