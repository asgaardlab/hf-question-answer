!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Andyrasika
conflicting_files: null
created_at: 2023-07-27 05:35:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
      fullname: Ankush Singal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andyrasika
      type: user
    createdAt: '2023-07-27T06:35:49.000Z'
    data:
      edited: true
      editors:
      - Andyrasika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.562260091304779
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
          fullname: Ankush Singal
          isHf: false
          isPro: false
          name: Andyrasika
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ ,<br>     While implementing your model (For reference, the colab notebook:\
          \ <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1DbMcJtY8m7-9MFrH6t6tgs10OTgbuOtG?usp=sharing\"\
          >https://colab.research.google.com/drive/1DbMcJtY8m7-9MFrH6t6tgs10OTgbuOtG?usp=sharing</a>)<br>i\
          \ got the following error:</p>\n<pre><code>WARNING:auto_gptq.modeling._base:use_triton\
          \ will force moving the whole model to GPU, make sure you have enough VRAM.\n\
          ---------------------------------------------------------------------------\n\
          FileNotFoundError                         Traceback (most recent call last)\n\
          &lt;ipython-input-13-d026160b78fa&gt; in &lt;cell line: 11&gt;()\n     \
          \ 9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \     10 \n---&gt; 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
          \n1 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in from_quantized(cls, save_dir, device, use_safetensors, use_triton,\
          \ max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \    512 \n    513         if not isfile(model_save_name):\n--&gt; 514 \
          \           raise FileNotFoundError(f\"Could not find model at {model_save_name}\"\
          )\n    515 \n    516         def skip(*args, **kwargs):\n\nFileNotFoundError:\
          \ Could not find model at TheBloke/koala-7B-GPTQ/koala-7b-GPTQ-4bit-128g.no-act.order.safetensors\n\
          </code></pre>\n<p>if i change:</p>\n<pre><code> quantize_config=None)\n\
          </code></pre>\n<p>i get the following error:</p>\n<pre><code>WARNING:auto_gptq.modeling._base:use_triton\
          \ will force moving the whole model to GPU, make sure you have enough VRAM.\n\
          ---------------------------------------------------------------------------\n\
          FileNotFoundError                         Traceback (most recent call last)\n\
          &lt;ipython-input-16-ede7706bade9&gt; in &lt;cell line: 11&gt;()\n     \
          \ 9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \     10 \n---&gt; 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
          \n2 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in from_pretrained(cls, save_dir)\n     49     @classmethod\n     50 \
          \    def from_pretrained(cls, save_dir: str):\n---&gt; 51         with open(join(save_dir,\
          \ \"quantize_config.json\"), \"r\", encoding=\"utf-8\") as f:\n     52 \
          \            return cls(**json.load(f))\n     53 \n\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'TheBloke/koala-7B-GPTQ/quantize_config.json'\n\
          </code></pre>\n<p>Looking forward to hearing from you.<br>Thanks,<br>Andy\
          \ </p>\n"
        raw: "Hi @TheBloke ,\n     While implementing your model (For reference, the\
          \ colab notebook: https://colab.research.google.com/drive/1DbMcJtY8m7-9MFrH6t6tgs10OTgbuOtG?usp=sharing)\n\
          i got the following error:\n```\nWARNING:auto_gptq.modeling._base:use_triton\
          \ will force moving the whole model to GPU, make sure you have enough VRAM.\n\
          ---------------------------------------------------------------------------\n\
          FileNotFoundError                         Traceback (most recent call last)\n\
          <ipython-input-13-d026160b78fa> in <cell line: 11>()\n      9 tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n \
          \    10 \n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
          \n1 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in from_quantized(cls, save_dir, device, use_safetensors, use_triton,\
          \ max_memory, device_map, quantize_config, model_basename, trust_remote_code)\n\
          \    512 \n    513         if not isfile(model_save_name):\n--> 514    \
          \        raise FileNotFoundError(f\"Could not find model at {model_save_name}\"\
          )\n    515 \n    516         def skip(*args, **kwargs):\n\nFileNotFoundError:\
          \ Could not find model at TheBloke/koala-7B-GPTQ/koala-7b-GPTQ-4bit-128g.no-act.order.safetensors\n\
          ```\n\nif i change:\n```\n quantize_config=None)\n```\ni get the following\
          \ error:\n```\nWARNING:auto_gptq.modeling._base:use_triton will force moving\
          \ the whole model to GPU, make sure you have enough VRAM.\n---------------------------------------------------------------------------\n\
          FileNotFoundError                         Traceback (most recent call last)\n\
          <ipython-input-16-ede7706bade9> in <cell line: 11>()\n      9 tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n \
          \    10 \n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
          \n2 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
          \ in from_pretrained(cls, save_dir)\n     49     @classmethod\n     50 \
          \    def from_pretrained(cls, save_dir: str):\n---> 51         with open(join(save_dir,\
          \ \"quantize_config.json\"), \"r\", encoding=\"utf-8\") as f:\n     52 \
          \            return cls(**json.load(f))\n     53 \n\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'TheBloke/koala-7B-GPTQ/quantize_config.json'\n\
          ```\nLooking forward to hearing from you.\nThanks,\nAndy "
        updatedAt: '2023-07-27T06:39:14.007Z'
      numEdits: 1
      reactions: []
    id: 64c21045c3b5e0a9761db2ac
    type: comment
  author: Andyrasika
  content: "Hi @TheBloke ,\n     While implementing your model (For reference, the\
    \ colab notebook: https://colab.research.google.com/drive/1DbMcJtY8m7-9MFrH6t6tgs10OTgbuOtG?usp=sharing)\n\
    i got the following error:\n```\nWARNING:auto_gptq.modeling._base:use_triton will\
    \ force moving the whole model to GPU, make sure you have enough VRAM.\n---------------------------------------------------------------------------\n\
    FileNotFoundError                         Traceback (most recent call last)\n\
    <ipython-input-13-d026160b78fa> in <cell line: 11>()\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n     10 \n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
    \n1 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
    \ in from_quantized(cls, save_dir, device, use_safetensors, use_triton, max_memory,\
    \ device_map, quantize_config, model_basename, trust_remote_code)\n    512 \n\
    \    513         if not isfile(model_save_name):\n--> 514            raise FileNotFoundError(f\"\
    Could not find model at {model_save_name}\")\n    515 \n    516         def skip(*args,\
    \ **kwargs):\n\nFileNotFoundError: Could not find model at TheBloke/koala-7B-GPTQ/koala-7b-GPTQ-4bit-128g.no-act.order.safetensors\n\
    ```\n\nif i change:\n```\n quantize_config=None)\n```\ni get the following error:\n\
    ```\nWARNING:auto_gptq.modeling._base:use_triton will force moving the whole model\
    \ to GPU, make sure you have enough VRAM.\n---------------------------------------------------------------------------\n\
    FileNotFoundError                         Traceback (most recent call last)\n\
    <ipython-input-16-ede7706bade9> in <cell line: 11>()\n      9 tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\n     10 \n---> 11 model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \     12         model_basename=model_basename,\n     13         use_safetensors=True,\n\
    \n2 frames\n/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\
    \ in from_pretrained(cls, save_dir)\n     49     @classmethod\n     50     def\
    \ from_pretrained(cls, save_dir: str):\n---> 51         with open(join(save_dir,\
    \ \"quantize_config.json\"), \"r\", encoding=\"utf-8\") as f:\n     52       \
    \      return cls(**json.load(f))\n     53 \n\nFileNotFoundError: [Errno 2] No\
    \ such file or directory: 'TheBloke/koala-7B-GPTQ/quantize_config.json'\n```\n\
    Looking forward to hearing from you.\nThanks,\nAndy "
  created_at: 2023-07-27 05:35:49+00:00
  edited: true
  hidden: false
  id: 64c21045c3b5e0a9761db2ac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/koala-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Re. Could not find model at TheBloke/koala-7B-GPTQ/koala-7b-GPTQ-4bit-128g.no-act.order.safetensors
