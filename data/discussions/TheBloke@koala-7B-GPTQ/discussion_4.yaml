!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mattymchen
conflicting_files: null
created_at: 2023-04-16 16:07:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-16T17:07:17.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , I try to run the inference using the latest GPTQ-for-LLaMa with a simpe\
          \ instruction \"Write a positive movie review\":</p>\n<pre><code>python\
          \ llama_inference.py TheBloke/koala-7B-GPTQ-4bit-128g --wbits 4 --groupsize\
          \ 128 --load koala-7B-4bit-128g.safetensors --text \"Write a positive movie\
          \ review\" --device=0\n</code></pre>\n<p>Yet, it returns nonsense output:</p>\n\
          <pre><code> \u2047  please write a positive movie review\\\\\\\\\\\\\\\\\
          \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
          \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
          \\\\\\\\\\\\\n</code></pre>\n<p>May I ask how to fix this?</p>\n<p>Thank\
          \ you!</p>\n"
        raw: "Hi @TheBloke , I try to run the inference using the latest GPTQ-for-LLaMa\
          \ with a simpe instruction \"Write a positive movie review\":\r\n```\r\n\
          python llama_inference.py TheBloke/koala-7B-GPTQ-4bit-128g --wbits 4 --groupsize\
          \ 128 --load koala-7B-4bit-128g.safetensors --text \"Write a positive movie\
          \ review\" --device=0\r\n```\r\n\r\nYet, it returns nonsense output:\r\n\
          ```\r\n \u2047  please write a positive movie review\\\\\\\\\\\\\\\\\\\\\
          \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
          \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
          \\\\\\\\\r\n```\r\n\r\nMay I ask how to fix this?\r\n\r\nThank you!"
        updatedAt: '2023-04-16T17:07:17.235Z'
      numEdits: 0
      reactions: []
    id: 643c2b45b409fef15e099ecb
    type: comment
  author: mattymchen
  content: "Hi @TheBloke , I try to run the inference using the latest GPTQ-for-LLaMa\
    \ with a simpe instruction \"Write a positive movie review\":\r\n```\r\npython\
    \ llama_inference.py TheBloke/koala-7B-GPTQ-4bit-128g --wbits 4 --groupsize 128\
    \ --load koala-7B-4bit-128g.safetensors --text \"Write a positive movie review\"\
    \ --device=0\r\n```\r\n\r\nYet, it returns nonsense output:\r\n```\r\n \u2047\
    \  please write a positive movie review\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
    \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\r\n```\r\n\r\nMay I ask\
    \ how to fix this?\r\n\r\nThank you!"
  created_at: 2023-04-16 16:07:17+00:00
  edited: false
  hidden: false
  id: 643c2b45b409fef15e099ecb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T17:14:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>As explained in the README, this is expected if you don''t use a
          more recent version of GPTQ-for-LLaMa inside text-generation-webui.</p>

          <p>I just updated the README to make this clearer.</p>

          <p>You could update the GPTQ-for-LLaMa installation, however you can''t
          use the very latest code because right now it doesn''t work with text-generation-webui.  You
          need to clone GPTQ-for-LLaMa at commit <code>58c8ab4c7aaccc50f507fd08cce941976affe5e0</code>.</p>

          <p>Or the easier method is to not use the <code>safetensors</code> file,
          and instead use <code>koala-7B-4bit-128g.no-act-order.ooba.pt</code></p>

          <p>So:</p>

          <ol>

          <li>Clone the repo locally</li>

          <li>Move/delete the <code>safetensors</code> file and <code>koala-7B-4bit-128g.pt</code>,
          such that the only model file remaining is <code>koala-7B-4bit-128g.no-act-order.ooba.pt</code></li>

          <li>Now run text-generation-webui and it will work.</li>

          </ol>

          <p>If you don''t want to clone the whole repo, just download all the JSON
          files, and the single model file <code>koala-7B-4bit-128g.no-act-order.ooba.pt</code>.</p>

          '
        raw: 'As explained in the README, this is expected if you don''t use a more
          recent version of GPTQ-for-LLaMa inside text-generation-webui.


          I just updated the README to make this clearer.


          You could update the GPTQ-for-LLaMa installation, however you can''t use
          the very latest code because right now it doesn''t work with text-generation-webui.  You
          need to clone GPTQ-for-LLaMa at commit `58c8ab4c7aaccc50f507fd08cce941976affe5e0`.


          Or the easier method is to not use the `safetensors` file, and instead use
          `koala-7B-4bit-128g.no-act-order.ooba.pt`


          So:

          1. Clone the repo locally

          2. Move/delete the `safetensors` file and `koala-7B-4bit-128g.pt`, such
          that the only model file remaining is `koala-7B-4bit-128g.no-act-order.ooba.pt`

          3. Now run text-generation-webui and it will work.


          If you don''t want to clone the whole repo, just download all the JSON files,
          and the single model file `koala-7B-4bit-128g.no-act-order.ooba.pt`.'
        updatedAt: '2023-04-16T17:14:22.329Z'
      numEdits: 0
      reactions: []
    id: 643c2cee21686867004006cc
    type: comment
  author: TheBloke
  content: 'As explained in the README, this is expected if you don''t use a more
    recent version of GPTQ-for-LLaMa inside text-generation-webui.


    I just updated the README to make this clearer.


    You could update the GPTQ-for-LLaMa installation, however you can''t use the very
    latest code because right now it doesn''t work with text-generation-webui.  You
    need to clone GPTQ-for-LLaMa at commit `58c8ab4c7aaccc50f507fd08cce941976affe5e0`.


    Or the easier method is to not use the `safetensors` file, and instead use `koala-7B-4bit-128g.no-act-order.ooba.pt`


    So:

    1. Clone the repo locally

    2. Move/delete the `safetensors` file and `koala-7B-4bit-128g.pt`, such that the
    only model file remaining is `koala-7B-4bit-128g.no-act-order.ooba.pt`

    3. Now run text-generation-webui and it will work.


    If you don''t want to clone the whole repo, just download all the JSON files,
    and the single model file `koala-7B-4bit-128g.no-act-order.ooba.pt`.'
  created_at: 2023-04-16 16:14:22+00:00
  edited: false
  hidden: false
  id: 643c2cee21686867004006cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-16T17:22:08.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>I just clone the latest version of GPTQ-for-LLaMa today (commit: 1cb5ae890f785a55f20ab07406423d0a05d22073).\
          \ I didn't use text-generation-webui. Instead, I just use the inference\
          \ code from GPTQ-for-LLAMA repo to get the generated results.</p>\n<pre><code>https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/triton/llama_inference.py\n\
          </code></pre>\n<p>Is this checkpoint not compatible with the GPTQ inference\
          \ code? Do I need to do any special preprocessing?</p>\n<p>Thank you!</p>\n"
        raw: 'Hi @TheBloke,


          I just clone the latest version of GPTQ-for-LLaMa today (commit: 1cb5ae890f785a55f20ab07406423d0a05d22073).
          I didn''t use text-generation-webui. Instead, I just use the inference code
          from GPTQ-for-LLAMA repo to get the generated results.

          ```

          https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/triton/llama_inference.py

          ```


          Is this checkpoint not compatible with the GPTQ inference code? Do I need
          to do any special preprocessing?


          Thank you!'
        updatedAt: '2023-04-16T17:22:08.778Z'
      numEdits: 0
      reactions: []
    id: 643c2ec02eeda4c05154c606
    type: comment
  author: mattymchen
  content: 'Hi @TheBloke,


    I just clone the latest version of GPTQ-for-LLaMa today (commit: 1cb5ae890f785a55f20ab07406423d0a05d22073).
    I didn''t use text-generation-webui. Instead, I just use the inference code from
    GPTQ-for-LLAMA repo to get the generated results.

    ```

    https://github.com/qwopqwop200/GPTQ-for-LLaMa/blob/triton/llama_inference.py

    ```


    Is this checkpoint not compatible with the GPTQ inference code? Do I need to do
    any special preprocessing?


    Thank you!'
  created_at: 2023-04-16 16:22:08+00:00
  edited: false
  hidden: false
  id: 643c2ec02eeda4c05154c606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T17:27:16.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh sorry, I did not read your initial message properly.  I didn''t
          spot you were calling <code>llama_inference.py</code></p>

          <p>Hmm, that result is surprising to me. I thought the latest GPTQ-for-LLaMa
          code would work with the <code>safetensors</code> file.</p>

          <p>Maybe he has made some changes that breaks it.</p>

          <p>Try this and let me know if it works:</p>

          <pre><code>git clone -n https://github.com/qwopqwop200/GPTQ-for-LLaMa gptq-April13

          cd gptq-April13

          git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0

          python llama_inference.py TheBloke/koala-7B-GPTQ-4bit-128g --wbits 4 --groupsize
          128 --load koala-7B-4bit-128g.safetensors --text "Write a positive movie
          review" --device=0

          </code></pre>

          '
        raw: 'Oh sorry, I did not read your initial message properly.  I didn''t spot
          you were calling `llama_inference.py`


          Hmm, that result is surprising to me. I thought the latest GPTQ-for-LLaMa
          code would work with the `safetensors` file.


          Maybe he has made some changes that breaks it.


          Try this and let me know if it works:

          ```

          git clone -n https://github.com/qwopqwop200/GPTQ-for-LLaMa gptq-April13

          cd gptq-April13

          git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0

          python llama_inference.py TheBloke/koala-7B-GPTQ-4bit-128g --wbits 4 --groupsize
          128 --load koala-7B-4bit-128g.safetensors --text "Write a positive movie
          review" --device=0

          ```'
        updatedAt: '2023-04-16T17:27:51.248Z'
      numEdits: 1
      reactions: []
    id: 643c2ff4c16819aac13443aa
    type: comment
  author: TheBloke
  content: 'Oh sorry, I did not read your initial message properly.  I didn''t spot
    you were calling `llama_inference.py`


    Hmm, that result is surprising to me. I thought the latest GPTQ-for-LLaMa code
    would work with the `safetensors` file.


    Maybe he has made some changes that breaks it.


    Try this and let me know if it works:

    ```

    git clone -n https://github.com/qwopqwop200/GPTQ-for-LLaMa gptq-April13

    cd gptq-April13

    git checkout 58c8ab4c7aaccc50f507fd08cce941976affe5e0

    python llama_inference.py TheBloke/koala-7B-GPTQ-4bit-128g --wbits 4 --groupsize
    128 --load koala-7B-4bit-128g.safetensors --text "Write a positive movie review"
    --device=0

    ```'
  created_at: 2023-04-16 16:27:16+00:00
  edited: true
  hidden: false
  id: 643c2ff4c16819aac13443aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-16T17:38:09.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: "<p>Thank you for the help. The output is much more reasonable now:</p>\n\
          <pre><code> \u2047  Write a positive movie review:\n\n\"I recently watched\
          \ a movie called \"The Lighthouse,\" and I was thoroughly impressed by its\
          \ unique style and captivating story. The two main characters, Thomas (played\
          \ by Willem\n</code></pre>\n<p>Do I need to format the input to a special\
          \ server client format, like</p>\n<pre><code>https://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n\
          </code></pre>\n<p>Or just plain instruction \"Write a positive movie review\"\
          \ is enough?</p>\n<p>Besides, I find that there are two version of koala.\
          \ This is v2, right?</p>\n"
        raw: "Thank you for the help. The output is much more reasonable now:\n```\n\
          \ \u2047  Write a positive movie review:\n\n\"I recently watched a movie\
          \ called \"The Lighthouse,\" and I was thoroughly impressed by its unique\
          \ style and captivating story. The two main characters, Thomas (played by\
          \ Willem\n```\n\nDo I need to format the input to a special server client\
          \ format, like\n```\nhttps://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n\
          ```\nOr just plain instruction \"Write a positive movie review\" is enough?\n\
          \nBesides, I find that there are two version of koala. This is v2, right?"
        updatedAt: '2023-04-16T17:38:09.575Z'
      numEdits: 0
      reactions: []
    id: 643c3281f925266fb59b9a46
    type: comment
  author: mattymchen
  content: "Thank you for the help. The output is much more reasonable now:\n```\n\
    \ \u2047  Write a positive movie review:\n\n\"I recently watched a movie called\
    \ \"The Lighthouse,\" and I was thoroughly impressed by its unique style and captivating\
    \ story. The two main characters, Thomas (played by Willem\n```\n\nDo I need to\
    \ format the input to a special server client format, like\n```\nhttps://github.com/lm-sys/FastChat/blob/main/fastchat/conversation.py\n\
    ```\nOr just plain instruction \"Write a positive movie review\" is enough?\n\n\
    Besides, I find that there are two version of koala. This is v2, right?"
  created_at: 2023-04-16 16:38:09+00:00
  edited: false
  hidden: false
  id: 643c3281f925266fb59b9a46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T17:42:04.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Great! Glad that's working now.</p>\n<p>Here is the format I use\
          \ for Koala. This seems to work best for how it was trained:</p>\n<pre><code>BEGINNING\
          \ OF CONVERSATION: \nUSER: write a story about llamas\nGPT:\n</code></pre>\n\
          <p>The other format htat is commonly used for these models is this - for\
          \ example, this is recommended on Vicuna:</p>\n<pre><code>Below is an instruction\
          \ that describes a task. Write a response that appropriately completes the\
          \ request.\n### Instruction:\nWrite a story about llamas\n### Response:\"\
          \n</code></pre>\n<p>I think the first prompt is meant to be better on Koala,\
          \ but you could try both and see how you get on.</p>\n<p>As for two versions:\
          \ I haven't heard of a second version of Koala?  This is definitely v1,\
          \ and I didn't know there was a v2.</p>\n<p>There was a v1.0 and v1.1 of\
          \ Vicuna - are you maybe thinking of that? I have done v1.1 uploads for\
          \ Vicuna if you want to try that.  It's very good. I have heard reports\
          \ that Koala may do better with some prompts, and Vicuna 1.1 may do better\
          \ with other prompts.  So they are both worth trying.</p>\n"
        raw: "Great! Glad that's working now.\n\nHere is the format I use for Koala.\
          \ This seems to work best for how it was trained:\n```\nBEGINNING OF CONVERSATION:\
          \ \nUSER: write a story about llamas\nGPT:\n```\n\nThe other format htat\
          \ is commonly used for these models is this - for example, this is recommended\
          \ on Vicuna:\n```\nBelow is an instruction that describes a task. Write\
          \ a response that appropriately completes the request.\n### Instruction:\n\
          Write a story about llamas\n### Response:\"\n```\n\nI think the first prompt\
          \ is meant to be better on Koala, but you could try both and see how you\
          \ get on.\n\nAs for two versions: I haven't heard of a second version of\
          \ Koala?  This is definitely v1, and I didn't know there was a v2.\n\nThere\
          \ was a v1.0 and v1.1 of Vicuna - are you maybe thinking of that? I have\
          \ done v1.1 uploads for Vicuna if you want to try that.  It's very good.\
          \ I have heard reports that Koala may do better with some prompts, and Vicuna\
          \ 1.1 may do better with other prompts.  So they are both worth trying."
        updatedAt: '2023-04-16T17:42:25.784Z'
      numEdits: 1
      reactions: []
    id: 643c336ce3a7bbe2cf3ce876
    type: comment
  author: TheBloke
  content: "Great! Glad that's working now.\n\nHere is the format I use for Koala.\
    \ This seems to work best for how it was trained:\n```\nBEGINNING OF CONVERSATION:\
    \ \nUSER: write a story about llamas\nGPT:\n```\n\nThe other format htat is commonly\
    \ used for these models is this - for example, this is recommended on Vicuna:\n\
    ```\nBelow is an instruction that describes a task. Write a response that appropriately\
    \ completes the request.\n### Instruction:\nWrite a story about llamas\n### Response:\"\
    \n```\n\nI think the first prompt is meant to be better on Koala, but you could\
    \ try both and see how you get on.\n\nAs for two versions: I haven't heard of\
    \ a second version of Koala?  This is definitely v1, and I didn't know there was\
    \ a v2.\n\nThere was a v1.0 and v1.1 of Vicuna - are you maybe thinking of that?\
    \ I have done v1.1 uploads for Vicuna if you want to try that.  It's very good.\
    \ I have heard reports that Koala may do better with some prompts, and Vicuna\
    \ 1.1 may do better with other prompts.  So they are both worth trying."
  created_at: 2023-04-16 16:42:04+00:00
  edited: true
  hidden: false
  id: 643c336ce3a7bbe2cf3ce876
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-16T17:47:11.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: '<p>Thank you for answering. I see there are two versions of koala here,
          v1 and v2:</p>

          <pre><code>https://huggingface.co/young-geng/koala/tree/main

          </code></pre>

          <p>The Vicuna prompt you sent is for v1 or v1.1?</p>

          <p>May I ask where can I find more resources about how to prompting vicuna
          and koala properly?</p>

          '
        raw: 'Thank you for answering. I see there are two versions of koala here,
          v1 and v2:

          ```

          https://huggingface.co/young-geng/koala/tree/main

          ```


          The Vicuna prompt you sent is for v1 or v1.1?


          May I ask where can I find more resources about how to prompting vicuna
          and koala properly?'
        updatedAt: '2023-04-16T17:47:11.257Z'
      numEdits: 0
      reactions: []
    id: 643c349ff925266fb59ba751
    type: comment
  author: mattymchen
  content: 'Thank you for answering. I see there are two versions of koala here, v1
    and v2:

    ```

    https://huggingface.co/young-geng/koala/tree/main

    ```


    The Vicuna prompt you sent is for v1 or v1.1?


    May I ask where can I find more resources about how to prompting vicuna and koala
    properly?'
  created_at: 2023-04-16 16:47:11+00:00
  edited: false
  hidden: false
  id: 643c349ff925266fb59ba751
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T17:51:49.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh OK yeah I see. Then yes this is Koala v2.  They published the
          v1 and v2 files at the same time, so I guess v1 was an earlier attempt and
          v2 was tweaked in some way. To my knowledge they never explained why there
          was v1 and v2. But yes I used the v2.</p>

          <p>The Vicuna prompt I sent works equally on v1 and v1.1.  There is only
          a small difference between Vicuna 1.0 and 1.1 - but 1.1 is slightly better.</p>

          <p>I don''t know of specific resources on prompting these models.  But I
          can recommend the YouTube channel of Sam Witteveen. Each time a new model
          comes out he tries it in Google Colab and publishes the code. I think it
          was on his YouTube about Koala that I saw the prompt that I showed you above.
          His channel is here: <a rel="nofollow" href="https://www.youtube.com/@samwitteveenai">https://www.youtube.com/@samwitteveenai</a></p>

          <p>You could also try discussing on Discord, if you use that.  Here are
          two Discord servers I use:<br>Nomic AI (GPT4ALL): <a rel="nofollow" href="https://discord.gg/ZHaesTnb">https://discord.gg/ZHaesTnb</a><br>Alpaca-Lora:
          <a rel="nofollow" href="https://discord.gg/2pUCtdeS">https://discord.gg/2pUCtdeS</a></p>

          <p>They both have good discussions on local models, how to prompt them,
          etc.</p>

          '
        raw: 'Oh OK yeah I see. Then yes this is Koala v2.  They published the v1
          and v2 files at the same time, so I guess v1 was an earlier attempt and
          v2 was tweaked in some way. To my knowledge they never explained why there
          was v1 and v2. But yes I used the v2.


          The Vicuna prompt I sent works equally on v1 and v1.1.  There is only a
          small difference between Vicuna 1.0 and 1.1 - but 1.1 is slightly better.


          I don''t know of specific resources on prompting these models.  But I can
          recommend the YouTube channel of Sam Witteveen. Each time a new model comes
          out he tries it in Google Colab and publishes the code. I think it was on
          his YouTube about Koala that I saw the prompt that I showed you above. His
          channel is here: https://www.youtube.com/@samwitteveenai


          You could also try discussing on Discord, if you use that.  Here are two
          Discord servers I use:

          Nomic AI (GPT4ALL): https://discord.gg/ZHaesTnb

          Alpaca-Lora: https://discord.gg/2pUCtdeS


          They both have good discussions on local models, how to prompt them, etc.'
        updatedAt: '2023-04-16T17:52:48.794Z'
      numEdits: 2
      reactions: []
    id: 643c35b525c7610a1cd696a0
    type: comment
  author: TheBloke
  content: 'Oh OK yeah I see. Then yes this is Koala v2.  They published the v1 and
    v2 files at the same time, so I guess v1 was an earlier attempt and v2 was tweaked
    in some way. To my knowledge they never explained why there was v1 and v2. But
    yes I used the v2.


    The Vicuna prompt I sent works equally on v1 and v1.1.  There is only a small
    difference between Vicuna 1.0 and 1.1 - but 1.1 is slightly better.


    I don''t know of specific resources on prompting these models.  But I can recommend
    the YouTube channel of Sam Witteveen. Each time a new model comes out he tries
    it in Google Colab and publishes the code. I think it was on his YouTube about
    Koala that I saw the prompt that I showed you above. His channel is here: https://www.youtube.com/@samwitteveenai


    You could also try discussing on Discord, if you use that.  Here are two Discord
    servers I use:

    Nomic AI (GPT4ALL): https://discord.gg/ZHaesTnb

    Alpaca-Lora: https://discord.gg/2pUCtdeS


    They both have good discussions on local models, how to prompt them, etc.'
  created_at: 2023-04-16 16:51:49+00:00
  edited: true
  hidden: false
  id: 643c35b525c7610a1cd696a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-16T17:55:51.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: '<p>Thank you so much. That''s very helpful.</p>

          <p>By the way, did you compare the inference speed of GPTQ and fp16?</p>

          '
        raw: 'Thank you so much. That''s very helpful.


          By the way, did you compare the inference speed of GPTQ and fp16?'
        updatedAt: '2023-04-16T17:55:51.500Z'
      numEdits: 0
      reactions: []
    id: 643c36a7c16819aac1346f83
    type: comment
  author: mattymchen
  content: 'Thank you so much. That''s very helpful.


    By the way, did you compare the inference speed of GPTQ and fp16?'
  created_at: 2023-04-16 16:55:51+00:00
  edited: false
  hidden: false
  id: 643c36a7c16819aac1346f83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T09:06:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Not formally. But I have done inference in both, and I seem to remember
          that GPTQ was always quite a bit quicker, even on an A100 with 40GB VRAM.</p>

          '
        raw: Not formally. But I have done inference in both, and I seem to remember
          that GPTQ was always quite a bit quicker, even on an A100 with 40GB VRAM.
        updatedAt: '2023-04-17T09:06:52.403Z'
      numEdits: 0
      reactions: []
    id: 643d0c2cd2c1e08a5ec8f377
    type: comment
  author: TheBloke
  content: Not formally. But I have done inference in both, and I seem to remember
    that GPTQ was always quite a bit quicker, even on an A100 with 40GB VRAM.
  created_at: 2023-04-17 08:06:52+00:00
  edited: false
  hidden: false
  id: 643d0c2cd2c1e08a5ec8f377
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-17T09:17:01.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: "<p>Thank you. I also find a weird thing for the tokenization:</p>\n\
          <pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer\n&gt;&gt;&gt;\
          \ tokenizer1=AutoTokenizer.from_pretrained('huggyllama/llama-7b',use_fast=False)\n\
          &gt;&gt;&gt; tokenizer2=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
          \ use_fast=False)\n&gt;&gt;&gt; tokenizer1.decode(tokenizer1.encode('hello',\
          \ add_special_tokens=True))\n' \u2047  hello'\n&gt;&gt;&gt; tokenizer2.decode(tokenizer2.encode('hello',\
          \ add_special_tokens=True))\n'&lt;s&gt;hello'\n</code></pre>\n<p>Have you\
          \ encountered the same issue?</p>\n"
        raw: "Thank you. I also find a weird thing for the tokenization:\n```\n>>>\
          \ from transformers import AutoTokenizer\n>>> tokenizer1=AutoTokenizer.from_pretrained('huggyllama/llama-7b',use_fast=False)\n\
          >>> tokenizer2=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF', use_fast=False)\n\
          >>> tokenizer1.decode(tokenizer1.encode('hello', add_special_tokens=True))\n\
          ' \u2047  hello'\n>>> tokenizer2.decode(tokenizer2.encode('hello', add_special_tokens=True))\n\
          '<s>hello'\n```\n\nHave you encountered the same issue?"
        updatedAt: '2023-04-17T09:17:01.643Z'
      numEdits: 0
      reactions: []
    id: 643d0e8d5ccaed9c172cb41c
    type: comment
  author: mattymchen
  content: "Thank you. I also find a weird thing for the tokenization:\n```\n>>> from\
    \ transformers import AutoTokenizer\n>>> tokenizer1=AutoTokenizer.from_pretrained('huggyllama/llama-7b',use_fast=False)\n\
    >>> tokenizer2=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF', use_fast=False)\n\
    >>> tokenizer1.decode(tokenizer1.encode('hello', add_special_tokens=True))\n'\
    \ \u2047  hello'\n>>> tokenizer2.decode(tokenizer2.encode('hello', add_special_tokens=True))\n\
    '<s>hello'\n```\n\nHave you encountered the same issue?"
  created_at: 2023-04-17 08:17:01+00:00
  edited: false
  hidden: false
  id: 643d0e8d5ccaed9c172cb41c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T09:21:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I think that''s normal? These later models define start-of-string
          (or start-of-generation) as <code>&lt;s&gt;</code>, end-of-string (or end
          of generation/end of text) as <code>&lt;/s&gt;</code> and also have an <code>&lt;unk&gt;</code>
          token.</p>

          <p>I think these are meant to be defined in special_tokens.json, like you
          see here in Vicuna: <a href="https://huggingface.co/TheBloke/vicuna-13B-1.1-HF/blob/main/special_tokens_map.json">https://huggingface.co/TheBloke/vicuna-13B-1.1-HF/blob/main/special_tokens_map.json</a></p>

          <p>But that file is empty in Koala. Maybe I should fill it out.  To be honest
          I''ve never been 100% certain as to what models added/changed those special
          tokens and whether I was meant to fill in that file manually if it wasn''t
          filled out by the conversion process.  But all the recent models seem to
          use that special_tokens_map.json so I think it''s correct.</p>

          '
        raw: 'I think that''s normal? These later models define start-of-string (or
          start-of-generation) as `<s>`, end-of-string (or end of generation/end of
          text) as `</s>` and also have an `<unk>` token.


          I think these are meant to be defined in special_tokens.json, like you see
          here in Vicuna: https://huggingface.co/TheBloke/vicuna-13B-1.1-HF/blob/main/special_tokens_map.json


          But that file is empty in Koala. Maybe I should fill it out.  To be honest
          I''ve never been 100% certain as to what models added/changed those special
          tokens and whether I was meant to fill in that file manually if it wasn''t
          filled out by the conversion process.  But all the recent models seem to
          use that special_tokens_map.json so I think it''s correct.'
        updatedAt: '2023-04-17T09:21:27.131Z'
      numEdits: 0
      reactions: []
    id: 643d0f9711ea48e24655e96d
    type: comment
  author: TheBloke
  content: 'I think that''s normal? These later models define start-of-string (or
    start-of-generation) as `<s>`, end-of-string (or end of generation/end of text)
    as `</s>` and also have an `<unk>` token.


    I think these are meant to be defined in special_tokens.json, like you see here
    in Vicuna: https://huggingface.co/TheBloke/vicuna-13B-1.1-HF/blob/main/special_tokens_map.json


    But that file is empty in Koala. Maybe I should fill it out.  To be honest I''ve
    never been 100% certain as to what models added/changed those special tokens and
    whether I was meant to fill in that file manually if it wasn''t filled out by
    the conversion process.  But all the recent models seem to use that special_tokens_map.json
    so I think it''s correct.'
  created_at: 2023-04-17 08:21:27+00:00
  edited: false
  hidden: false
  id: 643d0f9711ea48e24655e96d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-17T09:32:15.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: '<p>I think using vicuna or llama''s tokenizer config/mapping should
          be okay. Thanks.</p>

          '
        raw: I think using vicuna or llama's tokenizer config/mapping should be okay.
          Thanks.
        updatedAt: '2023-04-17T09:32:15.998Z'
      numEdits: 0
      reactions: []
    id: 643d121f1c29e1386bb7a8cc
    type: comment
  author: mattymchen
  content: I think using vicuna or llama's tokenizer config/mapping should be okay.
    Thanks.
  created_at: 2023-04-17 08:32:15+00:00
  edited: false
  hidden: false
  id: 643d121f1c29e1386bb7a8cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T09:45:38.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I just looked at the code for <a rel=\"nofollow\" href=\"https://github.com/young-geng/EasyLM\"\
          >Koala EasyLM</a>, the custom inference server they built (similar in principle\
          \ to Vicuna FastChat I believe).  And when they <a rel=\"nofollow\" href=\"\
          https://github.com/young-geng/EasyLM/blob/main/EasyLM/models/llama/convert_easylm_to_hf.py\"\
          >convert the model to HF</a> they specifically set <code>special_tokens_map.json</code>\
          \ to be empty:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">def</span> <span class=\"hljs-title function_\">write_tokenizer</span>(<span\
          \ class=\"hljs-params\">tokenizer_path, input_tokenizer_path</span>):\n\
          \    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Fetching the tokenizer from <span class=\"hljs-subst\">{input_tokenizer_path}</span>.\"\
          </span>)\n    os.makedirs(tokenizer_path, exist_ok=<span class=\"hljs-literal\"\
          >True</span>)\n    write_json({}, os.path.join(tokenizer_path, <span class=\"\
          hljs-string\">\"special_tokens_map.json\"</span>))\n    write_json(\n  \
          \      {\n            <span class=\"hljs-string\">\"bos_token\"</span>:\
          \ <span class=\"hljs-string\">\"\"</span>,\n            <span class=\"hljs-string\"\
          >\"eos_token\"</span>: <span class=\"hljs-string\">\"\"</span>,\n      \
          \      <span class=\"hljs-string\">\"model_max_length\"</span>: <span class=\"\
          hljs-built_in\">int</span>(<span class=\"hljs-number\">1e30</span>),\n \
          \           <span class=\"hljs-string\">\"tokenizer_class\"</span>: <span\
          \ class=\"hljs-string\">\"LlamaTokenizer\"</span>,\n            <span class=\"\
          hljs-string\">\"unk_token\"</span>: <span class=\"hljs-string\">\"\"</span>,\n\
          \        },\n        os.path.join(tokenizer_path, <span class=\"hljs-string\"\
          >\"tokenizer_config.json\"</span>),\n    )\n    shutil.copyfile(input_tokenizer_path,\
          \ os.path.join(tokenizer_path, <span class=\"hljs-string\">\"tokenizer.model\"\
          </span>))\n</code></pre>\n<p>So I'm not sure why they do that or what the\
          \ implications are, but it would seem that we're not meant to define the\
          \ <code>&lt;s&gt;</code> etc for Koala like we do for Vicuna.</p>\n<p>Also\
          \ I'm confused because I'm not getting the same results you are when I run\
          \ your test code. Comparing Koala and Vicuna, I do not get a <code>&lt;s&gt;</code>\
          \ from Koala like you did:</p>\n<pre><code>&gt;&gt;&gt; from transformers\
          \ import AutoTokenizer\n&gt;&gt;&gt; tokenizer_koala=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
          \ use_fast=False)\n&gt;&gt;&gt; tokenizer_vicuna=AutoTokenizer.from_pretrained('TheBloke/vicuna-13B-1.1-HF',\
          \ use_fast=False)\n&gt;&gt;&gt; tokenizer_koala.decode(tokenizer_koala.encode('hello',\
          \ add_special_tokens=True))\n' hello'\n&gt;&gt;&gt; tokenizer_vicuna.decode(tokenizer_vicuna.encode('hello',\
          \ add_special_tokens=True))\n'  &lt;s&gt;hello'\n</code></pre>\n"
        raw: "I just looked at the code for [Koala EasyLM](https://github.com/young-geng/EasyLM),\
          \ the custom inference server they built (similar in principle to Vicuna\
          \ FastChat I believe).  And when they [convert the model to HF](https://github.com/young-geng/EasyLM/blob/main/EasyLM/models/llama/convert_easylm_to_hf.py)\
          \ they specifically set `special_tokens_map.json` to be empty:\n\n```python\n\
          def write_tokenizer(tokenizer_path, input_tokenizer_path):\n    print(f\"\
          Fetching the tokenizer from {input_tokenizer_path}.\")\n    os.makedirs(tokenizer_path,\
          \ exist_ok=True)\n    write_json({}, os.path.join(tokenizer_path, \"special_tokens_map.json\"\
          ))\n    write_json(\n        {\n            \"bos_token\": \"\",\n     \
          \       \"eos_token\": \"\",\n            \"model_max_length\": int(1e30),\n\
          \            \"tokenizer_class\": \"LlamaTokenizer\",\n            \"unk_token\"\
          : \"\",\n        },\n        os.path.join(tokenizer_path, \"tokenizer_config.json\"\
          ),\n    )\n    shutil.copyfile(input_tokenizer_path, os.path.join(tokenizer_path,\
          \ \"tokenizer.model\"))\n\n```\n\nSo I'm not sure why they do that or what\
          \ the implications are, but it would seem that we're not meant to define\
          \ the `<s>` etc for Koala like we do for Vicuna.\n\nAlso I'm confused because\
          \ I'm not getting the same results you are when I run your test code. Comparing\
          \ Koala and Vicuna, I do not get a `<s>` from Koala like you did:\n\n```\n\
          >>> from transformers import AutoTokenizer\n>>> tokenizer_koala=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
          \ use_fast=False)\n>>> tokenizer_vicuna=AutoTokenizer.from_pretrained('TheBloke/vicuna-13B-1.1-HF',\
          \ use_fast=False)\n>>> tokenizer_koala.decode(tokenizer_koala.encode('hello',\
          \ add_special_tokens=True))\n' hello'\n>>> tokenizer_vicuna.decode(tokenizer_vicuna.encode('hello',\
          \ add_special_tokens=True))\n'  <s>hello'\n```"
        updatedAt: '2023-04-17T09:50:03.500Z'
      numEdits: 3
      reactions: []
    id: 643d15421c29e1386bb7c2ba
    type: comment
  author: TheBloke
  content: "I just looked at the code for [Koala EasyLM](https://github.com/young-geng/EasyLM),\
    \ the custom inference server they built (similar in principle to Vicuna FastChat\
    \ I believe).  And when they [convert the model to HF](https://github.com/young-geng/EasyLM/blob/main/EasyLM/models/llama/convert_easylm_to_hf.py)\
    \ they specifically set `special_tokens_map.json` to be empty:\n\n```python\n\
    def write_tokenizer(tokenizer_path, input_tokenizer_path):\n    print(f\"Fetching\
    \ the tokenizer from {input_tokenizer_path}.\")\n    os.makedirs(tokenizer_path,\
    \ exist_ok=True)\n    write_json({}, os.path.join(tokenizer_path, \"special_tokens_map.json\"\
    ))\n    write_json(\n        {\n            \"bos_token\": \"\",\n           \
    \ \"eos_token\": \"\",\n            \"model_max_length\": int(1e30),\n       \
    \     \"tokenizer_class\": \"LlamaTokenizer\",\n            \"unk_token\": \"\"\
    ,\n        },\n        os.path.join(tokenizer_path, \"tokenizer_config.json\"\
    ),\n    )\n    shutil.copyfile(input_tokenizer_path, os.path.join(tokenizer_path,\
    \ \"tokenizer.model\"))\n\n```\n\nSo I'm not sure why they do that or what the\
    \ implications are, but it would seem that we're not meant to define the `<s>`\
    \ etc for Koala like we do for Vicuna.\n\nAlso I'm confused because I'm not getting\
    \ the same results you are when I run your test code. Comparing Koala and Vicuna,\
    \ I do not get a `<s>` from Koala like you did:\n\n```\n>>> from transformers\
    \ import AutoTokenizer\n>>> tokenizer_koala=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
    \ use_fast=False)\n>>> tokenizer_vicuna=AutoTokenizer.from_pretrained('TheBloke/vicuna-13B-1.1-HF',\
    \ use_fast=False)\n>>> tokenizer_koala.decode(tokenizer_koala.encode('hello',\
    \ add_special_tokens=True))\n' hello'\n>>> tokenizer_vicuna.decode(tokenizer_vicuna.encode('hello',\
    \ add_special_tokens=True))\n'  <s>hello'\n```"
  created_at: 2023-04-17 08:45:38+00:00
  edited: true
  hidden: false
  id: 643d15421c29e1386bb7c2ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T09:54:17.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Ah, but Koala does definitely use those same tokens. Here's <a rel=\"\
          nofollow\" href=\"https://github.com/young-geng/EasyLM/blob/main/EasyLM/models/llama/llama_model.py\"\
          >EasyLM/models/llama/llama_model.py</a> :</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >__init__</span>(<span class=\"hljs-params\"></span>\n<span class=\"hljs-params\"\
          >        self,</span>\n<span class=\"hljs-params\">        vocab_file,</span>\n\
          <span class=\"hljs-params\">        unk_token=<span class=\"hljs-string\"\
          >\"&lt;unk&gt;\"</span>,</span>\n<span class=\"hljs-params\">        bos_token=<span\
          \ class=\"hljs-string\">\"&lt;s&gt;\"</span>,</span>\n<span class=\"hljs-params\"\
          >        eos_token=<span class=\"hljs-string\">\"&lt;/s&gt;\"</span>,</span>\n\
          <span class=\"hljs-params\">        sp_model_kwargs: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-type\">Dict</span>[<span class=\"hljs-built_in\"\
          >str</span>, <span class=\"hljs-type\">Any</span>]] = <span class=\"hljs-literal\"\
          >None</span>,</span>\n<span class=\"hljs-params\">        add_bos_token=<span\
          \ class=\"hljs-literal\">False</span>,</span>\n<span class=\"hljs-params\"\
          >        add_eos_token=<span class=\"hljs-literal\">False</span>,</span>\n\
          <span class=\"hljs-params\">        **kwargs,</span>\n<span class=\"hljs-params\"\
          >    </span>):\n        self.sp_model_kwargs = {} <span class=\"hljs-keyword\"\
          >if</span> sp_model_kwargs <span class=\"hljs-keyword\">is</span> <span\
          \ class=\"hljs-literal\">None</span> <span class=\"hljs-keyword\">else</span>\
          \ sp_model_kwargs\n        <span class=\"hljs-built_in\">super</span>().__init__(bos_token=bos_token,\
          \ eos_token=eos_token, unk_token=unk_token, **kwargs)\n        self.vocab_file\
          \ = vocab_file\n        self.add_bos_token = add_bos_token\n        self.add_eos_token\
          \ = add_eos_token\n        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
          \n        <span class=\"hljs-keyword\">with</span> tempfile.NamedTemporaryFile()\
          \ <span class=\"hljs-keyword\">as</span> tfile:\n            <span class=\"\
          hljs-keyword\">with</span> open_file(self.vocab_file, <span class=\"hljs-string\"\
          >'rb'</span>) <span class=\"hljs-keyword\">as</span> fin:\n            \
          \    tfile.write(fin.read())\n                tfile.flush()\n          \
          \      tfile.seek(<span class=\"hljs-number\">0</span>)\n            self.sp_model.Load(tfile.name)\n\
          \        <span class=\"hljs-string\">\"\"\" Initialisation\"\"\"</span>\n\
          \        self.add_special_tokens(<span class=\"hljs-built_in\">dict</span>(\n\
          \            unk_token=unk_token,\n            bos_token=bos_token,\n  \
          \          eos_token=eos_token,\n        ))\n        self.pad_token_id =\
          \ self.unk_token_id\n</code></pre>\n<p>So they are defining the same EOS,\
          \ BOS and UNK as we saw in Vicuna (and all the other models I've looked\
          \ at).  But for some reason they don't write that to <code>special_tokens_map.json</code>.</p>\n\
          <p>Yet, the Tokenizer still used those symbols?  I'm doing another test,\
          \ one sec..</p>\n"
        raw: "Ah, but Koala does definitely use those same tokens. Here's [EasyLM/models/llama/llama_model.py](https://github.com/young-geng/EasyLM/blob/main/EasyLM/models/llama/llama_model.py)\
          \ :\n\n```python\ndef __init__(\n        self,\n        vocab_file,\n  \
          \      unk_token=\"<unk>\",\n        bos_token=\"<s>\",\n        eos_token=\"\
          </s>\",\n        sp_model_kwargs: Optional[Dict[str, Any]] = None,\n   \
          \     add_bos_token=False,\n        add_eos_token=False,\n        **kwargs,\n\
          \    ):\n        self.sp_model_kwargs = {} if sp_model_kwargs is None else\
          \ sp_model_kwargs\n        super().__init__(bos_token=bos_token, eos_token=eos_token,\
          \ unk_token=unk_token, **kwargs)\n        self.vocab_file = vocab_file\n\
          \        self.add_bos_token = add_bos_token\n        self.add_eos_token\
          \ = add_eos_token\n        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
          \n        with tempfile.NamedTemporaryFile() as tfile:\n            with\
          \ open_file(self.vocab_file, 'rb') as fin:\n                tfile.write(fin.read())\n\
          \                tfile.flush()\n                tfile.seek(0)\n        \
          \    self.sp_model.Load(tfile.name)\n        \"\"\" Initialisation\"\"\"\
          \n        self.add_special_tokens(dict(\n            unk_token=unk_token,\n\
          \            bos_token=bos_token,\n            eos_token=eos_token,\n  \
          \      ))\n        self.pad_token_id = self.unk_token_id\n```\n\nSo they\
          \ are defining the same EOS, BOS and UNK as we saw in Vicuna (and all the\
          \ other models I've looked at).  But for some reason they don't write that\
          \ to `special_tokens_map.json`.\n\nYet, the Tokenizer still used those symbols?\
          \  I'm doing another test, one sec.."
        updatedAt: '2023-04-17T10:13:48.042Z'
      numEdits: 1
      reactions: []
    id: 643d174911ea48e2465629fb
    type: comment
  author: TheBloke
  content: "Ah, but Koala does definitely use those same tokens. Here's [EasyLM/models/llama/llama_model.py](https://github.com/young-geng/EasyLM/blob/main/EasyLM/models/llama/llama_model.py)\
    \ :\n\n```python\ndef __init__(\n        self,\n        vocab_file,\n        unk_token=\"\
    <unk>\",\n        bos_token=\"<s>\",\n        eos_token=\"</s>\",\n        sp_model_kwargs:\
    \ Optional[Dict[str, Any]] = None,\n        add_bos_token=False,\n        add_eos_token=False,\n\
    \        **kwargs,\n    ):\n        self.sp_model_kwargs = {} if sp_model_kwargs\
    \ is None else sp_model_kwargs\n        super().__init__(bos_token=bos_token,\
    \ eos_token=eos_token, unk_token=unk_token, **kwargs)\n        self.vocab_file\
    \ = vocab_file\n        self.add_bos_token = add_bos_token\n        self.add_eos_token\
    \ = add_eos_token\n        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n\
    \n        with tempfile.NamedTemporaryFile() as tfile:\n            with open_file(self.vocab_file,\
    \ 'rb') as fin:\n                tfile.write(fin.read())\n                tfile.flush()\n\
    \                tfile.seek(0)\n            self.sp_model.Load(tfile.name)\n \
    \       \"\"\" Initialisation\"\"\"\n        self.add_special_tokens(dict(\n \
    \           unk_token=unk_token,\n            bos_token=bos_token,\n         \
    \   eos_token=eos_token,\n        ))\n        self.pad_token_id = self.unk_token_id\n\
    ```\n\nSo they are defining the same EOS, BOS and UNK as we saw in Vicuna (and\
    \ all the other models I've looked at).  But for some reason they don't write\
    \ that to `special_tokens_map.json`.\n\nYet, the Tokenizer still used those symbols?\
    \  I'm doing another test, one sec.."
  created_at: 2023-04-17 08:54:17+00:00
  edited: true
  hidden: false
  id: 643d174911ea48e2465629fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T10:01:29.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK so I made a new local copy of my <code>koala-7B-HF</code>, and\
          \ I filled out <code>special_tokens_map.json</code> and <code>tokenizer_config.json</code>:</p>\n\
          <pre><code class=\"language-shell\"><span class=\"hljs-meta prompt_\">$\
          \ </span><span class=\"language-bash\"><span class=\"hljs-built_in\">cat</span>\
          \ /Users/tomj/src/huggingface/test-koala-7B-HF/special_tokens_map.json</span>\n\
          {\n  \"bos_token\": {\n    \"content\": \"&lt;s&gt;\",\n    \"lstrip\":\
          \ false,\n    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\"\
          : false\n  },\n  \"eos_token\": {\n    \"content\": \"&lt;/s&gt;\",\n  \
          \  \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\": false,\n\
          \    \"single_word\": false\n  },\n  \"unk_token\": {\n    \"content\":\
          \ \"&lt;unk&gt;\",\n    \"lstrip\": false,\n    \"normalized\": true,\n\
          \    \"rstrip\": false,\n    \"single_word\": false\n  }\n}\n<span class=\"\
          hljs-meta prompt_\"></span>\n<span class=\"hljs-meta prompt_\">$ </span><span\
          \ class=\"language-bash\"><span class=\"hljs-built_in\">cat</span> /Users/tomj/src/huggingface/test-koala-7B-HF/tokenizer_config.json</span>\n\
          {\n  \"add_bos_token\": true,\n  \"add_eos_token\": false,\n  \"bos_token\"\
          : {\n    \"__type\": \"AddedToken\",\n    \"content\": \"&lt;s&gt;\",\n\
          \    \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\": false,\n\
          \    \"single_word\": false\n  },\n  \"clean_up_tokenization_spaces\": false,\n\
          \  \"eos_token\": {\n    \"__type\": \"AddedToken\",\n    \"content\": \"\
          &lt;/s&gt;\",\n    \"lstrip\": false,\n    \"normalized\": true,\n    \"\
          rstrip\": false,\n    \"single_word\": false\n  },\n  \"model_max_length\"\
          : 1000000000000000019884624838656,\n  \"pad_token\": null,\n  \"sp_model_kwargs\"\
          : {},\n  \"tokenizer_class\": \"LlamaTokenizer\",\n  \"unk_token\": {\n\
          \    \"__type\": \"AddedToken\",\n    \"content\": \"&lt;unk&gt;\",\n  \
          \  \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\": false,\n\
          \    \"single_word\": false\n  }\n}\n</code></pre>\n<p>And then I re-ran\
          \ the code I mentioned a moment ago, referencing the local model:</p>\n\
          <pre><code>&gt;&gt;&gt; from transformers import AutoTokenizer\n&gt;&gt;&gt;\
          \ tokenizer_local_koala=AutoTokenizer.from_pretrained('/Users/tomj/src/huggingface/test-koala-7B-HF',\
          \ use_fast=False)\n&gt;&gt;&gt; tokenizer_local_koala.decode(tokenizer_local_koala.encode('hello',\
          \ add_special_tokens=True))\n'  &lt;s&gt;hello'\n&gt;&gt;&gt;\n&gt;&gt;&gt;\
          \ tokenizer_HF_koala=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
          \ use_fast=False)\n&gt;&gt;&gt; tokenizer_HF_koala.decode(tokenizer_HF_koala.encode('hello',\
          \ add_special_tokens=True))\n' hello'\n&gt;&gt;&gt;\n</code></pre>\n<p>And\
          \ now I get the expected beginning-of-string token from the updated local\
          \ copy.</p>\n<p>I think this means that those two JSON files should be updated\
          \ to match how they are in Vicuna and others. I'm just confused as to why\
          \ the Koala team didn't do that to start with.</p>\n<p>I suppose it could\
          \ be the case that because they add BOS, EOS and UNK in their own code,\
          \ it didn't matter that they hadn't specified it in <code>special_tokens_map.json</code>\
          \ and <code>tokenizer_config.json</code>. But that it is a mistake that\
          \ they didn't..</p>\n<p>As you can tell I'm trying to figure out what all\
          \ this means and how it's meant to work!  But I'm pretty sure Koala should\
          \ have the same values in those two files as the other models, so I'm going\
          \ to update it on HF.</p>\n"
        raw: "OK so I made a new local copy of my `koala-7B-HF`, and I filled out\
          \ `special_tokens_map.json` and `tokenizer_config.json`:\n\n```shell\n$\
          \ cat /Users/tomj/src/huggingface/test-koala-7B-HF/special_tokens_map.json\n\
          {\n  \"bos_token\": {\n    \"content\": \"<s>\",\n    \"lstrip\": false,\n\
          \    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\"\
          : false\n  },\n  \"eos_token\": {\n    \"content\": \"</s>\",\n    \"lstrip\"\
          : false,\n    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\"\
          : false\n  },\n  \"unk_token\": {\n    \"content\": \"<unk>\",\n    \"lstrip\"\
          : false,\n    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\"\
          : false\n  }\n}\n\n$ cat /Users/tomj/src/huggingface/test-koala-7B-HF/tokenizer_config.json\n\
          {\n  \"add_bos_token\": true,\n  \"add_eos_token\": false,\n  \"bos_token\"\
          : {\n    \"__type\": \"AddedToken\",\n    \"content\": \"<s>\",\n    \"\
          lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\": false,\n  \
          \  \"single_word\": false\n  },\n  \"clean_up_tokenization_spaces\": false,\n\
          \  \"eos_token\": {\n    \"__type\": \"AddedToken\",\n    \"content\": \"\
          </s>\",\n    \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\"\
          : false,\n    \"single_word\": false\n  },\n  \"model_max_length\": 1000000000000000019884624838656,\n\
          \  \"pad_token\": null,\n  \"sp_model_kwargs\": {},\n  \"tokenizer_class\"\
          : \"LlamaTokenizer\",\n  \"unk_token\": {\n    \"__type\": \"AddedToken\"\
          ,\n    \"content\": \"<unk>\",\n    \"lstrip\": false,\n    \"normalized\"\
          : true,\n    \"rstrip\": false,\n    \"single_word\": false\n  }\n}\n```\n\
          \nAnd then I re-ran the code I mentioned a moment ago, referencing the local\
          \ model:\n```\n>>> from transformers import AutoTokenizer\n>>> tokenizer_local_koala=AutoTokenizer.from_pretrained('/Users/tomj/src/huggingface/test-koala-7B-HF',\
          \ use_fast=False)\n>>> tokenizer_local_koala.decode(tokenizer_local_koala.encode('hello',\
          \ add_special_tokens=True))\n'  <s>hello'\n>>>\n>>> tokenizer_HF_koala=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
          \ use_fast=False)\n>>> tokenizer_HF_koala.decode(tokenizer_HF_koala.encode('hello',\
          \ add_special_tokens=True))\n' hello'\n>>>\n```\n\nAnd now I get the expected\
          \ beginning-of-string token from the updated local copy.\n\nI think this\
          \ means that those two JSON files should be updated to match how they are\
          \ in Vicuna and others. I'm just confused as to why the Koala team didn't\
          \ do that to start with.\n\nI suppose it could be the case that because\
          \ they add BOS, EOS and UNK in their own code, it didn't matter that they\
          \ hadn't specified it in `special_tokens_map.json` and `tokenizer_config.json`.\
          \ But that it is a mistake that they didn't..\n\nAs you can tell I'm trying\
          \ to figure out what all this means and how it's meant to work!  But I'm\
          \ pretty sure Koala should have the same values in those two files as the\
          \ other models, so I'm going to update it on HF."
        updatedAt: '2023-04-17T10:04:07.187Z'
      numEdits: 2
      reactions: []
    id: 643d18f97ede13bc7b556488
    type: comment
  author: TheBloke
  content: "OK so I made a new local copy of my `koala-7B-HF`, and I filled out `special_tokens_map.json`\
    \ and `tokenizer_config.json`:\n\n```shell\n$ cat /Users/tomj/src/huggingface/test-koala-7B-HF/special_tokens_map.json\n\
    {\n  \"bos_token\": {\n    \"content\": \"<s>\",\n    \"lstrip\": false,\n   \
    \ \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  },\n  \"eos_token\": {\n    \"content\": \"</s>\",\n    \"lstrip\": false,\n\
    \    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  },\n  \"unk_token\": {\n    \"content\": \"<unk>\",\n    \"lstrip\": false,\n\
    \    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  }\n}\n\n$ cat /Users/tomj/src/huggingface/test-koala-7B-HF/tokenizer_config.json\n\
    {\n  \"add_bos_token\": true,\n  \"add_eos_token\": false,\n  \"bos_token\": {\n\
    \    \"__type\": \"AddedToken\",\n    \"content\": \"<s>\",\n    \"lstrip\": false,\n\
    \    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  },\n  \"clean_up_tokenization_spaces\": false,\n  \"eos_token\": {\n    \"\
    __type\": \"AddedToken\",\n    \"content\": \"</s>\",\n    \"lstrip\": false,\n\
    \    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  },\n  \"model_max_length\": 1000000000000000019884624838656,\n  \"pad_token\"\
    : null,\n  \"sp_model_kwargs\": {},\n  \"tokenizer_class\": \"LlamaTokenizer\"\
    ,\n  \"unk_token\": {\n    \"__type\": \"AddedToken\",\n    \"content\": \"<unk>\"\
    ,\n    \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\": false,\n\
    \    \"single_word\": false\n  }\n}\n```\n\nAnd then I re-ran the code I mentioned\
    \ a moment ago, referencing the local model:\n```\n>>> from transformers import\
    \ AutoTokenizer\n>>> tokenizer_local_koala=AutoTokenizer.from_pretrained('/Users/tomj/src/huggingface/test-koala-7B-HF',\
    \ use_fast=False)\n>>> tokenizer_local_koala.decode(tokenizer_local_koala.encode('hello',\
    \ add_special_tokens=True))\n'  <s>hello'\n>>>\n>>> tokenizer_HF_koala=AutoTokenizer.from_pretrained('TheBloke/koala-7B-HF',\
    \ use_fast=False)\n>>> tokenizer_HF_koala.decode(tokenizer_HF_koala.encode('hello',\
    \ add_special_tokens=True))\n' hello'\n>>>\n```\n\nAnd now I get the expected\
    \ beginning-of-string token from the updated local copy.\n\nI think this means\
    \ that those two JSON files should be updated to match how they are in Vicuna\
    \ and others. I'm just confused as to why the Koala team didn't do that to start\
    \ with.\n\nI suppose it could be the case that because they add BOS, EOS and UNK\
    \ in their own code, it didn't matter that they hadn't specified it in `special_tokens_map.json`\
    \ and `tokenizer_config.json`. But that it is a mistake that they didn't..\n\n\
    As you can tell I'm trying to figure out what all this means and how it's meant\
    \ to work!  But I'm pretty sure Koala should have the same values in those two\
    \ files as the other models, so I'm going to update it on HF."
  created_at: 2023-04-17 09:01:29+00:00
  edited: true
  hidden: false
  id: 643d18f97ede13bc7b556488
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-17T10:09:28.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I''ve updated <code>special_tokens_map.json</code> and <code>tokenizer_config.json</code>
          in <code>TheBloke/Koala-7B-HF</code> and <code>TheBloke/Koala-13B-HF</code>
          and the two GPTQ repos.</p>

          '
        raw: OK I've updated `special_tokens_map.json` and `tokenizer_config.json`
          in `TheBloke/Koala-7B-HF` and `TheBloke/Koala-13B-HF` and the two GPTQ repos.
        updatedAt: '2023-04-17T10:12:29.453Z'
      numEdits: 1
      reactions: []
    id: 643d1ad8e3fe9bb0fdef3334
    type: comment
  author: TheBloke
  content: OK I've updated `special_tokens_map.json` and `tokenizer_config.json` in
    `TheBloke/Koala-7B-HF` and `TheBloke/Koala-13B-HF` and the two GPTQ repos.
  created_at: 2023-04-17 09:09:28+00:00
  edited: true
  hidden: false
  id: 643d1ad8e3fe9bb0fdef3334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
      fullname: Yiming Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mattymchen
      type: user
    createdAt: '2023-04-17T10:43:09.000Z'
    data:
      edited: false
      editors:
      - mattymchen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78eaff40c1e15cfebef65b1ebcf73afe.svg
          fullname: Yiming Chen
          isHf: false
          isPro: false
          name: mattymchen
          type: user
        html: '<p>Thank you for the help. It''s working normally now with the bos
          token.</p>

          '
        raw: Thank you for the help. It's working normally now with the bos token.
        updatedAt: '2023-04-17T10:43:09.943Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 643d22bdfdb3d500061e8ba9
    type: comment
  author: mattymchen
  content: Thank you for the help. It's working normally now with the bos token.
  created_at: 2023-04-17 09:43:09+00:00
  edited: false
  hidden: false
  id: 643d22bdfdb3d500061e8ba9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
      fullname: Ankush Singal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andyrasika
      type: user
    createdAt: '2023-07-27T04:06:50.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
          fullname: Ankush Singal
          isHf: false
          isPro: false
          name: Andyrasika
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-27T04:13:00.597Z'
      numEdits: 0
      reactions: []
    id: 64c1ed5a77655fcf3fe813d6
    type: comment
  author: Andyrasika
  content: This comment has been hidden
  created_at: 2023-07-27 03:06:50+00:00
  edited: true
  hidden: true
  id: 64c1ed5a77655fcf3fe813d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/koala-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Gibberish nonsense in GPTQ
