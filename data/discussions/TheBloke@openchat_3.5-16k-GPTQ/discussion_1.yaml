!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mstachow
conflicting_files: null
created_at: 2023-11-14 21:32:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-11-14T21:32:25.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9581031799316406
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>I''m running Windows, Nvidia Geforce 1070 (OLD! But free, and my
          other GPU is busy...). I load the model using the transformers sample code
          and it runs relatively quickly, maybe...a minute or two to generate about
          500 tokens? Not bad for the old computer it''s running on. However, when
          I look at the performance in task manager, it is clear that the GPU is storing
          the model but its utilization is nearly 0%, while the CPU is cranked. Did
          I miss something about how this is supposed to run?</p>

          '
        raw: I'm running Windows, Nvidia Geforce 1070 (OLD! But free, and my other
          GPU is busy...). I load the model using the transformers sample code and
          it runs relatively quickly, maybe...a minute or two to generate about 500
          tokens? Not bad for the old computer it's running on. However, when I look
          at the performance in task manager, it is clear that the GPU is storing
          the model but its utilization is nearly 0%, while the CPU is cranked. Did
          I miss something about how this is supposed to run?
        updatedAt: '2023-11-14T21:32:25.050Z'
      numEdits: 0
      reactions: []
    id: 6553e769d13e8d851dc7b7bd
    type: comment
  author: mstachow
  content: I'm running Windows, Nvidia Geforce 1070 (OLD! But free, and my other GPU
    is busy...). I load the model using the transformers sample code and it runs relatively
    quickly, maybe...a minute or two to generate about 500 tokens? Not bad for the
    old computer it's running on. However, when I look at the performance in task
    manager, it is clear that the GPU is storing the model but its utilization is
    nearly 0%, while the CPU is cranked. Did I miss something about how this is supposed
    to run?
  created_at: 2023-11-14 21:32:25+00:00
  edited: false
  hidden: false
  id: 6553e769d13e8d851dc7b7bd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/openchat_3.5-16k-GPTQ
repo_type: model
status: open
target_branch: null
title: Deeply confused about how this is running on my system - is it GPU or CPU?
