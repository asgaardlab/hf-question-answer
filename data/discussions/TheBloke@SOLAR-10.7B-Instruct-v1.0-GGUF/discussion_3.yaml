!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jdc4429
conflicting_files: null
created_at: 2023-12-19 00:15:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-12-19T00:15:49.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980780839920044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>Hi Tom,</p>

          <p>First off, thanks for all the great work you do.  Many of us appreciate
          it.  I only have an RTX 2070 to work with so I appreciate the quantized
          models as many others I''m sure.</p>

          <p>Secondly, I was wondering if there was a way you could fine tune the
          SOLAR-10.7B-Instruct model so that it has a memory of previous inputs?<br>For
          example, if I prompt: what is the fastest car in the world.  It gives an
          answer about the fastest car.  However if I then ask something like: ''Tell
          me more about this car'' It''s completely oblivious to what it already told
          me.  It has no memory at all which quite frankly makes the model pretty
          useless.</p>

          <p>Thanks again.</p>

          '
        raw: "Hi Tom,\r\n\r\nFirst off, thanks for all the great work you do.  Many\
          \ of us appreciate it.  I only have an RTX 2070 to work with so I appreciate\
          \ the quantized models as many others I'm sure.\r\n\r\nSecondly, I was wondering\
          \ if there was a way you could fine tune the SOLAR-10.7B-Instruct model\
          \ so that it has a memory of previous inputs?\r\nFor example, if I prompt:\
          \ what is the fastest car in the world.  It gives an answer about the fastest\
          \ car.  However if I then ask something like: 'Tell me more about this car'\
          \ It's completely oblivious to what it already told me.  It has no memory\
          \ at all which quite frankly makes the model pretty useless.\r\n\r\nThanks\
          \ again.\r\n"
        updatedAt: '2023-12-19T00:15:49.805Z'
      numEdits: 0
      reactions: []
    id: 6580e0b50d738799ecb627e2
    type: comment
  author: jdc4429
  content: "Hi Tom,\r\n\r\nFirst off, thanks for all the great work you do.  Many\
    \ of us appreciate it.  I only have an RTX 2070 to work with so I appreciate the\
    \ quantized models as many others I'm sure.\r\n\r\nSecondly, I was wondering if\
    \ there was a way you could fine tune the SOLAR-10.7B-Instruct model so that it\
    \ has a memory of previous inputs?\r\nFor example, if I prompt: what is the fastest\
    \ car in the world.  It gives an answer about the fastest car.  However if I then\
    \ ask something like: 'Tell me more about this car' It's completely oblivious\
    \ to what it already told me.  It has no memory at all which quite frankly makes\
    \ the model pretty useless.\r\n\r\nThanks again.\r\n"
  created_at: 2023-12-19 00:15:49+00:00
  edited: false
  hidden: false
  id: 6580e0b50d738799ecb627e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-19T01:19:49.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592096209526062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jdc4429&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jdc4429\">@<span class=\"\
          underline\">jdc4429</span></a></span>\n\n\t</span></span> you have to understand\
          \ llms are stateless so they don\u2019t have stored memory.</p>\n<p>Rather\
          \ you have to input the context like this.</p>\n<p>Let\u2019s say the first\
          \ question is</p>\n<h3 id=\"user\">User:</h3>\n<p>what is the fastest car</p>\n\
          <h3 id=\"assistant\">Assistant:</h3>\n<p>Then you would get the output of\
          \ solar model<br>So</p>\n<h3 id=\"user-1\">User:</h3>\n<p>What is the fastest\
          \ car</p>\n<h3 id=\"assistant-1\">Assistant:</h3>\n<p>The fastest car is\
          \ thrust ssc</p>\n<p>And then you would do</p>\n<h3 id=\"user-2\">User:</h3>\n\
          <p>What is the fastest car</p>\n<h3 id=\"assistant-2\">Assistant:</h3>\n\
          <p>The fastest car is thrust ssc</p>\n<h3 id=\"user-3\">User:</h3>\n<p>Tell\
          \ me more</p>\n<h3 id=\"assistant-3\">Assistant:</h3>\n<p>And you would\
          \ go on and on like that until it reaches the maximum context</p>\n"
        raw: "@jdc4429 you have to understand llms are stateless so they don\u2019\
          t have stored memory.\n\nRather you have to input the context like this.\n\
          \nLet\u2019s say the first question is\n### User:\nwhat is the fastest car\n\
          \n### Assistant:\n\nThen you would get the output of solar model \nSo\n\
          ### User:\nWhat is the fastest car\n\n### Assistant:\nThe fastest car is\
          \ thrust ssc\n\nAnd then you would do\n### User:\nWhat is the fastest car\n\
          \n### Assistant:\nThe fastest car is thrust ssc\n\n### User:\nTell me more\n\
          \n### Assistant:\n\nAnd you would go on and on like that until it reaches\
          \ the maximum context"
        updatedAt: '2023-12-19T01:19:49.260Z'
      numEdits: 0
      reactions: []
    id: 6580efb5f83ed6aa8a128b55
    type: comment
  author: YaTharThShaRma999
  content: "@jdc4429 you have to understand llms are stateless so they don\u2019t\
    \ have stored memory.\n\nRather you have to input the context like this.\n\nLet\u2019\
    s say the first question is\n### User:\nwhat is the fastest car\n\n### Assistant:\n\
    \nThen you would get the output of solar model \nSo\n### User:\nWhat is the fastest\
    \ car\n\n### Assistant:\nThe fastest car is thrust ssc\n\nAnd then you would do\n\
    ### User:\nWhat is the fastest car\n\n### Assistant:\nThe fastest car is thrust\
    \ ssc\n\n### User:\nTell me more\n\n### Assistant:\n\nAnd you would go on and\
    \ on like that until it reaches the maximum context"
  created_at: 2023-12-19 01:19:49+00:00
  edited: false
  hidden: false
  id: 6580efb5f83ed6aa8a128b55
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
      fullname: Jeff
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jdc4429
      type: user
    createdAt: '2023-12-21T22:26:33.000Z'
    data:
      edited: false
      editors:
      - jdc4429
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9384362697601318
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cba6fa9548861511b2a76af037b4e0f.svg
          fullname: Jeff
          isHf: false
          isPro: false
          name: jdc4429
          type: user
        html: '<p>In other models the context from previous questions stays in memory
          until you reset the chat.. this one does not.  Is it possible to change
          this?</p>

          '
        raw: In other models the context from previous questions stays in memory until
          you reset the chat.. this one does not.  Is it possible to change this?
        updatedAt: '2023-12-21T22:26:33.684Z'
      numEdits: 0
      reactions: []
    id: 6584bb99cf1596d002889332
    type: comment
  author: jdc4429
  content: In other models the context from previous questions stays in memory until
    you reset the chat.. this one does not.  Is it possible to change this?
  created_at: 2023-12-21 22:26:33+00:00
  edited: false
  hidden: false
  id: 6584bb99cf1596d002889332
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg
      fullname: Junlin Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jlzhou
      type: user
    createdAt: '2023-12-22T02:05:44.000Z'
    data:
      edited: false
      editors:
      - jlzhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9279143810272217
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg
          fullname: Junlin Zhou
          isHf: false
          isPro: false
          name: jlzhou
          type: user
        html: "<p>Which model has the ability to handle conversation memory itself?\
          \ <span data-props=\"{&quot;user&quot;:&quot;jdc4429&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jdc4429\">@<span class=\"\
          underline\">jdc4429</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'Which model has the ability to handle conversation memory itself? @jdc4429 '
        updatedAt: '2023-12-22T02:05:44.097Z'
      numEdits: 0
      reactions: []
    id: 6584eef85b94130d87cef409
    type: comment
  author: jlzhou
  content: 'Which model has the ability to handle conversation memory itself? @jdc4429 '
  created_at: 2023-12-22 02:05:44+00:00
  edited: false
  hidden: false
  id: 6584eef85b94130d87cef409
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-05T17:22:24.000Z'
    data:
      edited: true
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9367215633392334
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jdc4429&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jdc4429\">@<span class=\"\
          underline\">jdc4429</span></a></span>\n\n\t</span></span> hmm no, thats\
          \ usually just a feature like in text generation web ui. 99% of the models\
          \ including big llm that uses llama, mixtral, mistral, falcon, mpt have\
          \ any memory storage thing, you can only pass previous context like in solar\
          \ 10b. Even chatgpt or gpt4 doesnt have that</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;jlzhou&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/jlzhou\">@<span class=\"underline\">jlzhou</span></a></span>\n\
          \n\t</span></span> there is blenderbot model but its a very early model\
          \ from meta and its horrible compared to even tinyllama 1.1b . Blenderbot\
          \ only has a very weak chatting ability and basically 0 knowledge abt anything\
          \ else. It does have the ability to store memory without passing previous\
          \ context tho.</p>\n"
        raw: '@jdc4429 hmm no, thats usually just a feature like in text generation
          web ui. 99% of the models including big llm that uses llama, mixtral, mistral,
          falcon, mpt have any memory storage thing, you can only pass previous context
          like in solar 10b. Even chatgpt or gpt4 doesnt have that


          @jlzhou there is blenderbot model but its a very early model from meta and
          its horrible compared to even tinyllama 1.1b . Blenderbot only has a very
          weak chatting ability and basically 0 knowledge abt anything else. It does
          have the ability to store memory without passing previous context tho.'
        updatedAt: '2024-01-05T17:24:06.329Z'
      numEdits: 2
      reactions: []
    id: 65983ad0a3259bc4176cec1b
    type: comment
  author: YaTharThShaRma999
  content: '@jdc4429 hmm no, thats usually just a feature like in text generation
    web ui. 99% of the models including big llm that uses llama, mixtral, mistral,
    falcon, mpt have any memory storage thing, you can only pass previous context
    like in solar 10b. Even chatgpt or gpt4 doesnt have that


    @jlzhou there is blenderbot model but its a very early model from meta and its
    horrible compared to even tinyllama 1.1b . Blenderbot only has a very weak chatting
    ability and basically 0 knowledge abt anything else. It does have the ability
    to store memory without passing previous context tho.'
  created_at: 2024-01-05 17:22:24+00:00
  edited: true
  hidden: false
  id: 65983ad0a3259bc4176cec1b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF
repo_type: model
status: open
target_branch: null
title: No memory within model?
