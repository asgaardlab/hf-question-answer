!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ttkrpink
conflicting_files: null
created_at: 2023-12-18 08:37:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a79343fd5a90a2223d18e29470a0dec5.svg
      fullname: You Chao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ttkrpink
      type: user
    createdAt: '2023-12-18T08:37:32.000Z'
    data:
      edited: false
      editors:
      - ttkrpink
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6585419774055481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a79343fd5a90a2223d18e29470a0dec5.svg
          fullname: You Chao
          isHf: false
          isPro: false
          name: ttkrpink
          type: user
        html: "<p>python 3.10,<br>llama-cpp-python tried 0.2.23, 0.2.20, 0.1.76, 0.1.48.<br>All\
          \ have the same error message as following:</p>\n<pre><code>from llama_cpp\
          \ import Llama\n\nModel_Path = \"../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\
          \n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if\
          \ no GPU acceleration is available on your system.\nllm = Llama(\n    model_path=Model_Path,\
          \  # Download the model file first\n    n_ctx=512,  # The max sequence length\
          \ to use - note that longer sequence lengths require much more resources\n\
          \    n_threads=1,            # The number of CPU threads to use, tailor\
          \ to your system and the resulting performance\n    n_gpu_layers=35    \
          \     # The number of layers to offload to GPU, if you have GPU acceleration\
          \ available\n    )\n</code></pre>\n<pre><code>llama.cpp: loading model from\
          \ ../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\nerror\
          \ loading model: unknown (magic, version) combination: 46554747, 00000003;\
          \ is this really a GGML file?\nllama_load_model_from_file: failed to load\
          \ model\n---------------------------------------------------------------------------\n\
          AssertionError                            Traceback (most recent call last)\n\
          Cell In[45], line 6\n      3 Model_Path = \"../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\
          \n      5 # Set gpu_layers to the number of layers to offload to GPU. Set\
          \ to 0 if no GPU acceleration is available on your system.\n----&gt; 6 llm\
          \ = Llama(\n      7     model_path=Model_Path,  # Download the model file\
          \ first\n      8     n_ctx=512,  # The max sequence length to use - note\
          \ that longer sequence lengths require much more resources\n      9    \
          \ n_threads=1,            # The number of CPU threads to use, tailor to\
          \ your system and the resulting performance\n     10     n_gpu_layers=35\
          \         # The number of layers to offload to GPU, if you have GPU acceleration\
          \ available\n     11     )\n\nFile ~/anaconda3/envs/health-report-llm-local-deploy/lib/python3.10/site-packages/llama_cpp/llama.py:305,\
          \ in Llama.__init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed,\
          \ f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads,\
          \ n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split,\
          \ rope_freq_base, rope_freq_scale, verbose)\n    300     raise ValueError(f\"\
          Model path does not exist: {model_path}\")\n    302 self.model = llama_cpp.llama_load_model_from_file(\n\
          \    303     self.model_path.encode(\"utf-8\"), self.params\n    304 )\n\
          --&gt; 305 assert self.model is not None\n    307 self.ctx = llama_cpp.llama_new_context_with_model(self.model,\
          \ self.params)\n    309 assert self.ctx is not None\n\nAssertionError: \n\
          </code></pre>\n"
        raw: "python 3.10,\r\nllama-cpp-python tried 0.2.23, 0.2.20, 0.1.76, 0.1.48.\r\
          \nAll have the same error message as following:\r\n```\r\nfrom llama_cpp\
          \ import Llama\r\n\r\nModel_Path = \"../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\
          \r\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0\
          \ if no GPU acceleration is available on your system.\r\nllm = Llama(\r\n\
          \    model_path=Model_Path,  # Download the model file first\r\n    n_ctx=512,\
          \  # The max sequence length to use - note that longer sequence lengths\
          \ require much more resources\r\n    n_threads=1,            # The number\
          \ of CPU threads to use, tailor to your system and the resulting performance\r\
          \n    n_gpu_layers=35         # The number of layers to offload to GPU,\
          \ if you have GPU acceleration available\r\n    )\r\n```\r\n\r\n\r\n```\r\
          \nllama.cpp: loading model from ../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\r\
          \nerror loading model: unknown (magic, version) combination: 46554747, 00000003;\
          \ is this really a GGML file?\r\nllama_load_model_from_file: failed to load\
          \ model\r\n---------------------------------------------------------------------------\r\
          \nAssertionError                            Traceback (most recent call\
          \ last)\r\nCell In[45], line 6\r\n      3 Model_Path = \"../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\
          \r\n      5 # Set gpu_layers to the number of layers to offload to GPU.\
          \ Set to 0 if no GPU acceleration is available on your system.\r\n---->\
          \ 6 llm = Llama(\r\n      7     model_path=Model_Path,  # Download the model\
          \ file first\r\n      8     n_ctx=512,  # The max sequence length to use\
          \ - note that longer sequence lengths require much more resources\r\n  \
          \    9     n_threads=1,            # The number of CPU threads to use, tailor\
          \ to your system and the resulting performance\r\n     10     n_gpu_layers=35\
          \         # The number of layers to offload to GPU, if you have GPU acceleration\
          \ available\r\n     11     )\r\n\r\nFile ~/anaconda3/envs/health-report-llm-local-deploy/lib/python3.10/site-packages/llama_cpp/llama.py:305,\
          \ in Llama.__init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed,\
          \ f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads,\
          \ n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split,\
          \ rope_freq_base, rope_freq_scale, verbose)\r\n    300     raise ValueError(f\"\
          Model path does not exist: {model_path}\")\r\n    302 self.model = llama_cpp.llama_load_model_from_file(\r\
          \n    303     self.model_path.encode(\"utf-8\"), self.params\r\n    304\
          \ )\r\n--> 305 assert self.model is not None\r\n    307 self.ctx = llama_cpp.llama_new_context_with_model(self.model,\
          \ self.params)\r\n    309 assert self.ctx is not None\r\n\r\nAssertionError:\
          \ \r\n```"
        updatedAt: '2023-12-18T08:37:32.902Z'
      numEdits: 0
      reactions: []
    id: 658004cc69cfe9b4cd7bfab7
    type: comment
  author: ttkrpink
  content: "python 3.10,\r\nllama-cpp-python tried 0.2.23, 0.2.20, 0.1.76, 0.1.48.\r\
    \nAll have the same error message as following:\r\n```\r\nfrom llama_cpp import\
    \ Llama\r\n\r\nModel_Path = \"../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\
    \r\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no\
    \ GPU acceleration is available on your system.\r\nllm = Llama(\r\n    model_path=Model_Path,\
    \  # Download the model file first\r\n    n_ctx=512,  # The max sequence length\
    \ to use - note that longer sequence lengths require much more resources\r\n \
    \   n_threads=1,            # The number of CPU threads to use, tailor to your\
    \ system and the resulting performance\r\n    n_gpu_layers=35         # The number\
    \ of layers to offload to GPU, if you have GPU acceleration available\r\n    )\r\
    \n```\r\n\r\n\r\n```\r\nllama.cpp: loading model from ../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\r\
    \nerror loading model: unknown (magic, version) combination: 46554747, 00000003;\
    \ is this really a GGML file?\r\nllama_load_model_from_file: failed to load model\r\
    \n---------------------------------------------------------------------------\r\
    \nAssertionError                            Traceback (most recent call last)\r\
    \nCell In[45], line 6\r\n      3 Model_Path = \"../../HuggingFace/Solar_10.7B/solar-10.7b-instruct-v1.0.Q4_0.gguf\"\
    \r\n      5 # Set gpu_layers to the number of layers to offload to GPU. Set to\
    \ 0 if no GPU acceleration is available on your system.\r\n----> 6 llm = Llama(\r\
    \n      7     model_path=Model_Path,  # Download the model file first\r\n    \
    \  8     n_ctx=512,  # The max sequence length to use - note that longer sequence\
    \ lengths require much more resources\r\n      9     n_threads=1,            #\
    \ The number of CPU threads to use, tailor to your system and the resulting performance\r\
    \n     10     n_gpu_layers=35         # The number of layers to offload to GPU,\
    \ if you have GPU acceleration available\r\n     11     )\r\n\r\nFile ~/anaconda3/envs/health-report-llm-local-deploy/lib/python3.10/site-packages/llama_cpp/llama.py:305,\
    \ in Llama.__init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv,\
    \ logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch,\
    \ last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base,\
    \ rope_freq_scale, verbose)\r\n    300     raise ValueError(f\"Model path does\
    \ not exist: {model_path}\")\r\n    302 self.model = llama_cpp.llama_load_model_from_file(\r\
    \n    303     self.model_path.encode(\"utf-8\"), self.params\r\n    304 )\r\n\
    --> 305 assert self.model is not None\r\n    307 self.ctx = llama_cpp.llama_new_context_with_model(self.model,\
    \ self.params)\r\n    309 assert self.ctx is not None\r\n\r\nAssertionError: \r\
    \n```"
  created_at: 2023-12-18 08:37:32+00:00
  edited: false
  hidden: false
  id: 658004cc69cfe9b4cd7bfab7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF
repo_type: model
status: open
target_branch: null
title: 'AssertionError:  assert self.model is not None'
