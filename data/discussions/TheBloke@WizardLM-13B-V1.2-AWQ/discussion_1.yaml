!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dinchu
conflicting_files: null
created_at: 2023-10-05 06:44:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd8734091e391d2d85102be9b7866108.svg
      fullname: pablo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dinchu
      type: user
    createdAt: '2023-10-05T07:44:11.000Z'
    data:
      edited: true
      editors:
      - dinchu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9496756196022034
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd8734091e391d2d85102be9b7866108.svg
          fullname: pablo
          isHf: false
          isPro: false
          name: dinchu
          type: user
        html: '<p>I have been working with the original WizardLM-13B-V1.2 i tried
          to switch to this one for performance but the responses it produces are
          very bad quality (even using the exact same prompts), the percentage of
          useless responses is too high with this version, quantization went wrong
          i guess.<br>i am using fastchat and vllm to load 2 models in parallel, might
          also be related to any of these facts too so i will keep testing</p>

          '
        raw: 'I have been working with the original WizardLM-13B-V1.2 i tried to switch
          to this one for performance but the responses it produces are very bad quality
          (even using the exact same prompts), the percentage of useless responses
          is too high with this version, quantization went wrong i guess.

          i am using fastchat and vllm to load 2 models in parallel, might also be
          related to any of these facts too so i will keep testing'
        updatedAt: '2023-10-05T08:19:53.207Z'
      numEdits: 1
      reactions: []
    id: 651e694b833d83e1290f1536
    type: comment
  author: dinchu
  content: 'I have been working with the original WizardLM-13B-V1.2 i tried to switch
    to this one for performance but the responses it produces are very bad quality
    (even using the exact same prompts), the percentage of useless responses is too
    high with this version, quantization went wrong i guess.

    i am using fastchat and vllm to load 2 models in parallel, might also be related
    to any of these facts too so i will keep testing'
  created_at: 2023-10-05 06:44:11+00:00
  edited: true
  hidden: false
  id: 651e694b833d83e1290f1536
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/fd8734091e391d2d85102be9b7866108.svg
      fullname: pablo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dinchu
      type: user
    createdAt: '2023-10-05T08:19:12.000Z'
    data:
      from: This model is a bit useless
      to: Testing model
    id: 651e7180073d85a99d1a40f9
    type: title-change
  author: dinchu
  created_at: 2023-10-05 07:19:12+00:00
  id: 651e7180073d85a99d1a40f9
  new_title: Testing model
  old_title: This model is a bit useless
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-05T11:53:16.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9292082786560059
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>Hmm the problem might be that you are using the wrong prompt template?<br>It
          needs the wizardlm prompt template.</p>

          <p>Without it, it would perform pretty badly.</p>

          '
        raw: 'Hmm the problem might be that you are using the wrong prompt template?

          It needs the wizardlm prompt template.


          Without it, it would perform pretty badly.'
        updatedAt: '2023-10-05T11:53:16.209Z'
      numEdits: 0
      reactions: []
    id: 651ea3acc58c96b3851499a3
    type: comment
  author: YaTharThShaRma999
  content: 'Hmm the problem might be that you are using the wrong prompt template?

    It needs the wizardlm prompt template.


    Without it, it would perform pretty badly.'
  created_at: 2023-10-05 10:53:16+00:00
  edited: false
  hidden: false
  id: 651ea3acc58c96b3851499a3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-13B-V1.2-AWQ
repo_type: model
status: open
target_branch: null
title: Testing model
