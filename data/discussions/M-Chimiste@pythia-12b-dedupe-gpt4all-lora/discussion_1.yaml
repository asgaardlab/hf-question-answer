!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cekal
conflicting_files: null
created_at: 2023-04-13 16:15:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-04-13T17:15:53.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: '<p>Hi, I was wondering how can I run this in my Google Colab notebook?
          I''d like to see some of the outputs. If you have a script for generation
          please let me know, as I am getting size mismatch errors when I try to load
          the adapter. Thanks!</p>

          '
        raw: Hi, I was wondering how can I run this in my Google Colab notebook? I'd
          like to see some of the outputs. If you have a script for generation please
          let me know, as I am getting size mismatch errors when I try to load the
          adapter. Thanks!
        updatedAt: '2023-04-13T17:15:53.891Z'
      numEdits: 0
      reactions: []
    id: 643838c996565111c8e73498
    type: comment
  author: cekal
  content: Hi, I was wondering how can I run this in my Google Colab notebook? I'd
    like to see some of the outputs. If you have a script for generation please let
    me know, as I am getting size mismatch errors when I try to load the adapter.
    Thanks!
  created_at: 2023-04-13 16:15:53+00:00
  edited: false
  hidden: false
  id: 643838c996565111c8e73498
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
      fullname: Christian Merrill
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: M-Chimiste
      type: user
    createdAt: '2023-04-13T18:26:15.000Z'
    data:
      edited: false
      editors:
      - M-Chimiste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
          fullname: Christian Merrill
          isHf: false
          isPro: false
          name: M-Chimiste
          type: user
        html: '<p>Let me go back and look, I may have uploaded the wrong bloody thing....
          I was in a hurry to preserve it off my HDD due to a computer upgrade/reformat.  Worst
          case I have merged weights I can put here instead from my GDrive.</p>

          '
        raw: Let me go back and look, I may have uploaded the wrong bloody thing....
          I was in a hurry to preserve it off my HDD due to a computer upgrade/reformat.  Worst
          case I have merged weights I can put here instead from my GDrive.
        updatedAt: '2023-04-13T18:26:15.533Z'
      numEdits: 0
      reactions: []
    id: 643849479875b1b9701457df
    type: comment
  author: M-Chimiste
  content: Let me go back and look, I may have uploaded the wrong bloody thing....
    I was in a hurry to preserve it off my HDD due to a computer upgrade/reformat.  Worst
    case I have merged weights I can put here instead from my GDrive.
  created_at: 2023-04-13 17:26:15+00:00
  edited: false
  hidden: false
  id: 643849479875b1b9701457df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-04-13T18:28:45.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: '<p>Alright, let me know! I think the mistake is on my side, as I am
          a beginner in this machine learning stuff and my python skills aren''t the
          best either.</p>

          '
        raw: Alright, let me know! I think the mistake is on my side, as I am a beginner
          in this machine learning stuff and my python skills aren't the best either.
        updatedAt: '2023-04-13T18:28:45.934Z'
      numEdits: 0
      reactions: []
    id: 643849dd84cfa20fab7e9a2a
    type: comment
  author: cekal
  content: Alright, let me know! I think the mistake is on my side, as I am a beginner
    in this machine learning stuff and my python skills aren't the best either.
  created_at: 2023-04-13 17:28:45+00:00
  edited: false
  hidden: false
  id: 643849dd84cfa20fab7e9a2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
      fullname: Christian Merrill
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: M-Chimiste
      type: user
    createdAt: '2023-04-16T14:27:22.000Z'
    data:
      edited: false
      editors:
      - M-Chimiste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
          fullname: Christian Merrill
          isHf: false
          isPro: false
          name: M-Chimiste
          type: user
        html: "<p>Here is how the initial adapter was initialized for training:</p>\n\
          <pre><code>BASE_MODEL = \"EleutherAI/pythia-12b-deduped\"\nDATA_PATH = \"\
          /mnt/e/software-projects/cortex/data/100k_samples.json\"\nOUTPUT_DIR = \"\
          ./gpt-for-all-neox\"\n# training hyperparams\nBATCH_SIZE = 128\nMICRO_BATCH_SIZE\
          \ = 1\nEPOCHS = 1\nLEARNING_RATE = 3e-4\nCUTOFF_LEN = 512\nVAL_SET_SIZE\
          \ = 2000\n\n\n# lora hyperparams\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT\
          \ = 0.05\n\n# either \"query_key_value\" or \"q_proj\", \"v_proj\" based\
          \ on model\nLORA_TARGET_MODULES = [\n    \"query_key_value\",\n]\n# llm\
          \ hyperparams\nTRAIN_ON_INPUTS = True  # if False, masks out inputs in loss\n\
          GROUP_BY_LENGTH = False  # faster, but produces an odd training loss curve\n\
          DEVICE_MAP = \"auto\"\nWORLD_SIZE =  int(os.environ.get(\"WORLD_SIZE\",\
          \ 1))\nDDP = WORLD_SIZE != 1\n\ngradient_accumulation_steps = BATCH_SIZE\
          \ // MICRO_BATCH_SIZE\n\nif DDP:\n    DEVICE_MAP = {\"\": int(os.environ.get(\"\
          LOCAL_RANK\") or 0)}\n    gradient_accumulation_steps = gradient_accumulation_steps\
          \ // WORLD_SIZE\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n   \
          \     BASE_MODEL,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n\
          \        device_map=DEVICE_MAP,\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\
          \nmodel = prepare_model_for_int8_training(model)\n\nconfig = LoraConfig(\n\
          \    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=LORA_TARGET_MODULES,\n\
          \    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\
          )\n\nmodel = get_peft_model(model, config)\n</code></pre>\n<p>Here is some\
          \ really basic inference code that should allow you to use that adaptor:</p>\n\
          <pre><code>import torch\nfrom peft import PeftModel\n\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-12b-deduped\")\n\n\
          model = AutoModelForCausalLM.from_pretrained(\n    \"EleutherAI/pythia-12b-deduped\"\
          ,\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"\
          auto\",\n)\n\nmodel = PeftModel.from_pretrained(\n    model, \"M-Chimiste/pythia-12b-dedupe-gpt4all-lora\"\
          , torch_dtype=torch.float16\n)\n\n\ndef generate_prompt(instruction, input=None):\n\
          \    return f\"\"\"Below is an instruction that describes a task. Write\
          \ a response that appropriately completes the request.\n\n### Instruction:\n\
          {instruction}\n\n### Response:\"\"\"\n\n\nmodel.eval()\n\n\ndef evaluate(\n\
          \        instruction,\n        temperature=0.1,\n        top_p=0.75,\n \
          \       top_k=40,\n        num_beams=4,\n        **kwargs,\n):\n    prompt\
          \ = generate_prompt(instruction, input)\n    inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].cuda()\n\
          \    generation_config = GenerationConfig(\n        temperature=temperature,\n\
          \        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n\
          \        **kwargs,\n    )\n    with torch.no_grad():\n        generation_output\
          \ = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n\
          \            return_dict_in_generate=True,\n            output_scores=True,\n\
          \            max_new_tokens=200,\n            \n        )\n    s = generation_output.sequences[0]\n\
          \    output = tokenizer.decode(s)\n    return output.split(\"### Response:\"\
          )[1].strip()\n\n\nwhile True:\n    prompt = input(\"Prompt:\")\n    output\
          \ = evaluate(prompt)\n    print(output)\n</code></pre>\n<p>If this all still\
          \ blows up for you, I put the merged weights here: <a href=\"https://huggingface.co/M-Chimiste/Pythia-12b-instruction-tuned-v1\"\
          >https://huggingface.co/M-Chimiste/Pythia-12b-instruction-tuned-v1</a>.\
          \  You could still use the above inference code, but would want to remove\
          \ the PEFT components.</p>\n"
        raw: "Here is how the initial adapter was initialized for training:\n\n```\n\
          BASE_MODEL = \"EleutherAI/pythia-12b-deduped\"\nDATA_PATH = \"/mnt/e/software-projects/cortex/data/100k_samples.json\"\
          \nOUTPUT_DIR = \"./gpt-for-all-neox\"\n# training hyperparams\nBATCH_SIZE\
          \ = 128\nMICRO_BATCH_SIZE = 1\nEPOCHS = 1\nLEARNING_RATE = 3e-4\nCUTOFF_LEN\
          \ = 512\nVAL_SET_SIZE = 2000\n\n\n# lora hyperparams\nLORA_R = 8\nLORA_ALPHA\
          \ = 16\nLORA_DROPOUT = 0.05\n\n# either \"query_key_value\" or \"q_proj\"\
          , \"v_proj\" based on model\nLORA_TARGET_MODULES = [\n    \"query_key_value\"\
          ,\n]\n# llm hyperparams\nTRAIN_ON_INPUTS = True  # if False, masks out inputs\
          \ in loss\nGROUP_BY_LENGTH = False  # faster, but produces an odd training\
          \ loss curve\nDEVICE_MAP = \"auto\"\nWORLD_SIZE =  int(os.environ.get(\"\
          WORLD_SIZE\", 1))\nDDP = WORLD_SIZE != 1\n\ngradient_accumulation_steps\
          \ = BATCH_SIZE // MICRO_BATCH_SIZE\n\nif DDP:\n    DEVICE_MAP = {\"\": int(os.environ.get(\"\
          LOCAL_RANK\") or 0)}\n    gradient_accumulation_steps = gradient_accumulation_steps\
          \ // WORLD_SIZE\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n   \
          \     BASE_MODEL,\n        load_in_8bit=True,\n        torch_dtype=torch.float16,\n\
          \        device_map=DEVICE_MAP,\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\
          \nmodel = prepare_model_for_int8_training(model)\n\nconfig = LoraConfig(\n\
          \    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=LORA_TARGET_MODULES,\n\
          \    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\
          )\n\nmodel = get_peft_model(model, config)\n```\nHere is some really basic\
          \ inference code that should allow you to use that adaptor:\n\n```\nimport\
          \ torch\nfrom peft import PeftModel\n\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          EleutherAI/pythia-12b-deduped\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"EleutherAI/pythia-12b-deduped\",\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n\
          \    device_map=\"auto\",\n)\n\nmodel = PeftModel.from_pretrained(\n   \
          \ model, \"M-Chimiste/pythia-12b-dedupe-gpt4all-lora\", torch_dtype=torch.float16\n\
          )\n\n\ndef generate_prompt(instruction, input=None):\n    return f\"\"\"\
          Below is an instruction that describes a task. Write a response that appropriately\
          \ completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\"\
          \"\"\n\n\nmodel.eval()\n\n\ndef evaluate(\n        instruction,\n      \
          \  temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n\
          \        **kwargs,\n):\n    prompt = generate_prompt(instruction, input)\n\
          \    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids =\
          \ inputs[\"input_ids\"].cuda()\n    generation_config = GenerationConfig(\n\
          \        temperature=temperature,\n        top_p=top_p,\n        top_k=top_k,\n\
          \        num_beams=num_beams,\n        **kwargs,\n    )\n    with torch.no_grad():\n\
          \        generation_output = model.generate(\n            input_ids=input_ids,\n\
          \            generation_config=generation_config,\n            return_dict_in_generate=True,\n\
          \            output_scores=True,\n            max_new_tokens=200,\n    \
          \        \n        )\n    s = generation_output.sequences[0]\n    output\
          \ = tokenizer.decode(s)\n    return output.split(\"### Response:\")[1].strip()\n\
          \n\nwhile True:\n    prompt = input(\"Prompt:\")\n    output = evaluate(prompt)\n\
          \    print(output)\n```\n\nIf this all still blows up for you, I put the\
          \ merged weights here: https://huggingface.co/M-Chimiste/Pythia-12b-instruction-tuned-v1.\
          \  You could still use the above inference code, but would want to remove\
          \ the PEFT components."
        updatedAt: '2023-04-16T14:27:22.408Z'
      numEdits: 0
      reactions: []
    id: 643c05cae3a7bbe2cf3bba72
    type: comment
  author: M-Chimiste
  content: "Here is how the initial adapter was initialized for training:\n\n```\n\
    BASE_MODEL = \"EleutherAI/pythia-12b-deduped\"\nDATA_PATH = \"/mnt/e/software-projects/cortex/data/100k_samples.json\"\
    \nOUTPUT_DIR = \"./gpt-for-all-neox\"\n# training hyperparams\nBATCH_SIZE = 128\n\
    MICRO_BATCH_SIZE = 1\nEPOCHS = 1\nLEARNING_RATE = 3e-4\nCUTOFF_LEN = 512\nVAL_SET_SIZE\
    \ = 2000\n\n\n# lora hyperparams\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT =\
    \ 0.05\n\n# either \"query_key_value\" or \"q_proj\", \"v_proj\" based on model\n\
    LORA_TARGET_MODULES = [\n    \"query_key_value\",\n]\n# llm hyperparams\nTRAIN_ON_INPUTS\
    \ = True  # if False, masks out inputs in loss\nGROUP_BY_LENGTH = False  # faster,\
    \ but produces an odd training loss curve\nDEVICE_MAP = \"auto\"\nWORLD_SIZE =\
    \  int(os.environ.get(\"WORLD_SIZE\", 1))\nDDP = WORLD_SIZE != 1\n\ngradient_accumulation_steps\
    \ = BATCH_SIZE // MICRO_BATCH_SIZE\n\nif DDP:\n    DEVICE_MAP = {\"\": int(os.environ.get(\"\
    LOCAL_RANK\") or 0)}\n    gradient_accumulation_steps = gradient_accumulation_steps\
    \ // WORLD_SIZE\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL,\n\
    \        load_in_8bit=True,\n        torch_dtype=torch.float16,\n        device_map=DEVICE_MAP,\n\
    \    )\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nmodel = prepare_model_for_int8_training(model)\n\
    \nconfig = LoraConfig(\n    r=LORA_R,\n    lora_alpha=LORA_ALPHA,\n    target_modules=LORA_TARGET_MODULES,\n\
    \    lora_dropout=LORA_DROPOUT,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\
    )\n\nmodel = get_peft_model(model, config)\n```\nHere is some really basic inference\
    \ code that should allow you to use that adaptor:\n\n```\nimport torch\nfrom peft\
    \ import PeftModel\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
    \ GenerationConfig\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-12b-deduped\"\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"EleutherAI/pythia-12b-deduped\"\
    ,\n    load_in_8bit=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\
    ,\n)\n\nmodel = PeftModel.from_pretrained(\n    model, \"M-Chimiste/pythia-12b-dedupe-gpt4all-lora\"\
    , torch_dtype=torch.float16\n)\n\n\ndef generate_prompt(instruction, input=None):\n\
    \    return f\"\"\"Below is an instruction that describes a task. Write a response\
    \ that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\
    \n### Response:\"\"\"\n\n\nmodel.eval()\n\n\ndef evaluate(\n        instruction,\n\
    \        temperature=0.1,\n        top_p=0.75,\n        top_k=40,\n        num_beams=4,\n\
    \        **kwargs,\n):\n    prompt = generate_prompt(instruction, input)\n   \
    \ inputs = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = inputs[\"\
    input_ids\"].cuda()\n    generation_config = GenerationConfig(\n        temperature=temperature,\n\
    \        top_p=top_p,\n        top_k=top_k,\n        num_beams=num_beams,\n  \
    \      **kwargs,\n    )\n    with torch.no_grad():\n        generation_output\
    \ = model.generate(\n            input_ids=input_ids,\n            generation_config=generation_config,\n\
    \            return_dict_in_generate=True,\n            output_scores=True,\n\
    \            max_new_tokens=200,\n            \n        )\n    s = generation_output.sequences[0]\n\
    \    output = tokenizer.decode(s)\n    return output.split(\"### Response:\")[1].strip()\n\
    \n\nwhile True:\n    prompt = input(\"Prompt:\")\n    output = evaluate(prompt)\n\
    \    print(output)\n```\n\nIf this all still blows up for you, I put the merged\
    \ weights here: https://huggingface.co/M-Chimiste/Pythia-12b-instruction-tuned-v1.\
    \  You could still use the above inference code, but would want to remove the\
    \ PEFT components."
  created_at: 2023-04-16 13:27:22+00:00
  edited: false
  hidden: false
  id: 643c05cae3a7bbe2cf3bba72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
      fullname: Christian Merrill
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: M-Chimiste
      type: user
    createdAt: '2023-04-16T14:29:46.000Z'
    data:
      edited: false
      editors:
      - M-Chimiste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
          fullname: Christian Merrill
          isHf: false
          isPro: false
          name: M-Chimiste
          type: user
        html: '<p>Also important to note, when I was playing around with it, it''s
          obvious that it needs more training.  There are a bunch of new instruction
          sets that are being released under very permissible licenses which I''m
          currently compiling and will be training a new model on later this year
          when I go on vacation (my ML rig is also my gaming rig, so I don''t like
          to be entirely unable to do some gaming, etc for weeks at a time).</p>

          '
        raw: Also important to note, when I was playing around with it, it's obvious
          that it needs more training.  There are a bunch of new instruction sets
          that are being released under very permissible licenses which I'm currently
          compiling and will be training a new model on later this year when I go
          on vacation (my ML rig is also my gaming rig, so I don't like to be entirely
          unable to do some gaming, etc for weeks at a time).
        updatedAt: '2023-04-16T14:29:46.428Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - cekal
    id: 643c065a21686867003f093c
    type: comment
  author: M-Chimiste
  content: Also important to note, when I was playing around with it, it's obvious
    that it needs more training.  There are a bunch of new instruction sets that are
    being released under very permissible licenses which I'm currently compiling and
    will be training a new model on later this year when I go on vacation (my ML rig
    is also my gaming rig, so I don't like to be entirely unable to do some gaming,
    etc for weeks at a time).
  created_at: 2023-04-16 13:29:46+00:00
  edited: false
  hidden: false
  id: 643c065a21686867003f093c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
      fullname: Vojtech Cekal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: cekal
      type: user
    createdAt: '2023-04-17T00:17:22.000Z'
    data:
      edited: false
      editors:
      - cekal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ebfd71ca08a72ba9ce6fe0/WEXOVko_Lgvq_Y8_Zlb4o.png?w=200&h=200&f=face
          fullname: Vojtech Cekal
          isHf: false
          isPro: true
          name: cekal
          type: user
        html: '<p>Hi, thank you so much for the script. I trained LLaMA 30B on my
          own dataset (succeeded gpt-3.5-turbo) but now since I want to use it commercially
          and develop a company out of this technology, I will have to choose a different
          base model. Chose GPT NeoX after a few experiments, looks promising. I got
          this crazy idea where I''d like to combine this thing with a pair of glasses.
          Like, when someone asks you a question, the glasses will display the answer.
          So, you could be like a "superhuman" that knows everything, basically like
          if ChatGPT was a person. Answer to everything before your eyes, now that''s
          good I''d say. But before that, I''ll have to use this technology differently,
          as developing this tech would cost a lot so I need to make some money first.
          Also thought about some of the companies that invest in startups but all
          I currently have is just a LLM</p>

          <p>Anyway, thank you again for the script, I really appreciate it!</p>

          '
        raw: 'Hi, thank you so much for the script. I trained LLaMA 30B on my own
          dataset (succeeded gpt-3.5-turbo) but now since I want to use it commercially
          and develop a company out of this technology, I will have to choose a different
          base model. Chose GPT NeoX after a few experiments, looks promising. I got
          this crazy idea where I''d like to combine this thing with a pair of glasses.
          Like, when someone asks you a question, the glasses will display the answer.
          So, you could be like a "superhuman" that knows everything, basically like
          if ChatGPT was a person. Answer to everything before your eyes, now that''s
          good I''d say. But before that, I''ll have to use this technology differently,
          as developing this tech would cost a lot so I need to make some money first.
          Also thought about some of the companies that invest in startups but all
          I currently have is just a LLM


          Anyway, thank you again for the script, I really appreciate it!'
        updatedAt: '2023-04-17T00:17:22.290Z'
      numEdits: 0
      reactions: []
    id: 643c90122eeda4c05156fc31
    type: comment
  author: cekal
  content: 'Hi, thank you so much for the script. I trained LLaMA 30B on my own dataset
    (succeeded gpt-3.5-turbo) but now since I want to use it commercially and develop
    a company out of this technology, I will have to choose a different base model.
    Chose GPT NeoX after a few experiments, looks promising. I got this crazy idea
    where I''d like to combine this thing with a pair of glasses. Like, when someone
    asks you a question, the glasses will display the answer. So, you could be like
    a "superhuman" that knows everything, basically like if ChatGPT was a person.
    Answer to everything before your eyes, now that''s good I''d say. But before that,
    I''ll have to use this technology differently, as developing this tech would cost
    a lot so I need to make some money first. Also thought about some of the companies
    that invest in startups but all I currently have is just a LLM


    Anyway, thank you again for the script, I really appreciate it!'
  created_at: 2023-04-16 23:17:22+00:00
  edited: false
  hidden: false
  id: 643c90122eeda4c05156fc31
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
      fullname: Christian Merrill
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: M-Chimiste
      type: user
    createdAt: '2023-04-17T00:33:07.000Z'
    data:
      edited: false
      editors:
      - M-Chimiste
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
          fullname: Christian Merrill
          isHf: false
          isPro: false
          name: M-Chimiste
          type: user
        html: "<p>Pythia Deduped is probably the best foundation model you can get\
          \ right now with a permissible license or perhaps flan UL2. I have some\
          \ code sitting on my GitHub repo (same username as here) which should have\
          \ full scripts if you want to try them out. Just a heads up I don\u2019\
          t think Pythia will beat LLaMA as LLaMA was trained on a much bigger corpus\
          \ than Pythia. If you\u2019re looking to do speech to text look up Whisper,\
          \ it should provide pretty well and I believe it\u2019s got a permissible\
          \ license. I know that there is a team trying to train a permissible LLaMA\
          \ model, but it might be a while before that shows up, but I seriously hope\
          \ I\u2019m wrong. </p>\n<p>I\u2019m planning to mess around with UL 2 soon\
          \ to see if it can outperform Pythia.</p>\n"
        raw: "Pythia Deduped is probably the best foundation model you can get right\
          \ now with a permissible license or perhaps flan UL2. I have some code sitting\
          \ on my GitHub repo (same username as here) which should have full scripts\
          \ if you want to try them out. Just a heads up I don\u2019t think Pythia\
          \ will beat LLaMA as LLaMA was trained on a much bigger corpus than Pythia.\
          \ If you\u2019re looking to do speech to text look up Whisper, it should\
          \ provide pretty well and I believe it\u2019s got a permissible license.\
          \ I know that there is a team trying to train a permissible LLaMA model,\
          \ but it might be a while before that shows up, but I seriously hope I\u2019\
          m wrong. \n\nI\u2019m planning to mess around with UL 2 soon to see if it\
          \ can outperform Pythia."
        updatedAt: '2023-04-17T00:33:07.171Z'
      numEdits: 0
      reactions: []
    id: 643c93c35ff72e5a4ea2901c
    type: comment
  author: M-Chimiste
  content: "Pythia Deduped is probably the best foundation model you can get right\
    \ now with a permissible license or perhaps flan UL2. I have some code sitting\
    \ on my GitHub repo (same username as here) which should have full scripts if\
    \ you want to try them out. Just a heads up I don\u2019t think Pythia will beat\
    \ LLaMA as LLaMA was trained on a much bigger corpus than Pythia. If you\u2019\
    re looking to do speech to text look up Whisper, it should provide pretty well\
    \ and I believe it\u2019s got a permissible license. I know that there is a team\
    \ trying to train a permissible LLaMA model, but it might be a while before that\
    \ shows up, but I seriously hope I\u2019m wrong. \n\nI\u2019m planning to mess\
    \ around with UL 2 soon to see if it can outperform Pythia."
  created_at: 2023-04-16 23:33:07+00:00
  edited: false
  hidden: false
  id: 643c93c35ff72e5a4ea2901c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f2b73d19a1006ee284271aa3f8fd95d8.svg
      fullname: Christian Merrill
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: M-Chimiste
      type: user
    createdAt: '2023-04-18T18:20:52.000Z'
    data:
      status: closed
    id: 643edf84f2ed3bc5c0629dfc
    type: status-change
  author: M-Chimiste
  created_at: 2023-04-18 17:20:52+00:00
  id: 643edf84f2ed3bc5c0629dfc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: M-Chimiste/pythia-12b-dedupe-gpt4all-lora
repo_type: model
status: closed
target_branch: null
title: How do you run this thing
