!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tranhoangnguyen03
conflicting_files: null
created_at: 2023-10-23 05:59:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/397da2bc78a83f0022ab6764d591a4fb.svg
      fullname: Nguyen Tran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tranhoangnguyen03
      type: user
    createdAt: '2023-10-23T06:59:38.000Z'
    data:
      edited: false
      editors:
      - tranhoangnguyen03
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5012195706367493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/397da2bc78a83f0022ab6764d591a4fb.svg
          fullname: Nguyen Tran
          isHf: false
          isPro: false
          name: tranhoangnguyen03
          type: user
        html: '<p>error loading model: cannot find tokenizer merges in model file</p>

          <p>llama_load_model_from_file: failed to load model<br>Traceback (most recent
          call last):<br>  File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main<br>    return
          _run_code(code, main_globals, None,<br>  File "/usr/lib/python3.10/runpy.py",
          line 86, in _run_code<br>    exec(code, run_globals)<br>  File "/usr/local/lib/python3.10/dist-packages/llama_cpp/server/<strong>main</strong>.py",
          line 96, in <br>    app = create_app(settings=settings)<br>  File "/usr/local/lib/python3.10/dist-packages/llama_cpp/server/app.py",
          line 343, in create_app<br>    llama = llama_cpp.Llama(<br>  File "/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py",
          line 365, in <strong>init</strong><br>    assert self.model is not None<br>AssertionError</p>

          '
        raw: "error loading model: cannot find tokenizer merges in model file\r\n\r\
          \nllama_load_model_from_file: failed to load model\r\nTraceback (most recent\
          \ call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\
          \n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\"\
          , line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/server/__main__.py\"\
          , line 96, in <module>\r\n    app = create_app(settings=settings)\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/llama_cpp/server/app.py\", line\
          \ 343, in create_app\r\n    llama = llama_cpp.Llama(\r\n  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\"\
          , line 365, in __init__\r\n    assert self.model is not None\r\nAssertionError"
        updatedAt: '2023-10-23T06:59:38.287Z'
      numEdits: 0
      reactions: []
    id: 653619da8bde2fae19c0b323
    type: comment
  author: tranhoangnguyen03
  content: "error loading model: cannot find tokenizer merges in model file\r\n\r\n\
    llama_load_model_from_file: failed to load model\r\nTraceback (most recent call\
    \ last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\
    \n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\"\
    , line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/server/__main__.py\"\
    , line 96, in <module>\r\n    app = create_app(settings=settings)\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/llama_cpp/server/app.py\", line 343, in\
    \ create_app\r\n    llama = llama_cpp.Llama(\r\n  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py\"\
    , line 365, in __init__\r\n    assert self.model is not None\r\nAssertionError"
  created_at: 2023-10-23 05:59:38+00:00
  edited: false
  hidden: false
  id: 653619da8bde2fae19c0b323
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-23T14:11:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9881554841995239
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Apologies, the originally uploaded GGUFs had an error. Please try
          re-downloading; the newly uploaded GGUFs are confirmed to work with latest
          llama.cpp.</p>

          '
        raw: 'Apologies, the originally uploaded GGUFs had an error. Please try re-downloading;
          the newly uploaded GGUFs are confirmed to work with latest llama.cpp.

          '
        updatedAt: '2023-10-23T14:11:46.296Z'
      numEdits: 0
      reactions: []
    id: 65367f22d325b3f02f92057e
    type: comment
  author: TheBloke
  content: 'Apologies, the originally uploaded GGUFs had an error. Please try re-downloading;
    the newly uploaded GGUFs are confirmed to work with latest llama.cpp.

    '
  created_at: 2023-10-23 13:11:46+00:00
  edited: false
  hidden: false
  id: 65367f22d325b3f02f92057e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/73530fc92bdc74e04f6a64ecd6ced9b7.svg
      fullname: super_cheva
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cheva-513
      type: user
    createdAt: '2023-10-24T07:40:57.000Z'
    data:
      edited: false
      editors:
      - Cheva-513
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.24171555042266846
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/73530fc92bdc74e04f6a64ecd6ced9b7.svg
          fullname: super_cheva
          isHf: false
          isPro: false
          name: Cheva-513
          type: user
        html: "<p>I tried this model using latest llama.cpp,still have problem:</p>\n\
          <p>C:\\llama.cpp&gt;main.exe -m D:\\llmmodels\\TheBloke\\CausalLM-7B-GGUF\\\
          causallm_7b.Q3_K_M.gguf -ngl 18 -p \"introduce yourself.\"<br>Log start<br>main:\
          \ build = 1419 (e393259)<br>main: built with MSVC 19.35.32217.1 for x64<br>main:\
          \ seed  = 1698132297<br>ggml_init_cublas: found 1 CUDA devices:<br>  Device\
          \ 0: NVIDIA T400 4GB, compute capability 7.5<br>llama_model_loader: loaded\
          \ meta data with 21 key-value pairs and 291 tensors from D:\\llmmodels\\\
          TheBloke\\CausalLM-7B-GGUF\\causallm_7b.Q3_K_M.gguf (version unknown)<br>llama_model_loader:\
          \ - tensor    0:                token_embd.weight q3_K     [  4096, 151936,\
          \     1,     1 ]<br>llama_model_loader: - tensor    1:              blk.0.attn_q.weight\
          \ q3_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor\
          \    2:              blk.0.attn_k.weight q3_K     [  4096,  4096,     1,\
          \     1 ]<br>llama_model_loader: - tensor    3:              blk.0.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]<br>llama_model_loader: - tensor\
          \    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]<br>llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight\
          \ q3_K     [  4096, 11008,     1,     1 ]<br>llama_model_loader: - tensor\
          \    6:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,\
          \     1 ]<br>llama_model_loader: - tensor    7:            blk.0.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]<br>llama_model_loader: - tensor\
          \    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]<br>llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]<br>\u3002\u3002\u3002\u3002\u3002\
          \u3002\u3002\u3002</p>\n<p>llama_model_loader: - kv   0:               \
          \        general.architecture str<br>llama_model_loader: - kv   1:     \
          \                          general.name str<br>llama_model_loader: - kv\
          \   2:                       llama.context_length u32<br>llama_model_loader:\
          \ - kv   3:                     llama.embedding_length u32<br>llama_model_loader:\
          \ - kv   4:                          llama.block_count u32<br>llama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32<br>llama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32<br>llama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32<br>llama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32<br>llama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32<br>llama_model_loader:\
          \ - kv  10:                       llama.rope.freq_base f32<br>llama_model_loader:\
          \ - kv  11:                          general.file_type u32<br>llama_model_loader:\
          \ - kv  12:                       tokenizer.ggml.model str<br>llama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.tokens arr<br>llama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr<br>llama_model_loader:\
          \ - kv  15:                  tokenizer.ggml.token_type arr<br>llama_model_loader:\
          \ - kv  16:                      tokenizer.ggml.merges arr<br>llama_model_loader:\
          \ - kv  17:                tokenizer.ggml.bos_token_id u32<br>llama_model_loader:\
          \ - kv  18:                tokenizer.ggml.eos_token_id u32<br>llama_model_loader:\
          \ - kv  19:            tokenizer.ggml.padding_token_id u32<br>llama_model_loader:\
          \ - kv  20:               general.quantization_version u32<br>llama_model_loader:\
          \ - type  f32:   65 tensors<br>llama_model_loader: - type q3_K:  129 tensors<br>llama_model_loader:\
          \ - type q4_K:   92 tensors<br>llama_model_loader: - type q5_K:    4 tensors<br>llama_model_loader:\
          \ - type q6_K:    1 tensors<br>llm_load_vocab: mismatch in special tokens\
          \ definition ( 293/151936 vs 85/151936 ).<br>llm_load_print_meta: format\
          \           = unknown<br>llm_load_print_meta: arch             = llama<br>llm_load_print_meta:\
          \ vocab type       = BPE<br>llm_load_print_meta: n_vocab          = 151936<br>llm_load_print_meta:\
          \ n_merges         = 109170<br>llm_load_print_meta: n_ctx_train      = 8192<br>llm_load_print_meta:\
          \ n_embd           = 4096<br>llm_load_print_meta: n_head           = 32<br>llm_load_print_meta:\
          \ n_head_kv        = 32<br>llm_load_print_meta: n_layer          = 32<br>llm_load_print_meta:\
          \ n_rot            = 128<br>llm_load_print_meta: n_gqa            = 1<br>llm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00<br>llm_load_print_meta: f_norm_rms_eps   =\
          \ 1.0e-05<br>llm_load_print_meta: f_clamp_kqv      = 0.0e+00<br>llm_load_print_meta:\
          \ f_max_alibi_bias = 0.0e+00<br>llm_load_print_meta: n_ff             =\
          \ 11008<br>llm_load_print_meta: freq_base_train  = 10000.0<br>llm_load_print_meta:\
          \ freq_scale_train = 1<br>llm_load_print_meta: model type       = 7B<br>llm_load_print_meta:\
          \ model ftype      = mostly Q3_K - Medium<br>llm_load_print_meta: model\
          \ params     = 7.72 B<br>llm_load_print_meta: model size       = 3.64 GiB\
          \ (4.05 BPW)<br>llm_load_print_meta: general.name   = causallm_7b<br>llm_load_print_meta:\
          \ BOS token = 151643 '&lt;|endoftext|&gt;'<br>llm_load_print_meta: EOS token\
          \ = 151643 '&lt;|endoftext|&gt;'<br>llm_load_print_meta: PAD token = 151643\
          \ '&lt;|endoftext|&gt;'<br>llm_load_print_meta: LF token  = 30 '?'<br>llm_load_tensors:\
          \ ggml ctx size =    0.10 MB<br>llm_load_tensors: using CUDA for GPU acceleration<br>llm_load_tensors:\
          \ mem required  = 2057.64 MB<br>llm_load_tensors: offloading 18 repeating\
          \ layers to GPU<br>llm_load_tensors: offloaded 18/35 layers to GPU<br>llm_load_tensors:\
          \ VRAM used: 1672.59 MB<br>..................................................................................<br>llama_new_context_with_model:\
          \ n_ctx      = 512<br>llama_new_context_with_model: freq_base  = 10000.0<br>llama_new_context_with_model:\
          \ freq_scale = 1<br>llama_new_context_with_model: kv self size  =  256.00\
          \ MB<br>llama_new_context_with_model: compute buffer total size = 310.88\
          \ MB<br>llama_new_context_with_model: VRAM scratch buffer: 304.75 MB<br>llama_new_context_with_model:\
          \ total VRAM used: 1977.34 MB (model: 1672.59 MB, context: 304.75 MB)</p>\n\
          <p>system_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI\
          \ = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 |\
          \ FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX =\
          \ 0 |<br>sampling:<br>        repeat_last_n = 64, repeat_penalty = 1.100,\
          \ frequency_penalty = 0.000, presence_penalty = 0.000<br>        top_k =\
          \ 40, tfs_z = 1.000, top_p = 0.950, typical_p = 1.000, temp = 0.800<br>\
          \        mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000<br>generate:\
          \ n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0</p>\n<p>introduce\
          \ yourself. Where<br>CUDA error 9 at D:\\a\\llama.cpp\\llama.cpp\\ggml-cuda.cu:6862:\
          \ invalid configuration argument<br>current device: 0</p>\n"
        raw: "I tried this model using latest llama.cpp,still have problem:\n\nC:\\\
          llama.cpp>main.exe -m D:\\llmmodels\\TheBloke\\CausalLM-7B-GGUF\\causallm_7b.Q3_K_M.gguf\
          \ -ngl 18 -p \"introduce yourself.\"\nLog start\nmain: build = 1419 (e393259)\n\
          main: built with MSVC 19.35.32217.1 for x64\nmain: seed  = 1698132297\n\
          ggml_init_cublas: found 1 CUDA devices:\n  Device 0: NVIDIA T400 4GB, compute\
          \ capability 7.5\nllama_model_loader: loaded meta data with 21 key-value\
          \ pairs and 291 tensors from D:\\llmmodels\\TheBloke\\CausalLM-7B-GGUF\\\
          causallm_7b.Q3_K_M.gguf (version unknown)\nllama_model_loader: - tensor\
          \    0:                token_embd.weight q3_K     [  4096, 151936,     1,\
          \     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight\
          \ q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    2:              blk.0.attn_k.weight q3_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight\
          \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    4:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,\
          \     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight\
          \ q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor\
          \    6:              blk.0.ffn_up.weight q3_K     [  4096, 11008,     1,\
          \     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight\
          \ q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader: - tensor\
          \    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,\
          \     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight\
          \ f32      [  4096,     1,     1,     1 ]\n\u3002\u3002\u3002\u3002\u3002\
          \u3002\u3002\u3002\n\nllama_model_loader: - kv   0:                    \
          \   general.architecture str\nllama_model_loader: - kv   1:            \
          \                   general.name str\nllama_model_loader: - kv   2:    \
          \                   llama.context_length u32\nllama_model_loader: - kv \
          \  3:                     llama.embedding_length u32\nllama_model_loader:\
          \ - kv   4:                          llama.block_count u32\nllama_model_loader:\
          \ - kv   5:                  llama.feed_forward_length u32\nllama_model_loader:\
          \ - kv   6:                 llama.rope.dimension_count u32\nllama_model_loader:\
          \ - kv   7:                 llama.attention.head_count u32\nllama_model_loader:\
          \ - kv   8:              llama.attention.head_count_kv u32\nllama_model_loader:\
          \ - kv   9:     llama.attention.layer_norm_rms_epsilon f32\nllama_model_loader:\
          \ - kv  10:                       llama.rope.freq_base f32\nllama_model_loader:\
          \ - kv  11:                          general.file_type u32\nllama_model_loader:\
          \ - kv  12:                       tokenizer.ggml.model str\nllama_model_loader:\
          \ - kv  13:                      tokenizer.ggml.tokens arr\nllama_model_loader:\
          \ - kv  14:                      tokenizer.ggml.scores arr\nllama_model_loader:\
          \ - kv  15:                  tokenizer.ggml.token_type arr\nllama_model_loader:\
          \ - kv  16:                      tokenizer.ggml.merges arr\nllama_model_loader:\
          \ - kv  17:                tokenizer.ggml.bos_token_id u32\nllama_model_loader:\
          \ - kv  18:                tokenizer.ggml.eos_token_id u32\nllama_model_loader:\
          \ - kv  19:            tokenizer.ggml.padding_token_id u32\nllama_model_loader:\
          \ - kv  20:               general.quantization_version u32\nllama_model_loader:\
          \ - type  f32:   65 tensors\nllama_model_loader: - type q3_K:  129 tensors\n\
          llama_model_loader: - type q4_K:   92 tensors\nllama_model_loader: - type\
          \ q5_K:    4 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab:\
          \ mismatch in special tokens definition ( 293/151936 vs 85/151936 ).\nllm_load_print_meta:\
          \ format           = unknown\nllm_load_print_meta: arch             = llama\n\
          llm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab\
          \          = 151936\nllm_load_print_meta: n_merges         = 109170\nllm_load_print_meta:\
          \ n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\n\
          llm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv\
          \        = 32\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta:\
          \ n_rot            = 128\nllm_load_print_meta: n_gqa            = 1\nllm_load_print_meta:\
          \ f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n\
          llm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias\
          \ = 0.0e+00\nllm_load_print_meta: n_ff             = 11008\nllm_load_print_meta:\
          \ freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\n\
          llm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype\
          \      = mostly Q3_K - Medium\nllm_load_print_meta: model params     = 7.72\
          \ B\nllm_load_print_meta: model size       = 3.64 GiB (4.05 BPW)\nllm_load_print_meta:\
          \ general.name   = causallm_7b\nllm_load_print_meta: BOS token = 151643\
          \ '<|endoftext|>'\nllm_load_print_meta: EOS token = 151643 '<|endoftext|>'\n\
          llm_load_print_meta: PAD token = 151643 '<|endoftext|>'\nllm_load_print_meta:\
          \ LF token  = 30 '?'\nllm_load_tensors: ggml ctx size =    0.10 MB\nllm_load_tensors:\
          \ using CUDA for GPU acceleration\nllm_load_tensors: mem required  = 2057.64\
          \ MB\nllm_load_tensors: offloading 18 repeating layers to GPU\nllm_load_tensors:\
          \ offloaded 18/35 layers to GPU\nllm_load_tensors: VRAM used: 1672.59 MB\n\
          ..................................................................................\n\
          llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
          \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
          \ kv self size  =  256.00 MB\nllama_new_context_with_model: compute buffer\
          \ total size = 310.88 MB\nllama_new_context_with_model: VRAM scratch buffer:\
          \ 304.75 MB\nllama_new_context_with_model: total VRAM used: 1977.34 MB (model:\
          \ 1672.59 MB, context: 304.75 MB)\n\nsystem_info: n_threads = 4 / 8 | AVX\
          \ = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA\
          \ = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0\
          \ | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |\nsampling:\n        repeat_last_n\
          \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty\
          \ = 0.000\n        top_k = 40, tfs_z = 1.000, top_p = 0.950, typical_p =\
          \ 1.000, temp = 0.800\n        mirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\nintroduce yourself. Where\nCUDA error 9 at D:\\a\\llama.cpp\\\
          llama.cpp\\ggml-cuda.cu:6862: invalid configuration argument\ncurrent device:\
          \ 0"
        updatedAt: '2023-10-24T07:40:57.791Z'
      numEdits: 0
      reactions: []
    id: 6537750901281b544229d259
    type: comment
  author: Cheva-513
  content: "I tried this model using latest llama.cpp,still have problem:\n\nC:\\\
    llama.cpp>main.exe -m D:\\llmmodels\\TheBloke\\CausalLM-7B-GGUF\\causallm_7b.Q3_K_M.gguf\
    \ -ngl 18 -p \"introduce yourself.\"\nLog start\nmain: build = 1419 (e393259)\n\
    main: built with MSVC 19.35.32217.1 for x64\nmain: seed  = 1698132297\nggml_init_cublas:\
    \ found 1 CUDA devices:\n  Device 0: NVIDIA T400 4GB, compute capability 7.5\n\
    llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from\
    \ D:\\llmmodels\\TheBloke\\CausalLM-7B-GGUF\\causallm_7b.Q3_K_M.gguf (version\
    \ unknown)\nllama_model_loader: - tensor    0:                token_embd.weight\
    \ q3_K     [  4096, 151936,     1,     1 ]\nllama_model_loader: - tensor    1:\
    \              blk.0.attn_q.weight q3_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    2:              blk.0.attn_k.weight q3_K     [  4096,  4096,   \
    \  1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight\
    \ q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    4:\
    \         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    5:            blk.0.ffn_gate.weight q3_K     [  4096, 11008,   \
    \  1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight\
    \ q3_K     [  4096, 11008,     1,     1 ]\nllama_model_loader: - tensor    7:\
    \            blk.0.ffn_down.weight q5_K     [ 11008,  4096,     1,     1 ]\nllama_model_loader:\
    \ - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,   \
    \  1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight\
    \ f32      [  4096,     1,     1,     1 ]\n\u3002\u3002\u3002\u3002\u3002\u3002\
    \u3002\u3002\n\nllama_model_loader: - kv   0:                       general.architecture\
    \ str\nllama_model_loader: - kv   1:                               general.name\
    \ str\nllama_model_loader: - kv   2:                       llama.context_length\
    \ u32\nllama_model_loader: - kv   3:                     llama.embedding_length\
    \ u32\nllama_model_loader: - kv   4:                          llama.block_count\
    \ u32\nllama_model_loader: - kv   5:                  llama.feed_forward_length\
    \ u32\nllama_model_loader: - kv   6:                 llama.rope.dimension_count\
    \ u32\nllama_model_loader: - kv   7:                 llama.attention.head_count\
    \ u32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv\
    \ u32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon\
    \ f32\nllama_model_loader: - kv  10:                       llama.rope.freq_base\
    \ f32\nllama_model_loader: - kv  11:                          general.file_type\
    \ u32\nllama_model_loader: - kv  12:                       tokenizer.ggml.model\
    \ str\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens\
    \ arr\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores\
    \ arr\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type\
    \ arr\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges\
    \ arr\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id\
    \ u32\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id\
    \ u32\nllama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id\
    \ u32\nllama_model_loader: - kv  20:               general.quantization_version\
    \ u32\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type\
    \ q3_K:  129 tensors\nllama_model_loader: - type q4_K:   92 tensors\nllama_model_loader:\
    \ - type q5_K:    4 tensors\nllama_model_loader: - type q6_K:    1 tensors\nllm_load_vocab:\
    \ mismatch in special tokens definition ( 293/151936 vs 85/151936 ).\nllm_load_print_meta:\
    \ format           = unknown\nllm_load_print_meta: arch             = llama\n\
    llm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab    \
    \      = 151936\nllm_load_print_meta: n_merges         = 109170\nllm_load_print_meta:\
    \ n_ctx_train      = 8192\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta:\
    \ n_head           = 32\nllm_load_print_meta: n_head_kv        = 32\nllm_load_print_meta:\
    \ n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta:\
    \ n_gqa            = 1\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta:\
    \ f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\n\
    llm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: n_ff   \
    \          = 11008\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta:\
    \ freq_scale_train = 1\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta:\
    \ model ftype      = mostly Q3_K - Medium\nllm_load_print_meta: model params \
    \    = 7.72 B\nllm_load_print_meta: model size       = 3.64 GiB (4.05 BPW)\nllm_load_print_meta:\
    \ general.name   = causallm_7b\nllm_load_print_meta: BOS token = 151643 '<|endoftext|>'\n\
    llm_load_print_meta: EOS token = 151643 '<|endoftext|>'\nllm_load_print_meta:\
    \ PAD token = 151643 '<|endoftext|>'\nllm_load_print_meta: LF token  = 30 '?'\n\
    llm_load_tensors: ggml ctx size =    0.10 MB\nllm_load_tensors: using CUDA for\
    \ GPU acceleration\nllm_load_tensors: mem required  = 2057.64 MB\nllm_load_tensors:\
    \ offloading 18 repeating layers to GPU\nllm_load_tensors: offloaded 18/35 layers\
    \ to GPU\nllm_load_tensors: VRAM used: 1672.59 MB\n..................................................................................\n\
    llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
    \ freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model:\
    \ kv self size  =  256.00 MB\nllama_new_context_with_model: compute buffer total\
    \ size = 310.88 MB\nllama_new_context_with_model: VRAM scratch buffer: 304.75\
    \ MB\nllama_new_context_with_model: total VRAM used: 1977.34 MB (model: 1672.59\
    \ MB, context: 304.75 MB)\n\nsystem_info: n_threads = 4 / 8 | AVX = 1 | AVX2 =\
    \ 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA\
    \ = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 =\
    \ 0 | VSX = 0 |\nsampling:\n        repeat_last_n = 64, repeat_penalty = 1.100,\
    \ frequency_penalty = 0.000, presence_penalty = 0.000\n        top_k = 40, tfs_z\
    \ = 1.000, top_p = 0.950, typical_p = 1.000, temp = 0.800\n        mirostat =\
    \ 0, mirostat_lr = 0.100, mirostat_ent = 5.000\ngenerate: n_ctx = 512, n_batch\
    \ = 512, n_predict = -1, n_keep = 0\n\n\nintroduce yourself. Where\nCUDA error\
    \ 9 at D:\\a\\llama.cpp\\llama.cpp\\ggml-cuda.cu:6862: invalid configuration argument\n\
    current device: 0"
  created_at: 2023-10-24 06:40:57+00:00
  edited: false
  hidden: false
  id: 6537750901281b544229d259
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/CausalLM-7B-GGUF
repo_type: model
status: open
target_branch: null
title: 'error loading model: cannot find tokenizer merges in model file'
