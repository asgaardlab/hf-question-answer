!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Tonight223
conflicting_files: null
created_at: 2023-10-18 00:34:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637f083e2438d7485b8eb942/gO1NL5pquqQS-uGVQ4Sjc.png?w=200&h=200&f=face
      fullname: T-ML
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Tonight223
      type: user
    createdAt: '2023-10-18T01:34:19.000Z'
    data:
      edited: false
      editors:
      - Tonight223
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.830044150352478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637f083e2438d7485b8eb942/gO1NL5pquqQS-uGVQ4Sjc.png?w=200&h=200&f=face
          fullname: T-ML
          isHf: false
          isPro: false
          name: Tonight223
          type: user
        html: '<p>Any one know how to fix this? Running model, set seq length 4096.
          After set the seq length as tensor b (1479) it went ok again but one message
          later it shows similiar error again. And the offical model card said max
          seq length is 4096, what''s wrong? And I find out if I reload the model
          with 4096 len every time before sending the message it will be ok. I tried
          other AWQ model they have similiar issues.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/637f083e2438d7485b8eb942/90UQNbFiVvkviOsJjfirs.png"><img
          alt="Screenshot 2023-10-18 091141.png" src="https://cdn-uploads.huggingface.co/production/uploads/637f083e2438d7485b8eb942/90UQNbFiVvkviOsJjfirs.png"></a></p>

          '
        raw: "Any one know how to fix this? Running model, set seq length 4096. After\
          \ set the seq length as tensor b (1479) it went ok again but one message\
          \ later it shows similiar error again. And the offical model card said max\
          \ seq length is 4096, what's wrong? And I find out if I reload the model\
          \ with 4096 len every time before sending the message it will be ok. I tried\
          \ other AWQ model they have similiar issues.\r\n\r\n![Screenshot 2023-10-18\
          \ 091141.png](https://cdn-uploads.huggingface.co/production/uploads/637f083e2438d7485b8eb942/90UQNbFiVvkviOsJjfirs.png)\r\
          \n"
        updatedAt: '2023-10-18T01:34:19.183Z'
      numEdits: 0
      reactions: []
    id: 652f361b7b0079ff0367ca07
    type: comment
  author: Tonight223
  content: "Any one know how to fix this? Running model, set seq length 4096. After\
    \ set the seq length as tensor b (1479) it went ok again but one message later\
    \ it shows similiar error again. And the offical model card said max seq length\
    \ is 4096, what's wrong? And I find out if I reload the model with 4096 len every\
    \ time before sending the message it will be ok. I tried other AWQ model they\
    \ have similiar issues.\r\n\r\n![Screenshot 2023-10-18 091141.png](https://cdn-uploads.huggingface.co/production/uploads/637f083e2438d7485b8eb942/90UQNbFiVvkviOsJjfirs.png)\r\
    \n"
  created_at: 2023-10-18 00:34:19+00:00
  edited: false
  hidden: false
  id: 652f361b7b0079ff0367ca07
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/llemma_34b-AWQ
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: The size of tensor a (4096) must match the size of tensor b
  (1479) at non-singleton dimension 3'
