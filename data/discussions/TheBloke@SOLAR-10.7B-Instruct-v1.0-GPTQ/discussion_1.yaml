!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jesulo
conflicting_files: null
created_at: 2024-01-17 17:42:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda60a4ac139b9195d75b8ab14caf06d.svg
      fullname: "Jes\xFAs Leguizam\xF3n"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jesulo
      type: user
    createdAt: '2024-01-17T17:42:55.000Z'
    data:
      edited: false
      editors:
      - jesulo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3959730565547943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda60a4ac139b9195d75b8ab14caf06d.svg
          fullname: "Jes\xFAs Leguizam\xF3n"
          isHf: false
          isPro: false
          name: jesulo
          type: user
        html: '<p>17:02:14-578319 INFO     The AutoGPTQ params are: {''model_basename'':
          ''model'',<br>                         ''device'': ''cuda:0'', ''use_triton'':
          False,<br>                         ''inject_fused_attention'': True, ''inject_fused_mlp'':<br>                         True,
          ''use_safetensors'': True, ''trust_remote_code'':<br>                         False,
          ''max_memory'': {0: ''14900MiB'', 1: ''12000MiB'',<br>                         ''cpu'':
          ''99GiB''}, ''quantize_config'': None,<br>                         ''use_cuda_fp16'':
          True, ''disable_exllama'': False,<br>                         ''disable_exllamav2'':
          False}<br>17:03:27-901969 INFO     LOADER: AutoGPTQ<br>17:03:27-903669 INFO     TRUNCATION
          LENGTH: 4096<br>17:03:27-905051 INFO     INSTRUCTION TEMPLATE: Custom (obtained
          from model<br>                         metadata)<br>17:03:27-906589 INFO     Loaded
          the model in 73.33 seconds.<br>17:05:38-516164 INFO     Deleted logs/instruct/20240117-17-00-17.json.<br>Output
          generated in 0.28 seconds (0.00 tokens/s, 0 tokens, context 538, seed 679366167)</p>

          '
        raw: "17:02:14-578319 INFO     The AutoGPTQ params are: {'model_basename':\
          \ 'model',\r\n                         'device': 'cuda:0', 'use_triton':\
          \ False,\r\n                         'inject_fused_attention': True, 'inject_fused_mlp':\r\
          \n                         True, 'use_safetensors': True, 'trust_remote_code':\r\
          \n                         False, 'max_memory': {0: '14900MiB', 1: '12000MiB',\r\
          \n                         'cpu': '99GiB'}, 'quantize_config': None,\r\n\
          \                         'use_cuda_fp16': True, 'disable_exllama': False,\r\
          \n                         'disable_exllamav2': False}\r\n17:03:27-901969\
          \ INFO     LOADER: AutoGPTQ\r\n17:03:27-903669 INFO     TRUNCATION LENGTH:\
          \ 4096\r\n17:03:27-905051 INFO     INSTRUCTION TEMPLATE: Custom (obtained\
          \ from model\r\n                         metadata)\r\n17:03:27-906589 INFO\
          \     Loaded the model in 73.33 seconds.\r\n17:05:38-516164 INFO     Deleted\
          \ logs/instruct/20240117-17-00-17.json.\r\nOutput generated in 0.28 seconds\
          \ (0.00 tokens/s, 0 tokens, context 538, seed 679366167)"
        updatedAt: '2024-01-17T17:42:55.383Z'
      numEdits: 0
      reactions: []
    id: 65a8119fa92d5908dff15096
    type: comment
  author: jesulo
  content: "17:02:14-578319 INFO     The AutoGPTQ params are: {'model_basename': 'model',\r\
    \n                         'device': 'cuda:0', 'use_triton': False,\r\n      \
    \                   'inject_fused_attention': True, 'inject_fused_mlp':\r\n  \
    \                       True, 'use_safetensors': True, 'trust_remote_code':\r\n\
    \                         False, 'max_memory': {0: '14900MiB', 1: '12000MiB',\r\
    \n                         'cpu': '99GiB'}, 'quantize_config': None,\r\n     \
    \                    'use_cuda_fp16': True, 'disable_exllama': False,\r\n    \
    \                     'disable_exllamav2': False}\r\n17:03:27-901969 INFO    \
    \ LOADER: AutoGPTQ\r\n17:03:27-903669 INFO     TRUNCATION LENGTH: 4096\r\n17:03:27-905051\
    \ INFO     INSTRUCTION TEMPLATE: Custom (obtained from model\r\n             \
    \            metadata)\r\n17:03:27-906589 INFO     Loaded the model in 73.33 seconds.\r\
    \n17:05:38-516164 INFO     Deleted logs/instruct/20240117-17-00-17.json.\r\nOutput\
    \ generated in 0.28 seconds (0.00 tokens/s, 0 tokens, context 538, seed 679366167)"
  created_at: 2024-01-17 17:42:55+00:00
  edited: false
  hidden: false
  id: 65a8119fa92d5908dff15096
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/SOLAR-10.7B-Instruct-v1.0-GPTQ
repo_type: model
status: open
target_branch: null
title: No genetrate tokens in oobabooga
