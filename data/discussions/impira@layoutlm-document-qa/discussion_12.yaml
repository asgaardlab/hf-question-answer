!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Jack-cpu
conflicting_files: null
created_at: 2023-05-12 12:56:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a3ebee34fb27a6133edc0734488b08b6.svg
      fullname: Jack Wick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jack-cpu
      type: user
    createdAt: '2023-05-12T13:56:01.000Z'
    data:
      edited: false
      editors:
      - Jack-cpu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a3ebee34fb27a6133edc0734488b08b6.svg
          fullname: Jack Wick
          isHf: false
          isPro: false
          name: Jack-cpu
          type: user
        html: '<p>Hello,</p>

          <p>I''ve tried to implement docquery with different models like impira/layoutlm-document-qa
          but I always have the same error (on collab notebook or pycharm, windows
          or ubuntu always the same):</p>

          <p>Code :<br>from docquery import document, pipeline</p>

          <p>p = pipeline(task=''document-question-answering'', model = "impira/layoutlm-invoices")<br>doc
          = document.load_document(file)<br>s = p(question="What is the invoice number?",
          **doc.context)</p>

          <p>Error :</p>

          <p>Exception                                 Traceback (most recent call
          last)<br>Cell In[72], line 6<br>      4 p = pipeline(task=''document-question-answering'',
          model = "impira/layoutlm-invoices")<br>      5 doc = document.load_document(file)<br>----&gt;
          6 s = p(question="What is the invoice number?", **doc.context)</p>

          <p>File ~\Documents\GitHub\Llama\venv\lib\site-packages\docquery\ext\pipeline_document_question_answering.py:232,
          in DocumentQuestionAnsweringPipeline.<strong>call</strong>(self, image,
          question, **kwargs)<br>    229 else:<br>    230     normalized_images =
          [(image, None)]<br>--&gt; 232 return super().<strong>call</strong>({"question":
          question, "pages": normalized_images}, **kwargs)</p>

          <p>File ~\Documents\GitHub\Llama\venv\lib\site-packages\transformers\pipelines\base.py:1111,
          in Pipeline.<strong>call</strong>(self, inputs, num_workers, batch_size,
          *args, **kwargs)<br>   1109     return self.iterate(inputs, preprocess_params,
          forward_params, postprocess_params)<br>   1110 elif self.framework == "pt"
          and isinstance(self, ChunkPipeline):<br>-&gt; 1111     return next(<br>   1112         iter(<br>   1113             self.get_iterator(<br>   1114                 [inputs],
          num_workers, batch_size, preprocess_params, forward_params, postprocess_params<br>   1115             )<br>   1116         )<br>   1117     )<br>   1118
          else:<br>   1119     return self.run_single(inputs, preprocess_params, forward_params,
          postprocess_params)</p>

          <p>File ~\Documents\GitHub\Llama\venv\lib\site-packages\transformers\pipelines\pt_utils.py:124,
          in PipelineIterator.<strong>next</strong>(self)<br>    121     return self.loader_batch_item()<br>    123
          # We''re out of items within a batch<br>--&gt; 124 item = next(self.iterator)<br>    125
          processed = self.infer(item, **self.params)<br>    126 # We now have a batch
          of "inferred things".</p>

          <p>File ~\Documents\GitHub\Llama\venv\lib\site-packages\transformers\pipelines\pt_utils.py:291,
          in PipelinePackIterator.<strong>next</strong>(self)<br>    289     else:<br>    290         item
          = processed<br>--&gt; 291         is_last = item.pop("is_last")<br>    292         accumulator.append(item)<br>    293
          return accumulator</p>

          <p>File ~\Documents\GitHub\Llama\venv\lib\site-packages\transformers\utils\generic.py:310,
          in ModelOutput.pop(self, *args, **kwargs)<br>    309 def pop(self, *args,
          **kwargs):<br>--&gt; 310     raise Exception(f"You cannot use <code>pop</code>
          on a {self.<strong>class</strong>.<strong>name</strong>} instance.")</p>

          <p>Exception: You cannot use <code>pop</code> on a ModelOutput instance.</p>

          <p>Thank you for your help.</p>

          '
        raw: "Hello,\r\n\r\nI've tried to implement docquery with different models\
          \ like impira/layoutlm-document-qa but I always have the same error (on\
          \ collab notebook or pycharm, windows or ubuntu always the same):\r\n\r\n\
          Code :\r\nfrom docquery import document, pipeline\r\n\r\np = pipeline(task='document-question-answering',\
          \ model = \"impira/layoutlm-invoices\")\r\ndoc = document.load_document(file)\r\
          \ns = p(question=\"What is the invoice number?\", **doc.context)\r\n\r\n\
          Error :\r\n\r\nException                                 Traceback (most\
          \ recent call last)\r\nCell In[72], line 6\r\n      4 p = pipeline(task='document-question-answering',\
          \ model = \"impira/layoutlm-invoices\")\r\n      5 doc = document.load_document(file)\r\
          \n----> 6 s = p(question=\"What is the invoice number?\", **doc.context)\r\
          \n\r\nFile ~\\Documents\\GitHub\\Llama\\venv\\lib\\site-packages\\docquery\\\
          ext\\pipeline_document_question_answering.py:232, in DocumentQuestionAnsweringPipeline.__call__(self,\
          \ image, question, **kwargs)\r\n    229 else:\r\n    230     normalized_images\
          \ = [(image, None)]\r\n--> 232 return super().__call__({\"question\": question,\
          \ \"pages\": normalized_images}, **kwargs)\r\n\r\nFile ~\\Documents\\GitHub\\\
          Llama\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1111,\
          \ in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)\r\
          \n   1109     return self.iterate(inputs, preprocess_params, forward_params,\
          \ postprocess_params)\r\n   1110 elif self.framework == \"pt\" and isinstance(self,\
          \ ChunkPipeline):\r\n-> 1111     return next(\r\n   1112         iter(\r\
          \n   1113             self.get_iterator(\r\n   1114                 [inputs],\
          \ num_workers, batch_size, preprocess_params, forward_params, postprocess_params\r\
          \n   1115             )\r\n   1116         )\r\n   1117     )\r\n   1118\
          \ else:\r\n   1119     return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n\r\nFile ~\\Documents\\GitHub\\\
          Llama\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124,\
          \ in PipelineIterator.__next__(self)\r\n    121     return self.loader_batch_item()\r\
          \n    123 # We're out of items within a batch\r\n--> 124 item = next(self.iterator)\r\
          \n    125 processed = self.infer(item, **self.params)\r\n    126 # We now\
          \ have a batch of \"inferred things\".\r\n\r\nFile ~\\Documents\\GitHub\\\
          Llama\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:291,\
          \ in PipelinePackIterator.__next__(self)\r\n    289     else:\r\n    290\
          \         item = processed\r\n--> 291         is_last = item.pop(\"is_last\"\
          )\r\n    292         accumulator.append(item)\r\n    293 return accumulator\r\
          \n\r\nFile ~\\Documents\\GitHub\\Llama\\venv\\lib\\site-packages\\transformers\\\
          utils\\generic.py:310, in ModelOutput.pop(self, *args, **kwargs)\r\n   \
          \ 309 def pop(self, *args, **kwargs):\r\n--> 310     raise Exception(f\"\
          You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\r\n\r\
          \nException: You cannot use ``pop`` on a ModelOutput instance.\r\n\r\nThank\
          \ you for your help."
        updatedAt: '2023-05-12T13:56:01.591Z'
      numEdits: 0
      reactions: []
    id: 645e45714efa578bd10213f8
    type: comment
  author: Jack-cpu
  content: "Hello,\r\n\r\nI've tried to implement docquery with different models like\
    \ impira/layoutlm-document-qa but I always have the same error (on collab notebook\
    \ or pycharm, windows or ubuntu always the same):\r\n\r\nCode :\r\nfrom docquery\
    \ import document, pipeline\r\n\r\np = pipeline(task='document-question-answering',\
    \ model = \"impira/layoutlm-invoices\")\r\ndoc = document.load_document(file)\r\
    \ns = p(question=\"What is the invoice number?\", **doc.context)\r\n\r\nError\
    \ :\r\n\r\nException                                 Traceback (most recent call\
    \ last)\r\nCell In[72], line 6\r\n      4 p = pipeline(task='document-question-answering',\
    \ model = \"impira/layoutlm-invoices\")\r\n      5 doc = document.load_document(file)\r\
    \n----> 6 s = p(question=\"What is the invoice number?\", **doc.context)\r\n\r\
    \nFile ~\\Documents\\GitHub\\Llama\\venv\\lib\\site-packages\\docquery\\ext\\\
    pipeline_document_question_answering.py:232, in DocumentQuestionAnsweringPipeline.__call__(self,\
    \ image, question, **kwargs)\r\n    229 else:\r\n    230     normalized_images\
    \ = [(image, None)]\r\n--> 232 return super().__call__({\"question\": question,\
    \ \"pages\": normalized_images}, **kwargs)\r\n\r\nFile ~\\Documents\\GitHub\\\
    Llama\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:1111, in Pipeline.__call__(self,\
    \ inputs, num_workers, batch_size, *args, **kwargs)\r\n   1109     return self.iterate(inputs,\
    \ preprocess_params, forward_params, postprocess_params)\r\n   1110 elif self.framework\
    \ == \"pt\" and isinstance(self, ChunkPipeline):\r\n-> 1111     return next(\r\
    \n   1112         iter(\r\n   1113             self.get_iterator(\r\n   1114 \
    \                [inputs], num_workers, batch_size, preprocess_params, forward_params,\
    \ postprocess_params\r\n   1115             )\r\n   1116         )\r\n   1117\
    \     )\r\n   1118 else:\r\n   1119     return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n\r\nFile ~\\Documents\\GitHub\\Llama\\\
    venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124, in PipelineIterator.__next__(self)\r\
    \n    121     return self.loader_batch_item()\r\n    123 # We're out of items\
    \ within a batch\r\n--> 124 item = next(self.iterator)\r\n    125 processed =\
    \ self.infer(item, **self.params)\r\n    126 # We now have a batch of \"inferred\
    \ things\".\r\n\r\nFile ~\\Documents\\GitHub\\Llama\\venv\\lib\\site-packages\\\
    transformers\\pipelines\\pt_utils.py:291, in PipelinePackIterator.__next__(self)\r\
    \n    289     else:\r\n    290         item = processed\r\n--> 291         is_last\
    \ = item.pop(\"is_last\")\r\n    292         accumulator.append(item)\r\n    293\
    \ return accumulator\r\n\r\nFile ~\\Documents\\GitHub\\Llama\\venv\\lib\\site-packages\\\
    transformers\\utils\\generic.py:310, in ModelOutput.pop(self, *args, **kwargs)\r\
    \n    309 def pop(self, *args, **kwargs):\r\n--> 310     raise Exception(f\"You\
    \ cannot use ``pop`` on a {self.__class__.__name__} instance.\")\r\n\r\nException:\
    \ You cannot use ``pop`` on a ModelOutput instance.\r\n\r\nThank you for your\
    \ help."
  created_at: 2023-05-12 12:56:01+00:00
  edited: false
  hidden: false
  id: 645e45714efa578bd10213f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a3ebee34fb27a6133edc0734488b08b6.svg
      fullname: Jack Wick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jack-cpu
      type: user
    createdAt: '2023-07-23T13:33:11.000Z'
    data:
      status: closed
    id: 64bd2c174d2052b1aa1644d6
    type: status-change
  author: Jack-cpu
  created_at: 2023-07-23 12:33:11+00:00
  id: 64bd2c174d2052b1aa1644d6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: impira/layoutlm-document-qa
repo_type: model
status: closed
target_branch: null
title: Error on pipeline with docquery
