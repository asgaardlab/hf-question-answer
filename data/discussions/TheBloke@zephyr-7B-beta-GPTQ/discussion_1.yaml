!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shivam098
conflicting_files: null
created_at: 2023-10-30 09:43:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e5f690b23a12f95596765fbb8cee1436.svg
      fullname: Shivam Jha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shivam098
      type: user
    createdAt: '2023-10-30T10:43:08.000Z'
    data:
      edited: false
      editors:
      - Shivam098
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.23965297639369965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e5f690b23a12f95596765fbb8cee1436.svg
          fullname: Shivam Jha
          isHf: false
          isPro: false
          name: Shivam098
          type: user
        html: '<p>-max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens
          4096<br>How to increase using<br>output = model.generate(inputs=input_ids,
          temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)</p>

          '
        raw: "-max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens\
          \ 4096\r\nHow to increase using  \r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\r\
          \n"
        updatedAt: '2023-10-30T10:43:08.944Z'
      numEdits: 0
      reactions: []
    id: 653f88bc059fdd2587184096
    type: comment
  author: Shivam098
  content: "-max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens\
    \ 4096\r\nHow to increase using  \r\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\r\
    \n"
  created_at: 2023-10-30 09:43:08+00:00
  edited: false
  hidden: false
  id: 653f88bc059fdd2587184096
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
      fullname: dario
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prudant
      type: user
    createdAt: '2023-10-30T23:06:19.000Z'
    data:
      edited: false
      editors:
      - prudant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8689116835594177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/182b1994ad37ed23d8a066caeaef83d5.svg
          fullname: dario
          isHf: false
          isPro: false
          name: prudant
          type: user
        html: '<p>same here, the mistral 7b base model is 8k context lenght I understand,
          this model is 4k? or is a typo in the readme.md ?<br>Thanks!</p>

          '
        raw: "same here, the mistral 7b base model is 8k context lenght I understand,\
          \ this model is 4k? or is a typo in the readme.md ? \nThanks!"
        updatedAt: '2023-10-30T23:06:19.665Z'
      numEdits: 0
      reactions: []
    id: 654036ebb3f9aad245c366bd
    type: comment
  author: prudant
  content: "same here, the mistral 7b base model is 8k context lenght I understand,\
    \ this model is 4k? or is a typo in the readme.md ? \nThanks!"
  created_at: 2023-10-30 22:06:19+00:00
  edited: false
  hidden: false
  id: 654036ebb3f9aad245c366bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-30T23:16:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.37668198347091675
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Just change it to </p>

          <pre><code>---max-input-length 7892 --max-total-tokens 8192 --max-batch-prefill-tokens
          8192

          </code></pre>

          <p>or whatever you want</p>

          '
        raw: "Just change it to \n```\n---max-input-length 7892 --max-total-tokens\
          \ 8192 --max-batch-prefill-tokens 8192\n```\n\nor whatever you want"
        updatedAt: '2023-10-30T23:16:06.963Z'
      numEdits: 0
      reactions: []
    id: 654039361e97ebd6e9a252d3
    type: comment
  author: TheBloke
  content: "Just change it to \n```\n---max-input-length 7892 --max-total-tokens 8192\
    \ --max-batch-prefill-tokens 8192\n```\n\nor whatever you want"
  created_at: 2023-10-30 22:16:06+00:00
  edited: false
  hidden: false
  id: 654039361e97ebd6e9a252d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e5f690b23a12f95596765fbb8cee1436.svg
      fullname: Shivam Jha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shivam098
      type: user
    createdAt: '2023-10-31T04:13:42.000Z'
    data:
      edited: true
      editors:
      - Shivam098
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5786710381507874
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e5f690b23a12f95596765fbb8cee1436.svg
          fullname: Shivam Jha
          isHf: false
          isPro: false
          name: Shivam098
          type: user
        html: "<p>How to do here<br>import torch<br>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline<br>def run():<br>    model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\
          <br>    # To use a different branch, change revision<br>    # For example:\
          \ revision=\"gptq-4bit-32g-actorder_True\"<br>    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>\
          \                                                device_map=\"auto\",<br>\
          \                                                trust_remote_code=False,<br>\
          \                                                revision=\"main\")</p>\n\
          <pre><code>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\n# Open the text file for reading\nwith open('data.txt',\
          \ 'r') as file:\n    # Read the entire content of the file into a string\n\
          \    file_content = file.read()\n\nprompt = f\"Extract the usefull information\
          \ from the following given text: {file_content} and convert the extracted\
          \ data in the structured format using valid json only.\"\nprompt_template=f'''&lt;|system|&gt;\n\
          &lt;/s&gt;\n&lt;|user|&gt;\n{prompt}&lt;/s&gt;\n&lt;|assistant|&gt;\n'''\n\
          \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n\n# Inference can also be done using\
          \ transformers' pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n  \
          \  top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
          </code></pre>\n"
        raw: "How to do here \nimport torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline\ndef run():\n    model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\
          \n    # To use a different branch, change revision\n    # For example: revision=\"\
          gptq-4bit-32g-actorder_True\"\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                                device_map=\"auto\",\n\
          \                                                trust_remote_code=False,\n\
          \                                                revision=\"main\")\n\n\
          \    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \n    # Open the text file for reading\n    with open('data.txt', 'r') as\
          \ file:\n        # Read the entire content of the file into a string\n \
          \       file_content = file.read()\n\n    prompt = f\"Extract the usefull\
          \ information from the following given text: {file_content} and convert\
          \ the extracted data in the structured format using valid json only.\"\n\
          \    prompt_template=f'''<|system|>\n    </s>\n    <|user|>\n    {prompt}</s>\n\
          \    <|assistant|>\n    '''\n\n    print(\"\\n\\n*** Generate:\")\n\n  \
          \  input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          \    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\n    print(tokenizer.decode(output[0]))\n\
          \n    # Inference can also be done using transformers' pipeline\n\n    print(\"\
          *** Pipeline:\")\n    pipe = pipeline(\n        \"text-generation\",\n \
          \       model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n\
          \        do_sample=True,\n        temperature=0.7,\n        top_p=0.95,\n\
          \        top_k=40,\n        repetition_penalty=1.1\n    )\n\n    print(pipe(prompt_template)[0]['generated_text'])\n\
          \n\n\n\n"
        updatedAt: '2023-10-31T04:15:24.972Z'
      numEdits: 2
      reactions: []
    id: 65407ef6fcbd1aa0066e0dbf
    type: comment
  author: Shivam098
  content: "How to do here \nimport torch\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, pipeline\ndef run():\n    model_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\
    \n    # To use a different branch, change revision\n    # For example: revision=\"\
    gptq-4bit-32g-actorder_True\"\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \                                                device_map=\"auto\",\n      \
    \                                          trust_remote_code=False,\n        \
    \                                        revision=\"main\")\n\n    tokenizer =\
    \ AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\n    # Open\
    \ the text file for reading\n    with open('data.txt', 'r') as file:\n       \
    \ # Read the entire content of the file into a string\n        file_content =\
    \ file.read()\n\n    prompt = f\"Extract the usefull information from the following\
    \ given text: {file_content} and convert the extracted data in the structured\
    \ format using valid json only.\"\n    prompt_template=f'''<|system|>\n    </s>\n\
    \    <|user|>\n    {prompt}</s>\n    <|assistant|>\n    '''\n\n    print(\"\\\
    n\\n*** Generate:\")\n\n    input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    \    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
    \ top_p=0.95, top_k=40, max_new_tokens=512)\n    print(tokenizer.decode(output[0]))\n\
    \n    # Inference can also be done using transformers' pipeline\n\n    print(\"\
    *** Pipeline:\")\n    pipe = pipeline(\n        \"text-generation\",\n       \
    \ model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n  \
    \      do_sample=True,\n        temperature=0.7,\n        top_p=0.95,\n      \
    \  top_k=40,\n        repetition_penalty=1.1\n    )\n\n    print(pipe(prompt_template)[0]['generated_text'])\n\
    \n\n\n\n"
  created_at: 2023-10-31 03:13:42+00:00
  edited: true
  hidden: false
  id: 65407ef6fcbd1aa0066e0dbf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/zephyr-7B-beta-GPTQ
repo_type: model
status: open
target_branch: null
title: Context Length and Max New Tokens
