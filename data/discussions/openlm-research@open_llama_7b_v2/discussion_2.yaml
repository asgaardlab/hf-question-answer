!!python/object:huggingface_hub.community.DiscussionWithDetails
author: iekang
conflicting_files: null
created_at: 2023-07-14 21:10:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da79232b3507d4c977fb1db71903bc80.svg
      fullname: Edwin Kang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iekang
      type: user
    createdAt: '2023-07-14T22:10:35.000Z'
    data:
      edited: false
      editors:
      - iekang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4864087402820587
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da79232b3507d4c977fb1db71903bc80.svg
          fullname: Edwin Kang
          isHf: false
          isPro: false
          name: iekang
          type: user
        html: '<p>Thanks for sharing this amazing model!<br>When I try to run your
          example code from my server (with 8-GPUs, CUDA 11.4), I got the errors below,
          any insight?</p>

          <p>Traceback (most recent call last):<br>  File "/mnt/task_runtime/test_olm_llama.py",
          line 20, in <br>    generation_output = model.generate(<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py",
          line 1522, in generate<br>    return self.greedy_search(<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py",
          line 2339, in greedy_search<br>    outputs = self(<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py",
          line 688, in forward<br>    outputs = self.model(<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py",
          line 578, in forward<br>    layer_outputs = decoder_layer(<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py",
          line 292, in forward<br>    hidden_states, self_attn_weights, present_key_value
          = self.self_attn(<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py",
          line 194, in forward<br>    query_states = self.q_proj(hidden_states).view(bsz,
          q_len, self.num_heads, self.head_dim).transpose(1, 2)<br>  File "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py",
          line 1501, in <em>call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/linear.py",
          line 114, in forward<br>    return F.linear(input, self.weight, self.bias)<br>RuntimeError:
          "addmm_impl_cpu</em>" not implemented for ''Half''</p>

          '
        raw: "Thanks for sharing this amazing model! \r\nWhen I try to run your example\
          \ code from my server (with 8-GPUs, CUDA 11.4), I got the errors below,\
          \ any insight?\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/task_runtime/test_olm_llama.py\"\
          , line 20, in <module>\r\n    generation_output = model.generate(\r\n  File\
          \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 1522, in generate\r\n    return self.greedy_search(\r\n  File \"\
          /mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py\"\
          , line 2339, in greedy_search\r\n    outputs = self(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 688, in forward\r\n    outputs = self.model(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 578, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"\
          /mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 292, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 194, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/linear.py\"\
          , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
          \nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'"
        updatedAt: '2023-07-14T22:10:35.480Z'
      numEdits: 0
      reactions: []
    id: 64b1c7dbffed52962a917085
    type: comment
  author: iekang
  content: "Thanks for sharing this amazing model! \r\nWhen I try to run your example\
    \ code from my server (with 8-GPUs, CUDA 11.4), I got the errors below, any insight?\r\
    \n\r\nTraceback (most recent call last):\r\n  File \"/mnt/task_runtime/test_olm_llama.py\"\
    , line 20, in <module>\r\n    generation_output = model.generate(\r\n  File \"\
    /mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 1522, in generate\r\n    return self.greedy_search(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/generation/utils.py\"\
    , line 2339, in greedy_search\r\n    outputs = self(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 688, in forward\r\n    outputs = self.model(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 578, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 292, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 194, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/mnt/task_runtime/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/linear.py\"\
    , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
    \nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'"
  created_at: 2023-07-14 21:10:35+00:00
  edited: false
  hidden: false
  id: 64b1c7dbffed52962a917085
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677373824479-63036143fc783bfc743ecf50.jpeg?w=200&h=200&f=face
      fullname: Xinyang (Young) Geng
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: young-geng
      type: user
    createdAt: '2023-07-16T08:37:18.000Z'
    data:
      edited: false
      editors:
      - young-geng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.910685122013092
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677373824479-63036143fc783bfc743ecf50.jpeg?w=200&h=200&f=face
          fullname: Xinyang (Young) Geng
          isHf: false
          isPro: false
          name: young-geng
          type: user
        html: '<p>This is likely a result of running it on CPU, where the half-precision
          ops are not supported. To use it on CPU, you need to convert the data type
          to float32 before you run any inference.</p>

          '
        raw: This is likely a result of running it on CPU, where the half-precision
          ops are not supported. To use it on CPU, you need to convert the data type
          to float32 before you run any inference.
        updatedAt: '2023-07-16T08:37:18.726Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - acbdkk
    id: 64b3ac3e102ed6e7aef3ce3b
    type: comment
  author: young-geng
  content: This is likely a result of running it on CPU, where the half-precision
    ops are not supported. To use it on CPU, you need to convert the data type to
    float32 before you run any inference.
  created_at: 2023-07-16 07:37:18+00:00
  edited: false
  hidden: false
  id: 64b3ac3e102ed6e7aef3ce3b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
      fullname: qc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: captainst
      type: user
    createdAt: '2023-07-17T06:13:42.000Z'
    data:
      edited: false
      editors:
      - captainst
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9942988753318787
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a6ecd8eb870b226f8e50f108637e6e1.svg
          fullname: qc
          isHf: false
          isPro: false
          name: captainst
          type: user
        html: '<p>it would be helpful if you paste the sample code you were testing
          on.</p>

          '
        raw: it would be helpful if you paste the sample code you were testing on.
        updatedAt: '2023-07-17T06:13:42.680Z'
      numEdits: 0
      reactions: []
    id: 64b4dc16a15d33a1bcca7dc4
    type: comment
  author: captainst
  content: it would be helpful if you paste the sample code you were testing on.
  created_at: 2023-07-17 05:13:42+00:00
  edited: false
  hidden: false
  id: 64b4dc16a15d33a1bcca7dc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd6dbae86775652afc0382feed4cfe29.svg
      fullname: Sophie Aminu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chie2727
      type: user
    createdAt: '2023-08-14T05:44:11.000Z'
    data:
      edited: false
      editors:
      - Chie2727
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33348071575164795
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd6dbae86775652afc0382feed4cfe29.svg
          fullname: Sophie Aminu
          isHf: false
          isPro: false
          name: Chie2727
          type: user
        html: '<p>If you see this line comment it out:<br>torch.set_default_tensor_type(torch.cuda.HalfTensor)</p>

          '
        raw: 'If you see this line comment it out:

          torch.set_default_tensor_type(torch.cuda.HalfTensor)'
        updatedAt: '2023-08-14T05:44:11.920Z'
      numEdits: 0
      reactions: []
    id: 64d9bf2bbab152b247309c23
    type: comment
  author: Chie2727
  content: 'If you see this line comment it out:

    torch.set_default_tensor_type(torch.cuda.HalfTensor)'
  created_at: 2023-08-14 04:44:11+00:00
  edited: false
  hidden: false
  id: 64d9bf2bbab152b247309c23
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b1dc889ab30eae63b5ed1d38a1f8ae50.svg
      fullname: Rahul Chintamani Muninarayanappa VenkateshMurthy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RahulCmv
      type: user
    createdAt: '2023-08-18T02:12:09.000Z'
    data:
      edited: false
      editors:
      - RahulCmv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9361584186553955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b1dc889ab30eae63b5ed1d38a1f8ae50.svg
          fullname: Rahul Chintamani Muninarayanappa VenkateshMurthy
          isHf: false
          isPro: false
          name: RahulCmv
          type: user
        html: '<p>Where do we find that line? which file?</p>

          '
        raw: Where do we find that line? which file?
        updatedAt: '2023-08-18T02:12:09.732Z'
      numEdits: 0
      reactions: []
    id: 64ded3792802bd66a8acabdd
    type: comment
  author: RahulCmv
  content: Where do we find that line? which file?
  created_at: 2023-08-18 01:12:09+00:00
  edited: false
  hidden: false
  id: 64ded3792802bd66a8acabdd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd6dbae86775652afc0382feed4cfe29.svg
      fullname: Sophie Aminu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Chie2727
      type: user
    createdAt: '2023-08-24T01:12:05.000Z'
    data:
      edited: false
      editors:
      - Chie2727
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6442107558250427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd6dbae86775652afc0382feed4cfe29.svg
          fullname: Sophie Aminu
          isHf: false
          isPro: false
          name: Chie2727
          type: user
        html: "<p>If you downloaded the model directly from Meta, there should be\
          \ this python script: /llama/llama/generation.py<br>You can either comment\
          \ out line 100, or update it to be:</p>\n<pre><code>    if torch.cuda.is_available():\n\
          \        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n</code></pre>\n\
          <p>so that half-tensors are not used if you don't have CUDA.</p>\n"
        raw: "If you downloaded the model directly from Meta, there should be this\
          \ python script: /llama/llama/generation.py\nYou can either comment out\
          \ line 100, or update it to be:\n\n        if torch.cuda.is_available():\n\
          \            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n\nso\
          \ that half-tensors are not used if you don't have CUDA."
        updatedAt: '2023-08-24T01:12:05.336Z'
      numEdits: 0
      reactions: []
    id: 64e6ae658a431a73c81c491f
    type: comment
  author: Chie2727
  content: "If you downloaded the model directly from Meta, there should be this python\
    \ script: /llama/llama/generation.py\nYou can either comment out line 100, or\
    \ update it to be:\n\n        if torch.cuda.is_available():\n            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n\
    \nso that half-tensors are not used if you don't have CUDA."
  created_at: 2023-08-24 00:12:05+00:00
  edited: false
  hidden: false
  id: 64e6ae658a431a73c81c491f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e943c0fb45298c9484a629c39a1b298.svg
      fullname: Tapan Anand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tapananand
      type: user
    createdAt: '2023-09-16T13:37:45.000Z'
    data:
      edited: false
      editors:
      - tapananand
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9124667644500732
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e943c0fb45298c9484a629c39a1b298.svg
          fullname: Tapan Anand
          isHf: false
          isPro: false
          name: tapananand
          type: user
        html: '<p>It would be helpful if you can paste the code that gave you this
          error. I was getting this same error when trying out the code present in
          this tutorial: <a href="https://huggingface.co/blog/llama2">https://huggingface.co/blog/llama2</a>
          </p>

          <p>For me, changing torch_dtype from <code>torch.float16</code> to <code>torch.bfloat16</code>
          fixed the issue.</p>

          '
        raw: "It would be helpful if you can paste the code that gave you this error.\
          \ I was getting this same error when trying out the code present in this\
          \ tutorial: https://huggingface.co/blog/llama2 \n\nFor me, changing torch_dtype\
          \ from `torch.float16` to `torch.bfloat16` fixed the issue."
        updatedAt: '2023-09-16T13:37:45.486Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - dgarnitz
        - Yixuan0248
    id: 6505afa9a226ecc608ba494f
    type: comment
  author: tapananand
  content: "It would be helpful if you can paste the code that gave you this error.\
    \ I was getting this same error when trying out the code present in this tutorial:\
    \ https://huggingface.co/blog/llama2 \n\nFor me, changing torch_dtype from `torch.float16`\
    \ to `torch.bfloat16` fixed the issue."
  created_at: 2023-09-16 12:37:45+00:00
  edited: false
  hidden: false
  id: 6505afa9a226ecc608ba494f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c940c4b10780f09b4ee25324605fdde1.svg
      fullname: Jon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wingsuiting
      type: user
    createdAt: '2023-09-18T13:51:37.000Z'
    data:
      edited: false
      editors:
      - wingsuiting
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.758797824382782
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c940c4b10780f09b4ee25324605fdde1.svg
          fullname: Jon
          isHf: false
          isPro: false
          name: wingsuiting
          type: user
        html: '<p>For me this replicated the issue in colab:<br><a rel="nofollow"
          href="https://colab.research.google.com/drive/1SDN3rJhyL9EpDWuDVjyE3lJ6hV0Cfdd-?usp=sharing">https://colab.research.google.com/drive/1SDN3rJhyL9EpDWuDVjyE3lJ6hV0Cfdd-?usp=sharing</a></p>

          <p>The error was not resolved by changing torch_dtype from torch.float16
          to torch.bfloat16</p>

          '
        raw: 'For me this replicated the issue in colab:

          https://colab.research.google.com/drive/1SDN3rJhyL9EpDWuDVjyE3lJ6hV0Cfdd-?usp=sharing


          The error was not resolved by changing torch_dtype from torch.float16 to
          torch.bfloat16'
        updatedAt: '2023-09-18T13:51:37.793Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dgarnitz
    id: 650855e995c4559685498d84
    type: comment
  author: wingsuiting
  content: 'For me this replicated the issue in colab:

    https://colab.research.google.com/drive/1SDN3rJhyL9EpDWuDVjyE3lJ6hV0Cfdd-?usp=sharing


    The error was not resolved by changing torch_dtype from torch.float16 to torch.bfloat16'
  created_at: 2023-09-18 12:51:37+00:00
  edited: false
  hidden: false
  id: 650855e995c4559685498d84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7e2ab033e7197184ef7a1657afbd248.svg
      fullname: Valerio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: meValerio
      type: user
    createdAt: '2023-10-10T16:57:16.000Z'
    data:
      edited: true
      editors:
      - meValerio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6970328688621521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7e2ab033e7197184ef7a1657afbd248.svg
          fullname: Valerio
          isHf: false
          isPro: false
          name: meValerio
          type: user
        html: "<p>This is the code that generated the error. If you have a solution,\
          \ may you also describe the proposed solution's rationale?</p>\n<pre><code>from\
          \ transformers import AutoTokenizer\nimport transformers\nimport torch\n\
          \nmodel = \"meta-llama/Llama-2-70b-chat-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences\
          \ = pipeline(\n    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do\
          \ you have any recommendations of other shows I might like?\\n',\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n</code></pre>\n"
        raw: "This is the code that generated the error. If you have a solution, may\
          \ you also describe the proposed solution's rationale?\n\n```\nfrom transformers\
          \ import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"\
          meta-llama/Llama-2-70b-chat-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\
          \npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences\
          \ = pipeline(\n    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do\
          \ you have any recommendations of other shows I might like?\\n',\n    do_sample=True,\n\
          \    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          \    max_length=200,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\"\
          )\n```"
        updatedAt: '2023-10-10T16:58:17.619Z'
      numEdits: 2
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - mwgupta
        - tarou02
        - radusl
        - pzawierucha
        - yaanhaan
    id: 6525826c0251cee15a4f1669
    type: comment
  author: meValerio
  content: "This is the code that generated the error. If you have a solution, may\
    \ you also describe the proposed solution's rationale?\n\n```\nfrom transformers\
    \ import AutoTokenizer\nimport transformers\nimport torch\n\nmodel = \"meta-llama/Llama-2-70b-chat-hf\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model)\n\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n\
    \    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'I liked \"Breaking\
    \ Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows\
    \ I might like?\\n',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
    \    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq in\
    \ sequences:\n    print(f\"Result: {seq['generated_text']}\")\n```"
  created_at: 2023-10-10 15:57:16+00:00
  edited: true
  hidden: false
  id: 6525826c0251cee15a4f1669
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3aaae93925a03b2072ee7d3e867147c6.svg
      fullname: Fatima Zahid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fatimazahid1
      type: user
    createdAt: '2023-10-17T06:59:56.000Z'
    data:
      edited: false
      editors:
      - fatimazahid1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.802655816078186
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3aaae93925a03b2072ee7d3e867147c6.svg
          fullname: Fatima Zahid
          isHf: false
          isPro: false
          name: fatimazahid1
          type: user
        html: "<blockquote>\n<p>This is the code that generated the error. If you\
          \ have a solution, may you also describe the proposed solution's rationale?</p>\n\
          <pre><code>from transformers import AutoTokenizer\nimport transformers\n\
          import torch\n\nmodel = \"meta-llama/Llama-2-70b-chat-hf\"\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model)\n\npipeline = transformers.pipeline(\n\
          \    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n\
          \    device_map=\"auto\",\n)\n\nsequences = pipeline(\n    'I liked \"Breaking\
          \ Bad\" and \"Band of Brothers\". Do you have any recommendations of other\
          \ shows I might like?\\n',\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n    max_length=200,\n)\nfor seq\
          \ in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</code></pre>\n\
          </blockquote>\n<p>same issue, and by changing torch16 to torch32, its taking\
          \ forever to load, and consumes 99% of the RAM space and the notebook stops\
          \ then. If anyone knows why is it happening and any solution to it. Please\
          \ let me know. Thankyou!</p>\n"
        raw: "> This is the code that generated the error. If you have a solution,\
          \ may you also describe the proposed solution's rationale?\n> \n> ```\n\
          > from transformers import AutoTokenizer\n> import transformers\n> import\
          \ torch\n> \n> model = \"meta-llama/Llama-2-70b-chat-hf\"\n> \n> tokenizer\
          \ = AutoTokenizer.from_pretrained(model)\n> \n> pipeline = transformers.pipeline(\n\
          >     \"text-generation\",\n>     model=model,\n>     torch_dtype=torch.float16,\n\
          >     device_map=\"auto\",\n> )\n> \n> sequences = pipeline(\n>     'I liked\
          \ \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations\
          \ of other shows I might like?\\n',\n>     do_sample=True,\n>     top_k=10,\n\
          >     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id,\n\
          >     max_length=200,\n> )\n> for seq in sequences:\n>     print(f\"Result:\
          \ {seq['generated_text']}\")\n> ```\n\n\nsame issue, and by changing torch16\
          \ to torch32, its taking forever to load, and consumes 99% of the RAM space\
          \ and the notebook stops then. If anyone knows why is it happening and any\
          \ solution to it. Please let me know. Thankyou!\n"
        updatedAt: '2023-10-17T06:59:56.989Z'
      numEdits: 0
      reactions: []
    id: 652e30ec656cca7ce9ff2b81
    type: comment
  author: fatimazahid1
  content: "> This is the code that generated the error. If you have a solution, may\
    \ you also describe the proposed solution's rationale?\n> \n> ```\n> from transformers\
    \ import AutoTokenizer\n> import transformers\n> import torch\n> \n> model = \"\
    meta-llama/Llama-2-70b-chat-hf\"\n> \n> tokenizer = AutoTokenizer.from_pretrained(model)\n\
    > \n> pipeline = transformers.pipeline(\n>     \"text-generation\",\n>     model=model,\n\
    >     torch_dtype=torch.float16,\n>     device_map=\"auto\",\n> )\n> \n> sequences\
    \ = pipeline(\n>     'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you\
    \ have any recommendations of other shows I might like?\\n',\n>     do_sample=True,\n\
    >     top_k=10,\n>     num_return_sequences=1,\n>     eos_token_id=tokenizer.eos_token_id,\n\
    >     max_length=200,\n> )\n> for seq in sequences:\n>     print(f\"Result: {seq['generated_text']}\"\
    )\n> ```\n\n\nsame issue, and by changing torch16 to torch32, its taking forever\
    \ to load, and consumes 99% of the RAM space and the notebook stops then. If anyone\
    \ knows why is it happening and any solution to it. Please let me know. Thankyou!\n"
  created_at: 2023-10-17 05:59:56+00:00
  edited: false
  hidden: false
  id: 652e30ec656cca7ce9ff2b81
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: openlm-research/open_llama_7b_v2
repo_type: model
status: open
target_branch: null
title: 'example code returns RuntimeError: "addmm_impl_cpu_" not implemented for ''Half'''
