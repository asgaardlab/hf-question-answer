!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GamingDaveUK
conflicting_files: null
created_at: 2023-06-04 22:52:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87695e1fdec40517ac6506c6a9f3521a.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GamingDaveUK
      type: user
    createdAt: '2023-06-04T23:52:58.000Z'
    data:
      edited: false
      editors:
      - GamingDaveUK
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.954329252243042
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87695e1fdec40517ac6506c6a9f3521a.svg
          fullname: David
          isHf: false
          isPro: false
          name: GamingDaveUK
          type: user
        html: '<p>I see "It was created without group_size to minimise VRAM usage,
          and with --act-order to improve inference quality."<br>I have been using
          koboldai for story creation as you can easierly influence how you want the
          next sentence to start etc....but I am a novice.... hell i only got kobold
          to even work with 4bit models a couple of days ago (needs a special branch
          and you have to rename the safetensor file to 4bit.safetensor).<br>Not at
          home yet but going to give it a try when I am...but how do I add --act.order?
          Also I note someone said it used almost all of thier 3090 vram, I have seen
          that slows generation down in some models i have tried as the vram maxes
          out, is there any tricks to getting it to use a bit less.</p>

          <p>On a side note, Thank you for posting these, though i may not know how
          to use the models with out configs etc, the ones i can use have worked really
          well.</p>

          '
        raw: "I see \"It was created without group_size to minimise VRAM usage, and\
          \ with --act-order to improve inference quality.\" \r\nI have been using\
          \ koboldai for story creation as you can easierly influence how you want\
          \ the next sentence to start etc....but I am a novice.... hell i only got\
          \ kobold to even work with 4bit models a couple of days ago (needs a special\
          \ branch and you have to rename the safetensor file to 4bit.safetensor).\r\
          \nNot at home yet but going to give it a try when I am...but how do I add\
          \ --act.order? Also I note someone said it used almost all of thier 3090\
          \ vram, I have seen that slows generation down in some models i have tried\
          \ as the vram maxes out, is there any tricks to getting it to use a bit\
          \ less.\r\n\r\nOn a side note, Thank you for posting these, though i may\
          \ not know how to use the models with out configs etc, the ones i can use\
          \ have worked really well."
        updatedAt: '2023-06-04T23:52:58.476Z'
      numEdits: 0
      reactions: []
    id: 647d23da36e109abce3fb125
    type: comment
  author: GamingDaveUK
  content: "I see \"It was created without group_size to minimise VRAM usage, and\
    \ with --act-order to improve inference quality.\" \r\nI have been using koboldai\
    \ for story creation as you can easierly influence how you want the next sentence\
    \ to start etc....but I am a novice.... hell i only got kobold to even work with\
    \ 4bit models a couple of days ago (needs a special branch and you have to rename\
    \ the safetensor file to 4bit.safetensor).\r\nNot at home yet but going to give\
    \ it a try when I am...but how do I add --act.order? Also I note someone said\
    \ it used almost all of thier 3090 vram, I have seen that slows generation down\
    \ in some models i have tried as the vram maxes out, is there any tricks to getting\
    \ it to use a bit less.\r\n\r\nOn a side note, Thank you for posting these, though\
    \ i may not know how to use the models with out configs etc, the ones i can use\
    \ have worked really well."
  created_at: 2023-06-04 22:52:58+00:00
  edited: false
  hidden: false
  id: 647d23da36e109abce3fb125
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T00:18:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.970254123210907
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve not yet tried Kobald myself but at least with other UIs you
          shouldn''t need to do anything regarding act-order.  It should just work
          automatically.</p>

          <p>And yes it''s expected that a 30B model will fill a 24GB card.  In my
          testing I had 13MB VRAM free after generating 2000 tokens with a 30B model!  So
          there is no margin for error at all.</p>

          <p>This also means that if you''re using your GPU with a display, you will
          likely go out of memory.  With 30B need to have a GPU that''s only used
          for inference, and isn''t also being used by the OS to drive monitors.</p>

          <p>If that''s not the case for you then your options are:</p>

          <ol>

          <li>Use a 13B model instead</li>

          <li>Use a GGML 30B instead, with layers offloaded to GPU.  This will be
          slower than the GPTQ - perhaps half the inference speed - but it won''t
          ever go out of VRAM.  This is the method people use when they want to use
          a model bigger than they have VRAM for.</li>

          </ol>

          <p>For that you would use KobaldCpp which I also have no experience of,
          but I believe it works well.  You''d just need to confirm it works with
          CUDA GPU offload.</p>

          '
        raw: 'I''ve not yet tried Kobald myself but at least with other UIs you shouldn''t
          need to do anything regarding act-order.  It should just work automatically.


          And yes it''s expected that a 30B model will fill a 24GB card.  In my testing
          I had 13MB VRAM free after generating 2000 tokens with a 30B model!  So
          there is no margin for error at all.


          This also means that if you''re using your GPU with a display, you will
          likely go out of memory.  With 30B need to have a GPU that''s only used
          for inference, and isn''t also being used by the OS to drive monitors.


          If that''s not the case for you then your options are:

          1. Use a 13B model instead

          2. Use a GGML 30B instead, with layers offloaded to GPU.  This will be slower
          than the GPTQ - perhaps half the inference speed - but it won''t ever go
          out of VRAM.  This is the method people use when they want to use a model
          bigger than they have VRAM for.


          For that you would use KobaldCpp which I also have no experience of, but
          I believe it works well.  You''d just need to confirm it works with CUDA
          GPU offload.'
        updatedAt: '2023-06-05T00:18:20.949Z'
      numEdits: 0
      reactions: []
    id: 647d29cc60dfe0f35d6446f7
    type: comment
  author: TheBloke
  content: 'I''ve not yet tried Kobald myself but at least with other UIs you shouldn''t
    need to do anything regarding act-order.  It should just work automatically.


    And yes it''s expected that a 30B model will fill a 24GB card.  In my testing
    I had 13MB VRAM free after generating 2000 tokens with a 30B model!  So there
    is no margin for error at all.


    This also means that if you''re using your GPU with a display, you will likely
    go out of memory.  With 30B need to have a GPU that''s only used for inference,
    and isn''t also being used by the OS to drive monitors.


    If that''s not the case for you then your options are:

    1. Use a 13B model instead

    2. Use a GGML 30B instead, with layers offloaded to GPU.  This will be slower
    than the GPTQ - perhaps half the inference speed - but it won''t ever go out of
    VRAM.  This is the method people use when they want to use a model bigger than
    they have VRAM for.


    For that you would use KobaldCpp which I also have no experience of, but I believe
    it works well.  You''d just need to confirm it works with CUDA GPU offload.'
  created_at: 2023-06-04 23:18:20+00:00
  edited: false
  hidden: false
  id: 647d29cc60dfe0f35d6446f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87695e1fdec40517ac6506c6a9f3521a.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GamingDaveUK
      type: user
    createdAt: '2023-06-05T01:20:27.000Z'
    data:
      edited: true
      editors:
      - GamingDaveUK
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9093661904335022
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87695e1fdec40517ac6506c6a9f3521a.svg
          fullname: David
          isHf: false
          isPro: false
          name: GamingDaveUK
          type: user
        html: '<p>I used this guide <a rel="nofollow" href="https://docs.alpindale.dev/local-installation-(gpu)/koboldai4bit/">https://docs.alpindale.dev/local-installation-(gpu)/koboldai4bit/</a>
          (though make sure you click use new ui, or try new ui before loading the
          model)and your <a href="https://huggingface.co/TheBloke/hippogriff-30b-chat-GPTQ">https://huggingface.co/TheBloke/hippogriff-30b-chat-GPTQ</a>
          model, its pretty good, if you tell the prompt the start of your story and
          leave the last sentence unfinished, it then finishes the sentence, sometimes
          adding more. If you type the name of a character and hit return it continues
          the sentence or makes the character say something. 80% of the time its been
          coherant.... the rest of the time it does go off on a tangent. but not played
          with it much, also had tavern ai loaded up and that uses koboldai as an
          api (oogabooga can also be used as an api for that), but thats more chat
          bot based.<br>Kobold does let you split the layers, though i loaded all
          60 layers of hippogriff into my gpu<br>Should get a chance to try this model
          in a bit, will let you know how i get on.</p>

          <p>windows 11: rtx 3090: plugged into a 2k? monitor:<br>model loaded in
          kobold took 17.9gb....not bad<br>this jumped to 19.3gb when generating and
          then sat at 18gb.<br>Ok so this is hardly a scientific test but so far good
          lol<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/yott9sr_vrSRGhComvcFV.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/yott9sr_vrSRGhComvcFV.png"></a></p>

          <p>Bellow is the story (really really short story) I created with the AI''s
          help. White is what i typed, yellow is its reply. in koboldai if your in
          story mode its best to end with an incomplete sentence, it uses the model
          to finish it and then generate text so seeing the white part end with "
          is not a mistake but me getting the ai to add dialog... the reason for the
          subject matter here is that i also submitted this to a youtuber I follow
          in his discord, so i tailored it to a scenario he often asks llm''s to write
          a short poem or story about when he tests llm''s out. </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/Oy1REDnYe-ph0T-yUWZuX.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/Oy1REDnYe-ph0T-yUWZuX.png"></a></p>

          <p>Vram went up to 22gb during some of the longer generations, but always
          returned to about 18.3gb I have yet to test tavernai with kobold using this
          model.... not sure it will work as the model is for story telling not chat...but
          vram use shouldnt change much. I did notice the model had a tendency to
          try and finish the story and it copied my tired typing of i instead of I.
          </p>

          <p>ok I hope the info is helpful and than you again for the model, this
          is my new favorite, really impressed</p>

          '
        raw: "I used this guide https://docs.alpindale.dev/local-installation-(gpu)/koboldai4bit/\
          \ (though make sure you click use new ui, or try new ui before loading the\
          \ model)and your https://huggingface.co/TheBloke/hippogriff-30b-chat-GPTQ\
          \ model, its pretty good, if you tell the prompt the start of your story\
          \ and leave the last sentence unfinished, it then finishes the sentence,\
          \ sometimes adding more. If you type the name of a character and hit return\
          \ it continues the sentence or makes the character say something. 80% of\
          \ the time its been coherant.... the rest of the time it does go off on\
          \ a tangent. but not played with it much, also had tavern ai loaded up and\
          \ that uses koboldai as an api (oogabooga can also be used as an api for\
          \ that), but thats more chat bot based.\nKobold does let you split the layers,\
          \ though i loaded all 60 layers of hippogriff into my gpu\nShould get a\
          \ chance to try this model in a bit, will let you know how i get on.\n\n\
          windows 11: rtx 3090: plugged into a 2k? monitor: \nmodel loaded in kobold\
          \ took 17.9gb....not bad \nthis jumped to 19.3gb when generating and then\
          \ sat at 18gb. \nOk so this is hardly a scientific test but so far good\
          \ lol\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/yott9sr_vrSRGhComvcFV.png)\n\
          \nBellow is the story (really really short story) I created with the AI's\
          \ help. White is what i typed, yellow is its reply. in koboldai if your\
          \ in story mode its best to end with an incomplete sentence, it uses the\
          \ model to finish it and then generate text so seeing the white part end\
          \ with \" is not a mistake but me getting the ai to add dialog... the reason\
          \ for the subject matter here is that i also submitted this to a youtuber\
          \ I follow in his discord, so i tailored it to a scenario he often asks\
          \ llm's to write a short poem or story about when he tests llm's out. \n\
          \n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/Oy1REDnYe-ph0T-yUWZuX.png)\n\
          \nVram went up to 22gb during some of the longer generations, but always\
          \ returned to about 18.3gb I have yet to test tavernai with kobold using\
          \ this model.... not sure it will work as the model is for story telling\
          \ not chat...but vram use shouldnt change much. I did notice the model had\
          \ a tendency to try and finish the story and it copied my tired typing of\
          \ i instead of I. \n\nok I hope the info is helpful and than you again for\
          \ the model, this is my new favorite, really impressed"
        updatedAt: '2023-06-05T02:43:42.389Z'
      numEdits: 2
      reactions: []
    id: 647d385b83c62f32492e7fe2
    type: comment
  author: GamingDaveUK
  content: "I used this guide https://docs.alpindale.dev/local-installation-(gpu)/koboldai4bit/\
    \ (though make sure you click use new ui, or try new ui before loading the model)and\
    \ your https://huggingface.co/TheBloke/hippogriff-30b-chat-GPTQ model, its pretty\
    \ good, if you tell the prompt the start of your story and leave the last sentence\
    \ unfinished, it then finishes the sentence, sometimes adding more. If you type\
    \ the name of a character and hit return it continues the sentence or makes the\
    \ character say something. 80% of the time its been coherant.... the rest of the\
    \ time it does go off on a tangent. but not played with it much, also had tavern\
    \ ai loaded up and that uses koboldai as an api (oogabooga can also be used as\
    \ an api for that), but thats more chat bot based.\nKobold does let you split\
    \ the layers, though i loaded all 60 layers of hippogriff into my gpu\nShould\
    \ get a chance to try this model in a bit, will let you know how i get on.\n\n\
    windows 11: rtx 3090: plugged into a 2k? monitor: \nmodel loaded in kobold took\
    \ 17.9gb....not bad \nthis jumped to 19.3gb when generating and then sat at 18gb.\
    \ \nOk so this is hardly a scientific test but so far good lol\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/yott9sr_vrSRGhComvcFV.png)\n\
    \nBellow is the story (really really short story) I created with the AI's help.\
    \ White is what i typed, yellow is its reply. in koboldai if your in story mode\
    \ its best to end with an incomplete sentence, it uses the model to finish it\
    \ and then generate text so seeing the white part end with \" is not a mistake\
    \ but me getting the ai to add dialog... the reason for the subject matter here\
    \ is that i also submitted this to a youtuber I follow in his discord, so i tailored\
    \ it to a scenario he often asks llm's to write a short poem or story about when\
    \ he tests llm's out. \n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6369a47bf944bf39d83ab5bc/Oy1REDnYe-ph0T-yUWZuX.png)\n\
    \nVram went up to 22gb during some of the longer generations, but always returned\
    \ to about 18.3gb I have yet to test tavernai with kobold using this model....\
    \ not sure it will work as the model is for story telling not chat...but vram\
    \ use shouldnt change much. I did notice the model had a tendency to try and finish\
    \ the story and it copied my tired typing of i instead of I. \n\nok I hope the\
    \ info is helpful and than you again for the model, this is my new favorite, really\
    \ impressed"
  created_at: 2023-06-05 00:20:27+00:00
  edited: true
  hidden: false
  id: 647d385b83c62f32492e7fe2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-05T10:15:05.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9850049614906311
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great, thanks for the info and glad it''s working well for you !</p>

          '
        raw: Great, thanks for the info and glad it's working well for you !
        updatedAt: '2023-06-05T10:15:05.857Z'
      numEdits: 0
      reactions: []
    id: 647db5a910b7a3b157fdf1f8
    type: comment
  author: TheBloke
  content: Great, thanks for the info and glad it's working well for you !
  created_at: 2023-06-05 09:15:05+00:00
  edited: false
  hidden: false
  id: 647db5a910b7a3b157fdf1f8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ
repo_type: model
status: open
target_branch: null
title: KoboldAI and  --act-order?
