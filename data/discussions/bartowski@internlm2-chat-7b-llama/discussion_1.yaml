!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Liangmingxin
conflicting_files: null
created_at: 2024-01-19 23:21:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
      fullname: Will.Liang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Liangmingxin
      type: user
    createdAt: '2024-01-19T23:21:32.000Z'
    data:
      edited: false
      editors:
      - Liangmingxin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3815634250640869
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f83d777762ed82d42b46c/MgRpPSgGZPLOCo9AE4MuO.jpeg?w=200&h=200&f=face
          fullname: Will.Liang
          isHf: false
          isPro: false
          name: Liangmingxin
          type: user
        html: "<pre><code>(vllm) xx@DESKTOP-xx:~$ python /home/xx/vllm/vllm/entrypoints/openai/api_server.py\
          \ \\\n&gt; --model '/home/xx/internlm2-chat-7b-llama' \\\n&gt; --tokenizer\
          \ '/home/xx/internlm2-chat-7b-llama' \\\n&gt; --tokenizer-mode auto \\\n\
          &gt; --trust-remote-code \\\n&gt; --dtype float16 \\\n&gt; --enforce-eager\
          \ \\\n&gt; --tensor-parallel-size 2 \\\n&gt; --max-model-len 20000 \\\n\
          &gt; --chat-template 'vllm/examples/template_chatml.jinja' \\\n&gt; --worker-use-ray\
          \ \\\n&gt; --engine-use-ray \\\n&gt; --host 0.0.0.0 \\\n&gt; --port 6001\
          \ \\\n&gt; --disable-log-stats \\\n&gt; --disable-log-requests\nINFO 01-20\
          \ 06:51:14 api_server.py:742] args: Namespace(host='0.0.0.0', port=6001,\
          \ allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'],\
          \ allowed_headers=['*'], served_model_name=None, chat_template='vllm/examples/template_chatml.jinja',\
          \ response_role='assistant', ssl_keyfile=None, ssl_certfile=None, model='/home/xx/internlm2-chat-7b-llama',\
          \ tokenizer='/home/xx/internlm2-chat-7b-llama', revision=None, tokenizer_revision=None,\
          \ tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto',\
          \ dtype='float16', max_model_len=20000, worker_use_ray=True, pipeline_parallel_size=1,\
          \ tensor_parallel_size=2, max_parallel_loading_workers=None, block_size=16,\
          \ seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None,\
          \ max_num_seqs=256, max_paddings=256, disable_log_stats=True, quantization=None,\
          \ enforce_eager=True, max_context_len_to_capture=8192, engine_use_ray=True,\
          \ disable_log_requests=True, max_log_len=None)\n2024-01-20 06:51:17,483\
          \ INFO worker.py:1724 -- Started a local Ray instance.\nTraceback (most\
          \ recent call last):\n  File \"/home/xx/vllm/vllm/entrypoints/openai/api_server.py\"\
          , line 753, in &lt;module&gt;\n    engine_model_config = asyncio.run(engine.get_model_config())\n\
          \                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/runners.py\", line\
          \ 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n\
          \  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/runners.py\"\
          , line 118, in run\n    return self._loop.run_until_complete(task)\n   \
          \        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/base_events.py\"\
          , line 653, in run_until_complete\n    return future.result()\n        \
          \   ^^^^^^^^^^^^^^^\n  File \"/home/xx/vllm/vllm/engine/async_llm_engine.py\"\
          , line 484, in get_model_config\n    return await self.engine.get_model_config.remote()\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.RayActorError:\
          \ The actor died because of an error raised in its creation task, ray::_AsyncLLMEngine.__init__()\
          \ (pid=1065574, ip=172.20.4.244, actor_id=bb26c7f82bdf3238571ec97601000000,\
          \ repr=&lt;vllm.engine.async_llm_engine._AsyncLLMEngine object at 0x7fc5f2bc2550&gt;)\n\
          \  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 401, in __get_result\n    raise self._exception\n           ^^^^^^^^^^^^^^^^^^^^^\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/xx/vllm/vllm/engine/llm_engine.py\", line 95, in __init__\n\
          \    self.tokenizer = get_tokenizer(\n                     ^^^^^^^^^^^^^^\n\
          \  File \"/home/xx/vllm/vllm/transformers_utils/tokenizer.py\", line 28,\
          \ in get_tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n   \
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 770, in from_pretrained\n    tokenizer_class = get_class_from_dynamic_module(class_ref,\
          \ pretrained_model_name_or_path, **kwargs)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n\
          \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 294, in get_cached_module_file\n    resolved_module_file = cached_file(\n\
          \                           ^^^^^^^^^^^^\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/utils/hub.py\"\
          , line 360, in cached_file\n    raise EnvironmentError(\nOSError: /home/xx/internlm2-chat-7b-llama\
          \ does not appear to have a file named tokenization_internlm.py. Checkout\
          \ 'https://huggingface.co//home/xx/internlm2-chat-7b-llama/None' for available\
          \ files.\n(_AsyncLLMEngine pid=1065574) Could not locate the tokenization_internlm.py\
          \ inside /home/xx/internlm2-chat-7b-llama.\n(_AsyncLLMEngine pid=1065574)\
          \ Exception raised in creation task: The actor died because of an error\
          \ raised in its creation task, ray::_AsyncLLMEngine.__init__() (pid=1065574,\
          \ ip=172.20.4.244, actor_id=bb26c7f82bdf3238571ec97601000000, repr=&lt;vllm.engine.async_llm_engine._AsyncLLMEngine\
          \ object at 0x7fc5f2bc2550&gt;)\n(_AsyncLLMEngine pid=1065574)   File \"\
          /home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 449, in result\n(_AsyncLLMEngine pid=1065574)     return self.__get_result()\n\
          (_AsyncLLMEngine pid=1065574)            ^^^^^^^^^^^^^^^^^^^\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 401, in __get_result\n(_AsyncLLMEngine pid=1065574)     raise self._exception\n\
          (_AsyncLLMEngine pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^\n(_AsyncLLMEngine\
          \ pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(_AsyncLLMEngine\
          \ pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/vllm/vllm/engine/llm_engine.py\", line\
          \ 95, in __init__\n(_AsyncLLMEngine pid=1065574)     self.tokenizer = get_tokenizer(\n\
          (_AsyncLLMEngine pid=1065574)                      ^^^^^^^^^^^^^^\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/vllm/vllm/transformers_utils/tokenizer.py\"\
          , line 28, in get_tokenizer\n(_AsyncLLMEngine pid=1065574)     tokenizer\
          \ = AutoTokenizer.from_pretrained(\n(_AsyncLLMEngine pid=1065574)      \
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n(_AsyncLLMEngine pid=1065574)\
          \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 770, in from_pretrained\n(_AsyncLLMEngine pid=1065574)     tokenizer_class\
          \ = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path,\
          \ **kwargs)\n(_AsyncLLMEngine pid=1065574)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          (_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\n(_AsyncLLMEngine pid=1065574)\
          \     final_module = get_cached_module_file(\n(_AsyncLLMEngine pid=1065574)\
          \                    ^^^^^^^^^^^^^^^^^^^^^^^\n(_AsyncLLMEngine pid=1065574)\
          \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 294, in get_cached_module_file\n(_AsyncLLMEngine pid=1065574)   \
          \  resolved_module_file = cached_file(\n(_AsyncLLMEngine pid=1065574)  \
          \                          ^^^^^^^^^^^^\n(_AsyncLLMEngine pid=1065574) \
          \  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/utils/hub.py\"\
          , line 360, in cached_file\n(_AsyncLLMEngine pid=1065574)     raise EnvironmentError(\n\
          (_AsyncLLMEngine pid=1065574) OSError: /home/xx/internlm2-chat-7b-llama\
          \ does not appear to have a file named tokenization_internlm.py. Checkout\
          \ 'https://huggingface.co//home/xx/internlm2-chat-7b-llama/None' for available\
          \ files.\n(_AsyncLLMEngine pid=1065574) INFO 01-20 06:51:19 llm_engine.py:70]\
          \ Initializing an LLM engine with config: model='/home/xx/internlm2-chat-7b-llama',\
          \ tokenizer='/home/xx/internlm2-chat-7b-llama', tokenizer_mode=auto, revision=None,\
          \ tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16,\
          \ max_seq_len=20000, download_dir=None, load_format=auto, tensor_parallel_size=2,\
          \ quantization=None, enforce_eager=True, seed=0)\n(_AsyncLLMEngine pid=1065574)\
          \ [2024-01-20 06:51:19,081 E 1065574 1065574] logging.cc:104: Stack trace:\n\
          (_AsyncLLMEngine pid=1065574)  /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0xfe925a)\
          \ [0x7fc848e0a25a] ray::operator&lt;&lt;()\n(_AsyncLLMEngine pid=1065574)\
          \ /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0xfeb998)\
          \ [0x7fc848e0c998] ray::TerminateHandler()\n(_AsyncLLMEngine pid=1065574)\
          \ /home/xx/anaconda3/envs/vllm/bin/../lib/libstdc++.so.6(+0xb135a) [0x7fc847c9b35a]\
          \ __cxxabiv1::__terminate()\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/bin/../lib/libstdc++.so.6(+0xb13c5)\
          \ [0x7fc847c9b3c5]\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x7c92d0)\
          \ [0x7fc8485ea2d0] std::thread::_State_impl&lt;&gt;::~_State_impl()\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
          \ [0x7fc84844f5fa] std::_Sp_counted_base&lt;&gt;::_M_release()\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x7b22d2)\
          \ [0x7fc8485d32d2] std::_Sp_counted_ptr_inplace&lt;&gt;::_M_dispose()\n\
          (_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
          \ [0x7fc84844f5fa] std::_Sp_counted_base&lt;&gt;::_M_release()\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x6e9732)\
          \ [0x7fc84850a732] std::default_delete&lt;&gt;::operator()()\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorkerD1Ev+0xeb)\
          \ [0x7fc84857c6db] ray::core::CoreWorker::~CoreWorker()\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
          \ [0x7fc84844f5fa] std::_Sp_counted_base&lt;&gt;::_M_release()\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv+0x134)\
          \ [0x7fc8485ba854] ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n\
          (_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv+0x1d)\
          \ [0x7fc8485ba95d] ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n\
          (_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x5ae947)\
          \ [0x7fc8483cf947] __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n\
          (_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine() [0x535490] method_vectorcall_NOARGS\n\
          (_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(PyObject_Vectorcall+0x31)\
          \ [0x51bff1] PyObject_Vectorcall\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyEval_EvalFrameDefault+0x755)\
          \ [0x50f025] _PyEval_EvalFrameDefault\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5c82ce] _PyEval_Vector\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(PyEval_EvalCode+0x9f)\
          \ [0x5c79cf] PyEval_EvalCode\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5e8807] run_eval_code_obj\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5e4e40] run_mod\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5f9132] pyrun_file\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyRun_SimpleFileObject+0x19f)\
          \ [0x5f871f] _PyRun_SimpleFileObject\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyRun_AnyFileObject+0x43)\
          \ [0x5f8473] _PyRun_AnyFileObject\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(Py_RunMain+0x2ee)\
          \ [0x5f2fee] Py_RunMain\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(Py_BytesMain+0x39)\
          \ [0x5b6e19] Py_BytesMain\n(_AsyncLLMEngine pid=1065574) /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)\
          \ [0x7fc849f87083] __libc_start_main\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5b6c6f]\n(_AsyncLLMEngine pid=1065574)\n(_AsyncLLMEngine pid=1065574)\
          \ *** SIGABRT received at time=1705704679 on cpu 13 ***\n(_AsyncLLMEngine\
          \ pid=1065574) PC: @     0x7fc849fa600b  (unknown)  raise\n(_AsyncLLMEngine\
          \ pid=1065574)     @     0x7fc84a2c3420   74038176  (unknown)\n(_AsyncLLMEngine\
          \ pid=1065574)     @     0x7fc847c9b35a         80  __cxxabiv1::__terminate()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base&lt;&gt;::_M_release()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc8485d32d2         96  std::_Sp_counted_ptr_inplace&lt;&gt;::_M_dispose()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base&lt;&gt;::_M_release()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc84850a732        144  std::default_delete&lt;&gt;::operator()()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc84857c6db        128  ray::core::CoreWorker::~CoreWorker()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base&lt;&gt;::_M_release()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc8485ba854        112  ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc8485ba95d         32  ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n\
          (_AsyncLLMEngine pid=1065574)     @     0x7fc8483cf947         32  __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n\
          (_AsyncLLMEngine pid=1065574)     @           0x535490  (unknown)  method_vectorcall_NOARGS\n\
          (_AsyncLLMEngine pid=1065574)     @           0x875180  (unknown)  (unknown)\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,086 E 1065574 1065574]\
          \ logging.cc:361: *** SIGABRT received at time=1705704679 on cpu 13 ***\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,086 E 1065574 1065574]\
          \ logging.cc:361: PC: @     0x7fc849fa600b  (unknown)  raise\n(_AsyncLLMEngine\
          \ pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
          \     @     0x7fc84a2c3420   74038176  (unknown)\n(_AsyncLLMEngine pid=1065574)\
          \ [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:     @    \
          \ 0x7fc847c9b35a         80  __cxxabiv1::__terminate()\n(_AsyncLLMEngine\
          \ pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
          \     @     0x7fc84844f5fa         32  std::_Sp_counted_base&lt;&gt;::_M_release()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8485d32d2         96  std::_Sp_counted_ptr_inplace&lt;&gt;::_M_dispose()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84844f5fa         32  std::_Sp_counted_base&lt;&gt;::_M_release()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84850a732        144  std::default_delete&lt;&gt;::operator()()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84857c6db        128  ray::core::CoreWorker::~CoreWorker()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84844f5fa         32  std::_Sp_counted_base&lt;&gt;::_M_release()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8485ba854        112  ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8485ba95d         32  ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8483cf947         32  __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @           0x535490  (unknown)  method_vectorcall_NOARGS\n\
          (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @           0x875180  (unknown)  (unknown)\n(_AsyncLLMEngine\
          \ pid=1065574) Fatal Python error: Aborted\n(_AsyncLLMEngine pid=1065574)\n\
          (_AsyncLLMEngine pid=1065574) Stack (most recent call first):\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_private/worker.py\"\
          , line 847 in main_loop\n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_private/workers/default_worker.py\"\
          , line 282 in &lt;module&gt;\n(_AsyncLLMEngine pid=1065574)\n(_AsyncLLMEngine\
          \ pid=1065574) Extension modules: psutil._psutil_linux, psutil._psutil_posix,\
          \ msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md,\
          \ uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests,\
          \ numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common,\
          \ numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937,\
          \ numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64,\
          \ numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested,\
          \ torch._C._nn, torch._C._sparse, torch._C._special, pydantic.typing, pydantic.errors,\
          \ pydantic.version, pydantic.utils, pydantic.class_validators, pydantic.config,\
          \ pydantic.color, pydantic.datetime_parse, pydantic.validators, pydantic.networks,\
          \ pydantic.types, pydantic.json, pydantic.error_wrappers, pydantic.fields,\
          \ pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses,\
          \ pydantic.annotated_types, pydantic.decorator, pydantic.env_settings, pydantic.tools,\
          \ pydantic, sentencepiece._sentencepiece, pyarrow.lib, pyarrow._hdfsio,\
          \ pyarrow._json (total: 56)\n</code></pre>\n<p>It seems that there is little\
          \ official interest in adapting the vllm and llama frameworks, checking\
          \ the llm leaderboards it's about 68-70 points or so, well, sadly it's not\
          \ possible to continue experiencing the model...</p>\n"
        raw: "```\r\n(vllm) xx@DESKTOP-xx:~$ python /home/xx/vllm/vllm/entrypoints/openai/api_server.py\
          \ \\\r\n> --model '/home/xx/internlm2-chat-7b-llama' \\\r\n> --tokenizer\
          \ '/home/xx/internlm2-chat-7b-llama' \\\r\n> --tokenizer-mode auto \\\r\n\
          > --trust-remote-code \\\r\n> --dtype float16 \\\r\n> --enforce-eager \\\
          \r\n> --tensor-parallel-size 2 \\\r\n> --max-model-len 20000 \\\r\n> --chat-template\
          \ 'vllm/examples/template_chatml.jinja' \\\r\n> --worker-use-ray \\\r\n\
          > --engine-use-ray \\\r\n> --host 0.0.0.0 \\\r\n> --port 6001 \\\r\n> --disable-log-stats\
          \ \\\r\n> --disable-log-requests\r\nINFO 01-20 06:51:14 api_server.py:742]\
          \ args: Namespace(host='0.0.0.0', port=6001, allow_credentials=False, allowed_origins=['*'],\
          \ allowed_methods=['*'], allowed_headers=['*'], served_model_name=None,\
          \ chat_template='vllm/examples/template_chatml.jinja', response_role='assistant',\
          \ ssl_keyfile=None, ssl_certfile=None, model='/home/xx/internlm2-chat-7b-llama',\
          \ tokenizer='/home/xx/internlm2-chat-7b-llama', revision=None, tokenizer_revision=None,\
          \ tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto',\
          \ dtype='float16', max_model_len=20000, worker_use_ray=True, pipeline_parallel_size=1,\
          \ tensor_parallel_size=2, max_parallel_loading_workers=None, block_size=16,\
          \ seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None,\
          \ max_num_seqs=256, max_paddings=256, disable_log_stats=True, quantization=None,\
          \ enforce_eager=True, max_context_len_to_capture=8192, engine_use_ray=True,\
          \ disable_log_requests=True, max_log_len=None)\r\n2024-01-20 06:51:17,483\
          \ INFO worker.py:1724 -- Started a local Ray instance.\r\nTraceback (most\
          \ recent call last):\r\n  File \"/home/xx/vllm/vllm/entrypoints/openai/api_server.py\"\
          , line 753, in <module>\r\n    engine_model_config = asyncio.run(engine.get_model_config())\r\
          \n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n \
          \ File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/runners.py\"\
          , line 190, in run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\
          \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/runners.py\"\
          , line 118, in run\r\n    return self._loop.run_until_complete(task)\r\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/base_events.py\"\
          , line 653, in run_until_complete\r\n    return future.result()\r\n    \
          \       ^^^^^^^^^^^^^^^\r\n  File \"/home/xx/vllm/vllm/engine/async_llm_engine.py\"\
          , line 484, in get_model_config\r\n    return await self.engine.get_model_config.remote()\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nray.exceptions.RayActorError:\
          \ The actor died because of an error raised in its creation task, ray::_AsyncLLMEngine.__init__()\
          \ (pid=1065574, ip=172.20.4.244, actor_id=bb26c7f82bdf3238571ec97601000000,\
          \ repr=<vllm.engine.async_llm_engine._AsyncLLMEngine object at 0x7fc5f2bc2550>)\r\
          \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 401, in __get_result\r\n    raise self._exception\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/home/xx/vllm/vllm/engine/llm_engine.py\", line 95, in __init__\r\
          \n    self.tokenizer = get_tokenizer(\r\n                     ^^^^^^^^^^^^^^\r\
          \n  File \"/home/xx/vllm/vllm/transformers_utils/tokenizer.py\", line 28,\
          \ in get_tokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n\
          \                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 770, in from_pretrained\r\n    tokenizer_class = get_class_from_dynamic_module(class_ref,\
          \ pretrained_model_name_or_path, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\r\n    final_module = get_cached_module_file(\r\
          \n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 294, in get_cached_module_file\r\n    resolved_module_file = cached_file(\r\
          \n                           ^^^^^^^^^^^^\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/utils/hub.py\"\
          , line 360, in cached_file\r\n    raise EnvironmentError(\r\nOSError: /home/xx/internlm2-chat-7b-llama\
          \ does not appear to have a file named tokenization_internlm.py. Checkout\
          \ 'https://huggingface.co//home/xx/internlm2-chat-7b-llama/None' for available\
          \ files.\r\n(_AsyncLLMEngine pid=1065574) Could not locate the tokenization_internlm.py\
          \ inside /home/xx/internlm2-chat-7b-llama.\r\n(_AsyncLLMEngine pid=1065574)\
          \ Exception raised in creation task: The actor died because of an error\
          \ raised in its creation task, ray::_AsyncLLMEngine.__init__() (pid=1065574,\
          \ ip=172.20.4.244, actor_id=bb26c7f82bdf3238571ec97601000000, repr=<vllm.engine.async_llm_engine._AsyncLLMEngine\
          \ object at 0x7fc5f2bc2550>)\r\n(_AsyncLLMEngine pid=1065574)   File \"\
          /home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 449, in result\r\n(_AsyncLLMEngine pid=1065574)     return self.__get_result()\r\
          \n(_AsyncLLMEngine pid=1065574)            ^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
          , line 401, in __get_result\r\n(_AsyncLLMEngine pid=1065574)     raise self._exception\r\
          \n(_AsyncLLMEngine pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
          \ pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
          \ pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/vllm/vllm/engine/llm_engine.py\", line\
          \ 95, in __init__\r\n(_AsyncLLMEngine pid=1065574)     self.tokenizer =\
          \ get_tokenizer(\r\n(_AsyncLLMEngine pid=1065574)                      ^^^^^^^^^^^^^^\r\
          \n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/vllm/vllm/transformers_utils/tokenizer.py\"\
          , line 28, in get_tokenizer\r\n(_AsyncLLMEngine pid=1065574)     tokenizer\
          \ = AutoTokenizer.from_pretrained(\r\n(_AsyncLLMEngine pid=1065574)    \
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine pid=1065574)\
          \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\"\
          , line 770, in from_pretrained\r\n(_AsyncLLMEngine pid=1065574)     tokenizer_class\
          \ = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path,\
          \ **kwargs)\r\n(_AsyncLLMEngine pid=1065574)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\r\n(_AsyncLLMEngine pid=1065574)\
          \     final_module = get_cached_module_file(\r\n(_AsyncLLMEngine pid=1065574)\
          \                    ^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine pid=1065574)\
          \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 294, in get_cached_module_file\r\n(_AsyncLLMEngine pid=1065574) \
          \    resolved_module_file = cached_file(\r\n(_AsyncLLMEngine pid=1065574)\
          \                            ^^^^^^^^^^^^\r\n(_AsyncLLMEngine pid=1065574)\
          \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/utils/hub.py\"\
          , line 360, in cached_file\r\n(_AsyncLLMEngine pid=1065574)     raise EnvironmentError(\r\
          \n(_AsyncLLMEngine pid=1065574) OSError: /home/xx/internlm2-chat-7b-llama\
          \ does not appear to have a file named tokenization_internlm.py. Checkout\
          \ 'https://huggingface.co//home/xx/internlm2-chat-7b-llama/None' for available\
          \ files.\r\n(_AsyncLLMEngine pid=1065574) INFO 01-20 06:51:19 llm_engine.py:70]\
          \ Initializing an LLM engine with config: model='/home/xx/internlm2-chat-7b-llama',\
          \ tokenizer='/home/xx/internlm2-chat-7b-llama', tokenizer_mode=auto, revision=None,\
          \ tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16,\
          \ max_seq_len=20000, download_dir=None, load_format=auto, tensor_parallel_size=2,\
          \ quantization=None, enforce_eager=True, seed=0)\r\n(_AsyncLLMEngine pid=1065574)\
          \ [2024-01-20 06:51:19,081 E 1065574 1065574] logging.cc:104: Stack trace:\r\
          \n(_AsyncLLMEngine pid=1065574)  /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0xfe925a)\
          \ [0x7fc848e0a25a] ray::operator<<()\r\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0xfeb998)\
          \ [0x7fc848e0c998] ray::TerminateHandler()\r\n(_AsyncLLMEngine pid=1065574)\
          \ /home/xx/anaconda3/envs/vllm/bin/../lib/libstdc++.so.6(+0xb135a) [0x7fc847c9b35a]\
          \ __cxxabiv1::__terminate()\r\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/bin/../lib/libstdc++.so.6(+0xb13c5)\
          \ [0x7fc847c9b3c5]\r\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x7c92d0)\
          \ [0x7fc8485ea2d0] std::thread::_State_impl<>::~_State_impl()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
          \ [0x7fc84844f5fa] std::_Sp_counted_base<>::_M_release()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x7b22d2)\
          \ [0x7fc8485d32d2] std::_Sp_counted_ptr_inplace<>::_M_dispose()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
          \ [0x7fc84844f5fa] std::_Sp_counted_base<>::_M_release()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x6e9732)\
          \ [0x7fc84850a732] std::default_delete<>::operator()()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorkerD1Ev+0xeb)\
          \ [0x7fc84857c6db] ray::core::CoreWorker::~CoreWorker()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
          \ [0x7fc84844f5fa] std::_Sp_counted_base<>::_M_release()\r\n(_AsyncLLMEngine\
          \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv+0x134)\
          \ [0x7fc8485ba854] ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\r\
          \n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv+0x1d)\
          \ [0x7fc8485ba95d] ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\r\
          \n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x5ae947)\
          \ [0x7fc8483cf947] __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\r\
          \n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine() [0x535490] method_vectorcall_NOARGS\r\
          \n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(PyObject_Vectorcall+0x31)\
          \ [0x51bff1] PyObject_Vectorcall\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyEval_EvalFrameDefault+0x755)\
          \ [0x50f025] _PyEval_EvalFrameDefault\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5c82ce] _PyEval_Vector\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(PyEval_EvalCode+0x9f)\
          \ [0x5c79cf] PyEval_EvalCode\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5e8807] run_eval_code_obj\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5e4e40] run_mod\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5f9132] pyrun_file\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyRun_SimpleFileObject+0x19f)\
          \ [0x5f871f] _PyRun_SimpleFileObject\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyRun_AnyFileObject+0x43)\
          \ [0x5f8473] _PyRun_AnyFileObject\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(Py_RunMain+0x2ee)\
          \ [0x5f2fee] Py_RunMain\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(Py_BytesMain+0x39)\
          \ [0x5b6e19] Py_BytesMain\r\n(_AsyncLLMEngine pid=1065574) /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)\
          \ [0x7fc849f87083] __libc_start_main\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
          \ [0x5b6c6f]\r\n(_AsyncLLMEngine pid=1065574)\r\n(_AsyncLLMEngine pid=1065574)\
          \ *** SIGABRT received at time=1705704679 on cpu 13 ***\r\n(_AsyncLLMEngine\
          \ pid=1065574) PC: @     0x7fc849fa600b  (unknown)  raise\r\n(_AsyncLLMEngine\
          \ pid=1065574)     @     0x7fc84a2c3420   74038176  (unknown)\r\n(_AsyncLLMEngine\
          \ pid=1065574)     @     0x7fc847c9b35a         80  __cxxabiv1::__terminate()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8485d32d2         96  std::_Sp_counted_ptr_inplace<>::_M_dispose()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84850a732        144  std::default_delete<>::operator()()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84857c6db        128  ray::core::CoreWorker::~CoreWorker()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8485ba854        112  ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8485ba95d         32  ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\r\
          \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8483cf947         32  __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\r\
          \n(_AsyncLLMEngine pid=1065574)     @           0x535490  (unknown)  method_vectorcall_NOARGS\r\
          \n(_AsyncLLMEngine pid=1065574)     @           0x875180  (unknown)  (unknown)\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,086 E 1065574 1065574]\
          \ logging.cc:361: *** SIGABRT received at time=1705704679 on cpu 13 ***\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,086 E 1065574 1065574]\
          \ logging.cc:361: PC: @     0x7fc849fa600b  (unknown)  raise\r\n(_AsyncLLMEngine\
          \ pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
          \     @     0x7fc84a2c3420   74038176  (unknown)\r\n(_AsyncLLMEngine pid=1065574)\
          \ [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:     @    \
          \ 0x7fc847c9b35a         80  __cxxabiv1::__terminate()\r\n(_AsyncLLMEngine\
          \ pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
          \     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8485d32d2         96  std::_Sp_counted_ptr_inplace<>::_M_dispose()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84850a732        144  std::default_delete<>::operator()()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84857c6db        128  ray::core::CoreWorker::~CoreWorker()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8485ba854        112  ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8485ba95d         32  ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @     0x7fc8483cf947         32  __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @           0x535490  (unknown)  method_vectorcall_NOARGS\r\
          \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574]\
          \ logging.cc:361:     @           0x875180  (unknown)  (unknown)\r\n(_AsyncLLMEngine\
          \ pid=1065574) Fatal Python error: Aborted\r\n(_AsyncLLMEngine pid=1065574)\r\
          \n(_AsyncLLMEngine pid=1065574) Stack (most recent call first):\r\n(_AsyncLLMEngine\
          \ pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_private/worker.py\"\
          , line 847 in main_loop\r\n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_private/workers/default_worker.py\"\
          , line 282 in <module>\r\n(_AsyncLLMEngine pid=1065574)\r\n(_AsyncLLMEngine\
          \ pid=1065574) Extension modules: psutil._psutil_linux, psutil._psutil_posix,\
          \ msgpack._cmsgpack, google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md,\
          \ uvloop.loop, ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests,\
          \ numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common,\
          \ numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937,\
          \ numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64,\
          \ numpy.random._generator, torch._C, torch._C._fft, torch._C._linalg, torch._C._nested,\
          \ torch._C._nn, torch._C._sparse, torch._C._special, pydantic.typing, pydantic.errors,\
          \ pydantic.version, pydantic.utils, pydantic.class_validators, pydantic.config,\
          \ pydantic.color, pydantic.datetime_parse, pydantic.validators, pydantic.networks,\
          \ pydantic.types, pydantic.json, pydantic.error_wrappers, pydantic.fields,\
          \ pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses,\
          \ pydantic.annotated_types, pydantic.decorator, pydantic.env_settings, pydantic.tools,\
          \ pydantic, sentencepiece._sentencepiece, pyarrow.lib, pyarrow._hdfsio,\
          \ pyarrow._json (total: 56)\r\n```\r\n\r\nIt seems that there is little\
          \ official interest in adapting the vllm and llama frameworks, checking\
          \ the llm leaderboards it's about 68-70 points or so, well, sadly it's not\
          \ possible to continue experiencing the model..."
        updatedAt: '2024-01-19T23:21:32.598Z'
      numEdits: 0
      reactions: []
    id: 65ab03fce2a2c863567d9806
    type: comment
  author: Liangmingxin
  content: "```\r\n(vllm) xx@DESKTOP-xx:~$ python /home/xx/vllm/vllm/entrypoints/openai/api_server.py\
    \ \\\r\n> --model '/home/xx/internlm2-chat-7b-llama' \\\r\n> --tokenizer '/home/xx/internlm2-chat-7b-llama'\
    \ \\\r\n> --tokenizer-mode auto \\\r\n> --trust-remote-code \\\r\n> --dtype float16\
    \ \\\r\n> --enforce-eager \\\r\n> --tensor-parallel-size 2 \\\r\n> --max-model-len\
    \ 20000 \\\r\n> --chat-template 'vllm/examples/template_chatml.jinja' \\\r\n>\
    \ --worker-use-ray \\\r\n> --engine-use-ray \\\r\n> --host 0.0.0.0 \\\r\n> --port\
    \ 6001 \\\r\n> --disable-log-stats \\\r\n> --disable-log-requests\r\nINFO 01-20\
    \ 06:51:14 api_server.py:742] args: Namespace(host='0.0.0.0', port=6001, allow_credentials=False,\
    \ allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name=None,\
    \ chat_template='vllm/examples/template_chatml.jinja', response_role='assistant',\
    \ ssl_keyfile=None, ssl_certfile=None, model='/home/xx/internlm2-chat-7b-llama',\
    \ tokenizer='/home/xx/internlm2-chat-7b-llama', revision=None, tokenizer_revision=None,\
    \ tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto',\
    \ dtype='float16', max_model_len=20000, worker_use_ray=True, pipeline_parallel_size=1,\
    \ tensor_parallel_size=2, max_parallel_loading_workers=None, block_size=16, seed=0,\
    \ swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256,\
    \ max_paddings=256, disable_log_stats=True, quantization=None, enforce_eager=True,\
    \ max_context_len_to_capture=8192, engine_use_ray=True, disable_log_requests=True,\
    \ max_log_len=None)\r\n2024-01-20 06:51:17,483 INFO worker.py:1724 -- Started\
    \ a local Ray instance.\r\nTraceback (most recent call last):\r\n  File \"/home/xx/vllm/vllm/entrypoints/openai/api_server.py\"\
    , line 753, in <module>\r\n    engine_model_config = asyncio.run(engine.get_model_config())\r\
    \n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
    /home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/runners.py\", line 190, in\
    \ run\r\n    return runner.run(main)\r\n           ^^^^^^^^^^^^^^^^\r\n  File\
    \ \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/runners.py\", line 118,\
    \ in run\r\n    return self._loop.run_until_complete(task)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/asyncio/base_events.py\"\
    , line 653, in run_until_complete\r\n    return future.result()\r\n          \
    \ ^^^^^^^^^^^^^^^\r\n  File \"/home/xx/vllm/vllm/engine/async_llm_engine.py\"\
    , line 484, in get_model_config\r\n    return await self.engine.get_model_config.remote()\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nray.exceptions.RayActorError:\
    \ The actor died because of an error raised in its creation task, ray::_AsyncLLMEngine.__init__()\
    \ (pid=1065574, ip=172.20.4.244, actor_id=bb26c7f82bdf3238571ec97601000000, repr=<vllm.engine.async_llm_engine._AsyncLLMEngine\
    \ object at 0x7fc5f2bc2550>)\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
    , line 449, in result\r\n    return self.__get_result()\r\n           ^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
    , line 401, in __get_result\r\n    raise self._exception\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
    \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/xx/vllm/vllm/engine/llm_engine.py\", line 95, in __init__\r\n\
    \    self.tokenizer = get_tokenizer(\r\n                     ^^^^^^^^^^^^^^\r\n\
    \  File \"/home/xx/vllm/vllm/transformers_utils/tokenizer.py\", line 28, in get_tokenizer\r\
    \n    tokenizer = AutoTokenizer.from_pretrained(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 770, in from_pretrained\r\n    tokenizer_class = get_class_from_dynamic_module(class_ref,\
    \ pretrained_model_name_or_path, **kwargs)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 488, in get_class_from_dynamic_module\r\n    final_module = get_cached_module_file(\r\
    \n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 294, in get_cached_module_file\r\n    resolved_module_file = cached_file(\r\
    \n                           ^^^^^^^^^^^^\r\n  File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/utils/hub.py\"\
    , line 360, in cached_file\r\n    raise EnvironmentError(\r\nOSError: /home/xx/internlm2-chat-7b-llama\
    \ does not appear to have a file named tokenization_internlm.py. Checkout 'https://huggingface.co//home/xx/internlm2-chat-7b-llama/None'\
    \ for available files.\r\n(_AsyncLLMEngine pid=1065574) Could not locate the tokenization_internlm.py\
    \ inside /home/xx/internlm2-chat-7b-llama.\r\n(_AsyncLLMEngine pid=1065574) Exception\
    \ raised in creation task: The actor died because of an error raised in its creation\
    \ task, ray::_AsyncLLMEngine.__init__() (pid=1065574, ip=172.20.4.244, actor_id=bb26c7f82bdf3238571ec97601000000,\
    \ repr=<vllm.engine.async_llm_engine._AsyncLLMEngine object at 0x7fc5f2bc2550>)\r\
    \n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
    , line 449, in result\r\n(_AsyncLLMEngine pid=1065574)     return self.__get_result()\r\
    \n(_AsyncLLMEngine pid=1065574)            ^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
    \ pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/concurrent/futures/_base.py\"\
    , line 401, in __get_result\r\n(_AsyncLLMEngine pid=1065574)     raise self._exception\r\
    \n(_AsyncLLMEngine pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
    \ pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
    \ pid=1065574)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
    \ pid=1065574)   File \"/home/xx/vllm/vllm/engine/llm_engine.py\", line 95, in\
    \ __init__\r\n(_AsyncLLMEngine pid=1065574)     self.tokenizer = get_tokenizer(\r\
    \n(_AsyncLLMEngine pid=1065574)                      ^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine\
    \ pid=1065574)   File \"/home/xx/vllm/vllm/transformers_utils/tokenizer.py\",\
    \ line 28, in get_tokenizer\r\n(_AsyncLLMEngine pid=1065574)     tokenizer = AutoTokenizer.from_pretrained(\r\
    \n(_AsyncLLMEngine pid=1065574)                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py\"\
    , line 770, in from_pretrained\r\n(_AsyncLLMEngine pid=1065574)     tokenizer_class\
    \ = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\r\
    \n(_AsyncLLMEngine pid=1065574)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 488, in get_class_from_dynamic_module\r\n(_AsyncLLMEngine pid=1065574)\
    \     final_module = get_cached_module_file(\r\n(_AsyncLLMEngine pid=1065574)\
    \                    ^^^^^^^^^^^^^^^^^^^^^^^\r\n(_AsyncLLMEngine pid=1065574)\
    \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 294, in get_cached_module_file\r\n(_AsyncLLMEngine pid=1065574)     resolved_module_file\
    \ = cached_file(\r\n(_AsyncLLMEngine pid=1065574)                            ^^^^^^^^^^^^\r\
    \n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/transformers/utils/hub.py\"\
    , line 360, in cached_file\r\n(_AsyncLLMEngine pid=1065574)     raise EnvironmentError(\r\
    \n(_AsyncLLMEngine pid=1065574) OSError: /home/xx/internlm2-chat-7b-llama does\
    \ not appear to have a file named tokenization_internlm.py. Checkout 'https://huggingface.co//home/xx/internlm2-chat-7b-llama/None'\
    \ for available files.\r\n(_AsyncLLMEngine pid=1065574) INFO 01-20 06:51:19 llm_engine.py:70]\
    \ Initializing an LLM engine with config: model='/home/xx/internlm2-chat-7b-llama',\
    \ tokenizer='/home/xx/internlm2-chat-7b-llama', tokenizer_mode=auto, revision=None,\
    \ tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=20000,\
    \ download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None,\
    \ enforce_eager=True, seed=0)\r\n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,081\
    \ E 1065574 1065574] logging.cc:104: Stack trace:\r\n(_AsyncLLMEngine pid=1065574)\
    \  /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0xfe925a)\
    \ [0x7fc848e0a25a] ray::operator<<()\r\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0xfeb998)\
    \ [0x7fc848e0c998] ray::TerminateHandler()\r\n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/bin/../lib/libstdc++.so.6(+0xb135a)\
    \ [0x7fc847c9b35a] __cxxabiv1::__terminate()\r\n(_AsyncLLMEngine pid=1065574)\
    \ /home/xx/anaconda3/envs/vllm/bin/../lib/libstdc++.so.6(+0xb13c5) [0x7fc847c9b3c5]\r\
    \n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x7c92d0)\
    \ [0x7fc8485ea2d0] std::thread::_State_impl<>::~_State_impl()\r\n(_AsyncLLMEngine\
    \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
    \ [0x7fc84844f5fa] std::_Sp_counted_base<>::_M_release()\r\n(_AsyncLLMEngine pid=1065574)\
    \ /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x7b22d2)\
    \ [0x7fc8485d32d2] std::_Sp_counted_ptr_inplace<>::_M_dispose()\r\n(_AsyncLLMEngine\
    \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
    \ [0x7fc84844f5fa] std::_Sp_counted_base<>::_M_release()\r\n(_AsyncLLMEngine pid=1065574)\
    \ /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x6e9732)\
    \ [0x7fc84850a732] std::default_delete<>::operator()()\r\n(_AsyncLLMEngine pid=1065574)\
    \ /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core10CoreWorkerD1Ev+0xeb)\
    \ [0x7fc84857c6db] ray::core::CoreWorker::~CoreWorker()\r\n(_AsyncLLMEngine pid=1065574)\
    \ /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x62e5fa)\
    \ [0x7fc84844f5fa] std::_Sp_counted_base<>::_M_release()\r\n(_AsyncLLMEngine pid=1065574)\
    \ /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv+0x134)\
    \ [0x7fc8485ba854] ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\r\
    \n(_AsyncLLMEngine pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(_ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv+0x1d)\
    \ [0x7fc8485ba95d] ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\r\n(_AsyncLLMEngine\
    \ pid=1065574) /home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_raylet.so(+0x5ae947)\
    \ [0x7fc8483cf947] __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\r\n(_AsyncLLMEngine\
    \ pid=1065574) ray::_AsyncLLMEngine() [0x535490] method_vectorcall_NOARGS\r\n\
    (_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(PyObject_Vectorcall+0x31) [0x51bff1]\
    \ PyObject_Vectorcall\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyEval_EvalFrameDefault+0x755)\
    \ [0x50f025] _PyEval_EvalFrameDefault\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
    \ [0x5c82ce] _PyEval_Vector\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(PyEval_EvalCode+0x9f)\
    \ [0x5c79cf] PyEval_EvalCode\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
    \ [0x5e8807] run_eval_code_obj\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
    \ [0x5e4e40] run_mod\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine() [0x5f9132]\
    \ pyrun_file\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyRun_SimpleFileObject+0x19f)\
    \ [0x5f871f] _PyRun_SimpleFileObject\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(_PyRun_AnyFileObject+0x43)\
    \ [0x5f8473] _PyRun_AnyFileObject\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(Py_RunMain+0x2ee)\
    \ [0x5f2fee] Py_RunMain\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine(Py_BytesMain+0x39)\
    \ [0x5b6e19] Py_BytesMain\r\n(_AsyncLLMEngine pid=1065574) /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3)\
    \ [0x7fc849f87083] __libc_start_main\r\n(_AsyncLLMEngine pid=1065574) ray::_AsyncLLMEngine()\
    \ [0x5b6c6f]\r\n(_AsyncLLMEngine pid=1065574)\r\n(_AsyncLLMEngine pid=1065574)\
    \ *** SIGABRT received at time=1705704679 on cpu 13 ***\r\n(_AsyncLLMEngine pid=1065574)\
    \ PC: @     0x7fc849fa600b  (unknown)  raise\r\n(_AsyncLLMEngine pid=1065574)\
    \     @     0x7fc84a2c3420   74038176  (unknown)\r\n(_AsyncLLMEngine pid=1065574)\
    \     @     0x7fc847c9b35a         80  __cxxabiv1::__terminate()\r\n(_AsyncLLMEngine\
    \ pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8485d32d2         96  std::_Sp_counted_ptr_inplace<>::_M_dispose()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84850a732        144  std::default_delete<>::operator()()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84857c6db        128  ray::core::CoreWorker::~CoreWorker()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8485ba854        112  ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8485ba95d         32  ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\r\
    \n(_AsyncLLMEngine pid=1065574)     @     0x7fc8483cf947         32  __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\r\
    \n(_AsyncLLMEngine pid=1065574)     @           0x535490  (unknown)  method_vectorcall_NOARGS\r\
    \n(_AsyncLLMEngine pid=1065574)     @           0x875180  (unknown)  (unknown)\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,086 E 1065574 1065574] logging.cc:361:\
    \ *** SIGABRT received at time=1705704679 on cpu 13 ***\r\n(_AsyncLLMEngine pid=1065574)\
    \ [2024-01-20 06:51:19,086 E 1065574 1065574] logging.cc:361: PC: @     0x7fc849fa600b\
    \  (unknown)  raise\r\n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087\
    \ E 1065574 1065574] logging.cc:361:     @     0x7fc84a2c3420   74038176  (unknown)\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc847c9b35a         80  __cxxabiv1::__terminate()\r\n(_AsyncLLMEngine\
    \ pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:   \
    \  @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\n\
    (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc8485d32d2         96  std::_Sp_counted_ptr_inplace<>::_M_dispose()\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc84850a732        144  std::default_delete<>::operator()()\r\n\
    (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc84857c6db        128  ray::core::CoreWorker::~CoreWorker()\r\n\
    (_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc84844f5fa         32  std::_Sp_counted_base<>::_M_release()\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc8485ba854        112  ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc8485ba95d         32  ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @     0x7fc8483cf947         32  __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\r\
    \n(_AsyncLLMEngine pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:\
    \     @           0x535490  (unknown)  method_vectorcall_NOARGS\r\n(_AsyncLLMEngine\
    \ pid=1065574) [2024-01-20 06:51:19,087 E 1065574 1065574] logging.cc:361:   \
    \  @           0x875180  (unknown)  (unknown)\r\n(_AsyncLLMEngine pid=1065574)\
    \ Fatal Python error: Aborted\r\n(_AsyncLLMEngine pid=1065574)\r\n(_AsyncLLMEngine\
    \ pid=1065574) Stack (most recent call first):\r\n(_AsyncLLMEngine pid=1065574)\
    \   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_private/worker.py\"\
    , line 847 in main_loop\r\n(_AsyncLLMEngine pid=1065574)   File \"/home/xx/anaconda3/envs/vllm/lib/python3.11/site-packages/ray/_private/workers/default_worker.py\"\
    , line 282 in <module>\r\n(_AsyncLLMEngine pid=1065574)\r\n(_AsyncLLMEngine pid=1065574)\
    \ Extension modules: psutil._psutil_linux, psutil._psutil_posix, msgpack._cmsgpack,\
    \ google._upb._message, setproctitle, yaml._yaml, charset_normalizer.md, uvloop.loop,\
    \ ray._raylet, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg,\
    \ numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator,\
    \ numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand,\
    \ numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator,\
    \ torch._C, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse,\
    \ torch._C._special, pydantic.typing, pydantic.errors, pydantic.version, pydantic.utils,\
    \ pydantic.class_validators, pydantic.config, pydantic.color, pydantic.datetime_parse,\
    \ pydantic.validators, pydantic.networks, pydantic.types, pydantic.json, pydantic.error_wrappers,\
    \ pydantic.fields, pydantic.parse, pydantic.schema, pydantic.main, pydantic.dataclasses,\
    \ pydantic.annotated_types, pydantic.decorator, pydantic.env_settings, pydantic.tools,\
    \ pydantic, sentencepiece._sentencepiece, pyarrow.lib, pyarrow._hdfsio, pyarrow._json\
    \ (total: 56)\r\n```\r\n\r\nIt seems that there is little official interest in\
    \ adapting the vllm and llama frameworks, checking the llm leaderboards it's about\
    \ 68-70 points or so, well, sadly it's not possible to continue experiencing the\
    \ model..."
  created_at: 2024-01-19 23:21:32+00:00
  edited: false
  hidden: false
  id: 65ab03fce2a2c863567d9806
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bartowski
      type: user
    createdAt: '2024-01-20T18:22:07.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8761826157569885
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: '<p>Interesting, the error seems to suggest that you need to go download
          the file from the original repo: <a href="https://huggingface.co/internlm/internlm2-chat-7b/blob/main/tokenization_internlm.py">https://huggingface.co/internlm/internlm2-chat-7b/blob/main/tokenization_internlm.py</a></p>

          <p>Could give that a try?</p>

          '
        raw: 'Interesting, the error seems to suggest that you need to go download
          the file from the original repo: https://huggingface.co/internlm/internlm2-chat-7b/blob/main/tokenization_internlm.py


          Could give that a try?'
        updatedAt: '2024-01-20T18:22:07.833Z'
      numEdits: 0
      reactions: []
    id: 65ac0f4fa8f716b32e4f610c
    type: comment
  author: bartowski
  content: 'Interesting, the error seems to suggest that you need to go download the
    file from the original repo: https://huggingface.co/internlm/internlm2-chat-7b/blob/main/tokenization_internlm.py


    Could give that a try?'
  created_at: 2024-01-20 18:22:07+00:00
  edited: false
  hidden: false
  id: 65ac0f4fa8f716b32e4f610c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
      fullname: Bartowski
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bartowski
      type: user
    createdAt: '2024-01-20T18:22:49.000Z'
    data:
      edited: false
      editors:
      - bartowski
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9677761793136597
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435718aaaef013d1aec3b8b/XKf-8MA47tjVAM6SCX0MP.jpeg?w=200&h=200&f=face
          fullname: Bartowski
          isHf: false
          isPro: false
          name: bartowski
          type: user
        html: '<p>Didn''t expect to need any of those files after conversion, I can
          include them if that fixes your error </p>

          '
        raw: 'Didn''t expect to need any of those files after conversion, I can include
          them if that fixes your error '
        updatedAt: '2024-01-20T18:22:49.558Z'
      numEdits: 0
      reactions: []
    id: 65ac0f79819fbfaf49d4060e
    type: comment
  author: bartowski
  content: 'Didn''t expect to need any of those files after conversion, I can include
    them if that fixes your error '
  created_at: 2024-01-20 18:22:49+00:00
  edited: false
  hidden: false
  id: 65ac0f79819fbfaf49d4060e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bartowski/internlm2-chat-7b-llama
repo_type: model
status: open
target_branch: null
title: vllm deployment failed
