!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2023-12-16 19:45:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-16T19:45:49.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9625883102416992
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> I got the nllb\
          \ moe to work with 4 bit of bitsandbytes. Since recently we can store the\
          \ weights in 4 bit, it works, uploading now.<br>the weights load quickly,\
          \ like 20 seconds vs 15 minutes (into 4 bit). But execution is slow.<br>I\
          \ don't know about this, but since you got the mistral moe into gptq, is\
          \ it possible to quantize this model, too? It's unique. I guess it outperforms\
          \ the recent seamless translation models of meta. it's just huge. I guess\
          \ there is no support in gptq nor gguf.<br>but this is a unique resource\
          \ for machine translation, it's properly unsurpassed open access weights,\
          \ although they are already dated. </p>\n"
        raw: "@TheBloke I got the nllb moe to work with 4 bit of bitsandbytes. Since\
          \ recently we can store the weights in 4 bit, it works, uploading now. \r\
          \nthe weights load quickly, like 20 seconds vs 15 minutes (into 4 bit).\
          \ But execution is slow. \r\nI don't know about this, but since you got\
          \ the mistral moe into gptq, is it possible to quantize this model, too?\
          \ It's unique. I guess it outperforms the recent seamless translation models\
          \ of meta. it's just huge. I guess there is no support in gptq nor gguf.\
          \ \r\nbut this is a unique resource for machine translation, it's properly\
          \ unsurpassed open access weights, although they are already dated. "
        updatedAt: '2023-12-16T19:45:49.127Z'
      numEdits: 0
      reactions: []
    id: 657dfe6df010d76b6e218fd6
    type: comment
  author: KnutJaegersberg
  content: "@TheBloke I got the nllb moe to work with 4 bit of bitsandbytes. Since\
    \ recently we can store the weights in 4 bit, it works, uploading now. \r\nthe\
    \ weights load quickly, like 20 seconds vs 15 minutes (into 4 bit). But execution\
    \ is slow. \r\nI don't know about this, but since you got the mistral moe into\
    \ gptq, is it possible to quantize this model, too? It's unique. I guess it outperforms\
    \ the recent seamless translation models of meta. it's just huge. I guess there\
    \ is no support in gptq nor gguf. \r\nbut this is a unique resource for machine\
    \ translation, it's properly unsurpassed open access weights, although they are\
    \ already dated. "
  created_at: 2023-12-16 19:45:49+00:00
  edited: false
  hidden: false
  id: 657dfe6df010d76b6e218fd6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2023-12-16T19:47:00.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9373660683631897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>this quantization uses 37gb, with double quantization it is more
          like 35gb vram. I have not even tried long translations yet, only the examples.
          It''s slow. but it is properly the best. </p>

          '
        raw: 'this quantization uses 37gb, with double quantization it is more like
          35gb vram. I have not even tried long translations yet, only the examples.
          It''s slow. but it is properly the best. '
        updatedAt: '2023-12-16T19:47:00.340Z'
      numEdits: 0
      reactions: []
    id: 657dfeb4e50ca9a699c2df54
    type: comment
  author: KnutJaegersberg
  content: 'this quantization uses 37gb, with double quantization it is more like
    35gb vram. I have not even tried long translations yet, only the examples. It''s
    slow. but it is properly the best. '
  created_at: 2023-12-16 19:47:00+00:00
  edited: false
  hidden: false
  id: 657dfeb4e50ca9a699c2df54
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: KnutJaegersberg/nllb-moe-54b-4bit
repo_type: model
status: open
target_branch: null
title: gptq
