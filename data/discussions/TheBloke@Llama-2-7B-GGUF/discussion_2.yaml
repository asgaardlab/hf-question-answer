!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AsierRG55
conflicting_files: null
created_at: 2023-09-25 10:42:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01a97ee4afa51d34ff9657150ecc2c80.svg
      fullname: R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AsierRG55
      type: user
    createdAt: '2023-09-25T11:42:35.000Z'
    data:
      edited: false
      editors:
      - AsierRG55
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9624316692352295
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01a97ee4afa51d34ff9657150ecc2c80.svg
          fullname: R
          isHf: false
          isPro: false
          name: AsierRG55
          type: user
        html: '<p>I thought Llama2''s maximum context length was 4,096 tokens. When
          I went to perform an inference through this model I saw that the maximum
          context length is 512. What is the reason for this modification? </p>

          <p>Thank you</p>

          '
        raw: "I thought Llama2's maximum context length was 4,096 tokens. When I went\
          \ to perform an inference through this model I saw that the maximum context\
          \ length is 512. What is the reason for this modification? \r\n\r\nThank\
          \ you"
        updatedAt: '2023-09-25T11:42:35.855Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NerdIterator
    id: 6511722baa4a062093156293
    type: comment
  author: AsierRG55
  content: "I thought Llama2's maximum context length was 4,096 tokens. When I went\
    \ to perform an inference through this model I saw that the maximum context length\
    \ is 512. What is the reason for this modification? \r\n\r\nThank you"
  created_at: 2023-09-25 10:42:35+00:00
  edited: false
  hidden: false
  id: 6511722baa4a062093156293
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/820472235f43fdd590bb0a7ce1603dd6.svg
      fullname: Marek Kerka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marekk
      type: user
    createdAt: '2023-09-25T11:49:15.000Z'
    data:
      edited: false
      editors:
      - marekk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5142927765846252
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/820472235f43fdd590bb0a7ce1603dd6.svg
          fullname: Marek Kerka
          isHf: false
          isPro: false
          name: marekk
          type: user
        html: '<p>You have to set n_ctx parametr. 512 is default.<br>For example:</p>

          <pre><code>from llama_cpp import Llama

          llm = Llama(model_path="wizardlm-1.0-uncensored-llama2-13b.Q5_K_S.gguf",
          n_ctx=4096, n_gpu_layers=-1)

          </code></pre>

          '
        raw: 'You have to set n_ctx parametr. 512 is default.

          For example:

          ```

          from llama_cpp import Llama

          llm = Llama(model_path="wizardlm-1.0-uncensored-llama2-13b.Q5_K_S.gguf",
          n_ctx=4096, n_gpu_layers=-1)

          ```'
        updatedAt: '2023-09-25T11:49:15.638Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - NerdIterator
        - NagaLakshmiKamireddy
    id: 651173bb2a88fb0bc20ff08c
    type: comment
  author: marekk
  content: 'You have to set n_ctx parametr. 512 is default.

    For example:

    ```

    from llama_cpp import Llama

    llm = Llama(model_path="wizardlm-1.0-uncensored-llama2-13b.Q5_K_S.gguf", n_ctx=4096,
    n_gpu_layers=-1)

    ```'
  created_at: 2023-09-25 10:49:15+00:00
  edited: false
  hidden: false
  id: 651173bb2a88fb0bc20ff08c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01a97ee4afa51d34ff9657150ecc2c80.svg
      fullname: R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AsierRG55
      type: user
    createdAt: '2023-09-25T13:39:36.000Z'
    data:
      edited: false
      editors:
      - AsierRG55
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7310022115707397
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01a97ee4afa51d34ff9657150ecc2c80.svg
          fullname: R
          isHf: false
          isPro: false
          name: AsierRG55
          type: user
        html: '<p>Thanks a lot. And what for ctransformers implementation? I don''t
          see the parameter either in AutoModelForCausalLM.from_pretrained nor in
          generation method.</p>

          '
        raw: 'Thanks a lot. And what for ctransformers implementation? I don''t see
          the parameter either in AutoModelForCausalLM.from_pretrained nor in generation
          method.

          '
        updatedAt: '2023-09-25T13:39:36.226Z'
      numEdits: 0
      reactions: []
    id: 65118d98b94d5922e3cd369b
    type: comment
  author: AsierRG55
  content: 'Thanks a lot. And what for ctransformers implementation? I don''t see
    the parameter either in AutoModelForCausalLM.from_pretrained nor in generation
    method.

    '
  created_at: 2023-09-25 12:39:36+00:00
  edited: false
  hidden: false
  id: 65118d98b94d5922e3cd369b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/820472235f43fdd590bb0a7ce1603dd6.svg
      fullname: Marek Kerka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marekk
      type: user
    createdAt: '2023-09-25T13:51:17.000Z'
    data:
      edited: false
      editors:
      - marekk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9083544611930847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/820472235f43fdd590bb0a7ce1603dd6.svg
          fullname: Marek Kerka
          isHf: false
          isPro: false
          name: marekk
          type: user
        html: '<p>I had to move from ctransformers to llama-cpp-python. <a rel="nofollow"
          href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a></p>

          '
        raw: 'I had to move from ctransformers to llama-cpp-python. https://github.com/abetlen/llama-cpp-python

          '
        updatedAt: '2023-09-25T13:51:17.816Z'
      numEdits: 0
      reactions: []
    id: 6511905589d7167ede6fd242
    type: comment
  author: marekk
  content: 'I had to move from ctransformers to llama-cpp-python. https://github.com/abetlen/llama-cpp-python

    '
  created_at: 2023-09-25 12:51:17+00:00
  edited: false
  hidden: false
  id: 6511905589d7167ede6fd242
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6460df7a933afb0106a861a0/X-NXl6RlcKHdzF8xJBPER.jpeg?w=200&h=200&f=face
      fullname: Karel Minarik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: karmiq
      type: user
    createdAt: '2023-11-04T08:51:59.000Z'
    data:
      edited: false
      editors:
      - karmiq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6239416003227234
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6460df7a933afb0106a861a0/X-NXl6RlcKHdzF8xJBPER.jpeg?w=200&h=200&f=face
          fullname: Karel Minarik
          isHf: false
          isPro: false
          name: karmiq
          type: user
        html: "<p>There's currently the <code>context_length</code> parameter available\
          \ in ctransformers: <a rel=\"nofollow\" href=\"https://github.com/marella/ctransformers#config\"\
          >https://github.com/marella/ctransformers#config</a>. So you can set something\
          \ like this:</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\n\
          \    \"TheBloke/Llama-2-7b-Chat-GGUF\",\n    # ...\n    context_length=4096,\n\
          )\n</code></pre>\n"
        raw: "There's currently the `context_length` parameter available in ctransformers:\
          \ https://github.com/marella/ctransformers#config. So you can set something\
          \ like this:\n\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n   \
          \ \"TheBloke/Llama-2-7b-Chat-GGUF\",\n    # ...\n    context_length=4096,\n\
          )\n```"
        updatedAt: '2023-11-04T08:51:59.612Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - horsesword
        - NagaLakshmiKamireddy
        - Aigerimtbin
    id: 6546062f117ecae648aba2ba
    type: comment
  author: karmiq
  content: "There's currently the `context_length` parameter available in ctransformers:\
    \ https://github.com/marella/ctransformers#config. So you can set something like\
    \ this:\n\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TheBloke/Llama-2-7b-Chat-GGUF\"\
    ,\n    # ...\n    context_length=4096,\n)\n```"
  created_at: 2023-11-04 07:51:59+00:00
  edited: false
  hidden: false
  id: 6546062f117ecae648aba2ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/eHnU7c43k-7g5xTW9hW8S.jpeg?w=200&h=200&f=face
      fullname: R2 Decide
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: r2decide
      type: user
    createdAt: '2024-01-01T16:52:13.000Z'
    data:
      edited: false
      editors:
      - r2decide
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6959885358810425
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/eHnU7c43k-7g5xTW9hW8S.jpeg?w=200&h=200&f=face
          fullname: R2 Decide
          isHf: false
          isPro: true
          name: r2decide
          type: user
        html: '<p>Is there a way to set this when using the Inference Endpoints/ API?</p>

          '
        raw: Is there a way to set this when using the Inference Endpoints/ API?
        updatedAt: '2024-01-01T16:52:13.506Z'
      numEdits: 0
      reactions: []
    id: 6592edbd11f68f12eaa369c1
    type: comment
  author: r2decide
  content: Is there a way to set this when using the Inference Endpoints/ API?
  created_at: 2024-01-01 16:52:13+00:00
  edited: false
  hidden: false
  id: 6592edbd11f68f12eaa369c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-7B-GGUF
repo_type: model
status: open
target_branch: null
title: Maximum context length (512)
