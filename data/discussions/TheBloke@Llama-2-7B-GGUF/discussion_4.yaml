!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jrudolph
conflicting_files: null
created_at: 2023-10-05 20:05:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64f6e19203e7846687b256f2ae4b6b0d.svg
      fullname: Johannes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jrudolph
      type: user
    createdAt: '2023-10-05T21:05:39.000Z'
    data:
      edited: false
      editors:
      - jrudolph
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619163274765015
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64f6e19203e7846687b256f2ae4b6b0d.svg
          fullname: Johannes
          isHf: false
          isPro: false
          name: jrudolph
          type: user
        html: '<p>Is this intended?  It seems to depend on whether <code>LLAMA_NO_K_QUANTS</code>
          is set during compilation and <code>quantize_output_tensor</code> during
          quantization time. Is that something you changed in your process?</p>

          <p>I noticed, because in my custom engine, I have not yet implemented all
          quantization methods (though k-quants seem attractive anyway...), and having
          models only with q4_0 (+ maybe q8_0) and fp32 was quite convenient. Not
          a big inconvenience, in the worst case, I can just dequantize at load time
          (to avoid having to implement full quantized matmul).</p>

          '
        raw: "Is this intended?  It seems to depend on whether `LLAMA_NO_K_QUANTS`\
          \ is set during compilation and `quantize_output_tensor` during quantization\
          \ time. Is that something you changed in your process?\r\n\r\nI noticed,\
          \ because in my custom engine, I have not yet implemented all quantization\
          \ methods (though k-quants seem attractive anyway...), and having models\
          \ only with q4_0 (+ maybe q8_0) and fp32 was quite convenient. Not a big\
          \ inconvenience, in the worst case, I can just dequantize at load time (to\
          \ avoid having to implement full quantized matmul)."
        updatedAt: '2023-10-05T21:05:39.834Z'
      numEdits: 0
      reactions: []
    id: 651f2523cf1335cc44b1322d
    type: comment
  author: jrudolph
  content: "Is this intended?  It seems to depend on whether `LLAMA_NO_K_QUANTS` is\
    \ set during compilation and `quantize_output_tensor` during quantization time.\
    \ Is that something you changed in your process?\r\n\r\nI noticed, because in\
    \ my custom engine, I have not yet implemented all quantization methods (though\
    \ k-quants seem attractive anyway...), and having models only with q4_0 (+ maybe\
    \ q8_0) and fp32 was quite convenient. Not a big inconvenience, in the worst case,\
    \ I can just dequantize at load time (to avoid having to implement full quantized\
    \ matmul)."
  created_at: 2023-10-05 20:05:39+00:00
  edited: false
  hidden: false
  id: 651f2523cf1335cc44b1322d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-05T22:43:26.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6795179843902588
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>No, I haven''t changed anything in my process. I build llama.cpp
          with default options</p>

          <pre><code> make clean &amp;&amp; LLAMA_CUBLAS=1 make -j

          </code></pre>

          <p>And I quantise with, for example:</p>

          <pre><code>quantize airoboros-l2-13b-3.0.fp16.gguf airoboros-l2-13b-3.0.Q4_K_M.gguf
          Q4_K_M

          </code></pre>

          <p>I believe quantising the output tensor with q6_k has been standard since
          around the time GGUF was first released.</p>

          '
        raw: "No, I haven't changed anything in my process. I build llama.cpp with\
          \ default options\n```\n make clean && LLAMA_CUBLAS=1 make -j\n```\n\nAnd\
          \ I quantise with, for example:\n```\nquantize airoboros-l2-13b-3.0.fp16.gguf\
          \ airoboros-l2-13b-3.0.Q4_K_M.gguf Q4_K_M\n```\n\nI believe quantising the\
          \ output tensor with q6_k has been standard since around the time GGUF was\
          \ first released."
        updatedAt: '2023-10-05T22:43:55.499Z'
      numEdits: 1
      reactions: []
    id: 651f3c0e444b79d2c6f3d3a7
    type: comment
  author: TheBloke
  content: "No, I haven't changed anything in my process. I build llama.cpp with default\
    \ options\n```\n make clean && LLAMA_CUBLAS=1 make -j\n```\n\nAnd I quantise with,\
    \ for example:\n```\nquantize airoboros-l2-13b-3.0.fp16.gguf airoboros-l2-13b-3.0.Q4_K_M.gguf\
    \ Q4_K_M\n```\n\nI believe quantising the output tensor with q6_k has been standard\
    \ since around the time GGUF was first released."
  created_at: 2023-10-05 21:43:26+00:00
  edited: true
  hidden: false
  id: 651f3c0e444b79d2c6f3d3a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64f6e19203e7846687b256f2ae4b6b0d.svg
      fullname: Johannes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jrudolph
      type: user
    createdAt: '2023-10-06T07:53:25.000Z'
    data:
      edited: false
      editors:
      - jrudolph
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9516835808753967
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64f6e19203e7846687b256f2ae4b6b0d.svg
          fullname: Johannes
          isHf: false
          isPro: false
          name: jrudolph
          type: user
        html: '<p>Yep, probably, not a big deal, I guess. Thanks for the answer.</p>

          '
        raw: Yep, probably, not a big deal, I guess. Thanks for the answer.
        updatedAt: '2023-10-06T07:53:25.679Z'
      numEdits: 0
      reactions: []
      relatedEventId: 651fbcf5fb2de4bf4b3b19c9
    id: 651fbcf5fb2de4bf4b3b19c6
    type: comment
  author: jrudolph
  content: Yep, probably, not a big deal, I guess. Thanks for the answer.
  created_at: 2023-10-06 06:53:25+00:00
  edited: false
  hidden: false
  id: 651fbcf5fb2de4bf4b3b19c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/64f6e19203e7846687b256f2ae4b6b0d.svg
      fullname: Johannes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jrudolph
      type: user
    createdAt: '2023-10-06T07:53:25.000Z'
    data:
      status: closed
    id: 651fbcf5fb2de4bf4b3b19c9
    type: status-change
  author: jrudolph
  created_at: 2023-10-06 06:53:25+00:00
  id: 651fbcf5fb2de4bf4b3b19c9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Llama-2-7B-GGUF
repo_type: model
status: closed
target_branch: null
title: GGUF q4_0 model variants quantize the "output.weight" tensor with q6_k (compared
  to former ggml files which left them at fp32 iirc)
