!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RajeshkumarV
conflicting_files: null
created_at: 2023-09-06 09:03:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
      fullname: Rajesh Kumar V
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RajeshkumarV
      type: user
    createdAt: '2023-09-06T10:03:43.000Z'
    data:
      edited: false
      editors:
      - RajeshkumarV
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8305058479309082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ce5f70298ae4c65e33c3593a8e81ac4.svg
          fullname: Rajesh Kumar V
          isHf: false
          isPro: false
          name: RajeshkumarV
          type: user
        html: '<p>When i try to either embed or vectorize a PDF document or try to
          retrieve a vector i get this error. I was able to vectorize the data in
          the earlier ggml format, but that stopped working with few errors and then
          the community moved to this new format of GGUF, but ever since then facing
          this issue. </p>

          <p>ggml_allocr_alloc: not enough space in the buffer (needed 136059008,
          largest block available 16891904)<br>GGML_ASSERT: C:\Users\Rajesh_Kumar_V1\AppData\Local\Temp\pip-install-0ohg_aj6\llama-cpp-python_29c4846b4af1471bbb28a41659b32aa3\vendor\llama.cpp\ggml-alloc.c:144:
          !"not enough space in the buffer"</p>

          '
        raw: "When i try to either embed or vectorize a PDF document or try to retrieve\
          \ a vector i get this error. I was able to vectorize the data in the earlier\
          \ ggml format, but that stopped working with few errors and then the community\
          \ moved to this new format of GGUF, but ever since then facing this issue.\
          \ \r\n\r\nggml_allocr_alloc: not enough space in the buffer (needed 136059008,\
          \ largest block available 16891904)\r\nGGML_ASSERT: C:\\Users\\Rajesh_Kumar_V1\\\
          AppData\\Local\\Temp\\pip-install-0ohg_aj6\\llama-cpp-python_29c4846b4af1471bbb28a41659b32aa3\\\
          vendor\\llama.cpp\\ggml-alloc.c:144: !\"not enough space in the buffer\""
        updatedAt: '2023-09-06T10:03:43.737Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pe4enov
    id: 64f84e7f6a71cea1c7e64767
    type: comment
  author: RajeshkumarV
  content: "When i try to either embed or vectorize a PDF document or try to retrieve\
    \ a vector i get this error. I was able to vectorize the data in the earlier ggml\
    \ format, but that stopped working with few errors and then the community moved\
    \ to this new format of GGUF, but ever since then facing this issue. \r\n\r\n\
    ggml_allocr_alloc: not enough space in the buffer (needed 136059008, largest block\
    \ available 16891904)\r\nGGML_ASSERT: C:\\Users\\Rajesh_Kumar_V1\\AppData\\Local\\\
    Temp\\pip-install-0ohg_aj6\\llama-cpp-python_29c4846b4af1471bbb28a41659b32aa3\\\
    vendor\\llama.cpp\\ggml-alloc.c:144: !\"not enough space in the buffer\""
  created_at: 2023-09-06 09:03:43+00:00
  edited: false
  hidden: false
  id: 64f84e7f6a71cea1c7e64767
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/baea954d6bb8da882d7d09b8cfe34bfd.svg
      fullname: "Mathias Grund S\xF8rensen"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grund
      type: user
    createdAt: '2023-09-15T11:42:02.000Z'
    data:
      edited: true
      editors:
      - grund
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9591203331947327
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/baea954d6bb8da882d7d09b8cfe34bfd.svg
          fullname: "Mathias Grund S\xF8rensen"
          isHf: false
          isPro: false
          name: grund
          type: user
        html: '<p>I''ve encountered the same and while I can''t give you an exact
          root cause for why it''s exceeding allocated VRAM nor remember exactly what
          I did to avoid it, you should be able to work around it by reducing any
          dimension that causes VRAM usage to grow beyond the allocation (ctx size
          etc.). Not sure if maybe you can handle it just by reducing batch size?</p>

          '
        raw: I've encountered the same and while I can't give you an exact root cause
          for why it's exceeding allocated VRAM nor remember exactly what I did to
          avoid it, you should be able to work around it by reducing any dimension
          that causes VRAM usage to grow beyond the allocation (ctx size etc.). Not
          sure if maybe you can handle it just by reducing batch size?
        updatedAt: '2023-09-15T11:42:46.762Z'
      numEdits: 1
      reactions: []
    id: 6504430a453165de0b97e2eb
    type: comment
  author: grund
  content: I've encountered the same and while I can't give you an exact root cause
    for why it's exceeding allocated VRAM nor remember exactly what I did to avoid
    it, you should be able to work around it by reducing any dimension that causes
    VRAM usage to grow beyond the allocation (ctx size etc.). Not sure if maybe you
    can handle it just by reducing batch size?
  created_at: 2023-09-15 10:42:02+00:00
  edited: true
  hidden: false
  id: 6504430a453165de0b97e2eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93feb89b8e5a00b282c79ad5717f6dcd.svg
      fullname: lasantha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: llcoollasa
      type: user
    createdAt: '2023-11-04T14:53:53.000Z'
    data:
      edited: false
      editors:
      - llcoollasa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9775112271308899
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93feb89b8e5a00b282c79ad5717f6dcd.svg
          fullname: lasantha
          isHf: false
          isPro: false
          name: llcoollasa
          type: user
        html: '<p>I am also having the same problem. Any solutions?</p>

          '
        raw: I am also having the same problem. Any solutions?
        updatedAt: '2023-11-04T14:53:53.940Z'
      numEdits: 0
      reactions: []
    id: 65465b012fe2a1e686638a08
    type: comment
  author: llcoollasa
  content: I am also having the same problem. Any solutions?
  created_at: 2023-11-04 13:53:53+00:00
  edited: false
  hidden: false
  id: 65465b012fe2a1e686638a08
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-7B-GGUF
repo_type: model
status: open
target_branch: null
title: 'Error -- ggml_allocr_alloc: not enough space in the buffer (needed 136059008,
  largest block available 16891904)'
