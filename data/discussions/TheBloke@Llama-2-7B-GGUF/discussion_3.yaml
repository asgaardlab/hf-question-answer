!!python/object:huggingface_hub.community.DiscussionWithDetails
author: niranjanakella
conflicting_files: null
created_at: 2023-09-27 07:46:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646dad89f813cfe153eebe94/q_zVk6-asU00mIPT1ngoe.png?w=200&h=200&f=face
      fullname: Niranjan Akella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niranjanakella
      type: user
    createdAt: '2023-09-27T08:46:30.000Z'
    data:
      edited: false
      editors:
      - niranjanakella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6701189279556274
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646dad89f813cfe153eebe94/q_zVk6-asU00mIPT1ngoe.png?w=200&h=200&f=face
          fullname: Niranjan Akella
          isHf: false
          isPro: false
          name: niranjanakella
          type: user
        html: "<p>Hello, I am very interested in the new GGUF format for inferencing\
          \ LLMs. </p>\n<p>I am currently trying to convert flan-t5-large model at\
          \ <a href=\"https://huggingface.co/google/flan-t5-large/tree/main\">https://huggingface.co/google/flan-t5-large/tree/main</a>,\
          \ to GGUF format. </p>\n<p>I have tried building the llama.cpp and then\
          \ running the convert.py but getting unknown format. </p>\n<p>Loading model\
          \ file /Users/nakella/MyWorkspace/Experiments/llama.cpp/models/flan-t5-large/pytorch_model.bin<br>Traceback\
          \ (most recent call last):<br>  File \"convert.py\", line 1208, in <br>\
          \    main()<br>  File \"convert.py\", line 1149, in main<br>    model_plus\
          \ = load_some_model(args.model)<br>  File \"convert.py\", line 1069, in\
          \ load_some_model<br>    models_plus.append(lazy_load_file(path))<br>  File\
          \ \"convert.py\", line 763, in lazy_load_file<br>    raise ValueError(f\"\
          unknown format: {path}\")<br>ValueError: unknown format: /Users/nakella/MyWorkspace/Experiments/llama.cpp/models/flan-t5-large/pytorch_model.bin</p>\n\
          <p>Can <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;RonanMcGovern&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/RonanMcGovern\">@<span class=\"underline\"\
          >RonanMcGovern</span></a></span>\n\n\t</span></span> you please kindly help\
          \ me with this. Is there any other way? </p>\n"
        raw: "Hello, I am very interested in the new GGUF format for inferencing LLMs.\
          \ \r\n\r\nI am currently trying to convert flan-t5-large model at https://huggingface.co/google/flan-t5-large/tree/main,\
          \ to GGUF format. \r\n\r\nI have tried building the llama.cpp and then running\
          \ the convert.py but getting unknown format. \r\n\r\nLoading model file\
          \ /Users/nakella/MyWorkspace/Experiments/llama.cpp/models/flan-t5-large/pytorch_model.bin\r\
          \nTraceback (most recent call last):\r\n  File \"convert.py\", line 1208,\
          \ in <module>\r\n    main()\r\n  File \"convert.py\", line 1149, in main\r\
          \n    model_plus = load_some_model(args.model)\r\n  File \"convert.py\"\
          , line 1069, in load_some_model\r\n    models_plus.append(lazy_load_file(path))\r\
          \n  File \"convert.py\", line 763, in lazy_load_file\r\n    raise ValueError(f\"\
          unknown format: {path}\")\r\nValueError: unknown format: /Users/nakella/MyWorkspace/Experiments/llama.cpp/models/flan-t5-large/pytorch_model.bin\r\
          \n\r\nCan @TheBloke @RonanMcGovern you please kindly help me with this.\
          \ Is there any other way? "
        updatedAt: '2023-09-27T08:46:30.749Z'
      numEdits: 0
      reactions: []
    id: 6513ebe6ddff6f952948bafd
    type: comment
  author: niranjanakella
  content: "Hello, I am very interested in the new GGUF format for inferencing LLMs.\
    \ \r\n\r\nI am currently trying to convert flan-t5-large model at https://huggingface.co/google/flan-t5-large/tree/main,\
    \ to GGUF format. \r\n\r\nI have tried building the llama.cpp and then running\
    \ the convert.py but getting unknown format. \r\n\r\nLoading model file /Users/nakella/MyWorkspace/Experiments/llama.cpp/models/flan-t5-large/pytorch_model.bin\r\
    \nTraceback (most recent call last):\r\n  File \"convert.py\", line 1208, in <module>\r\
    \n    main()\r\n  File \"convert.py\", line 1149, in main\r\n    model_plus =\
    \ load_some_model(args.model)\r\n  File \"convert.py\", line 1069, in load_some_model\r\
    \n    models_plus.append(lazy_load_file(path))\r\n  File \"convert.py\", line\
    \ 763, in lazy_load_file\r\n    raise ValueError(f\"unknown format: {path}\")\r\
    \nValueError: unknown format: /Users/nakella/MyWorkspace/Experiments/llama.cpp/models/flan-t5-large/pytorch_model.bin\r\
    \n\r\nCan @TheBloke @RonanMcGovern you please kindly help me with this. Is there\
    \ any other way? "
  created_at: 2023-09-27 07:46:30+00:00
  edited: false
  hidden: false
  id: 6513ebe6ddff6f952948bafd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-27T14:25:56.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.31812819838523865
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Try the colab referenced <a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/13#64fc3bea4010eccccc323de2">here</a></p>

          '
        raw: Try the colab referenced [here](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/13#64fc3bea4010eccccc323de2)
        updatedAt: '2023-09-27T14:25:56.651Z'
      numEdits: 0
      reactions: []
    id: 65143b7443afebbc0a6c903a
    type: comment
  author: RonanMcGovern
  content: Try the colab referenced [here](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/discussions/13#64fc3bea4010eccccc323de2)
  created_at: 2023-09-27 13:25:56+00:00
  edited: false
  hidden: false
  id: 65143b7443afebbc0a6c903a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646dad89f813cfe153eebe94/q_zVk6-asU00mIPT1ngoe.png?w=200&h=200&f=face
      fullname: Niranjan Akella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niranjanakella
      type: user
    createdAt: '2023-09-29T07:07:55.000Z'
    data:
      edited: false
      editors:
      - niranjanakella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5792982578277588
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646dad89f813cfe153eebe94/q_zVk6-asU00mIPT1ngoe.png?w=200&h=200&f=face
          fullname: Niranjan Akella
          isHf: false
          isPro: false
          name: niranjanakella
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ I have referred to the colab notebook. But in my case I am trying to convert\
          \ a AutoModelForSeq2SeqLM flan-t5-large model. </p>\n<pre><code>model_name\
          \ = 'google/flan-t5-large'\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n\
          \    model_name,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n\
          \    device_map='cpu',\n    offload_folder='offload',\n    cache_dir=cache_dir\n\
          )\n</code></pre>\n<p>It is failing at <code>!python convert.py models/</code></p>\n\
          <p><b>ERROR:</b></p>\n<pre><code>Loading model file models/pytorch_model.bin\n\
          Traceback (most recent call last):\n  File \"/content/llama.cpp/convert.py\"\
          , line 1208, in &lt;module&gt;\n    main()\n  File \"/content/llama.cpp/convert.py\"\
          , line 1157, in main\n    params = Params.load(model_plus)\n  File \"/content/llama.cpp/convert.py\"\
          , line 288, in load\n    params = Params.loadHFTransformerJson(model_plus.model,\
          \ hf_config_path)\n  File \"/content/llama.cpp/convert.py\", line 203, in\
          \ loadHFTransformerJson\n    n_embd           = config[\"hidden_size\"]\n\
          KeyError: 'hidden_size'\n</code></pre>\n"
        raw: "@RonanMcGovern I have referred to the colab notebook. But in my case\
          \ I am trying to convert a AutoModelForSeq2SeqLM flan-t5-large model. \n\
          \n```\nmodel_name = 'google/flan-t5-large'\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n\
          \    model_name,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n\
          \    device_map='cpu',\n    offload_folder='offload',\n    cache_dir=cache_dir\n\
          )\n```\nIt is failing at `!python convert.py models/`\n\n<b>ERROR:</b>\n\
          ```\nLoading model file models/pytorch_model.bin\nTraceback (most recent\
          \ call last):\n  File \"/content/llama.cpp/convert.py\", line 1208, in <module>\n\
          \    main()\n  File \"/content/llama.cpp/convert.py\", line 1157, in main\n\
          \    params = Params.load(model_plus)\n  File \"/content/llama.cpp/convert.py\"\
          , line 288, in load\n    params = Params.loadHFTransformerJson(model_plus.model,\
          \ hf_config_path)\n  File \"/content/llama.cpp/convert.py\", line 203, in\
          \ loadHFTransformerJson\n    n_embd           = config[\"hidden_size\"]\n\
          KeyError: 'hidden_size'\n```"
        updatedAt: '2023-09-29T07:07:55.125Z'
      numEdits: 0
      reactions: []
    id: 651677cbb2d07d0b83ed08b1
    type: comment
  author: niranjanakella
  content: "@RonanMcGovern I have referred to the colab notebook. But in my case I\
    \ am trying to convert a AutoModelForSeq2SeqLM flan-t5-large model. \n\n```\n\
    model_name = 'google/flan-t5-large'\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n\
    \    model_name,\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16,\n\
    \    device_map='cpu',\n    offload_folder='offload',\n    cache_dir=cache_dir\n\
    )\n```\nIt is failing at `!python convert.py models/`\n\n<b>ERROR:</b>\n```\n\
    Loading model file models/pytorch_model.bin\nTraceback (most recent call last):\n\
    \  File \"/content/llama.cpp/convert.py\", line 1208, in <module>\n    main()\n\
    \  File \"/content/llama.cpp/convert.py\", line 1157, in main\n    params = Params.load(model_plus)\n\
    \  File \"/content/llama.cpp/convert.py\", line 288, in load\n    params = Params.loadHFTransformerJson(model_plus.model,\
    \ hf_config_path)\n  File \"/content/llama.cpp/convert.py\", line 203, in loadHFTransformerJson\n\
    \    n_embd           = config[\"hidden_size\"]\nKeyError: 'hidden_size'\n```"
  created_at: 2023-09-29 06:07:55+00:00
  edited: false
  hidden: false
  id: 651677cbb2d07d0b83ed08b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-29T08:54:22.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9308456778526306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Ah, I get you now. I''m not an expert but the <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp">docs</a> say that''s not a
          supported model.</p>

          '
        raw: Ah, I get you now. I'm not an expert but the [docs](https://github.com/ggerganov/llama.cpp)
          say that's not a supported model.
        updatedAt: '2023-09-29T08:54:22.863Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - skehlet
    id: 651690beb2d07d0b83f0d982
    type: comment
  author: RonanMcGovern
  content: Ah, I get you now. I'm not an expert but the [docs](https://github.com/ggerganov/llama.cpp)
    say that's not a supported model.
  created_at: 2023-09-29 07:54:22+00:00
  edited: false
  hidden: false
  id: 651690beb2d07d0b83f0d982
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646dad89f813cfe153eebe94/q_zVk6-asU00mIPT1ngoe.png?w=200&h=200&f=face
      fullname: Niranjan Akella
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: niranjanakella
      type: user
    createdAt: '2023-09-29T09:02:34.000Z'
    data:
      edited: false
      editors:
      - niranjanakella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602636098861694
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646dad89f813cfe153eebe94/q_zVk6-asU00mIPT1ngoe.png?w=200&h=200&f=face
          fullname: Niranjan Akella
          isHf: false
          isPro: false
          name: niranjanakella
          type: user
        html: "<p>Yes <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>,\
          \ So is there any way to convert it to GGUF in any other way? Or have you\
          \ come across any repo or snippet that could help? or if not how to approach\
          \ this. Thank you.</p>\n"
        raw: Yes @RonanMcGovern, So is there any way to convert it to GGUF in any
          other way? Or have you come across any repo or snippet that could help?
          or if not how to approach this. Thank you.
        updatedAt: '2023-09-29T09:02:34.853Z'
      numEdits: 0
      reactions: []
    id: 651692aa18f3a57f869a52aa
    type: comment
  author: niranjanakella
  content: Yes @RonanMcGovern, So is there any way to convert it to GGUF in any other
    way? Or have you come across any repo or snippet that could help? or if not how
    to approach this. Thank you.
  created_at: 2023-09-29 08:02:34+00:00
  edited: false
  hidden: false
  id: 651692aa18f3a57f869a52aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-09-29T09:04:21.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608417749404907
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Maybe post an issue in Llama cpp and ask for guidance on how to
          approach it.</p>

          '
        raw: Maybe post an issue in Llama cpp and ask for guidance on how to approach
          it.
        updatedAt: '2023-09-29T09:04:21.937Z'
      numEdits: 0
      reactions: []
    id: 651693153e5a12e0cec092fe
    type: comment
  author: RonanMcGovern
  content: Maybe post an issue in Llama cpp and ask for guidance on how to approach
    it.
  created_at: 2023-09-29 08:04:21+00:00
  edited: false
  hidden: false
  id: 651693153e5a12e0cec092fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-7B-GGUF
repo_type: model
status: open
target_branch: null
title: How do I convert flan-t5-large model to GGUF? Already tried convert.py from
  llama.cpp
