!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kusoge
conflicting_files: null
created_at: 2023-04-21 15:46:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf0eaf89fa1ea6ad1a350d41a82f03af.svg
      fullname: Shin Asura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kusoge
      type: user
    createdAt: '2023-04-21T16:46:27.000Z'
    data:
      edited: true
      editors:
      - kusoge
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf0eaf89fa1ea6ad1a350d41a82f03af.svg
          fullname: Shin Asura
          isHf: false
          isPro: false
          name: kusoge
          type: user
        html: '<p>Any idea how much RAM is needed to preload the model and VRAM to
          actually use it?<br>The 4bit 13B models I tried fills my 16GB RAM and during
          usage my VRAM completely fills my 12GB VRAM.<br>Will 32GB RAM and 18GB VRAM
          be enough to load and use this? Thanks</p>

          '
        raw: "Any idea how much RAM is needed to preload the model and VRAM to actually\
          \ use it? \nThe 4bit 13B models I tried fills my 16GB RAM and during usage\
          \ my VRAM completely fills my 12GB VRAM.\nWill 32GB RAM and 18GB VRAM be\
          \ enough to load and use this? Thanks"
        updatedAt: '2023-04-21T16:46:54.814Z'
      numEdits: 1
      reactions: []
    id: 6442bde39788699939b17822
    type: comment
  author: kusoge
  content: "Any idea how much RAM is needed to preload the model and VRAM to actually\
    \ use it? \nThe 4bit 13B models I tried fills my 16GB RAM and during usage my\
    \ VRAM completely fills my 12GB VRAM.\nWill 32GB RAM and 18GB VRAM be enough to\
    \ load and use this? Thanks"
  created_at: 2023-04-21 15:46:27+00:00
  edited: true
  hidden: false
  id: 6442bde39788699939b17822
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d4a400640bb0f77180091b/vegaepayjkuTcdXmB6qb_.png?w=200&h=200&f=face
      fullname: Peepy
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Peeepy
      type: user
    createdAt: '2023-04-23T03:31:18.000Z'
    data:
      edited: false
      editors:
      - Peeepy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d4a400640bb0f77180091b/vegaepayjkuTcdXmB6qb_.png?w=200&h=200&f=face
          fullname: Peepy
          isHf: false
          isPro: false
          name: Peeepy
          type: user
        html: '<p>Hey! Sorry. I didn''t notice this until just now.<br>For a better
          overview of the exact requirements see: <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/">https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/</a><br>But
          TL;DR: No. Minimum VRAM needed is 20GB, 23GB with full context. 24GB RAM
          is needed, so on the RAM side you''d be set.</p>

          '
        raw: 'Hey! Sorry. I didn''t notice this until just now.

          For a better overview of the exact requirements see: https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/

          But TL;DR: No. Minimum VRAM needed is 20GB, 23GB with full context. 24GB
          RAM is needed, so on the RAM side you''d be set.'
        updatedAt: '2023-04-23T03:31:18.668Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - kusoge
    id: 6444a686c63001ae635c0ebc
    type: comment
  author: Peeepy
  content: 'Hey! Sorry. I didn''t notice this until just now.

    For a better overview of the exact requirements see: https://www.reddit.com/r/LocalLLaMA/comments/11o6o3f/how_to_install_llama_8bit_and_4bit/

    But TL;DR: No. Minimum VRAM needed is 20GB, 23GB with full context. 24GB RAM is
    needed, so on the RAM side you''d be set.'
  created_at: 2023-04-23 02:31:18+00:00
  edited: false
  hidden: false
  id: 6444a686c63001ae635c0ebc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Peeepy/llama-30b-oasst-4bit-128g
repo_type: model
status: open
target_branch: null
title: Minimum system requirements?
