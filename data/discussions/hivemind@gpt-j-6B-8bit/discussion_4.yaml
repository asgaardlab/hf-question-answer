!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomwjhtom
conflicting_files: null
created_at: 2022-09-03 02:37:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
      fullname: Junhao Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomwjhtom
      type: user
    createdAt: '2022-09-03T03:37:29.000Z'
    data:
      edited: false
      editors:
      - tomwjhtom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
          fullname: Junhao Wang
          isHf: false
          isPro: false
          name: tomwjhtom
          type: user
        html: '<p>In Colab, <code>pip install bitsandbytes-cuda111==0.26.0</code>
          fails with error message </p>

          <pre><code class="language-console">Looking in indexes: https://pypi.org/simple,
          https://us-python.pkg.dev/colab-wheels/public/simple/

          ERROR: Could not find a version that satisfies the requirement bitsandbytes-cuda111==0.26.0
          (from versions: 0.26.0.post2)

          ERROR: No matching distribution found for bitsandbytes-cuda111==0.26.0

          </code></pre>

          '
        raw: "In Colab, `pip install bitsandbytes-cuda111==0.26.0` fails with error\
          \ message \r\n```console\r\nLooking in indexes: https://pypi.org/simple,\
          \ https://us-python.pkg.dev/colab-wheels/public/simple/\r\nERROR: Could\
          \ not find a version that satisfies the requirement bitsandbytes-cuda111==0.26.0\
          \ (from versions: 0.26.0.post2)\r\nERROR: No matching distribution found\
          \ for bitsandbytes-cuda111==0.26.0\r\n```"
        updatedAt: '2022-09-03T03:37:29.917Z'
      numEdits: 0
      reactions: []
    id: 6312cbf999791aa61d19dc1a
    type: comment
  author: tomwjhtom
  content: "In Colab, `pip install bitsandbytes-cuda111==0.26.0` fails with error\
    \ message \r\n```console\r\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\r\
    \nERROR: Could not find a version that satisfies the requirement bitsandbytes-cuda111==0.26.0\
    \ (from versions: 0.26.0.post2)\r\nERROR: No matching distribution found for bitsandbytes-cuda111==0.26.0\r\
    \n```"
  created_at: 2022-09-03 02:37:29+00:00
  edited: false
  hidden: false
  id: 6312cbf999791aa61d19dc1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
      fullname: Mukesh Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MukeshSharma
      type: user
    createdAt: '2022-09-03T09:59:52.000Z'
    data:
      edited: false
      editors:
      - MukeshSharma
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9e47a2bf15ad2674fa3faf8eabf1aa0.svg
          fullname: Mukesh Sharma
          isHf: false
          isPro: false
          name: MukeshSharma
          type: user
        html: '<p>nothing to worry for it , just use pip install bitsandbytes and
          everything will work as usual . But there is error in loading the model
          , this is from last 3 days</p>

          '
        raw: nothing to worry for it , just use pip install bitsandbytes and everything
          will work as usual . But there is error in loading the model , this is from
          last 3 days
        updatedAt: '2022-09-03T09:59:52.217Z'
      numEdits: 0
      reactions: []
    id: 6313259817838d05194bc751
    type: comment
  author: MukeshSharma
  content: nothing to worry for it , just use pip install bitsandbytes and everything
    will work as usual . But there is error in loading the model , this is from last
    3 days
  created_at: 2022-09-03 08:59:52+00:00
  edited: false
  hidden: false
  id: 6313259817838d05194bc751
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
      fullname: Junhao Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomwjhtom
      type: user
    createdAt: '2022-09-03T16:45:08.000Z'
    data:
      edited: false
      editors:
      - tomwjhtom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/083e21832a595b0e92e2a9133912ed2d.svg
          fullname: Junhao Wang
          isHf: false
          isPro: false
          name: tomwjhtom
          type: user
        html: "<p>Is this the error you are referring to?<br>Running this block below</p>\n\
          <pre><code class=\"language-python\">gpt = GPTJForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"hivemind/gpt-j-6B-8bit\"</span>, low_cpu_mem_usage=<span\
          \ class=\"hljs-literal\">True</span>)\n\ndevice = <span class=\"hljs-string\"\
          >'cuda'</span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">'cpu'</span>\n\
          gpt.to(device)\n</code></pre>\n<p>leads to this error message</p>\n<pre><code\
          \ class=\"language-python\">Downloading config.json: <span class=\"hljs-number\"\
          >100</span>%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | <span class=\"hljs-number\">1.00</span>k/<span class=\"hljs-number\">1.00</span>k\
          \ [<span class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span>&lt;<span\
          \ class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span>,\
          \ 420kB/s]\nDownloading pytorch_model.<span class=\"hljs-built_in\">bin</span>:\
          \ <span class=\"hljs-number\">100</span>%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| <span class=\"hljs-number\">5.75</span>G/<span\
          \ class=\"hljs-number\">5.75</span>G [01:<span class=\"hljs-number\">52</span>&lt;<span\
          \ class=\"hljs-number\">00</span>:<span class=\"hljs-number\">00</span>,\
          \ <span class=\"hljs-number\">54.9</span>MB/s]\n---------------------------------------------------------------------------\n\
          NameError                                 Traceback (most recent call last)\n\
          /home/junhao/play/finetune_gpt_j_6B_8bit.ipynb Cell <span class=\"hljs-number\"\
          >11</span> <span class=\"hljs-keyword\">in</span> &lt;cell line: <span class=\"\
          hljs-number\">1</span>&gt;()\n----&gt; <span class=\"hljs-number\">1</span>\
          \ gpt = GPTJForCausalLM.from_pretrained(<span class=\"hljs-string\">\"hivemind/gpt-j-6B-8bit\"\
          </span>, low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>)\n \
          \     <span class=\"hljs-number\">3</span> device = <span class=\"hljs-string\"\
          >'cuda'</span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">'cpu'</span>\n\
          \      <span class=\"hljs-number\">4</span> gpt.to(device)\n\nFile ~/play/.env_gptj/lib/python3<span\
          \ class=\"hljs-number\">.8</span>/site-packages/transformers/modeling_utils.py:<span\
          \ class=\"hljs-number\">2110</span>, <span class=\"hljs-keyword\">in</span>\
          \ PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
          \ **kwargs)\n   <span class=\"hljs-number\">2108</span>     init_contexts\
          \ = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())] + init_contexts\n\
          \   <span class=\"hljs-number\">2109</span> <span class=\"hljs-keyword\"\
          >elif</span> low_cpu_mem_usage:\n-&gt; <span class=\"hljs-number\">2110</span>\
          \     init_contexts.append(init_empty_weights())\n   <span class=\"hljs-number\"\
          >2112</span> <span class=\"hljs-keyword\">with</span> ContextManagers(init_contexts):\n\
          \   <span class=\"hljs-number\">2113</span>     model = cls(config, *model_args,\
          \ **model_kwargs)\n\nNameError: name <span class=\"hljs-string\">'init_empty_weights'</span>\
          \ <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\">not</span>\
          \ defined\n</code></pre>\n"
        raw: "Is this the error you are referring to?\nRunning this block below\n\
          ```python\ngpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\"\
          , low_cpu_mem_usage=True)\n\ndevice = 'cuda' if torch.cuda.is_available()\
          \ else 'cpu'\ngpt.to(device)\n```\nleads to this error message\n```python\n\
          Downloading config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 1.00k/1.00k [00:00<00:00, 420kB/s]\nDownloading pytorch_model.bin:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.75G/5.75G\
          \ [01:52<00:00, 54.9MB/s]\n---------------------------------------------------------------------------\n\
          NameError                                 Traceback (most recent call last)\n\
          /home/junhao/play/finetune_gpt_j_6B_8bit.ipynb Cell 11 in <cell line: 1>()\n\
          ----> 1 gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\"\
          , low_cpu_mem_usage=True)\n      3 device = 'cuda' if torch.cuda.is_available()\
          \ else 'cpu'\n      4 gpt.to(device)\n\nFile ~/play/.env_gptj/lib/python3.8/site-packages/transformers/modeling_utils.py:2110,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2108     init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())]\
          \ + init_contexts\n   2109 elif low_cpu_mem_usage:\n-> 2110     init_contexts.append(init_empty_weights())\n\
          \   2112 with ContextManagers(init_contexts):\n   2113     model = cls(config,\
          \ *model_args, **model_kwargs)\n\nNameError: name 'init_empty_weights' is\
          \ not defined\n```"
        updatedAt: '2022-09-03T16:45:08.323Z'
      numEdits: 0
      reactions: []
    id: 631384940b24eab4746d19ec
    type: comment
  author: tomwjhtom
  content: "Is this the error you are referring to?\nRunning this block below\n```python\n\
    gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n\
    \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ngpt.to(device)\n```\n\
    leads to this error message\n```python\nDownloading config.json: 100%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.00k/1.00k [00:00<00:00, 420kB/s]\n\
    Downloading pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 5.75G/5.75G [01:52<00:00, 54.9MB/s]\n---------------------------------------------------------------------------\n\
    NameError                                 Traceback (most recent call last)\n\
    /home/junhao/play/finetune_gpt_j_6B_8bit.ipynb Cell 11 in <cell line: 1>()\n---->\
    \ 1 gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n\
    \      3 device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      4 gpt.to(device)\n\
    \nFile ~/play/.env_gptj/lib/python3.8/site-packages/transformers/modeling_utils.py:2110,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n   2108     init_contexts = [deepspeed.zero.Init(config_dict_or_path=deepspeed_config())]\
    \ + init_contexts\n   2109 elif low_cpu_mem_usage:\n-> 2110     init_contexts.append(init_empty_weights())\n\
    \   2112 with ContextManagers(init_contexts):\n   2113     model = cls(config,\
    \ *model_args, **model_kwargs)\n\nNameError: name 'init_empty_weights' is not\
    \ defined\n```"
  created_at: 2022-09-03 15:45:08+00:00
  edited: false
  hidden: false
  id: 631384940b24eab4746d19ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
      fullname: Yozh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: justheuristic
      type: user
    createdAt: '2022-09-06T14:27:57.000Z'
    data:
      edited: false
      editors:
      - justheuristic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
          fullname: Yozh
          isHf: false
          isPro: false
          name: justheuristic
          type: user
        html: '<p>This specific error is fixed by installing <code>accelerate</code><br>It
          tries to import from accelerate, and if it''s not installed, throws this
          error( <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L74">reference</a>)</p>

          <p>However, please note that our code  was superceded by the <a rel="nofollow"
          href="https://github.com/huggingface/transformers/pull/17901"><code>load_in_8bit=True</code>
          feature in transformers</a><br>by Younes Belkada and Tim Dettmers. Please
          see <a rel="nofollow" href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O">this
          usage example</a>.<br>This legacy model was built for <a rel="nofollow"
          href="https://github.com/huggingface/transformers/releases/tag/v4.15.0">transformers
          v4.15.0</a> and pytorch 1.11. Newer versions could work, but are not supported.</p>

          '
        raw: 'This specific error is fixed by installing `accelerate`

          It tries to import from accelerate, and if it''s not installed, throws this
          error( [reference](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L74))


          However, please note that our code  was superceded by the [`load_in_8bit=True`
          feature in transformers](https://github.com/huggingface/transformers/pull/17901)

          by Younes Belkada and Tim Dettmers. Please see [this usage example](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O).

          This legacy model was built for [transformers v4.15.0](https://github.com/huggingface/transformers/releases/tag/v4.15.0)
          and pytorch 1.11. Newer versions could work, but are not supported.'
        updatedAt: '2022-09-06T14:27:57.065Z'
      numEdits: 0
      reactions: []
    id: 631758ed23ac76623d6021fc
    type: comment
  author: justheuristic
  content: 'This specific error is fixed by installing `accelerate`

    It tries to import from accelerate, and if it''s not installed, throws this error(
    [reference](https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L74))


    However, please note that our code  was superceded by the [`load_in_8bit=True`
    feature in transformers](https://github.com/huggingface/transformers/pull/17901)

    by Younes Belkada and Tim Dettmers. Please see [this usage example](https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4#scrollTo=W8tQtyjp75O).

    This legacy model was built for [transformers v4.15.0](https://github.com/huggingface/transformers/releases/tag/v4.15.0)
    and pytorch 1.11. Newer versions could work, but are not supported.'
  created_at: 2022-09-06 13:27:57+00:00
  edited: false
  hidden: false
  id: 631758ed23ac76623d6021fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f75788572c021be584f4c097016cbcab.svg
      fullname: Ayush Bihani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hsuyab
      type: user
    createdAt: '2023-03-26T12:00:25.000Z'
    data:
      edited: false
      editors:
      - hsuyab
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f75788572c021be584f4c097016cbcab.svg
          fullname: Ayush Bihani
          isHf: false
          isPro: false
          name: hsuyab
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;justheuristic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/justheuristic\"\
          >@<span class=\"underline\">justheuristic</span></a></span>\n\n\t</span></span>\
          \ If I understand this correctly, this work to use an 8bit quantized model\
          \ can be done just by passing the <code>load_in_8bit=True</code> in transformers\
          \ load model pipeline when calling the <strong>Eluether/gpt-j-6b model</strong>,\
          \ right?</p>\n"
        raw: '@justheuristic If I understand this correctly, this work to use an 8bit
          quantized model can be done just by passing the `load_in_8bit=True` in transformers
          load model pipeline when calling the **Eluether/gpt-j-6b model**, right?'
        updatedAt: '2023-03-26T12:00:25.187Z'
      numEdits: 0
      reactions: []
    id: 642033d9328527d316de521a
    type: comment
  author: hsuyab
  content: '@justheuristic If I understand this correctly, this work to use an 8bit
    quantized model can be done just by passing the `load_in_8bit=True` in transformers
    load model pipeline when calling the **Eluether/gpt-j-6b model**, right?'
  created_at: 2023-03-26 11:00:25+00:00
  edited: false
  hidden: false
  id: 642033d9328527d316de521a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: hivemind/gpt-j-6B-8bit
repo_type: model
status: open
target_branch: null
title: bitsandbytes-cuda111==0.26.0 not found
