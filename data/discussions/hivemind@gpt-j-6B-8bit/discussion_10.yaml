!!python/object:huggingface_hub.community.DiscussionWithDetails
author: realtimeriddle
conflicting_files: null
created_at: 2022-10-18 00:34:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
      fullname: Adam Riddle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: realtimeriddle
      type: user
    createdAt: '2022-10-18T01:34:23.000Z'
    data:
      edited: false
      editors:
      - realtimeriddle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
          fullname: Adam Riddle
          isHf: false
          isPro: false
          name: realtimeriddle
          type: user
        html: '<p>Is there a good way to add special tokens when training? I get an
          error when I use the resize_token_embedding() function because this model
          uses FrozenBNBEmbeddings. I already have tried recreating the function with
          the frozen embeddings in mind but I''m not sure that can work with this
          kind of model.</p>

          '
        raw: Is there a good way to add special tokens when training? I get an error
          when I use the resize_token_embedding() function because this model uses
          FrozenBNBEmbeddings. I already have tried recreating the function with the
          frozen embeddings in mind but I'm not sure that can work with this kind
          of model.
        updatedAt: '2022-10-18T01:34:23.622Z'
      numEdits: 0
      reactions: []
    id: 634e029f418913d58466fc2f
    type: comment
  author: realtimeriddle
  content: Is there a good way to add special tokens when training? I get an error
    when I use the resize_token_embedding() function because this model uses FrozenBNBEmbeddings.
    I already have tried recreating the function with the frozen embeddings in mind
    but I'm not sure that can work with this kind of model.
  created_at: 2022-10-18 00:34:23+00:00
  edited: false
  hidden: false
  id: 634e029f418913d58466fc2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
      fullname: Yozh
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: justheuristic
      type: user
    createdAt: '2022-10-18T11:47:09.000Z'
    data:
      edited: false
      editors:
      - justheuristic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640134437444-609baae0fe087f3d04cf0481.jpeg?w=200&h=200&f=face
          fullname: Yozh
          isHf: false
          isPro: false
          name: justheuristic
          type: user
        html: '<p>Please note: this code has been deprecated for 4 months now. See
          README for the updated version.</p>

          <ol>

          <li>you can look inside the existing tokenizer - there are already some
          ~unused tokens that can be reused as special tokens</li>

          <li>you can resize embeddings in the original model, then run the quantization
          notebook to get its 8-bit version (see README) . This will take some effort</li>

          </ol>

          <p>However, the newly added tokens will <em>not</em> be trained because
          their embeddings are <em>frozen</em>. You can also implement a custom forward
          code where the new tokens will be stored in a <em>separate</em> torch.nn.Embedding
          layer which is not quantized, and hence, fully trainable. In that case,
          you will need to slightly modify the existing forward pass code - which
          is something that you will need to figure out by yourself (see note above).</p>

          '
        raw: 'Please note: this code has been deprecated for 4 months now. See README
          for the updated version.


          1) you can look inside the existing tokenizer - there are already some ~unused
          tokens that can be reused as special tokens

          2) you can resize embeddings in the original model, then run the quantization
          notebook to get its 8-bit version (see README) . This will take some effort


          However, the newly added tokens will *not* be trained because their embeddings
          are *frozen*. You can also implement a custom forward code where the new
          tokens will be stored in a *separate* torch.nn.Embedding layer which is
          not quantized, and hence, fully trainable. In that case, you will need to
          slightly modify the existing forward pass code - which is something that
          you will need to figure out by yourself (see note above).'
        updatedAt: '2022-10-18T11:47:09.939Z'
      numEdits: 0
      reactions: []
    id: 634e923d23788038c5ecadde
    type: comment
  author: justheuristic
  content: 'Please note: this code has been deprecated for 4 months now. See README
    for the updated version.


    1) you can look inside the existing tokenizer - there are already some ~unused
    tokens that can be reused as special tokens

    2) you can resize embeddings in the original model, then run the quantization
    notebook to get its 8-bit version (see README) . This will take some effort


    However, the newly added tokens will *not* be trained because their embeddings
    are *frozen*. You can also implement a custom forward code where the new tokens
    will be stored in a *separate* torch.nn.Embedding layer which is not quantized,
    and hence, fully trainable. In that case, you will need to slightly modify the
    existing forward pass code - which is something that you will need to figure out
    by yourself (see note above).'
  created_at: 2022-10-18 10:47:09+00:00
  edited: false
  hidden: false
  id: 634e923d23788038c5ecadde
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
      fullname: Adam Riddle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: realtimeriddle
      type: user
    createdAt: '2022-10-18T15:07:39.000Z'
    data:
      edited: false
      editors:
      - realtimeriddle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
          fullname: Adam Riddle
          isHf: false
          isPro: false
          name: realtimeriddle
          type: user
        html: '<p>Yes, resizing the original model and then running the quantization
          notebook does seem to work.<br>Also, the new token are not being trained.
          Thank you for clarifying.</p>

          '
        raw: 'Yes, resizing the original model and then running the quantization notebook
          does seem to work.

          Also, the new token are not being trained. Thank you for clarifying.'
        updatedAt: '2022-10-18T15:07:39.507Z'
      numEdits: 0
      reactions: []
    id: 634ec13b2b8c811f3cc0b110
    type: comment
  author: realtimeriddle
  content: 'Yes, resizing the original model and then running the quantization notebook
    does seem to work.

    Also, the new token are not being trained. Thank you for clarifying.'
  created_at: 2022-10-18 14:07:39+00:00
  edited: false
  hidden: false
  id: 634ec13b2b8c811f3cc0b110
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
      fullname: Victor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vinnitu
      type: user
    createdAt: '2023-05-12T14:43:32.000Z'
    data:
      edited: false
      editors:
      - vinnitu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9167f9f0abda20ccacf5bfcadb55fafa.svg
          fullname: Victor
          isHf: false
          isPro: false
          name: vinnitu
          type: user
        html: '<p>can we use &lt;|extratoken_1|&gt;, &lt;|extratoken_2|&gt;, etc.?</p>

          '
        raw: can we use <|extratoken_1|>, <|extratoken_2|>, etc.?
        updatedAt: '2023-05-12T14:43:32.765Z'
      numEdits: 0
      reactions: []
    id: 645e50946d343f4bb611889e
    type: comment
  author: vinnitu
  content: can we use <|extratoken_1|>, <|extratoken_2|>, etc.?
  created_at: 2023-05-12 13:43:32+00:00
  edited: false
  hidden: false
  id: 645e50946d343f4bb611889e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
      fullname: Adam Riddle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: realtimeriddle
      type: user
    createdAt: '2023-05-23T16:04:16.000Z'
    data:
      edited: false
      editors:
      - realtimeriddle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
          fullname: Adam Riddle
          isHf: false
          isPro: false
          name: realtimeriddle
          type: user
        html: '<p>Maybe? I tried that back in October and I forget how it went. In
          any case, justheuristic was correct, the model was depreciated at the time
          and I think the never version of the transformers library has a better solution
          for training 8-bit models now anyways.</p>

          '
        raw: Maybe? I tried that back in October and I forget how it went. In any
          case, justheuristic was correct, the model was depreciated at the time and
          I think the never version of the transformers library has a better solution
          for training 8-bit models now anyways.
        updatedAt: '2023-05-23T16:04:16.213Z'
      numEdits: 0
      reactions: []
    id: 646ce400acc13867a12a7f1e
    type: comment
  author: realtimeriddle
  content: Maybe? I tried that back in October and I forget how it went. In any case,
    justheuristic was correct, the model was depreciated at the time and I think the
    never version of the transformers library has a better solution for training 8-bit
    models now anyways.
  created_at: 2023-05-23 15:04:16+00:00
  edited: false
  hidden: false
  id: 646ce400acc13867a12a7f1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a0cb01167b145d37a5bb695407927584.svg
      fullname: Adam Riddle
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: realtimeriddle
      type: user
    createdAt: '2023-05-23T16:04:24.000Z'
    data:
      status: closed
    id: 646ce408e0c5e395734b44ce
    type: status-change
  author: realtimeriddle
  created_at: 2023-05-23 15:04:24+00:00
  id: 646ce408e0c5e395734b44ce
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: hivemind/gpt-j-6B-8bit
repo_type: model
status: closed
target_branch: null
title: Adding Special Tokens for Training
