!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mht0228
conflicting_files: null
created_at: 2023-09-22 07:15:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0885e7d04425969f3c823c785e205be.svg
      fullname: tavon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mht0228
      type: user
    createdAt: '2023-09-22T08:15:39.000Z'
    data:
      edited: false
      editors:
      - mht0228
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6345987915992737
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0885e7d04425969f3c823c785e205be.svg
          fullname: tavon
          isHf: false
          isPro: false
          name: mht0228
          type: user
        html: "<p>I am working on T5 finetune model , and I found the t5-small-finetuned-samsum-en\
          \ model on huggingface and the rouge metric on samsum Benchmark (Summarization)\
          \ | Papers With Code<br>It shows: (I have delete other models except t5-small-finetuned-samsum-en)</p>\n\
          <p>Rank\tModel\tROUGE-1\tROUGE-2\tROUGE-L\tROUGE-LSUM\tgen_len\tloss\tDetails\t\
          Year\tTags<br>12\tt5-small-finetuned-samsum-en\t40.039\t15.85\t31.808\t\
          36.089\t18.107\t2.192\t\t2022\t<br>But when I use transformers examples\
          \ code run_summarization.py run the model I have download from huggingface\
          \ [t5-small-finetuned-samsum-en] , The result is not consistent with above.<br>My\
          \ result is: my ROUGE-1 is 3.34 but the result above is 40.039, It is very\
          \ different. Here is my result: (The value should *100)<br>***** eval metrics\
          \ *****<br>eval_gen_len = 12.2066<br>eval_loss = 7.7219<br>eval_rouge1_high_fmeasure\
          \ = 0.0383<br>eval_rouge1_high_precision = 0.2029<br>eval_rouge1_high_recall\
          \ = 0.0225<br>eval_rouge1_low_fmeasure = 0.0334<br>eval_rouge1_low_precision\
          \ = 0.1792<br>eval_rouge1_low_recall = 0.0194<br>eval_rouge1_mid_fmeasure\
          \ = 0.0358<br>eval_rouge1_mid_precision = 0.1909<br>eval_rouge1_mid_recall\
          \ = 0.0209<br>eval_rouge2_high_fmeasure = 0.0021<br>eval_rouge2_high_precision\
          \ = 0.0114<br>eval_rouge2_high_recall = 0.0012<br>eval_rouge2_low_fmeasure\
          \ = 0.0011<br>eval_rouge2_low_precision = 0.006<br>eval_rouge2_low_recall\
          \ = 0.0006<br>eval_rouge2_mid_fmeasure = 0.0016<br>eval_rouge2_mid_precision\
          \ = 0.0086<br>eval_rouge2_mid_recall = 0.0009<br>eval_rougeL_high_fmeasure\
          \ = 0.0334<br>eval_rougeL_high_precision = 0.1803<br>eval_rougeL_high_recall\
          \ = 0.0197<br>eval_rougeL_low_fmeasure = 0.0293<br>eval_rougeL_low_precision\
          \ = 0.1587<br>eval_rougeL_low_recall = 0.0171<br>eval_rougeL_mid_fmeasure\
          \ = 0.0314<br>eval_rougeL_mid_precision = 0.1688<br>eval_rougeL_mid_recall\
          \ = 0.0184<br>eval_rougeLsum_high_fmeasure = 0.0362<br>eval_rougeLsum_high_precision\
          \ = 0.1951<br>eval_rougeLsum_high_recall = 0.0213<br>eval_rougeLsum_low_fmeasure\
          \ = 0.0317<br>eval_rougeLsum_low_precision = 0.1708<br>eval_rougeLsum_low_recall\
          \ = 0.0184<br>eval_rougeLsum_mid_fmeasure = 0.034<br>eval_rougeLsum_mid_precision\
          \ = 0.1832<br>eval_rougeLsum_mid_recall = 0.0199<br>eval_runtime = 0:00:54.57<br>eval_samples\
          \ = 818<br>eval_samples_per_second = 14.988<br>eval_steps_per_second = 14.988<br>And\
          \ I have not change the run_summarization.py script ,<br>My shell script\
          \ is:<br>python ./run_summarization_.py<br>\u2013model_name_or_path $T5_DIR<br>\u2013\
          do_eval<br>\u2013source_prefix \"summarize: \"<br>\u2013output_dir ./output/huggingface-summarization<br>\u2013\
          dataset_name $samsum_path<br>\u2013dataset_config \u201C3.0.0\u201D<br>\u2013\
          per_device_train_batch_size=1<br>\u2013per_device_eval_batch_size=1<br>\u2013\
          overwrite_output_dir<br>\u2013predict_with_generate \\</p>\n<p>Can anyone\
          \ help me on this issue? Thanks a lot.</p>\n"
        raw: "I am working on T5 finetune model , and I found the t5-small-finetuned-samsum-en\
          \ model on huggingface and the rouge metric on samsum Benchmark (Summarization)\
          \ | Papers With Code\r\nIt shows: (I have delete other models except t5-small-finetuned-samsum-en)\r\
          \n\r\n\r\nRank\tModel\tROUGE-1\tROUGE-2\tROUGE-L\tROUGE-LSUM\tgen_len\t\
          loss\tDetails\tYear\tTags\r\n12\tt5-small-finetuned-samsum-en\t40.039\t\
          15.85\t31.808\t36.089\t18.107\t2.192\t\t2022\t\r\nBut when I use transformers\
          \ examples code run_summarization.py run the model I have download from\
          \ huggingface [t5-small-finetuned-samsum-en] , The result is not consistent\
          \ with above.\r\nMy result is: my ROUGE-1 is 3.34 but the result above is\
          \ 40.039, It is very different. Here is my result: (The value should *100)\r\
          \n***** eval metrics *****\r\neval_gen_len = 12.2066\r\neval_loss = 7.7219\r\
          \neval_rouge1_high_fmeasure = 0.0383\r\neval_rouge1_high_precision = 0.2029\r\
          \neval_rouge1_high_recall = 0.0225\r\neval_rouge1_low_fmeasure = 0.0334\r\
          \neval_rouge1_low_precision = 0.1792\r\neval_rouge1_low_recall = 0.0194\r\
          \neval_rouge1_mid_fmeasure = 0.0358\r\neval_rouge1_mid_precision = 0.1909\r\
          \neval_rouge1_mid_recall = 0.0209\r\neval_rouge2_high_fmeasure = 0.0021\r\
          \neval_rouge2_high_precision = 0.0114\r\neval_rouge2_high_recall = 0.0012\r\
          \neval_rouge2_low_fmeasure = 0.0011\r\neval_rouge2_low_precision = 0.006\r\
          \neval_rouge2_low_recall = 0.0006\r\neval_rouge2_mid_fmeasure = 0.0016\r\
          \neval_rouge2_mid_precision = 0.0086\r\neval_rouge2_mid_recall = 0.0009\r\
          \neval_rougeL_high_fmeasure = 0.0334\r\neval_rougeL_high_precision = 0.1803\r\
          \neval_rougeL_high_recall = 0.0197\r\neval_rougeL_low_fmeasure = 0.0293\r\
          \neval_rougeL_low_precision = 0.1587\r\neval_rougeL_low_recall = 0.0171\r\
          \neval_rougeL_mid_fmeasure = 0.0314\r\neval_rougeL_mid_precision = 0.1688\r\
          \neval_rougeL_mid_recall = 0.0184\r\neval_rougeLsum_high_fmeasure = 0.0362\r\
          \neval_rougeLsum_high_precision = 0.1951\r\neval_rougeLsum_high_recall =\
          \ 0.0213\r\neval_rougeLsum_low_fmeasure = 0.0317\r\neval_rougeLsum_low_precision\
          \ = 0.1708\r\neval_rougeLsum_low_recall = 0.0184\r\neval_rougeLsum_mid_fmeasure\
          \ = 0.034\r\neval_rougeLsum_mid_precision = 0.1832\r\neval_rougeLsum_mid_recall\
          \ = 0.0199\r\neval_runtime = 0:00:54.57\r\neval_samples = 818\r\neval_samples_per_second\
          \ = 14.988\r\neval_steps_per_second = 14.988\r\nAnd I have not change the\
          \ run_summarization.py script ,\r\nMy shell script is:\r\npython ./run_summarization_.py\r\
          \n\u2013model_name_or_path $T5_DIR\r\n\u2013do_eval\r\n\u2013source_prefix\
          \ \"summarize: \"\r\n\u2013output_dir ./output/huggingface-summarization\r\
          \n\u2013dataset_name $samsum_path\r\n\u2013dataset_config \u201C3.0.0\u201D\
          \r\n\u2013per_device_train_batch_size=1\r\n\u2013per_device_eval_batch_size=1\r\
          \n\u2013overwrite_output_dir\r\n\u2013predict_with_generate \\\r\n\r\nCan\
          \ anyone help me on this issue? Thanks a lot."
        updatedAt: '2023-09-22T08:15:39.183Z'
      numEdits: 0
      reactions: []
    id: 650d4d2b3e1cba5f625f08eb
    type: comment
  author: mht0228
  content: "I am working on T5 finetune model , and I found the t5-small-finetuned-samsum-en\
    \ model on huggingface and the rouge metric on samsum Benchmark (Summarization)\
    \ | Papers With Code\r\nIt shows: (I have delete other models except t5-small-finetuned-samsum-en)\r\
    \n\r\n\r\nRank\tModel\tROUGE-1\tROUGE-2\tROUGE-L\tROUGE-LSUM\tgen_len\tloss\t\
    Details\tYear\tTags\r\n12\tt5-small-finetuned-samsum-en\t40.039\t15.85\t31.808\t\
    36.089\t18.107\t2.192\t\t2022\t\r\nBut when I use transformers examples code run_summarization.py\
    \ run the model I have download from huggingface [t5-small-finetuned-samsum-en]\
    \ , The result is not consistent with above.\r\nMy result is: my ROUGE-1 is 3.34\
    \ but the result above is 40.039, It is very different. Here is my result: (The\
    \ value should *100)\r\n***** eval metrics *****\r\neval_gen_len = 12.2066\r\n\
    eval_loss = 7.7219\r\neval_rouge1_high_fmeasure = 0.0383\r\neval_rouge1_high_precision\
    \ = 0.2029\r\neval_rouge1_high_recall = 0.0225\r\neval_rouge1_low_fmeasure = 0.0334\r\
    \neval_rouge1_low_precision = 0.1792\r\neval_rouge1_low_recall = 0.0194\r\neval_rouge1_mid_fmeasure\
    \ = 0.0358\r\neval_rouge1_mid_precision = 0.1909\r\neval_rouge1_mid_recall = 0.0209\r\
    \neval_rouge2_high_fmeasure = 0.0021\r\neval_rouge2_high_precision = 0.0114\r\n\
    eval_rouge2_high_recall = 0.0012\r\neval_rouge2_low_fmeasure = 0.0011\r\neval_rouge2_low_precision\
    \ = 0.006\r\neval_rouge2_low_recall = 0.0006\r\neval_rouge2_mid_fmeasure = 0.0016\r\
    \neval_rouge2_mid_precision = 0.0086\r\neval_rouge2_mid_recall = 0.0009\r\neval_rougeL_high_fmeasure\
    \ = 0.0334\r\neval_rougeL_high_precision = 0.1803\r\neval_rougeL_high_recall =\
    \ 0.0197\r\neval_rougeL_low_fmeasure = 0.0293\r\neval_rougeL_low_precision = 0.1587\r\
    \neval_rougeL_low_recall = 0.0171\r\neval_rougeL_mid_fmeasure = 0.0314\r\neval_rougeL_mid_precision\
    \ = 0.1688\r\neval_rougeL_mid_recall = 0.0184\r\neval_rougeLsum_high_fmeasure\
    \ = 0.0362\r\neval_rougeLsum_high_precision = 0.1951\r\neval_rougeLsum_high_recall\
    \ = 0.0213\r\neval_rougeLsum_low_fmeasure = 0.0317\r\neval_rougeLsum_low_precision\
    \ = 0.1708\r\neval_rougeLsum_low_recall = 0.0184\r\neval_rougeLsum_mid_fmeasure\
    \ = 0.034\r\neval_rougeLsum_mid_precision = 0.1832\r\neval_rougeLsum_mid_recall\
    \ = 0.0199\r\neval_runtime = 0:00:54.57\r\neval_samples = 818\r\neval_samples_per_second\
    \ = 14.988\r\neval_steps_per_second = 14.988\r\nAnd I have not change the run_summarization.py\
    \ script ,\r\nMy shell script is:\r\npython ./run_summarization_.py\r\n\u2013\
    model_name_or_path $T5_DIR\r\n\u2013do_eval\r\n\u2013source_prefix \"summarize:\
    \ \"\r\n\u2013output_dir ./output/huggingface-summarization\r\n\u2013dataset_name\
    \ $samsum_path\r\n\u2013dataset_config \u201C3.0.0\u201D\r\n\u2013per_device_train_batch_size=1\r\
    \n\u2013per_device_eval_batch_size=1\r\n\u2013overwrite_output_dir\r\n\u2013predict_with_generate\
    \ \\\r\n\r\nCan anyone help me on this issue? Thanks a lot."
  created_at: 2023-09-22 07:15:39+00:00
  edited: false
  hidden: false
  id: 650d4d2b3e1cba5f625f08eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: santiviquez/t5-small-finetuned-samsum-en
repo_type: model
status: open
target_branch: null
title: t5 finetune model output inconsistent results
