!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Maxxim69
conflicting_files: null
created_at: 2023-12-24 22:23:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/91fd44fa164f266ba3f800a80204a3ab.svg
      fullname: Maxxim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maxxim69
      type: user
    createdAt: '2023-12-24T22:23:51.000Z'
    data:
      edited: false
      editors:
      - Maxxim69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7519097924232483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/91fd44fa164f266ba3f800a80204a3ab.svg
          fullname: Maxxim
          isHf: false
          isPro: false
          name: Maxxim69
          type: user
        html: '<p>The character sequence &lt;0x0A&gt; is output every time instead
          of a newline.<br><a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/18abvwx/this_model_is_extremely_good/kbxnb6a/">This
          redditor suggests</a> there may be a problem with the tokenizer in the original
          non-GGUF model that carried over.</p>

          '
        raw: "The character sequence <0x0A> is output every time instead of a newline.\r\
          \n<a href=\"https://www.reddit.com/r/LocalLLaMA/comments/18abvwx/this_model_is_extremely_good/kbxnb6a/\"\
          >This redditor suggests</a> there may be a problem with the tokenizer in\
          \ the original non-GGUF model that carried over."
        updatedAt: '2023-12-24T22:23:51.103Z'
      numEdits: 0
      reactions: []
    id: 6588af77a3bfb30fdbb26e54
    type: comment
  author: Maxxim69
  content: "The character sequence <0x0A> is output every time instead of a newline.\r\
    \n<a href=\"https://www.reddit.com/r/LocalLLaMA/comments/18abvwx/this_model_is_extremely_good/kbxnb6a/\"\
    >This redditor suggests</a> there may be a problem with the tokenizer in the original\
    \ non-GGUF model that carried over."
  created_at: 2023-12-24 22:23:51+00:00
  edited: false
  hidden: false
  id: 6588af77a3bfb30fdbb26e54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
      fullname: Rob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kroonen
      type: user
    createdAt: '2023-12-25T23:59:19.000Z'
    data:
      edited: false
      editors:
      - kroonen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968573272228241
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a6a5bde4a2ea6ffd1a7552e6821615af.svg
          fullname: Rob
          isHf: false
          isPro: false
          name: kroonen
          type: user
        html: '<p>Are you using LM Studio? I saw something similar but I bet it has
          to do with the Preset you have chosen.</p>

          '
        raw: Are you using LM Studio? I saw something similar but I bet it has to
          do with the Preset you have chosen.
        updatedAt: '2023-12-25T23:59:19.894Z'
      numEdits: 0
      reactions: []
    id: 658a1757d1331d552b1f9b3a
    type: comment
  author: kroonen
  content: Are you using LM Studio? I saw something similar but I bet it has to do
    with the Preset you have chosen.
  created_at: 2023-12-25 23:59:19+00:00
  edited: false
  hidden: false
  id: 658a1757d1331d552b1f9b3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac8ae1ecaca0f2969655d5b3b8233783.svg
      fullname: No One
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kaneda2004
      type: user
    createdAt: '2023-12-26T16:03:19.000Z'
    data:
      edited: false
      editors:
      - kaneda2004
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.952135443687439
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac8ae1ecaca0f2969655d5b3b8233783.svg
          fullname: No One
          isHf: false
          isPro: false
          name: kaneda2004
          type: user
        html: '<p>It happens using Ollama as well -- the model is outputting the token
          for a newline, but it''s not interpreted as such.<br>Could be solved programatically
          if you''re using this model in a server environment with its outputs passed
          to a program, otherwise - I''m not sure of another solution.</p>

          '
        raw: 'It happens using Ollama as well -- the model is outputting the token
          for a newline, but it''s not interpreted as such.

          Could be solved programatically if you''re using this model in a server
          environment with its outputs passed to a program, otherwise - I''m not sure
          of another solution.'
        updatedAt: '2023-12-26T16:03:19.932Z'
      numEdits: 0
      reactions: []
    id: 658af947137a0f90a2878bd8
    type: comment
  author: kaneda2004
  content: 'It happens using Ollama as well -- the model is outputting the token for
    a newline, but it''s not interpreted as such.

    Could be solved programatically if you''re using this model in a server environment
    with its outputs passed to a program, otherwise - I''m not sure of another solution.'
  created_at: 2023-12-26 16:03:19+00:00
  edited: false
  hidden: false
  id: 658af947137a0f90a2878bd8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab78181bb763debdf149638f74cee54.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kitano-o
      type: user
    createdAt: '2023-12-26T22:11:37.000Z'
    data:
      edited: false
      editors:
      - kitano-o
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9467989206314087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab78181bb763debdf149638f74cee54.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: kitano-o
          type: user
        html: '<p>I have the same bug. I tried on few versions of koboldCpp, in their
          KoboldLite front-end, in SillyTavern  with different chat templates, but
          bug stays. For me this is the best Mixtral model i tried, even better that
          x8 moe models. Its really good in staying in character, speech style etc.  </p>

          '
        raw: 'I have the same bug. I tried on few versions of koboldCpp, in their
          KoboldLite front-end, in SillyTavern  with different chat templates, but
          bug stays. For me this is the best Mixtral model i tried, even better that
          x8 moe models. Its really good in staying in character, speech style etc.  '
        updatedAt: '2023-12-26T22:11:37.238Z'
      numEdits: 0
      reactions: []
    id: 658b4f994dd2ca8497cc0330
    type: comment
  author: kitano-o
  content: 'I have the same bug. I tried on few versions of koboldCpp, in their KoboldLite
    front-end, in SillyTavern  with different chat templates, but bug stays. For me
    this is the best Mixtral model i tried, even better that x8 moe models. Its really
    good in staying in character, speech style etc.  '
  created_at: 2023-12-26 22:11:37+00:00
  edited: false
  hidden: false
  id: 658b4f994dd2ca8497cc0330
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/948d7f022e0b077b6fbe468c5c7b00b1.svg
      fullname: Wukong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wukongai
      type: user
    createdAt: '2023-12-31T12:45:47.000Z'
    data:
      edited: false
      editors:
      - wukongai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9974449872970581
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/948d7f022e0b077b6fbe468c5c7b00b1.svg
          fullname: Wukong
          isHf: false
          isPro: false
          name: wukongai
          type: user
        html: '<p>same as above.</p>

          '
        raw: same as above.
        updatedAt: '2023-12-31T12:45:47.961Z'
      numEdits: 0
      reactions: []
    id: 6591627b754092f6b1b96533
    type: comment
  author: wukongai
  content: same as above.
  created_at: 2023-12-31 12:45:47+00:00
  edited: false
  hidden: false
  id: 6591627b754092f6b1b96533
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5210553d1ac285ca7ed39d1c4e0ec01c.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XiaoRui2012
      type: user
    createdAt: '2023-12-31T14:51:53.000Z'
    data:
      edited: true
      editors:
      - XiaoRui2012
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446057081222534
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5210553d1ac285ca7ed39d1c4e0ec01c.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: XiaoRui2012
          type: user
        html: '<p>I have the same issue.  I think this problem about the original
          model(Mixtral 7Bx2 MoE) missing tokenizer.model file.<br>Here is how i fix:</p>

          <ol>

          <li><code>git clone https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE</code></li>

          <li><code>cd Mixtral_7Bx2_MoE &amp;&amp; curl -L -O https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.model</code></li>

          <li>use llama.cpp reconvert model <code>python convert.py ../Mixtral_7Bx2_MoE</code></li>

          <li><code>./quantize ../Mixtral_7Bx2_MoE/ggml-model-f16.gguf ../Mixtral_7Bx2_MoE/ggml-model-q4_K_M.gguf
          q4_K_M</code></li>

          </ol>

          '
        raw: "I have the same issue.  I think this problem about the original model(Mixtral\
          \ 7Bx2 MoE) missing tokenizer.model file. \nHere is how i fix:\n1. `git\
          \ clone https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE`\n2. `cd Mixtral_7Bx2_MoE\
          \ && curl -L -O https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.model`\n\
          3.  use llama.cpp reconvert model `python convert.py ../Mixtral_7Bx2_MoE`\n\
          4. `./quantize ../Mixtral_7Bx2_MoE/ggml-model-f16.gguf ../Mixtral_7Bx2_MoE/ggml-model-q4_K_M.gguf\
          \ q4_K_M`"
        updatedAt: '2024-01-02T02:31:38.181Z'
      numEdits: 4
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Wylo
        - kitano-o
        - Maxxim69
    id: 65918009e7c71d6d9e3c194e
    type: comment
  author: XiaoRui2012
  content: "I have the same issue.  I think this problem about the original model(Mixtral\
    \ 7Bx2 MoE) missing tokenizer.model file. \nHere is how i fix:\n1. `git clone\
    \ https://huggingface.co/cloudyu/Mixtral_7Bx2_MoE`\n2. `cd Mixtral_7Bx2_MoE &&\
    \ curl -L -O https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/tokenizer.model`\n\
    3.  use llama.cpp reconvert model `python convert.py ../Mixtral_7Bx2_MoE`\n4.\
    \ `./quantize ../Mixtral_7Bx2_MoE/ggml-model-f16.gguf ../Mixtral_7Bx2_MoE/ggml-model-q4_K_M.gguf\
    \ q4_K_M`"
  created_at: 2023-12-31 14:51:53+00:00
  edited: true
  hidden: false
  id: 65918009e7c71d6d9e3c194e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ced169d7b808ec86944ffb742dfabf63.svg
      fullname: hattranthi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hattran
      type: user
    createdAt: '2024-01-02T04:26:21.000Z'
    data:
      edited: false
      editors:
      - hattran
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9873741269111633
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ced169d7b808ec86944ffb742dfabf63.svg
          fullname: hattranthi
          isHf: false
          isPro: false
          name: hattran
          type: user
        html: '<p>I can''t load this model by ctranformers</p>

          '
        raw: I can't load this model by ctranformers
        updatedAt: '2024-01-02T04:26:21.593Z'
      numEdits: 0
      reactions: []
    id: 6593906d50d39af7f47a931f
    type: comment
  author: hattran
  content: I can't load this model by ctranformers
  created_at: 2024-01-02 04:26:21+00:00
  edited: false
  hidden: false
  id: 6593906d50d39af7f47a931f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Mixtral_7Bx2_MoE-GGUF
repo_type: model
status: open
target_branch: null
title: '[Bug Report] <0x0A> is output instead of a newline'
