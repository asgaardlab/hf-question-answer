!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kik0220
conflicting_files: null
created_at: 2023-12-24 22:55:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/62edbe79a3a8f9bdbf985ba71d1ea8d8.svg
      fullname: kik0220
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kik0220
      type: user
    createdAt: '2023-12-24T22:55:15.000Z'
    data:
      edited: false
      editors:
      - kik0220
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3235107958316803
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/62edbe79a3a8f9bdbf985ba71d1ea8d8.svg
          fullname: kik0220
          isHf: false
          isPro: false
          name: kik0220
          type: user
        html: "<p>I executed the following code.</p>\n<pre><code>import torch\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name =\
          \ \"augmxnt_shisa-gamma-7b-v1\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = AutoModelForCausalLM.from_pretrained(\n  model_name,\n  torch_dtype=\"\
          auto\",\n)\nseed = 23  \ntorch.manual_seed(seed)\nmodel.cuda()\ninputs =\
          \ tokenizer(\"AI \u3067\u79D1\u5B66\u7814\u7A76\u3092\u52A0\u901F\u3059\u308B\
          \u306B\u306F\u3001\", return_tensors=\"pt\").to(\"cuda\")\ntokens = model.generate(\n\
          \  **inputs,\n  max_new_tokens=64,\n  temperature=0.75,\n  top_p=0.95,\n\
          \  do_sample=True,\n  pad_token_id=tokenizer.eos_token_id,\n)\nprint(tokenizer.decode(tokens[0],\
          \ skip_special_tokens=True))\n# AI \u3067\u79D1\u5B66\u7814\u7A76\u3092\u52A0\
          \u901F\u3059\u308B\u306B\u306F\u3001\u3053\u308C\u3089\u306E\u8AB2\u984C\
          \u306B\u53D6\u308A\u7D44\u307F\u3001\u6700\u5148\u7AEF\u306E AI \u3092\u79D1\
          \u5B66\u7814\u7A76\u306B\u9069\u7528\u3059\u308B\u3053\u3068\u304C\u5FC5\
          \u8981\u3067\u3059\u3002\n# Google Cloud \u306F\u3001\u6B21\u306E\u3088\u3046\
          \u306A\u79D1\u5B66\u7814\u7A76\u3092\u52A0\u901F\u3059\u308B\u305F\u3081\
          \u306E AI\n</code></pre>\n<p>The results were as follows.</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"D:\\text-generation-webui\\models\\\
          _test_augmxnt_shisa-gamma-7b-v1.py\", line 13, in &lt;module&gt;\n    tokens\
          \ = model.generate(\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return\
          \ func(*args, **kwargs)\n  File \"D:\\text-generation-webui\\venv\\lib\\\
          site-packages\\transformers\\generation\\utils.py\", line 1764, in generate\n\
          \    return self.sample(\n  File \"D:\\text-generation-webui\\venv\\lib\\\
          site-packages\\transformers\\generation\\utils.py\", line 2861, in sample\n\
          \    outputs = self(\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n    return\
          \ self._call_impl(*args, **kwargs)\n  File \"D:\\text-generation-webui\\\
          venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"D:\\text-generation-webui\\\
          venv\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\"\
          , line 1053, in forward\n    outputs = self.model(\n  File \"D:\\text-generation-webui\\\
          venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in\
          \ _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File\
          \ \"D:\\text-generation-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n\
          \  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\transformers\\\
          models\\mistral\\modeling_mistral.py\", line 908, in forward\n    attention_mask\
          \ = _prepare_4d_causal_attention_mask(\n  File \"D:\\text-generation-webui\\\
          venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py\", line\
          \ 306, in _prepare_4d_causal_attention_mask\n    attention_mask = attn_mask_converter.to_4d(\n\
          \  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\transformers\\\
          modeling_attn_mask_utils.py\", line 121, in to_4d\n    causal_4d_mask =\
          \ self._make_causal_mask(\n  File \"D:\\text-generation-webui\\venv\\lib\\\
          site-packages\\transformers\\modeling_attn_mask_utils.py\", line 156, in\
          \ _make_causal_mask\n    mask_cond = torch.arange(mask.size(-1), device=device)\n\
          RuntimeError: CUDA error: device-side assert triggered\nCUDA kernel errors\
          \ might be asynchronously reported at some other API call, so the stacktrace\
          \ below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n\
          Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n</code></pre>\n\
          <p>However, it seemed that text was generated when I copied and executed\
          \ the tokenizer from stabilityai/japanese-stablelm-base-gamma-7b.</p>\n"
        raw: "I executed the following code.\r\n```\r\nimport torch\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\r\nmodel_name = \"augmxnt_shisa-gamma-7b-v1\"\
          \r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
          \n  model_name,\r\n  torch_dtype=\"auto\",\r\n)\r\nseed = 23  \r\ntorch.manual_seed(seed)\r\
          \nmodel.cuda()\r\ninputs = tokenizer(\"AI \u3067\u79D1\u5B66\u7814\u7A76\
          \u3092\u52A0\u901F\u3059\u308B\u306B\u306F\u3001\", return_tensors=\"pt\"\
          ).to(\"cuda\")\r\ntokens = model.generate(\r\n  **inputs,\r\n  max_new_tokens=64,\r\
          \n  temperature=0.75,\r\n  top_p=0.95,\r\n  do_sample=True,\r\n  pad_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\r\n\
          # AI \u3067\u79D1\u5B66\u7814\u7A76\u3092\u52A0\u901F\u3059\u308B\u306B\u306F\
          \u3001\u3053\u308C\u3089\u306E\u8AB2\u984C\u306B\u53D6\u308A\u7D44\u307F\
          \u3001\u6700\u5148\u7AEF\u306E AI \u3092\u79D1\u5B66\u7814\u7A76\u306B\u9069\
          \u7528\u3059\u308B\u3053\u3068\u304C\u5FC5\u8981\u3067\u3059\u3002\r\n#\
          \ Google Cloud \u306F\u3001\u6B21\u306E\u3088\u3046\u306A\u79D1\u5B66\u7814\
          \u7A76\u3092\u52A0\u901F\u3059\u308B\u305F\u3081\u306E AI\r\n```\r\n\r\n\
          The results were as follows.\r\n```\r\nTraceback (most recent call last):\r\
          \n  File \"D:\\text-generation-webui\\models\\_test_augmxnt_shisa-gamma-7b-v1.py\"\
          , line 13, in <module>\r\n    tokens = model.generate(\r\n  File \"D:\\\
          text-generation-webui\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1764, in generate\r\n    return self.sample(\r\
          \n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 2861, in sample\r\n    outputs = self(\r\n\
          \  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"D:\\text-generation-webui\\\
          venv\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\"\
          , line 1053, in forward\r\n    outputs = self.model(\r\n  File \"D:\\text-generation-webui\\\
          venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in\
          \ _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n\
          \  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
          transformers\\models\\mistral\\modeling_mistral.py\", line 908, in forward\r\
          \n    attention_mask = _prepare_4d_causal_attention_mask(\r\n  File \"D:\\\
          text-generation-webui\\venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py\"\
          , line 306, in _prepare_4d_causal_attention_mask\r\n    attention_mask =\
          \ attn_mask_converter.to_4d(\r\n  File \"D:\\text-generation-webui\\venv\\\
          lib\\site-packages\\transformers\\modeling_attn_mask_utils.py\", line 121,\
          \ in to_4d\r\n    causal_4d_mask = self._make_causal_mask(\r\n  File \"\
          D:\\text-generation-webui\\venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py\"\
          , line 156, in _make_causal_mask\r\n    mask_cond = torch.arange(mask.size(-1),\
          \ device=device)\r\nRuntimeError: CUDA error: device-side assert triggered\r\
          \nCUDA kernel errors might be asynchronously reported at some other API\
          \ call, so the stacktrace below might be incorrect.\r\nFor debugging consider\
          \ passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to\
          \ enable device-side assertions.\r\n```\r\n\r\nHowever, it seemed that text\
          \ was generated when I copied and executed the tokenizer from stabilityai/japanese-stablelm-base-gamma-7b."
        updatedAt: '2023-12-24T22:55:15.329Z'
      numEdits: 0
      reactions: []
    id: 6588b6d3509bcae23f880210
    type: comment
  author: kik0220
  content: "I executed the following code.\r\n```\r\nimport torch\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\nmodel_name = \"augmxnt_shisa-gamma-7b-v1\"\
    \r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\
    \n  model_name,\r\n  torch_dtype=\"auto\",\r\n)\r\nseed = 23  \r\ntorch.manual_seed(seed)\r\
    \nmodel.cuda()\r\ninputs = tokenizer(\"AI \u3067\u79D1\u5B66\u7814\u7A76\u3092\
    \u52A0\u901F\u3059\u308B\u306B\u306F\u3001\", return_tensors=\"pt\").to(\"cuda\"\
    )\r\ntokens = model.generate(\r\n  **inputs,\r\n  max_new_tokens=64,\r\n  temperature=0.75,\r\
    \n  top_p=0.95,\r\n  do_sample=True,\r\n  pad_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\r\n# AI \u3067\
    \u79D1\u5B66\u7814\u7A76\u3092\u52A0\u901F\u3059\u308B\u306B\u306F\u3001\u3053\
    \u308C\u3089\u306E\u8AB2\u984C\u306B\u53D6\u308A\u7D44\u307F\u3001\u6700\u5148\
    \u7AEF\u306E AI \u3092\u79D1\u5B66\u7814\u7A76\u306B\u9069\u7528\u3059\u308B\u3053\
    \u3068\u304C\u5FC5\u8981\u3067\u3059\u3002\r\n# Google Cloud \u306F\u3001\u6B21\
    \u306E\u3088\u3046\u306A\u79D1\u5B66\u7814\u7A76\u3092\u52A0\u901F\u3059\u308B\
    \u305F\u3081\u306E AI\r\n```\r\n\r\nThe results were as follows.\r\n```\r\nTraceback\
    \ (most recent call last):\r\n  File \"D:\\text-generation-webui\\models\\_test_augmxnt_shisa-gamma-7b-v1.py\"\
    , line 13, in <module>\r\n    tokens = model.generate(\r\n  File \"D:\\text-generation-webui\\\
    venv\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\
    \n    return func(*args, **kwargs)\r\n  File \"D:\\text-generation-webui\\venv\\\
    lib\\site-packages\\transformers\\generation\\utils.py\", line 1764, in generate\r\
    \n    return self.sample(\r\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 2861, in sample\r\n    outputs = self(\r\
    \n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
    \ **kwargs)\r\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\\
    transformers\\models\\mistral\\modeling_mistral.py\", line 1053, in forward\r\n\
    \    outputs = self.model(\r\n  File \"D:\\text-generation-webui\\venv\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\r\
    \n    return self._call_impl(*args, **kwargs)\r\n  File \"D:\\text-generation-webui\\\
    venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"D:\\text-generation-webui\\\
    venv\\lib\\site-packages\\transformers\\models\\mistral\\modeling_mistral.py\"\
    , line 908, in forward\r\n    attention_mask = _prepare_4d_causal_attention_mask(\r\
    \n  File \"D:\\text-generation-webui\\venv\\lib\\site-packages\\transformers\\\
    modeling_attn_mask_utils.py\", line 306, in _prepare_4d_causal_attention_mask\r\
    \n    attention_mask = attn_mask_converter.to_4d(\r\n  File \"D:\\text-generation-webui\\\
    venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py\", line 121,\
    \ in to_4d\r\n    causal_4d_mask = self._make_causal_mask(\r\n  File \"D:\\text-generation-webui\\\
    venv\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py\", line 156,\
    \ in _make_causal_mask\r\n    mask_cond = torch.arange(mask.size(-1), device=device)\r\
    \nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors\
    \ might be asynchronously reported at some other API call, so the stacktrace below\
    \ might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\
    \nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\
    \n\r\nHowever, it seemed that text was generated when I copied and executed the\
    \ tokenizer from stabilityai/japanese-stablelm-base-gamma-7b."
  created_at: 2023-12-24 22:55:15+00:00
  edited: false
  hidden: false
  id: 6588b6d3509bcae23f880210
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b954d0d7a481bd12d75516282c188d5.svg
      fullname: Yuki Tomita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuki-tomita-127
      type: user
    createdAt: '2023-12-27T09:22:51.000Z'
    data:
      edited: false
      editors:
      - yuki-tomita-127
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.981065571308136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b954d0d7a481bd12d75516282c188d5.svg
          fullname: Yuki Tomita
          isHf: false
          isPro: false
          name: yuki-tomita-127
          type: user
        html: "<p>I encountered the same error as <span data-props=\"{&quot;user&quot;:&quot;kik0220&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kik0220\"\
          >@<span class=\"underline\">kik0220</span></a></span>\n\n\t</span></span>\
          \ and was also able to run the model using the tokenizer in <code>stabilityai/japanese-stablelm-base-gamma-7b</code>.<br>I\
          \ would like to know if this is intentional or not, as the performance of\
          \ the model certainly seems good.</p>\n"
        raw: 'I encountered the same error as @kik0220 and was also able to run the
          model using the tokenizer in `stabilityai/japanese-stablelm-base-gamma-7b`.

          I would like to know if this is intentional or not, as the performance of
          the model certainly seems good.'
        updatedAt: '2023-12-27T09:22:51.185Z'
      numEdits: 0
      reactions: []
    id: 658becebbff14f41f4ac7342
    type: comment
  author: yuki-tomita-127
  content: 'I encountered the same error as @kik0220 and was also able to run the
    model using the tokenizer in `stabilityai/japanese-stablelm-base-gamma-7b`.

    I would like to know if this is intentional or not, as the performance of the
    model certainly seems good.'
  created_at: 2023-12-27 09:22:51+00:00
  edited: false
  hidden: false
  id: 658becebbff14f41f4ac7342
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2023-12-27T15:51:49.000Z'
    data:
      edited: false
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.955471396446228
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: '<p>Hey guys, wrong tokenizer might have gotten copied in (this was
          more of an test model w/ good results I just threw up). Traveling atm but
          when I get a chance I''ll take a look and update if necessary.</p>

          '
        raw: Hey guys, wrong tokenizer might have gotten copied in (this was more
          of an test model w/ good results I just threw up). Traveling atm but when
          I get a chance I'll take a look and update if necessary.
        updatedAt: '2023-12-27T15:51:49.329Z'
      numEdits: 0
      reactions: []
    id: 658c48159440d69c8b54961a
    type: comment
  author: leonardlin
  content: Hey guys, wrong tokenizer might have gotten copied in (this was more of
    an test model w/ good results I just threw up). Traveling atm but when I get a
    chance I'll take a look and update if necessary.
  created_at: 2023-12-27 15:51:49+00:00
  edited: false
  hidden: false
  id: 658c48159440d69c8b54961a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2024-01-02T16:00:58.000Z'
    data:
      edited: false
      editors:
      - leonardlin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9363217353820801
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
          fullname: lhl
          isHf: false
          isPro: false
          name: leonardlin
          type: user
        html: "<p>Sorry for the delay w/ holiday travels (and not having access to\
          \ my dev box!). I've uploaded the tokenizer fix to the repo (but using the\
          \ original Gamma tokenizer was of course the solution). Thanks <span data-props=\"\
          {&quot;user&quot;:&quot;kik0220&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/kik0220\">@<span class=\"underline\">kik0220</span></a></span>\n\
          \n\t</span></span> for the report, and happy new year everyone!</p>\n"
        raw: Sorry for the delay w/ holiday travels (and not having access to my dev
          box!). I've uploaded the tokenizer fix to the repo (but using the original
          Gamma tokenizer was of course the solution). Thanks @kik0220 for the report,
          and happy new year everyone!
        updatedAt: '2024-01-02T16:00:58.024Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yuki-tomita-127
      relatedEventId: 6594333a57a556fbe14347e2
    id: 6594333a57a556fbe14347e0
    type: comment
  author: leonardlin
  content: Sorry for the delay w/ holiday travels (and not having access to my dev
    box!). I've uploaded the tokenizer fix to the repo (but using the original Gamma
    tokenizer was of course the solution). Thanks @kik0220 for the report, and happy
    new year everyone!
  created_at: 2024-01-02 16:00:58+00:00
  edited: false
  hidden: false
  id: 6594333a57a556fbe14347e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a7422854f1d0225b075bfc/XGYAcDPZG5ZEsNBWG6guw.jpeg?w=200&h=200&f=face
      fullname: lhl
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leonardlin
      type: user
    createdAt: '2024-01-02T16:00:58.000Z'
    data:
      status: closed
    id: 6594333a57a556fbe14347e2
    type: status-change
  author: leonardlin
  created_at: 2024-01-02 16:00:58+00:00
  id: 6594333a57a556fbe14347e2
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: augmxnt/shisa-gamma-7b-v1
repo_type: model
status: closed
target_branch: null
title: Does this tokenizer work?
