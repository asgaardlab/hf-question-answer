!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Henk717
conflicting_files: null
created_at: 2023-12-12 17:55:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-12-12T17:55:34.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9683842658996582
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>Possibly concept to further improve this, have each expert contain
          a mild merge of the other expects to ensure overlap.<br>This would make
          it a bit closer to what MDEL / Aurora is doing, since they plan to have
          one base model that is a traditional merge with MoE''s on top for enhancement.
          Since I believe this architecture is a little different we might be able
          to create a similar effect by merging each expert at a low percentage and
          then using the resulting models for the MoE model.</p>

          '
        raw: "Possibly concept to further improve this, have each expert contain a\
          \ mild merge of the other expects to ensure overlap.\r\nThis would make\
          \ it a bit closer to what MDEL / Aurora is doing, since they plan to have\
          \ one base model that is a traditional merge with MoE's on top for enhancement.\
          \ Since I believe this architecture is a little different we might be able\
          \ to create a similar effect by merging each expert at a low percentage\
          \ and then using the resulting models for the MoE model."
        updatedAt: '2023-12-12T17:55:34.678Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - theycallmeloki
        - chargoddard
    id: 65789e966756d0b0167bf8b9
    type: comment
  author: Henk717
  content: "Possibly concept to further improve this, have each expert contain a mild\
    \ merge of the other expects to ensure overlap.\r\nThis would make it a bit closer\
    \ to what MDEL / Aurora is doing, since they plan to have one base model that\
    \ is a traditional merge with MoE's on top for enhancement. Since I believe this\
    \ architecture is a little different we might be able to create a similar effect\
    \ by merging each expert at a low percentage and then using the resulting models\
    \ for the MoE model."
  created_at: 2023-12-12 17:55:34+00:00
  edited: false
  hidden: false
  id: 65789e966756d0b0167bf8b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
      fullname: Charles Goddard
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: chargoddard
      type: user
    createdAt: '2023-12-15T09:59:34.000Z'
    data:
      edited: false
      editors:
      - chargoddard
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9955843687057495
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630495b0ce6b12280b193c25/bT61kBtQPhDYk00AI0o0g.png?w=200&h=200&f=face
          fullname: Charles Goddard
          isHf: false
          isPro: false
          name: chargoddard
          type: user
        html: '<p>That''s an interesting thought - I''ll definitely have to try it.
          Thanks!</p>

          '
        raw: That's an interesting thought - I'll definitely have to try it. Thanks!
        updatedAt: '2023-12-15T09:59:34.431Z'
      numEdits: 0
      reactions: []
    id: 657c2386837e0145ea4ba452
    type: comment
  author: chargoddard
  content: That's an interesting thought - I'll definitely have to try it. Thanks!
  created_at: 2023-12-15 09:59:34+00:00
  edited: false
  hidden: false
  id: 657c2386837e0145ea4ba452
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: chargoddard/mixtralnt-4x7b-test
repo_type: model
status: open
target_branch: null
title: Idea for the future - combine this with traditional merging
