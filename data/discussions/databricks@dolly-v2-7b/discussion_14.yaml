!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gkrishnan
conflicting_files: null
created_at: 2023-08-06 19:35:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a6b7244132df8df07df0ec954e03b8f.svg
      fullname: Ganesh Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gkrishnan
      type: user
    createdAt: '2023-08-06T20:35:56.000Z'
    data:
      edited: false
      editors:
      - gkrishnan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5513067841529846
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a6b7244132df8df07df0ec954e03b8f.svg
          fullname: Ganesh Krishnan
          isHf: false
          isPro: false
          name: gkrishnan
          type: user
        html: '<p>So here is my other question. If you were to work from OpenAI to
          LangChain. We take a given document and put it in a vector database and
          then you persist your Chroma vector store like so:</p>

          <p>from langchain.embeddings.openai import OpenAIEmbeddings<br>embeddings
          = OpenAIEmbeddings()</p>

          <p>from langchain.vectorstores import Chroma<br>persist_directory = "vector_db"<br>vectordb
          = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=persist_directory)</p>

          <p>vectordb.persist()<br>vectordb = None<br>vectordb = Chroma(persist_directory=persist_directory,
          embedding_function=embeddings)</p>

          <p>from langchain.chat_models import ChatOpenAI<br>llm = ChatOpenAI(temperature=0)<br>doc_retriever
          = vectordb.as_retriever()</p>

          <p>from langchain.chains import RetrievalQA<br>resume_qa = RetrievalQA.from_chain_type(llm=llm,
          chain_type="stuff", retriever=doc_retriever)</p>

          <p>As you can see here the "vectordb" retriever is stored in "doc_retriever".
          This is very simple. At this point you can run any query based off the documentation
          you uploaded. Now I am trying to do this with Dolly.</p>

          <p>Now here is my code:<br>loader = PyPDFLoader("/content/drive/MyDrive/Data
          Science/Capstone/Resume_guide.pdf")<br>pages = loader.load_and_split()</p>

          <p>import torch<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("databricks/dolly-v2-7b")</p>

          <p>from langchain.text_splitter import RecursiveCharacterTextSplitter</p>

          <p>text_splitter = RecursiveCharacterTextSplitter(<br>chunk_size = 1000,<br>chunk_overlap
          = 20,<br>length_function = len,<br>)</p>

          <p>documents = text_splitter.split_documents(pages)<br>from langchain.embeddings
          import HuggingFaceEmbeddings<br>embeddings = HuggingFaceEmbeddings()</p>

          <p>from langchain.vectorstores import Chroma</p>

          <p>persist_directory = "vector_db"<br>vectordb = Chroma.from_documents(documents=documents,
          embedding=embeddings, persist_directory=persist_directory)</p>

          <p>vectordb.persist()<br>vectordb = None</p>

          <p>vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)</p>

          <p>doc_retriever = vectordb.as_retriever()</p>

          <p>#Dolly Stuff<br>import torch<br>import os<br>os.environ["CUDA_VISIBLE_DEVICES"]="0"<br>from
          transformers import pipeline</p>

          <p>generate_text = pipeline(model="databricks/dolly-v2-7b", torch_dtype=torch.bfloat16,<br>trust_remote_code=True,
          device_map="auto", return_full_text=True)</p>

          <p>#Dolly Stuff<br>from langchain import PromptTemplate, LLMChain<br>from
          langchain.llms import HuggingFacePipeline</p>

          <p>#template for an instruction with no input<br>prompt = PromptTemplate(<br>input_variables=["instruction"],<br>template="{instruction}")</p>

          <p>#template for an instruction with input<br>prompt_with_context = PromptTemplate(<br>input_variables=["instruction",
          "context"],<br>template="{instruction}\n\nInput:\n{context}")</p>

          <p>hf_pipeline = HuggingFacePipeline(pipeline=generate_text)</p>

          <p>llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)<br>llm_context_chain
          = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)</p>

          <p>What I don''t understand is how will "doc_retriever" be used in the llm_context_chain
          or llm_chain variables?</p>

          <p>I noticed no where in my code, I am not calling "doc_retriever". How
          would I do that? Is there any documentation for uploading your own documentation
          in Dolly while using HuggingFace? Any help would be greatly appreciated!</p>

          '
        raw: "So here is my other question. If you were to work from OpenAI to LangChain.\
          \ We take a given document and put it in a vector database and then you\
          \ persist your Chroma vector store like so:\r\n\r\nfrom langchain.embeddings.openai\
          \ import OpenAIEmbeddings\r\nembeddings = OpenAIEmbeddings()\r\n\r\nfrom\
          \ langchain.vectorstores import Chroma\r\npersist_directory = \"vector_db\"\
          \r\nvectordb = Chroma.from_documents(documents=documents, embedding=embeddings,\
          \ persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\nvectordb\
          \ = None\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
          \n\r\nfrom langchain.chat_models import ChatOpenAI\r\nllm = ChatOpenAI(temperature=0)\r\
          \ndoc_retriever = vectordb.as_retriever()\r\n\r\nfrom langchain.chains import\
          \ RetrievalQA\r\nresume_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"\
          stuff\", retriever=doc_retriever)\r\n\r\nAs you can see here the \"vectordb\"\
          \ retriever is stored in \"doc_retriever\". This is very simple. At this\
          \ point you can run any query based off the documentation you uploaded.\
          \ Now I am trying to do this with Dolly.\r\n\r\nNow here is my code:\r\n\
          loader = PyPDFLoader(\"/content/drive/MyDrive/Data Science/Capstone/Resume_guide.pdf\"\
          )\r\npages = loader.load_and_split()\r\n\r\nimport torch\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          databricks/dolly-v2-7b\")\r\n\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\
          \n\r\ntext_splitter = RecursiveCharacterTextSplitter(\r\nchunk_size = 1000,\r\
          \nchunk_overlap = 20,\r\nlength_function = len,\r\n)\r\n\r\ndocuments =\
          \ text_splitter.split_documents(pages)\r\nfrom langchain.embeddings import\
          \ HuggingFaceEmbeddings\r\nembeddings = HuggingFaceEmbeddings()\r\n\r\n\
          from langchain.vectorstores import Chroma\r\n\r\npersist_directory = \"\
          vector_db\"\r\nvectordb = Chroma.from_documents(documents=documents, embedding=embeddings,\
          \ persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\nvectordb\
          \ = None\r\n\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
          \n\r\ndoc_retriever = vectordb.as_retriever()\r\n\r\n#Dolly Stuff\r\nimport\
          \ torch\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\nfrom\
          \ transformers import pipeline\r\n\r\ngenerate_text = pipeline(model=\"\
          databricks/dolly-v2-7b\", torch_dtype=torch.bfloat16,\r\ntrust_remote_code=True,\
          \ device_map=\"auto\", return_full_text=True)\r\n\r\n#Dolly Stuff\r\nfrom\
          \ langchain import PromptTemplate, LLMChain\r\nfrom langchain.llms import\
          \ HuggingFacePipeline\r\n\r\n#template for an instruction with no input\r\
          \nprompt = PromptTemplate(\r\ninput_variables=[\"instruction\"],\r\ntemplate=\"\
          {instruction}\")\r\n\r\n#template for an instruction with input\r\nprompt_with_context\
          \ = PromptTemplate(\r\ninput_variables=[\"instruction\", \"context\"],\r\
          \ntemplate=\"{instruction}\\n\\nInput:\\n{context}\")\r\n\r\nhf_pipeline\
          \ = HuggingFacePipeline(pipeline=generate_text)\r\n\r\nllm_chain = LLMChain(llm=hf_pipeline,\
          \ prompt=prompt)\r\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\r\
          \n\r\nWhat I don't understand is how will \"doc_retriever\" be used in the\
          \ llm_context_chain or llm_chain variables?\r\n\r\nI noticed no where in\
          \ my code, I am not calling \"doc_retriever\". How would I do that? Is there\
          \ any documentation for uploading your own documentation in Dolly while\
          \ using HuggingFace? Any help would be greatly appreciated!"
        updatedAt: '2023-08-06T20:35:56.531Z'
      numEdits: 0
      reactions: []
    id: 64d0042c7e20ec9ea0c60d2c
    type: comment
  author: gkrishnan
  content: "So here is my other question. If you were to work from OpenAI to LangChain.\
    \ We take a given document and put it in a vector database and then you persist\
    \ your Chroma vector store like so:\r\n\r\nfrom langchain.embeddings.openai import\
    \ OpenAIEmbeddings\r\nembeddings = OpenAIEmbeddings()\r\n\r\nfrom langchain.vectorstores\
    \ import Chroma\r\npersist_directory = \"vector_db\"\r\nvectordb = Chroma.from_documents(documents=documents,\
    \ embedding=embeddings, persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\
    \nvectordb = None\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
    \n\r\nfrom langchain.chat_models import ChatOpenAI\r\nllm = ChatOpenAI(temperature=0)\r\
    \ndoc_retriever = vectordb.as_retriever()\r\n\r\nfrom langchain.chains import\
    \ RetrievalQA\r\nresume_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"\
    stuff\", retriever=doc_retriever)\r\n\r\nAs you can see here the \"vectordb\"\
    \ retriever is stored in \"doc_retriever\". This is very simple. At this point\
    \ you can run any query based off the documentation you uploaded. Now I am trying\
    \ to do this with Dolly.\r\n\r\nNow here is my code:\r\nloader = PyPDFLoader(\"\
    /content/drive/MyDrive/Data Science/Capstone/Resume_guide.pdf\")\r\npages = loader.load_and_split()\r\
    \n\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"databricks/dolly-v2-7b\")\r\n\r\n\
    from langchain.text_splitter import RecursiveCharacterTextSplitter\r\n\r\ntext_splitter\
    \ = RecursiveCharacterTextSplitter(\r\nchunk_size = 1000,\r\nchunk_overlap = 20,\r\
    \nlength_function = len,\r\n)\r\n\r\ndocuments = text_splitter.split_documents(pages)\r\
    \nfrom langchain.embeddings import HuggingFaceEmbeddings\r\nembeddings = HuggingFaceEmbeddings()\r\
    \n\r\nfrom langchain.vectorstores import Chroma\r\n\r\npersist_directory = \"\
    vector_db\"\r\nvectordb = Chroma.from_documents(documents=documents, embedding=embeddings,\
    \ persist_directory=persist_directory)\r\n\r\nvectordb.persist()\r\nvectordb =\
    \ None\r\n\r\nvectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\
    \n\r\ndoc_retriever = vectordb.as_retriever()\r\n\r\n#Dolly Stuff\r\nimport torch\r\
    \nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\nfrom transformers\
    \ import pipeline\r\n\r\ngenerate_text = pipeline(model=\"databricks/dolly-v2-7b\"\
    , torch_dtype=torch.bfloat16,\r\ntrust_remote_code=True, device_map=\"auto\",\
    \ return_full_text=True)\r\n\r\n#Dolly Stuff\r\nfrom langchain import PromptTemplate,\
    \ LLMChain\r\nfrom langchain.llms import HuggingFacePipeline\r\n\r\n#template\
    \ for an instruction with no input\r\nprompt = PromptTemplate(\r\ninput_variables=[\"\
    instruction\"],\r\ntemplate=\"{instruction}\")\r\n\r\n#template for an instruction\
    \ with input\r\nprompt_with_context = PromptTemplate(\r\ninput_variables=[\"instruction\"\
    , \"context\"],\r\ntemplate=\"{instruction}\\n\\nInput:\\n{context}\")\r\n\r\n\
    hf_pipeline = HuggingFacePipeline(pipeline=generate_text)\r\n\r\nllm_chain = LLMChain(llm=hf_pipeline,\
    \ prompt=prompt)\r\nllm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)\r\
    \n\r\nWhat I don't understand is how will \"doc_retriever\" be used in the llm_context_chain\
    \ or llm_chain variables?\r\n\r\nI noticed no where in my code, I am not calling\
    \ \"doc_retriever\". How would I do that? Is there any documentation for uploading\
    \ your own documentation in Dolly while using HuggingFace? Any help would be greatly\
    \ appreciated!"
  created_at: 2023-08-06 19:35:56+00:00
  edited: false
  hidden: false
  id: 64d0042c7e20ec9ea0c60d2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-08-06T20:43:08.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8677183389663696
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>This looks like it was taken from the Databricks demo at <a rel="nofollow"
          href="https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot">https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot</a><br>But
          that same example (notebook 03) shows calling the vector DB and passing
          the result to langchain</p>

          '
        raw: 'This looks like it was taken from the Databricks demo at https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot

          But that same example (notebook 03) shows calling the vector DB and passing
          the result to langchain'
        updatedAt: '2023-08-06T20:43:08.248Z'
      numEdits: 0
      reactions: []
    id: 64d005dc97ca59bcf71e8a93
    type: comment
  author: srowen
  content: 'This looks like it was taken from the Databricks demo at https://www.dbdemos.ai/demo-notebooks.html?demoName=llm-dolly-chatbot

    But that same example (notebook 03) shows calling the vector DB and passing the
    result to langchain'
  created_at: 2023-08-06 19:43:08+00:00
  edited: false
  hidden: false
  id: 64d005dc97ca59bcf71e8a93
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: databricks/dolly-v2-7b
repo_type: model
status: open
target_branch: null
title: How do I pass a Vector database into a Dolly LLM?
