!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Iamexperimenting
conflicting_files: null
created_at: 2023-08-08 09:58:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
      fullname: IamexperimentingNow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Iamexperimenting
      type: user
    createdAt: '2023-08-08T10:58:46.000Z'
    data:
      edited: false
      editors:
      - Iamexperimenting
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7531694173812866
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c8ef28358d8ff81e6bc1dc4558de43d.svg
          fullname: IamexperimentingNow
          isHf: false
          isPro: false
          name: Iamexperimenting
          type: user
        html: "<p>Hi team,</p>\n<p>I followed this example <a rel=\"nofollow\" href=\"\
          https://notebooks.databricks.com/demos/llm-dolly-chatbot/index.html#\">https://notebooks.databricks.com/demos/llm-dolly-chatbot/index.html#</a>\
          \ to build the same . I used the same template, my pdfs documents are related\
          \ to commercial domain( like specification details about electronics products).\
          \ </p>\n<p>I did a test to verify, if I ask out of box questions to DollyV2\
          \ model whether it returns 'I don't know' or it provides answer on it own.</p>\n\
          <p>Example question:</p>\n<ol>\n<li>how many players will be playing in\
          \ a basket ball match?</li>\n</ol>\n<p>I expect model to respond 'I don't\
          \ know' since these details aren't present in my knowledge base(in my pdfs),\
          \ but unfortunately model returns answer on its own. Could you please tell\
          \ me how to stop this?</p>\n<p>please find the code below</p>\n<pre><code>from\
          \ transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nimport\
          \ torch\nfrom langchain import PromptTemplate\nfrom langchain.llms import\
          \ HuggingFacePipeline\nfrom langchain.chains.question_answering import load_qa_chain\n\
          \ \ndef build_qa_chain():\n  torch.cuda.empty_cache()\n  model_name = \"\
          databricks/dolly-v2-7b\" # can use dolly-v2-3b or dolly-v2-7b for smaller\
          \ model and faster inferences.\n \n  # Increase max_new_tokens for a longer\
          \ response\n  # Other settings might give better results! Play around\n\
          \  instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\", \n                      \
          \         return_full_text=True, max_new_tokens=256, top_p=0.25, top_k=5)\n\
          \  # Note: if you use dolly 12B or smaller model but a GPU with less than\
          \ 24GB RAM, use 8bit. This requires %pip install bitsandbytes\n  # instruct_pipeline\
          \ = pipeline(model=model_name, trust_remote_code=True, device_map=\"auto\"\
          , model_kwargs={'load_in_8bit': True})\n  # For GPUs without bfloat16 support,\
          \ like the T4 or V100, use torch_dtype=torch.float16 below\n  # model =\
          \ AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",\
          \ torch_dtype=torch.float16, trust_remote_code=True)\n \n  # Defining our\
          \ prompt content.\n  # langchain will load our similar documents as {context}\n\
          \  template = \"\"\"Below is an instruction that describes a task. Write\
          \ a response that appropriately completes the request.\n \n  Instruction:\
          \ \n  You are a question answering agent and your job is to help providing\
          \ the best answer. \n  Use only information in the following paragraphs\
          \ to answer the question at the end. Explain the answer with reference to\
          \ these paragraphs. If you don't know, say that you do not know.\n \n  {context}\n\
          \ \n  Question: {question}\n \n  Response:\n  \"\"\"\n  prompt = PromptTemplate(input_variables=['context',\
          \ 'question'], template=template)\n \n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n\
          \  # Set verbose=True to see the full prompt:\n  return load_qa_chain(llm=hf_pipe,\
          \ chain_type=\"stuff\", prompt=prompt, verbose=True)\n</code></pre>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;srowen&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/srowen\">@<span class=\"underline\"\
          >srowen</span></a></span>\n\n\t</span></span> can you please help me here?</p>\n"
        raw: "Hi team,\r\n\r\nI followed this example https://notebooks.databricks.com/demos/llm-dolly-chatbot/index.html#\
          \ to build the same . I used the same template, my pdfs documents are related\
          \ to commercial domain( like specification details about electronics products).\
          \ \r\n\r\nI did a test to verify, if I ask out of box questions to DollyV2\
          \ model whether it returns 'I don't know' or it provides answer on it own.\r\
          \n\r\nExample question:\r\n1. how many players will be playing in a basket\
          \ ball match?\r\n\r\nI expect model to respond 'I don't know' since these\
          \ details aren't present in my knowledge base(in my pdfs), but unfortunately\
          \ model returns answer on its own. Could you please tell me how to stop\
          \ this?\r\n\r\nplease find the code below\r\n\r\n```\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM, pipeline\r\nimport torch\r\
          \nfrom langchain import PromptTemplate\r\nfrom langchain.llms import HuggingFacePipeline\r\
          \nfrom langchain.chains.question_answering import load_qa_chain\r\n \r\n\
          def build_qa_chain():\r\n  torch.cuda.empty_cache()\r\n  model_name = \"\
          databricks/dolly-v2-7b\" # can use dolly-v2-3b or dolly-v2-7b for smaller\
          \ model and faster inferences.\r\n \r\n  # Increase max_new_tokens for a\
          \ longer response\r\n  # Other settings might give better results! Play\
          \ around\r\n  instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16,\
          \ trust_remote_code=True, device_map=\"auto\", \r\n                    \
          \           return_full_text=True, max_new_tokens=256, top_p=0.25, top_k=5)\r\
          \n  # Note: if you use dolly 12B or smaller model but a GPU with less than\
          \ 24GB RAM, use 8bit. This requires %pip install bitsandbytes\r\n  # instruct_pipeline\
          \ = pipeline(model=model_name, trust_remote_code=True, device_map=\"auto\"\
          , model_kwargs={'load_in_8bit': True})\r\n  # For GPUs without bfloat16\
          \ support, like the T4 or V100, use torch_dtype=torch.float16 below\r\n\
          \  # model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
          auto\", torch_dtype=torch.float16, trust_remote_code=True)\r\n \r\n  # Defining\
          \ our prompt content.\r\n  # langchain will load our similar documents as\
          \ {context}\r\n  template = \"\"\"Below is an instruction that describes\
          \ a task. Write a response that appropriately completes the request.\r\n\
          \ \r\n  Instruction: \r\n  You are a question answering agent and your job\
          \ is to help providing the best answer. \r\n  Use only information in the\
          \ following paragraphs to answer the question at the end. Explain the answer\
          \ with reference to these paragraphs. If you don't know, say that you do\
          \ not know.\r\n \r\n  {context}\r\n \r\n  Question: {question}\r\n \r\n\
          \  Response:\r\n  \"\"\"\r\n  prompt = PromptTemplate(input_variables=['context',\
          \ 'question'], template=template)\r\n \r\n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\r\
          \n  # Set verbose=True to see the full prompt:\r\n  return load_qa_chain(llm=hf_pipe,\
          \ chain_type=\"stuff\", prompt=prompt, verbose=True)\r\n```\r\n\r\n@srowen\
          \ can you please help me here?"
        updatedAt: '2023-08-08T10:58:46.868Z'
      numEdits: 0
      reactions: []
    id: 64d21fe6c2bd235422fffb88
    type: comment
  author: Iamexperimenting
  content: "Hi team,\r\n\r\nI followed this example https://notebooks.databricks.com/demos/llm-dolly-chatbot/index.html#\
    \ to build the same . I used the same template, my pdfs documents are related\
    \ to commercial domain( like specification details about electronics products).\
    \ \r\n\r\nI did a test to verify, if I ask out of box questions to DollyV2 model\
    \ whether it returns 'I don't know' or it provides answer on it own.\r\n\r\nExample\
    \ question:\r\n1. how many players will be playing in a basket ball match?\r\n\
    \r\nI expect model to respond 'I don't know' since these details aren't present\
    \ in my knowledge base(in my pdfs), but unfortunately model returns answer on\
    \ its own. Could you please tell me how to stop this?\r\n\r\nplease find the code\
    \ below\r\n\r\n```\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
    \ pipeline\r\nimport torch\r\nfrom langchain import PromptTemplate\r\nfrom langchain.llms\
    \ import HuggingFacePipeline\r\nfrom langchain.chains.question_answering import\
    \ load_qa_chain\r\n \r\ndef build_qa_chain():\r\n  torch.cuda.empty_cache()\r\n\
    \  model_name = \"databricks/dolly-v2-7b\" # can use dolly-v2-3b or dolly-v2-7b\
    \ for smaller model and faster inferences.\r\n \r\n  # Increase max_new_tokens\
    \ for a longer response\r\n  # Other settings might give better results! Play\
    \ around\r\n  instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16,\
    \ trust_remote_code=True, device_map=\"auto\", \r\n                          \
    \     return_full_text=True, max_new_tokens=256, top_p=0.25, top_k=5)\r\n  # Note:\
    \ if you use dolly 12B or smaller model but a GPU with less than 24GB RAM, use\
    \ 8bit. This requires %pip install bitsandbytes\r\n  # instruct_pipeline = pipeline(model=model_name,\
    \ trust_remote_code=True, device_map=\"auto\", model_kwargs={'load_in_8bit': True})\r\
    \n  # For GPUs without bfloat16 support, like the T4 or V100, use torch_dtype=torch.float16\
    \ below\r\n  # model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"\
    auto\", torch_dtype=torch.float16, trust_remote_code=True)\r\n \r\n  # Defining\
    \ our prompt content.\r\n  # langchain will load our similar documents as {context}\r\
    \n  template = \"\"\"Below is an instruction that describes a task. Write a response\
    \ that appropriately completes the request.\r\n \r\n  Instruction: \r\n  You are\
    \ a question answering agent and your job is to help providing the best answer.\
    \ \r\n  Use only information in the following paragraphs to answer the question\
    \ at the end. Explain the answer with reference to these paragraphs. If you don't\
    \ know, say that you do not know.\r\n \r\n  {context}\r\n \r\n  Question: {question}\r\
    \n \r\n  Response:\r\n  \"\"\"\r\n  prompt = PromptTemplate(input_variables=['context',\
    \ 'question'], template=template)\r\n \r\n  hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\r\
    \n  # Set verbose=True to see the full prompt:\r\n  return load_qa_chain(llm=hf_pipe,\
    \ chain_type=\"stuff\", prompt=prompt, verbose=True)\r\n```\r\n\r\n@srowen can\
    \ you please help me here?"
  created_at: 2023-08-08 09:58:46+00:00
  edited: false
  hidden: false
  id: 64d21fe6c2bd235422fffb88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-08-08T12:15:40.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.968607485294342
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>That isn''t how LLMs will work -  they were trained on vast amounts
          of text from the internet. You can try to write a prompt that encourages
          the model to respond only with reference to the text you supply, or try
          fine-tuning</p>

          '
        raw: That isn't how LLMs will work -  they were trained on vast amounts of
          text from the internet. You can try to write a prompt that encourages the
          model to respond only with reference to the text you supply, or try fine-tuning
        updatedAt: '2023-08-08T12:15:40.463Z'
      numEdits: 0
      reactions: []
    id: 64d231ec2607da25ef548e2c
    type: comment
  author: srowen
  content: That isn't how LLMs will work -  they were trained on vast amounts of text
    from the internet. You can try to write a prompt that encourages the model to
    respond only with reference to the text you supply, or try fine-tuning
  created_at: 2023-08-08 11:15:40+00:00
  edited: false
  hidden: false
  id: 64d231ec2607da25ef548e2c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: databricks/dolly-v2-7b
repo_type: model
status: open
target_branch: null
title: Dolly answers on its own.
