!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rpwr021
conflicting_files: null
created_at: 2023-04-19 18:58:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28301f5fddeb7b789fdf76690cfec2b2.svg
      fullname: rpwr021
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rpwr021
      type: user
    createdAt: '2023-04-19T19:58:23.000Z'
    data:
      edited: false
      editors:
      - rpwr021
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28301f5fddeb7b789fdf76690cfec2b2.svg
          fullname: rpwr021
          isHf: false
          isPro: false
          name: rpwr021
          type: user
        html: '<p>The responses seem to be very different in each prediction once
          the model is loaded? wondering if there are any tweaks to control this behavior?</p>

          <p>for example; for the instruction/question </p>

          <p>"What is the capital of France?"</p>

          <p>I get three different answers ranging from one word to multiple sentences
          </p>

          '
        raw: "The responses seem to be very different in each prediction once the\
          \ model is loaded? wondering if there are any tweaks to control this behavior?\r\
          \n\r\nfor example; for the instruction/question \r\n\r\n\"What is the capital\
          \ of France?\"\r\n\r\nI get three different answers ranging from one word\
          \ to multiple sentences "
        updatedAt: '2023-04-19T19:58:23.577Z'
      numEdits: 0
      reactions: []
    id: 644047dfdc984afcbbbe4653
    type: comment
  author: rpwr021
  content: "The responses seem to be very different in each prediction once the model\
    \ is loaded? wondering if there are any tweaks to control this behavior?\r\n\r\
    \nfor example; for the instruction/question \r\n\r\n\"What is the capital of France?\"\
    \r\n\r\nI get three different answers ranging from one word to multiple sentences "
  created_at: 2023-04-19 18:58:23+00:00
  edited: false
  hidden: false
  id: 644047dfdc984afcbbbe4653
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
      fullname: Matthew Hayes
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: matthayes
      type: user
    createdAt: '2023-04-19T20:00:42.000Z'
    data:
      edited: false
      editors:
      - matthayes
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6424a91ff1d18f46decba5b3/5RrvrmYbW9nh6G1uxZjE6.jpeg?w=200&h=200&f=face
          fullname: Matthew Hayes
          isHf: false
          isPro: false
          name: matthayes
          type: user
        html: '<p>You can experiment with different generation config settings.  Here''s
          some reference documentation: <a href="https://huggingface.co/docs/transformers/main_classes/text_generation">https://huggingface.co/docs/transformers/main_classes/text_generation</a></p>

          <p>Our pipeline has some default settings that cause it to generate different
          samples each time you run it.   If you follow the the link below you can
          see what we''re using.</p>

          <p><a href="https://huggingface.co/databricks/dolly-v2-7b/blob/main/instruct_pipeline.py#L62">https://huggingface.co/databricks/dolly-v2-7b/blob/main/instruct_pipeline.py#L62</a></p>

          '
        raw: 'You can experiment with different generation config settings.  Here''s
          some reference documentation: https://huggingface.co/docs/transformers/main_classes/text_generation


          Our pipeline has some default settings that cause it to generate different
          samples each time you run it.   If you follow the the link below you can
          see what we''re using.


          https://huggingface.co/databricks/dolly-v2-7b/blob/main/instruct_pipeline.py#L62'
        updatedAt: '2023-04-19T20:00:42.510Z'
      numEdits: 0
      reactions: []
    id: 6440486a4164a65ca12ded82
    type: comment
  author: matthayes
  content: 'You can experiment with different generation config settings.  Here''s
    some reference documentation: https://huggingface.co/docs/transformers/main_classes/text_generation


    Our pipeline has some default settings that cause it to generate different samples
    each time you run it.   If you follow the the link below you can see what we''re
    using.


    https://huggingface.co/databricks/dolly-v2-7b/blob/main/instruct_pipeline.py#L62'
  created_at: 2023-04-19 19:00:42+00:00
  edited: false
  hidden: false
  id: 6440486a4164a65ca12ded82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-19T20:15:14.000Z'
    data:
      edited: false
      editors:
      - srowen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
          fullname: Sean Owen
          isHf: false
          isPro: false
          name: srowen
          type: user
        html: '<p>Set do_sample=False , in particular</p>

          '
        raw: Set do_sample=False , in particular
        updatedAt: '2023-04-19T20:15:14.245Z'
      numEdits: 0
      reactions: []
    id: 64404bd22113f7dfcb56b42b
    type: comment
  author: srowen
  content: Set do_sample=False , in particular
  created_at: 2023-04-19 19:15:14+00:00
  edited: false
  hidden: false
  id: 64404bd22113f7dfcb56b42b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/28301f5fddeb7b789fdf76690cfec2b2.svg
      fullname: rpwr021
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rpwr021
      type: user
    createdAt: '2023-04-20T19:20:20.000Z'
    data:
      edited: false
      editors:
      - rpwr021
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/28301f5fddeb7b789fdf76690cfec2b2.svg
          fullname: rpwr021
          isHf: false
          isPro: false
          name: rpwr021
          type: user
        html: '<p>Thank you, that helped</p>

          '
        raw: Thank you, that helped
        updatedAt: '2023-04-20T19:20:20.458Z'
      numEdits: 0
      reactions: []
    id: 644190744c2acf3398abe9b7
    type: comment
  author: rpwr021
  content: Thank you, that helped
  created_at: 2023-04-20 18:20:20+00:00
  edited: false
  hidden: false
  id: 644190744c2acf3398abe9b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63050fbfce6b12280b1e2976/2lJphRSgdt9B_5YAQ1SIs.jpeg?w=200&h=200&f=face
      fullname: Sean Owen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: srowen
      type: user
    createdAt: '2023-04-22T14:03:39.000Z'
    data:
      status: closed
    id: 6443e93bc63001ae6350077c
    type: status-change
  author: srowen
  created_at: 2023-04-22 13:03:39+00:00
  id: 6443e93bc63001ae6350077c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: databricks/dolly-v2-7b
repo_type: model
status: closed
target_branch: null
title: different responses on each prediction?
