!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mullerse
conflicting_files: null
created_at: 2023-12-21 09:30:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
      fullname: "M\xFCller"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mullerse
      type: user
    createdAt: '2023-12-21T09:30:23.000Z'
    data:
      edited: true
      editors:
      - mullerse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6149420142173767
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9918b3a7bad2615f9190320b51d1109.svg
          fullname: "M\xFCller"
          isHf: false
          isPro: false
          name: mullerse
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ , </p>\n<p>this mixtral version of TheBloke is really awesome. It works\
          \ and the quality of the responses is outstanding!<br>My problem relates\
          \ to the response times of the local AI.</p>\n<p>To compare this, I have\
          \ installed the Python version (see source code below) and Oobabooga on\
          \ the same system. Both are running with:<br>Model: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ<br>Branch:\
          \ gptq-4bit-32g-actorder_True</p>\n<h3>Oobabooga:</h3>\n<b>Question:</b>\
          \ How much drinking water is there on our planet?<br>\n<b>Answer:</b> 00:00:11<br>\n\
          <b>Task:</b> Write a poem with minimum 200 words about dolphins.<br>\n<b>Answer:</b>\
          \ 00:00:21\n<br><br>\n<h3>Python/PyCharm:</h3>\n<b>Question:</b> How much\
          \ drinking water is there on our planet?<br>\n<b>Answer Generate:</b> 00:01:53<br>\n\
          <b>Answer Pipeline:</b> 00:01:39<br><br>\n<b>Task:</b> Write a poem with\
          \ minimum 200 words about dolphins.<br>\n<b>Answer Generate:</b> 00:02:25<br>\n\
          <b>Answer Pipeline:</b> 00:02:34<br>\n\n<p>In both test cases the model\
          \ is loaded into the dedicated GPU memory.</p>\n<p>As you can see, the response\
          \ times for the same model and the same hardware are extremely different!<br>Do\
          \ you have any idea why Oobawooga is so much faster or what I can install\
          \ in the Pycharm project to make it perform much better?</p>\n<h3>System:</h3>\n\
          <table><tbody><tr><td>GPU</td><td>2 x RTX4090</td></tr><tr><td>RAM</td><td>100GB\
          \ DDR4</td></tr><tr><td>CPU</td><td>AMD EPYC 7282</td></tr><tr><td>OS</td><td>Windows\
          \ 10</td></tr><tr><td>Python</td><td>3.10</td></tr><tr><td>PyCharm</td><td>2023.2.1</td></tr><tr><td>torch.__version__</td><td><b>2.1.0+cu121</b></td></tr><tr><td>torch.cuda.is_available():</td><td><b>True</b></td></tr><tr><td>transformers</td><td>4.37.0.dev0</td></tr><tr><td>optimum</td><td>1.16.0</td></tr><tr><td>auto-gptq</td><td><b>0.7.0.dev0+cu121*</b></td></tr></tbody></table>\n\
          <b>*:</b><br>\nYou told us to do this:<br>\n<code>pip3 uninstall -y auto-gptq<br>\n\
          git clone https://github.com/PanQiWei/AutoGPTQ<br>\ncd AutoGPTQ<br>\nDISABLE_QIGEN=1\
          \ pip3 install .<br></code>\nDISABLE_QIGEN=1 does not work for me:\n<code>DISABLE_QIGEN=1\
          \ : Die Benennung \"DISABLE_QIGEN=1\" wurde nicht als Name eines Cmdlet,\
          \ einer Funktion, einer Skriptdatei oder eines ausf\xFChrbaren Programms\
          \ erkannt. \xDCberpr\xFCfen Sie die Schreibweise des Namens, oder ob der\
          \ Pfad korrekt ist (sofern enthalten), und wiederholen Sie den Vorgang.\n\
          In Zeile:1 Zeichen:1\n+ DISABLE_QIGEN=1 pip3 install .\n+ ~~~~~~~~~~~~~~~\n\
          \    + CategoryInfo          : ObjectNotFound: (DISABLE_QIGEN=1:String)\
          \ [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException<br></code>\n\
          So i just installed it with \"pip3 install .\"<br>\n<u>Should this unset\
          \ parameter be responsible for this slow behaviour?</u>\n\n<h3>Sourcecode:</h3>\n\
          <code>from datetime import *\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline\n# https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n\
          \n<p>model_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"<br>model\
          \ = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>       \
          \                                      device_map=\"auto\",<br>        \
          \                                     trust_remote_code=False,<br>     \
          \                                        revision=\"gptq-4bit-32g-actorder_True\"\
          )</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)</p>\n<p>prompt = \"Write a poem with minimum 200 words\
          \ about dolphins.\"<br>system_message = \"You have an extremely high level\
          \ of general knowledge and always answer in English.\"<br>prompt_template=f'''[INST]\
          \ &lt;&gt;{system_message}&lt;&gt;{prompt} [/INST]'''</p>\n<p>print(\"\\\
          n\\n*** Generate:\")<br>print(str(datetime.now().strftime('%d.%m.%Y - %H:%M:%S'))\
          \ + \": Start Generate\")<br>input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()<br>output\
          \ = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95,\
          \ top_k=40, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))<br>print(str(datetime.now().strftime('%d.%m.%Y\
          \ - %H:%M:%S')) + \": End Generate\\n\")</p>\n</code><p><code>print(\"***\
          \ Pipeline:\")<br>print(str(datetime.now().strftime('%d.%m.%Y - %H:%M:%S'))\
          \ + \": Start Pipeline\")<br>pipe = pipeline(<br>    \"text-generation\"\
          ,<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>\
          \    do_sample=True,<br>    temperature=0.7,<br>    top_p=0.95,<br>    top_k=40,<br>\
          \    repetition_penalty=1.1<br>)<br>print(pipe(prompt_template)[0]['generated_text'])<br>print(str(datetime.now().strftime('%d.%m.%Y\
          \ - %H:%M:%S')) + \": End Pipeline\")<br></code></p>\n<p><b>No errors appear\
          \ during execution, only the following warning:</b><br>Warning (from warnings\
          \ module):<br>  File \"(...)\\Python\\Python310\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1547<br>    warnings.warn(<br>UserWarning:\
          \ You have modified the pretrained model configuration to control generation.\
          \ This is a deprecated strategy to control generation and will be removed\
          \ soon, in a future version. Please use and modify the model generation\
          \ configuration (see <a href=\"https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\"\
          >https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration</a>\
          \ )</p>\n<p><b>nvidia-smi</b><font face=\"Courier New\"><br>+---------------------------------------------------------------------------------------+<br>|\
          \ NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version:\
          \ 12.2     |<br>|-----------------------------------------+----------------------+----------------------+<br>|\
          \ GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |<br>|                                        \
          \ |                      |               MIG M. |<br>|=========================================+======================+======================|<br>|\
          \   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:00.0 Off |     \
          \             Off |<br>|  0%   41C    P8              12W / 450W |    514MiB\
          \ / 24564MiB |      6%      Default |<br>|                             \
          \            |                      |                  N/A |<br>+-----------------------------------------+----------------------+----------------------+<br>|\
          \   1  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:01.0 Off |     \
          \             Off |<br>+---------------------------------------------------------------------------------------+<br>|\
          \ Processes:                                                           \
          \                 |<br>|  GPU   GI   CI        PID   Type   Process name\
          \                            GPU Memory |<br>|        ID   ID          \
          \                                                   Usage      |<br>|=======================================================================================|<br>|\
          \    0   N/A  N/A      2632    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe\
          \    N/A      |<br>|    0   N/A  N/A      2924    C+G   ...oogle\\Chrome\\\
          Application\\chrome.exe    N/A      |<br>|    0   N/A  N/A      6764   \
          \ C+G   C:\\Windows\\explorer.exe                   N/A      |<br>|    0\
          \   N/A  N/A      7656    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe\
          \    N/A      |<br>|    0   N/A  N/A      7748    C+G   ....Search_cw5n1h2txyewy\\\
          SearchApp.exe    N/A      |<br>|    0   N/A  N/A      8816    C+G   ...crosoft\\\
          Edge\\Application\\msedge.exe    N/A      |<br>|    0   N/A  N/A     10340\
          \    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |<br>|  \
          \  1   N/A  N/A      4980    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe\
          \    N/A      |<br>+---------------------------------------------------------------------------------------+</font></p>\n\
          <p>Thank you very much in advance! :) </p>\n"
        raw: "Hey @TheBloke , \n\nthis mixtral version of TheBloke is really awesome.\
          \ It works and the quality of the responses is outstanding!\nMy problem\
          \ relates to the response times of the local AI.\n\nTo compare this, I have\
          \ installed the Python version (see source code below) and Oobabooga on\
          \ the same system. Both are running with:\nModel: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n\
          Branch: gptq-4bit-32g-actorder_True\n<h3>Oobabooga:</h3>\n<b>Question:</b>\
          \ How much drinking water is there on our planet?<br>\n<b>Answer:</b> 00:00:11<br>\n\
          <b>Task:</b> Write a poem with minimum 200 words about dolphins.<br>\n<b>Answer:</b>\
          \ 00:00:21\n<br><br>\n<h3>Python/PyCharm:</h3>\n<b>Question:</b> How much\
          \ drinking water is there on our planet?<br>\n<b>Answer Generate:</b> 00:01:53<br>\n\
          <b>Answer Pipeline:</b> 00:01:39<br><br>\n<b>Task:</b> Write a poem with\
          \ minimum 200 words about dolphins.<br>\n<b>Answer Generate:</b> 00:02:25<br>\n\
          <b>Answer Pipeline:</b> 00:02:34<br>\n\nIn both test cases the model is\
          \ loaded into the dedicated GPU memory.\n\nAs you can see, the response\
          \ times for the same model and the same hardware are extremely different!\n\
          Do you have any idea why Oobawooga is so much faster or what I can install\
          \ in the Pycharm project to make it perform much better?\n\n<h3>System:</h3>\n\
          <table><tr><td>GPU</td><td>2 x RTX4090</td></tr><tr><td>RAM</td><td>100GB\
          \ DDR4</td></tr><tr><td>CPU</td><td>AMD EPYC 7282</td></tr><tr><td>OS</td><td>Windows\
          \ 10</td></tr><tr><td>Python</td><td>3.10</td></tr><tr><td>PyCharm</td><td>2023.2.1</td></tr><tr><td>torch.__version__</td><td><b>2.1.0+cu121</b></td></tr><tr><td>torch.cuda.is_available():</td><td><b>True</b></td></tr><tr><td>transformers</td><td>4.37.0.dev0</td></tr><tr><td>optimum</td><td>1.16.0</td></tr><tr><td>auto-gptq</td><td><b>0.7.0.dev0+cu121*</b></td></tr></table>\n\
          <b>*:</b><br>\nYou told us to do this:<br>\n<code>pip3 uninstall -y auto-gptq<br>\n\
          git clone https://github.com/PanQiWei/AutoGPTQ<br>\ncd AutoGPTQ<br>\nDISABLE_QIGEN=1\
          \ pip3 install .<br></code>\nDISABLE_QIGEN=1 does not work for me:\n<code>DISABLE_QIGEN=1\
          \ : Die Benennung \"DISABLE_QIGEN=1\" wurde nicht als Name eines Cmdlet,\
          \ einer Funktion, einer Skriptdatei oder eines ausf\xFChrbaren Programms\
          \ erkannt. \xDCberpr\xFCfen Sie die Schreibweise des Namens, oder ob der\
          \ Pfad korrekt ist (sofern enthalten), und wiederholen Sie den Vorgang.\n\
          In Zeile:1 Zeichen:1\n+ DISABLE_QIGEN=1 pip3 install .\n+ ~~~~~~~~~~~~~~~\n\
          \    + CategoryInfo          : ObjectNotFound: (DISABLE_QIGEN=1:String)\
          \ [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException<br></code>\n\
          So i just installed it with \"pip3 install .\"<br>\n<u>Should this unset\
          \ parameter be responsible for this slow behaviour?</u>\n\n<h3>Sourcecode:</h3>\n\
          <code>from datetime import *\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, pipeline\n# https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n\
          \nmodel_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n         \
          \                                    device_map=\"auto\",\n            \
          \                                 trust_remote_code=False,\n           \
          \                                  revision=\"gptq-4bit-32g-actorder_True\"\
          )\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
          \nprompt = \"Write a poem with minimum 200 words about dolphins.\"\nsystem_message\
          \ = \"You have an extremely high level of general knowledge and always answer\
          \ in English.\"\nprompt_template=f'''[INST] <<SYS>>{system_message}<</SYS>>{prompt}\
          \ [/INST]'''\n\nprint(\"\\n\\n*** Generate:\")\nprint(str(datetime.now().strftime('%d.%m.%Y\
          \ - %H:%M:%S')) + \": Start Generate\")\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\nprint(str(datetime.now().strftime('%d.%m.%Y\
          \ - %H:%M:%S')) + \": End Generate\\n\")\n\nprint(\"*** Pipeline:\")\nprint(str(datetime.now().strftime('%d.%m.%Y\
          \ - %H:%M:%S')) + \": Start Pipeline\")\npipe = pipeline(\n    \"text-generation\"\
          ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
          \    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n\
          \    repetition_penalty=1.1\n)\nprint(pipe(prompt_template)[0]['generated_text'])\n\
          print(str(datetime.now().strftime('%d.%m.%Y - %H:%M:%S')) + \": End Pipeline\"\
          )\n</code>\n\n<b>No errors appear during execution, only the following warning:</b>\n\
          Warning (from warnings module):\n  File \"(...)\\Python\\Python310\\lib\\\
          site-packages\\transformers\\generation\\utils.py\", line 1547\n    warnings.warn(\n\
          UserWarning: You have modified the pretrained model configuration to control\
          \ generation. This is a deprecated strategy to control generation and will\
          \ be removed soon, in a future version. Please use and modify the model\
          \ generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
          \ )\n\n<b>nvidia-smi</b><font face=\"Courier New\">\n+---------------------------------------------------------------------------------------+\n\
          | NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version:\
          \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
          | GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\n|                                         |\
          \                      |               MIG M. |\n|=========================================+======================+======================|\n\
          |   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:00.0 Off |     \
          \             Off |\n|  0%   41C    P8              12W / 450W |    514MiB\
          \ / 24564MiB |      6%      Default |\n|                               \
          \          |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
          |   1  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:01.0 Off |     \
          \             Off |\n+---------------------------------------------------------------------------------------+\n\
          | Processes:                                                           \
          \                 |\n|  GPU   GI   CI        PID   Type   Process name \
          \                           GPU Memory |\n|        ID   ID             \
          \                                                Usage      |\n|=======================================================================================|\n\
          |    0   N/A  N/A      2632    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe\
          \    N/A      |\n|    0   N/A  N/A      2924    C+G   ...oogle\\Chrome\\\
          Application\\chrome.exe    N/A      |\n|    0   N/A  N/A      6764    C+G\
          \   C:\\Windows\\explorer.exe                   N/A      |\n|    0   N/A\
          \  N/A      7656    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A\
          \      |\n|    0   N/A  N/A      7748    C+G   ....Search_cw5n1h2txyewy\\\
          SearchApp.exe    N/A      |\n|    0   N/A  N/A      8816    C+G   ...crosoft\\\
          Edge\\Application\\msedge.exe    N/A      |\n|    0   N/A  N/A     10340\
          \    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n|    1\
          \   N/A  N/A      4980    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe\
          \    N/A      |\n+---------------------------------------------------------------------------------------+</font>\n\
          \n\nThank you very much in advance! :) "
        updatedAt: '2023-12-21T11:55:10.778Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - samcomber
    id: 658405afe09e5df0308d4a3e
    type: comment
  author: mullerse
  content: "Hey @TheBloke , \n\nthis mixtral version of TheBloke is really awesome.\
    \ It works and the quality of the responses is outstanding!\nMy problem relates\
    \ to the response times of the local AI.\n\nTo compare this, I have installed\
    \ the Python version (see source code below) and Oobabooga on the same system.\
    \ Both are running with:\nModel: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\nBranch:\
    \ gptq-4bit-32g-actorder_True\n<h3>Oobabooga:</h3>\n<b>Question:</b> How much\
    \ drinking water is there on our planet?<br>\n<b>Answer:</b> 00:00:11<br>\n<b>Task:</b>\
    \ Write a poem with minimum 200 words about dolphins.<br>\n<b>Answer:</b> 00:00:21\n\
    <br><br>\n<h3>Python/PyCharm:</h3>\n<b>Question:</b> How much drinking water is\
    \ there on our planet?<br>\n<b>Answer Generate:</b> 00:01:53<br>\n<b>Answer Pipeline:</b>\
    \ 00:01:39<br><br>\n<b>Task:</b> Write a poem with minimum 200 words about dolphins.<br>\n\
    <b>Answer Generate:</b> 00:02:25<br>\n<b>Answer Pipeline:</b> 00:02:34<br>\n\n\
    In both test cases the model is loaded into the dedicated GPU memory.\n\nAs you\
    \ can see, the response times for the same model and the same hardware are extremely\
    \ different!\nDo you have any idea why Oobawooga is so much faster or what I can\
    \ install in the Pycharm project to make it perform much better?\n\n<h3>System:</h3>\n\
    <table><tr><td>GPU</td><td>2 x RTX4090</td></tr><tr><td>RAM</td><td>100GB DDR4</td></tr><tr><td>CPU</td><td>AMD\
    \ EPYC 7282</td></tr><tr><td>OS</td><td>Windows 10</td></tr><tr><td>Python</td><td>3.10</td></tr><tr><td>PyCharm</td><td>2023.2.1</td></tr><tr><td>torch.__version__</td><td><b>2.1.0+cu121</b></td></tr><tr><td>torch.cuda.is_available():</td><td><b>True</b></td></tr><tr><td>transformers</td><td>4.37.0.dev0</td></tr><tr><td>optimum</td><td>1.16.0</td></tr><tr><td>auto-gptq</td><td><b>0.7.0.dev0+cu121*</b></td></tr></table>\n\
    <b>*:</b><br>\nYou told us to do this:<br>\n<code>pip3 uninstall -y auto-gptq<br>\n\
    git clone https://github.com/PanQiWei/AutoGPTQ<br>\ncd AutoGPTQ<br>\nDISABLE_QIGEN=1\
    \ pip3 install .<br></code>\nDISABLE_QIGEN=1 does not work for me:\n<code>DISABLE_QIGEN=1\
    \ : Die Benennung \"DISABLE_QIGEN=1\" wurde nicht als Name eines Cmdlet, einer\
    \ Funktion, einer Skriptdatei oder eines ausf\xFChrbaren Programms erkannt. \xDC\
    berpr\xFCfen Sie die Schreibweise des Namens, oder ob der Pfad korrekt ist (sofern\
    \ enthalten), und wiederholen Sie den Vorgang.\nIn Zeile:1 Zeichen:1\n+ DISABLE_QIGEN=1\
    \ pip3 install .\n+ ~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound:\
    \ (DISABLE_QIGEN=1:String) [], CommandNotFoundException\n    + FullyQualifiedErrorId\
    \ : CommandNotFoundException<br></code>\nSo i just installed it with \"pip3 install\
    \ .\"<br>\n<u>Should this unset parameter be responsible for this slow behaviour?</u>\n\
    \n<h3>Sourcecode:</h3>\n<code>from datetime import *\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer, pipeline\n# https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\n\
    \nmodel_name_or_path = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    \                                             device_map=\"auto\",\n         \
    \                                    trust_remote_code=False,\n              \
    \                               revision=\"gptq-4bit-32g-actorder_True\")\n\n\
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\
    \nprompt = \"Write a poem with minimum 200 words about dolphins.\"\nsystem_message\
    \ = \"You have an extremely high level of general knowledge and always answer\
    \ in English.\"\nprompt_template=f'''[INST] <<SYS>>{system_message}<</SYS>>{prompt}\
    \ [/INST]'''\n\nprint(\"\\n\\n*** Generate:\")\nprint(str(datetime.now().strftime('%d.%m.%Y\
    \ - %H:%M:%S')) + \": Start Generate\")\ninput_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\nprint(str(datetime.now().strftime('%d.%m.%Y\
    \ - %H:%M:%S')) + \": End Generate\\n\")\n\nprint(\"*** Pipeline:\")\nprint(str(datetime.now().strftime('%d.%m.%Y\
    \ - %H:%M:%S')) + \": Start Pipeline\")\npipe = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
    )\nprint(pipe(prompt_template)[0]['generated_text'])\nprint(str(datetime.now().strftime('%d.%m.%Y\
    \ - %H:%M:%S')) + \": End Pipeline\")\n</code>\n\n<b>No errors appear during execution,\
    \ only the following warning:</b>\nWarning (from warnings module):\n  File \"\
    (...)\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py\"\
    , line 1547\n    warnings.warn(\nUserWarning: You have modified the pretrained\
    \ model configuration to control generation. This is a deprecated strategy to\
    \ control generation and will be removed soon, in a future version. Please use\
    \ and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration\
    \ )\n\n<b>nvidia-smi</b><font face=\"Courier New\">\n+---------------------------------------------------------------------------------------+\n\
    | NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version:\
    \ 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n\
    | GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr.\
    \ ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util\
    \  Compute M. |\n|                                         |                 \
    \     |               MIG M. |\n|=========================================+======================+======================|\n\
    |   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:00.0 Off |           \
    \       Off |\n|  0%   41C    P8              12W / 450W |    514MiB / 24564MiB\
    \ |      6%      Default |\n|                                         |      \
    \                |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n\
    |   1  NVIDIA GeForce RTX 4090      WDDM  | 00000000:03:01.0 Off |           \
    \       Off |\n+---------------------------------------------------------------------------------------+\n\
    | Processes:                                                                 \
    \           |\n|  GPU   GI   CI        PID   Type   Process name             \
    \               GPU Memory |\n|        ID   ID                               \
    \                              Usage      |\n|=======================================================================================|\n\
    |    0   N/A  N/A      2632    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe \
    \   N/A      |\n|    0   N/A  N/A      2924    C+G   ...oogle\\Chrome\\Application\\\
    chrome.exe    N/A      |\n|    0   N/A  N/A      6764    C+G   C:\\Windows\\explorer.exe\
    \                   N/A      |\n|    0   N/A  N/A      7656    C+G   ...5n1h2txyewy\\\
    ShellExperienceHost.exe    N/A      |\n|    0   N/A  N/A      7748    C+G   ....Search_cw5n1h2txyewy\\\
    SearchApp.exe    N/A      |\n|    0   N/A  N/A      8816    C+G   ...crosoft\\\
    Edge\\Application\\msedge.exe    N/A      |\n|    0   N/A  N/A     10340    C+G\
    \   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n|    1   N/A  N/A \
    \     4980    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n+---------------------------------------------------------------------------------------+</font>\n\
    \n\nThank you very much in advance! :) "
  created_at: 2023-12-21 09:30:23+00:00
  edited: true
  hidden: false
  id: 658405afe09e5df0308d4a3e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Is working with 2xRTX4090 and GPTQ but extremly slow
