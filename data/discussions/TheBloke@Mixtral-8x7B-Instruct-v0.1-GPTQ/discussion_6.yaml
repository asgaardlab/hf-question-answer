!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nazrak-atlassian
conflicting_files: null
created_at: 2023-12-17 22:12:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
      fullname: Nathan Azrak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nazrak-atlassian
      type: user
    createdAt: '2023-12-17T22:12:18.000Z'
    data:
      edited: true
      editors:
      - nazrak-atlassian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6825132369995117
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
          fullname: Nathan Azrak
          isHf: false
          isPro: false
          name: nazrak-atlassian
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>,\
          \ my guess is that this isn't on you, but curious if you were able to get\
          \ it working. Using TGI latest on EC2 with a <code>g5.12xlarge</code>:<br><code>docker\
          \ run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data ghcr.io/huggingface/text-generation-inference:latest\
          \ --model-id TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ --sharded true --num-shard\
          \ 4 --quantize gptq</code></p>\n<p>This installs TGI version 1.3. Image\
          \ SHA is <code>4cb7c8ab86a48d5445ba2237044a3855d989d94d77224dd4d80bc469f962d2ca</code>\
          \ and was pushed 2 days ago so I assume it has the latest TGI hotfixes for\
          \ 1.3.3.</p>\n<p>I get an error to do with world size.</p>\n<pre><code class=\"\
          language-py\"><span class=\"hljs-number\">2023</span>-<span class=\"hljs-number\"\
          >12</span>-17T06:<span class=\"hljs-number\">28</span>:<span class=\"hljs-number\"\
          >28.002180</span>Z  WARN text_generation_launcher: Disabling exllama v2\
          \ <span class=\"hljs-keyword\">and</span> using v1 instead because there\
          \ are issues when sharding\n<span class=\"hljs-number\">2023</span>-<span\
          \ class=\"hljs-number\">12</span>-17T06:<span class=\"hljs-number\">28</span>:<span\
          \ class=\"hljs-number\">30.812512</span>Z  INFO text_generation_launcher:\
          \ Using exllama kernels v1\n<span class=\"hljs-number\">2023</span>-<span\
          \ class=\"hljs-number\">12</span>-17T06:<span class=\"hljs-number\">28</span>:<span\
          \ class=\"hljs-number\">30.816449</span>Z ERROR text_generation_launcher:\
          \ Error when initializing model\n...\n    self.model = MixtralModel(config,\
          \ weights)\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          </span>, line <span class=\"hljs-number\">749</span>, <span class=\"hljs-keyword\"\
          >in</span> __init__\n    [\n  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          </span>, line <span class=\"hljs-number\">750</span>, <span class=\"hljs-keyword\"\
          >in</span> &lt;listcomp&gt;\n    MixtralLayer(\n  File <span class=\"hljs-string\"\
          >\"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          </span>, line <span class=\"hljs-number\">684</span>, <span class=\"hljs-keyword\"\
          >in</span> __init__\n    self.self_attn = MixtralAttention(\n  File <span\
          \ class=\"hljs-string\">\"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          </span>, line <span class=\"hljs-number\">226</span>, <span class=\"hljs-keyword\"\
          >in</span> __init__\n    self.o_proj = TensorParallelRowLinear.load(\n \
          \ File <span class=\"hljs-string\">\"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py\"\
          </span>, line <span class=\"hljs-number\">478</span>, <span class=\"hljs-keyword\"\
          >in</span> load\n    weight = weights.get_multi_weights_row(prefix, quantize=config.quantize)\n\
          \  File <span class=\"hljs-string\">\"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/weights.py\"\
          </span>, line <span class=\"hljs-number\">285</span>, <span class=\"hljs-keyword\"\
          >in</span> get_multi_weights_row\n    qzeros = self.get_sharded(<span class=\"\
          hljs-string\">f\"<span class=\"hljs-subst\">{prefix}</span>.qzeros\"</span>,\
          \ dim=<span class=\"hljs-number\">0</span>)\n  File <span class=\"hljs-string\"\
          >\"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/weights.py\"\
          </span>, line <span class=\"hljs-number\">118</span>, <span class=\"hljs-keyword\"\
          >in</span> get_sharded\n    size % world_size == <span class=\"hljs-number\"\
          >0</span>\nAssertionError: The choosen size <span class=\"hljs-number\"\
          >1</span> <span class=\"hljs-keyword\">is</span> <span class=\"hljs-keyword\"\
          >not</span> compatible <span class=\"hljs-keyword\">with</span> sharding\
          \ on <span class=\"hljs-number\">4</span> shards\n</code></pre>\n<p>That's\
          \ not the full traceback, but I think the important points. As far as you\
          \ know is there any architectural limitation that would prevent the model\
          \ from working when sharded?</p>\n"
        raw: "Hey @TheBloke, my guess is that this isn't on you, but curious if you\
          \ were able to get it working. Using TGI latest on EC2 with a `g5.12xlarge`:\n\
          ```docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data\
          \ ghcr.io/huggingface/text-generation-inference:latest --model-id TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\
          \ --sharded true --num-shard 4 --quantize gptq```\n\nThis installs TGI version\
          \ 1.3. Image SHA is `4cb7c8ab86a48d5445ba2237044a3855d989d94d77224dd4d80bc469f962d2ca`\
          \ and was pushed 2 days ago so I assume it has the latest TGI hotfixes for\
          \ 1.3.3.\n\nI get an error to do with world size.\n```py\n2023-12-17T06:28:28.002180Z\
          \  WARN text_generation_launcher: Disabling exllama v2 and using v1 instead\
          \ because there are issues when sharding\n2023-12-17T06:28:30.812512Z  INFO\
          \ text_generation_launcher: Using exllama kernels v1\n2023-12-17T06:28:30.816449Z\
          \ ERROR text_generation_launcher: Error when initializing model\n...\n \
          \   self.model = MixtralModel(config, weights)\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          , line 749, in __init__\n    [\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          , line 750, in <listcomp>\n    MixtralLayer(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          , line 684, in __init__\n    self.self_attn = MixtralAttention(\n  File\
          \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
          , line 226, in __init__\n    self.o_proj = TensorParallelRowLinear.load(\n\
          \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py\"\
          , line 478, in load\n    weight = weights.get_multi_weights_row(prefix,\
          \ quantize=config.quantize)\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/weights.py\"\
          , line 285, in get_multi_weights_row\n    qzeros = self.get_sharded(f\"\
          {prefix}.qzeros\", dim=0)\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/weights.py\"\
          , line 118, in get_sharded\n    size % world_size == 0\nAssertionError:\
          \ The choosen size 1 is not compatible with sharding on 4 shards\n```\n\n\
          That's not the full traceback, but I think the important points. As far\
          \ as you know is there any architectural limitation that would prevent the\
          \ model from working when sharded?"
        updatedAt: '2023-12-17T22:14:30.700Z'
      numEdits: 2
      reactions: []
    id: 657f7242869d5bb0e5a98baa
    type: comment
  author: nazrak-atlassian
  content: "Hey @TheBloke, my guess is that this isn't on you, but curious if you\
    \ were able to get it working. Using TGI latest on EC2 with a `g5.12xlarge`:\n\
    ```docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ --sharded true --num-shard\
    \ 4 --quantize gptq```\n\nThis installs TGI version 1.3. Image SHA is `4cb7c8ab86a48d5445ba2237044a3855d989d94d77224dd4d80bc469f962d2ca`\
    \ and was pushed 2 days ago so I assume it has the latest TGI hotfixes for 1.3.3.\n\
    \nI get an error to do with world size.\n```py\n2023-12-17T06:28:28.002180Z  WARN\
    \ text_generation_launcher: Disabling exllama v2 and using v1 instead because\
    \ there are issues when sharding\n2023-12-17T06:28:30.812512Z  INFO text_generation_launcher:\
    \ Using exllama kernels v1\n2023-12-17T06:28:30.816449Z ERROR text_generation_launcher:\
    \ Error when initializing model\n...\n    self.model = MixtralModel(config, weights)\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
    , line 749, in __init__\n    [\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
    , line 750, in <listcomp>\n    MixtralLayer(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
    , line 684, in __init__\n    self.self_attn = MixtralAttention(\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/models/custom_modeling/flash_mixtral_modeling.py\"\
    , line 226, in __init__\n    self.o_proj = TensorParallelRowLinear.load(\n  File\
    \ \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/layers.py\"\
    , line 478, in load\n    weight = weights.get_multi_weights_row(prefix, quantize=config.quantize)\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/weights.py\"\
    , line 285, in get_multi_weights_row\n    qzeros = self.get_sharded(f\"{prefix}.qzeros\"\
    , dim=0)\n  File \"/opt/conda/lib/python3.10/site-packages/text_generation_server/utils/weights.py\"\
    , line 118, in get_sharded\n    size % world_size == 0\nAssertionError: The choosen\
    \ size 1 is not compatible with sharding on 4 shards\n```\n\nThat's not the full\
    \ traceback, but I think the important points. As far as you know is there any\
    \ architectural limitation that would prevent the model from working when sharded?"
  created_at: 2023-12-17 22:12:18+00:00
  edited: true
  hidden: false
  id: 657f7242869d5bb0e5a98baa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
      fullname: Nathan Azrak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nazrak-atlassian
      type: user
    createdAt: '2023-12-18T00:46:24.000Z'
    data:
      edited: true
      editors:
      - nazrak-atlassian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9590182900428772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
          fullname: Nathan Azrak
          isHf: false
          isPro: false
          name: nazrak-atlassian
          type: user
        html: '<p>Just noticed in the model card (sorry missed it the first time,
          it was pretty far down):</p>

          <pre><code>Serving this model from Text Generation Inference (TGI)

          Not currently supported for Mixtral models

          </code></pre>

          <p>Can you provide any clarity on what the blocker is here? I know TGI supports
          Mixtral at a base level and have deployed a non-quantized version.</p>

          '
        raw: 'Just noticed in the model card (sorry missed it the first time, it was
          pretty far down):

          ```

          Serving this model from Text Generation Inference (TGI)

          Not currently supported for Mixtral models

          ```


          Can you provide any clarity on what the blocker is here? I know TGI supports
          Mixtral at a base level and have deployed a non-quantized version.'
        updatedAt: '2023-12-18T00:46:45.767Z'
      numEdits: 3
      reactions: []
    id: 657f96601ede8a7bb705f3b7
    type: comment
  author: nazrak-atlassian
  content: 'Just noticed in the model card (sorry missed it the first time, it was
    pretty far down):

    ```

    Serving this model from Text Generation Inference (TGI)

    Not currently supported for Mixtral models

    ```


    Can you provide any clarity on what the blocker is here? I know TGI supports Mixtral
    at a base level and have deployed a non-quantized version.'
  created_at: 2023-12-18 00:46:24+00:00
  edited: true
  hidden: false
  id: 657f96601ede8a7bb705f3b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-18T16:14:34.000Z'
    data:
      edited: false
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9736332893371582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: '<p>I''m facing the same issue, any suggestion? </p>

          <p>Thanks in advance</p>

          '
        raw: "I'm facing the same issue, any suggestion? \n\nThanks in advance"
        updatedAt: '2023-12-18T16:14:34.485Z'
      numEdits: 0
      reactions: []
    id: 65806feaeca6f774556c60f0
    type: comment
  author: AbRds
  content: "I'm facing the same issue, any suggestion? \n\nThanks in advance"
  created_at: 2023-12-18 16:14:34+00:00
  edited: false
  hidden: false
  id: 65806feaeca6f774556c60f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
      fullname: Nathan Azrak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nazrak-atlassian
      type: user
    createdAt: '2023-12-19T02:12:19.000Z'
    data:
      edited: false
      editors:
      - nazrak-atlassian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9826948642730713
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
          fullname: Nathan Azrak
          isHf: false
          isPro: false
          name: nazrak-atlassian
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;AbRds&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AbRds\">@<span class=\"\
          underline\">AbRds</span></a></span>\n\n\t</span></span> I've swapped to\
          \ EETQ for quantization for now. I believe it's supported from TGI 1.3.2\
          \ onward.</p>\n"
        raw: '@AbRds I''ve swapped to EETQ for quantization for now. I believe it''s
          supported from TGI 1.3.2 onward.'
        updatedAt: '2023-12-19T02:12:19.908Z'
      numEdits: 0
      reactions: []
    id: 6580fc034c23e9d7d7eccf38
    type: comment
  author: nazrak-atlassian
  content: '@AbRds I''ve swapped to EETQ for quantization for now. I believe it''s
    supported from TGI 1.3.2 onward.'
  created_at: 2023-12-19 02:12:19+00:00
  edited: false
  hidden: false
  id: 6580fc034c23e9d7d7eccf38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-19T11:08:34.000Z'
    data:
      edited: false
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8847866058349609
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;AbRds&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AbRds\"\
          >@<span class=\"underline\">AbRds</span></a></span>\n\n\t</span></span>\
          \ I've swapped to EETQ for quantization for now. I believe it's supported\
          \ from TGI 1.3.2 onward.</p>\n</blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nazrak-atlassian&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nazrak-atlassian\"\
          >@<span class=\"underline\">nazrak-atlassian</span></a></span>\n\n\t</span></span>\
          \ I'm not familiar with EETQ, how it works? I just have to pass the parameter\
          \ eetq instead of gptq?<br>I've tried passing the eetq argument but I have\
          \ receive another error:</p>\n<p><code>RuntimeError: weight model.layers.0.self_attn.q_proj.weight\
          \ does not exist</code></p>\n"
        raw: "> @AbRds I've swapped to EETQ for quantization for now. I believe it's\
          \ supported from TGI 1.3.2 onward.\n\nHi @nazrak-atlassian I'm not familiar\
          \ with EETQ, how it works? I just have to pass the parameter eetq instead\
          \ of gptq? \nI've tried passing the eetq argument but I have receive another\
          \ error:\n\n```RuntimeError: weight model.layers.0.self_attn.q_proj.weight\
          \ does not exist```\n"
        updatedAt: '2023-12-19T11:08:34.245Z'
      numEdits: 0
      reactions: []
    id: 658179b223a7aac397eb65f0
    type: comment
  author: AbRds
  content: "> @AbRds I've swapped to EETQ for quantization for now. I believe it's\
    \ supported from TGI 1.3.2 onward.\n\nHi @nazrak-atlassian I'm not familiar with\
    \ EETQ, how it works? I just have to pass the parameter eetq instead of gptq?\
    \ \nI've tried passing the eetq argument but I have receive another error:\n\n\
    ```RuntimeError: weight model.layers.0.self_attn.q_proj.weight does not exist```\n"
  created_at: 2023-12-19 11:08:34+00:00
  edited: false
  hidden: false
  id: 658179b223a7aac397eb65f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
      fullname: Nathan Azrak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nazrak-atlassian
      type: user
    createdAt: '2023-12-19T15:15:18.000Z'
    data:
      edited: true
      editors:
      - nazrak-atlassian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.867013692855835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66670f471646900e79f37432681219e6.svg
          fullname: Nathan Azrak
          isHf: false
          isPro: false
          name: nazrak-atlassian
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;AbRds&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/AbRds\">@<span class=\"\
          underline\">AbRds</span></a></span>\n\n\t</span></span> Yes you should be\
          \ able to change the --quantize arg. EETQ an improved in-place 8bit quant\
          \ technique (supposedly better performing than bitsandbytes'), but I couldn't\
          \ speak to it more than that. It worked for me with TGI 1.3.3 with the following\
          \ docker run:</p>\n<pre><code class=\"language-bash\">docker run --gpus\
          \ all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data ghcr.io/huggingface/text-generation-inference:latest\
          \ --model-id mistralai/Mixtral-8x7B-v0.1 --sharded <span class=\"hljs-literal\"\
          >true</span> --num-shard 4 --quantize eetq\n</code></pre>\n<p>My guess is\
          \ that you are trying to use TheBloke's GPTQ model with eetq. It should\
          \ be run on the original model, and will perform the quantization on the\
          \ fly. Note that it's only an 8bit quantization, so if you require 4bit\
          \ for your VRAM requirements it will not work unfortunately.</p>\n"
        raw: '@AbRds Yes you should be able to change the --quantize arg. EETQ an
          improved in-place 8bit quant technique (supposedly better performing than
          bitsandbytes''), but I couldn''t speak to it more than that. It worked for
          me with TGI 1.3.3 with the following docker run:


          ```bash

          docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data
          ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-v0.1
          --sharded true --num-shard 4 --quantize eetq

          ```


          My guess is that you are trying to use TheBloke''s GPTQ model with eetq.
          It should be run on the original model, and will perform the quantization
          on the fly. Note that it''s only an 8bit quantization, so if you require
          4bit for your VRAM requirements it will not work unfortunately.'
        updatedAt: '2023-12-19T15:15:38.322Z'
      numEdits: 1
      reactions: []
    id: 6581b386c2d7971187f7d007
    type: comment
  author: nazrak-atlassian
  content: '@AbRds Yes you should be able to change the --quantize arg. EETQ an improved
    in-place 8bit quant technique (supposedly better performing than bitsandbytes''),
    but I couldn''t speak to it more than that. It worked for me with TGI 1.3.3 with
    the following docker run:


    ```bash

    docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data ghcr.io/huggingface/text-generation-inference:latest
    --model-id mistralai/Mixtral-8x7B-v0.1 --sharded true --num-shard 4 --quantize
    eetq

    ```


    My guess is that you are trying to use TheBloke''s GPTQ model with eetq. It should
    be run on the original model, and will perform the quantization on the fly. Note
    that it''s only an 8bit quantization, so if you require 4bit for your VRAM requirements
    it will not work unfortunately.'
  created_at: 2023-12-19 15:15:18+00:00
  edited: true
  hidden: false
  id: 6581b386c2d7971187f7d007
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
      fullname: Rojas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbRds
      type: user
    createdAt: '2023-12-19T15:54:35.000Z'
    data:
      edited: false
      editors:
      - AbRds
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8867924809455872
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2bf6c287895de41e26cc44a709c8cfa.svg
          fullname: Rojas
          isHf: false
          isPro: false
          name: AbRds
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;AbRds&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AbRds\"\
          >@<span class=\"underline\">AbRds</span></a></span>\n\n\t</span></span>\
          \ Yes you should be able to change the --quantize arg. EETQ an improved\
          \ in-place 8bit quant technique (supposedly better performing than bitsandbytes'),\
          \ but I couldn't speak to it more than that. It worked for me with TGI 1.3.3\
          \ with the following docker run:</p>\n<pre><code class=\"language-bash\"\
          >docker run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data\
          \ ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-v0.1\
          \ --sharded <span class=\"hljs-literal\">true</span> --num-shard 4 --quantize\
          \ eetq\n</code></pre>\n<p>My guess is that you are trying to use TheBloke's\
          \ GPTQ model with eetq. It should be run on the original model, and will\
          \ perform the quantization on the fly. Note that it's only an 8bit quantization,\
          \ so if you require 4bit for your VRAM requirements it will not work unfortunately.</p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;nazrak-atlassian&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nazrak-atlassian\"\
          >@<span class=\"underline\">nazrak-atlassian</span></a></span>\n\n\t</span></span>\
          \ you're right, I was trying to run Thebloke's version instead of the original\
          \ one, now works perfectly.</p>\n<p>Thanks a lot. </p>\n"
        raw: "> @AbRds Yes you should be able to change the --quantize arg. EETQ an\
          \ improved in-place 8bit quant technique (supposedly better performing than\
          \ bitsandbytes'), but I couldn't speak to it more than that. It worked for\
          \ me with TGI 1.3.3 with the following docker run:\n> \n> ```bash\n> docker\
          \ run --gpus all --shm-size 1g -p 8080:80 -v /opt/dlami/nvme/data:/data\
          \ ghcr.io/huggingface/text-generation-inference:latest --model-id mistralai/Mixtral-8x7B-v0.1\
          \ --sharded true --num-shard 4 --quantize eetq\n> ```\n> \n> My guess is\
          \ that you are trying to use TheBloke's GPTQ model with eetq. It should\
          \ be run on the original model, and will perform the quantization on the\
          \ fly. Note that it's only an 8bit quantization, so if you require 4bit\
          \ for your VRAM requirements it will not work unfortunately.\n\n@nazrak-atlassian\
          \ you're right, I was trying to run Thebloke's version instead of the original\
          \ one, now works perfectly.\n\nThanks a lot. \n\n"
        updatedAt: '2023-12-19T15:54:35.021Z'
      numEdits: 0
      reactions: []
    id: 6581bcbb44ffabc5d2120675
    type: comment
  author: AbRds
  content: "> @AbRds Yes you should be able to change the --quantize arg. EETQ an\
    \ improved in-place 8bit quant technique (supposedly better performing than bitsandbytes'),\
    \ but I couldn't speak to it more than that. It worked for me with TGI 1.3.3 with\
    \ the following docker run:\n> \n> ```bash\n> docker run --gpus all --shm-size\
    \ 1g -p 8080:80 -v /opt/dlami/nvme/data:/data ghcr.io/huggingface/text-generation-inference:latest\
    \ --model-id mistralai/Mixtral-8x7B-v0.1 --sharded true --num-shard 4 --quantize\
    \ eetq\n> ```\n> \n> My guess is that you are trying to use TheBloke's GPTQ model\
    \ with eetq. It should be run on the original model, and will perform the quantization\
    \ on the fly. Note that it's only an 8bit quantization, so if you require 4bit\
    \ for your VRAM requirements it will not work unfortunately.\n\n@nazrak-atlassian\
    \ you're right, I was trying to run Thebloke's version instead of the original\
    \ one, now works perfectly.\n\nThanks a lot. \n\n"
  created_at: 2023-12-19 15:54:35+00:00
  edited: false
  hidden: false
  id: 6581bcbb44ffabc5d2120675
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Does not seem to work with TGI sharding
