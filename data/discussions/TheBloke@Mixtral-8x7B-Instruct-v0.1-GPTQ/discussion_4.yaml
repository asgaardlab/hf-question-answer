!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rjmehta
conflicting_files: null
created_at: 2023-12-15 18:30:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-15T18:30:34.000Z'
    data:
      edited: true
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3141072392463684
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>Getting this error when loading in exllamav2. </p>

          <p>!! Warning, unknown architecture: [''MixtralForCausalLM'']<br> !! Loading
          as LlamaForCausalLM</p>

          <hr>

          <p>ValueError                                Traceback (most recent call
          last)<br>Cell In[1], line 30<br>     28 config = ExLlamaV2Config()<br>     29
          config.model_dir = model_directory<br>---&gt; 30 config.prepare()<br>     31
          model = ExLlamaV2(config)<br>     32 #config.max_position_embeddings = 4096<br>     33
          #config.max_seq_len = 4096<br>     34 #model.max_position_embeddings = 4096<br>     35
          #model.max_seq_len = 4096<br>    154    break<br>    155    else:<br>--&gt;
          156         raise ValueError(f" ## Could not find {prefix}.* in model")<br>    158
          # Model dimensions<br>    160 self.head_dim = self.hidden_size // self.num_attention_heads</p>

          <p>ValueError:  ## Could not find model.layers.0.mlp.down_proj.* in model</p>

          '
        raw: "Getting this error when loading in exllamav2. \n\n!! Warning, unknown\
          \ architecture: ['MixtralForCausalLM']\n !! Loading as LlamaForCausalLM\n\
          ---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[1], line 30\n     28 config = ExLlamaV2Config()\n     29 config.model_dir\
          \ = model_directory\n---> 30 config.prepare()\n     31 model = ExLlamaV2(config)\n\
          \     32 #config.max_position_embeddings = 4096\n     33 #config.max_seq_len\
          \ = 4096\n     34 #model.max_position_embeddings = 4096\n     35 #model.max_seq_len\
          \ = 4096\n    154    break\n    155    else:\n--> 156         raise ValueError(f\"\
          \ ## Could not find {prefix}.* in model\")\n    158 # Model dimensions\n\
          \    160 self.head_dim = self.hidden_size // self.num_attention_heads\n\n\
          ValueError:  ## Could not find model.layers.0.mlp.down_proj.* in model"
        updatedAt: '2023-12-15T18:31:05.466Z'
      numEdits: 1
      reactions: []
    id: 657c9b4a19ca6a5e92358c09
    type: comment
  author: rjmehta
  content: "Getting this error when loading in exllamav2. \n\n!! Warning, unknown\
    \ architecture: ['MixtralForCausalLM']\n !! Loading as LlamaForCausalLM\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[1], line 30\n     28 config = ExLlamaV2Config()\n     29 config.model_dir\
    \ = model_directory\n---> 30 config.prepare()\n     31 model = ExLlamaV2(config)\n\
    \     32 #config.max_position_embeddings = 4096\n     33 #config.max_seq_len =\
    \ 4096\n     34 #model.max_position_embeddings = 4096\n     35 #model.max_seq_len\
    \ = 4096\n    154    break\n    155    else:\n--> 156         raise ValueError(f\"\
    \ ## Could not find {prefix}.* in model\")\n    158 # Model dimensions\n    160\
    \ self.head_dim = self.hidden_size // self.num_attention_heads\n\nValueError:\
    \  ## Could not find model.layers.0.mlp.down_proj.* in model"
  created_at: 2023-12-15 18:30:34+00:00
  edited: true
  hidden: false
  id: 657c9b4a19ca6a5e92358c09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-15T18:32:09.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9350274801254272
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>Okay. Turbo is working on mixtral architecture. Wrong thread to
          question. Thanks anyways.</p>

          '
        raw: Okay. Turbo is working on mixtral architecture. Wrong thread to question.
          Thanks anyways.
        updatedAt: '2023-12-15T18:32:09.037Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657c9ba9e37618d867541c2b
    id: 657c9ba9e37618d867541c29
    type: comment
  author: rjmehta
  content: Okay. Turbo is working on mixtral architecture. Wrong thread to question.
    Thanks anyways.
  created_at: 2023-12-15 18:32:09+00:00
  edited: false
  hidden: false
  id: 657c9ba9e37618d867541c29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-15T18:32:09.000Z'
    data:
      status: closed
    id: 657c9ba9e37618d867541c2b
    type: status-change
  author: rjmehta
  created_at: 2023-12-15 18:32:09+00:00
  id: 657c9ba9e37618d867541c2b
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-15T18:32:28.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8108168244361877
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/turboderp/exllamav2/issues/223">https://github.com/turboderp/exllamav2/issues/223</a></p>

          '
        raw: https://github.com/turboderp/exllamav2/issues/223
        updatedAt: '2023-12-15T18:32:28.614Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
    id: 657c9bbcdb7adc7f731448fc
    type: comment
  author: rjmehta
  content: https://github.com/turboderp/exllamav2/issues/223
  created_at: 2023-12-15 18:32:28+00:00
  edited: false
  hidden: false
  id: 657c9bbcdb7adc7f731448fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-16T09:34:03.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8844505548477173
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rjmehta&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rjmehta\">@<span class=\"\
          underline\">rjmehta</span></a></span>\n\n\t</span></span> You need to build\
          \ exllamav2 if you will with cuda12.1 locally on this branch <a rel=\"nofollow\"\
          \ href=\"https://github.com/turboderp/exllamav2/tree/experimental\">https://github.com/turboderp/exllamav2/tree/experimental</a></p>\n"
        raw: '@rjmehta You need to build exllamav2 if you will with cuda12.1 locally
          on this branch https://github.com/turboderp/exllamav2/tree/experimental'
        updatedAt: '2023-12-16T09:34:03.325Z'
      numEdits: 0
      reactions: []
    id: 657d6f0b416635415fe7be3c
    type: comment
  author: Yhyu13
  content: '@rjmehta You need to build exllamav2 if you will with cuda12.1 locally
    on this branch https://github.com/turboderp/exllamav2/tree/experimental'
  created_at: 2023-12-16 09:34:03+00:00
  edited: false
  hidden: false
  id: 657d6f0b416635415fe7be3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: When can the exllamav2 be supported?
