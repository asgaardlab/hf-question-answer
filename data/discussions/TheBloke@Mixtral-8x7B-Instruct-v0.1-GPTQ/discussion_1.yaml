!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dimaischenko
conflicting_files: null
created_at: 2023-12-12 15:24:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-12-12T15:24:15.000Z'
    data:
      edited: true
      editors:
      - dimaischenko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5279365181922913
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: "<p>Did anyone get it to run? My setup:</p>\n<pre><code>cuda 11.7, RTX3090\
          \ 24 Gb\n\ntorch==2.1.1+cu118\ntransformers==4.36.0\nauto-gptq==0.6.0.dev0+cu118\
          \  [from source:  https://github.com/LaaZa/AutoGPTQ/tree/Mixtral]\n</code></pre>\n\
          <p>Try to load:</p>\n<pre><code>from auto_gptq import AutoGPTQForCausalLM\n\
          \nmodel = AutoGPTQForCausalLM.from_quantized(\n                \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\
          ,\n                model_basename=\"model\",\n                revision=\"\
          gptq-3bit-128g-actorder_True\",\n                strict=False,  # Tried\
          \ with and without this parameter. The result is the same\n            \
          \    use_triton=False,\n                use_safetensors=True,\n        \
          \        trust_remote_code=False,\n                device=\"cuda:0\",\n\
          \                disable_exllama=True,\n                disable_exllamav2=True,\n\
          \                quantize_config=None)\n</code></pre>\n<p>Get error:</p>\n\
          <pre><code>File \"/root/venv/lib/python3.8/site-packages/accelerate/utils/modeling.py\"\
          , line 276, in set_module_tensor_to_device\n    raise ValueError(f\"{module}\
          \ does not have a parameter or a buffer named {tensor_name}.\")\nValueError:\
          \ QuantLinear() does not have a parameter or a buffer named weight.\n</code></pre>\n"
        raw: "Did anyone get it to run? My setup:\n\n```\ncuda 11.7, RTX3090 24 Gb\n\
          \ntorch==2.1.1+cu118\ntransformers==4.36.0\nauto-gptq==0.6.0.dev0+cu118\
          \  [from source:  https://github.com/LaaZa/AutoGPTQ/tree/Mixtral]\n```\n\
          \nTry to load:\n\n```\nfrom auto_gptq import AutoGPTQForCausalLM\n\nmodel\
          \ = AutoGPTQForCausalLM.from_quantized(\n                \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\
          ,\n                model_basename=\"model\",\n                revision=\"\
          gptq-3bit-128g-actorder_True\",\n                strict=False,  # Tried\
          \ with and without this parameter. The result is the same\n            \
          \    use_triton=False,\n                use_safetensors=True,\n        \
          \        trust_remote_code=False,\n                device=\"cuda:0\",\n\
          \                disable_exllama=True,\n                disable_exllamav2=True,\n\
          \                quantize_config=None)\n```\n\nGet error:\n```\nFile \"\
          /root/venv/lib/python3.8/site-packages/accelerate/utils/modeling.py\", line\
          \ 276, in set_module_tensor_to_device\n    raise ValueError(f\"{module}\
          \ does not have a parameter or a buffer named {tensor_name}.\")\nValueError:\
          \ QuantLinear() does not have a parameter or a buffer named weight.\n```"
        updatedAt: '2023-12-12T16:09:39.020Z'
      numEdits: 4
      reactions: []
    id: 65787b1f3ceeb2f07810de01
    type: comment
  author: dimaischenko
  content: "Did anyone get it to run? My setup:\n\n```\ncuda 11.7, RTX3090 24 Gb\n\
    \ntorch==2.1.1+cu118\ntransformers==4.36.0\nauto-gptq==0.6.0.dev0+cu118  [from\
    \ source:  https://github.com/LaaZa/AutoGPTQ/tree/Mixtral]\n```\n\nTry to load:\n\
    \n```\nfrom auto_gptq import AutoGPTQForCausalLM\n\nmodel = AutoGPTQForCausalLM.from_quantized(\n\
    \                \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n             \
    \   model_basename=\"model\",\n                revision=\"gptq-3bit-128g-actorder_True\"\
    ,\n                strict=False,  # Tried with and without this parameter. The\
    \ result is the same\n                use_triton=False,\n                use_safetensors=True,\n\
    \                trust_remote_code=False,\n                device=\"cuda:0\",\n\
    \                disable_exllama=True,\n                disable_exllamav2=True,\n\
    \                quantize_config=None)\n```\n\nGet error:\n```\nFile \"/root/venv/lib/python3.8/site-packages/accelerate/utils/modeling.py\"\
    , line 276, in set_module_tensor_to_device\n    raise ValueError(f\"{module} does\
    \ not have a parameter or a buffer named {tensor_name}.\")\nValueError: QuantLinear()\
    \ does not have a parameter or a buffer named weight.\n```"
  created_at: 2023-12-12 15:24:15+00:00
  edited: true
  hidden: false
  id: 65787b1f3ceeb2f07810de01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-12-12T16:38:39.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7708877921104431
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: '<p>Tried the same but with <code>CUDA 12.1</code> , <code>torch==2.1.1+cu121</code>
          and built <code>auto-gptq==0.6.0.dev0+cu121</code> from source. The same
          error.</p>

          '
        raw: Tried the same but with `CUDA 12.1` , `torch==2.1.1+cu121` and built
          `auto-gptq==0.6.0.dev0+cu121` from source. The same error.
        updatedAt: '2023-12-12T16:38:39.785Z'
      numEdits: 0
      reactions: []
    id: 65788c8f224758a1fc85b440
    type: comment
  author: dimaischenko
  content: Tried the same but with `CUDA 12.1` , `torch==2.1.1+cu121` and built `auto-gptq==0.6.0.dev0+cu121`
    from source. The same error.
  created_at: 2023-12-12 16:38:39+00:00
  edited: false
  hidden: false
  id: 65788c8f224758a1fc85b440
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-12T23:40:02.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798051118850708
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Unfortunately there was an issue with the branch I linked; I didn''t
          realise that the author had made another commit to it which broke inference
          again.  I''ve now updated the README to reference a different branch.</p>

          <p>The newly linked PR will now work: <a rel="nofollow" href="https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix">https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix</a></p>

          '
        raw: 'Unfortunately there was an issue with the branch I linked; I didn''t
          realise that the author had made another commit to it which broke inference
          again.  I''ve now updated the README to reference a different branch.


          The newly linked PR will now work: https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix'
        updatedAt: '2023-12-12T23:40:02.619Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - Igor7777
        - dimaischenko
    id: 6578ef520c22bb8984de796a
    type: comment
  author: TheBloke
  content: 'Unfortunately there was an issue with the branch I linked; I didn''t realise
    that the author had made another commit to it which broke inference again.  I''ve
    now updated the README to reference a different branch.


    The newly linked PR will now work: https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix'
  created_at: 2023-12-12 23:40:02+00:00
  edited: false
  hidden: false
  id: 6578ef520c22bb8984de796a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
      fullname: Thierry SALVOCH
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tsalvoch
      type: user
    createdAt: '2023-12-13T15:53:50.000Z'
    data:
      edited: false
      editors:
      - tsalvoch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2585071325302124
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c59edc8ff54b9520df32b816fdd5f5bf.svg
          fullname: Thierry SALVOCH
          isHf: false
          isPro: false
          name: tsalvoch
          type: user
        html: '<p>Build AutoGPT OK with CUDA 12.1, transformers 4.36.0 and torch==2.1.1+cu121
          = auto-gptq==0.6.0.dev0+cu121<br>But model loading failed in text-generation-webui:</p>

          <p>Traceback (most recent call last):<br>  File "/home/me/text-generation-webui/modules/ui_model_menu.py",
          line 208, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(selected_model, loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/me/text-generation-webui/modules/models.py", line 89, in load_model<br>    output
          = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/me/text-generation-webui/modules/models.py", line 380, in AutoGPTQ_loader<br>    return
          modules.AutoGPTQ_loader.load_quantized(model_name)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/me/text-generation-webui/modules/AutoGPTQ_loader.py", line 58, in
          load_quantized<br>    model = AutoGPTQForCausalLM.from_quantized(path_to_model,
          **params)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/auto.py",
          line 102, in from_quantized<br>    model_type = check_and_get_model_type(model_name_or_path,
          trust_remote_code)<br>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/_utils.py",
          line 232, in check_and_get_model_type<br>    raise TypeError(f"{config.model_type}
          isn''t supported yet.")<br>TypeError: mixtral isn''t supported yet.</p>

          <p>I probably missed something to have that:<br>mixtral isn''t supported
          yet</p>

          <p>But what?</p>

          '
        raw: "Build AutoGPT OK with CUDA 12.1, transformers 4.36.0 and torch==2.1.1+cu121\
          \ = auto-gptq==0.6.0.dev0+cu121\nBut model loading failed in text-generation-webui:\n\
          \n\nTraceback (most recent call last):\n  File \"/home/me/text-generation-webui/modules/ui_model_menu.py\"\
          , line 208, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(selected_model, loader)\n                                 \
          \    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/me/text-generation-webui/modules/models.py\"\
          , line 89, in load_model\n    output = load_func_map[loader](model_name)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/me/text-generation-webui/modules/models.py\"\
          , line 380, in AutoGPTQ_loader\n    return modules.AutoGPTQ_loader.load_quantized(model_name)\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"\
          /home/me/text-generation-webui/modules/AutoGPTQ_loader.py\", line 58, in\
          \ load_quantized\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/auto.py\"\
          , line 102, in from_quantized\n    model_type = check_and_get_model_type(model_name_or_path,\
          \ trust_remote_code)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/_utils.py\"\
          , line 232, in check_and_get_model_type\n    raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")\nTypeError: mixtral isn't supported yet.\n\n\n\
          I probably missed something to have that: \nmixtral isn't supported yet\n\
          \nBut what?"
        updatedAt: '2023-12-13T15:53:50.602Z'
      numEdits: 0
      reactions: []
    id: 6579d38e563044badca8fae5
    type: comment
  author: tsalvoch
  content: "Build AutoGPT OK with CUDA 12.1, transformers 4.36.0 and torch==2.1.1+cu121\
    \ = auto-gptq==0.6.0.dev0+cu121\nBut model loading failed in text-generation-webui:\n\
    \n\nTraceback (most recent call last):\n  File \"/home/me/text-generation-webui/modules/ui_model_menu.py\"\
    , line 208, in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/me/text-generation-webui/modules/models.py\", line 89, in load_model\n\
    \    output = load_func_map[loader](model_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/me/text-generation-webui/modules/models.py\", line 380, in AutoGPTQ_loader\n\
    \    return modules.AutoGPTQ_loader.load_quantized(model_name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/me/text-generation-webui/modules/AutoGPTQ_loader.py\", line 58,\
    \ in load_quantized\n    model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/auto.py\"\
    , line 102, in from_quantized\n    model_type = check_and_get_model_type(model_name_or_path,\
    \ trust_remote_code)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/me/miniconda3/envs/textgen/lib/python3.11/site-packages/auto_gptq/modeling/_utils.py\"\
    , line 232, in check_and_get_model_type\n    raise TypeError(f\"{config.model_type}\
    \ isn't supported yet.\")\nTypeError: mixtral isn't supported yet.\n\n\nI probably\
    \ missed something to have that: \nmixtral isn't supported yet\n\nBut what?"
  created_at: 2023-12-13 15:53:50+00:00
  edited: false
  hidden: false
  id: 6579d38e563044badca8fae5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-12-13T16:07:02.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8570960760116577
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;tsalvoch&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/tsalvoch\">@<span class=\"\
          underline\">tsalvoch</span></a></span>\n\n\t</span></span> most likely you\
          \ did not build <code>auto-gptq</code> from the <code>Mixtral-fix</code>\
          \ git branch. I had the same error when I built it from the <code>master</code>\
          \ branch</p>\n<p><a rel=\"nofollow\" href=\"https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix\"\
          >https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix</a></p>\n<p><code>git\
          \ checkout Mixtral-fix</code></p>\n"
        raw: '@tsalvoch most likely you did not build `auto-gptq` from the `Mixtral-fix`
          git branch. I had the same error when I built it from the `master` branch


          https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix


          `git checkout Mixtral-fix`'
        updatedAt: '2023-12-13T16:07:02.892Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - tsalvoch
    id: 6579d6a64191a2728df106f4
    type: comment
  author: dimaischenko
  content: '@tsalvoch most likely you did not build `auto-gptq` from the `Mixtral-fix`
    git branch. I had the same error when I built it from the `master` branch


    https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix


    `git checkout Mixtral-fix`'
  created_at: 2023-12-13 16:07:02+00:00
  edited: false
  hidden: false
  id: 6579d6a64191a2728df106f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-12-13T16:10:59.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733430743217468
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: "<blockquote>\n<p>Unfortunately there was an issue with the branch I\
          \ linked; I didn't realise that the author had made another commit to it\
          \ which broke inference again.  I've now updated the README to reference\
          \ a different branch.</p>\n<p>The newly linked PR will now work: <a rel=\"\
          nofollow\" href=\"https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix\">https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix</a></p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ Thank you!</p>\n"
        raw: "> Unfortunately there was an issue with the branch I linked; I didn't\
          \ realise that the author had made another commit to it which broke inference\
          \ again.  I've now updated the README to reference a different branch.\n\
          > \n> The newly linked PR will now work: https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix\n\
          \n@TheBloke Thank you!"
        updatedAt: '2023-12-13T16:10:59.149Z'
      numEdits: 0
      reactions: []
    id: 6579d793c0c04c7b043427b7
    type: comment
  author: dimaischenko
  content: "> Unfortunately there was an issue with the branch I linked; I didn't\
    \ realise that the author had made another commit to it which broke inference\
    \ again.  I've now updated the README to reference a different branch.\n> \n>\
    \ The newly linked PR will now work: https://github.com/LaaZa/AutoGPTQ/tree/Mixtral-fix\n\
    \n@TheBloke Thank you!"
  created_at: 2023-12-13 16:10:59+00:00
  edited: false
  hidden: false
  id: 6579d793c0c04c7b043427b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
      fullname: Bruce D'Ambrosio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bdambrosio
      type: user
    createdAt: '2023-12-13T17:16:11.000Z'
    data:
      edited: false
      editors:
      - bdambrosio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9282547235488892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
          fullname: Bruce D'Ambrosio
          isHf: false
          isPro: false
          name: bdambrosio
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dimaischenko&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dimaischenko\"\
          >@<span class=\"underline\">dimaischenko</span></a></span>\n\n\t</span></span>\
          \ - How did you get this to run on a 3090? with Mixtral-fix it does try\
          \ to load, but runs out of memory on my 4090.<br>I do have 2x4090, guess\
          \ I'll look through the code base to see if/how to specify multiple gpu.</p>\n"
        raw: '@dimaischenko - How did you get this to run on a 3090? with Mixtral-fix
          it does try to load, but runs out of memory on my 4090.

          I do have 2x4090, guess I''ll look through the code base to see if/how to
          specify multiple gpu.


          '
        updatedAt: '2023-12-13T17:16:11.426Z'
      numEdits: 0
      reactions: []
    id: 6579e6db21a8b318365bee4e
    type: comment
  author: bdambrosio
  content: '@dimaischenko - How did you get this to run on a 3090? with Mixtral-fix
    it does try to load, but runs out of memory on my 4090.

    I do have 2x4090, guess I''ll look through the code base to see if/how to specify
    multiple gpu.


    '
  created_at: 2023-12-13 17:16:11+00:00
  edited: false
  hidden: false
  id: 6579e6db21a8b318365bee4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
      fullname: Dmytro Ishchenko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dimaischenko
      type: user
    createdAt: '2023-12-13T17:24:50.000Z'
    data:
      edited: false
      editors:
      - dimaischenko
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.812066912651062
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6304c78e9aef62c4013e91ae/DccjU9447vyJFHvc_5IHw.jpeg?w=200&h=200&f=face
          fullname: Dmytro Ishchenko
          isHf: false
          isPro: false
          name: dimaischenko
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bdambrosio&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bdambrosio\">@<span class=\"\
          underline\">bdambrosio</span></a></span>\n\n\t</span></span> I am ok with\
          \ 3090. Even for <code>revision=\"main\"</code>, but you can try <code>revision=\"\
          gptq-3bit-128g-actorder_True\"</code> it takes about 19 Gb (example in my\
          \ first thread post)</p>\n"
        raw: '@bdambrosio I am ok with 3090. Even for `revision="main"`, but you can
          try `revision="gptq-3bit-128g-actorder_True"` it takes about 19 Gb (example
          in my first thread post)'
        updatedAt: '2023-12-13T17:24:50.166Z'
      numEdits: 0
      reactions: []
    id: 6579e8e234939bae8af14acc
    type: comment
  author: dimaischenko
  content: '@bdambrosio I am ok with 3090. Even for `revision="main"`, but you can
    try `revision="gptq-3bit-128g-actorder_True"` it takes about 19 Gb (example in
    my first thread post)'
  created_at: 2023-12-13 17:24:50+00:00
  edited: false
  hidden: false
  id: 6579e8e234939bae8af14acc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
      fullname: Bruce D'Ambrosio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bdambrosio
      type: user
    createdAt: '2023-12-13T17:27:37.000Z'
    data:
      edited: false
      editors:
      - bdambrosio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9491677284240723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
          fullname: Bruce D'Ambrosio
          isHf: false
          isPro: false
          name: bdambrosio
          type: user
        html: '<p>Ah, yup, just realized my error. I had loaded a larger version assuming
          I would use both gpus. Downloading smaller version now, while also trying
          to figure out syntax of AutoGPTQ .from_pretrained device parameter.</p>

          <p>tnx!</p>

          '
        raw: 'Ah, yup, just realized my error. I had loaded a larger version assuming
          I would use both gpus. Downloading smaller version now, while also trying
          to figure out syntax of AutoGPTQ .from_pretrained device parameter.


          tnx!'
        updatedAt: '2023-12-13T17:27:37.979Z'
      numEdits: 0
      reactions: []
    id: 6579e98907b1fc747dfde222
    type: comment
  author: bdambrosio
  content: 'Ah, yup, just realized my error. I had loaded a larger version assuming
    I would use both gpus. Downloading smaller version now, while also trying to figure
    out syntax of AutoGPTQ .from_pretrained device parameter.


    tnx!'
  created_at: 2023-12-13 17:27:37+00:00
  edited: false
  hidden: false
  id: 6579e98907b1fc747dfde222
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
      fullname: Bruce D'Ambrosio
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bdambrosio
      type: user
    createdAt: '2023-12-13T17:34:23.000Z'
    data:
      edited: true
      editors:
      - bdambrosio
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38365766406059265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5cbe88512c48efa55279c262e84c396.svg
          fullname: Bruce D'Ambrosio
          isHf: false
          isPro: false
          name: bdambrosio
          type: user
        html: "<p>Ah - In case anyone else stumbles here - <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ - any ideas?</p>\n<p>gptq-4bit-128g-actorder_True\t4:</p>\n<p>model =\
          \ AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>           \
          \                                model_basename=\"model\",<br>         \
          \                                  use_safetensors=True,<br>           \
          \                                per_gpu_max_memory={0:\"20GIB\",1:\"20GIB\"\
          },<br>                                           )</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True, trust_remote_code=False)</p>\n<p>prompt = \"Tell me about\
          \ AI\"<br>prompt_template=fquotequotequote<s>[INST] {prompt} [/INST]<br>print(\"\
          \\n\\n*** Generate:\")</s></p><s>\n<p>input_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()<br>output = model.generate(inputs=input_ids,\
          \ temperature=0.1, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>\n\
          <p>(mistral) bruce@bruce-AI:~/Downloads/alphawave/tests/Sam$ python mixtral-8x-GPTQ.py<br>MixtralGPTQForCausalLM\
          \ hasn't fused attention module yet, will skip inject fused attention.<br>MixtralGPTQForCausalLM\
          \ hasn't fused mlp module yet, will skip inject fused mlp.</p>\n<p>*** Generate:<br>Traceback\
          \ (most recent call last):<br>  File \"/home/bruce/Downloads/alphawave/tests/Sam/mixtral-8x-GPTQ.py\"\
          , line 31, in <br>    output = model.generate(inputs=input_ids, temperature=0.7,\
          \ do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)<br>  File \"\
          /home/bruce/miniconda3/envs/mistral/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 447, in generate<br>    return self.model.generate(**kwargs)<br>\
          \  File \"/home/bruce/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1764, in generate<br>    return self.sample(<br>  File \"/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2897, in sample<br>    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)<br>RuntimeError:\
          \ probability tensor contains either <code>inf</code>, <code>nan</code>\
          \ or element &lt; 0</p>\n</s>"
        raw: "Ah - In case anyone else stumbles here - @TheBloke - any ideas?\n  \
          \                                \ngptq-4bit-128g-actorder_True\t4:\n\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n       \
          \                                    model_basename=\"model\",\n       \
          \                                    use_safetensors=True,\n           \
          \                                per_gpu_max_memory={0:\"20GIB\",1:\"20GIB\"\
          },\n                                           )\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True, trust_remote_code=False)\n\nprompt = \"Tell me about AI\"\
          \nprompt_template=fquotequotequote<s>[INST] {prompt} [/INST]\nprint(\"\\\
          n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.1, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          \n\n(mistral) bruce@bruce-AI:~/Downloads/alphawave/tests/Sam$ python mixtral-8x-GPTQ.py\
          \ \nMixtralGPTQForCausalLM hasn't fused attention module yet, will skip\
          \ inject fused attention.         \nMixtralGPTQForCausalLM hasn't fused\
          \ mlp module yet, will skip inject fused mlp.\n\n\n*** Generate:\nTraceback\
          \ (most recent call last):\n  File \"/home/bruce/Downloads/alphawave/tests/Sam/mixtral-8x-GPTQ.py\"\
          , line 31, in <module>\n    output = model.generate(inputs=input_ids, temperature=0.7,\
          \ do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n  File \"/home/bruce/miniconda3/envs/mistral/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 447, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"/home/bruce/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1764, in generate\n    return self.sample(\n  File \"/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2897, in sample\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\
          RuntimeError: probability tensor contains either `inf`, `nan` or element\
          \ < 0\n"
        updatedAt: '2023-12-13T18:06:15.653Z'
      numEdits: 4
      reactions: []
    id: 6579eb1f479c85a20f7c6f70
    type: comment
  author: bdambrosio
  content: "Ah - In case anyone else stumbles here - @TheBloke - any ideas?\n    \
    \                              \ngptq-4bit-128g-actorder_True\t4:\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \                                           model_basename=\"model\",\n      \
    \                                     use_safetensors=True,\n                \
    \                           per_gpu_max_memory={0:\"20GIB\",1:\"20GIB\"},\n  \
    \                                         )\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True, trust_remote_code=False)\n\nprompt = \"Tell me about AI\"\nprompt_template=fquotequotequote<s>[INST]\
    \ {prompt} [/INST]\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.1, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n\n(mistral) bruce@bruce-AI:~/Downloads/alphawave/tests/Sam$\
    \ python mixtral-8x-GPTQ.py \nMixtralGPTQForCausalLM hasn't fused attention module\
    \ yet, will skip inject fused attention.         \nMixtralGPTQForCausalLM hasn't\
    \ fused mlp module yet, will skip inject fused mlp.\n\n\n*** Generate:\nTraceback\
    \ (most recent call last):\n  File \"/home/bruce/Downloads/alphawave/tests/Sam/mixtral-8x-GPTQ.py\"\
    , line 31, in <module>\n    output = model.generate(inputs=input_ids, temperature=0.7,\
    \ do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n  File \"/home/bruce/miniconda3/envs/mistral/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 447, in generate\n    return self.model.generate(**kwargs)\n  File \"/home/bruce/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1764, in generate\n    return self.sample(\n  File \"/home/bruce/.local/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2897, in sample\n    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\
    RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\n"
  created_at: 2023-12-13 17:34:23+00:00
  edited: true
  hidden: false
  id: 6579eb1f479c85a20f7c6f70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-12-16T18:00:13.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-16T18:30:40.783Z'
      numEdits: 0
      reactions: []
    id: 657de5ad17f67d5b87b7950e
    type: comment
  author: robert1968
  content: This comment has been hidden
  created_at: 2023-12-16 18:00:13+00:00
  edited: true
  hidden: true
  id: 657de5ad17f67d5b87b7950e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
      fullname: Robert Szabo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: robert1968
      type: user
    createdAt: '2023-12-16T18:06:42.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/401db8d5d4c552c4d53d0992675d28e6.svg
          fullname: Robert Szabo
          isHf: false
          isPro: false
          name: robert1968
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-12-16T18:30:32.632Z'
      numEdits: 1
      reactions: []
    id: 657de732ce825cd97499bb4f
    type: comment
  author: robert1968
  content: This comment has been hidden
  created_at: 2023-12-16 18:06:42+00:00
  edited: true
  hidden: true
  id: 657de732ce825cd97499bb4f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
repo_type: model
status: open
target_branch: null
title: Did anyone get it to run?
