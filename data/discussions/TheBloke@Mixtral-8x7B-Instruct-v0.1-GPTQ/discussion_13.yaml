!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SpaceCowboy850
conflicting_files: null
created_at: 2024-01-23 22:55:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a7fbd6660b42718b0de6e833c981a11.svg
      fullname: Brandon Rowlett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpaceCowboy850
      type: user
    createdAt: '2024-01-23T22:55:49.000Z'
    data:
      edited: true
      editors:
      - SpaceCowboy850
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5824106335639954
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a7fbd6660b42718b0de6e833c981a11.svg
          fullname: Brandon Rowlett
          isHf: false
          isPro: false
          name: SpaceCowboy850
          type: user
        html: "<p>I haven't \"built from source\" as instructed, but the official\
          \ releases of both Transformers and Auto_GPTQ are now at/beyond the dev\
          \ branch suggested.  Is it reasonable to assume that the latest versions\
          \ have what is needed to run this?</p>\n<p>I'm running this:  GPTQ_3bit_128g_actorderTrue</p>\n\
          <p>On a 4090, and ultimately get this</p>\n<pre><code>return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\\
          transformers\\models\\mixtral\\modeling_mixtral.py\", line 802, in forward\n\
          \    router_logits = self.gate(hidden_states)\n  File \"C:\\Anaconda3\\\
          envs\\huggingface\\lib\\site-packages\\torch\\nn\\modules\\module.py\",\
          \ line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\\
          auto_gptq\\nn_modules\\qlinear\\qlinear_cuda.py\", line 245, in forward\n\
          \    zeros = zeros.reshape(self.scales.shape)\nRuntimeError: shape '[32,\
          \ 8]' is invalid for input of size 0\n</code></pre>\n<p>Making sure there\
          \ are no conflicts, here are the versions from a python command line:</p>\n\
          <pre><code>&gt;&gt;&gt; import transformers\n&gt;&gt;&gt; transformers.__version__\n\
          '4.37.0'\n&gt;&gt;&gt; import auto_gptq\n&gt;&gt;&gt; auto_gptq.__version__\n\
          '0.6.0'\n</code></pre>\n"
        raw: "I haven't \"built from source\" as instructed, but the official releases\
          \ of both Transformers and Auto_GPTQ are now at/beyond the dev branch suggested.\
          \  Is it reasonable to assume that the latest versions have what is needed\
          \ to run this?\n\nI'm running this:  GPTQ_3bit_128g_actorderTrue\n\nOn a\
          \ 4090, and ultimately get this\n\n```\nreturn forward_call(*args, **kwargs)\n\
          \  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\transformers\\\
          models\\mixtral\\modeling_mixtral.py\", line 802, in forward\n    router_logits\
          \ = self.gate(hidden_states)\n  File \"C:\\Anaconda3\\envs\\huggingface\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n\
          \    return self._call_impl(*args, **kwargs)\n  File \"C:\\Anaconda3\\envs\\\
          huggingface\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527,\
          \ in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\\
          Anaconda3\\envs\\huggingface\\lib\\site-packages\\auto_gptq\\nn_modules\\\
          qlinear\\qlinear_cuda.py\", line 245, in forward\n    zeros = zeros.reshape(self.scales.shape)\n\
          RuntimeError: shape '[32, 8]' is invalid for input of size 0\n```\n\nMaking\
          \ sure there are no conflicts, here are the versions from a python command\
          \ line:\n```\n>>> import transformers\n>>> transformers.__version__\n'4.37.0'\n\
          >>> import auto_gptq\n>>> auto_gptq.__version__\n'0.6.0'\n```"
        updatedAt: '2024-01-23T22:56:15.154Z'
      numEdits: 1
      reactions: []
    id: 65b043f57febbcc2afbd4c51
    type: comment
  author: SpaceCowboy850
  content: "I haven't \"built from source\" as instructed, but the official releases\
    \ of both Transformers and Auto_GPTQ are now at/beyond the dev branch suggested.\
    \  Is it reasonable to assume that the latest versions have what is needed to\
    \ run this?\n\nI'm running this:  GPTQ_3bit_128g_actorderTrue\n\nOn a 4090, and\
    \ ultimately get this\n\n```\nreturn forward_call(*args, **kwargs)\n  File \"\
    C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\transformers\\models\\mixtral\\\
    modeling_mixtral.py\", line 802, in forward\n    router_logits = self.gate(hidden_states)\n\
    \  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args,\
    \ **kwargs)\n  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1527, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\Anaconda3\\envs\\huggingface\\lib\\site-packages\\auto_gptq\\\
    nn_modules\\qlinear\\qlinear_cuda.py\", line 245, in forward\n    zeros = zeros.reshape(self.scales.shape)\n\
    RuntimeError: shape '[32, 8]' is invalid for input of size 0\n```\n\nMaking sure\
    \ there are no conflicts, here are the versions from a python command line:\n\
    ```\n>>> import transformers\n>>> transformers.__version__\n'4.37.0'\n>>> import\
    \ auto_gptq\n>>> auto_gptq.__version__\n'0.6.0'\n```"
  created_at: 2024-01-23 22:55:49+00:00
  edited: true
  hidden: false
  id: 65b043f57febbcc2afbd4c51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a7fbd6660b42718b0de6e833c981a11.svg
      fullname: Brandon Rowlett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpaceCowboy850
      type: user
    createdAt: '2024-01-25T15:55:53.000Z'
    data:
      edited: false
      editors:
      - SpaceCowboy850
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9603980183601379
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a7fbd6660b42718b0de6e833c981a11.svg
          fullname: Brandon Rowlett
          isHf: false
          isPro: false
          name: SpaceCowboy850
          type: user
        html: '<p>Posting this here as a link for anyone that finds it.  I still haven''t
          solved my problem, but the primary discussion seems to be in this thread<br><a
          href="https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/discussions/5">https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/discussions/5</a></p>

          '
        raw: 'Posting this here as a link for anyone that finds it.  I still haven''t
          solved my problem, but the primary discussion seems to be in this thread

          https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/discussions/5


          '
        updatedAt: '2024-01-25T15:55:53.902Z'
      numEdits: 0
      reactions: []
    id: 65b284894f699ea319c4ecf8
    type: comment
  author: SpaceCowboy850
  content: 'Posting this here as a link for anyone that finds it.  I still haven''t
    solved my problem, but the primary discussion seems to be in this thread

    https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ/discussions/5


    '
  created_at: 2024-01-25 15:55:53+00:00
  edited: false
  hidden: false
  id: 65b284894f699ea319c4ecf8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3a7fbd6660b42718b0de6e833c981a11.svg
      fullname: Brandon Rowlett
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SpaceCowboy850
      type: user
    createdAt: '2024-01-25T15:55:59.000Z'
    data:
      status: closed
    id: 65b2848fc8a577067df4def7
    type: status-change
  author: SpaceCowboy850
  created_at: 2024-01-25 15:55:59+00:00
  id: 65b2848fc8a577067df4def7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: shape ''[32, 8]'' is invalid for input of size 0'
