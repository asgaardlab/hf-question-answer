!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spirilis
conflicting_files: null
created_at: 2023-05-03 12:44:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54d4fe9374648d8b54e5626ec4ecb62e.svg
      fullname: Eric Brundick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spirilis
      type: user
    createdAt: '2023-05-03T13:44:23.000Z'
    data:
      edited: false
      editors:
      - spirilis
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54d4fe9374648d8b54e5626ec4ecb62e.svg
          fullname: Eric Brundick
          isHf: false
          isPro: false
          name: spirilis
          type: user
        html: '<p>Fwiw, I used the recommend prompting and it didn''t work:</p>

          <blockquote>

          <p> &lt;|prompter|&gt;Show me an example of a resume for a candidate for
          tech management, a small team of up to 5 people. &lt;|assistant|&gt;: I
          don''t know what you mean by "prompter." Are you referring to the assistant?
          [end of text]</p>

          <p>llama_print_timings:        load time = 318112.27 ms<br>llama_print_timings:      sample
          time =    32.39 ms /    22 runs   (    1.47 ms per run)<br>llama_print_timings:
          prompt eval time = 315783.52 ms /    40 tokens ( 7894.59 ms per token)<br>llama_print_timings:        eval
          time = 189354.92 ms /    21 runs   ( 9016.90 ms per run)<br>llama_print_timings:       total
          time = 507514.39 ms</p>

          </blockquote>

          '
        raw: "Fwiw, I used the recommend prompting and it didn't work:\r\n\r\n>  <|prompter|>Show\
          \ me an example of a resume for a candidate for tech management, a small\
          \ team of up to 5 people. <|assistant|>: I don't know what you mean by \"\
          prompter.\" Are you referring to the assistant? [end of text]\r\n> \r\n\
          > llama_print_timings:        load time = 318112.27 ms\r\n> llama_print_timings:\
          \      sample time =    32.39 ms /    22 runs   (    1.47 ms per run)\r\n\
          > llama_print_timings: prompt eval time = 315783.52 ms /    40 tokens (\
          \ 7894.59 ms per token)\r\n> llama_print_timings:        eval time = 189354.92\
          \ ms /    21 runs   ( 9016.90 ms per run)\r\n> llama_print_timings:    \
          \   total time = 507514.39 ms\r\n"
        updatedAt: '2023-05-03T13:44:23.333Z'
      numEdits: 0
      reactions: []
    id: 64526537515bd8f74176e819
    type: comment
  author: spirilis
  content: "Fwiw, I used the recommend prompting and it didn't work:\r\n\r\n>  <|prompter|>Show\
    \ me an example of a resume for a candidate for tech management, a small team\
    \ of up to 5 people. <|assistant|>: I don't know what you mean by \"prompter.\"\
    \ Are you referring to the assistant? [end of text]\r\n> \r\n> llama_print_timings:\
    \        load time = 318112.27 ms\r\n> llama_print_timings:      sample time =\
    \    32.39 ms /    22 runs   (    1.47 ms per run)\r\n> llama_print_timings: prompt\
    \ eval time = 315783.52 ms /    40 tokens ( 7894.59 ms per token)\r\n> llama_print_timings:\
    \        eval time = 189354.92 ms /    21 runs   ( 9016.90 ms per run)\r\n> llama_print_timings:\
    \       total time = 507514.39 ms\r\n"
  created_at: 2023-05-03 12:44:23+00:00
  edited: false
  hidden: false
  id: 64526537515bd8f74176e819
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-03T20:55:34.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<p>I noticed that these <code>&lt;|prompter|&gt;</code> and <code>&lt;|assistant|&gt;</code>
          are not single tokens as they were supposed to be. Maybe it has something
          to do with it.</p>

          '
        raw: I noticed that these `<|prompter|>` and `<|assistant|>` are not single
          tokens as they were supposed to be. Maybe it has something to do with it.
        updatedAt: '2023-05-03T20:55:34.186Z'
      numEdits: 0
      reactions: []
    id: 6452ca46a0c0a664a24a6228
    type: comment
  author: kurnevsky
  content: I noticed that these `<|prompter|>` and `<|assistant|>` are not single
    tokens as they were supposed to be. Maybe it has something to do with it.
  created_at: 2023-05-03 19:55:34+00:00
  edited: false
  hidden: false
  id: 6452ca46a0c0a664a24a6228
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-04T21:03:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Can you show your full command and output?  It seems to work fine\
          \ for me</p>\n<pre><code>tomj@Eddie ~/src/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML\
          \ $ llama -t 10 -m /Volumes/EVOB/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML/OpenAssistant-30B-epoch7.ggml.q4_2.bin\
          \ --color -c 2048 --temp 0.9 --repeat_penalty 1.1 -n -1 -p \"&lt;|prompter|&gt;Write\
          \ a short story about llamas &lt;|assistant|&gt;\"\nmain: seed = 1683233888\n\
          llama.cpp: loading model from /Volumes/EVOB/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML/OpenAssistant-30B-epoch7.ggml.q4_2.bin\n\
          llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32016\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 5 (mostly Q4_2)\n\
          llama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size = 110.30 KB\nllama_model_load_internal: mem required  =\
          \ 21695.59 MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size\
          \  = 3120.00 MB\n\nsystem_info: n_threads = 10 / 36 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000,\
          \ presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40,\
          \ tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.900000,\
          \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate:\
          \ n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n &lt;|prompter|&gt;Write\
          \ a short story about llamas &lt;|assistant|&gt;Okay! Here's your Llama\
          \ Story...\nThe Great llama Race. There once was a great race in the town\
          \ of llamas. In which the greatest llama racers came to compete for the\
          \ great championship and the grand prize of 10 carrots. However, when the\
          \ day of the race arrived a great storm came also and soaked the race track\
          \ making it impossible to run. All the llamas were sad and felt that they\
          \ would not be able to win the prize, but then a clever llama thought of\
          \ a solution. \"Why don't we all just walk instead?\" And thus the race\
          \ became known as The Great Llama Walk, and every llama won 10 carrots.\n\
          </code></pre>\n"
        raw: "Can you show your full command and output?  It seems to work fine for\
          \ me\n```\ntomj@Eddie ~/src/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML\
          \ $ llama -t 10 -m /Volumes/EVOB/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML/OpenAssistant-30B-epoch7.ggml.q4_2.bin\
          \ --color -c 2048 --temp 0.9 --repeat_penalty 1.1 -n -1 -p \"<|prompter|>Write\
          \ a short story about llamas <|assistant|>\"\nmain: seed = 1683233888\n\
          llama.cpp: loading model from /Volumes/EVOB/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML/OpenAssistant-30B-epoch7.ggml.q4_2.bin\n\
          llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
          \ n_vocab    = 32016\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 5 (mostly Q4_2)\n\
          llama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size = 110.30 KB\nllama_model_load_internal: mem required  =\
          \ 21695.59 MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size\
          \  = 3120.00 MB\n\nsystem_info: n_threads = 10 / 36 | AVX = 1 | AVX2 = 1\
          \ | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3\
          \ = 1 | VSX = 0 |\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000,\
          \ presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40,\
          \ tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.900000,\
          \ mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate:\
          \ n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n <|prompter|>Write\
          \ a short story about llamas <|assistant|>Okay! Here's your Llama Story...\n\
          The Great llama Race. There once was a great race in the town of llamas.\
          \ In which the greatest llama racers came to compete for the great championship\
          \ and the grand prize of 10 carrots. However, when the day of the race arrived\
          \ a great storm came also and soaked the race track making it impossible\
          \ to run. All the llamas were sad and felt that they would not be able to\
          \ win the prize, but then a clever llama thought of a solution. \"Why don't\
          \ we all just walk instead?\" And thus the race became known as The Great\
          \ Llama Walk, and every llama won 10 carrots.\n```"
        updatedAt: '2023-05-04T21:03:51.681Z'
      numEdits: 0
      reactions: []
    id: 64541db76b61332c0e4554b6
    type: comment
  author: TheBloke
  content: "Can you show your full command and output?  It seems to work fine for\
    \ me\n```\ntomj@Eddie ~/src/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML\
    \ $ llama -t 10 -m /Volumes/EVOB/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML/OpenAssistant-30B-epoch7.ggml.q4_2.bin\
    \ --color -c 2048 --temp 0.9 --repeat_penalty 1.1 -n -1 -p \"<|prompter|>Write\
    \ a short story about llamas <|assistant|>\"\nmain: seed = 1683233888\nllama.cpp:\
    \ loading model from /Volumes/EVOB/huggingface/TheBloke_OpenAssistant-SFT-7-Llama-30B-GGML/OpenAssistant-30B-epoch7.ggml.q4_2.bin\n\
    llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32016\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: ftype      = 5 (mostly Q4_2)\n\
    llama_model_load_internal: n_ff       = 17920\nllama_model_load_internal: n_parts\
    \    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
    \ ggml ctx size = 110.30 KB\nllama_model_load_internal: mem required  = 21695.59\
    \ MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size  = 3120.00 MB\n\
    \nsystem_info: n_threads = 10 / 36 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI\
    \ = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA\
    \ = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 |\nsampling: repeat_last_n\
    \ = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty\
    \ = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000,\
    \ temp = 0.900000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n\
    generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 0\n\n\n <|prompter|>Write\
    \ a short story about llamas <|assistant|>Okay! Here's your Llama Story...\nThe\
    \ Great llama Race. There once was a great race in the town of llamas. In which\
    \ the greatest llama racers came to compete for the great championship and the\
    \ grand prize of 10 carrots. However, when the day of the race arrived a great\
    \ storm came also and soaked the race track making it impossible to run. All the\
    \ llamas were sad and felt that they would not be able to win the prize, but then\
    \ a clever llama thought of a solution. \"Why don't we all just walk instead?\"\
    \ And thus the race became known as The Great Llama Walk, and every llama won\
    \ 10 carrots.\n```"
  created_at: 2023-05-04 20:03:51+00:00
  edited: false
  hidden: false
  id: 64541db76b61332c0e4554b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T13:21:28.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: "<p>I haven't seen the exact same result as <span data-props=\"{&quot;user&quot;:&quot;spirilis&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/spirilis\"\
          >@<span class=\"underline\">spirilis</span></a></span>\n\n\t</span></span>\
          \ but I have a feeling that this model doesn't handle <code>&lt;|assistant|&gt;</code>\
          \ and <code>&lt;|prompter|&gt;</code> correctly. Firstly as I said before\
          \ it tokenizes these parts with several tokens instead of one as it was\
          \ supposed in original model. And second, it doesn't end the generation\
          \ with <code>&lt;|endoftext|&gt;</code> or <code>&lt;|prompter|&gt;</code>\
          \ way too often. Instead it just continues and might insert other things\
          \ like <code>&lt;|user|&gt;</code> or similar.</p>\n<p>Here how I run it:</p>\n\
          <pre><code class=\"language-sh\">./main --interactive -t 32 -m OpenAssistant-30B-epoch7.ggml.q5_1.bin\
          \ --color -c 2048 --temp 0.7 -p <span class=\"hljs-string\">\"&lt;|prompter|&gt;Some\
          \ question here.&lt;|endoftext|&gt;&lt;|assistant|&gt;\"</span>\n</code></pre>\n"
        raw: 'I haven''t seen the exact same result as @spirilis but I have a feeling
          that this model doesn''t handle `<|assistant|>` and `<|prompter|>` correctly.
          Firstly as I said before it tokenizes these parts with several tokens instead
          of one as it was supposed in original model. And second, it doesn''t end
          the generation with `<|endoftext|>` or `<|prompter|>` way too often. Instead
          it just continues and might insert other things like `<|user|>` or similar.


          Here how I run it:

          ```sh

          ./main --interactive -t 32 -m OpenAssistant-30B-epoch7.ggml.q5_1.bin --color
          -c 2048 --temp 0.7 -p "<|prompter|>Some question here.<|endoftext|><|assistant|>"

          ```'
        updatedAt: '2023-05-05T13:21:28.285Z'
      numEdits: 0
      reactions: []
    id: 645502d8f61f10d69dc68ce3
    type: comment
  author: kurnevsky
  content: 'I haven''t seen the exact same result as @spirilis but I have a feeling
    that this model doesn''t handle `<|assistant|>` and `<|prompter|>` correctly.
    Firstly as I said before it tokenizes these parts with several tokens instead
    of one as it was supposed in original model. And second, it doesn''t end the generation
    with `<|endoftext|>` or `<|prompter|>` way too often. Instead it just continues
    and might insert other things like `<|user|>` or similar.


    Here how I run it:

    ```sh

    ./main --interactive -t 32 -m OpenAssistant-30B-epoch7.ggml.q5_1.bin --color -c
    2048 --temp 0.7 -p "<|prompter|>Some question here.<|endoftext|><|assistant|>"

    ```'
  created_at: 2023-05-05 12:21:28+00:00
  edited: false
  hidden: false
  id: 645502d8f61f10d69dc68ce3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T13:33:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>In which case it's possible something has gone awry in the conversion\
          \ to GGML.  There was an issue when I tried to convert to GGML.  </p>\n\
          <p>To do HF -&gt; GGML conversion I use <code>convert.py</code> in the llama.cpp\
          \ repo. This has a check on the number of vocab entries.  Trying to run\
          \ the conversion on this model initially failed. The script threw an error\
          \ reporting that model is meant to have 32016 tokens according to its <code>vocab_size</code>,\
          \  but the provided <code>added_tokens.json</code> only lists five extra\
          \ tokens:</p>\n<pre><code class=\"language-json\"><span class=\"hljs-punctuation\"\
          >{</span>\n  <span class=\"hljs-attr\">\"&lt;|assistant|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32004</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|prefix_begin|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32000</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|prefix_end|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32003</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|prompter|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32002</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|system|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32001</span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n\
          <p>The only way I found around this was to edit added_tokens.json like so:</p>\n\
          <pre><code class=\"language-json\"><span class=\"hljs-punctuation\">{</span>\n\
          \ <span class=\"hljs-attr\">\"&lt;|dummy3|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32007</span><span class=\"hljs-punctuation\"\
          >,</span>\n <span class=\"hljs-attr\">\"&lt;|dummy2|&gt;\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-number\">32006</span><span\
          \ class=\"hljs-punctuation\">,</span>\n <span class=\"hljs-attr\">\"&lt;|dummy1|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32005</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|assistant|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32004</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|prefix_begin|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32000</span><span\
          \ class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"&lt;|prefix_end|&gt;\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\"\
          >32003</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"\
          hljs-attr\">\"&lt;|prompter|&gt;\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-number\">32002</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"&lt;|system|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">32001</span>\n\
          <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<p>etc, up to\
          \ 32015.</p>\n<p>Quite possibly this was wrong! But without knowing what\
          \ the other 11 tokens were meant to be, it's all I could think to do, and\
          \ it's what the GGML repo for OpenAssistant epoch 6 did as well.</p>\n<p>Now\
          \ you mention it, <code>&lt;|endoftext|&gt;</code> should likely be in that\
          \ list. But what token ID? And why wasn't it in the provided <code>added_tokens.json</code>\
          \ already?</p>\n<p>If you know how to find the answers to any of these questions\
          \ I am happy to do the GGML conversion again with a new added_tokens.json,\
          \ if we can figure out what it should actually contain.</p>\n"
        raw: "In which case it's possible something has gone awry in the conversion\
          \ to GGML.  There was an issue when I tried to convert to GGML.  \n\nTo\
          \ do HF -> GGML conversion I use `convert.py` in the llama.cpp repo. This\
          \ has a check on the number of vocab entries.  Trying to run the conversion\
          \ on this model initially failed. The script threw an error reporting that\
          \ model is meant to have 32016 tokens according to its `vocab_size`,  but\
          \ the provided `added_tokens.json` only lists five extra tokens:\n```json\n\
          {\n  \"<|assistant|>\": 32004,\n  \"<|prefix_begin|>\": 32000,\n  \"<|prefix_end|>\"\
          : 32003,\n  \"<|prompter|>\": 32002,\n  \"<|system|>\": 32001\n}\n```\n\n\
          The only way I found around this was to edit added_tokens.json like so:\n\
          \n```json\n{\n \"<|dummy3|>\": 32007,\n \"<|dummy2|>\": 32006,\n \"<|dummy1|>\"\
          : 32005,\n  \"<|assistant|>\": 32004,\n  \"<|prefix_begin|>\": 32000,\n\
          \  \"<|prefix_end|>\": 32003,\n  \"<|prompter|>\": 32002,\n  \"<|system|>\"\
          : 32001\n}\n```\netc, up to 32015.\n\nQuite possibly this was wrong! But\
          \ without knowing what the other 11 tokens were meant to be, it's all I\
          \ could think to do, and it's what the GGML repo for OpenAssistant epoch\
          \ 6 did as well.\n\nNow you mention it, `<|endoftext|>` should likely be\
          \ in that list. But what token ID? And why wasn't it in the provided `added_tokens.json`\
          \ already?\n\nIf you know how to find the answers to any of these questions\
          \ I am happy to do the GGML conversion again with a new added_tokens.json,\
          \ if we can figure out what it should actually contain."
        updatedAt: '2023-05-05T13:33:41.530Z'
      numEdits: 0
      reactions: []
    id: 645505b5d55525a4fee52e5b
    type: comment
  author: TheBloke
  content: "In which case it's possible something has gone awry in the conversion\
    \ to GGML.  There was an issue when I tried to convert to GGML.  \n\nTo do HF\
    \ -> GGML conversion I use `convert.py` in the llama.cpp repo. This has a check\
    \ on the number of vocab entries.  Trying to run the conversion on this model\
    \ initially failed. The script threw an error reporting that model is meant to\
    \ have 32016 tokens according to its `vocab_size`,  but the provided `added_tokens.json`\
    \ only lists five extra tokens:\n```json\n{\n  \"<|assistant|>\": 32004,\n  \"\
    <|prefix_begin|>\": 32000,\n  \"<|prefix_end|>\": 32003,\n  \"<|prompter|>\":\
    \ 32002,\n  \"<|system|>\": 32001\n}\n```\n\nThe only way I found around this\
    \ was to edit added_tokens.json like so:\n\n```json\n{\n \"<|dummy3|>\": 32007,\n\
    \ \"<|dummy2|>\": 32006,\n \"<|dummy1|>\": 32005,\n  \"<|assistant|>\": 32004,\n\
    \  \"<|prefix_begin|>\": 32000,\n  \"<|prefix_end|>\": 32003,\n  \"<|prompter|>\"\
    : 32002,\n  \"<|system|>\": 32001\n}\n```\netc, up to 32015.\n\nQuite possibly\
    \ this was wrong! But without knowing what the other 11 tokens were meant to be,\
    \ it's all I could think to do, and it's what the GGML repo for OpenAssistant\
    \ epoch 6 did as well.\n\nNow you mention it, `<|endoftext|>` should likely be\
    \ in that list. But what token ID? And why wasn't it in the provided `added_tokens.json`\
    \ already?\n\nIf you know how to find the answers to any of these questions I\
    \ am happy to do the GGML conversion again with a new added_tokens.json, if we\
    \ can figure out what it should actually contain."
  created_at: 2023-05-05 12:33:41+00:00
  edited: false
  hidden: false
  id: 645505b5d55525a4fee52e5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T13:36:30.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: "<p>Here is an example where it gives weird response:</p>\n<pre><code>main:\
          \ build = 499 (6daa09d)\nmain: seed  = 1683293324\nllama.cpp: loading model\
          \ from OpenAssistant-30B-epoch7.ggml.q5_1.bin\nllama_model_load_internal:\
          \ format     = ggjt v1 (latest)\nllama_model_load_internal: n_vocab    =\
          \ 32016\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\n\
          llama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size = 127.27 KB\nllama_model_load_internal: mem required  =\
          \ 25573.29 MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size\
          \  = 3120.00 MB\n\nsystem_info: n_threads = 32 / 32 | AVX = 0 | AVX2 = 0\
          \ | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3\
          \ = 0 | VSX = 0 | \nmain: interactive mode on.\nsampling: repeat_last_n\
          \ = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty\
          \ = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p\
          \ = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent\
          \ = 5.000000\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject\
          \ at any time.\n - Press Return to return control to LLaMa.\n - If you want\
          \ to submit another line, end your input in '\\'.\n\n &lt;|prompter|&gt;Calculate\
          \ 324+263.&lt;|endoftext|&gt;&lt;|assistant|&gt;Please use the following\
          \ format for your response: \"&lt;answer&gt;=&lt;answer&gt;\"\n&lt;|assistant|&gt;So,\
          \ what is the answer to this calculation?&lt;/p&gt;\n\n&lt;p&gt;\n\n&lt;|prompter|&gt;324+263=\n\
          &lt;|endoftext|&gt;&lt;|assistant|&gt;Please use the following format for^C\n\
          </code></pre>\n<p>Where my prompt was <code>&lt;|prompter|&gt;Calculate\
          \ 324+263.&lt;|endoftext|&gt;&lt;|assistant|&gt;</code>.  It just writes\
          \ <code>&lt;|assistant|&gt;</code> second time, writes <code>&lt;p&gt;</code>,\
          \ which seems a consequence of <code>&lt;|assistant|&gt;</code> and <code>&lt;|prompter|&gt;</code>\
          \ not tokenized right.</p>\n"
        raw: "Here is an example where it gives weird response:\n```\nmain: build\
          \ = 499 (6daa09d)\nmain: seed  = 1683293324\nllama.cpp: loading model from\
          \ OpenAssistant-30B-epoch7.ggml.q5_1.bin\nllama_model_load_internal: format\
          \     = ggjt v1 (latest)\nllama_model_load_internal: n_vocab    = 32016\n\
          llama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
          \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
          \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
          \ n_rot      = 128\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\n\
          llama_model_load_internal: n_ff       = 17920\nllama_model_load_internal:\
          \ n_parts    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
          \ ggml ctx size = 127.27 KB\nllama_model_load_internal: mem required  =\
          \ 25573.29 MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size\
          \  = 3120.00 MB\n\nsystem_info: n_threads = 32 / 32 | AVX = 0 | AVX2 = 0\
          \ | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0\
          \ | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3\
          \ = 0 | VSX = 0 | \nmain: interactive mode on.\nsampling: repeat_last_n\
          \ = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty\
          \ = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p\
          \ = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent\
          \ = 5.000000\ngenerate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep\
          \ = 0\n\n\n== Running in interactive mode. ==\n - Press Ctrl+C to interject\
          \ at any time.\n - Press Return to return control to LLaMa.\n - If you want\
          \ to submit another line, end your input in '\\'.\n\n <|prompter|>Calculate\
          \ 324+263.<|endoftext|><|assistant|>Please use the following format for\
          \ your response: \"<answer>=<answer>\"\n<|assistant|>So, what is the answer\
          \ to this calculation?</p>\n\n<p>\n\n<|prompter|>324+263=\n<|endoftext|><|assistant|>Please\
          \ use the following format for^C\n```\n\nWhere my prompt was `<|prompter|>Calculate\
          \ 324+263.<|endoftext|><|assistant|>`.  It just writes `<|assistant|>` second\
          \ time, writes `<p>`, which seems a consequence of `<|assistant|>` and `<|prompter|>`\
          \ not tokenized right."
        updatedAt: '2023-05-05T13:36:30.000Z'
      numEdits: 0
      reactions: []
    id: 6455065ed55525a4fee53e08
    type: comment
  author: kurnevsky
  content: "Here is an example where it gives weird response:\n```\nmain: build =\
    \ 499 (6daa09d)\nmain: seed  = 1683293324\nllama.cpp: loading model from OpenAssistant-30B-epoch7.ggml.q5_1.bin\n\
    llama_model_load_internal: format     = ggjt v1 (latest)\nllama_model_load_internal:\
    \ n_vocab    = 32016\nllama_model_load_internal: n_ctx      = 2048\nllama_model_load_internal:\
    \ n_embd     = 6656\nllama_model_load_internal: n_mult     = 256\nllama_model_load_internal:\
    \ n_head     = 52\nllama_model_load_internal: n_layer    = 60\nllama_model_load_internal:\
    \ n_rot      = 128\nllama_model_load_internal: ftype      = 9 (mostly Q5_1)\n\
    llama_model_load_internal: n_ff       = 17920\nllama_model_load_internal: n_parts\
    \    = 1\nllama_model_load_internal: model size = 30B\nllama_model_load_internal:\
    \ ggml ctx size = 127.27 KB\nllama_model_load_internal: mem required  = 25573.29\
    \ MB (+ 3124.00 MB per state)\nllama_init_from_file: kv self size  = 3120.00 MB\n\
    \nsystem_info: n_threads = 32 / 32 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI\
    \ = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA\
    \ = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | VSX = 0 | \nmain: interactive mode\
    \ on.\nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty\
    \ = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p\
    \ = 0.950000, typical_p = 1.000000, temp = 0.700000, mirostat = 0, mirostat_lr\
    \ = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 2048, n_batch = 512,\
    \ n_predict = -1, n_keep = 0\n\n\n== Running in interactive mode. ==\n - Press\
    \ Ctrl+C to interject at any time.\n - Press Return to return control to LLaMa.\n\
    \ - If you want to submit another line, end your input in '\\'.\n\n <|prompter|>Calculate\
    \ 324+263.<|endoftext|><|assistant|>Please use the following format for your response:\
    \ \"<answer>=<answer>\"\n<|assistant|>So, what is the answer to this calculation?</p>\n\
    \n<p>\n\n<|prompter|>324+263=\n<|endoftext|><|assistant|>Please use the following\
    \ format for^C\n```\n\nWhere my prompt was `<|prompter|>Calculate 324+263.<|endoftext|><|assistant|>`.\
    \  It just writes `<|assistant|>` second time, writes `<p>`, which seems a consequence\
    \ of `<|assistant|>` and `<|prompter|>` not tokenized right."
  created_at: 2023-05-05 12:36:30+00:00
  edited: false
  hidden: false
  id: 6455065ed55525a4fee53e08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T13:40:48.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<blockquote>

          <p>Now you mention it, &lt;|endoftext|&gt; should likely be in that list.
          But what token ID? And why wasn''t it in the provided added_tokens.json
          already?</p>

          </blockquote>

          <p>I''d guess that this <code>&lt;|endoftext|&gt;</code> should be mapped
          to <code>oes</code> that should be already present in the llama model.</p>

          '
        raw: '> Now you mention it, <|endoftext|> should likely be in that list. But
          what token ID? And why wasn''t it in the provided added_tokens.json already?


          I''d guess that this `<|endoftext|>` should be mapped to `oes` that should
          be already present in the llama model.'
        updatedAt: '2023-05-05T13:40:48.437Z'
      numEdits: 0
      reactions: []
    id: 64550760a473375be56f1e4a
    type: comment
  author: kurnevsky
  content: '> Now you mention it, <|endoftext|> should likely be in that list. But
    what token ID? And why wasn''t it in the provided added_tokens.json already?


    I''d guess that this `<|endoftext|>` should be mapped to `oes` that should be
    already present in the llama model.'
  created_at: 2023-05-05 12:40:48+00:00
  edited: false
  hidden: false
  id: 64550760a473375be56f1e4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T13:46:48.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>No it isn't - it uses standard <code>&lt;/s&gt;</code> for that:<br><code>special_tokens_map.json</code>:</p>\n\
          <pre><code class=\"language-json\"><span class=\"hljs-punctuation\">{</span>\n\
          \  <span class=\"hljs-attr\">\"additional_special_tokens\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n\
          \    <span class=\"hljs-string\">\"&lt;|prompter|&gt;\"</span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-string\">\"&lt;|system|&gt;\"\
          </span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-string\"\
          >\"&lt;|prefix_begin|&gt;\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \    <span class=\"hljs-string\">\"&lt;|prefix_end|&gt;\"</span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-string\">\"&lt;|assistant|&gt;\"\
          </span>\n  <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"bos_token\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n\
          \    <span class=\"hljs-attr\">\"content\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-attr\">\"lstrip\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">false</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \    <span class=\"hljs-attr\">\"normalized\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-literal\"><span class=\"hljs-keyword\">true</span></span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"\
          rstrip\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-literal\"><span class=\"hljs-keyword\">false</span></span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"single_word\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">false</span></span>\n  <span class=\"hljs-punctuation\"\
          >}</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\"\
          >\"eos_token\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-string\">\"&lt;/s&gt;\"</span><span class=\"hljs-punctuation\">,</span>\n\
          \  <span class=\"hljs-attr\">\"pad_token\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"&lt;/s&gt;\"</span><span class=\"\
          hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"sep_token\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"&lt;s&gt;\"\
          </span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\"\
          >\"unk_token\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-punctuation\">{</span>\n    <span class=\"hljs-attr\">\"content\"</span><span\
          \ class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"\"</span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"\
          lstrip\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-literal\"><span class=\"hljs-keyword\">false</span></span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"normalized\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">true</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-attr\">\"rstrip\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">false</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \    <span class=\"hljs-attr\">\"single_word\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-literal\"><span class=\"hljs-keyword\">false</span></span>\n\
          \  <span class=\"hljs-punctuation\">}</span>\n<span class=\"hljs-punctuation\"\
          >}</span>\n</code></pre>\n<p>Maybe <code>&lt;|endoftext|&gt;</code> <em>should</em>\
          \ be mapped to EOS as you said. But that wasn't what was in the files they\
          \ released.  Could be the supplied .json's were wrong of course.</p>\n<p>Have\
          \ you tested without trying to use <code>&lt;|endoftext|&gt;</code>?</p>\n\
          <p>I can do some research to see if anyone else has figured out a better\
          \ fix for the GGML conversion since I made this model.  If there's any progress\
          \ there, I'll try making new GGMLs.  But right now I don't know what to\
          \ change to try and make it better.</p>\n"
        raw: "No it isn't - it uses standard `</s>` for that:\n`special_tokens_map.json`:\n\
          ```json\n{\n  \"additional_special_tokens\": [\n    \"<|prompter|>\",\n\
          \    \"<|system|>\",\n    \"<|prefix_begin|>\",\n    \"<|prefix_end|>\"\
          ,\n    \"<|assistant|>\"\n  ],\n  \"bos_token\": {\n    \"content\": \"\"\
          ,\n    \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\": false,\n\
          \    \"single_word\": false\n  },\n  \"eos_token\": \"</s>\",\n  \"pad_token\"\
          : \"</s>\",\n  \"sep_token\": \"<s>\",\n  \"unk_token\": {\n    \"content\"\
          : \"\",\n    \"lstrip\": false,\n    \"normalized\": true,\n    \"rstrip\"\
          : false,\n    \"single_word\": false\n  }\n}\n```\nMaybe `<|endoftext|>`\
          \ *should* be mapped to EOS as you said. But that wasn't what was in the\
          \ files they released.  Could be the supplied .json's were wrong of course.\n\
          \nHave you tested without trying to use `<|endoftext|>`?\n\nI can do some\
          \ research to see if anyone else has figured out a better fix for the GGML\
          \ conversion since I made this model.  If there's any progress there, I'll\
          \ try making new GGMLs.  But right now I don't know what to change to try\
          \ and make it better."
        updatedAt: '2023-05-05T13:47:49.321Z'
      numEdits: 2
      reactions: []
    id: 645508c8fe2f48cb4b65a743
    type: comment
  author: TheBloke
  content: "No it isn't - it uses standard `</s>` for that:\n`special_tokens_map.json`:\n\
    ```json\n{\n  \"additional_special_tokens\": [\n    \"<|prompter|>\",\n    \"\
    <|system|>\",\n    \"<|prefix_begin|>\",\n    \"<|prefix_end|>\",\n    \"<|assistant|>\"\
    \n  ],\n  \"bos_token\": {\n    \"content\": \"\",\n    \"lstrip\": false,\n \
    \   \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  },\n  \"eos_token\": \"</s>\",\n  \"pad_token\": \"</s>\",\n  \"sep_token\"\
    : \"<s>\",\n  \"unk_token\": {\n    \"content\": \"\",\n    \"lstrip\": false,\n\
    \    \"normalized\": true,\n    \"rstrip\": false,\n    \"single_word\": false\n\
    \  }\n}\n```\nMaybe `<|endoftext|>` *should* be mapped to EOS as you said. But\
    \ that wasn't what was in the files they released.  Could be the supplied .json's\
    \ were wrong of course.\n\nHave you tested without trying to use `<|endoftext|>`?\n\
    \nI can do some research to see if anyone else has figured out a better fix for\
    \ the GGML conversion since I made this model.  If there's any progress there,\
    \ I'll try making new GGMLs.  But right now I don't know what to change to try\
    \ and make it better."
  created_at: 2023-05-05 12:46:48+00:00
  edited: true
  hidden: false
  id: 645508c8fe2f48cb4b65a743
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T13:52:43.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<p>Here is the issue you described, but without an answer: <a href="https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2">https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2</a></p>

          '
        raw: 'Here is the issue you described, but without an answer: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2'
        updatedAt: '2023-05-05T13:52:43.958Z'
      numEdits: 0
      reactions: []
    id: 64550a2ba473375be56f5717
    type: comment
  author: kurnevsky
  content: 'Here is the issue you described, but without an answer: https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor/discussions/2'
  created_at: 2023-05-05 12:52:43+00:00
  edited: false
  hidden: false
  id: 64550a2ba473375be56f5717
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T14:08:02.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<blockquote>

          <p>Have you tested without trying to use &lt;|endoftext|&gt;?</p>

          </blockquote>

          <p>Just tried, looks like it works a bit better, but it''s impossible to
          define stop criteria:</p>

          <pre><code class="language-sh">./main --interactive -t 32 -m OpenAssistant-30B-epoch7.ggml.q5_1.bin
          --color -c 2048 --temp 0.7 -p <span class="hljs-string">"&lt;|prompter|&gt;Calculate
          324+263.&lt;|assistant|&gt;"</span>

          </code></pre>

          <p>With seeds 1 or 4 it just continues to generate without <code>eos</code>,
          <code>&lt;|endoftext|&gt;</code> or <code>&lt;|prompter|&gt;</code>.</p>

          '
        raw: '> Have you tested without trying to use <|endoftext|>?


          Just tried, looks like it works a bit better, but it''s impossible to define
          stop criteria:


          ```sh

          ./main --interactive -t 32 -m OpenAssistant-30B-epoch7.ggml.q5_1.bin --color
          -c 2048 --temp 0.7 -p "<|prompter|>Calculate 324+263.<|assistant|>"

          ```


          With seeds 1 or 4 it just continues to generate without `eos`, `<|endoftext|>`
          or `<|prompter|>`.'
        updatedAt: '2023-05-05T14:08:02.986Z'
      numEdits: 0
      reactions: []
    id: 64550dc2a473375be56fb086
    type: comment
  author: kurnevsky
  content: '> Have you tested without trying to use <|endoftext|>?


    Just tried, looks like it works a bit better, but it''s impossible to define stop
    criteria:


    ```sh

    ./main --interactive -t 32 -m OpenAssistant-30B-epoch7.ggml.q5_1.bin --color -c
    2048 --temp 0.7 -p "<|prompter|>Calculate 324+263.<|assistant|>"

    ```


    With seeds 1 or 4 it just continues to generate without `eos`, `<|endoftext|>`
    or `<|prompter|>`.'
  created_at: 2023-05-05 13:08:02+00:00
  edited: false
  hidden: false
  id: 64550dc2a473375be56fb086
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T14:46:43.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<p>Looking at <a rel="nofollow" href="https://github.com/LAION-AI/Open-Assistant/blob/a8ddf0f5f03af9b7d1fbb67d980646259534b9cd/model/model_training/utils/utils.py#L187">https://github.com/LAION-AI/Open-Assistant/blob/a8ddf0f5f03af9b7d1fbb67d980646259534b9cd/model/model_training/utils/utils.py#L187</a>
          it seems that for llama we indeed should use <code>&lt;s&gt;</code> token
          instead of <code>&lt;|endoftext|&gt;</code>.</p>

          '
        raw: Looking at https://github.com/LAION-AI/Open-Assistant/blob/a8ddf0f5f03af9b7d1fbb67d980646259534b9cd/model/model_training/utils/utils.py#L187
          it seems that for llama we indeed should use `<s>` token instead of `<|endoftext|>`.
        updatedAt: '2023-05-05T14:46:43.930Z'
      numEdits: 0
      reactions: []
    id: 645516d3a473375be570a386
    type: comment
  author: kurnevsky
  content: Looking at https://github.com/LAION-AI/Open-Assistant/blob/a8ddf0f5f03af9b7d1fbb67d980646259534b9cd/model/model_training/utils/utils.py#L187
    it seems that for llama we indeed should use `<s>` token instead of `<|endoftext|>`.
  created_at: 2023-05-05 13:46:43+00:00
  edited: false
  hidden: false
  id: 645516d3a473375be570a386
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T15:20:21.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<p>I checked <code>llama.cpp</code> and token <code>32004</code> indeed
          has <code> &lt;|assistant|&gt;</code> value. But it still doesn''t tokenize
          <code> &lt;|assistant|&gt;</code> back to <code>32004</code> so it might
          be a limitation of <code>llama.cpp</code> itself...</p>

          '
        raw: I checked `llama.cpp` and token `32004` indeed has ` <|assistant|>` value.
          But it still doesn't tokenize ` <|assistant|>` back to `32004` so it might
          be a limitation of `llama.cpp` itself...
        updatedAt: '2023-05-05T15:20:21.647Z'
      numEdits: 0
      reactions: []
    id: 64551eb5d55525a4fee7c3f4
    type: comment
  author: kurnevsky
  content: I checked `llama.cpp` and token `32004` indeed has ` <|assistant|>` value.
    But it still doesn't tokenize ` <|assistant|>` back to `32004` so it might be
    a limitation of `llama.cpp` itself...
  created_at: 2023-05-05 14:20:21+00:00
  edited: false
  hidden: false
  id: 64551eb5d55525a4fee7c3f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T16:09:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK thanks for the findings.</p>

          '
        raw: OK thanks for the findings.
        updatedAt: '2023-05-05T16:09:49.114Z'
      numEdits: 0
      reactions: []
    id: 64552a4da473375be57301bf
    type: comment
  author: TheBloke
  content: OK thanks for the findings.
  created_at: 2023-05-05 15:09:49+00:00
  edited: false
  hidden: false
  id: 64552a4da473375be57301bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-05T16:14:55.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: "<p>Another thing I tried is to insert these tokens programmatically,\
          \ like:</p>\n<pre><code>    embd_inp.insert(embd_inp.begin(), 32002);\n\
          \    embd_inp.push_back(llama_token_eos());\n    embd_inp.push_back(32004);\n\
          </code></pre>\n<p>It seems to work, but the model never produces any of\
          \ these tokens itself, just continues to generate endless text, like:</p>\n\
          <pre><code>&lt;|prompter|&gt; Hi.&lt;|assistant|&gt; Hello! How can I help\
          \ you today? Is there anything specific you would like to know or discuss?\
          \ I'm here to assist with any questions you may have. Let me know if there\
          \ is anything I can do for you.\n\nNote: If you are having trouble understanding\
          \ my responses, try rephrasing your question or providing more context.\
          \ I am a machine learning model and sometimes I might misunderstand your\
          \ request.\n\nI hope this helps! Let me know if there's anything else I\
          \ can assist with.\n\nBest regards,\nOpen Assistant\n\nNote: This is an\
          \ automated response generated by Open Assistant. If you have any concerns\
          \ or issues with my responses, please let me know and I will do my best\
          \ to improve.\n\nDisclaimer: The information provided is for general informational\
          \ purposes only and is not a substitute for professional advice. The use\
          \ of any information provided is solely at your own risk.\n\nIs there anything\
          \ else you would like to ask or discuss? Let me know if there's anything\
          \ I can help with!\n\nBest regards,\nOpen^C\n</code></pre>\n<p>So not sure\
          \ what is going on here :)</p>\n"
        raw: "Another thing I tried is to insert these tokens programmatically, like:\n\
          \n```\n    embd_inp.insert(embd_inp.begin(), 32002);\n    embd_inp.push_back(llama_token_eos());\n\
          \    embd_inp.push_back(32004);\n```\n\nIt seems to work, but the model\
          \ never produces any of these tokens itself, just continues to generate\
          \ endless text, like:\n\n```\n<|prompter|> Hi.<|assistant|> Hello! How can\
          \ I help you today? Is there anything specific you would like to know or\
          \ discuss? I'm here to assist with any questions you may have. Let me know\
          \ if there is anything I can do for you.\n\nNote: If you are having trouble\
          \ understanding my responses, try rephrasing your question or providing\
          \ more context. I am a machine learning model and sometimes I might misunderstand\
          \ your request.\n\nI hope this helps! Let me know if there's anything else\
          \ I can assist with.\n\nBest regards,\nOpen Assistant\n\nNote: This is an\
          \ automated response generated by Open Assistant. If you have any concerns\
          \ or issues with my responses, please let me know and I will do my best\
          \ to improve.\n\nDisclaimer: The information provided is for general informational\
          \ purposes only and is not a substitute for professional advice. The use\
          \ of any information provided is solely at your own risk.\n\nIs there anything\
          \ else you would like to ask or discuss? Let me know if there's anything\
          \ I can help with!\n\nBest regards,\nOpen^C\n```\n\nSo not sure what is\
          \ going on here :)"
        updatedAt: '2023-05-05T16:14:55.970Z'
      numEdits: 0
      reactions: []
    id: 64552b7fd55525a4fee9551f
    type: comment
  author: kurnevsky
  content: "Another thing I tried is to insert these tokens programmatically, like:\n\
    \n```\n    embd_inp.insert(embd_inp.begin(), 32002);\n    embd_inp.push_back(llama_token_eos());\n\
    \    embd_inp.push_back(32004);\n```\n\nIt seems to work, but the model never\
    \ produces any of these tokens itself, just continues to generate endless text,\
    \ like:\n\n```\n<|prompter|> Hi.<|assistant|> Hello! How can I help you today?\
    \ Is there anything specific you would like to know or discuss? I'm here to assist\
    \ with any questions you may have. Let me know if there is anything I can do for\
    \ you.\n\nNote: If you are having trouble understanding my responses, try rephrasing\
    \ your question or providing more context. I am a machine learning model and sometimes\
    \ I might misunderstand your request.\n\nI hope this helps! Let me know if there's\
    \ anything else I can assist with.\n\nBest regards,\nOpen Assistant\n\nNote: This\
    \ is an automated response generated by Open Assistant. If you have any concerns\
    \ or issues with my responses, please let me know and I will do my best to improve.\n\
    \nDisclaimer: The information provided is for general informational purposes only\
    \ and is not a substitute for professional advice. The use of any information\
    \ provided is solely at your own risk.\n\nIs there anything else you would like\
    \ to ask or discuss? Let me know if there's anything I can help with!\n\nBest\
    \ regards,\nOpen^C\n```\n\nSo not sure what is going on here :)"
  created_at: 2023-05-05 15:14:55+00:00
  edited: false
  hidden: false
  id: 64552b7fd55525a4fee9551f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T22:59:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah me neither!</p>

          <p>Where did you run that Python code? Does that mean you''re having the
          same issue with GPU inference also?</p>

          '
        raw: 'Yeah me neither!


          Where did you run that Python code? Does that mean you''re having the same
          issue with GPU inference also?'
        updatedAt: '2023-05-05T22:59:49.583Z'
      numEdits: 0
      reactions: []
    id: 64558a65d55525a4fef04c0a
    type: comment
  author: TheBloke
  content: 'Yeah me neither!


    Where did you run that Python code? Does that mean you''re having the same issue
    with GPU inference also?'
  created_at: 2023-05-05 21:59:49+00:00
  edited: false
  hidden: false
  id: 64558a65d55525a4fef04c0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
      fullname: Evgeny Kurnevsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kurnevsky
      type: user
    createdAt: '2023-05-06T06:33:53.000Z'
    data:
      edited: false
      editors:
      - kurnevsky
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf33f15c0773ef5abb61a91c26d352da.svg
          fullname: Evgeny Kurnevsky
          isHf: false
          isPro: false
          name: kurnevsky
          type: user
        html: '<p>It''s a c++ code - I just patched <code>main.cpp</code> in <code>llama.cpp</code>.
          I don''t have enough memory to run python version.</p>

          '
        raw: It's a c++ code - I just patched `main.cpp` in `llama.cpp`. I don't have
          enough memory to run python version.
        updatedAt: '2023-05-06T06:33:53.752Z'
      numEdits: 0
      reactions: []
    id: 6455f4d103625871eb6d7f17
    type: comment
  author: kurnevsky
  content: It's a c++ code - I just patched `main.cpp` in `llama.cpp`. I don't have
    enough memory to run python version.
  created_at: 2023-05-06 05:33:53+00:00
  edited: false
  hidden: false
  id: 6455f4d103625871eb6d7f17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-05-18T13:48:08.000Z'
    data:
      edited: false
      editors:
      - jeffwadsworth
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: '<p>The following prompt setup works for me:<br>main -m WizardLM-7B-uncensored.ggml.q4_0.bin
          --color --threads 12 --batch_size 256 --n_predict -1 --top_k 12 --top_p
          1 --temp 0.0 --repeat_penalty 1.05 --ctx_size 2048 --instruct --reverse-prompt
          "### Human:"</p>

          <p>Of course, use the correct model call, etc.</p>

          '
        raw: 'The following prompt setup works for me:

          main -m WizardLM-7B-uncensored.ggml.q4_0.bin --color --threads 12 --batch_size
          256 --n_predict -1 --top_k 12 --top_p 1 --temp 0.0 --repeat_penalty 1.05
          --ctx_size 2048 --instruct --reverse-prompt "### Human:"


          Of course, use the correct model call, etc.'
        updatedAt: '2023-05-18T13:48:08.063Z'
      numEdits: 0
      reactions: []
    id: 64662c98e0fe831b478f8309
    type: comment
  author: jeffwadsworth
  content: 'The following prompt setup works for me:

    main -m WizardLM-7B-uncensored.ggml.q4_0.bin --color --threads 12 --batch_size
    256 --n_predict -1 --top_k 12 --top_p 1 --temp 0.0 --repeat_penalty 1.05 --ctx_size
    2048 --instruct --reverse-prompt "### Human:"


    Of course, use the correct model call, etc.'
  created_at: 2023-05-18 12:48:08+00:00
  edited: false
  hidden: false
  id: 64662c98e0fe831b478f8309
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML
repo_type: model
status: open
target_branch: null
title: Prompts
