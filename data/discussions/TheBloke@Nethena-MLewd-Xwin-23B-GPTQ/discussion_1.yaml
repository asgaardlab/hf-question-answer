!!python/object:huggingface_hub.community.DiscussionWithDetails
author: akoyaki
conflicting_files: null
created_at: 2023-11-02 15:49:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-11-02T16:49:17.000Z'
    data:
      edited: true
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22208543121814728
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: "<p>System\uFF1AWindows 10<br>Hardware\uFF1A 2x3090<br>Model Version\uFF1A\
          \ gptq-8bit-128g-actorder_True<br>Model loader:ExLlama_HF,ExLlamav2_HF,ExLlama,ExLlamav2</p>\n\
          <p>Try other model like \"TheBloke_Chronos-70B-v2-GPTQ_gptq-4bit-32g-actorder_True\"\
          \ is work fine, only this model showing the problem<br>Model is loading\
          \ on GPU (GPU vram usage is raising) and will get error when complete loaded</p>\n\
          <p>\xB7<br>Traceback (most recent call last):</p>\n<p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\ui_model_menu.py\", line 206, in load_model_wrapper</p>\n<p>shared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)</p>\n<pre><code>\
          \                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n</code></pre>\n\
          <p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\modules\\models.py\"\
          , line 84, in load_model</p>\n<p>output = load_func_map<a rel=\"nofollow\"\
          \ href=\"model_name\">loader</a></p>\n<pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          </code></pre>\n<p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\models.py\", line 343, in ExLlama_HF_loader</p>\n<p>return ExllamaHF.from_pretrained(model_name)</p>\n\
          <pre><code>   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>File\
          \ \"D:\\text-generation-webui-snapshot-2023-10-29\\modules\\exllama_hf.py\"\
          , line 174, in from_pretrained</p>\n<p>return ExllamaHF(config)</p>\n<pre><code>\
          \   ^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\exllama_hf.py\", line 31, in init</p>\n<p>self.ex_model = ExLlama(self.ex_config)</p>\n\
          <pre><code>            ^^^^^^^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>File \"\
          D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\env\\Lib\\\
          site-packages\\exllama\\model.py\", line 889, in init</p>\n<p>layer = ExLlamaDecoderLayer(self.config,\
          \ tensors, f\"model.layers.{i}\", i, sin, cos)</p>\n<pre><code>    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          </code></pre>\n<p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          installer_files\\env\\Lib\\site-packages\\exllama\\model.py\", line 517,\
          \ in init</p>\n<p>self.self_attn = ExLlamaAttention(self.config, tensors,\
          \ key + \".self_attn\", sin, cos, self.index)</p>\n<pre><code>         \
          \    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          </code></pre>\n<p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          installer_files\\env\\Lib\\site-packages\\exllama\\model.py\", line 304,\
          \ in init</p>\n<p>self.q_proj = Ex4bitLinear(config, self.config.hidden_size,\
          \ self.config.num_attention_heads * self.config.head_dim, False, tensors,\
          \ key + \".q_proj\")</p>\n<pre><code>          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          </code></pre>\n<p>File \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          installer_files\\env\\Lib\\site-packages\\exllama\\model.py\", line 154,\
          \ in init</p>\n<p>self.q4 = cuda_ext.ext_make_q4(self.qweight,</p>\n<pre><code>\
          \      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n</code></pre>\n<p>File \"D:\\\
          text-generation-webui-snapshot-2023-10-29\\installer_files\\env\\Lib\\site-packages\\\
          exllama\\cuda_ext.py\", line 33, in ext_make_q4</p>\n<p>return make_q4(qweight,</p>\n\
          <pre><code>   ^^^^^^^^^^^^^^^^\n</code></pre>\n<p>RuntimeError: qweight\
          \ and qzeros have incompatible shapes<br>\xB7</p>\n"
        raw: "System\uFF1AWindows 10\nHardware\uFF1A 2x3090\nModel Version\uFF1A gptq-8bit-128g-actorder_True\n\
          Model loader:ExLlama_HF,ExLlamav2_HF,ExLlama,ExLlamav2\n\nTry other model\
          \ like \"TheBloke_Chronos-70B-v2-GPTQ_gptq-4bit-32g-actorder_True\" is work\
          \ fine, only this model showing the problem\nModel is loading on GPU (GPU\
          \ vram usage is raising) and will get error when complete loaded\n\n\xB7\
          \nTraceback (most recent call last):\n\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\ui_model_menu.py\", line 206, in load_model_wrapper\n\n\nshared.model,\
          \ shared.tokenizer = load_model(shared.model_name, loader)\n\n         \
          \                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"\
          D:\\text-generation-webui-snapshot-2023-10-29\\modules\\models.py\", line\
          \ 84, in load_model\n\n\noutput = load_func_map[loader](model_name)\n\n\
          \         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\models.py\", line 343, in ExLlama_HF_loader\n\n\nreturn ExllamaHF.from_pretrained(model_name)\n\
          \n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\exllama_hf.py\", line 174, in from_pretrained\n\n\nreturn ExllamaHF(config)\n\
          \n       ^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          modules\\exllama_hf.py\", line 31, in init\n\n\nself.ex_model = ExLlama(self.ex_config)\n\
          \n                ^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
          installer_files\\env\\Lib\\site-packages\\exllama\\model.py\", line 889,\
          \ in init\n\n\nlayer = ExLlamaDecoderLayer(self.config, tensors, f\"model.layers.{i}\"\
          , i, sin, cos)\n\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\\
          env\\Lib\\site-packages\\exllama\\model.py\", line 517, in init\n\n\nself.self_attn\
          \ = ExLlamaAttention(self.config, tensors, key + \".self_attn\", sin, cos,\
          \ self.index)\n\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\\
          env\\Lib\\site-packages\\exllama\\model.py\", line 304, in init\n\n\nself.q_proj\
          \ = Ex4bitLinear(config, self.config.hidden_size, self.config.num_attention_heads\
          \ * self.config.head_dim, False, tensors, key + \".q_proj\")\n\n       \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\\
          env\\Lib\\site-packages\\exllama\\model.py\", line 154, in init\n\n\nself.q4\
          \ = cuda_ext.ext_make_q4(self.qweight,\n\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\\
          env\\Lib\\site-packages\\exllama\\cuda_ext.py\", line 33, in ext_make_q4\n\
          \n\nreturn make_q4(qweight,\n\n       ^^^^^^^^^^^^^^^^\nRuntimeError: qweight\
          \ and qzeros have incompatible shapes\n\xB7\n"
        updatedAt: '2023-11-02T16:50:32.338Z'
      numEdits: 1
      reactions: []
    id: 6543d30d5a13979f82d81fbe
    type: comment
  author: akoyaki
  content: "System\uFF1AWindows 10\nHardware\uFF1A 2x3090\nModel Version\uFF1A gptq-8bit-128g-actorder_True\n\
    Model loader:ExLlama_HF,ExLlamav2_HF,ExLlama,ExLlamav2\n\nTry other model like\
    \ \"TheBloke_Chronos-70B-v2-GPTQ_gptq-4bit-32g-actorder_True\" is work fine, only\
    \ this model showing the problem\nModel is loading on GPU (GPU vram usage is raising)\
    \ and will get error when complete loaded\n\n\xB7\nTraceback (most recent call\
    \ last):\n\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\modules\\ui_model_menu.py\"\
    , line 206, in load_model_wrapper\n\n\nshared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\n\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"D:\\text-generation-webui-snapshot-2023-10-29\\modules\\models.py\", line\
    \ 84, in load_model\n\n\noutput = load_func_map[loader](model_name)\n\n      \
    \   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
    modules\\models.py\", line 343, in ExLlama_HF_loader\n\n\nreturn ExllamaHF.from_pretrained(model_name)\n\
    \n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
    modules\\exllama_hf.py\", line 174, in from_pretrained\n\n\nreturn ExllamaHF(config)\n\
    \n       ^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
    modules\\exllama_hf.py\", line 31, in init\n\n\nself.ex_model = ExLlama(self.ex_config)\n\
    \n                ^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
    installer_files\\env\\Lib\\site-packages\\exllama\\model.py\", line 889, in init\n\
    \n\nlayer = ExLlamaDecoderLayer(self.config, tensors, f\"model.layers.{i}\", i,\
    \ sin, cos)\n\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\env\\Lib\\\
    site-packages\\exllama\\model.py\", line 517, in init\n\n\nself.self_attn = ExLlamaAttention(self.config,\
    \ tensors, key + \".self_attn\", sin, cos, self.index)\n\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\env\\Lib\\\
    site-packages\\exllama\\model.py\", line 304, in init\n\n\nself.q_proj = Ex4bitLinear(config,\
    \ self.config.hidden_size, self.config.num_attention_heads * self.config.head_dim,\
    \ False, tensors, key + \".q_proj\")\n\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    File \"D:\\text-generation-webui-snapshot-2023-10-29\\installer_files\\env\\Lib\\\
    site-packages\\exllama\\model.py\", line 154, in init\n\n\nself.q4 = cuda_ext.ext_make_q4(self.qweight,\n\
    \n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"D:\\text-generation-webui-snapshot-2023-10-29\\\
    installer_files\\env\\Lib\\site-packages\\exllama\\cuda_ext.py\", line 33, in\
    \ ext_make_q4\n\n\nreturn make_q4(qweight,\n\n       ^^^^^^^^^^^^^^^^\nRuntimeError:\
    \ qweight and qzeros have incompatible shapes\n\xB7\n"
  created_at: 2023-11-02 15:49:17+00:00
  edited: true
  hidden: false
  id: 6543d30d5a13979f82d81fbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-11-02T16:49:56.000Z'
    data:
      from: 'Get error "RuntimeError: qweight and qzeros have incompatible shapes"
        on gptq-8bit-128g-actorder_True'
      to: 'Get error "RuntimeError: qweight and qzeros have incompatible shapes" on
        gptq-8bit-128g-actorder_True in text generation webui'
    id: 6543d334c868220f1c9973de
    type: title-change
  author: akoyaki
  created_at: 2023-11-02 15:49:56+00:00
  id: 6543d334c868220f1c9973de
  new_title: 'Get error "RuntimeError: qweight and qzeros have incompatible shapes"
    on gptq-8bit-128g-actorder_True in text generation webui'
  old_title: 'Get error "RuntimeError: qweight and qzeros have incompatible shapes"
    on gptq-8bit-128g-actorder_True'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-02T17:19:49.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710836410522461
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>ExLlama doesn''t support 8-bit GPTQs I''m afraid. Only 4-bit.</p>

          '
        raw: ExLlama doesn't support 8-bit GPTQs I'm afraid. Only 4-bit.
        updatedAt: '2023-11-02T17:19:49.240Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - akoyaki
        - cochese9000
    id: 6543da357a23df0e63662427
    type: comment
  author: TheBloke
  content: ExLlama doesn't support 8-bit GPTQs I'm afraid. Only 4-bit.
  created_at: 2023-11-02 16:19:49+00:00
  edited: false
  hidden: false
  id: 6543da357a23df0e63662427
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
      fullname: ayagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akoyaki
      type: user
    createdAt: '2023-11-02T17:57:57.000Z'
    data:
      edited: true
      editors:
      - akoyaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9407932758331299
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6279157c1513f33a55e7b0c372a88f49.svg
          fullname: ayagi
          isHf: false
          isPro: false
          name: akoyaki
          type: user
        html: '<blockquote>

          <p>ExLlama doesn''t support 8-bit GPTQs I''m afraid. Only 4-bit.</p>

          </blockquote>

          <p>Thanks for that information<br>I only used some 70b 4bit model and never
          try 8bit before, learned something new lol</p>

          '
        raw: '> ExLlama doesn''t support 8-bit GPTQs I''m afraid. Only 4-bit.


          Thanks for that information

          I only used some 70b 4bit model and never try 8bit before, learned something
          new lol

          '
        updatedAt: '2023-11-02T17:58:05.642Z'
      numEdits: 1
      reactions: []
    id: 6543e325b978e0b0f71c8839
    type: comment
  author: akoyaki
  content: '> ExLlama doesn''t support 8-bit GPTQs I''m afraid. Only 4-bit.


    Thanks for that information

    I only used some 70b 4bit model and never try 8bit before, learned something new
    lol

    '
  created_at: 2023-11-02 16:57:57+00:00
  edited: true
  hidden: false
  id: 6543e325b978e0b0f71c8839
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Nethena-MLewd-Xwin-23B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Get error "RuntimeError: qweight and qzeros have incompatible shapes" on gptq-8bit-128g-actorder_True
  in text generation webui'
