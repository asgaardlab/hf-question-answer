!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nobitha
conflicting_files: null
created_at: 2023-07-05 03:18:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/16b6cef3995753c1c131e419fde2fd1c.svg
      fullname: 'prameela reddi '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nobitha
      type: user
    createdAt: '2023-07-05T04:18:24.000Z'
    data:
      edited: false
      editors:
      - nobitha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8570564389228821
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/16b6cef3995753c1c131e419fde2fd1c.svg
          fullname: 'prameela reddi '
          isHf: false
          isPro: false
          name: nobitha
          type: user
        html: "<p>i am getting an error while downloading the model .</p>\n<p>ValueError:\
          \ The state dictionary of the model you are trying to load is corrupted.\
          \ Are you sure it was properly<br>saved?</p>\n<p>tokenizer = LlamaTokenizer.from_pretrained(\"\
          NousResearch/Redmond-Hermes-Coder\")\n  </p>\n"
        raw: "i am getting an error while downloading the model .\r\n\r\nValueError:\
          \ The state dictionary of the model you are trying to load is corrupted.\
          \ Are you sure it was properly \r\nsaved?\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"\
          NousResearch/Redmond-Hermes-Coder\")\r\n  "
        updatedAt: '2023-07-05T04:18:24.571Z'
      numEdits: 0
      reactions: []
    id: 64a4ef10927c1e320ea40ef0
    type: comment
  author: nobitha
  content: "i am getting an error while downloading the model .\r\n\r\nValueError:\
    \ The state dictionary of the model you are trying to load is corrupted. Are you\
    \ sure it was properly \r\nsaved?\r\n\r\ntokenizer = LlamaTokenizer.from_pretrained(\"\
    NousResearch/Redmond-Hermes-Coder\")\r\n  "
  created_at: 2023-07-05 03:18:24+00:00
  edited: false
  hidden: false
  id: 64a4ef10927c1e320ea40ef0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
      fullname: karan4d
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: karan4d
      type: user
    createdAt: '2023-07-05T23:51:06.000Z'
    data:
      edited: false
      editors:
      - karan4d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4545099139213562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
          fullname: karan4d
          isHf: false
          isPro: false
          name: karan4d
          type: user
        html: '<p>it''s not a llama model</p>

          <p>do this:</p>

          <pre><code># pip install -q transformers

          from transformers import AutoModelForCausalLM, AutoTokenizer


          checkpoint = "bigcode/starcoder"

          device = "cuda" # for GPU usage or "cpu" for CPU usage


          tokenizer = AutoTokenizer.from_pretrained(checkpoint)

          model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)


          inputs = tokenizer.encode("def print_hello_world():", return_tensors="pt").to(device)

          outputs = model.generate(inputs)

          print(tokenizer.decode(outputs[0]))

          </code></pre>

          '
        raw: 'it''s not a llama model


          do this:


          ```

          # pip install -q transformers

          from transformers import AutoModelForCausalLM, AutoTokenizer


          checkpoint = "bigcode/starcoder"

          device = "cuda" # for GPU usage or "cpu" for CPU usage


          tokenizer = AutoTokenizer.from_pretrained(checkpoint)

          model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)


          inputs = tokenizer.encode("def print_hello_world():", return_tensors="pt").to(device)

          outputs = model.generate(inputs)

          print(tokenizer.decode(outputs[0]))

          ```'
        updatedAt: '2023-07-05T23:51:06.819Z'
      numEdits: 0
      reactions: []
    id: 64a601ea16cb0fca09023e9f
    type: comment
  author: karan4d
  content: 'it''s not a llama model


    do this:


    ```

    # pip install -q transformers

    from transformers import AutoModelForCausalLM, AutoTokenizer


    checkpoint = "bigcode/starcoder"

    device = "cuda" # for GPU usage or "cpu" for CPU usage


    tokenizer = AutoTokenizer.from_pretrained(checkpoint)

    model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)


    inputs = tokenizer.encode("def print_hello_world():", return_tensors="pt").to(device)

    outputs = model.generate(inputs)

    print(tokenizer.decode(outputs[0]))

    ```'
  created_at: 2023-07-05 22:51:06+00:00
  edited: false
  hidden: false
  id: 64a601ea16cb0fca09023e9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
      fullname: karan4d
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: karan4d
      type: user
    createdAt: '2023-07-05T23:51:10.000Z'
    data:
      status: closed
    id: 64a601ee0c2ce52c64e80d6f
    type: status-change
  author: karan4d
  created_at: 2023-07-05 22:51:10+00:00
  id: 64a601ee0c2ce52c64e80d6f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NousResearch/Redmond-Hermes-Coder
repo_type: model
status: closed
target_branch: null
title: Are you sure it was properly  saved?
