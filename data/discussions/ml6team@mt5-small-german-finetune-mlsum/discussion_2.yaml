!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JingFan
conflicting_files: null
created_at: 2022-07-01 13:17:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acae1030bf660b3baa1837ee1566879f.svg
      fullname: Jing Fan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JingFan
      type: user
    createdAt: '2022-07-01T14:17:19.000Z'
    data:
      edited: false
      editors:
      - JingFan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acae1030bf660b3baa1837ee1566879f.svg
          fullname: Jing Fan
          isHf: false
          isPro: false
          name: JingFan
          type: user
        html: "<p>Hi there,</p>\n<p>my colleague <span data-props=\"{&quot;user&quot;:&quot;dennlinger&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dennlinger\"\
          >@<span class=\"underline\">dennlinger</span></a></span>\n\n\t</span></span>\
          \  and I are from the Institute of Computer Science at Heidelberg University,\
          \ currently investigating the performance of German abstractive summarizers.\
          \ We are very interested in your model and have tested it (among others)\
          \ on the <a href=\"https://huggingface.co/datasets/mlsum\">MLSUM test set</a>\
          \ (all samples). In case you are interested, see the results listed in the\
          \ table below, which indicates even better results than those <a href=\"\
          https://huggingface.co/ml6team/mt5-small-german-finetune-mlsum\">reported\
          \ in the model card</a>. </p>\n<div class=\"max-w-full overflow-auto\">\n\
          \t<table>\n\t\t<thead><tr>\n<th>Parameters</th>\n<th>Rouge1-F1</th>\n<th>Rouge2-F1</th>\n\
          <th>RougeL-F1</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n<td>MLSUM (max_length=354,\
          \ min_length=13, do_sample=false, truncation=True)</td>\n<td>0.4265</td>\n\
          <td>0.3321</td>\n<td>0.3978</td>\n</tr>\n</tbody>\n\t</table>\n</div>\n\
          <p>Aside from this, we  have some further questions regarding the model\
          \ and evaluation choices:</p>\n<ol>\n<li>Why did you use the validation\
          \ set instead of the test set for the evaluation?</li>\n<li>The fine-tuned\
          \ model was evaluated on 2000 random articles from the validation set. Did\
          \ you use a seed in this process? Alternatively, could you share the subset\
          \ of the 2000 articles that you used for the evaluation (IDs, etc.)?</li>\n\
          <li>Could you specify the hyperparameter choices that you used for the training\
          \ and evaluation process? We were unable to load the <code>training_args.bin</code>\
          \ file, potentially related to <a rel=\"nofollow\" href=\"https://discuss.huggingface.co/t/cannot-load-training-args-bin/10614\"\
          >this issue</a>.</li>\n<li>We checked the first five articles in the test\
          \ set and and found that the summaries primarily (4/5 articles) consist\
          \ of copies of the leading sentences of the reference articles. Are you\
          \ aware of this problem or did you perform any additional filtering?</li>\n\
          </ol>\n<p>Thank you in advance for your response and input!</p>\n<p>Best\
          \ wishes,</p>\n<p>Dennis and Jing</p>\n"
        raw: "Hi there,\r\n\r\nmy colleague @dennlinger  and I are from the Institute\
          \ of Computer Science at Heidelberg University, currently investigating\
          \ the performance of German abstractive summarizers. We are very interested\
          \ in your model and have tested it (among others) on the [MLSUM test set](https://huggingface.co/datasets/mlsum)\
          \ (all samples). In case you are interested, see the results listed in the\
          \ table below, which indicates even better results than those [reported\
          \ in the model card](https://huggingface.co/ml6team/mt5-small-german-finetune-mlsum).\
          \ \r\n\r\n| Parameters | Rouge1-F1 |Rouge2-F1 |RougeL-F1 |\r\n| ------ |\
          \ ------ |------ |------ |\r\n| MLSUM (max_length=354, min_length=13, do_sample=false,\
          \ truncation=True) | 0.4265 | 0.3321 | 0.3978|\r\n\r\n\r\nAside from this,\
          \ we  have some further questions regarding the model and evaluation choices:\r\
          \n\r\n1. Why did you use the validation set instead of the test set for\
          \ the evaluation?\r\n2. The fine-tuned model was evaluated on 2000 random\
          \ articles from the validation set. Did you use a seed in this process?\
          \ Alternatively, could you share the subset of the 2000 articles that you\
          \ used for the evaluation (IDs, etc.)?\r\n3. Could you specify the hyperparameter\
          \ choices that you used for the training and evaluation process? We were\
          \ unable to load the `training_args.bin` file, potentially related to [this\
          \ issue](https://discuss.huggingface.co/t/cannot-load-training-args-bin/10614).\r\
          \n4. We checked the first five articles in the test set and and found that\
          \ the summaries primarily (4/5 articles) consist of copies of the leading\
          \ sentences of the reference articles. Are you aware of this problem or\
          \ did you perform any additional filtering?\r\n\r\nThank you in advance\
          \ for your response and input!\r\n\r\nBest wishes,\r\n\r\nDennis and Jing"
        updatedAt: '2022-07-01T14:17:19.490Z'
      numEdits: 0
      reactions: []
    id: 62bf01effbc7a6370ba5cbd4
    type: comment
  author: JingFan
  content: "Hi there,\r\n\r\nmy colleague @dennlinger  and I are from the Institute\
    \ of Computer Science at Heidelberg University, currently investigating the performance\
    \ of German abstractive summarizers. We are very interested in your model and\
    \ have tested it (among others) on the [MLSUM test set](https://huggingface.co/datasets/mlsum)\
    \ (all samples). In case you are interested, see the results listed in the table\
    \ below, which indicates even better results than those [reported in the model\
    \ card](https://huggingface.co/ml6team/mt5-small-german-finetune-mlsum). \r\n\r\
    \n| Parameters | Rouge1-F1 |Rouge2-F1 |RougeL-F1 |\r\n| ------ | ------ |------\
    \ |------ |\r\n| MLSUM (max_length=354, min_length=13, do_sample=false, truncation=True)\
    \ | 0.4265 | 0.3321 | 0.3978|\r\n\r\n\r\nAside from this, we  have some further\
    \ questions regarding the model and evaluation choices:\r\n\r\n1. Why did you\
    \ use the validation set instead of the test set for the evaluation?\r\n2. The\
    \ fine-tuned model was evaluated on 2000 random articles from the validation set.\
    \ Did you use a seed in this process? Alternatively, could you share the subset\
    \ of the 2000 articles that you used for the evaluation (IDs, etc.)?\r\n3. Could\
    \ you specify the hyperparameter choices that you used for the training and evaluation\
    \ process? We were unable to load the `training_args.bin` file, potentially related\
    \ to [this issue](https://discuss.huggingface.co/t/cannot-load-training-args-bin/10614).\r\
    \n4. We checked the first five articles in the test set and and found that the\
    \ summaries primarily (4/5 articles) consist of copies of the leading sentences\
    \ of the reference articles. Are you aware of this problem or did you perform\
    \ any additional filtering?\r\n\r\nThank you in advance for your response and\
    \ input!\r\n\r\nBest wishes,\r\n\r\nDennis and Jing"
  created_at: 2022-07-01 13:17:19+00:00
  edited: false
  hidden: false
  id: 62bf01effbc7a6370ba5cbd4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ml6team/mt5-small-german-finetune-mlsum
repo_type: model
status: open
target_branch: null
title: Better results than the reported results and some questions about the model
