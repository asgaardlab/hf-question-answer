!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JaimeUPM
conflicting_files: null
created_at: 2023-12-21 16:25:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d3f64fa8ea77176dd9960b50d1b3233.svg
      fullname: Jaime Bellver
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JaimeUPM
      type: user
    createdAt: '2023-12-21T16:25:16.000Z'
    data:
      edited: false
      editors:
      - JaimeUPM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9024203419685364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d3f64fa8ea77176dd9960b50d1b3233.svg
          fullname: Jaime Bellver
          isHf: false
          isPro: false
          name: JaimeUPM
          type: user
        html: "<p>Hello! \U0001F44B</p>\n<p>I've been working on replicating the AUPRC\
          \ score of 0.62 achieved in ToxiChat using the LlamaGuard-7b model, and\
          \ I'm seeking some guidance on the specifics of the process. I would greatly\
          \ appreciate insights from anyone familiar with the model or the dataset.</p>\n\
          <ol>\n<li><p>Prompt Configuration: Could someone shed light on the ideal\
          \ prompt used for the ToxiChat model? What kind of inputs or context tend\
          \ to yield optimal results?</p>\n</li>\n<li><p>Dataset Handling: How is\
          \ the ToxiChat dataset treated during inference? Any special preprocessing\
          \ steps or considerations that contribute to the model's success?</p>\n\
          </li>\n<li><p>Probability for \"safe\" and \"unsafe\": Could someone share\
          \ insights on how to extract the probabilities associated with the predictions\
          \ of the words \"safe\" and \"unsafe\" from the model's output? Right now,\
          \ I obtain the logits of the words \"safe\" and \"unsafe\" and pass them\
          \ through a softmax to obtain the probabilities that add up to 1.</p>\n\
          </li>\n<li><p>Probability Thresholds for AUPRC: In calculating the AUPRC,\
          \ which probability thresholds are  used to determine positive and negative\
          \ predictions?</p>\n</li>\n</ol>\n<p>Looking forward to a fruitful discussion!</p>\n\
          <p>Thank you! \U0001F64C</p>\n"
        raw: "Hello! \U0001F44B\r\n\r\nI've been working on replicating the AUPRC\
          \ score of 0.62 achieved in ToxiChat using the LlamaGuard-7b model, and\
          \ I'm seeking some guidance on the specifics of the process. I would greatly\
          \ appreciate insights from anyone familiar with the model or the dataset.\r\
          \n\r\n1. Prompt Configuration: Could someone shed light on the ideal prompt\
          \ used for the ToxiChat model? What kind of inputs or context tend to yield\
          \ optimal results?\r\n\r\n2. Dataset Handling: How is the ToxiChat dataset\
          \ treated during inference? Any special preprocessing steps or considerations\
          \ that contribute to the model's success?\r\n\r\n3. Probability for \"safe\"\
          \ and \"unsafe\": Could someone share insights on how to extract the probabilities\
          \ associated with the predictions of the words \"safe\" and \"unsafe\" from\
          \ the model's output? Right now, I obtain the logits of the words \"safe\"\
          \ and \"unsafe\" and pass them through a softmax to obtain the probabilities\
          \ that add up to 1.\r\n\r\n4. Probability Thresholds for AUPRC: In calculating\
          \ the AUPRC, which probability thresholds are  used to determine positive\
          \ and negative predictions?\r\n\r\nLooking forward to a fruitful discussion!\r\
          \n\r\nThank you! \U0001F64C"
        updatedAt: '2023-12-21T16:25:16.550Z'
      numEdits: 0
      reactions: []
    id: 658466ec867e4340239fe038
    type: comment
  author: JaimeUPM
  content: "Hello! \U0001F44B\r\n\r\nI've been working on replicating the AUPRC score\
    \ of 0.62 achieved in ToxiChat using the LlamaGuard-7b model, and I'm seeking\
    \ some guidance on the specifics of the process. I would greatly appreciate insights\
    \ from anyone familiar with the model or the dataset.\r\n\r\n1. Prompt Configuration:\
    \ Could someone shed light on the ideal prompt used for the ToxiChat model? What\
    \ kind of inputs or context tend to yield optimal results?\r\n\r\n2. Dataset Handling:\
    \ How is the ToxiChat dataset treated during inference? Any special preprocessing\
    \ steps or considerations that contribute to the model's success?\r\n\r\n3. Probability\
    \ for \"safe\" and \"unsafe\": Could someone share insights on how to extract\
    \ the probabilities associated with the predictions of the words \"safe\" and\
    \ \"unsafe\" from the model's output? Right now, I obtain the logits of the words\
    \ \"safe\" and \"unsafe\" and pass them through a softmax to obtain the probabilities\
    \ that add up to 1.\r\n\r\n4. Probability Thresholds for AUPRC: In calculating\
    \ the AUPRC, which probability thresholds are  used to determine positive and\
    \ negative predictions?\r\n\r\nLooking forward to a fruitful discussion!\r\n\r\
    \nThank you! \U0001F64C"
  created_at: 2023-12-21 16:25:16+00:00
  edited: false
  hidden: false
  id: 658466ec867e4340239fe038
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: meta-llama/LlamaGuard-7b
repo_type: model
status: open
target_branch: null
title: 'Replicating AUPRC of 0.624 in ToxiChat: Understanding Model Inference'
