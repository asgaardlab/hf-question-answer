!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fangleen
conflicting_files: null
created_at: 2023-09-07 01:06:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36e2e4bd7ba36f37a904d2b5a8d7b5db.svg
      fullname: leen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fangleen
      type: user
    createdAt: '2023-09-07T02:06:09.000Z'
    data:
      edited: false
      editors:
      - fangleen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7666670083999634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36e2e4bd7ba36f37a904d2b5a8d7b5db.svg
          fullname: leen
          isHf: false
          isPro: false
          name: fangleen
          type: user
        html: "<p>Hi,<br>I ran the script in the Deploy menu above in AWS Sagemaker,\
          \ but after a while, it failed with the OOM error. The same issue happened\
          \ when I tried ml.g5.2xlarge and ml.g5.12xlarge. Is it the AWS environment\
          \ problem? Did anyone have this issue? </p>\n<p>Thanks,</p>\n<p>Error from\
          \ CloudWatch:</p>\n<p>torch.cuda.OutOfMemoryError: Allocation on device\
          \ 0 would exceed allowed memory. (out of memory)</p>\n<p>2023-09-06T15:39:42.570+08:00\t\
          Currently allocated : 21.13 GiB</p>\n<p>2023-09-06T15:39:42.570+08:00\t\
          Requested : 150.00 MiB</p>\n<p>2023-09-06T15:39:42.570+08:00\tDevice limit\
          \ : 22.20 GiB</p>\n<p>2023-09-06T15:39:42.570+08:00\tFree (according to\
          \ CUDA): 25.12 MiB</p>\n<p>2023-09-06T15:39:45.076+08:00\tPyTorch limit\
          \ (set by user-supplied memory fraction) : 17179869184.00 GiB</p>\n"
        raw: "Hi, \r\nI ran the script in the Deploy menu above in AWS Sagemaker,\
          \ but after a while, it failed with the OOM error. The same issue happened\
          \ when I tried ml.g5.2xlarge and ml.g5.12xlarge. Is it the AWS environment\
          \ problem? Did anyone have this issue? \r\n\r\nThanks,\r\n\r\nError from\
          \ CloudWatch:\r\n\r\ntorch.cuda.OutOfMemoryError: Allocation on device 0\
          \ would exceed allowed memory. (out of memory)\r\n\r\n2023-09-06T15:39:42.570+08:00\t\
          Currently allocated : 21.13 GiB\r\n\r\n2023-09-06T15:39:42.570+08:00\tRequested\
          \ : 150.00 MiB\r\n\r\n2023-09-06T15:39:42.570+08:00\tDevice limit : 22.20\
          \ GiB\r\n\r\n2023-09-06T15:39:42.570+08:00\tFree (according to CUDA): 25.12\
          \ MiB\r\n\r\n2023-09-06T15:39:45.076+08:00\tPyTorch limit (set by user-supplied\
          \ memory fraction) : 17179869184.00 GiB"
        updatedAt: '2023-09-07T02:06:09.904Z'
      numEdits: 0
      reactions: []
    id: 64f930116a71cea1c709fa97
    type: comment
  author: fangleen
  content: "Hi, \r\nI ran the script in the Deploy menu above in AWS Sagemaker, but\
    \ after a while, it failed with the OOM error. The same issue happened when I\
    \ tried ml.g5.2xlarge and ml.g5.12xlarge. Is it the AWS environment problem? Did\
    \ anyone have this issue? \r\n\r\nThanks,\r\n\r\nError from CloudWatch:\r\n\r\n\
    torch.cuda.OutOfMemoryError: Allocation on device 0 would exceed allowed memory.\
    \ (out of memory)\r\n\r\n2023-09-06T15:39:42.570+08:00\tCurrently allocated :\
    \ 21.13 GiB\r\n\r\n2023-09-06T15:39:42.570+08:00\tRequested : 150.00 MiB\r\n\r\
    \n2023-09-06T15:39:42.570+08:00\tDevice limit : 22.20 GiB\r\n\r\n2023-09-06T15:39:42.570+08:00\t\
    Free (according to CUDA): 25.12 MiB\r\n\r\n2023-09-06T15:39:45.076+08:00\tPyTorch\
    \ limit (set by user-supplied memory fraction) : 17179869184.00 GiB"
  created_at: 2023-09-07 01:06:09+00:00
  edited: false
  hidden: false
  id: 64f930116a71cea1c709fa97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-07T02:48:35.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9929113984107971
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<p>You don''t have a big enough gpu</p>

          '
        raw: You don't have a big enough gpu
        updatedAt: '2023-09-07T02:48:35.064Z'
      numEdits: 0
      reactions: []
    id: 64f93a0383807928d26faeb5
    type: comment
  author: teknium
  content: You don't have a big enough gpu
  created_at: 2023-09-07 01:48:35+00:00
  edited: false
  hidden: false
  id: 64f93a0383807928d26faeb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36e2e4bd7ba36f37a904d2b5a8d7b5db.svg
      fullname: leen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fangleen
      type: user
    createdAt: '2023-09-07T05:53:22.000Z'
    data:
      edited: true
      editors:
      - fangleen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9144789576530457
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36e2e4bd7ba36f37a904d2b5a8d7b5db.svg
          fullname: leen
          isHf: false
          isPro: false
          name: fangleen
          type: user
        html: "<p>Thanks for your help <span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/teknium\"\
          >@<span class=\"underline\">teknium</span></a></span>\n\n\t</span></span>\
          \ . I'm using ml.g5.12xlarge, isn't that enough? I can run llama2-13b from\
          \ meta on that instance.</p>\n<p>The deploy code suggests using 2xlarge:<br><code>predictor\
          \ = huggingface_model.deploy(     initial_instance_count=1,     instance_type=\"\
          ml.g5.2xlarge\",     container_startup_health_check_timeout=300,   )</code></p>\n"
        raw: "Thanks for your help @teknium . I'm using ml.g5.12xlarge, isn't that\
          \ enough? I can run llama2-13b from meta on that instance.\n\nThe deploy\
          \ code suggests using 2xlarge:\n`\npredictor = huggingface_model.deploy(\n\
          \tinitial_instance_count=1,\n\tinstance_type=\"ml.g5.2xlarge\",\n\tcontainer_startup_health_check_timeout=300,\n\
          \  )\n`"
        updatedAt: '2023-09-07T05:54:51.431Z'
      numEdits: 1
      reactions: []
    id: 64f965522003abb61b39e779
    type: comment
  author: fangleen
  content: "Thanks for your help @teknium . I'm using ml.g5.12xlarge, isn't that enough?\
    \ I can run llama2-13b from meta on that instance.\n\nThe deploy code suggests\
    \ using 2xlarge:\n`\npredictor = huggingface_model.deploy(\n\tinitial_instance_count=1,\n\
    \tinstance_type=\"ml.g5.2xlarge\",\n\tcontainer_startup_health_check_timeout=300,\n\
    \  )\n`"
  created_at: 2023-09-07 04:53:22+00:00
  edited: true
  hidden: false
  id: 64f965522003abb61b39e779
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-09-09T03:34:49.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5871416330337524
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: "<blockquote>\n<p>Thanks for your help <span data-props=\"{&quot;user&quot;:&quot;teknium&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/teknium\"\
          >@<span class=\"underline\">teknium</span></a></span>\n\n\t</span></span>\
          \ . I'm using ml.g5.12xlarge, isn't that enough? I can run llama2-13b from\
          \ meta on that instance.</p>\n<p>The deploy code suggests using 2xlarge:<br><code>predictor\
          \ = huggingface_model.deploy(     initial_instance_count=1,     instance_type=\"\
          ml.g5.2xlarge\",     container_startup_health_check_timeout=300,   )</code></p>\n\
          </blockquote>\n<p>To fit it on a 24gb gpu, either set</p>\n<p>\"device_map\"\
          : \"auto\"</p>\n<p>or pip install bitsandbytes</p>\n<p>and use load_in_8bit=True\
          \ or load_in_4bit=True </p>\n<p>all of these are LlamaForCausalLM.from_pretrained\
          \ args, i.e.<br>        self.model = LlamaForCausalLM.from_pretrained(<br>\
          \            \"./openhermes13b/\",<br>            torch_dtype=torch.float16,<br>\
          \            device_map='auto',<br>            #load_in_8bit=True<br>  \
          \      )</p>\n"
        raw: "> Thanks for your help @teknium . I'm using ml.g5.12xlarge, isn't that\
          \ enough? I can run llama2-13b from meta on that instance.\n> \n> The deploy\
          \ code suggests using 2xlarge:\n> `\n> predictor = huggingface_model.deploy(\n\
          > \tinitial_instance_count=1,\n> \tinstance_type=\"ml.g5.2xlarge\",\n> \t\
          container_startup_health_check_timeout=300,\n>   )\n> `\n\nTo fit it on\
          \ a 24gb gpu, either set\n\n\"device_map\": \"auto\"\n\nor pip install bitsandbytes\n\
          \nand use load_in_8bit=True or load_in_4bit=True \n\nall of these are LlamaForCausalLM.from_pretrained\
          \ args, i.e.\n        self.model = LlamaForCausalLM.from_pretrained(\n \
          \           \"./openhermes13b/\",\n            torch_dtype=torch.float16,\n\
          \            device_map='auto',\n            #load_in_8bit=True\n      \
          \  )"
        updatedAt: '2023-09-09T03:34:49.319Z'
      numEdits: 0
      reactions: []
    id: 64fbe7d901aedd0e86c9a8c8
    type: comment
  author: teknium
  content: "> Thanks for your help @teknium . I'm using ml.g5.12xlarge, isn't that\
    \ enough? I can run llama2-13b from meta on that instance.\n> \n> The deploy code\
    \ suggests using 2xlarge:\n> `\n> predictor = huggingface_model.deploy(\n> \t\
    initial_instance_count=1,\n> \tinstance_type=\"ml.g5.2xlarge\",\n> \tcontainer_startup_health_check_timeout=300,\n\
    >   )\n> `\n\nTo fit it on a 24gb gpu, either set\n\n\"device_map\": \"auto\"\n\
    \nor pip install bitsandbytes\n\nand use load_in_8bit=True or load_in_4bit=True\
    \ \n\nall of these are LlamaForCausalLM.from_pretrained args, i.e.\n        self.model\
    \ = LlamaForCausalLM.from_pretrained(\n            \"./openhermes13b/\",\n   \
    \         torch_dtype=torch.float16,\n            device_map='auto',\n       \
    \     #load_in_8bit=True\n        )"
  created_at: 2023-09-09 02:34:49+00:00
  edited: false
  hidden: false
  id: 64fbe7d901aedd0e86c9a8c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36e2e4bd7ba36f37a904d2b5a8d7b5db.svg
      fullname: leen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fangleen
      type: user
    createdAt: '2023-09-11T01:58:33.000Z'
    data:
      edited: false
      editors:
      - fangleen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9774367213249207
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36e2e4bd7ba36f37a904d2b5a8d7b5db.svg
          fullname: leen
          isHf: false
          isPro: false
          name: fangleen
          type: user
        html: '<p>Thanks a lot. </p>

          '
        raw: 'Thanks a lot. '
        updatedAt: '2023-09-11T01:58:33.888Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - teknium
    id: 64fe744901aedd0e860f05b6
    type: comment
  author: fangleen
  content: 'Thanks a lot. '
  created_at: 2023-09-11 00:58:33+00:00
  edited: false
  hidden: false
  id: 64fe744901aedd0e860f05b6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: NousResearch/Nous-Hermes-Llama2-13b
repo_type: model
status: open
target_branch: null
title: Failed to run in AWS SageMaker
