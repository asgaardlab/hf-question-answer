!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Gryphe
conflicting_files: null
created_at: 2023-07-21 18:47:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ae4107ad6218d51a2a7d0c/3dcor68aYBKEcTlOUHJpK.png?w=200&h=200&f=face
      fullname: Gryphe Padar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gryphe
      type: user
    createdAt: '2023-07-21T19:47:35.000Z'
    data:
      edited: false
      editors:
      - Gryphe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5393638610839844
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ae4107ad6218d51a2a7d0c/3dcor68aYBKEcTlOUHJpK.png?w=200&h=200&f=face
          fullname: Gryphe Padar
          isHf: false
          isPro: false
          name: Gryphe
          type: user
        html: '<p>Just a FYI (I''m aware you made a GGML available yourself)</p>

          <p>Exception: Vocab size mismatch (model has 32032, but I:\HF\Storage\NousResearch_Nous-Hermes-Llama2-13b\tokenizer.model
          combined with I:\HF\Storage\NousResearch_Nous-Hermes-Llama2-13b\added_tokens.json
          has 32001).</p>

          '
        raw: "Just a FYI (I'm aware you made a GGML available yourself)\r\n\r\nException:\
          \ Vocab size mismatch (model has 32032, but I:\\HF\\Storage\\NousResearch_Nous-Hermes-Llama2-13b\\\
          tokenizer.model combined with I:\\HF\\Storage\\NousResearch_Nous-Hermes-Llama2-13b\\\
          added_tokens.json has 32001)."
        updatedAt: '2023-07-21T19:47:35.377Z'
      numEdits: 0
      reactions: []
    id: 64bae0d75c457ccaa4d7b114
    type: comment
  author: Gryphe
  content: "Just a FYI (I'm aware you made a GGML available yourself)\r\n\r\nException:\
    \ Vocab size mismatch (model has 32032, but I:\\HF\\Storage\\NousResearch_Nous-Hermes-Llama2-13b\\\
    tokenizer.model combined with I:\\HF\\Storage\\NousResearch_Nous-Hermes-Llama2-13b\\\
    added_tokens.json has 32001)."
  created_at: 2023-07-21 18:47:35+00:00
  edited: false
  hidden: false
  id: 64bae0d75c457ccaa4d7b114
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ae4107ad6218d51a2a7d0c/3dcor68aYBKEcTlOUHJpK.png?w=200&h=200&f=face
      fullname: Gryphe Padar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gryphe
      type: user
    createdAt: '2023-07-21T19:48:08.000Z'
    data:
      from: Errors during quantization
      to: Error during quantization
    id: 64bae0f8f671da974eecdff8
    type: title-change
  author: Gryphe
  created_at: 2023-07-21 18:48:08+00:00
  id: 64bae0f8f671da974eecdff8
  new_title: Error during quantization
  old_title: Errors during quantization
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-07-21T21:19:29.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9262347221374512
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<p>Same finding here. </p>

          <p>Also when I attempted quantization from the provided ggml fp16, I''m
          getting notified that certain tensors aren''t k-quant compatible due to
          dimensions not being a multiple of 256 - presumably also related to the
          vocab changes.</p>

          '
        raw: "Same finding here. \n\nAlso when I attempted quantization from the provided\
          \ ggml fp16, I'm getting notified that certain tensors aren't k-quant compatible\
          \ due to dimensions not being a multiple of 256 - presumably also related\
          \ to the vocab changes."
        updatedAt: '2023-07-21T21:19:29.081Z'
      numEdits: 0
      reactions: []
    id: 64baf6618e051085bac28c44
    type: comment
  author: Doctor-Shotgun
  content: "Same finding here. \n\nAlso when I attempted quantization from the provided\
    \ ggml fp16, I'm getting notified that certain tensors aren't k-quant compatible\
    \ due to dimensions not being a multiple of 256 - presumably also related to the\
    \ vocab changes."
  created_at: 2023-07-21 20:19:29+00:00
  edited: false
  hidden: false
  id: 64baf6618e051085bac28c44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f26fdb21bebb6d2aafa03b4814dfa471.svg
      fullname: Praneet Reddy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Praneet
      type: user
    createdAt: '2023-07-22T02:24:36.000Z'
    data:
      edited: false
      editors:
      - Praneet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9896666407585144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f26fdb21bebb6d2aafa03b4814dfa471.svg
          fullname: Praneet Reddy
          isHf: false
          isPro: false
          name: Praneet
          type: user
        html: '<p>Yup, doesn''t seem to work with 4 bit or 8 bit quantization offered
          through bitsandbytes</p>

          '
        raw: Yup, doesn't seem to work with 4 bit or 8 bit quantization offered through
          bitsandbytes
        updatedAt: '2023-07-22T02:24:36.383Z'
      numEdits: 0
      reactions: []
    id: 64bb3de4ae436c8813de0d04
    type: comment
  author: Praneet
  content: Yup, doesn't seem to work with 4 bit or 8 bit quantization offered through
    bitsandbytes
  created_at: 2023-07-22 01:24:36+00:00
  edited: false
  hidden: false
  id: 64bb3de4ae436c8813de0d04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-07-22T04:07:03.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8313726186752319
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>BnB on newer transformers can be fixed with pretraining_tp": 1 in
          the config file</p>

          '
        raw: 'BnB on newer transformers can be fixed with pretraining_tp": 1 in the
          config file'
        updatedAt: '2023-07-22T04:07:03.070Z'
      numEdits: 0
      reactions: []
    id: 64bb55e7e90972519d48e809
    type: comment
  author: Henk717
  content: 'BnB on newer transformers can be fixed with pretraining_tp": 1 in the
    config file'
  created_at: 2023-07-22 03:07:03+00:00
  edited: false
  hidden: false
  id: 64bb55e7e90972519d48e809
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f0ffc5c9cc0c046755775cb99928321.svg
      fullname: Ozcan Esen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ozcanesen
      type: user
    createdAt: '2023-07-23T14:40:46.000Z'
    data:
      edited: false
      editors:
      - ozcanesen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9070690274238586
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f0ffc5c9cc0c046755775cb99928321.svg
          fullname: Ozcan Esen
          isHf: false
          isPro: false
          name: ozcanesen
          type: user
        html: '<p>Same problem here.<br>config.json says "vocab_size": 32032<br>while
          largest id in tokenizer.json is 32000</p>

          <p>Does anyone know how to solve this?</p>

          '
        raw: 'Same problem here.

          config.json says "vocab_size": 32032

          while largest id in tokenizer.json is 32000


          Does anyone know how to solve this?'
        updatedAt: '2023-07-23T14:40:46.009Z'
      numEdits: 0
      reactions: []
    id: 64bd3beeb567ae97c3331a86
    type: comment
  author: ozcanesen
  content: 'Same problem here.

    config.json says "vocab_size": 32032

    while largest id in tokenizer.json is 32000


    Does anyone know how to solve this?'
  created_at: 2023-07-23 13:40:46+00:00
  edited: false
  hidden: false
  id: 64bd3beeb567ae97c3331a86
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-07-24T05:05:15.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8902496099472046
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<blockquote>

          <p>Same problem here.<br>config.json says "vocab_size": 32032<br>while largest
          id in tokenizer.json is 32000</p>

          <p>Does anyone know how to solve this?</p>

          </blockquote>

          <p>You can add 32 dummy tokens to added_tokens.json to make it match the
          tensor size. Not sure the reason it''s set up like this.</p>

          '
        raw: "> Same problem here.\n> config.json says \"vocab_size\": 32032\n> while\
          \ largest id in tokenizer.json is 32000\n> \n> Does anyone know how to solve\
          \ this?\n\nYou can add 32 dummy tokens to added_tokens.json to make it match\
          \ the tensor size. Not sure the reason it's set up like this."
        updatedAt: '2023-07-24T05:05:15.659Z'
      numEdits: 0
      reactions: []
    id: 64be068b36eb058cd90cc0b7
    type: comment
  author: Doctor-Shotgun
  content: "> Same problem here.\n> config.json says \"vocab_size\": 32032\n> while\
    \ largest id in tokenizer.json is 32000\n> \n> Does anyone know how to solve this?\n\
    \nYou can add 32 dummy tokens to added_tokens.json to make it match the tensor\
    \ size. Not sure the reason it's set up like this."
  created_at: 2023-07-24 04:05:15+00:00
  edited: false
  hidden: false
  id: 64be068b36eb058cd90cc0b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
      fullname: karan4d
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: karan4d
      type: user
    createdAt: '2023-07-24T06:11:58.000Z'
    data:
      edited: false
      editors:
      - karan4d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.929893434047699
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62fc06a221c444a56f7cc595/Ln3m7sK52hwGuMnPe4mfU.jpeg?w=200&h=200&f=face
          fullname: karan4d
          isHf: false
          isPro: false
          name: karan4d
          type: user
        html: '<blockquote>

          <p>BnB on newer transformers can be fixed with pretraining_tp": 1 in the
          config file</p>

          </blockquote>

          <p>this is the real fix. its an issue on behalf of huggingface and broke
          lots of the llama 2 finetunes dropped that day.</p>

          <p>fix has been pushed on the model if you wanna just download the new config.json</p>

          <p>if still having issues can do the dummy token thing but not recommended</p>

          '
        raw: '> BnB on newer transformers can be fixed with pretraining_tp": 1 in
          the config file


          this is the real fix. its an issue on behalf of huggingface and broke lots
          of the llama 2 finetunes dropped that day.


          fix has been pushed on the model if you wanna just download the new config.json


          if still having issues can do the dummy token thing but not recommended'
        updatedAt: '2023-07-24T06:11:58.658Z'
      numEdits: 0
      reactions: []
    id: 64be162e4b4ff0d509629864
    type: comment
  author: karan4d
  content: '> BnB on newer transformers can be fixed with pretraining_tp": 1 in the
    config file


    this is the real fix. its an issue on behalf of huggingface and broke lots of
    the llama 2 finetunes dropped that day.


    fix has been pushed on the model if you wanna just download the new config.json


    if still having issues can do the dummy token thing but not recommended'
  created_at: 2023-07-24 05:11:58+00:00
  edited: false
  hidden: false
  id: 64be162e4b4ff0d509629864
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
      fullname: Crestfall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crestf411
      type: user
    createdAt: '2023-07-24T13:41:44.000Z'
    data:
      edited: false
      editors:
      - crestf411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8686609864234924
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
          fullname: Crestfall
          isHf: false
          isPro: false
          name: crestf411
          type: user
        html: '<p>I upgraded transformers and bitsandbytes to latest versions, but
          I am still getting vocab size mismatch when trying to run convert.py in
          llama.cpp. What am I missing?</p>

          '
        raw: I upgraded transformers and bitsandbytes to latest versions, but I am
          still getting vocab size mismatch when trying to run convert.py in llama.cpp.
          What am I missing?
        updatedAt: '2023-07-24T13:41:44.546Z'
      numEdits: 0
      reactions: []
    id: 64be7f9830a1f0f0f0a9ac5d
    type: comment
  author: crestf411
  content: I upgraded transformers and bitsandbytes to latest versions, but I am still
    getting vocab size mismatch when trying to run convert.py in llama.cpp. What am
    I missing?
  created_at: 2023-07-24 12:41:44+00:00
  edited: false
  hidden: false
  id: 64be7f9830a1f0f0f0a9ac5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
      fullname: Crestfall
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crestf411
      type: user
    createdAt: '2023-07-26T02:04:22.000Z'
    data:
      edited: false
      editors:
      - crestf411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9922276139259338
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
          fullname: Crestfall
          isHf: false
          isPro: false
          name: crestf411
          type: user
        html: '<p>Only solution I could find was to add a bunch of dummy tokens to
          add_tokens.json, which works, but seems like a dumb fix that could lead
          to issues. Better than nothing, I guess.</p>

          '
        raw: Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,
          which works, but seems like a dumb fix that could lead to issues. Better
          than nothing, I guess.
        updatedAt: '2023-07-26T02:04:22.071Z'
      numEdits: 0
      reactions: []
    id: 64c07f26a4cb9ca1400c4598
    type: comment
  author: crestf411
  content: Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,
    which works, but seems like a dumb fix that could lead to issues. Better than
    nothing, I guess.
  created_at: 2023-07-26 01:04:22+00:00
  edited: false
  hidden: false
  id: 64c07f26a4cb9ca1400c4598
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9703072c762617fa4f4bf9ce01dba957.svg
      fullname: xiaochaozhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zhanluwufang
      type: user
    createdAt: '2023-07-27T02:52:03.000Z'
    data:
      edited: false
      editors:
      - zhanluwufang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9836204051971436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9703072c762617fa4f4bf9ce01dba957.svg
          fullname: xiaochaozhan
          isHf: false
          isPro: false
          name: zhanluwufang
          type: user
        html: '<blockquote>

          <p>Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,
          which works, but seems like a dumb fix that could lead to issues. Better
          than nothing, I guess.</p>

          </blockquote>

          <p>Please tell me .How do I add a bunch of dummy tokens?</p>

          '
        raw: '> Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,
          which works, but seems like a dumb fix that could lead to issues. Better
          than nothing, I guess.


          Please tell me .How do I add a bunch of dummy tokens?'
        updatedAt: '2023-07-27T02:52:03.551Z'
      numEdits: 0
      reactions: []
    id: 64c1dbd3a3d4c23e0e93172d
    type: comment
  author: zhanluwufang
  content: '> Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,
    which works, but seems like a dumb fix that could lead to issues. Better than
    nothing, I guess.


    Please tell me .How do I add a bunch of dummy tokens?'
  created_at: 2023-07-27 01:52:03+00:00
  edited: false
  hidden: false
  id: 64c1dbd3a3d4c23e0e93172d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f0ffc5c9cc0c046755775cb99928321.svg
      fullname: Ozcan Esen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ozcanesen
      type: user
    createdAt: '2023-07-27T19:20:57.000Z'
    data:
      edited: false
      editors:
      - ozcanesen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5830389857292175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f0ffc5c9cc0c046755775cb99928321.svg
          fullname: Ozcan Esen
          isHf: false
          isPro: false
          name: ozcanesen
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,
          which works, but seems like a dumb fix that could lead to issues. Better
          than nothing, I guess.</p>

          </blockquote>

          <p>Please tell me .How do I add a bunch of dummy tokens?</p>

          </blockquote>

          <p>this is my added_tokens.json file with dummy tokens to make it total
          of 32032 tokens:</p>

          <p><code> {"&lt;pad&gt;": 32000, "&lt;pad1&gt;": 32001, "&lt;pad2&gt;":
          32002, "&lt;pad3&gt;": 32003, "&lt;pad4&gt;": 32004, "&lt;pad5&gt;": 32005,
          "&lt;pad6&gt;": 32006, "&lt;pad7&gt;": 32007, "&lt;pad8&gt;": 32008, "&lt;pad9&gt;":
          32009, "&lt;pad10&gt;": 32010, "&lt;pad11&gt;": 32011, "&lt;pad12&gt;":
          32012, "&lt;pad13&gt;": 32013, "&lt;pad14&gt;": 32014, "&lt;pad15&gt;":
          32015, "&lt;pad16&gt;": 32016, "&lt;pad17&gt;": 32017, "&lt;pad18&gt;":
          32018, "&lt;pad19&gt;": 32019, "&lt;pad20&gt;": 32020, "&lt;pad21&gt;":
          32021, "&lt;pad22&gt;": 32022, "&lt;pad23&gt;": 32023, "&lt;pad24&gt;":
          32024, "&lt;pad25&gt;": 32025, "&lt;pad26&gt;": 32026, "&lt;pad27&gt;":
          32027, "&lt;pad28&gt;": 32028, "&lt;pad29&gt;": 32029, "&lt;pad30&gt;":
          32030,"&lt;pad31&gt;": 32031}</code></p>

          '
        raw: "> > Only solution I could find was to add a bunch of dummy tokens to\
          \ add_tokens.json, which works, but seems like a dumb fix that could lead\
          \ to issues. Better than nothing, I guess.\n> \n> Please tell me .How do\
          \ I add a bunch of dummy tokens?\n\nthis is my added_tokens.json file with\
          \ dummy tokens to make it total of 32032 tokens:\n```\n{\"<pad>\": 32000,\
          \ \"<pad1>\": 32001, \"<pad2>\": 32002, \"<pad3>\": 32003, \"<pad4>\": 32004,\
          \ \"<pad5>\": 32005, \"<pad6>\": 32006, \"<pad7>\": 32007, \"<pad8>\": 32008,\
          \ \"<pad9>\": 32009, \"<pad10>\": 32010, \"<pad11>\": 32011, \"<pad12>\"\
          : 32012, \"<pad13>\": 32013, \"<pad14>\": 32014, \"<pad15>\": 32015, \"\
          <pad16>\": 32016, \"<pad17>\": 32017, \"<pad18>\": 32018, \"<pad19>\": 32019,\
          \ \"<pad20>\": 32020, \"<pad21>\": 32021, \"<pad22>\": 32022, \"<pad23>\"\
          : 32023, \"<pad24>\": 32024, \"<pad25>\": 32025, \"<pad26>\": 32026, \"\
          <pad27>\": 32027, \"<pad28>\": 32028, \"<pad29>\": 32029, \"<pad30>\": 32030,\"\
          <pad31>\": 32031}```"
        updatedAt: '2023-07-27T19:20:57.095Z'
      numEdits: 0
      reactions: []
    id: 64c2c399819b150fbbff0acf
    type: comment
  author: ozcanesen
  content: "> > Only solution I could find was to add a bunch of dummy tokens to add_tokens.json,\
    \ which works, but seems like a dumb fix that could lead to issues. Better than\
    \ nothing, I guess.\n> \n> Please tell me .How do I add a bunch of dummy tokens?\n\
    \nthis is my added_tokens.json file with dummy tokens to make it total of 32032\
    \ tokens:\n```\n{\"<pad>\": 32000, \"<pad1>\": 32001, \"<pad2>\": 32002, \"<pad3>\"\
    : 32003, \"<pad4>\": 32004, \"<pad5>\": 32005, \"<pad6>\": 32006, \"<pad7>\":\
    \ 32007, \"<pad8>\": 32008, \"<pad9>\": 32009, \"<pad10>\": 32010, \"<pad11>\"\
    : 32011, \"<pad12>\": 32012, \"<pad13>\": 32013, \"<pad14>\": 32014, \"<pad15>\"\
    : 32015, \"<pad16>\": 32016, \"<pad17>\": 32017, \"<pad18>\": 32018, \"<pad19>\"\
    : 32019, \"<pad20>\": 32020, \"<pad21>\": 32021, \"<pad22>\": 32022, \"<pad23>\"\
    : 32023, \"<pad24>\": 32024, \"<pad25>\": 32025, \"<pad26>\": 32026, \"<pad27>\"\
    : 32027, \"<pad28>\": 32028, \"<pad29>\": 32029, \"<pad30>\": 32030,\"<pad31>\"\
    : 32031}```"
  created_at: 2023-07-27 18:20:57+00:00
  edited: false
  hidden: false
  id: 64c2c399819b150fbbff0acf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-07-29T10:35:00.000Z'
    data:
      edited: false
      editors:
      - teknium
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9226664900779724
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
          fullname: Teknium
          isHf: false
          isPro: false
          name: teknium
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Same problem here.<br>config.json says "vocab_size": 32032<br>while largest
          id in tokenizer.json is 32000</p>

          <p>Does anyone know how to solve this?</p>

          </blockquote>

          <p>You can add 32 dummy tokens to added_tokens.json to make it match the
          tensor size. Not sure the reason it''s set up like this.</p>

          </blockquote>

          <p>Seems it was the trainer we used, axolotl. It has been fixed in the trainer
          but still dont know how to fix it here</p>

          '
        raw: "> > Same problem here.\n> > config.json says \"vocab_size\": 32032\n\
          > > while largest id in tokenizer.json is 32000\n> > \n> > Does anyone know\
          \ how to solve this?\n> \n> You can add 32 dummy tokens to added_tokens.json\
          \ to make it match the tensor size. Not sure the reason it's set up like\
          \ this.\n\nSeems it was the trainer we used, axolotl. It has been fixed\
          \ in the trainer but still dont know how to fix it here"
        updatedAt: '2023-07-29T10:35:00.723Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c4eb54d07620bdc9a14554
    id: 64c4eb54d07620bdc9a14553
    type: comment
  author: teknium
  content: "> > Same problem here.\n> > config.json says \"vocab_size\": 32032\n>\
    \ > while largest id in tokenizer.json is 32000\n> > \n> > Does anyone know how\
    \ to solve this?\n> \n> You can add 32 dummy tokens to added_tokens.json to make\
    \ it match the tensor size. Not sure the reason it's set up like this.\n\nSeems\
    \ it was the trainer we used, axolotl. It has been fixed in the trainer but still\
    \ dont know how to fix it here"
  created_at: 2023-07-29 09:35:00+00:00
  edited: false
  hidden: false
  id: 64c4eb54d07620bdc9a14553
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg?w=200&h=200&f=face
      fullname: Teknium
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: teknium
      type: user
    createdAt: '2023-07-29T10:35:00.000Z'
    data:
      status: closed
    id: 64c4eb54d07620bdc9a14554
    type: status-change
  author: teknium
  created_at: 2023-07-29 09:35:00+00:00
  id: 64c4eb54d07620bdc9a14554
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b8f312d868af9947cd2360f15ecb9ac.svg
      fullname: Bart
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bart-ml-lora
      type: user
    createdAt: '2023-08-11T11:20:43.000Z'
    data:
      edited: true
      editors:
      - bart-ml-lora
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3009042739868164
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b8f312d868af9947cd2360f15ecb9ac.svg
          fullname: Bart
          isHf: false
          isPro: false
          name: bart-ml-lora
          type: user
        html: '<p>Python script to generate valid tokenizer.model:</p>

          <pre><code class="language-python">

          <span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span>
          Path

          <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span>
          load_dataset

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer


          tokenizer_model_name = <span class="hljs-string">''NousResearch/Llama-2-7b-hf''</span>

          model_path = <span class="hljs-string">''output''</span>

          new_tokens = [<span class="hljs-string">f"&lt;pad<span class="hljs-subst">{i}</span>&gt;"</span>
          <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>
          <span class="hljs-built_in">range</span>(<span class="hljs-number">31</span>)]


          tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name, trust_remote_code=<span
          class="hljs-literal">True</span>)

          tokenizer.pad_token = tokenizer.eos_token

          tokenizer.padding_side = <span class="hljs-string">"right"</span>


          tokenizer.add_tokens(new_tokens)


          tokenizer.save_pretrained(Path(model_path))

          tokenizer.save_vocabulary(model_path)

          </code></pre>

          '
        raw: 'Python script to generate valid tokenizer.model:


          ```python


          from pathlib import Path

          from datasets import load_dataset

          from transformers import AutoTokenizer


          tokenizer_model_name = ''NousResearch/Llama-2-7b-hf''

          model_path = ''output''

          new_tokens = [f"<pad{i}>" for i in range(31)]


          tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name, trust_remote_code=True)

          tokenizer.pad_token = tokenizer.eos_token

          tokenizer.padding_side = "right"


          tokenizer.add_tokens(new_tokens)


          tokenizer.save_pretrained(Path(model_path))

          tokenizer.save_vocabulary(model_path)


          ```'
        updatedAt: '2023-08-11T11:21:05.864Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - markush1
    id: 64d6198b36b05ef8ffc26b9f
    type: comment
  author: bart-ml-lora
  content: 'Python script to generate valid tokenizer.model:


    ```python


    from pathlib import Path

    from datasets import load_dataset

    from transformers import AutoTokenizer


    tokenizer_model_name = ''NousResearch/Llama-2-7b-hf''

    model_path = ''output''

    new_tokens = [f"<pad{i}>" for i in range(31)]


    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name, trust_remote_code=True)

    tokenizer.pad_token = tokenizer.eos_token

    tokenizer.padding_side = "right"


    tokenizer.add_tokens(new_tokens)


    tokenizer.save_pretrained(Path(model_path))

    tokenizer.save_vocabulary(model_path)


    ```'
  created_at: 2023-08-11 10:20:43+00:00
  edited: true
  hidden: false
  id: 64d6198b36b05ef8ffc26b9f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NousResearch/Nous-Hermes-Llama2-13b
repo_type: model
status: closed
target_branch: null
title: Error during quantization
