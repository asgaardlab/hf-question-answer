!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KnutJaegersberg
conflicting_files: null
created_at: 2024-01-17 16:45:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-17T16:45:02.000Z'
    data:
      edited: true
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.963668704032898
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Hi, I''ve tried to simply change the config and tokenizer config
          of this version of the model to llama model, without changing anything else.<br>Inference
          worked locally and looked reasonable and I could submit the model to the
          openllm leaderboard, but the average score was way too low for what it should
          have achieved. I''ve deleted that version of the model already.<br>I suspect
          it was not properly llamafied, perhaps the tokenizer introducing noise.
          Or it could also be that llamafication only works with the 4k token context
          model. How do you think can we llamafy this model without performance decrease?
          It would be great to speed up inference with exllama. </p>

          '
        raw: "Hi, I've tried to simply change the config and tokenizer config of this\
          \ version of the model to llama model, without changing anything else. \n\
          Inference worked locally and looked reasonable and I could submit the model\
          \ to the openllm leaderboard, but the average score was way too low for\
          \ what it should have achieved. I've deleted that version of the model already.\
          \ \nI suspect it was not properly llamafied, perhaps the tokenizer introducing\
          \ noise. Or it could also be that llamafication only works with the 4k token\
          \ context model. How do you think can we llamafy this model without performance\
          \ decrease? It would be great to speed up inference with exllama. "
        updatedAt: '2024-01-17T16:46:02.994Z'
      numEdits: 1
      reactions: []
    id: 65a8040e2a20de2d7afc417c
    type: comment
  author: KnutJaegersberg
  content: "Hi, I've tried to simply change the config and tokenizer config of this\
    \ version of the model to llama model, without changing anything else. \nInference\
    \ worked locally and looked reasonable and I could submit the model to the openllm\
    \ leaderboard, but the average score was way too low for what it should have achieved.\
    \ I've deleted that version of the model already. \nI suspect it was not properly\
    \ llamafied, perhaps the tokenizer introducing noise. Or it could also be that\
    \ llamafication only works with the 4k token context model. How do you think can\
    \ we llamafy this model without performance decrease? It would be great to speed\
    \ up inference with exllama. "
  created_at: 2024-01-17 16:45:02+00:00
  edited: true
  hidden: false
  id: 65a8040e2a20de2d7afc417c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e0881e0908bc00ac728489689497e83.svg
      fullname: Ran Le
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leran1995
      type: user
    createdAt: '2024-01-19T06:59:29.000Z'
    data:
      edited: false
      editors:
      - leran1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446736574172974
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e0881e0908bc00ac728489689497e83.svg
          fullname: Ran Le
          isHf: false
          isPro: false
          name: leran1995
          type: user
        html: "<p>Hi, thanks for your interest in our 32k-Base model.<br>Recently,\
          \ we noticed someone has llamafied our Base model and uploaded it to the\
          \ OpenLLM leaderboard.<br><a href=\"https://huggingface.co/duoqi/Nanbeige-16B-Base-Llama/tree/main\"\
          >https://huggingface.co/duoqi/Nanbeige-16B-Base-Llama/tree/main</a>.</p>\n\
          <p>It seems that all you need to do is modify the config.json and tokenizer.config\
          \ files.<br>We think llamafication could work for both the 4k and 32k versions,\
          \ and the OpenLLM average score of the 32k version should be comparable\
          \ (maybe a little worse) to the 4k version.</p>\n<p>BTW, we are planning\
          \ to release the next generation of the Nanbeige series models soon, which\
          \ could have better performance and could handle longer context length \U0001F60A\
          \ </p>\n"
        raw: "Hi, thanks for your interest in our 32k-Base model. \nRecently, we noticed\
          \ someone has llamafied our Base model and uploaded it to the OpenLLM leaderboard.\n\
          https://huggingface.co/duoqi/Nanbeige-16B-Base-Llama/tree/main.\n\nIt seems\
          \ that all you need to do is modify the config.json and tokenizer.config\
          \ files.\nWe think llamafication could work for both the 4k and 32k versions,\
          \ and the OpenLLM average score of the 32k version should be comparable\
          \ (maybe a little worse) to the 4k version.\n\nBTW, we are planning to release\
          \ the next generation of the Nanbeige series models soon, which could have\
          \ better performance and could handle longer context length \U0001F60A "
        updatedAt: '2024-01-19T06:59:29.839Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - KnutJaegersberg
    id: 65aa1dd16a55aac02a8018ac
    type: comment
  author: leran1995
  content: "Hi, thanks for your interest in our 32k-Base model. \nRecently, we noticed\
    \ someone has llamafied our Base model and uploaded it to the OpenLLM leaderboard.\n\
    https://huggingface.co/duoqi/Nanbeige-16B-Base-Llama/tree/main.\n\nIt seems that\
    \ all you need to do is modify the config.json and tokenizer.config files.\nWe\
    \ think llamafication could work for both the 4k and 32k versions, and the OpenLLM\
    \ average score of the 32k version should be comparable (maybe a little worse)\
    \ to the 4k version.\n\nBTW, we are planning to release the next generation of\
    \ the Nanbeige series models soon, which could have better performance and could\
    \ handle longer context length \U0001F60A "
  created_at: 2024-01-19 06:59:29+00:00
  edited: false
  hidden: false
  id: 65aa1dd16a55aac02a8018ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-19T07:51:16.000Z'
    data:
      edited: false
      editors:
      - KnutJaegersberg
      hidden: false
      identifiedLanguage:
        language: hu
        probability: 0.19420602917671204
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
          fullname: "Knut J\xE4gersberg"
          isHf: false
          isPro: false
          name: KnutJaegersberg
          type: user
        html: '<p>Great! </p>

          '
        raw: 'Great! '
        updatedAt: '2024-01-19T07:51:16.458Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65aa29f4356bf23b4a4d08d4
    id: 65aa29f4356bf23b4a4d08d3
    type: comment
  author: KnutJaegersberg
  content: 'Great! '
  created_at: 2024-01-19 07:51:16+00:00
  edited: false
  hidden: false
  id: 65aa29f4356bf23b4a4d08d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669551186189-63732ebbbd81fae2b3aaf3fb.jpeg?w=200&h=200&f=face
      fullname: "Knut J\xE4gersberg"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KnutJaegersberg
      type: user
    createdAt: '2024-01-19T07:51:16.000Z'
    data:
      status: closed
    id: 65aa29f4356bf23b4a4d08d4
    type: status-change
  author: KnutJaegersberg
  created_at: 2024-01-19 07:51:16+00:00
  id: 65aa29f4356bf23b4a4d08d4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Nanbeige/Nanbeige-16B-Base-32K
repo_type: model
status: closed
target_branch: null
title: Llamafied model
