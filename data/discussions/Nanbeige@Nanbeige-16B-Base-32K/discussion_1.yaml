!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ddh0
conflicting_files: null
created_at: 2023-11-18 23:37:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc5b4512d00c4589dfa8a6/0U8IW9ZKT5crAIR30sLP_.png?w=200&h=200&f=face
      fullname: ddh0
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ddh0
      type: user
    createdAt: '2023-11-18T23:37:14.000Z'
    data:
      edited: false
      editors:
      - ddh0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7609875202178955
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64bc5b4512d00c4589dfa8a6/0U8IW9ZKT5crAIR30sLP_.png?w=200&h=200&f=face
          fullname: ddh0
          isHf: false
          isPro: false
          name: ddh0
          type: user
        html: "<p>Hello! I'm getting an error when trying to convert this to GGUF.\
          \ I get the same exact error on the Chat variant of the model too.</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
          , line 1208, in &lt;module&gt;\n    main()\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
          , line 1203, in main\n    OutputFile.write_all(outfile, ftype, params, model,\
          \ vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)\n\
          \  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\", line 908,\
          \ in write_all\n    check_vocab_size(params, vocab)\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
          , line 795, in check_vocab_size\n    raise Exception(msg)\nException: Vocab\
          \ size mismatch (model has 59392, but Nanbeige-16B-Base-32K/tokenizer.model\
          \ has 58978).\n</code></pre>\n<p>If there's anything I'm doing wrong I'd\
          \ appreciate some guidance.</p>\n<p>Also, in config.json, <code>max_position_embeddings</code>\
          \ is 4096. Shouldn't it be 32768, if the context length is 32k? This is\
          \ also the same for this model and the Chat model.</p>\n<p>Thanks in advance!</p>\n"
        raw: "Hello! I'm getting an error when trying to convert this to GGUF. I get\
          \ the same exact error on the Chat variant of the model too.\r\n```\r\n\
          Traceback (most recent call last):\r\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
          , line 1208, in <module>\r\n    main()\r\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
          , line 1203, in main\r\n    OutputFile.write_all(outfile, ftype, params,\
          \ model, vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)\r\
          \n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\", line 908,\
          \ in write_all\r\n    check_vocab_size(params, vocab)\r\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
          , line 795, in check_vocab_size\r\n    raise Exception(msg)\r\nException:\
          \ Vocab size mismatch (model has 59392, but Nanbeige-16B-Base-32K/tokenizer.model\
          \ has 58978).\r\n```\r\n\r\nIf there's anything I'm doing wrong I'd appreciate\
          \ some guidance.\r\n\r\nAlso, in config.json, `max_position_embeddings`\
          \ is 4096. Shouldn't it be 32768, if the context length is 32k? This is\
          \ also the same for this model and the Chat model.\r\n\r\nThanks in advance!"
        updatedAt: '2023-11-18T23:37:14.080Z'
      numEdits: 0
      reactions: []
    id: 65594aaae0a6202d364bf817
    type: comment
  author: ddh0
  content: "Hello! I'm getting an error when trying to convert this to GGUF. I get\
    \ the same exact error on the Chat variant of the model too.\r\n```\r\nTraceback\
    \ (most recent call last):\r\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
    , line 1208, in <module>\r\n    main()\r\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
    , line 1203, in main\r\n    OutputFile.write_all(outfile, ftype, params, model,\
    \ vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)\r\
    \n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\", line 908, in write_all\r\
    \n    check_vocab_size(params, vocab)\r\n  File \"/Users/dylan/Documents/AI/./llama.cpp/convert.py\"\
    , line 795, in check_vocab_size\r\n    raise Exception(msg)\r\nException: Vocab\
    \ size mismatch (model has 59392, but Nanbeige-16B-Base-32K/tokenizer.model has\
    \ 58978).\r\n```\r\n\r\nIf there's anything I'm doing wrong I'd appreciate some\
    \ guidance.\r\n\r\nAlso, in config.json, `max_position_embeddings` is 4096. Shouldn't\
    \ it be 32768, if the context length is 32k? This is also the same for this model\
    \ and the Chat model.\r\n\r\nThanks in advance!"
  created_at: 2023-11-18 23:37:14+00:00
  edited: false
  hidden: false
  id: 65594aaae0a6202d364bf817
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e0881e0908bc00ac728489689497e83.svg
      fullname: Ran Le
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: leran1995
      type: user
    createdAt: '2023-11-20T02:50:42.000Z'
    data:
      edited: false
      editors:
      - leran1995
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7676482200622559
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e0881e0908bc00ac728489689497e83.svg
          fullname: Ran Le
          isHf: false
          isPro: false
          name: leran1995
          type: user
        html: '<p>Hi! Thanks for your concern about our 32k model.</p>

          <p>For the vocab-size issue: we pad the vocab-size of each tp part to the
          times of 128,  which increases the embedding num from 58978 to 59392 in
          our 32k model (both 32k-Base and 32k-Chat). The indexes from 58978 to 59391
          are padding tokens.<br>For the max_position_embeddings issue: we leverage
          the yarn (<a rel="nofollow" href="https://arxiv.org/pdf/2309.00071.pdf">https://arxiv.org/pdf/2309.00071.pdf</a>)
          method in our modeling_nanbeige.py for long-context extension. The original
          max_position_embeddings is 4096 and the yarn-scale is 8 (as referenced in
          config.json), thus the context-length is 32k.</p>

          <p>Hope this will help you :)</p>

          '
        raw: 'Hi! Thanks for your concern about our 32k model.


          For the vocab-size issue: we pad the vocab-size of each tp part to the times
          of 128,  which increases the embedding num from 58978 to 59392 in our 32k
          model (both 32k-Base and 32k-Chat). The indexes from 58978 to 59391 are
          padding tokens.

          For the max_position_embeddings issue: we leverage the yarn (https://arxiv.org/pdf/2309.00071.pdf)
          method in our modeling_nanbeige.py for long-context extension. The original
          max_position_embeddings is 4096 and the yarn-scale is 8 (as referenced in
          config.json), thus the context-length is 32k.


          Hope this will help you :)

          '
        updatedAt: '2023-11-20T02:50:42.119Z'
      numEdits: 0
      reactions: []
    id: 655ac9824cd8d44865f1768a
    type: comment
  author: leran1995
  content: 'Hi! Thanks for your concern about our 32k model.


    For the vocab-size issue: we pad the vocab-size of each tp part to the times of
    128,  which increases the embedding num from 58978 to 59392 in our 32k model (both
    32k-Base and 32k-Chat). The indexes from 58978 to 59391 are padding tokens.

    For the max_position_embeddings issue: we leverage the yarn (https://arxiv.org/pdf/2309.00071.pdf)
    method in our modeling_nanbeige.py for long-context extension. The original max_position_embeddings
    is 4096 and the yarn-scale is 8 (as referenced in config.json), thus the context-length
    is 32k.


    Hope this will help you :)

    '
  created_at: 2023-11-20 02:50:42+00:00
  edited: false
  hidden: false
  id: 655ac9824cd8d44865f1768a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Nanbeige/Nanbeige-16B-Base-32K
repo_type: model
status: open
target_branch: null
title: Vocab size mismatch between model and tokenizer model
