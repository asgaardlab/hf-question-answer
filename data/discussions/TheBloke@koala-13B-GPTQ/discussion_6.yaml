!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deeplypeppermint
conflicting_files: null
created_at: 2023-07-10 08:45:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7e302622baeec2141156fdfbc9cea34.svg
      fullname: Marcelo L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deeplypeppermint
      type: user
    createdAt: '2023-07-10T09:45:50.000Z'
    data:
      edited: false
      editors:
      - deeplypeppermint
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.2985842823982239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7e302622baeec2141156fdfbc9cea34.svg
          fullname: Marcelo L
          isHf: false
          isPro: false
          name: deeplypeppermint
          type: user
        html: "<p>Hi, I've been having some trouble testing this model on google colab\
          \ free. There seems to be an error when I try to run AutoGPTQForCausalLM.from_quantized.\
          \ Could anyone please help?</p>\n<p>My code:</p>\n<p>!pip install transformers\
          \ accelerate einops sentencepiece<br>!git clone <a rel=\"nofollow\" href=\"\
          https://github.com/PanQiWei/AutoGPTQ\">https://github.com/PanQiWei/AutoGPTQ</a><br>!pip\
          \ install ./AutoGPTQ/<br>import torch<br>from transformers import AutoTokenizer<br>from\
          \ auto_gptq import AutoGPTQForCausalLM<br>model_name_or_path = \"TheBloke/koala-13B-GPTQ-4bit-128g\"\
          <br>model_basename = \"koala-13B-4bit-128g.safetensors\"<br>tokenizer =\
          \ AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)<br>model\
          \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path, model_basename=model_basename,\
          \ device=\"cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.float32,\
          \ trust_remote_code=False)</p>\n<p>My error:</p>\n<p>in &lt;cell line: 3&gt;:3\
          \                                                                      \
          \        \u2502<br>\u2502                                              \
          \                                                    \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:85\
          \ in from_quantized          \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502    82 \u2502   \u2502   model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_path, trust_remo   \u2502<br>\u2502    83 \u2502   \u2502\
          \   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized   \
          \                \u2502<br>\u2502    84 \u2502   \u2502   keywords = {key:\
          \ kwargs[key] for key in signature(quant_func).parameters if key    \u2502\
          <br>\u2502 \u2771  85 \u2502   \u2502   return quant_func(             \
          \                                                    \u2502<br>\u2502  \
          \  86 \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,\
          \                                         \u2502<br>\u2502    87 \u2502\
          \   \u2502   \u2502   save_dir=save_dir,                               \
          \                              \u2502<br>\u2502    88 \u2502   \u2502  \
          \ \u2502   device_map=device_map,                                      \
          \                   \u2502<br>\u2502                                   \
          \                                                               \u2502<br>\u2502\
          \ /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:666\
          \ in from_quantized        \u2502<br>\u2502                            \
          \                                                                      \u2502\
          <br>\u2502   663 \u2502   \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                   \u2502<br>\u2502   664 \u2502\
          \   \u2502                                                             \
          \                         \u2502<br>\u2502   665 \u2502   \u2502   if quantize_config\
          \ is None:                                                        \u2502\
          <br>\u2502 \u2771 666 \u2502   \u2502   \u2502   quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path,\
          \ **k   \u2502<br>\u2502   667 \u2502   \u2502                         \
          \                                                             \u2502<br>\u2502\
          \   668 \u2502   \u2502   if model_basename is None:                   \
          \                                      \u2502<br>\u2502   669 \u2502   \u2502\
          \   \u2502   if quantize_config.model_file_base_name:                  \
          \                     \u2502<br>\u2502                                 \
          \                                                                 \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:90\
          \ in from_pretrained        \u2502<br>\u2502                           \
          \                                                                      \
          \ \u2502<br>\u2502    87 \u2502   \u2502   \u2502   \u2502   \u2502   _commit_hash=commit_hash,\
          \                                              \u2502<br>\u2502    88 \u2502\
          \   \u2502   \u2502      )                                             \
          \                              \u2502<br>\u2502    89 \u2502   \u2502  \
          \                                                                      \
          \              \u2502<br>\u2502 \u2771  90 \u2502   \u2502   with open(resolved_config_file,\
          \ \"r\", encoding=\"utf-8\") as f:                       \u2502<br>\u2502\
          \    91 \u2502   \u2502   \u2502   return cls(**json.load(f))          \
          \                                           \u2502<br>\u2502    92 \u2502\
          \                                                                      \
          \                    \u2502<br>\u2502    93 \u2502   def to_dict(self):\
          \                                                                     \u2502\
          <br>\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F<br>TypeError: expected str, bytes or os.PathLike\
          \ object, not NoneType</p>\n"
        raw: "Hi, I've been having some trouble testing this model on google colab\
          \ free. There seems to be an error when I try to run AutoGPTQForCausalLM.from_quantized.\
          \ Could anyone please help?\r\n\r\nMy code:\r\n\r\n!pip install transformers\
          \ accelerate einops sentencepiece\r\n!git clone https://github.com/PanQiWei/AutoGPTQ\r\
          \n!pip install ./AutoGPTQ/\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\
          \nfrom auto_gptq import AutoGPTQForCausalLM\r\nmodel_name_or_path = \"TheBloke/koala-13B-GPTQ-4bit-128g\"\
          \r\nmodel_basename = \"koala-13B-4bit-128g.safetensors\"\r\ntokenizer =\
          \ AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\r\n\
          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path, model_basename=model_basename,\
          \ device=\"cuda:0\", use_triton=False, use_safetensors=True, torch_dtype=torch.float32,\
          \ trust_remote_code=False)\r\n\r\n\r\nMy error:\r\n\r\nin <cell line: 3>:3\
          \                                                                      \
          \        \u2502\r\n\u2502                                              \
          \                                                    \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:85\
          \ in from_quantized          \u2502\r\n\u2502                          \
          \                                                                      \
          \  \u2502\r\n\u2502    82 \u2502   \u2502   model_type = check_and_get_model_type(save_dir\
          \ or model_name_or_path, trust_remo   \u2502\r\n\u2502    83 \u2502   \u2502\
          \   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized   \
          \                \u2502\r\n\u2502    84 \u2502   \u2502   keywords = {key:\
          \ kwargs[key] for key in signature(quant_func).parameters if key    \u2502\
          \r\n\u2502 \u2771  85 \u2502   \u2502   return quant_func(             \
          \                                                    \u2502\r\n\u2502  \
          \  86 \u2502   \u2502   \u2502   model_name_or_path=model_name_or_path,\
          \                                         \u2502\r\n\u2502    87 \u2502\
          \   \u2502   \u2502   save_dir=save_dir,                               \
          \                              \u2502\r\n\u2502    88 \u2502   \u2502  \
          \ \u2502   device_map=device_map,                                      \
          \                   \u2502\r\n\u2502                                   \
          \                                                               \u2502\r\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:666\
          \ in from_quantized        \u2502\r\n\u2502                            \
          \                                                                      \u2502\
          \r\n\u2502   663 \u2502   \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                   \u2502\r\n\u2502   664 \u2502\
          \   \u2502                                                             \
          \                         \u2502\r\n\u2502   665 \u2502   \u2502   if quantize_config\
          \ is None:                                                        \u2502\
          \r\n\u2502 \u2771 666 \u2502   \u2502   \u2502   quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path,\
          \ **k   \u2502\r\n\u2502   667 \u2502   \u2502                         \
          \                                                             \u2502\r\n\
          \u2502   668 \u2502   \u2502   if model_basename is None:              \
          \                                           \u2502\r\n\u2502   669 \u2502\
          \   \u2502   \u2502   if quantize_config.model_file_base_name:         \
          \                              \u2502\r\n\u2502                        \
          \                                                                      \
          \    \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:90\
          \ in from_pretrained        \u2502\r\n\u2502                           \
          \                                                                      \
          \ \u2502\r\n\u2502    87 \u2502   \u2502   \u2502   \u2502   \u2502   _commit_hash=commit_hash,\
          \                                              \u2502\r\n\u2502    88 \u2502\
          \   \u2502   \u2502      )                                             \
          \                              \u2502\r\n\u2502    89 \u2502   \u2502  \
          \                                                                      \
          \              \u2502\r\n\u2502 \u2771  90 \u2502   \u2502   with open(resolved_config_file,\
          \ \"r\", encoding=\"utf-8\") as f:                       \u2502\r\n\u2502\
          \    91 \u2502   \u2502   \u2502   return cls(**json.load(f))          \
          \                                           \u2502\r\n\u2502    92 \u2502\
          \                                                                      \
          \                    \u2502\r\n\u2502    93 \u2502   def to_dict(self):\
          \                                                                     \u2502\
          \r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F\r\nTypeError: expected str, bytes or os.PathLike\
          \ object, not NoneType\r\n"
        updatedAt: '2023-07-10T09:45:50.216Z'
      numEdits: 0
      reactions: []
    id: 64abd34e73790912c7b48586
    type: comment
  author: deeplypeppermint
  content: "Hi, I've been having some trouble testing this model on google colab free.\
    \ There seems to be an error when I try to run AutoGPTQForCausalLM.from_quantized.\
    \ Could anyone please help?\r\n\r\nMy code:\r\n\r\n!pip install transformers accelerate\
    \ einops sentencepiece\r\n!git clone https://github.com/PanQiWei/AutoGPTQ\r\n\
    !pip install ./AutoGPTQ/\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\
    \nfrom auto_gptq import AutoGPTQForCausalLM\r\nmodel_name_or_path = \"TheBloke/koala-13B-GPTQ-4bit-128g\"\
    \r\nmodel_basename = \"koala-13B-4bit-128g.safetensors\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=False)\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
    \ model_basename=model_basename, device=\"cuda:0\", use_triton=False, use_safetensors=True,\
    \ torch_dtype=torch.float32, trust_remote_code=False)\r\n\r\n\r\nMy error:\r\n\
    \r\nin <cell line: 3>:3                                                      \
    \                        \u2502\r\n\u2502                                    \
    \                                                              \u2502\r\n\u2502\
    \ /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:85 in from_quantized\
    \          \u2502\r\n\u2502                                                  \
    \                                                \u2502\r\n\u2502    82 \u2502\
    \   \u2502   model_type = check_and_get_model_type(save_dir or model_name_or_path,\
    \ trust_remo   \u2502\r\n\u2502    83 \u2502   \u2502   quant_func = GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized\
    \                   \u2502\r\n\u2502    84 \u2502   \u2502   keywords = {key:\
    \ kwargs[key] for key in signature(quant_func).parameters if key    \u2502\r\n\
    \u2502 \u2771  85 \u2502   \u2502   return quant_func(                       \
    \                                          \u2502\r\n\u2502    86 \u2502   \u2502\
    \   \u2502   model_name_or_path=model_name_or_path,                          \
    \               \u2502\r\n\u2502    87 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                             \u2502\r\n\u2502\
    \    88 \u2502   \u2502   \u2502   device_map=device_map,                    \
    \                                     \u2502\r\n\u2502                       \
    \                                                                           \u2502\
    \r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:666\
    \ in from_quantized        \u2502\r\n\u2502                                  \
    \                                                                \u2502\r\n\u2502\
    \   663 \u2502   \u2502   \u2502   raise TypeError(f\"{config.model_type} isn't\
    \ supported yet.\")                   \u2502\r\n\u2502   664 \u2502   \u2502 \
    \                                                                            \
    \         \u2502\r\n\u2502   665 \u2502   \u2502   if quantize_config is None:\
    \                                                        \u2502\r\n\u2502 \u2771\
    \ 666 \u2502   \u2502   \u2502   quantize_config = BaseQuantizeConfig.from_pretrained(model_name_or_path,\
    \ **k   \u2502\r\n\u2502   667 \u2502   \u2502                               \
    \                                                       \u2502\r\n\u2502   668\
    \ \u2502   \u2502   if model_basename is None:                               \
    \                          \u2502\r\n\u2502   669 \u2502   \u2502   \u2502   if\
    \ quantize_config.model_file_base_name:                                      \
    \ \u2502\r\n\u2502                                                           \
    \                                       \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:90\
    \ in from_pretrained        \u2502\r\n\u2502                                 \
    \                                                                 \u2502\r\n\u2502\
    \    87 \u2502   \u2502   \u2502   \u2502   \u2502   _commit_hash=commit_hash,\
    \                                              \u2502\r\n\u2502    88 \u2502 \
    \  \u2502   \u2502      )                                                    \
    \                       \u2502\r\n\u2502    89 \u2502   \u2502               \
    \                                                                       \u2502\
    \r\n\u2502 \u2771  90 \u2502   \u2502   with open(resolved_config_file, \"r\"\
    , encoding=\"utf-8\") as f:                       \u2502\r\n\u2502    91 \u2502\
    \   \u2502   \u2502   return cls(**json.load(f))                             \
    \                        \u2502\r\n\u2502    92 \u2502                       \
    \                                                                   \u2502\r\n\
    \u2502    93 \u2502   def to_dict(self):                                     \
    \                                \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u256F\r\nTypeError: expected str, bytes or os.PathLike object, not\
    \ NoneType\r\n"
  created_at: 2023-07-10 08:45:50+00:00
  edited: false
  hidden: false
  id: 64abd34e73790912c7b48586
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
      fullname: Boddu Surya Venkat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLconArtist
      type: user
    createdAt: '2023-08-30T04:06:03.000Z'
    data:
      edited: true
      editors:
      - MLconArtist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.960829496383667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
          fullname: Boddu Surya Venkat
          isHf: false
          isPro: false
          name: MLconArtist
          type: user
        html: '<p>Hey, so did you find a way to run gptq models on colab?</p>

          '
        raw: Hey, so did you find a way to run gptq models on colab?
        updatedAt: '2023-08-30T04:06:09.993Z'
      numEdits: 1
      reactions: []
    id: 64eec02bc392a3e66be106af
    type: comment
  author: MLconArtist
  content: Hey, so did you find a way to run gptq models on colab?
  created_at: 2023-08-30 03:06:03+00:00
  edited: true
  hidden: false
  id: 64eec02bc392a3e66be106af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7e302622baeec2141156fdfbc9cea34.svg
      fullname: Marcelo L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deeplypeppermint
      type: user
    createdAt: '2023-08-30T07:41:02.000Z'
    data:
      edited: false
      editors:
      - deeplypeppermint
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.926712691783905
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7e302622baeec2141156fdfbc9cea34.svg
          fullname: Marcelo L
          isHf: false
          isPro: false
          name: deeplypeppermint
          type: user
        html: '<p>If you are only looking to do inference I was able to run some models
          on Colab using very simple code taken from other sources:</p>

          <p><a rel="nofollow" href="https://colab.research.google.com/drive/1rqLLYCoD4YlcSkzVp_b0NKpHxUBe3a2S?usp=sharing">https://colab.research.google.com/drive/1rqLLYCoD4YlcSkzVp_b0NKpHxUBe3a2S?usp=sharing</a>
          </p>

          <p>I was able to run Falcon (very slow) and Vicuna but not Koala</p>

          '
        raw: "If you are only looking to do inference I was able to run some models\
          \ on Colab using very simple code taken from other sources:\n\nhttps://colab.research.google.com/drive/1rqLLYCoD4YlcSkzVp_b0NKpHxUBe3a2S?usp=sharing\
          \ \n\nI was able to run Falcon (very slow) and Vicuna but not Koala\n\n"
        updatedAt: '2023-08-30T07:41:02.847Z'
      numEdits: 0
      reactions: []
    id: 64eef28e990c88cd7789b7f0
    type: comment
  author: deeplypeppermint
  content: "If you are only looking to do inference I was able to run some models\
    \ on Colab using very simple code taken from other sources:\n\nhttps://colab.research.google.com/drive/1rqLLYCoD4YlcSkzVp_b0NKpHxUBe3a2S?usp=sharing\
    \ \n\nI was able to run Falcon (very slow) and Vicuna but not Koala\n\n"
  created_at: 2023-08-30 06:41:02+00:00
  edited: false
  hidden: false
  id: 64eef28e990c88cd7789b7f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
      fullname: Boddu Surya Venkat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLconArtist
      type: user
    createdAt: '2023-08-30T07:45:44.000Z'
    data:
      edited: false
      editors:
      - MLconArtist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619046449661255
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
          fullname: Boddu Surya Venkat
          isHf: false
          isPro: false
          name: MLconArtist
          type: user
        html: '<p>Thanks but I figured out a way to run llama-2 on colab. It was actually
          pretty fast</p>

          '
        raw: Thanks but I figured out a way to run llama-2 on colab. It was actually
          pretty fast
        updatedAt: '2023-08-30T07:45:44.776Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - deeplypeppermint
    id: 64eef3a8069c3f6f78e8fd4a
    type: comment
  author: MLconArtist
  content: Thanks but I figured out a way to run llama-2 on colab. It was actually
    pretty fast
  created_at: 2023-08-30 06:45:44+00:00
  edited: false
  hidden: false
  id: 64eef3a8069c3f6f78e8fd4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7e302622baeec2141156fdfbc9cea34.svg
      fullname: Marcelo L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deeplypeppermint
      type: user
    createdAt: '2023-08-30T07:47:34.000Z'
    data:
      edited: false
      editors:
      - deeplypeppermint
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9738972187042236
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7e302622baeec2141156fdfbc9cea34.svg
          fullname: Marcelo L
          isHf: false
          isPro: false
          name: deeplypeppermint
          type: user
        html: '<p>Nice! I haven''t tried llama-2 yet but If you don''t mind sharing
          your source it would help me a lot</p>

          '
        raw: Nice! I haven't tried llama-2 yet but If you don't mind sharing your
          source it would help me a lot
        updatedAt: '2023-08-30T07:47:34.759Z'
      numEdits: 0
      reactions: []
    id: 64eef416b95aad4be440b222
    type: comment
  author: deeplypeppermint
  content: Nice! I haven't tried llama-2 yet but If you don't mind sharing your source
    it would help me a lot
  created_at: 2023-08-30 06:47:34+00:00
  edited: false
  hidden: false
  id: 64eef416b95aad4be440b222
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/koala-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Running this model on colab
