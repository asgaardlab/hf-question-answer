!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Handgun1773
conflicting_files: null
created_at: 2024-01-25 09:54:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b03217c22442b7bfed9beac2bf50d17.svg
      fullname: Alex Daminger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Handgun1773
      type: user
    createdAt: '2024-01-25T09:54:35.000Z'
    data:
      edited: false
      editors:
      - Handgun1773
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9588814377784729
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b03217c22442b7bfed9beac2bf50d17.svg
          fullname: Alex Daminger
          isHf: false
          isPro: false
          name: Handgun1773
          type: user
        html: '<p>I''m not sure that you are using positive and negative prompts properly.<br>From
          what I understood, they are a hack to bypass training the router. Based
          on the user prompt and cosine similarity, they will route the token to one
          or another model of the frankenmoe. The goal is to have let''s say a math
          expert and a code expert, if the prompt has more math vocabulary and the
          positive prompts of the math expert are math vocabulary, he will be choosen
          to handle the request.</p>

          <p>I''m not sure if it changes the route for each token or chooses expert
          once at the beginning of the request.</p>

          <p>Anyway, from what you''ve put in your parameters, it seems you misunderstood
          how they work. But maybe I misunderstood. Anyway, it is always a great subject
          of discussion, because a lot of people seem to not get how MOEs work at
          all. Original mixtral has a router that select 2 of the 7 experts for each
          layer, and each token, and after analysis, there isn''t a math expert, a
          physic expert etc, it''s way more nuanced than that.</p>

          '
        raw: "I'm not sure that you are using positive and negative prompts properly.\r\
          \nFrom what I understood, they are a hack to bypass training the router.\
          \ Based on the user prompt and cosine similarity, they will route the token\
          \ to one or another model of the frankenmoe. The goal is to have let's say\
          \ a math expert and a code expert, if the prompt has more math vocabulary\
          \ and the positive prompts of the math expert are math vocabulary, he will\
          \ be choosen to handle the request.\r\n\r\nI'm not sure if it changes the\
          \ route for each token or chooses expert once at the beginning of the request.\r\
          \n\r\nAnyway, from what you've put in your parameters, it seems you misunderstood\
          \ how they work. But maybe I misunderstood. Anyway, it is always a great\
          \ subject of discussion, because a lot of people seem to not get how MOEs\
          \ work at all. Original mixtral has a router that select 2 of the 7 experts\
          \ for each layer, and each token, and after analysis, there isn't a math\
          \ expert, a physic expert etc, it's way more nuanced than that."
        updatedAt: '2024-01-25T09:54:35.839Z'
      numEdits: 0
      reactions: []
    id: 65b22fdb54fa188a2fdc7bb4
    type: comment
  author: Handgun1773
  content: "I'm not sure that you are using positive and negative prompts properly.\r\
    \nFrom what I understood, they are a hack to bypass training the router. Based\
    \ on the user prompt and cosine similarity, they will route the token to one or\
    \ another model of the frankenmoe. The goal is to have let's say a math expert\
    \ and a code expert, if the prompt has more math vocabulary and the positive prompts\
    \ of the math expert are math vocabulary, he will be choosen to handle the request.\r\
    \n\r\nI'm not sure if it changes the route for each token or chooses expert once\
    \ at the beginning of the request.\r\n\r\nAnyway, from what you've put in your\
    \ parameters, it seems you misunderstood how they work. But maybe I misunderstood.\
    \ Anyway, it is always a great subject of discussion, because a lot of people\
    \ seem to not get how MOEs work at all. Original mixtral has a router that select\
    \ 2 of the 7 experts for each layer, and each token, and after analysis, there\
    \ isn't a math expert, a physic expert etc, it's way more nuanced than that."
  created_at: 2024-01-25 09:54:35+00:00
  edited: false
  hidden: false
  id: 65b22fdb54fa188a2fdc7bb4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rombodawg/deepseek-coder-moe_8x6.7b-base
repo_type: model
status: open
target_branch: null
title: Positive and negative prompts
