!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BrainSlugs83
conflicting_files: null
created_at: 2023-11-27 21:12:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ac777cca059f2e12510475f85dcc19a.svg
      fullname: Mikey Jensen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BrainSlugs83
      type: user
    createdAt: '2023-11-27T21:12:16.000Z'
    data:
      edited: true
      editors:
      - BrainSlugs83
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9565372467041016
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ac777cca059f2e12510475f85dcc19a.svg
          fullname: Mikey Jensen
          isHf: false
          isPro: false
          name: BrainSlugs83
          type: user
        html: '<p>Just trying to use this as a stand in for 3.5, it seems to get very
          poor results.  I''m not testing in any well known benchmarks or anything,
          just trying to chat with it, -- asking it to summarize passages of text,
          etc. -- and it seems to get confused very quickly or give wrong answers
          to simple questions / misunderstand what the user is trying to say. -- Open
          Chat 3.5 (non-16k) behaves much better out of the box.</p>

          <p>Is there a specific format change or specific configuration required
          for this one vs the base?</p>

          <p>I''m using LlamaSharp (a wrapper for llama.cpp) if it helps.</p>

          '
        raw: 'Just trying to use this as a stand in for 3.5, it seems to get very
          poor results.  I''m not testing in any well known benchmarks or anything,
          just trying to chat with it, -- asking it to summarize passages of text,
          etc. -- and it seems to get confused very quickly or give wrong answers
          to simple questions / misunderstand what the user is trying to say. -- Open
          Chat 3.5 (non-16k) behaves much better out of the box.


          Is there a specific format change or specific configuration required for
          this one vs the base?


          I''m using LlamaSharp (a wrapper for llama.cpp) if it helps.'
        updatedAt: '2023-11-27T21:12:54.205Z'
      numEdits: 1
      reactions: []
    id: 6565063074be0169d80be98d
    type: comment
  author: BrainSlugs83
  content: 'Just trying to use this as a stand in for 3.5, it seems to get very poor
    results.  I''m not testing in any well known benchmarks or anything, just trying
    to chat with it, -- asking it to summarize passages of text, etc. -- and it seems
    to get confused very quickly or give wrong answers to simple questions / misunderstand
    what the user is trying to say. -- Open Chat 3.5 (non-16k) behaves much better
    out of the box.


    Is there a specific format change or specific configuration required for this
    one vs the base?


    I''m using LlamaSharp (a wrapper for llama.cpp) if it helps.'
  created_at: 2023-11-27 21:12:16+00:00
  edited: true
  hidden: false
  id: 6565063074be0169d80be98d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-27T23:09:18.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9536466002464294
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>Not sure how llamasharp works, but this was tested using transformers
          and also using the original openchat prompt. Llama.cpp only works with gguf
          files, and this is not that.</p>

          '
        raw: Not sure how llamasharp works, but this was tested using transformers
          and also using the original openchat prompt. Llama.cpp only works with gguf
          files, and this is not that.
        updatedAt: '2023-11-27T23:09:18.434Z'
      numEdits: 0
      reactions: []
    id: 6565219ee5aac326bfaaf13a
    type: comment
  author: perlthoughts
  content: Not sure how llamasharp works, but this was tested using transformers and
    also using the original openchat prompt. Llama.cpp only works with gguf files,
    and this is not that.
  created_at: 2023-11-27 23:09:18+00:00
  edited: false
  hidden: false
  id: 6565219ee5aac326bfaaf13a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-27T23:10:15.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9861815571784973
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>also try with lower temp and high top_p. Hope that helps.</p>

          '
        raw: also try with lower temp and high top_p. Hope that helps.
        updatedAt: '2023-11-27T23:10:15.624Z'
      numEdits: 0
      reactions: []
    id: 656521d7890de10695be204c
    type: comment
  author: perlthoughts
  content: also try with lower temp and high top_p. Hope that helps.
  created_at: 2023-11-27 23:10:15+00:00
  edited: false
  hidden: false
  id: 656521d7890de10695be204c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ac777cca059f2e12510475f85dcc19a.svg
      fullname: Mikey Jensen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BrainSlugs83
      type: user
    createdAt: '2023-11-30T20:22:33.000Z'
    data:
      edited: true
      editors:
      - BrainSlugs83
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9397469162940979
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ac777cca059f2e12510475f85dcc19a.svg
          fullname: Mikey Jensen
          isHf: false
          isPro: false
          name: BrainSlugs83
          type: user
        html: "<blockquote>\n<p>Not sure how llamasharp works, but this was tested\
          \ using transformers and also using the original openchat prompt. Llama.cpp\
          \ only works with gguf files, and this is not that.</p>\n</blockquote>\n\
          <p>Oh, apologies! -- you are correct -- to be clear I'm using <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span>'s gguf conversions for both models at Q8 for comparison.</p>\n\
          <ul>\n<li><a href=\"https://huggingface.co/TheBloke/openchat_3.5-GGUF\"\
          >https://huggingface.co/TheBloke/openchat_3.5-GGUF</a></li>\n<li><a href=\"\
          https://huggingface.co/TheBloke/openchat_3.5-16k-GGUF\">https://huggingface.co/TheBloke/openchat_3.5-16k-GGUF</a></li>\n\
          </ul>\n<p>FWIW, he seems to be \"the guy\" for uploading GGUF conversions\
          \ of popular models, and his model page links back to this one as the source\
          \ for the 16k variant.  But maybe I should check on that page and log an\
          \ issue there first?</p>\n<blockquote>\n<p>also try with lower temp and\
          \ high top_p. Hope that helps.</p>\n</blockquote>\n<p>I'll give it a shot.</p>\n\
          <p>Though another thing just occurred to me -- Is there anything regarding\
          \ yarn / rope scaling that needs to be configured when using this variant\
          \ of the model?</p>\n"
        raw: '> Not sure how llamasharp works, but this was tested using transformers
          and also using the original openchat prompt. Llama.cpp only works with gguf
          files, and this is not that.


          Oh, apologies! -- you are correct -- to be clear I''m using @TheBloke''s
          gguf conversions for both models at Q8 for comparison.

          - https://huggingface.co/TheBloke/openchat_3.5-GGUF

          - https://huggingface.co/TheBloke/openchat_3.5-16k-GGUF



          FWIW, he seems to be "the guy" for uploading GGUF conversions of popular
          models, and his model page links back to this one as the source for the
          16k variant.  But maybe I should check on that page and log an issue there
          first?


          > also try with lower temp and high top_p. Hope that helps.


          I''ll give it a shot.


          Though another thing just occurred to me -- Is there anything regarding
          yarn / rope scaling that needs to be configured when using this variant
          of the model?'
        updatedAt: '2023-11-30T20:24:18.743Z'
      numEdits: 2
      reactions: []
    id: 6568ef0915c7a64e887d9063
    type: comment
  author: BrainSlugs83
  content: '> Not sure how llamasharp works, but this was tested using transformers
    and also using the original openchat prompt. Llama.cpp only works with gguf files,
    and this is not that.


    Oh, apologies! -- you are correct -- to be clear I''m using @TheBloke''s gguf
    conversions for both models at Q8 for comparison.

    - https://huggingface.co/TheBloke/openchat_3.5-GGUF

    - https://huggingface.co/TheBloke/openchat_3.5-16k-GGUF



    FWIW, he seems to be "the guy" for uploading GGUF conversions of popular models,
    and his model page links back to this one as the source for the 16k variant.  But
    maybe I should check on that page and log an issue there first?


    > also try with lower temp and high top_p. Hope that helps.


    I''ll give it a shot.


    Though another thing just occurred to me -- Is there anything regarding yarn /
    rope scaling that needs to be configured when using this variant of the model?'
  created_at: 2023-11-30 20:22:33+00:00
  edited: true
  hidden: false
  id: 6568ef0915c7a64e887d9063
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:30:53.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8409202098846436
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>llama.cpp should read it from gguf file set rope values to 0 should
          work.</p>

          '
        raw: llama.cpp should read it from gguf file set rope values to 0 should work.
        updatedAt: '2023-11-30T20:30:53.029Z'
      numEdits: 0
      reactions: []
    id: 6568f0fdef451c6ba632d11d
    type: comment
  author: perlthoughts
  content: llama.cpp should read it from gguf file set rope values to 0 should work.
  created_at: 2023-11-30 20:30:53+00:00
  edited: false
  hidden: false
  id: 6568f0fdef451c6ba632d11d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:33:10.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8891468048095703
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>Also check out this reddit thread for all the stuff we figured out
          to make openchat prompt work correctly with llama.cpp it was rather tricky:
          <a rel="nofollow" href="https://www.reddit.com/r/LocalLLaMA/comments/185my1b/">https://www.reddit.com/r/LocalLLaMA/comments/185my1b/</a></p>

          <p>the model is starling but it still uses openchat prompt so just wanted
          to show you what we found.</p>

          '
        raw: 'Also check out this reddit thread for all the stuff we figured out to
          make openchat prompt work correctly with llama.cpp it was rather tricky:
          https://www.reddit.com/r/LocalLLaMA/comments/185my1b/


          the model is starling but it still uses openchat prompt so just wanted to
          show you what we found.'
        updatedAt: '2023-11-30T20:33:10.391Z'
      numEdits: 0
      reactions: []
    id: 6568f1869c96f1a47beadfd0
    type: comment
  author: perlthoughts
  content: 'Also check out this reddit thread for all the stuff we figured out to
    make openchat prompt work correctly with llama.cpp it was rather tricky: https://www.reddit.com/r/LocalLLaMA/comments/185my1b/


    the model is starling but it still uses openchat prompt so just wanted to show
    you what we found.'
  created_at: 2023-11-30 20:33:10+00:00
  edited: false
  hidden: false
  id: 6568f1869c96f1a47beadfd0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:35:41.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9784079194068909
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>do not use the rope values that you see in that thread however as
          they are incorrect for the 16k model</p>

          '
        raw: do not use the rope values that you see in that thread however as they
          are incorrect for the 16k model
        updatedAt: '2023-11-30T20:35:41.013Z'
      numEdits: 0
      reactions: []
    id: 6568f21d8c995188206b9ba7
    type: comment
  author: perlthoughts
  content: do not use the rope values that you see in that thread however as they
    are incorrect for the 16k model
  created_at: 2023-11-30 20:35:41+00:00
  edited: false
  hidden: false
  id: 6568f21d8c995188206b9ba7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:37:05.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7646402716636658
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>Yours should be either rope_freq_base = 0, rope_freq_scale = 0,
          and if that doesn''t work set the real values of: 100000.0, and 1</p>

          '
        raw: 'Yours should be either rope_freq_base = 0, rope_freq_scale = 0, and
          if that doesn''t work set the real values of: 100000.0, and 1'
        updatedAt: '2023-11-30T20:37:05.097Z'
      numEdits: 0
      reactions: []
    id: 6568f2718e96268a528354c9
    type: comment
  author: perlthoughts
  content: 'Yours should be either rope_freq_base = 0, rope_freq_scale = 0, and if
    that doesn''t work set the real values of: 100000.0, and 1'
  created_at: 2023-11-30 20:37:05+00:00
  edited: false
  hidden: false
  id: 6568f2718e96268a528354c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:37:29.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.97447669506073
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>those values would be correct for the 16k, I hope that helps. Let
          me know.</p>

          '
        raw: those values would be correct for the 16k, I hope that helps. Let me
          know.
        updatedAt: '2023-11-30T20:37:29.768Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BrainSlugs83
    id: 6568f2898c995188206bb845
    type: comment
  author: perlthoughts
  content: those values would be correct for the 16k, I hope that helps. Let me know.
  created_at: 2023-11-30 20:37:29+00:00
  edited: false
  hidden: false
  id: 6568f2898c995188206bb845
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:40:59.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7754437327384949
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>I just realized i can link you to the settings config for lmstudio
          and starling: <a href="https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json">https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json</a></p>

          <p>Remember though do not use those values for rope scaling, but everything
          else should be the same, oh and the context size obviously would be larger
          for 16k not 4096.</p>

          '
        raw: 'I just realized i can link you to the settings config for lmstudio and
          starling: https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json


          Remember though do not use those values for rope scaling, but everything
          else should be the same, oh and the context size obviously would be larger
          for 16k not 4096.'
        updatedAt: '2023-11-30T20:40:59.783Z'
      numEdits: 0
      reactions: []
    id: 6568f35b2f7ea4b5ac08fc8e
    type: comment
  author: perlthoughts
  content: 'I just realized i can link you to the settings config for lmstudio and
    starling: https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json


    Remember though do not use those values for rope scaling, but everything else
    should be the same, oh and the context size obviously would be larger for 16k
    not 4096.'
  created_at: 2023-11-30 20:40:59+00:00
  edited: false
  hidden: false
  id: 6568f35b2f7ea4b5ac08fc8e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
      fullname: Ray Hernandez
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: perlthoughts
      type: user
    createdAt: '2023-11-30T20:41:40.000Z'
    data:
      edited: false
      editors:
      - perlthoughts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9421056509017944
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6448573b30fa4ecb85e2d184/Rn2MWykBh5BfOEZ2z0Vgi.png?w=200&h=200&f=face
          fullname: Ray Hernandez
          isHf: false
          isPro: false
          name: perlthoughts
          type: user
        html: '<p>with openchat once we figured out the template properly it really
          unlocked the full potential of both openchat and starling.</p>

          '
        raw: with openchat once we figured out the template properly it really unlocked
          the full potential of both openchat and starling.
        updatedAt: '2023-11-30T20:41:40.175Z'
      numEdits: 0
      reactions: []
    id: 6568f384ef451c6ba63359b2
    type: comment
  author: perlthoughts
  content: with openchat once we figured out the template properly it really unlocked
    the full potential of both openchat and starling.
  created_at: 2023-11-30 20:41:40+00:00
  edited: false
  hidden: false
  id: 6568f384ef451c6ba63359b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b701160f0aa120c4ef51e66ea0fa9887.svg
      fullname: JamesM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jtsaint346
      type: user
    createdAt: '2023-12-03T21:37:10.000Z'
    data:
      edited: false
      editors:
      - jtsaint346
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8664883375167847
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b701160f0aa120c4ef51e66ea0fa9887.svg
          fullname: JamesM
          isHf: false
          isPro: false
          name: jtsaint346
          type: user
        html: '<p>how to run as per the guide using VLLM on OpenChat - this one failed
          complaining no openchat.json file ! THanks</p>

          '
        raw: how to run as per the guide using VLLM on OpenChat - this one failed
          complaining no openchat.json file ! THanks
        updatedAt: '2023-12-03T21:37:10.748Z'
      numEdits: 0
      reactions: []
    id: 656cf50602a56b531ab68ffa
    type: comment
  author: jtsaint346
  content: how to run as per the guide using VLLM on OpenChat - this one failed complaining
    no openchat.json file ! THanks
  created_at: 2023-12-03 21:37:10+00:00
  edited: false
  hidden: false
  id: 656cf50602a56b531ab68ffa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ac777cca059f2e12510475f85dcc19a.svg
      fullname: Mikey Jensen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BrainSlugs83
      type: user
    createdAt: '2023-12-12T21:42:49.000Z'
    data:
      edited: false
      editors:
      - BrainSlugs83
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7878385782241821
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ac777cca059f2e12510475f85dcc19a.svg
          fullname: Mikey Jensen
          isHf: false
          isPro: false
          name: BrainSlugs83
          type: user
        html: '<blockquote>

          <p>I just realized i can link you to the settings config for lmstudio and
          starling: <a href="https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json">https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json</a></p>

          <p>Remember though do not use those values for rope scaling, but everything
          else should be the same, oh and the context size obviously would be larger
          for 16k not 4096.</p>

          </blockquote>

          <p>Link is 404''d.</p>

          '
        raw: "> I just realized i can link you to the settings config for lmstudio\
          \ and starling: https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json\n\
          > \n> Remember though do not use those values for rope scaling, but everything\
          \ else should be the same, oh and the context size obviously would be larger\
          \ for 16k not 4096.\n\nLink is 404'd."
        updatedAt: '2023-12-12T21:42:49.745Z'
      numEdits: 0
      reactions: []
    id: 6578d3d98369b885d44365fa
    type: comment
  author: BrainSlugs83
  content: "> I just realized i can link you to the settings config for lmstudio and\
    \ starling: https://huggingface.co/NurtureAI/Starling-LM-11B-alpha-v1-GGUF/blob/main/lmstudio-config.json\n\
    > \n> Remember though do not use those values for rope scaling, but everything\
    \ else should be the same, oh and the context size obviously would be larger for\
    \ 16k not 4096.\n\nLink is 404'd."
  created_at: 2023-12-12 21:42:49+00:00
  edited: false
  hidden: false
  id: 6578d3d98369b885d44365fa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: NurtureAI/openchat_3.5-16k
repo_type: model
status: open
target_branch: null
title: It seems worse than Open Chat 3.5 base?
