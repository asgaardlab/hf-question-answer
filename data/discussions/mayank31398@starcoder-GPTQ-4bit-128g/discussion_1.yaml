!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CyberTimon
conflicting_files: null
created_at: 2023-05-05 10:18:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-05-05T11:18:30.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>title</p>

          '
        raw: title
        updatedAt: '2023-05-05T11:18:30.944Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - bluesongcurls
    id: 6454e6061f9406d488007174
    type: comment
  author: CyberTimon
  content: title
  created_at: 2023-05-05 10:18:30+00:00
  edited: false
  hidden: false
  id: 6454e6061f9406d488007174
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f152893742efc579e42de9cb20d06338.svg
      fullname: euc dee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eucdee
      type: user
    createdAt: '2023-05-06T20:40:29.000Z'
    data:
      edited: false
      editors:
      - eucdee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f152893742efc579e42de9cb20d06338.svg
          fullname: euc dee
          isHf: false
          isPro: false
          name: eucdee
          type: user
        html: '<p>ive tried yesterday, it has its own inference mode and the model
          wont get recognized..<br>i couldnt make it work on colab maybe someone else
          can. maybe tomorrow i can try a local install..<br>but it seems you need  santacoder_inference
          to make it work.. i dont think ooba supports it yet</p>

          <p><code>python -m santacoder_inference bigcode/starcoder --wbits 4 --load
          starcoder-GPTQ-4bit-128g/model.pt</code></p>

          '
        raw: 'ive tried yesterday, it has its own inference mode and the model wont
          get recognized..

          i couldnt make it work on colab maybe someone else can. maybe tomorrow i
          can try a local install..

          but it seems you need  santacoder_inference to make it work.. i dont think
          ooba supports it yet


          ```python -m santacoder_inference bigcode/starcoder --wbits 4 --load starcoder-GPTQ-4bit-128g/model.pt```'
        updatedAt: '2023-05-06T20:40:29.870Z'
      numEdits: 0
      reactions: []
    id: 6456bb3d03625871eb7a81f9
    type: comment
  author: eucdee
  content: 'ive tried yesterday, it has its own inference mode and the model wont
    get recognized..

    i couldnt make it work on colab maybe someone else can. maybe tomorrow i can try
    a local install..

    but it seems you need  santacoder_inference to make it work.. i dont think ooba
    supports it yet


    ```python -m santacoder_inference bigcode/starcoder --wbits 4 --load starcoder-GPTQ-4bit-128g/model.pt```'
  created_at: 2023-05-06 19:40:29+00:00
  edited: false
  hidden: false
  id: 6456bb3d03625871eb7a81f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-07T00:23:05.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: "<p>hey <span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\"\
          >@<span class=\"underline\">CyberTimon</span></a></span>\n\n\t</span></span>\
          \ it should work.<br>I tried the inference with santacoder using this command\
          \ a few days ago and it was working.<br>I haven't tried starcoder though\
          \ but I don't see a reason why it shouldn't work.</p>\n<p>The webui won't\
          \ work though.</p>\n"
        raw: 'hey @CyberTimon it should work.

          I tried the inference with santacoder using this command a few days ago
          and it was working.

          I haven''t tried starcoder though but I don''t see a reason why it shouldn''t
          work.


          The webui won''t work though.'
        updatedAt: '2023-05-07T00:23:05.320Z'
      numEdits: 0
      reactions: []
    id: 6456ef6903625871eb7d3233
    type: comment
  author: mayank31398
  content: 'hey @CyberTimon it should work.

    I tried the inference with santacoder using this command a few days ago and it
    was working.

    I haven''t tried starcoder though but I don''t see a reason why it shouldn''t
    work.


    The webui won''t work though.'
  created_at: 2023-05-06 23:23:05+00:00
  edited: false
  hidden: false
  id: 6456ef6903625871eb7d3233
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
      fullname: k cramp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kcramp858
      type: user
    createdAt: '2023-05-07T19:56:54.000Z'
    data:
      edited: true
      editors:
      - kcramp858
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
          fullname: k cramp
          isHf: false
          isPro: false
          name: kcramp858
          type: user
        html: '<p>I also cannot figure out how to make this thing work.. even running
          python -m santacoder_inference bigcode/starcoder --wbits 4 --load ../models/starcoder-GPTQ-4bit-128g/model.pt
          just tries to download the model files again.</p>

          <p>I already downloaded starcoder-GPTQ-4bit-128g/model.pt .. </p>

          <p>Can anyone point me in the right direction? If I can figure it out, ill
          write a guide on what I did.</p>

          <p>-edit- I used GPT 4 to help me rearrange everything until I could run
          it. Main issue seems to be the config.json file.. I keep getting the errors
          below (I didnt paste them all); GPT4 says its because my config.json is
          from the original starcoder and not from the gptq 4bit 128 version. Still
          looking into how to solve this issue. Maybe I''m just an idiot? Will confirm
          soon</p>

          <p>for-SantaCoder$ python santacoder_inference.py bigcode/starcoder --wbits
          4 --load /mnt/i/ai/text-generation-webui/models/starcoder-GPTQ-4bit-128g/model.pt<br>Traceback
          (most recent call last):<br>  File "/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py",
          line 114, in <br>    main()<br>  File "/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py",
          line 104, in main<br>    model = get_santacoder(args.model, args.load, args.wbits)<br>  File
          "/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py",
          line 58, in get_santacoder<br>    model.load_state_dict(state_dict_original)<br>  File
          "/home/kcramp/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py",
          line 2056, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for GPTBigCodeForCausalLM:<br>        Unexpected key(s) in state_dict:
          "transformer.h.0.attn.c_attn.zeros", "transformer.h.0.attn.c_proj.zeros",
          "transformer.h.0.mlp.c_fc.zeros", "transformer.h.0.mlp.c_proj.zeros", "transformer.h.1.attn.c_attn.zeros",
          "transformer.h.1.attn.c_proj.zeros", "transformer.h.1.mlp.c_fc.zeros", "transformer.h.1.mlp.c_proj.zeros",
          "transformer.h.2.attn.c_attn.zeros", "transformer.h.2.attn.c_proj.zeros",
          "transformer.h.2.mlp.c_fc.zeros", "transformer.h.2.mlp.c_proj.zeros", "transformer.h.3.attn.c_attn.zeros",
          "transformer.h.3.attn.c_proj.zeros", "transformer.h.3.mlp.c_fc.zeros", "transformer.h.3.mlp.c_proj.zeros",
          "transformer.h.4.attn.c_attn.zeros", "transformer.h.4.attn.c_proj.zeros",
          "transformer.h.4.mlp.c_fc.zeros", "transformer.h.4.mlp.c_proj.zeros", "
          size mismatch for transformer.h.0.attn.c_attn.weight: copying a param with
          shape torch.Size([768, 6400]) from checkpoint, the shape in current model
          is torch.Size([6400, 6144]).<br>        size mismatch for transformer.h.0.attn.c_proj.weight:
          copying a param with shape torch.Size([768, 6144]) from checkpoint, the
          shape in current model is torch.Size([6144, 6144]).<br>        size mismatch
          for transformer.h.0.mlp.c_fc.weight: copying a param with shape torch.Size([768,
          24576]) from checkpoint, the shape in current model is torch.Size([24576,
          6144]).<br>        size mismatch for transformer.h.0.mlp.c_proj.weight:
          copying a param with shape torch.Size([3072, 6144]) from checkpoint, the
          shape in current model is torch.Size([6144, 24576]).<br>        size mismatch
          for transformer.h.1.attn.c_attn.weight: copying a param with shape torch.Size([768,
          6400]) from checkpoint, the shape in current model is torch.Size([6400,
          6144]).<br>        size mismatch for transformer.h.1.attn.c_proj.weight:
          copying a param with shape torch.Size([768, 6144]) from checkpoint, the
          shape in current model is torch.Size([6144, 6144]).<br>        size mismatch
          for transformer.h.1.mlp.c_fc.weight: copying a param with shape torch.Size([768,
          24576]) from checkpoint, the shape in current model is torch.Size([24576,
          6144]).<br>        size mismatch for transformer.h.1.mlp.c_proj.weight:
          copying a param with shape torch.Size([3072, 6144]) from checkpoint, the
          shape in current model is torch.Size([6144, 24576]).</p>

          '
        raw: "I also cannot figure out how to make this thing work.. even running\
          \ python -m santacoder_inference bigcode/starcoder --wbits 4 --load ../models/starcoder-GPTQ-4bit-128g/model.pt\
          \ just tries to download the model files again.\n\nI already downloaded\
          \ starcoder-GPTQ-4bit-128g/model.pt .. \n\nCan anyone point me in the right\
          \ direction? If I can figure it out, ill write a guide on what I did.\n\n\
          -edit- I used GPT 4 to help me rearrange everything until I could run it.\
          \ Main issue seems to be the config.json file.. I keep getting the errors\
          \ below (I didnt paste them all); GPT4 says its because my config.json is\
          \ from the original starcoder and not from the gptq 4bit 128 version. Still\
          \ looking into how to solve this issue. Maybe I'm just an idiot? Will confirm\
          \ soon\n\nfor-SantaCoder$ python santacoder_inference.py bigcode/starcoder\
          \ --wbits 4 --load /mnt/i/ai/text-generation-webui/models/starcoder-GPTQ-4bit-128g/model.pt\n\
          Traceback (most recent call last):\n  File \"/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py\"\
          , line 114, in <module>\n    main()\n  File \"/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py\"\
          , line 104, in main\n    model = get_santacoder(args.model, args.load, args.wbits)\n\
          \  File \"/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py\"\
          , line 58, in get_santacoder\n    model.load_state_dict(state_dict_original)\n\
          \  File \"/home/kcramp/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 2056, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for GPTBigCodeForCausalLM:\n        Unexpected key(s) in state_dict:\
          \ \"transformer.h.0.attn.c_attn.zeros\", \"transformer.h.0.attn.c_proj.zeros\"\
          , \"transformer.h.0.mlp.c_fc.zeros\", \"transformer.h.0.mlp.c_proj.zeros\"\
          , \"transformer.h.1.attn.c_attn.zeros\", \"transformer.h.1.attn.c_proj.zeros\"\
          , \"transformer.h.1.mlp.c_fc.zeros\", \"transformer.h.1.mlp.c_proj.zeros\"\
          , \"transformer.h.2.attn.c_attn.zeros\", \"transformer.h.2.attn.c_proj.zeros\"\
          , \"transformer.h.2.mlp.c_fc.zeros\", \"transformer.h.2.mlp.c_proj.zeros\"\
          , \"transformer.h.3.attn.c_attn.zeros\", \"transformer.h.3.attn.c_proj.zeros\"\
          , \"transformer.h.3.mlp.c_fc.zeros\", \"transformer.h.3.mlp.c_proj.zeros\"\
          , \"transformer.h.4.attn.c_attn.zeros\", \"transformer.h.4.attn.c_proj.zeros\"\
          , \"transformer.h.4.mlp.c_fc.zeros\", \"transformer.h.4.mlp.c_proj.zeros\"\
          , \" size mismatch for transformer.h.0.attn.c_attn.weight: copying a param\
          \ with shape torch.Size([768, 6400]) from checkpoint, the shape in current\
          \ model is torch.Size([6400, 6144]).\n        size mismatch for transformer.h.0.attn.c_proj.weight:\
          \ copying a param with shape torch.Size([768, 6144]) from checkpoint, the\
          \ shape in current model is torch.Size([6144, 6144]).\n        size mismatch\
          \ for transformer.h.0.mlp.c_fc.weight: copying a param with shape torch.Size([768,\
          \ 24576]) from checkpoint, the shape in current model is torch.Size([24576,\
          \ 6144]).\n        size mismatch for transformer.h.0.mlp.c_proj.weight:\
          \ copying a param with shape torch.Size([3072, 6144]) from checkpoint, the\
          \ shape in current model is torch.Size([6144, 24576]).\n        size mismatch\
          \ for transformer.h.1.attn.c_attn.weight: copying a param with shape torch.Size([768,\
          \ 6400]) from checkpoint, the shape in current model is torch.Size([6400,\
          \ 6144]).\n        size mismatch for transformer.h.1.attn.c_proj.weight:\
          \ copying a param with shape torch.Size([768, 6144]) from checkpoint, the\
          \ shape in current model is torch.Size([6144, 6144]).\n        size mismatch\
          \ for transformer.h.1.mlp.c_fc.weight: copying a param with shape torch.Size([768,\
          \ 24576]) from checkpoint, the shape in current model is torch.Size([24576,\
          \ 6144]).\n        size mismatch for transformer.h.1.mlp.c_proj.weight:\
          \ copying a param with shape torch.Size([3072, 6144]) from checkpoint, the\
          \ shape in current model is torch.Size([6144, 24576])."
        updatedAt: '2023-05-08T02:02:04.081Z'
      numEdits: 1
      reactions: []
    id: 64580286116c6b3c62e227a3
    type: comment
  author: kcramp858
  content: "I also cannot figure out how to make this thing work.. even running python\
    \ -m santacoder_inference bigcode/starcoder --wbits 4 --load ../models/starcoder-GPTQ-4bit-128g/model.pt\
    \ just tries to download the model files again.\n\nI already downloaded starcoder-GPTQ-4bit-128g/model.pt\
    \ .. \n\nCan anyone point me in the right direction? If I can figure it out, ill\
    \ write a guide on what I did.\n\n-edit- I used GPT 4 to help me rearrange everything\
    \ until I could run it. Main issue seems to be the config.json file.. I keep getting\
    \ the errors below (I didnt paste them all); GPT4 says its because my config.json\
    \ is from the original starcoder and not from the gptq 4bit 128 version. Still\
    \ looking into how to solve this issue. Maybe I'm just an idiot? Will confirm\
    \ soon\n\nfor-SantaCoder$ python santacoder_inference.py bigcode/starcoder --wbits\
    \ 4 --load /mnt/i/ai/text-generation-webui/models/starcoder-GPTQ-4bit-128g/model.pt\n\
    Traceback (most recent call last):\n  File \"/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py\"\
    , line 114, in <module>\n    main()\n  File \"/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py\"\
    , line 104, in main\n    model = get_santacoder(args.model, args.load, args.wbits)\n\
    \  File \"/mnt/i/ai/text-generation-webui/repositories/GPTQ-for-SantaCoder/santacoder_inference.py\"\
    , line 58, in get_santacoder\n    model.load_state_dict(state_dict_original)\n\
    \  File \"/home/kcramp/miniconda3/envs/textgen/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 2056, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for GPTBigCodeForCausalLM:\n\
    \        Unexpected key(s) in state_dict: \"transformer.h.0.attn.c_attn.zeros\"\
    , \"transformer.h.0.attn.c_proj.zeros\", \"transformer.h.0.mlp.c_fc.zeros\", \"\
    transformer.h.0.mlp.c_proj.zeros\", \"transformer.h.1.attn.c_attn.zeros\", \"\
    transformer.h.1.attn.c_proj.zeros\", \"transformer.h.1.mlp.c_fc.zeros\", \"transformer.h.1.mlp.c_proj.zeros\"\
    , \"transformer.h.2.attn.c_attn.zeros\", \"transformer.h.2.attn.c_proj.zeros\"\
    , \"transformer.h.2.mlp.c_fc.zeros\", \"transformer.h.2.mlp.c_proj.zeros\", \"\
    transformer.h.3.attn.c_attn.zeros\", \"transformer.h.3.attn.c_proj.zeros\", \"\
    transformer.h.3.mlp.c_fc.zeros\", \"transformer.h.3.mlp.c_proj.zeros\", \"transformer.h.4.attn.c_attn.zeros\"\
    , \"transformer.h.4.attn.c_proj.zeros\", \"transformer.h.4.mlp.c_fc.zeros\", \"\
    transformer.h.4.mlp.c_proj.zeros\", \" size mismatch for transformer.h.0.attn.c_attn.weight:\
    \ copying a param with shape torch.Size([768, 6400]) from checkpoint, the shape\
    \ in current model is torch.Size([6400, 6144]).\n        size mismatch for transformer.h.0.attn.c_proj.weight:\
    \ copying a param with shape torch.Size([768, 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([6144, 6144]).\n        size mismatch for transformer.h.0.mlp.c_fc.weight:\
    \ copying a param with shape torch.Size([768, 24576]) from checkpoint, the shape\
    \ in current model is torch.Size([24576, 6144]).\n        size mismatch for transformer.h.0.mlp.c_proj.weight:\
    \ copying a param with shape torch.Size([3072, 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([6144, 24576]).\n        size mismatch for transformer.h.1.attn.c_attn.weight:\
    \ copying a param with shape torch.Size([768, 6400]) from checkpoint, the shape\
    \ in current model is torch.Size([6400, 6144]).\n        size mismatch for transformer.h.1.attn.c_proj.weight:\
    \ copying a param with shape torch.Size([768, 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([6144, 6144]).\n        size mismatch for transformer.h.1.mlp.c_fc.weight:\
    \ copying a param with shape torch.Size([768, 24576]) from checkpoint, the shape\
    \ in current model is torch.Size([24576, 6144]).\n        size mismatch for transformer.h.1.mlp.c_proj.weight:\
    \ copying a param with shape torch.Size([3072, 6144]) from checkpoint, the shape\
    \ in current model is torch.Size([6144, 24576])."
  created_at: 2023-05-07 18:56:54+00:00
  edited: true
  hidden: false
  id: 64580286116c6b3c62e227a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-08T02:36:39.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kcramp858&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kcramp858\">@<span class=\"\
          underline\">kcramp858</span></a></span>\n\n\t</span></span> yeah, I think\
          \ that will download the model files from the original repo for the model\
          \ again.<br>Let me outline how this works:<br>The model is loaded in fp16\
          \ and then we inject the int8/int4 weights into the model.<br>Regarding,\
          \ the error you are seeing, I am unsure. I will investigate.</p>\n<p>Thanks\
          \ for trying it out :)</p>\n"
        raw: '@kcramp858 yeah, I think that will download the model files from the
          original repo for the model again.

          Let me outline how this works:

          The model is loaded in fp16 and then we inject the int8/int4 weights into
          the model.

          Regarding, the error you are seeing, I am unsure. I will investigate.


          Thanks for trying it out :)'
        updatedAt: '2023-05-08T02:36:39.991Z'
      numEdits: 0
      reactions: []
    id: 64586037c9af80de217ab925
    type: comment
  author: mayank31398
  content: '@kcramp858 yeah, I think that will download the model files from the original
    repo for the model again.

    Let me outline how this works:

    The model is loaded in fp16 and then we inject the int8/int4 weights into the
    model.

    Regarding, the error you are seeing, I am unsure. I will investigate.


    Thanks for trying it out :)'
  created_at: 2023-05-08 01:36:39+00:00
  edited: false
  hidden: false
  id: 64586037c9af80de217ab925
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-08T03:52:33.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>Hey guys, sorry.<br>I have fixed the bug.<br>Context: I was debugging
          something and had accidentally hardcoded groupsize to <code>-1</code>.<br>Can
          you try specifying <code>--groupsize 128</code> for starcoder during inference.
          I just tried and it worked for me :)<br>Please note for santacoder, you
          should specify -1.</p>

          <p>Fixed in the latest commit: <a rel="nofollow" href="https://github.com/mayank31398/GPTQ-for-SantaCoder/commit/40df38b03e4ebdaf9e5a444e9f7b4b6df79cff39">https://github.com/mayank31398/GPTQ-for-SantaCoder/commit/40df38b03e4ebdaf9e5a444e9f7b4b6df79cff39</a><br>Please
          pull the changes :)</p>

          '
        raw: 'Hey guys, sorry.

          I have fixed the bug.

          Context: I was debugging something and had accidentally hardcoded groupsize
          to `-1`.

          Can you try specifying `--groupsize 128` for starcoder during inference.
          I just tried and it worked for me :)

          Please note for santacoder, you should specify -1.


          Fixed in the latest commit: https://github.com/mayank31398/GPTQ-for-SantaCoder/commit/40df38b03e4ebdaf9e5a444e9f7b4b6df79cff39

          Please pull the changes :)'
        updatedAt: '2023-05-08T03:52:33.803Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Tom-Neverwinter
    id: 64587201116c6b3c62e989af
    type: comment
  author: mayank31398
  content: 'Hey guys, sorry.

    I have fixed the bug.

    Context: I was debugging something and had accidentally hardcoded groupsize to
    `-1`.

    Can you try specifying `--groupsize 128` for starcoder during inference. I just
    tried and it worked for me :)

    Please note for santacoder, you should specify -1.


    Fixed in the latest commit: https://github.com/mayank31398/GPTQ-for-SantaCoder/commit/40df38b03e4ebdaf9e5a444e9f7b4b6df79cff39

    Please pull the changes :)'
  created_at: 2023-05-08 02:52:33+00:00
  edited: false
  hidden: false
  id: 64587201116c6b3c62e989af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-11T11:34:00.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>can we close this?</p>

          '
        raw: can we close this?
        updatedAt: '2023-05-11T11:34:00.781Z'
      numEdits: 0
      reactions: []
    id: 645cd2a88ce4443cae6e7af2
    type: comment
  author: mayank31398
  content: can we close this?
  created_at: 2023-05-11 10:34:00+00:00
  edited: false
  hidden: false
  id: 645cd2a88ce4443cae6e7af2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-05-11T20:14:10.000Z'
    data:
      edited: false
      editors:
      - CyberTimon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
          fullname: "Timon K\xE4ch"
          isHf: false
          isPro: false
          name: CyberTimon
          type: user
        html: '<p>Sure, but it still doesn''t work in oobabooga but I don''t think
          you can easily change this as this has to do with the architecture of the
          model </p>

          '
        raw: 'Sure, but it still doesn''t work in oobabooga but I don''t think you
          can easily change this as this has to do with the architecture of the model '
        updatedAt: '2023-05-11T20:14:10.026Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645d4c924438da4fcc201791
    id: 645d4c924438da4fcc201790
    type: comment
  author: CyberTimon
  content: 'Sure, but it still doesn''t work in oobabooga but I don''t think you can
    easily change this as this has to do with the architecture of the model '
  created_at: 2023-05-11 19:14:10+00:00
  edited: false
  hidden: false
  id: 645d4c924438da4fcc201790
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6391f799f112e7097aea0b8d/TYUzVrosZd6s04jQ8IG-U.jpeg?w=200&h=200&f=face
      fullname: "Timon K\xE4ch"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CyberTimon
      type: user
    createdAt: '2023-05-11T20:14:10.000Z'
    data:
      status: closed
    id: 645d4c924438da4fcc201791
    type: status-change
  author: CyberTimon
  created_at: 2023-05-11 19:14:10+00:00
  id: 645d4c924438da4fcc201791
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6857d87d45dafe3b56ae784455634f7.svg
      fullname: r7l
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: r7l
      type: user
    createdAt: '2023-05-11T22:05:38.000Z'
    data:
      edited: false
      editors:
      - r7l
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6857d87d45dafe3b56ae784455634f7.svg
          fullname: r7l
          isHf: false
          isPro: false
          name: r7l
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CyberTimon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CyberTimon\">@<span class=\"\
          underline\">CyberTimon</span></a></span>\n\n\t</span></span> I've tried\
          \ this model: <a href=\"https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct\"\
          >https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct</a></p>\n\
          <p>It works with oobabooga but it's huge and slow.</p>\n"
        raw: '@CyberTimon I''ve tried this model: https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct


          It works with oobabooga but it''s huge and slow.'
        updatedAt: '2023-05-11T22:05:38.430Z'
      numEdits: 0
      reactions: []
    id: 645d66b2afa77201e2fec518
    type: comment
  author: r7l
  content: '@CyberTimon I''ve tried this model: https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct


    It works with oobabooga but it''s huge and slow.'
  created_at: 2023-05-11 21:05:38+00:00
  edited: false
  hidden: false
  id: 645d66b2afa77201e2fec518
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-12T16:06:23.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>yeah, you will need to quantize that model yourself.<br>You can
          take a look at the scripts provided in my repo.</p>

          '
        raw: 'yeah, you will need to quantize that model yourself.

          You can take a look at the scripts provided in my repo.'
        updatedAt: '2023-05-12T16:06:23.248Z'
      numEdits: 0
      reactions: []
    id: 645e63ff6d343f4bb61240a0
    type: comment
  author: mayank31398
  content: 'yeah, you will need to quantize that model yourself.

    You can take a look at the scripts provided in my repo.'
  created_at: 2023-05-12 15:06:23+00:00
  edited: false
  hidden: false
  id: 645e63ff6d343f4bb61240a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d6857d87d45dafe3b56ae784455634f7.svg
      fullname: r7l
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: r7l
      type: user
    createdAt: '2023-05-13T21:29:01.000Z'
    data:
      edited: false
      editors:
      - r7l
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d6857d87d45dafe3b56ae784455634f7.svg
          fullname: r7l
          isHf: false
          isPro: false
          name: r7l
          type: user
        html: '<p>Would this model run on oobabooga if quantized? I''ve not done anything
          like this yet. How long would this take and would it be possible with a
          normal PC and 3090 GPU?</p>

          '
        raw: Would this model run on oobabooga if quantized? I've not done anything
          like this yet. How long would this take and would it be possible with a
          normal PC and 3090 GPU?
        updatedAt: '2023-05-13T21:29:01.807Z'
      numEdits: 0
      reactions: []
    id: 6460011d98464f9fcb028384
    type: comment
  author: r7l
  content: Would this model run on oobabooga if quantized? I've not done anything
    like this yet. How long would this take and would it be possible with a normal
    PC and 3090 GPU?
  created_at: 2023-05-13 20:29:01+00:00
  edited: false
  hidden: false
  id: 6460011d98464f9fcb028384
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
      fullname: Mayank Mishra
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mayank31398
      type: user
    createdAt: '2023-05-15T11:46:31.000Z'
    data:
      edited: false
      editors:
      - mayank31398
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62cd5057674cdb524450093d/Kc2vqHdAc96lAJjkLf1gB.jpeg?w=200&h=200&f=face
          fullname: Mayank Mishra
          isHf: false
          isPro: false
          name: mayank31398
          type: user
        html: '<p>I am not sure.<br>I am planning to add a quantized version of starchat
          by this week too.</p>

          '
        raw: 'I am not sure.

          I am planning to add a quantized version of starchat by this week too.'
        updatedAt: '2023-05-15T11:46:31.521Z'
      numEdits: 0
      reactions: []
    id: 64621b97a0b47a9ad4eb2f1c
    type: comment
  author: mayank31398
  content: 'I am not sure.

    I am planning to add a quantized version of starchat by this week too.'
  created_at: 2023-05-15 10:46:31+00:00
  edited: false
  hidden: false
  id: 64621b97a0b47a9ad4eb2f1c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mayank31398/starcoder-GPTQ-4bit-128g
repo_type: model
status: closed
target_branch: null
title: Can I use this model in text-generation-webui?
