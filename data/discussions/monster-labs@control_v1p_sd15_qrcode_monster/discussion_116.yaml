!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dfischer96
conflicting_files: null
created_at: 2023-10-07 17:33:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/18bad41d9c9641deb247ba0a200b3f1d.svg
      fullname: Daniel Fischer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfischer96
      type: user
    createdAt: '2023-10-07T18:33:17.000Z'
    data:
      edited: false
      editors:
      - dfischer96
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5325372815132141
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/18bad41d9c9641deb247ba0a200b3f1d.svg
          fullname: Daniel Fischer
          isHf: false
          isPro: false
          name: dfischer96
          type: user
        html: '<h1 id="pip-install-opencv-python-transformers-accelerate">!pip install
          opencv-python transformers accelerate</h1>

          <p>from diffusers import StableDiffusionControlNetPipeline, ControlNetModel,
          UniPCMultistepScheduler<br>from diffusers.utils import load_image<br>import
          numpy as np<br>import torch</p>

          <p>import cv2<br>from PIL import Image</p>

          <h1 id="download-an-image">download an image</h1>

          <p>image = load_image(<br>    "<a rel="nofollow" href="https://wb-web.de/_Resources/Persistent/f/e/b/7/feb7a0d8ee35621aa5ac515bd019ed1e9f59746b/QRCode1.jpg&quot;">https://wb-web.de/_Resources/Persistent/f/e/b/7/feb7a0d8ee35621aa5ac515bd019ed1e9f59746b/QRCode1.jpg"</a><br>)<br>image
          = np.array(image)</p>

          <h1 id="get-canny-image">get canny image</h1>

          <p>image = cv2.Canny(image, 100, 200)<br>image = image[:, :, None]<br>image
          = np.concatenate([image, image, image], axis=2)<br>canny_image = Image.fromarray(image)</p>

          <h1 id="load-control-net-and-stable-diffusion-v1-5">load control net and
          stable diffusion v1-5</h1>

          <p>controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny",
          torch_dtype=torch.float16)<br>pipe = StableDiffusionControlNetPipeline.from_pretrained(<br>    "runwayml/stable-diffusion-v1-5",
          controlnet=controlnet, torch_dtype=torch.float16<br>)</p>

          <h1 id="speed-up-diffusion-process-with-faster-scheduler-and-memory-optimization">speed
          up diffusion process with faster scheduler and memory optimization</h1>

          <p>pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)</p>

          <h1 id="remove-following-line-if-xformers-is-not-installed">remove following
          line if xformers is not installed</h1>

          <p>pipe.enable_xformers_memory_efficient_attention()</p>

          <p>pipe.enable_model_cpu_offload()</p>

          <h1 id="generate-image">generate image</h1>

          <p>generator = torch.manual_seed(0)<br>image = pipe(<br>    "Harry Potter",
          num_inference_steps=20, generator=generator, image=canny_image<br>).images[0]</p>

          <h1 id="save-the-image-to-a-file">Save the image to a file</h1>

          <p>image.save("output.png")</p>

          <p>Am I using the model right? How can I put in your model files in the
          pipeline and controlnet parameters to fully function? Sorry for the dumb
          question im still learning.</p>

          '
        raw: "# !pip install opencv-python transformers accelerate\r\nfrom diffusers\
          \ import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\r\
          \nfrom diffusers.utils import load_image\r\nimport numpy as np\r\nimport\
          \ torch\r\n\r\nimport cv2\r\nfrom PIL import Image\r\n\r\n# download an\
          \ image\r\nimage = load_image(\r\n    \"https://wb-web.de/_Resources/Persistent/f/e/b/7/feb7a0d8ee35621aa5ac515bd019ed1e9f59746b/QRCode1.jpg\"\
          \r\n)\r\nimage = np.array(image)\r\n\r\n# get canny image\r\nimage = cv2.Canny(image,\
          \ 100, 200)\r\nimage = image[:, :, None]\r\nimage = np.concatenate([image,\
          \ image, image], axis=2)\r\ncanny_image = Image.fromarray(image)\r\n\r\n\
          # load control net and stable diffusion v1-5\r\ncontrolnet = ControlNetModel.from_pretrained(\"\
          lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\r\npipe = StableDiffusionControlNetPipeline.from_pretrained(\r\
          \n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\r\
          \n)\r\n\r\n# speed up diffusion process with faster scheduler and memory\
          \ optimization\r\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\r\
          \n# remove following line if xformers is not installed\r\npipe.enable_xformers_memory_efficient_attention()\r\
          \n\r\npipe.enable_model_cpu_offload()\r\n\r\n# generate image\r\ngenerator\
          \ = torch.manual_seed(0)\r\nimage = pipe(\r\n    \"Harry Potter\", num_inference_steps=20,\
          \ generator=generator, image=canny_image\r\n).images[0]\r\n\r\n# Save the\
          \ image to a file\r\nimage.save(\"output.png\")\r\n\r\nAm I using the model\
          \ right? How can I put in your model files in the pipeline and controlnet\
          \ parameters to fully function? Sorry for the dumb question im still learning."
        updatedAt: '2023-10-07T18:33:17.407Z'
      numEdits: 0
      reactions: []
    id: 6521a46dd89bc7773dba8374
    type: comment
  author: dfischer96
  content: "# !pip install opencv-python transformers accelerate\r\nfrom diffusers\
    \ import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\r\
    \nfrom diffusers.utils import load_image\r\nimport numpy as np\r\nimport torch\r\
    \n\r\nimport cv2\r\nfrom PIL import Image\r\n\r\n# download an image\r\nimage\
    \ = load_image(\r\n    \"https://wb-web.de/_Resources/Persistent/f/e/b/7/feb7a0d8ee35621aa5ac515bd019ed1e9f59746b/QRCode1.jpg\"\
    \r\n)\r\nimage = np.array(image)\r\n\r\n# get canny image\r\nimage = cv2.Canny(image,\
    \ 100, 200)\r\nimage = image[:, :, None]\r\nimage = np.concatenate([image, image,\
    \ image], axis=2)\r\ncanny_image = Image.fromarray(image)\r\n\r\n# load control\
    \ net and stable diffusion v1-5\r\ncontrolnet = ControlNetModel.from_pretrained(\"\
    lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\r\npipe = StableDiffusionControlNetPipeline.from_pretrained(\r\
    \n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\r\
    \n)\r\n\r\n# speed up diffusion process with faster scheduler and memory optimization\r\
    \npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\r\
    \n# remove following line if xformers is not installed\r\npipe.enable_xformers_memory_efficient_attention()\r\
    \n\r\npipe.enable_model_cpu_offload()\r\n\r\n# generate image\r\ngenerator = torch.manual_seed(0)\r\
    \nimage = pipe(\r\n    \"Harry Potter\", num_inference_steps=20, generator=generator,\
    \ image=canny_image\r\n).images[0]\r\n\r\n# Save the image to a file\r\nimage.save(\"\
    output.png\")\r\n\r\nAm I using the model right? How can I put in your model files\
    \ in the pipeline and controlnet parameters to fully function? Sorry for the dumb\
    \ question im still learning."
  created_at: 2023-10-07 17:33:17+00:00
  edited: false
  hidden: false
  id: 6521a46dd89bc7773dba8374
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/752e536716cb8af27a5463bbf77e48ed.svg
      fullname: Louis Mouhat
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: achiru
      type: user
    createdAt: '2023-10-08T00:11:35.000Z'
    data:
      edited: false
      editors:
      - achiru
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6562814712524414
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/752e536716cb8af27a5463bbf77e48ed.svg
          fullname: Louis Mouhat
          isHf: false
          isPro: false
          name: achiru
          type: user
        html: "<p>I have a full example here: <a href=\"https://huggingface.co/spaces/monster-labs/Controlnet-QRCode-Monster-V1/blob/main/app.py\"\
          >https://huggingface.co/spaces/monster-labs/Controlnet-QRCode-Monster-V1/blob/main/app.py</a><br>This\
          \ is for v1, so you should replace</p>\n<pre><code class=\"language-python\"\
          >controlnet = ControlNetModel.from_pretrained(\n    <span class=\"hljs-string\"\
          >\"monster-labs/control_v1p_sd15_qrcode_monster\"</span>, torch_dtype=torch.float16\n\
          )\n</code></pre>\n<p>with</p>\n<pre><code class=\"language-python\">ControlNetModel.from_pretrained(<span\
          \ class=\"hljs-string\">\"monster-labs/control_v1p_sd15_qrcode_monster\"\
          </span>, torch_dtype=torch.float16, subfolder=<span class=\"hljs-string\"\
          >\"v2\"</span>)\n</code></pre>\n<p>You don't seem to need gradio, so take\
          \ just what you need (probably the pipeline initialization and inference\
          \ parts) from the file.</p>\n"
        raw: "I have a full example here: https://huggingface.co/spaces/monster-labs/Controlnet-QRCode-Monster-V1/blob/main/app.py\n\
          This is for v1, so you should replace\n```python\ncontrolnet = ControlNetModel.from_pretrained(\n\
          \    \"monster-labs/control_v1p_sd15_qrcode_monster\", torch_dtype=torch.float16\n\
          )\n```\nwith\n```python\nControlNetModel.from_pretrained(\"monster-labs/control_v1p_sd15_qrcode_monster\"\
          , torch_dtype=torch.float16, subfolder=\"v2\")\n```\nYou don't seem to need\
          \ gradio, so take just what you need (probably the pipeline initialization\
          \ and inference parts) from the file."
        updatedAt: '2023-10-08T00:11:35.248Z'
      numEdits: 0
      reactions: []
    id: 6521f3b7ef06bb9975364776
    type: comment
  author: achiru
  content: "I have a full example here: https://huggingface.co/spaces/monster-labs/Controlnet-QRCode-Monster-V1/blob/main/app.py\n\
    This is for v1, so you should replace\n```python\ncontrolnet = ControlNetModel.from_pretrained(\n\
    \    \"monster-labs/control_v1p_sd15_qrcode_monster\", torch_dtype=torch.float16\n\
    )\n```\nwith\n```python\nControlNetModel.from_pretrained(\"monster-labs/control_v1p_sd15_qrcode_monster\"\
    , torch_dtype=torch.float16, subfolder=\"v2\")\n```\nYou don't seem to need gradio,\
    \ so take just what you need (probably the pipeline initialization and inference\
    \ parts) from the file."
  created_at: 2023-10-07 23:11:35+00:00
  edited: false
  hidden: false
  id: 6521f3b7ef06bb9975364776
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 116
repo_id: monster-labs/control_v1p_sd15_qrcode_monster
repo_type: model
status: open
target_branch: null
title: Model usage
