!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DemonMaike
conflicting_files: null
created_at: 2023-11-02 16:14:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/M57emMqveu7urIqjGt4Fd.png?w=200&h=200&f=face
      fullname: Dmitry Malyshev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DemonMaike
      type: user
    createdAt: '2023-11-02T17:14:37.000Z'
    data:
      edited: false
      editors:
      - DemonMaike
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9047532677650452
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/M57emMqveu7urIqjGt4Fd.png?w=200&h=200&f=face
          fullname: Dmitry Malyshev
          isHf: false
          isPro: false
          name: DemonMaike
          type: user
        html: '<p>Hi and thanks for your work and model.</p>

          <p>Could you help me with one question about memory?<br>Now i see your model
          work with 32k context window, but if i use my context ~23k tokens, i have
          error about memory, but i work with A100 80GB.<br>Maybe i don''t understand
          and your working context window less then 32k tokens or i must use special
          settings?</p>

          <p>About how my using your model, i use base encode\decode methods all my
          context into your model, and model work good with small context.<br>Thanks.</p>

          '
        raw: "Hi and thanks for your work and model.\r\n\r\nCould you help me with\
          \ one question about memory? \r\nNow i see your model work with 32k context\
          \ window, but if i use my context ~23k tokens, i have error about memory,\
          \ but i work with A100 80GB.\r\nMaybe i don't understand and your working\
          \ context window less then 32k tokens or i must use special settings?\r\n\
          \r\nAbout how my using your model, i use base encode\\decode methods all\
          \ my context into your model, and model work good with small context.\r\n\
          Thanks."
        updatedAt: '2023-11-02T17:14:38.001Z'
      numEdits: 0
      reactions: []
    id: 6543d8fd998b3248a1008476
    type: comment
  author: DemonMaike
  content: "Hi and thanks for your work and model.\r\n\r\nCould you help me with one\
    \ question about memory? \r\nNow i see your model work with 32k context window,\
    \ but if i use my context ~23k tokens, i have error about memory, but i work with\
    \ A100 80GB.\r\nMaybe i don't understand and your working context window less\
    \ then 32k tokens or i must use special settings?\r\n\r\nAbout how my using your\
    \ model, i use base encode\\decode methods all my context into your model, and\
    \ model work good with small context.\r\nThanks."
  created_at: 2023-11-02 16:14:37+00:00
  edited: false
  hidden: false
  id: 6543d8fd998b3248a1008476
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-04T23:26:01.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9330281615257263
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>The vanilla transformers backend is really inefficient with long
          context. It may actually be using 80GB?</p>

          <p>You need to use a quant, or at least load it in 8 bits.</p>

          <p>Also, the SciPhi instruct model doesn''t work well past 9-10k (but works
          very well under that limit), so you may not want to use 32k for this iteration
          yet anyway.</p>

          '
        raw: 'The vanilla transformers backend is really inefficient with long context.
          It may actually be using 80GB?


          You need to use a quant, or at least load it in 8 bits.


          Also, the SciPhi instruct model doesn''t work well past 9-10k (but works
          very well under that limit), so you may not want to use 32k for this iteration
          yet anyway.'
        updatedAt: '2023-11-04T23:26:13.009Z'
      numEdits: 1
      reactions: []
    id: 6546d3092119c8bdf2706e7e
    type: comment
  author: brucethemoose
  content: 'The vanilla transformers backend is really inefficient with long context.
    It may actually be using 80GB?


    You need to use a quant, or at least load it in 8 bits.


    Also, the SciPhi instruct model doesn''t work well past 9-10k (but works very
    well under that limit), so you may not want to use 32k for this iteration yet
    anyway.'
  created_at: 2023-11-04 22:26:01+00:00
  edited: true
  hidden: false
  id: 6546d3092119c8bdf2706e7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/M57emMqveu7urIqjGt4Fd.png?w=200&h=200&f=face
      fullname: Dmitry Malyshev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DemonMaike
      type: user
    createdAt: '2023-11-05T22:02:38.000Z'
    data:
      edited: true
      editors:
      - DemonMaike
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9103525876998901
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/M57emMqveu7urIqjGt4Fd.png?w=200&h=200&f=face
          fullname: Dmitry Malyshev
          isHf: false
          isPro: false
          name: DemonMaike
          type: user
        html: "<p>Thanks for your answer and description of limits your model.<br>Of\
          \ course, i will try to use 4-8bit version.<br>Good luck with your work\
          \ \U0001F4AA</p>\n"
        raw: "Thanks for your answer and description of limits your model.\nOf course,\
          \ i will try to use 4-8bit version.\nGood luck with your work \U0001F4AA"
        updatedAt: '2023-11-05T22:03:14.253Z'
      numEdits: 1
      reactions: []
    id: 654810fe92d9b890bb9d082e
    type: comment
  author: DemonMaike
  content: "Thanks for your answer and description of limits your model.\nOf course,\
    \ i will try to use 4-8bit version.\nGood luck with your work \U0001F4AA"
  created_at: 2023-11-05 22:02:38+00:00
  edited: true
  hidden: false
  id: 654810fe92d9b890bb9d082e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-05T22:45:57.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9519023299217224
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Its not my model, though I like the idea :P</p>

          '
        raw: Its not my model, though I like the idea :P
        updatedAt: '2023-11-05T22:45:57.872Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - DemonMaike
        - gardner
    id: 65481b25cd0a56213954dfa1
    type: comment
  author: brucethemoose
  content: Its not my model, though I like the idea :P
  created_at: 2023-11-05 22:45:57+00:00
  edited: false
  hidden: false
  id: 65481b25cd0a56213954dfa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bbbb2576f96c91d546a7f4d30e9585e.svg
      fullname: Adhil Aseem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adhiltortil
      type: user
    createdAt: '2023-12-19T05:35:24.000Z'
    data:
      edited: false
      editors:
      - adhiltortil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7910456657409668
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bbbb2576f96c91d546a7f4d30e9585e.svg
          fullname: Adhil Aseem
          isHf: false
          isPro: false
          name: adhiltortil
          type: user
        html: '<p>OutOfMemoryError: CUDA out of memory. Tried to allocate 50.64 GiB
          (GPU 0; 79.35 GiB total capacity; 13.25 GiB already allocated; 23.54 GiB
          free; 55.07 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;
          allocated memory try setting max_split_size_mb to avoid fragmentation. See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF.</p>

          <p>I have two gpus each of 80 gb memory. How do I solve this error?</p>

          '
        raw: 'OutOfMemoryError: CUDA out of memory. Tried to allocate 50.64 GiB (GPU
          0; 79.35 GiB total capacity; 13.25 GiB already allocated; 23.54 GiB free;
          55.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated
          memory try setting max_split_size_mb to avoid fragmentation. See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF.


          I have two gpus each of 80 gb memory. How do I solve this error?

          '
        updatedAt: '2023-12-19T05:35:24.957Z'
      numEdits: 0
      reactions: []
    id: 65812b9c54a75cbee69c944b
    type: comment
  author: adhiltortil
  content: 'OutOfMemoryError: CUDA out of memory. Tried to allocate 50.64 GiB (GPU
    0; 79.35 GiB total capacity; 13.25 GiB already allocated; 23.54 GiB free; 55.07
    GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try
    setting max_split_size_mb to avoid fragmentation. See documentation for Memory
    Management and PYTORCH_CUDA_ALLOC_CONF.


    I have two gpus each of 80 gb memory. How do I solve this error?

    '
  created_at: 2023-12-19 05:35:24+00:00
  edited: false
  hidden: false
  id: 65812b9c54a75cbee69c944b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: SciPhi/SciPhi-Self-RAG-Mistral-7B-32k
repo_type: model
status: open
target_branch: null
title: OutOfMemory
