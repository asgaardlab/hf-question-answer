!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2024-01-09 19:17:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-09T19:17:24.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9681631922721863
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I really appreciate this model!</p>

          <p>Ive really been looking forward to it as I think its one step forward
          to advancing Mixtral, and Ai overall. I plan on using this to create the
          next version of my Open_Gpt4 series by merging this bagel model with mixtral-instruct.
          I hope the results are good!</p>

          '
        raw: "I really appreciate this model!\r\n\r\nIve really been looking forward\
          \ to it as I think its one step forward to advancing Mixtral, and Ai overall.\
          \ I plan on using this to create the next version of my Open_Gpt4 series\
          \ by merging this bagel model with mixtral-instruct. I hope the results\
          \ are good!"
        updatedAt: '2024-01-09T19:17:24.574Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - jondurbin
        - algorithm
    id: 659d9bc423a7c54495b98d7d
    type: comment
  author: rombodawg
  content: "I really appreciate this model!\r\n\r\nIve really been looking forward\
    \ to it as I think its one step forward to advancing Mixtral, and Ai overall.\
    \ I plan on using this to create the next version of my Open_Gpt4 series by merging\
    \ this bagel model with mixtral-instruct. I hope the results are good!"
  created_at: 2024-01-09 19:17:24+00:00
  edited: false
  hidden: false
  id: 659d9bc423a7c54495b98d7d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2024-01-09T19:26:24.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4815892279148102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>DPO COOL</p>

          '
        raw: DPO COOL
        updatedAt: '2024-01-09T19:26:24.459Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - rombodawg
        - jondurbin
    id: 659d9de00ce6bc9fbdf0ae48
    type: comment
  author: NickyNicky
  content: DPO COOL
  created_at: 2024-01-09 19:26:24+00:00
  edited: false
  hidden: false
  id: 659d9de00ce6bc9fbdf0ae48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2024-01-09T19:40:50.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9063947200775146
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<blockquote>

          <p>I plan on using this to create the next version of my Open_Gpt4 series
          by merging this bagel model with mixtral-instruct. I hope the results are
          good!</p>

          </blockquote>

          <p>Awesome, let me know the results!</p>

          '
        raw: '> I plan on using this to create the next version of my Open_Gpt4 series
          by merging this bagel model with mixtral-instruct. I hope the results are
          good!


          Awesome, let me know the results!'
        updatedAt: '2024-01-09T19:40:50.774Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - algorithm
    id: 659da142aa29d05820aeb358
    type: comment
  author: jondurbin
  content: '> I plan on using this to create the next version of my Open_Gpt4 series
    by merging this bagel model with mixtral-instruct. I hope the results are good!


    Awesome, let me know the results!'
  created_at: 2024-01-09 19:40:50+00:00
  edited: false
  hidden: false
  id: 659da142aa29d05820aeb358
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2024-01-09T19:47:13.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9941450357437134
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>Thank you for this model, I would like to know how you did fine
          tune since I understand that mixtral models are difficult to fine tune,
          thank you.</p>

          '
        raw: Thank you for this model, I would like to know how you did fine tune
          since I understand that mixtral models are difficult to fine tune, thank
          you.
        updatedAt: '2024-01-09T19:47:13.708Z'
      numEdits: 0
      reactions: []
    id: 659da2c104b93eb6db80944c
    type: comment
  author: NickyNicky
  content: Thank you for this model, I would like to know how you did fine tune since
    I understand that mixtral models are difficult to fine tune, thank you.
  created_at: 2024-01-09 19:47:13+00:00
  edited: false
  hidden: false
  id: 659da2c104b93eb6db80944c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-11T18:13:38.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9553676843643188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>So far the model is pretty promising, ive been testing my gguf quant
          and although its still not as good as gpt-4, its getting closer to the quality.
          I want to say it might even be better than base mixtral-instruct but that
          only a guess based on some limited testing, more testing as well as the
          score on open llm leaderboard is required to validate that claim. You can
          find it here</p>

          <ul>

          <li><a href="https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2">https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2</a></li>

          </ul>

          <p>And my gguf quant will be here by the end of today or early tomorrow
          depending on when it gets done uploading:</p>

          <ul>

          <li><a href="https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf">https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf</a></li>

          </ul>

          '
        raw: 'So far the model is pretty promising, ive been testing my gguf quant
          and although its still not as good as gpt-4, its getting closer to the quality.
          I want to say it might even be better than base mixtral-instruct but that
          only a guess based on some limited testing, more testing as well as the
          score on open llm leaderboard is required to validate that claim. You can
          find it here


          - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2


          And my gguf quant will be here by the end of today or early tomorrow depending
          on when it gets done uploading:


          - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf'
        updatedAt: '2024-01-11T18:14:19.959Z'
      numEdits: 1
      reactions: []
    id: 65a02fd2bbbb7b8dd1651aa1
    type: comment
  author: rombodawg
  content: 'So far the model is pretty promising, ive been testing my gguf quant and
    although its still not as good as gpt-4, its getting closer to the quality. I
    want to say it might even be better than base mixtral-instruct but that only a
    guess based on some limited testing, more testing as well as the score on open
    llm leaderboard is required to validate that claim. You can find it here


    - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2


    And my gguf quant will be here by the end of today or early tomorrow depending
    on when it gets done uploading:


    - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf'
  created_at: 2024-01-11 18:13:38+00:00
  edited: true
  hidden: false
  id: 65a02fd2bbbb7b8dd1651aa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2024-01-11T20:48:19.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9257878661155701
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<blockquote>

          <p>Thank you for this model, I would like to know how you did fine tune
          since I understand that mixtral models are difficult to fine tune, thank
          you.</p>

          </blockquote>

          <p>I used my fork of qlora here: <a rel="nofollow" href="https://github.com/jondurbin/qlora">https://github.com/jondurbin/qlora</a>,
          with the configuration you can find on weights and biases:<br><a rel="nofollow"
          href="https://wandb.ai/jondurbin/bagel-8x7b-v0.2/runs/agxjjdso/overview?workspace=user-jondurbin">https://wandb.ai/jondurbin/bagel-8x7b-v0.2/runs/agxjjdso/overview?workspace=user-jondurbin</a></p>

          <p>I used the latest main branch of transformers, but at the time these
          had not yet been merged, so I pulled them in manually:<br><a rel="nofollow"
          href="https://github.com/huggingface/transformers/pull/28115">https://github.com/huggingface/transformers/pull/28115</a><br><a
          rel="nofollow" href="https://github.com/huggingface/transformers/pull/28256">https://github.com/huggingface/transformers/pull/28256</a></p>

          <p>I think now, if you build transformers from source using the latest main
          checkout, it should be somewhat fixed, although the mistral folks in discord
          did say the implementation is wrong so any fine tunes of mixtral are probably
          suboptimal right now (regardless of how good they may do, they should be
          better).</p>

          '
        raw: '> Thank you for this model, I would like to know how you did fine tune
          since I understand that mixtral models are difficult to fine tune, thank
          you.


          I used my fork of qlora here: https://github.com/jondurbin/qlora, with the
          configuration you can find on weights and biases:

          https://wandb.ai/jondurbin/bagel-8x7b-v0.2/runs/agxjjdso/overview?workspace=user-jondurbin


          I used the latest main branch of transformers, but at the time these had
          not yet been merged, so I pulled them in manually:

          https://github.com/huggingface/transformers/pull/28115

          https://github.com/huggingface/transformers/pull/28256


          I think now, if you build transformers from source using the latest main
          checkout, it should be somewhat fixed, although the mistral folks in discord
          did say the implementation is wrong so any fine tunes of mixtral are probably
          suboptimal right now (regardless of how good they may do, they should be
          better).'
        updatedAt: '2024-01-11T20:48:19.763Z'
      numEdits: 0
      reactions: []
    id: 65a05413b430dfb7e0b6b731
    type: comment
  author: jondurbin
  content: '> Thank you for this model, I would like to know how you did fine tune
    since I understand that mixtral models are difficult to fine tune, thank you.


    I used my fork of qlora here: https://github.com/jondurbin/qlora, with the configuration
    you can find on weights and biases:

    https://wandb.ai/jondurbin/bagel-8x7b-v0.2/runs/agxjjdso/overview?workspace=user-jondurbin


    I used the latest main branch of transformers, but at the time these had not yet
    been merged, so I pulled them in manually:

    https://github.com/huggingface/transformers/pull/28115

    https://github.com/huggingface/transformers/pull/28256


    I think now, if you build transformers from source using the latest main checkout,
    it should be somewhat fixed, although the mistral folks in discord did say the
    implementation is wrong so any fine tunes of mixtral are probably suboptimal right
    now (regardless of how good they may do, they should be better).'
  created_at: 2024-01-11 20:48:19+00:00
  edited: false
  hidden: false
  id: 65a05413b430dfb7e0b6b731
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2024-01-11T20:48:42.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9488720297813416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<blockquote>

          <p>So far the model is pretty promising, ive been testing my gguf quant
          and although its still not as good as gpt-4, its getting closer to the quality.
          I want to say it might even be better than base mixtral-instruct but that
          only a guess based on some limited testing, more testing as well as the
          score on open llm leaderboard is required to validate that claim. You can
          find it here</p>

          <ul>

          <li><a href="https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2">https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2</a></li>

          </ul>

          <p>And my gguf quant will be here by the end of today or early tomorrow
          depending on when it gets done uploading:</p>

          <ul>

          <li><a href="https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf">https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf</a></li>

          </ul>

          </blockquote>

          <p>Awesome!</p>

          '
        raw: "> So far the model is pretty promising, ive been testing my gguf quant\
          \ and although its still not as good as gpt-4, its getting closer to the\
          \ quality. I want to say it might even be better than base mixtral-instruct\
          \ but that only a guess based on some limited testing, more testing as well\
          \ as the score on open llm leaderboard is required to validate that claim.\
          \ You can find it here\n> \n> - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2\n\
          > \n> And my gguf quant will be here by the end of today or early tomorrow\
          \ depending on when it gets done uploading:\n> \n> - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf\n\
          \nAwesome!"
        updatedAt: '2024-01-11T20:48:42.131Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - algorithm
    id: 65a0542a684c018bd4be3bea
    type: comment
  author: jondurbin
  content: "> So far the model is pretty promising, ive been testing my gguf quant\
    \ and although its still not as good as gpt-4, its getting closer to the quality.\
    \ I want to say it might even be better than base mixtral-instruct but that only\
    \ a guess based on some limited testing, more testing as well as the score on\
    \ open llm leaderboard is required to validate that claim. You can find it here\n\
    > \n> - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2\n> \n> And my gguf\
    \ quant will be here by the end of today or early tomorrow depending on when it\
    \ gets done uploading:\n> \n> - https://huggingface.co/rombodawg/Open_Gpt4_8x7B_v0.2_q8_0_gguf\n\
    \nAwesome!"
  created_at: 2024-01-11 20:48:42+00:00
  edited: false
  hidden: false
  id: 65a0542a684c018bd4be3bea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0844709e1fe931c2075971fbd37697a5.svg
      fullname: Joshua Hartman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jhartman
      type: user
    createdAt: '2024-01-15T18:15:39.000Z'
    data:
      edited: false
      editors:
      - jhartman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761802554130554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0844709e1fe931c2075971fbd37697a5.svg
          fullname: Joshua Hartman
          isHf: false
          isPro: false
          name: jhartman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jondurbin\">@<span class=\"\
          underline\">jondurbin</span></a></span>\n\n\t</span></span> Do you have\
          \ plans to run a fine-tune on top of Mixtral-8x7B-Instruct? That model has\
          \ already been instruction fine-tuned but there are likely many things in\
          \ the bagel dataset it hasn't seen that would improve performance. </p>\n"
        raw: '@jondurbin Do you have plans to run a fine-tune on top of Mixtral-8x7B-Instruct?
          That model has already been instruction fine-tuned but there are likely
          many things in the bagel dataset it hasn''t seen that would improve performance. '
        updatedAt: '2024-01-15T18:15:39.579Z'
      numEdits: 0
      reactions: []
    id: 65a5764bfdc98e08a5b54f88
    type: comment
  author: jhartman
  content: '@jondurbin Do you have plans to run a fine-tune on top of Mixtral-8x7B-Instruct?
    That model has already been instruction fine-tuned but there are likely many things
    in the bagel dataset it hasn''t seen that would improve performance. '
  created_at: 2024-01-15 18:15:39+00:00
  edited: false
  hidden: false
  id: 65a5764bfdc98e08a5b54f88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2024-01-15T18:45:15.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9762410521507263
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jondurbin\"\
          >@<span class=\"underline\">jondurbin</span></a></span>\n\n\t</span></span>\
          \ Do you have plans to run a fine-tune on top of Mixtral-8x7B-Instruct?\
          \ That model has already been instruction fine-tuned but there are likely\
          \ many things in the bagel dataset it hasn't seen that would improve performance.</p>\n\
          </blockquote>\n<p>I may, but probably not until we can confirm the issues\
          \ with the mixtral implementation in HF are fixed.  It appears there are\
          \ discrepancies, as hinted by the mistral folks in discord, but sadly they\
          \ refuse to help correct or even identify the issue.</p>\n"
        raw: '> @jondurbin Do you have plans to run a fine-tune on top of Mixtral-8x7B-Instruct?
          That model has already been instruction fine-tuned but there are likely
          many things in the bagel dataset it hasn''t seen that would improve performance.


          I may, but probably not until we can confirm the issues with the mixtral
          implementation in HF are fixed.  It appears there are discrepancies, as
          hinted by the mistral folks in discord, but sadly they refuse to help correct
          or even identify the issue.'
        updatedAt: '2024-01-15T18:45:15.736Z'
      numEdits: 0
      reactions: []
    id: 65a57d3b576772f53192f91b
    type: comment
  author: jondurbin
  content: '> @jondurbin Do you have plans to run a fine-tune on top of Mixtral-8x7B-Instruct?
    That model has already been instruction fine-tuned but there are likely many things
    in the bagel dataset it hasn''t seen that would improve performance.


    I may, but probably not until we can confirm the issues with the mixtral implementation
    in HF are fixed.  It appears there are discrepancies, as hinted by the mistral
    folks in discord, but sadly they refuse to help correct or even identify the issue.'
  created_at: 2024-01-15 18:45:15+00:00
  edited: false
  hidden: false
  id: 65a57d3b576772f53192f91b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2024-01-15T18:59:04.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8390038013458252
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jondurbin\">@<span class=\"\
          underline\">jondurbin</span></a></span>\n\n\t</span></span> My recommendation\
          \ as Ive stated in my write up is to basically forget mistralai and their\
          \ models and start creating out own using my techniques and mergekit. Make\
          \ your own base models like how i made mine, and train on top of those.\
          \ </p>\n<p>My model:<br><a href=\"https://huggingface.co/rombodawg/Everyone-Coder-4x7b-Base\"\
          >https://huggingface.co/rombodawg/Everyone-Coder-4x7b-Base</a></p>\n<p>My\
          \ write up:<br><a rel=\"nofollow\" href=\"https://docs.google.com/document/d/1_vOftBnrk9NRk5h10UqrfJ5CDih9KBKL61yvrZtVWPE/edit?usp=sharing\"\
          >https://docs.google.com/document/d/1_vOftBnrk9NRk5h10UqrfJ5CDih9KBKL61yvrZtVWPE/edit?usp=sharing</a></p>\n"
        raw: "@jondurbin My recommendation as Ive stated in my write up is to basically\
          \ forget mistralai and their models and start creating out own using my\
          \ techniques and mergekit. Make your own base models like how i made mine,\
          \ and train on top of those. \n\nMy model:\nhttps://huggingface.co/rombodawg/Everyone-Coder-4x7b-Base\n\
          \nMy write up:\nhttps://docs.google.com/document/d/1_vOftBnrk9NRk5h10UqrfJ5CDih9KBKL61yvrZtVWPE/edit?usp=sharing"
        updatedAt: '2024-01-15T18:59:04.690Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jondurbin
    id: 65a58078636afd03b2fbf32e
    type: comment
  author: rombodawg
  content: "@jondurbin My recommendation as Ive stated in my write up is to basically\
    \ forget mistralai and their models and start creating out own using my techniques\
    \ and mergekit. Make your own base models like how i made mine, and train on top\
    \ of those. \n\nMy model:\nhttps://huggingface.co/rombodawg/Everyone-Coder-4x7b-Base\n\
    \nMy write up:\nhttps://docs.google.com/document/d/1_vOftBnrk9NRk5h10UqrfJ5CDih9KBKL61yvrZtVWPE/edit?usp=sharing"
  created_at: 2024-01-15 18:59:04+00:00
  edited: false
  hidden: false
  id: 65a58078636afd03b2fbf32e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0844709e1fe931c2075971fbd37697a5.svg
      fullname: Joshua Hartman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jhartman
      type: user
    createdAt: '2024-01-16T22:54:53.000Z'
    data:
      edited: false
      editors:
      - jhartman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.971612274646759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0844709e1fe931c2075971fbd37697a5.svg
          fullname: Joshua Hartman
          isHf: false
          isPro: false
          name: jhartman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jondurbin\">@<span class=\"\
          underline\">jondurbin</span></a></span>\n\n\t</span></span> Makes sense\
          \ to wait. I do think it would be very interesting to see given how promising\
          \ your bagel fine-tune on Yi was and the strength of this fine-tune as well.\
          \ Too bad your hardware restricts you to LORA rather than a full fine-tune\
          \ :(</p>\n"
        raw: '@jondurbin Makes sense to wait. I do think it would be very interesting
          to see given how promising your bagel fine-tune on Yi was and the strength
          of this fine-tune as well. Too bad your hardware restricts you to LORA rather
          than a full fine-tune :('
        updatedAt: '2024-01-16T22:54:53.703Z'
      numEdits: 0
      reactions: []
    id: 65a7093de65b8ffd3bcbfb0f
    type: comment
  author: jhartman
  content: '@jondurbin Makes sense to wait. I do think it would be very interesting
    to see given how promising your bagel fine-tune on Yi was and the strength of
    this fine-tune as well. Too bad your hardware restricts you to LORA rather than
    a full fine-tune :('
  created_at: 2024-01-16 22:54:53+00:00
  edited: false
  hidden: false
  id: 65a7093de65b8ffd3bcbfb0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3772383eb360de80ebc34340fa38bb53.svg
      fullname: Matthieu Herrmann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MH1P
      type: user
    createdAt: '2024-01-23T06:28:12.000Z'
    data:
      edited: false
      editors:
      - MH1P
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.644610583782196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3772383eb360de80ebc34340fa38bb53.svg
          fullname: Matthieu Herrmann
          isHf: false
          isPro: false
          name: MH1P
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jondurbin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jondurbin\">@<span class=\"\
          underline\">jondurbin</span></a></span>\n\n\t</span></span> Thanks a lot!<br>Speaking\
          \ of hardware, what do you use to finetune/run your models?</p>\n"
        raw: '@jondurbin Thanks a lot!

          Speaking of hardware, what do you use to finetune/run your models?'
        updatedAt: '2024-01-23T06:28:12.252Z'
      numEdits: 0
      reactions: []
    id: 65af5c7c563e362f557c3692
    type: comment
  author: MH1P
  content: '@jondurbin Thanks a lot!

    Speaking of hardware, what do you use to finetune/run your models?'
  created_at: 2024-01-23 06:28:12+00:00
  edited: false
  hidden: false
  id: 65af5c7c563e362f557c3692
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/bagel-dpo-8x7b-v0.2
repo_type: model
status: open
target_branch: null
title: Thank you for your model!
