!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ausboss
conflicting_files: null
created_at: 2023-05-26 17:07:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b82ef659060ca9f4c79b73/CKRDlGGcxIQujL78hpSoE.jpeg?w=200&h=200&f=face
      fullname: ausboss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ausboss
      type: user
    createdAt: '2023-05-26T18:07:51.000Z'
    data:
      edited: true
      editors:
      - ausboss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b82ef659060ca9f4c79b73/CKRDlGGcxIQujL78hpSoE.jpeg?w=200&h=200&f=face
          fullname: ausboss
          isHf: false
          isPro: false
          name: ausboss
          type: user
        html: '<p>I''m interested in training a Falcon40b version of this Lora and
          I was curious about the training parameters you used like epochs/steps and
          other potential info that I can use to try and get the closest results to
          what you got with the Supercot Lama30B Lora (the llama30bsupercott merge
          was <a href="/kaiokendev/SuperCOT-LoRA/discussions/1">#1</a> on hugging
          face leader board until recently, now its 2nd and Falcon40B is 1st) . Any
          info would be greatly appreciated.</p>

          '
        raw: 'I''m interested in training a Falcon40b version of this Lora and I was
          curious about the training parameters you used like epochs/steps and other
          potential info that I can use to try and get the closest results to what
          you got with the Supercot Lama30B Lora (the llama30bsupercott merge was
          #1 on hugging face leader board until recently, now its 2nd and Falcon40B
          is 1st) . Any info would be greatly appreciated.'
        updatedAt: '2023-05-26T18:08:30.189Z'
      numEdits: 1
      reactions: []
    id: 6470f577fa9fd77212e198fc
    type: comment
  author: ausboss
  content: 'I''m interested in training a Falcon40b version of this Lora and I was
    curious about the training parameters you used like epochs/steps and other potential
    info that I can use to try and get the closest results to what you got with the
    Supercot Lama30B Lora (the llama30bsupercott merge was #1 on hugging face leader
    board until recently, now its 2nd and Falcon40B is 1st) . Any info would be greatly
    appreciated.'
  created_at: 2023-05-26 17:07:51+00:00
  edited: true
  hidden: false
  id: 6470f577fa9fd77212e198fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-05-26T20:05:57.000Z'
    data:
      edited: false
      editors:
      - kaiokendev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>epochs: 3<br>learning rate: 3e-4<br>lora rank: 8<br>lora alpha:
          16<br>lora dropout: 0.05 for cutoff 1024 13B, otherwise no dropout due to
          gradient checkpointing<br>masking: none<br>mbatch size: 4 (1 for 30B)<br>batch
          size: 8 (2 for 30B)<br>val set size: 0.2<br>sdp implementation: xformers<br>optimizer:
          AdamW<br>eval strategy: none</p>

          <p>Dataset preparation code is here: <a rel="nofollow" href="https://github.com/johnsmith0031/alpaca_lora_4bit/blob/35caccd3764d78afb8da4f4db1faa6faec53ba25/train_data.py#L130">https://github.com/johnsmith0031/alpaca_lora_4bit/blob/35caccd3764d78afb8da4f4db1faa6faec53ba25/train_data.py#L130</a></p>

          '
        raw: 'epochs: 3

          learning rate: 3e-4

          lora rank: 8

          lora alpha: 16

          lora dropout: 0.05 for cutoff 1024 13B, otherwise no dropout due to gradient
          checkpointing

          masking: none

          mbatch size: 4 (1 for 30B)

          batch size: 8 (2 for 30B)

          val set size: 0.2

          sdp implementation: xformers

          optimizer: AdamW

          eval strategy: none


          Dataset preparation code is here: https://github.com/johnsmith0031/alpaca_lora_4bit/blob/35caccd3764d78afb8da4f4db1faa6faec53ba25/train_data.py#L130'
        updatedAt: '2023-05-26T20:05:57.954Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ausboss
    id: 6471112534a8a81ebd8e2ecd
    type: comment
  author: kaiokendev
  content: 'epochs: 3

    learning rate: 3e-4

    lora rank: 8

    lora alpha: 16

    lora dropout: 0.05 for cutoff 1024 13B, otherwise no dropout due to gradient checkpointing

    masking: none

    mbatch size: 4 (1 for 30B)

    batch size: 8 (2 for 30B)

    val set size: 0.2

    sdp implementation: xformers

    optimizer: AdamW

    eval strategy: none


    Dataset preparation code is here: https://github.com/johnsmith0031/alpaca_lora_4bit/blob/35caccd3764d78afb8da4f4db1faa6faec53ba25/train_data.py#L130'
  created_at: 2023-05-26 19:05:57+00:00
  edited: false
  hidden: false
  id: 6471112534a8a81ebd8e2ecd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b82ef659060ca9f4c79b73/CKRDlGGcxIQujL78hpSoE.jpeg?w=200&h=200&f=face
      fullname: ausboss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ausboss
      type: user
    createdAt: '2023-05-26T20:09:56.000Z'
    data:
      edited: false
      editors:
      - ausboss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b82ef659060ca9f4c79b73/CKRDlGGcxIQujL78hpSoE.jpeg?w=200&h=200&f=face
          fullname: ausboss
          isHf: false
          isPro: false
          name: ausboss
          type: user
        html: "<p>Thanks so much \U0001F64F that is exactly what I needed</p>\n"
        raw: "Thanks so much \U0001F64F that is exactly what I needed"
        updatedAt: '2023-05-26T20:09:56.942Z'
      numEdits: 0
      reactions: []
    id: 647112141c2bfd5b7bf90a3e
    type: comment
  author: ausboss
  content: "Thanks so much \U0001F64F that is exactly what I needed"
  created_at: 2023-05-26 19:09:56+00:00
  edited: false
  hidden: false
  id: 647112141c2bfd5b7bf90a3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-05-26T20:32:45.000Z'
    data:
      edited: false
      editors:
      - kaiokendev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>I should mention this LoRA uses the dataset here only: <a href="https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset">https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset</a><br>The
          other datasets referenced have already been incorporated into it and cleaned</p>

          '
        raw: 'I should mention this LoRA uses the dataset here only: https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset

          The other datasets referenced have already been incorporated into it and
          cleaned'
        updatedAt: '2023-05-26T20:32:45.357Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ausboss
    id: 6471176d34a8a81ebd8e90de
    type: comment
  author: kaiokendev
  content: 'I should mention this LoRA uses the dataset here only: https://huggingface.co/datasets/kaiokendev/SuperCOT-dataset

    The other datasets referenced have already been incorporated into it and cleaned'
  created_at: 2023-05-26 19:32:45+00:00
  edited: false
  hidden: false
  id: 6471176d34a8a81ebd8e90de
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: kaiokendev/SuperCOT-LoRA
repo_type: model
status: open
target_branch: null
title: Training info
