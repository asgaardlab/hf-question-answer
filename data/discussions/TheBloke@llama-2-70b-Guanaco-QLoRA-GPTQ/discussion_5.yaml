!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jackboot
conflicting_files: null
created_at: 2023-07-22 18:15:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
      fullname: Jack Boot
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jackboot
      type: user
    createdAt: '2023-07-22T19:15:39.000Z'
    data:
      edited: false
      editors:
      - jackboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9712625741958618
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3637feae0735d26cfb196148c87eef07.svg
          fullname: Jack Boot
          isHf: false
          isPro: false
          name: jackboot
          type: user
        html: '<p>I notice that 128g is used without desc_act. In oobaboga tests it
          was proven that group size alone caused higher perplexity than no grouping
          at all. I realize that using groups + desc_act makes the model less compatible
          with autogptq and other hardware, but this would be the worst of both worlds.
          The higher memory usage and the higher perplexity.</p>

          <p>I think that uploading one model with only desc_act and no groups and
          another model with desc_act + 128g would be ideal to cover both cases and
          still have some benefit.</p>

          <p>What do you think? I know it''s not a big difference and I''m happy this
          exists at all.</p>

          '
        raw: "I notice that 128g is used without desc_act. In oobaboga tests it was\
          \ proven that group size alone caused higher perplexity than no grouping\
          \ at all. I realize that using groups + desc_act makes the model less compatible\
          \ with autogptq and other hardware, but this would be the worst of both\
          \ worlds. The higher memory usage and the higher perplexity.\r\n\r\nI think\
          \ that uploading one model with only desc_act and no groups and another\
          \ model with desc_act + 128g would be ideal to cover both cases and still\
          \ have some benefit.\r\n\r\nWhat do you think? I know it's not a big difference\
          \ and I'm happy this exists at all.\r\n"
        updatedAt: '2023-07-22T19:15:39.384Z'
      numEdits: 0
      reactions: []
    id: 64bc2adb9f94ea2554df5bbf
    type: comment
  author: jackboot
  content: "I notice that 128g is used without desc_act. In oobaboga tests it was\
    \ proven that group size alone caused higher perplexity than no grouping at all.\
    \ I realize that using groups + desc_act makes the model less compatible with\
    \ autogptq and other hardware, but this would be the worst of both worlds. The\
    \ higher memory usage and the higher perplexity.\r\n\r\nI think that uploading\
    \ one model with only desc_act and no groups and another model with desc_act +\
    \ 128g would be ideal to cover both cases and still have some benefit.\r\n\r\n\
    What do you think? I know it's not a big difference and I'm happy this exists\
    \ at all.\r\n"
  created_at: 2023-07-22 18:15:39+00:00
  edited: false
  hidden: false
  id: 64bc2adb9f94ea2554df5bbf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ
repo_type: model
status: open
target_branch: null
title: desc_act?
