!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-07-21 15:21:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-21T16:21:00.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9568551182746887
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I see the link to the GGML model but its a broken link, does this
          mean the ggml version is in the works?</p>

          '
        raw: "I see the link to the GGML model but its a broken link, does this mean\
          \ the ggml version is in the works?\r\n"
        updatedAt: '2023-07-21T16:21:00.882Z'
      numEdits: 0
      reactions: []
    id: 64bab06c4b4ff0d509021dc0
    type: comment
  author: rombodawg
  content: "I see the link to the GGML model but its a broken link, does this mean\
    \ the ggml version is in the works?\r\n"
  created_at: 2023-07-21 15:21:00+00:00
  edited: false
  hidden: false
  id: 64bab06c4b4ff0d509021dc0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-21T16:25:08.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.951453685760498
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That was a mistake. I forgot to disable GGML when configuring this
          quant. I deleted the repo but forgot to update this. I''ve removed the link
          for now</p>

          <p>I may try 70B GGML shortly. The llama.cpp PR seems to be working on CPU
          only. So I could put out some files marked experimental.</p>

          <p>To be clear, they will only work with llama.cpp on the command line,
          not any of the normal GGML UIs (text-generation-webui, LM Studio, KoboldCpp,
          etc) and you will have to compile it from source yourself.  And they will
          be slow as hell due to being CPU only, and you will probably need 64GB RAM.
          </p>

          '
        raw: 'That was a mistake. I forgot to disable GGML when configuring this quant.
          I deleted the repo but forgot to update this. I''ve removed the link for
          now


          I may try 70B GGML shortly. The llama.cpp PR seems to be working on CPU
          only. So I could put out some files marked experimental.


          To be clear, they will only work with llama.cpp on the command line, not
          any of the normal GGML UIs (text-generation-webui, LM Studio, KoboldCpp,
          etc) and you will have to compile it from source yourself.  And they will
          be slow as hell due to being CPU only, and you will probably need 64GB RAM. '
        updatedAt: '2023-07-21T16:25:08.882Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F614"
        users:
        - rombodawg
        - Izard
        - mirek190
    id: 64bab1641363b5c79962ebe1
    type: comment
  author: TheBloke
  content: 'That was a mistake. I forgot to disable GGML when configuring this quant.
    I deleted the repo but forgot to update this. I''ve removed the link for now


    I may try 70B GGML shortly. The llama.cpp PR seems to be working on CPU only.
    So I could put out some files marked experimental.


    To be clear, they will only work with llama.cpp on the command line, not any of
    the normal GGML UIs (text-generation-webui, LM Studio, KoboldCpp, etc) and you
    will have to compile it from source yourself.  And they will be slow as hell due
    to being CPU only, and you will probably need 64GB RAM. '
  created_at: 2023-07-21 15:25:08+00:00
  edited: false
  hidden: false
  id: 64bab1641363b5c79962ebe1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-21T16:26:29.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9949973225593567
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Fair enough</p>

          '
        raw: Fair enough
        updatedAt: '2023-07-21T16:26:29.603Z'
      numEdits: 0
      reactions: []
    id: 64bab1b5d1c7b6d8487bbf10
    type: comment
  author: rombodawg
  content: Fair enough
  created_at: 2023-07-21 15:26:29+00:00
  edited: false
  hidden: false
  id: 64bab1b5d1c7b6d8487bbf10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-07-21T19:10:31.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9798061847686768
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>I am using console anyway for ggml models ;)<br>Also only for CPU
          is not concern for me as well as I have cpu 7950x3d  and getting on cpu
          almost 2t/s with 65b models. </p>

          '
        raw: "I am using console anyway for ggml models ;) \nAlso only for CPU is\
          \ not concern for me as well as I have cpu 7950x3d  and getting on cpu almost\
          \ 2t/s with 65b models. "
        updatedAt: '2023-07-21T19:10:31.542Z'
      numEdits: 0
      reactions: []
    id: 64bad827ae436c8813d2521d
    type: comment
  author: mirek190
  content: "I am using console anyway for ggml models ;) \nAlso only for CPU is not\
    \ concern for me as well as I have cpu 7950x3d  and getting on cpu almost 2t/s\
    \ with 65b models. "
  created_at: 2023-07-21 18:10:31+00:00
  edited: false
  hidden: false
  id: 64bad827ae436c8813d2521d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T08:27:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8416365385055542
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I tried the llama.cpp CPU-only support last night and I couldn''t
          get it working. Watch this for updates: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/2276#issuecomment-1646328190">https://github.com/ggerganov/llama.cpp/pull/2276#issuecomment-1646328190</a></p>

          '
        raw: 'I tried the llama.cpp CPU-only support last night and I couldn''t get
          it working. Watch this for updates: https://github.com/ggerganov/llama.cpp/pull/2276#issuecomment-1646328190'
        updatedAt: '2023-07-22T08:27:21.252Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ronenzyroff
        - mirek190
    id: 64bb92e98e051085bad33446
    type: comment
  author: TheBloke
  content: 'I tried the llama.cpp CPU-only support last night and I couldn''t get
    it working. Watch this for updates: https://github.com/ggerganov/llama.cpp/pull/2276#issuecomment-1646328190'
  created_at: 2023-07-22 07:27:21+00:00
  edited: false
  hidden: false
  id: 64bb92e98e051085bad33446
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ
repo_type: model
status: open
target_branch: null
title: How soon until GGML?
