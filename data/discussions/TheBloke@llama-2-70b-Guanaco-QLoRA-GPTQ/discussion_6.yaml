!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FareedKhan
conflicting_files: null
created_at: 2023-08-08 17:32:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642acc26d651bae3c11db18d/HzQLTkNp8Xkh2i29BJYvW.jpeg?w=200&h=200&f=face
      fullname: Fareed Hassan Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FareedKhan
      type: user
    createdAt: '2023-08-08T18:32:03.000Z'
    data:
      edited: false
      editors:
      - FareedKhan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4437021315097809
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642acc26d651bae3c11db18d/HzQLTkNp8Xkh2i29BJYvW.jpeg?w=200&h=200&f=face
          fullname: Fareed Hassan Khan
          isHf: false
          isPro: false
          name: FareedKhan
          type: user
        html: '<p>ValueError                                Traceback (most recent
          call last)<br> in &lt;cell line: 6&gt;()<br>      4 ''''''<br>      5<br>----&gt;
          6 print(pipe(prompt_template)[0][''generated_text''])</p>

          <p>16 frames<br>/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/fused_llama_attn.py
          in forward(self, hidden_states, past_key_value, attention_mask, position_ids,
          output_attentions, use_cache, **kwargs)<br>     52<br>     53         qkv_states
          = self.qkv_proj(hidden_states)<br>---&gt; 54         query_states, key_states,
          value_states = torch.split(qkv_states, self.hidden_size, dim=2)<br>     55<br>     56         query_states
          = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,
          2)</p>

          <p>ValueError: not enough values to unpack (expected 3, got 2)</p>

          '
        raw: "ValueError                                Traceback (most recent call\
          \ last)\r\n<ipython-input-7-be12ce3e6f68> in <cell line: 6>()\r\n      4\
          \ '''\r\n      5 \r\n----> 6 print(pipe(prompt_template)[0]['generated_text'])\r\
          \n\r\n16 frames\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/fused_llama_attn.py\
          \ in forward(self, hidden_states, past_key_value, attention_mask, position_ids,\
          \ output_attentions, use_cache, **kwargs)\r\n     52 \r\n     53       \
          \  qkv_states = self.qkv_proj(hidden_states)\r\n---> 54         query_states,\
          \ key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\r\
          \n     55 \r\n     56         query_states = query_states.view(bsz, q_len,\
          \ self.num_heads, self.head_dim).transpose(1, 2)\r\n\r\nValueError: not\
          \ enough values to unpack (expected 3, got 2)"
        updatedAt: '2023-08-08T18:32:03.984Z'
      numEdits: 0
      reactions: []
    id: 64d28a23e9cb7c9fdd8925f7
    type: comment
  author: FareedKhan
  content: "ValueError                                Traceback (most recent call\
    \ last)\r\n<ipython-input-7-be12ce3e6f68> in <cell line: 6>()\r\n      4 '''\r\
    \n      5 \r\n----> 6 print(pipe(prompt_template)[0]['generated_text'])\r\n\r\n\
    16 frames\r\n/usr/local/lib/python3.10/dist-packages/auto_gptq/nn_modules/fused_llama_attn.py\
    \ in forward(self, hidden_states, past_key_value, attention_mask, position_ids,\
    \ output_attentions, use_cache, **kwargs)\r\n     52 \r\n     53         qkv_states\
    \ = self.qkv_proj(hidden_states)\r\n---> 54         query_states, key_states,\
    \ value_states = torch.split(qkv_states, self.hidden_size, dim=2)\r\n     55 \r\
    \n     56         query_states = query_states.view(bsz, q_len, self.num_heads,\
    \ self.head_dim).transpose(1, 2)\r\n\r\nValueError: not enough values to unpack\
    \ (expected 3, got 2)"
  created_at: 2023-08-08 17:32:03+00:00
  edited: false
  hidden: false
  id: 64d28a23e9cb7c9fdd8925f7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ
repo_type: model
status: open
target_branch: null
title: 'ValueError: not enough values to unpack'
