!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ipechman
conflicting_files: null
created_at: 2024-01-05 07:10:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666298585081-noauth.png?w=200&h=200&f=face
      fullname: Ilan Pechman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ipechman
      type: user
    createdAt: '2024-01-05T07:10:10.000Z'
    data:
      edited: false
      editors:
      - ipechman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8817938566207886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666298585081-noauth.png?w=200&h=200&f=face
          fullname: Ilan Pechman
          isHf: false
          isPro: false
          name: ipechman
          type: user
        html: '<p>This is an awesome model, one of the best I''ve tested so far, but
          I believe there is a problem with the tokenizer:<br><a href="https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/discussions/1">https://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/discussions/1</a></p>

          '
        raw: "This is an awesome model, one of the best I've tested so far, but I\
          \ believe there is a problem with the tokenizer:\r\nhttps://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/discussions/1"
        updatedAt: '2024-01-05T07:10:10.428Z'
      numEdits: 0
      reactions: []
    id: 6597ab5267b8fc627934ec5a
    type: comment
  author: ipechman
  content: "This is an awesome model, one of the best I've tested so far, but I believe\
    \ there is a problem with the tokenizer:\r\nhttps://huggingface.co/TheBloke/Mixtral_7Bx2_MoE-GGUF/discussions/1"
  created_at: 2024-01-05 07:10:10+00:00
  edited: false
  hidden: false
  id: 6597ab5267b8fc627934ec5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2024-01-05T08:03:44.000Z'
    data:
      edited: true
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4149147570133209
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: "<p>try the following code:</p>\n<pre><code>import torch\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\nimport math\nmodel_path =\
          \ \"cloudyu/Mixtral_7Bx2_MoE\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_default_system_prompt=False)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    model_path, torch_dtype=torch.float32, device_map='auto',local_files_only=False,\
          \ load_in_4bit=True\n)\nprint(model)\nprompt = input(\"please input prompt:\"\
          )\nwhile len(prompt) &gt; 0:\n  input_ids = tokenizer(prompt, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\n\n  generation_output = model.generate(\n\
          \    input_ids=input_ids, max_new_tokens=500,repetition_penalty=1.2\n  )\n\
          \  print(tokenizer.decode(generation_output[0]))\n  prompt = input(\"please\
          \ input prompt:\")\n</code></pre>\n<p>#new line  output as expected, so\
          \ I think it's problem of gguf or llama.cpp</p>\n"
        raw: "try the following code:\n```\nimport torch\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\nimport math\nmodel_path = \"cloudyu/Mixtral_7Bx2_MoE\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_default_system_prompt=False)\n\
          model = AutoModelForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float32,\
          \ device_map='auto',local_files_only=False, load_in_4bit=True\n)\nprint(model)\n\
          prompt = input(\"please input prompt:\")\nwhile len(prompt) > 0:\n  input_ids\
          \ = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n\
          \  generation_output = model.generate(\n    input_ids=input_ids, max_new_tokens=500,repetition_penalty=1.2\n\
          \  )\n  print(tokenizer.decode(generation_output[0]))\n  prompt = input(\"\
          please input prompt:\")\n\n```\n#new line  output as expected, so I think\
          \ it's problem of gguf or llama.cpp"
        updatedAt: '2024-01-05T08:04:12.102Z'
      numEdits: 1
      reactions: []
    id: 6597b7e0ce92304a716ac83f
    type: comment
  author: cloudyu
  content: "try the following code:\n```\nimport torch\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\nimport math\nmodel_path = \"cloudyu/Mixtral_7Bx2_MoE\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model_path, use_default_system_prompt=False)\n\
    model = AutoModelForCausalLM.from_pretrained(\n    model_path, torch_dtype=torch.float32,\
    \ device_map='auto',local_files_only=False, load_in_4bit=True\n)\nprint(model)\n\
    prompt = input(\"please input prompt:\")\nwhile len(prompt) > 0:\n  input_ids\
    \ = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\n  generation_output\
    \ = model.generate(\n    input_ids=input_ids, max_new_tokens=500,repetition_penalty=1.2\n\
    \  )\n  print(tokenizer.decode(generation_output[0]))\n  prompt = input(\"please\
    \ input prompt:\")\n\n```\n#new line  output as expected, so I think it's problem\
    \ of gguf or llama.cpp"
  created_at: 2024-01-05 08:03:44+00:00
  edited: true
  hidden: false
  id: 6597b7e0ce92304a716ac83f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: cloudyu/Mixtral_7Bx2_MoE
repo_type: model
status: open
target_branch: null
title: Problem with tokenizer
