!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-23 16:13:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-23T16:13:11.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.827640950679779
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>Just curious how do you create custom mixtral style models? Do they all
          have to be mistral derivatives and same size?</p>

          <p>Thanks!</p>

          '
        raw: 'Hi,


          Just curious how do you create custom mixtral style models? Do they all
          have to be mistral derivatives and same size?


          Thanks!'
        updatedAt: '2023-12-23T16:13:22.037Z'
      numEdits: 1
      reactions: []
    id: 65870717085a5bce61096fac
    type: comment
  author: Yhyu13
  content: 'Hi,


    Just curious how do you create custom mixtral style models? Do they all have to
    be mistral derivatives and same size?


    Thanks!'
  created_at: 2023-12-23 16:13:11+00:00
  edited: true
  hidden: false
  id: 65870717085a5bce61096fac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ac0c386e729cccdc8290cb5d28ddc4a4.svg
      fullname: Virt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Virt-io
      type: user
    createdAt: '2023-12-24T03:28:05.000Z'
    data:
      edited: false
      editors:
      - Virt-io
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.779075562953949
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ac0c386e729cccdc8290cb5d28ddc4a4.svg
          fullname: Virt
          isHf: false
          isPro: false
          name: Virt-io
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span><br>Before you try\
          \ this make sure you have a lot of ram or a big swap file (this will make\
          \ it take forever)</p>\n<p>I think you can do this with llama models, but\
          \ all the models used have to be of the same type and size.<br>So you can't\
          \ mix llama models with mistral models, or 7B with 13B.</p>\n<p><a rel=\"\
          nofollow\" href=\"https://github.com/cg123/mergekit/blob/mixtral/moe.md\"\
          >How to use</a></p>\n<pre><code>git clone https://github.com/cg123/mergekit\n\
          cd mergekit\ngit switch mixtral\ngit pull\n\n# Use python venv or conda\n\
          pip install -e .\n\n# if you want to use the --load-in-4bit or --load-in-8bit\
          \ flag\npip install scipy bitsandbytes\n</code></pre>\n"
        raw: "@Yhyu13 \nBefore you try this make sure you have a lot of ram or a big\
          \ swap file (this will make it take forever)\n\nI think you can do this\
          \ with llama models, but all the models used have to be of the same type\
          \ and size.\nSo you can't mix llama models with mistral models, or 7B with\
          \ 13B.\n\n[How to use](https://github.com/cg123/mergekit/blob/mixtral/moe.md)\n\
          \n```\ngit clone https://github.com/cg123/mergekit\ncd mergekit\ngit switch\
          \ mixtral\ngit pull\n\n# Use python venv or conda\npip install -e .\n\n\
          # if you want to use the --load-in-4bit or --load-in-8bit flag\npip install\
          \ scipy bitsandbytes\n```"
        updatedAt: '2023-12-24T03:28:05.433Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - sra2022
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 6587a545991d8e7fb2f9fc9c
    type: comment
  author: Virt-io
  content: "@Yhyu13 \nBefore you try this make sure you have a lot of ram or a big\
    \ swap file (this will make it take forever)\n\nI think you can do this with llama\
    \ models, but all the models used have to be of the same type and size.\nSo you\
    \ can't mix llama models with mistral models, or 7B with 13B.\n\n[How to use](https://github.com/cg123/mergekit/blob/mixtral/moe.md)\n\
    \n```\ngit clone https://github.com/cg123/mergekit\ncd mergekit\ngit switch mixtral\n\
    git pull\n\n# Use python venv or conda\npip install -e .\n\n# if you want to use\
    \ the --load-in-4bit or --load-in-8bit flag\npip install scipy bitsandbytes\n\
    ```"
  created_at: 2023-12-24 03:28:05+00:00
  edited: false
  hidden: false
  id: 6587a545991d8e7fb2f9fc9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2023-12-24T03:31:06.000Z'
    data:
      edited: false
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7791278958320618
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\"\
          >@<span class=\"underline\">Yhyu13</span></a></span>\n\n\t</span></span><br>Before\
          \ you try this make sure you have a lot of ram or a big swap file (this\
          \ will make it take forever)</p>\n<p>I think you can do this with llama\
          \ models, but all the models used have to be of the same type and size.<br>So\
          \ you can't mix llama models with mistral models, or 7B with 13B.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://github.com/cg123/mergekit/blob/mixtral/moe.md\"\
          >How to use</a></p>\n<pre><code>git clone https://github.com/cg123/mergekit\n\
          cd mergekit\ngit switch mixtral\ngit pull\n\n# Use python venv or conda\n\
          pip install -e .\n\n# if you want to use the --load-in-4bit or --load-in-8bit\
          \ flag\npip install scipy bitsandbytes\n</code></pre>\n</blockquote>\n<p>yes,\
          \ I totally agree. thanks for your replay.</p>\n"
        raw: "> @Yhyu13 \n> Before you try this make sure you have a lot of ram or\
          \ a big swap file (this will make it take forever)\n> \n> I think you can\
          \ do this with llama models, but all the models used have to be of the same\
          \ type and size.\n> So you can't mix llama models with mistral models, or\
          \ 7B with 13B.\n> \n> [How to use](https://github.com/cg123/mergekit/blob/mixtral/moe.md)\n\
          > \n> ```\n> git clone https://github.com/cg123/mergekit\n> cd mergekit\n\
          > git switch mixtral\n> git pull\n> \n> # Use python venv or conda\n> pip\
          \ install -e .\n> \n> # if you want to use the --load-in-4bit or --load-in-8bit\
          \ flag\n> pip install scipy bitsandbytes\n> ```\n\nyes, I totally agree.\
          \ thanks for your replay."
        updatedAt: '2023-12-24T03:31:06.243Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Virt-io
    id: 6587a5fa509bcae23f64ef96
    type: comment
  author: cloudyu
  content: "> @Yhyu13 \n> Before you try this make sure you have a lot of ram or a\
    \ big swap file (this will make it take forever)\n> \n> I think you can do this\
    \ with llama models, but all the models used have to be of the same type and size.\n\
    > So you can't mix llama models with mistral models, or 7B with 13B.\n> \n> [How\
    \ to use](https://github.com/cg123/mergekit/blob/mixtral/moe.md)\n> \n> ```\n\
    > git clone https://github.com/cg123/mergekit\n> cd mergekit\n> git switch mixtral\n\
    > git pull\n> \n> # Use python venv or conda\n> pip install -e .\n> \n> # if you\
    \ want to use the --load-in-4bit or --load-in-8bit flag\n> pip install scipy bitsandbytes\n\
    > ```\n\nyes, I totally agree. thanks for your replay."
  created_at: 2023-12-24 03:31:06+00:00
  edited: false
  hidden: false
  id: 6587a5fa509bcae23f64ef96
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: cloudyu/Mixtral_7Bx2_MoE
repo_type: model
status: open
target_branch: null
title: How to merge models into moe?
