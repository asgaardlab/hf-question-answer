!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sergei6000
conflicting_files: null
created_at: 2023-06-23 01:51:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
      fullname: Sergei Yakovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sergei6000
      type: user
    createdAt: '2023-06-23T02:51:44.000Z'
    data:
      edited: false
      editors:
      - Sergei6000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7563629746437073
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
          fullname: Sergei Yakovich
          isHf: false
          isPro: false
          name: Sergei6000
          type: user
        html: '<p>Could be an issue  on my end but I get an error that points torwards
          corrupted model + the downloaded filesize is only 6.5GB instead of 7.01.<br>Can
          anybody confirm this?</p>

          <p>Part of the error in gptq-for-llama is<br> size mismatch for model.layers.29.self_attn.o_proj.scales:
          copying a param with shape torch.Size([1, 5120]) from checkpoint, the shape
          in current model is torch.Size([40, 5120]). size mismatch for model.layers.29.self_attn.q_proj.qzeros:
          copying a param with shape torch.Size([1, 640]) from checkpoint, the shape
          in current model is torch.Size([40, 640]). size mismatch for model.layers.29.self_attn.q_proj.scales:
          copying a param with shape torch.Size([1, 5120]) from checkpoint, the shape
          in current model is torch.Size([40, 5120]). size mismatch for model.layers.29.self_attn.v_proj.qzeros:
          copying a param with shape torch.Size([1, 640]) from checkpoint, the shape
          in current model is torch.Size([40, 640]). size mismatch for model.layers.29.self_attn.v_proj.scales:
          copying a param with shape torch.Size([1, 5120]) from checkpoint, the shape
          in current model is torch.Size([40, 5120]). size mismatch for model.layers.29.mlp.down_proj.qzeros:
          copying a param with shape torch.Size([1, 640]) from</p>

          <p>I got that before on a corrupted downloaded.<br>Tried multiple times
          with ooba and manually in the browser.</p>

          '
        raw: "Could be an issue  on my end but I get an error that points torwards\
          \ corrupted model + the downloaded filesize is only 6.5GB instead of 7.01.\r\
          \nCan anybody confirm this?\r\n\r\nPart of the error in gptq-for-llama is\r\
          \n size mismatch for model.layers.29.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([1, 5120]) from checkpoint, the shape in current\
          \ model is torch.Size([40, 5120]). size mismatch for model.layers.29.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 640]) from checkpoint, the shape\
          \ in current model is torch.Size([40, 640]). size mismatch for model.layers.29.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([1, 5120]) from checkpoint, the\
          \ shape in current model is torch.Size([40, 5120]). size mismatch for model.layers.29.self_attn.v_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 640]) from checkpoint, the shape\
          \ in current model is torch.Size([40, 640]). size mismatch for model.layers.29.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([1, 5120]) from checkpoint, the\
          \ shape in current model is torch.Size([40, 5120]). size mismatch for model.layers.29.mlp.down_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 640]) from\r\n\r\nI got that\
          \ before on a corrupted downloaded.\r\nTried multiple times with ooba and\
          \ manually in the browser."
        updatedAt: '2023-06-23T02:51:44.816Z'
      numEdits: 0
      reactions: []
    id: 649508c0de90020134710839
    type: comment
  author: Sergei6000
  content: "Could be an issue  on my end but I get an error that points torwards corrupted\
    \ model + the downloaded filesize is only 6.5GB instead of 7.01.\r\nCan anybody\
    \ confirm this?\r\n\r\nPart of the error in gptq-for-llama is\r\n size mismatch\
    \ for model.layers.29.self_attn.o_proj.scales: copying a param with shape torch.Size([1,\
    \ 5120]) from checkpoint, the shape in current model is torch.Size([40, 5120]).\
    \ size mismatch for model.layers.29.self_attn.q_proj.qzeros: copying a param with\
    \ shape torch.Size([1, 640]) from checkpoint, the shape in current model is torch.Size([40,\
    \ 640]). size mismatch for model.layers.29.self_attn.q_proj.scales: copying a\
    \ param with shape torch.Size([1, 5120]) from checkpoint, the shape in current\
    \ model is torch.Size([40, 5120]). size mismatch for model.layers.29.self_attn.v_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 640]) from checkpoint, the shape in\
    \ current model is torch.Size([40, 640]). size mismatch for model.layers.29.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([1, 5120]) from checkpoint, the shape\
    \ in current model is torch.Size([40, 5120]). size mismatch for model.layers.29.mlp.down_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 640]) from\r\n\r\nI got that before\
    \ on a corrupted downloaded.\r\nTried multiple times with ooba and manually in\
    \ the browser."
  created_at: 2023-06-23 01:51:44+00:00
  edited: false
  hidden: false
  id: 649508c0de90020134710839
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T03:03:44.000Z'
    data:
      edited: true
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9552410840988159
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>Try downloading again, I''ve tried downloading multiple times and
          it works fine. I use git lfs clone to download, so <code>git lfs clone https://huggingface.co/flashvenom/Airoboros-13B-SuperHOT-8K-GPTQ</code>
          should work</p>

          '
        raw: Try downloading again, I've tried downloading multiple times and it works
          fine. I use git lfs clone to download, so `git lfs clone https://huggingface.co/flashvenom/Airoboros-13B-SuperHOT-8K-GPTQ`
          should work
        updatedAt: '2023-06-23T03:04:18.410Z'
      numEdits: 1
      reactions: []
    id: 64950b90394b1e1ee4f25e3f
    type: comment
  author: flashvenom
  content: Try downloading again, I've tried downloading multiple times and it works
    fine. I use git lfs clone to download, so `git lfs clone https://huggingface.co/flashvenom/Airoboros-13B-SuperHOT-8K-GPTQ`
    should work
  created_at: 2023-06-23 02:03:44+00:00
  edited: true
  hidden: false
  id: 64950b90394b1e1ee4f25e3f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
      fullname: Sergei Yakovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sergei6000
      type: user
    createdAt: '2023-06-23T03:34:15.000Z'
    data:
      edited: false
      editors:
      - Sergei6000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.776534914970398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
          fullname: Sergei Yakovich
          isHf: false
          isPro: false
          name: Sergei6000
          type: user
        html: '<p>Weird, with that method I could get it to load in exllama. Still
          getting the same error in gptq-for-llama.<br>Could be an issue on my end,
          but I can load other models.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63fda5d794cc8f815d53a10e/RsmwPi_0u7kZvIFSY8Dxm.png"><img
          alt="Screenshot_20230623_123146.png" src="https://cdn-uploads.huggingface.co/production/uploads/63fda5d794cc8f815d53a10e/RsmwPi_0u7kZvIFSY8Dxm.png"></a></p>

          '
        raw: 'Weird, with that method I could get it to load in exllama. Still getting
          the same error in gptq-for-llama.

          Could be an issue on my end, but I can load other models.

          ![Screenshot_20230623_123146.png](https://cdn-uploads.huggingface.co/production/uploads/63fda5d794cc8f815d53a10e/RsmwPi_0u7kZvIFSY8Dxm.png)

          '
        updatedAt: '2023-06-23T03:34:15.949Z'
      numEdits: 0
      reactions: []
    id: 649512b752a791c445c16684
    type: comment
  author: Sergei6000
  content: 'Weird, with that method I could get it to load in exllama. Still getting
    the same error in gptq-for-llama.

    Could be an issue on my end, but I can load other models.

    ![Screenshot_20230623_123146.png](https://cdn-uploads.huggingface.co/production/uploads/63fda5d794cc8f815d53a10e/RsmwPi_0u7kZvIFSY8Dxm.png)

    '
  created_at: 2023-06-23 02:34:15+00:00
  edited: false
  hidden: false
  id: 649512b752a791c445c16684
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T04:25:58.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9691041111946106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>Ah, I''ve only tried on exLLaMa, I wonder what happened, I will
          re-run the quantize when I get some time</p>

          '
        raw: Ah, I've only tried on exLLaMa, I wonder what happened, I will re-run
          the quantize when I get some time
        updatedAt: '2023-06-23T04:25:58.499Z'
      numEdits: 0
      reactions: []
    id: 64951ed653e3698dd64eef79
    type: comment
  author: flashvenom
  content: Ah, I've only tried on exLLaMa, I wonder what happened, I will re-run the
    quantize when I get some time
  created_at: 2023-06-23 03:25:58+00:00
  edited: false
  hidden: false
  id: 64951ed653e3698dd64eef79
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
      fullname: Sergei Yakovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sergei6000
      type: user
    createdAt: '2023-06-23T06:05:47.000Z'
    data:
      edited: false
      editors:
      - Sergei6000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.965893030166626
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
          fullname: Sergei Yakovich
          isHf: false
          isPro: false
          name: Sergei6000
          type: user
        html: '<p>Thanks, much appreciated!<br>For some reason exllama is slower on
          my system. Could be because of my 2 pascal cards.</p>

          '
        raw: 'Thanks, much appreciated!

          For some reason exllama is slower on my system. Could be because of my 2
          pascal cards.'
        updatedAt: '2023-06-23T06:05:47.621Z'
      numEdits: 0
      reactions: []
    id: 6495363bb39d8731ce14eb5d
    type: comment
  author: Sergei6000
  content: 'Thanks, much appreciated!

    For some reason exllama is slower on my system. Could be because of my 2 pascal
    cards.'
  created_at: 2023-06-23 05:05:47+00:00
  edited: false
  hidden: false
  id: 6495363bb39d8731ce14eb5d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-06-23T06:26:34.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8085208535194397
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>This model is really slow<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/q2oxR92qC2ZdSxYwaGD9D.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/q2oxR92qC2ZdSxYwaGD9D.png"></a></p>

          <p>I tried to fix that in the config.json by changing "use_cache = False"
          to "use_cache = True" but when I do that it doesn''t want to generate anymore</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/7zq6f0s_V4kjP9N55cCV-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/7zq6f0s_V4kjP9N55cCV-.png"></a></p>

          '
        raw: 'This model is really slow

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/q2oxR92qC2ZdSxYwaGD9D.png)


          I tried to fix that in the config.json by changing "use_cache = False" to
          "use_cache = True" but when I do that it doesn''t want to generate anymore


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/7zq6f0s_V4kjP9N55cCV-.png)

          '
        updatedAt: '2023-06-23T06:26:34.683Z'
      numEdits: 0
      reactions: []
    id: 64953b1ab70715a4bc2ce101
    type: comment
  author: TheYuriLover
  content: 'This model is really slow

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/q2oxR92qC2ZdSxYwaGD9D.png)


    I tried to fix that in the config.json by changing "use_cache = False" to "use_cache
    = True" but when I do that it doesn''t want to generate anymore


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/639d9061f87da5e2eb0b2724/7zq6f0s_V4kjP9N55cCV-.png)

    '
  created_at: 2023-06-23 05:26:34+00:00
  edited: false
  hidden: false
  id: 64953b1ab70715a4bc2ce101
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T07:03:46.000Z'
    data:
      edited: true
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609073400497437
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: "<p>What is your setup? It's probably slow cause you are using AutoGPTQ,\
          \ try exllama instead. <span data-props=\"{&quot;user&quot;:&quot;Sergei6000&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Sergei6000\"\
          >@<span class=\"underline\">Sergei6000</span></a></span>\n\n\t</span></span>\
          \ new model is uploaded, idk if it will change anything but try -- if it\
          \ doesn't then I'm unsure what's going on, prob incompatible with llama.cpp\
          \ in its current form</p>\n"
        raw: What is your setup? It's probably slow cause you are using AutoGPTQ,
          try exllama instead. @Sergei6000 new model is uploaded, idk if it will change
          anything but try -- if it doesn't then I'm unsure what's going on, prob
          incompatible with llama.cpp in its current form
        updatedAt: '2023-06-23T07:20:27.304Z'
      numEdits: 1
      reactions: []
    id: 649543d2c59c67dd4e165a48
    type: comment
  author: flashvenom
  content: What is your setup? It's probably slow cause you are using AutoGPTQ, try
    exllama instead. @Sergei6000 new model is uploaded, idk if it will change anything
    but try -- if it doesn't then I'm unsure what's going on, prob incompatible with
    llama.cpp in its current form
  created_at: 2023-06-23 06:03:46+00:00
  edited: true
  hidden: false
  id: 649543d2c59c67dd4e165a48
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-06-23T07:09:59.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754449725151062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;flashvenom&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/flashvenom\">@<span class=\"\
          underline\">flashvenom</span></a></span>\n\n\t</span></span> It's not supposed\
          \ to be that slow, on AutoGPTQ I usually get 7-8 tokens/s with 13b models,\
          \ but like I said your model is too slow because it has \"use_cache = False\"\
          \ on the config.json, but I can't put it into true I got errors when doing\
          \ that</p>\n<p>Yeah I should be using exllama but it doesn't load models\
          \ on windows I don't know why :(</p>\n"
        raw: '@flashvenom It''s not supposed to be that slow, on AutoGPTQ I usually
          get 7-8 tokens/s with 13b models, but like I said your model is too slow
          because it has "use_cache = False" on the config.json, but I can''t put
          it into true I got errors when doing that


          Yeah I should be using exllama but it doesn''t load models on windows I
          don''t know why :('
        updatedAt: '2023-06-23T07:09:59.839Z'
      numEdits: 0
      reactions: []
    id: 64954547de9002013476f6f7
    type: comment
  author: TheYuriLover
  content: '@flashvenom It''s not supposed to be that slow, on AutoGPTQ I usually
    get 7-8 tokens/s with 13b models, but like I said your model is too slow because
    it has "use_cache = False" on the config.json, but I can''t put it into true I
    got errors when doing that


    Yeah I should be using exllama but it doesn''t load models on windows I don''t
    know why :('
  created_at: 2023-06-23 06:09:59+00:00
  edited: false
  hidden: false
  id: 64954547de9002013476f6f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T07:18:10.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9666875600814819
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>Ah, I see, well I just quantisized the model to use with exLLama
          but I don''t know the specifics of why it is slow with AutoGPTQ. I can look
          into it when I get some time. For exllama not loading on windows, try WSL2
          if you can</p>

          '
        raw: Ah, I see, well I just quantisized the model to use with exLLama but
          I don't know the specifics of why it is slow with AutoGPTQ. I can look into
          it when I get some time. For exllama not loading on windows, try WSL2 if
          you can
        updatedAt: '2023-06-23T07:18:10.550Z'
      numEdits: 0
      reactions: []
    id: 64954732c59c67dd4e16baf0
    type: comment
  author: flashvenom
  content: Ah, I see, well I just quantisized the model to use with exLLama but I
    don't know the specifics of why it is slow with AutoGPTQ. I can look into it when
    I get some time. For exllama not loading on windows, try WSL2 if you can
  created_at: 2023-06-23 06:18:10+00:00
  edited: false
  hidden: false
  id: 64954732c59c67dd4e16baf0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-06-23T07:31:43.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9817851781845093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Didn''t know you could quantize the model to fit the exllama paradigm.
          And yeah it works for WSL2, it''s just that the loading is much slower when
          doing that, I hope it''ll get fixed in the future.</p>

          <p>Btw I just tried your model (through WSL2 of course lol) and I''m a big
          fan, like airoboros was really good at following orders and eloquant prose,
          but it was a bit rigid and never added something new to the table, adding
          SuperHOT fixed that issue. This is probably the best 13b model we got right
          there :D</p>

          '
        raw: 'Didn''t know you could quantize the model to fit the exllama paradigm.
          And yeah it works for WSL2, it''s just that the loading is much slower when
          doing that, I hope it''ll get fixed in the future.


          Btw I just tried your model (through WSL2 of course lol) and I''m a big
          fan, like airoboros was really good at following orders and eloquant prose,
          but it was a bit rigid and never added something new to the table, adding
          SuperHOT fixed that issue. This is probably the best 13b model we got right
          there :D'
        updatedAt: '2023-06-23T07:31:55.662Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - flashvenom
    id: 64954a5f5ba8e6c66e1d9de5
    type: comment
  author: TheYuriLover
  content: 'Didn''t know you could quantize the model to fit the exllama paradigm.
    And yeah it works for WSL2, it''s just that the loading is much slower when doing
    that, I hope it''ll get fixed in the future.


    Btw I just tried your model (through WSL2 of course lol) and I''m a big fan, like
    airoboros was really good at following orders and eloquant prose, but it was a
    bit rigid and never added something new to the table, adding SuperHOT fixed that
    issue. This is probably the best 13b model we got right there :D'
  created_at: 2023-06-23 06:31:43+00:00
  edited: true
  hidden: false
  id: 64954a5f5ba8e6c66e1d9de5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
      fullname: Sergei Yakovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sergei6000
      type: user
    createdAt: '2023-06-23T07:46:57.000Z'
    data:
      edited: true
      editors:
      - Sergei6000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8853082060813904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
          fullname: Sergei Yakovich
          isHf: false
          isPro: false
          name: Sergei6000
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;flashvenom&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/flashvenom\">@<span class=\"\
          underline\">flashvenom</span></a></span>\n\n\t</span></span><br>Unfortunately\
          \ didn't help.<br>Its probably because I  use the gptq-for-llama from ooba.<br><a\
          \ rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/issues/1661\"\
          >https://github.com/oobabooga/text-generation-webui/issues/1661</a></p>\n\
          <p>I tried updating to <a rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/old-cuda\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/old-cuda</a>.<br>Able\
          \ to load the model with that but get a RuntimeError: expected scalar type\
          \ Float but found Half on generating.<br>Couldn't really get triton to work\
          \ so I'm out of luck :(</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheYuriLover&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheYuriLover\"\
          >@<span class=\"underline\">TheYuriLover</span></a></span>\n\n\t</span></span><br>Definitely\
          \ the best 13B I have seen yet.<br>Sometimes it really likes to go full\
          \ \"gpt\" though in writing though. Thats most likely from Airoboros though.<br>Haven't\
          \ seen a 13B giving long coherent text outputs like this before.Its really\
          \ good.</p>\n"
        raw: "@flashvenom \nUnfortunately didn't help.\nIts probably because I  use\
          \ the gptq-for-llama from ooba.\nhttps://github.com/oobabooga/text-generation-webui/issues/1661\n\
          \nI tried updating to https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/old-cuda.\n\
          Able to load the model with that but get a RuntimeError: expected scalar\
          \ type Float but found Half on generating.\nCouldn't really get triton to\
          \ work so I'm out of luck :(\n\n@TheYuriLover \nDefinitely the best 13B\
          \ I have seen yet.\nSometimes it really likes to go full \"gpt\" though\
          \ in writing though. Thats most likely from Airoboros though.\nHaven't seen\
          \ a 13B giving long coherent text outputs like this before.Its really good."
        updatedAt: '2023-06-23T07:49:24.839Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - flashvenom
    id: 64954df147b9a900b60cacd7
    type: comment
  author: Sergei6000
  content: "@flashvenom \nUnfortunately didn't help.\nIts probably because I  use\
    \ the gptq-for-llama from ooba.\nhttps://github.com/oobabooga/text-generation-webui/issues/1661\n\
    \nI tried updating to https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/old-cuda.\n\
    Able to load the model with that but get a RuntimeError: expected scalar type\
    \ Float but found Half on generating.\nCouldn't really get triton to work so I'm\
    \ out of luck :(\n\n@TheYuriLover \nDefinitely the best 13B I have seen yet.\n\
    Sometimes it really likes to go full \"gpt\" though in writing though. Thats most\
    \ likely from Airoboros though.\nHaven't seen a 13B giving long coherent text\
    \ outputs like this before.Its really good."
  created_at: 2023-06-23 06:46:57+00:00
  edited: true
  hidden: false
  id: 64954df147b9a900b60cacd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T07:48:01.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8829874396324158
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: "<p>why don't you try exllama? <span data-props=\"{&quot;user&quot;:&quot;Sergei6000&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Sergei6000\"\
          >@<span class=\"underline\">Sergei6000</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'why don''t you try exllama? @Sergei6000 '
        updatedAt: '2023-06-23T07:48:01.938Z'
      numEdits: 0
      reactions: []
    id: 64954e3170964faa8be66ff6
    type: comment
  author: flashvenom
  content: 'why don''t you try exllama? @Sergei6000 '
  created_at: 2023-06-23 06:48:01+00:00
  edited: false
  hidden: false
  id: 64954e3170964faa8be66ff6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
      fullname: Sergei Yakovich
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sergei6000
      type: user
    createdAt: '2023-06-23T08:04:49.000Z'
    data:
      edited: true
      editors:
      - Sergei6000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9674703478813171
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b927ddd9cd9b0854118351477be516d2.svg
          fullname: Sergei Yakovich
          isHf: false
          isPro: false
          name: Sergei6000
          type: user
        html: '<p>Exllama works, but its a bit funky.<br>It seems like the prompt
          takes very long but the actual new output is fast.<br>In Tavernai with a
          long prompt it gets really slow. Almost 50 sec for a reply. Which is almost
          30B levels with cpu offload for me.<br>But its fine for testing. Thanks
          that you tried requantizing for me, might be my older pascal cards.</p>

          '
        raw: "Exllama works, but its a bit funky. \nIt seems like the prompt takes\
          \ very long but the actual new output is fast.\nIn Tavernai with a long\
          \ prompt it gets really slow. Almost 50 sec for a reply. Which is almost\
          \ 30B levels with cpu offload for me.\nBut its fine for testing. Thanks\
          \ that you tried requantizing for me, might be my older pascal cards."
        updatedAt: '2023-06-23T08:05:15.472Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flashvenom
    id: 64955221b7e3e9d4eaa61d0d
    type: comment
  author: Sergei6000
  content: "Exllama works, but its a bit funky. \nIt seems like the prompt takes very\
    \ long but the actual new output is fast.\nIn Tavernai with a long prompt it gets\
    \ really slow. Almost 50 sec for a reply. Which is almost 30B levels with cpu\
    \ offload for me.\nBut its fine for testing. Thanks that you tried requantizing\
    \ for me, might be my older pascal cards."
  created_at: 2023-06-23 07:04:49+00:00
  edited: true
  hidden: false
  id: 64955221b7e3e9d4eaa61d0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e5579e089a57e52149b322e2ed22421a.svg
      fullname: Max Fry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: max-fry
      type: user
    createdAt: '2023-06-24T12:31:12.000Z'
    data:
      edited: false
      editors:
      - max-fry
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733565449714661
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e5579e089a57e52149b322e2ed22421a.svg
          fullname: Max Fry
          isHf: false
          isPro: false
          name: max-fry
          type: user
        html: '<p>It looks like the patch is made for hugging face transformers. Does
          it actually affect ExLlama in any way?</p>

          '
        raw: It looks like the patch is made for hugging face transformers. Does it
          actually affect ExLlama in any way?
        updatedAt: '2023-06-24T12:31:12.526Z'
      numEdits: 0
      reactions: []
    id: 6496e2104a9a7e1fe428f38c
    type: comment
  author: max-fry
  content: It looks like the patch is made for hugging face transformers. Does it
    actually affect ExLlama in any way?
  created_at: 2023-06-24 11:31:12+00:00
  edited: false
  hidden: false
  id: 6496e2104a9a7e1fe428f38c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-24T17:40:34.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9934300780296326
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>exLLama has a built in option to do scaling, its called -cpe</p>

          '
        raw: exLLama has a built in option to do scaling, its called -cpe
        updatedAt: '2023-06-24T17:40:34.792Z'
      numEdits: 0
      reactions: []
    id: 64972a92b41e3c82ac385e93
    type: comment
  author: flashvenom
  content: exLLama has a built in option to do scaling, its called -cpe
  created_at: 2023-06-24 16:40:34+00:00
  edited: false
  hidden: false
  id: 64972a92b41e3c82ac385e93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03b44c61363f3b9625864bce3147c156.svg
      fullname: Giovanni Theissen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giovanith
      type: user
    createdAt: '2023-07-01T11:31:54.000Z'
    data:
      edited: false
      editors:
      - giovanith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8750085830688477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03b44c61363f3b9625864bce3147c156.svg
          fullname: Giovanni Theissen
          isHf: false
          isPro: false
          name: giovanith
          type: user
        html: '<p>Not running here (windows oogabooga).<br>I tried several configs,
          but it returns only ''no-sense'', almost randomly words in chat.<br>I also
          tested Airoboros 13b from TheBloke, yet no success, same wrong answers completely
          messy words in chat.</p>

          <p>Any clue ?<br>Thanks<br>Giovani - Brazil</p>

          '
        raw: "Not running here (windows oogabooga).\nI tried several configs, but\
          \ it returns only 'no-sense', almost randomly words in chat.\nI also tested\
          \ Airoboros 13b from TheBloke, yet no success, same wrong answers completely\
          \ messy words in chat.\n\nAny clue ? \nThanks\nGiovani - Brazil"
        updatedAt: '2023-07-01T11:31:54.289Z'
      numEdits: 0
      reactions: []
    id: 64a00eaae634fdbf5d3555e0
    type: comment
  author: giovanith
  content: "Not running here (windows oogabooga).\nI tried several configs, but it\
    \ returns only 'no-sense', almost randomly words in chat.\nI also tested Airoboros\
    \ 13b from TheBloke, yet no success, same wrong answers completely messy words\
    \ in chat.\n\nAny clue ? \nThanks\nGiovani - Brazil"
  created_at: 2023-07-01 10:31:54+00:00
  edited: false
  hidden: false
  id: 64a00eaae634fdbf5d3555e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36ab2550d5ad9829e0c23cde4c91686f.svg
      fullname: Xeddius Tripp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Xeddius
      type: user
    createdAt: '2023-08-21T12:30:42.000Z'
    data:
      edited: false
      editors:
      - Xeddius
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9377315044403076
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36ab2550d5ad9829e0c23cde4c91686f.svg
          fullname: Xeddius Tripp
          isHf: false
          isPro: false
          name: Xeddius
          type: user
        html: '<p>Can anyone port this to work with faraday.dev or koboldcpp?</p>

          '
        raw: Can anyone port this to work with faraday.dev or koboldcpp?
        updatedAt: '2023-08-21T12:30:42.884Z'
      numEdits: 0
      reactions: []
    id: 64e358f2417214c59cbefda6
    type: comment
  author: Xeddius
  content: Can anyone port this to work with faraday.dev or koboldcpp?
  created_at: 2023-08-21 11:30:42+00:00
  edited: false
  hidden: false
  id: 64e358f2417214c59cbefda6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flashvenom/Airoboros-13B-SuperHOT-8K-4bit-GPTQ
repo_type: model
status: open
target_branch: null
title: Download corrupted/Not working
