!!python/object:huggingface_hub.community.DiscussionWithDetails
author: justsumguy
conflicting_files: null
created_at: 2023-06-25 22:47:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
      fullname: Jared H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: justsumguy
      type: user
    createdAt: '2023-06-25T23:47:59.000Z'
    data:
      edited: false
      editors:
      - justsumguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8956841826438904
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/057c51ca4dcd9bd5ce94a86c062f00a2.svg
          fullname: Jared H
          isHf: false
          isPro: false
          name: justsumguy
          type: user
        html: '<p>Originally the model card said for ExLlama that you had to manually
          add the patch which I couldn''t figure out how to do, then you recently
          updated it with this note:<br>If you are using exllama the monkey-patch
          is built into the engine, please use -cpe to set the scaling factor, ie.
          if you are running it at 4k context, pass -cpe 2 -l 4096</p>

          <p>This may work from running on a command-line but does not specify how
          to pass those parameters to Oobabooga. Applying them to the CMD_FLAGS after
          <code>--loader exllama</code> or <code>--monkey-patch</code> is not a recognized
          argument. Also, that example doesn''t indicate whether cpe goes up to 4
          for 8k context or down to 1, requiring further pre-requisite knowledge to
          use this model properly</p>

          '
        raw: "Originally the model card said for ExLlama that you had to manually\
          \ add the patch which I couldn't figure out how to do, then you recently\
          \ updated it with this note:\r\nIf you are using exllama the monkey-patch\
          \ is built into the engine, please use -cpe to set the scaling factor, ie.\
          \ if you are running it at 4k context, pass -cpe 2 -l 4096\r\n\r\nThis may\
          \ work from running on a command-line but does not specify how to pass those\
          \ parameters to Oobabooga. Applying them to the CMD_FLAGS after `--loader\
          \ exllama` or `--monkey-patch` is not a recognized argument. Also, that\
          \ example doesn't indicate whether cpe goes up to 4 for 8k context or down\
          \ to 1, requiring further pre-requisite knowledge to use this model properly"
        updatedAt: '2023-06-25T23:47:59.079Z'
      numEdits: 0
      reactions: []
    id: 6498d22fc7894887202a0907
    type: comment
  author: justsumguy
  content: "Originally the model card said for ExLlama that you had to manually add\
    \ the patch which I couldn't figure out how to do, then you recently updated it\
    \ with this note:\r\nIf you are using exllama the monkey-patch is built into the\
    \ engine, please use -cpe to set the scaling factor, ie. if you are running it\
    \ at 4k context, pass -cpe 2 -l 4096\r\n\r\nThis may work from running on a command-line\
    \ but does not specify how to pass those parameters to Oobabooga. Applying them\
    \ to the CMD_FLAGS after `--loader exllama` or `--monkey-patch` is not a recognized\
    \ argument. Also, that example doesn't indicate whether cpe goes up to 4 for 8k\
    \ context or down to 1, requiring further pre-requisite knowledge to use this\
    \ model properly"
  created_at: 2023-06-25 22:47:59+00:00
  edited: false
  hidden: false
  id: 6498d22fc7894887202a0907
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-06-26T00:18:34.000Z'
    data:
      edited: false
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9419050216674805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>Same question here, I think it is related to exllama itself. Someone
          would have (I think) a way to add the parameter into ooba.</p>

          '
        raw: Same question here, I think it is related to exllama itself. Someone
          would have (I think) a way to add the parameter into ooba.
        updatedAt: '2023-06-26T00:18:34.397Z'
      numEdits: 0
      reactions: []
    id: 6498d95a6117099581f1fb6a
    type: comment
  author: Panchovix
  content: Same question here, I think it is related to exllama itself. Someone would
    have (I think) a way to add the parameter into ooba.
  created_at: 2023-06-25 23:18:34+00:00
  edited: false
  hidden: false
  id: 6498d95a6117099581f1fb6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    createdAt: '2023-06-26T00:57:25.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2024-01-12T17:02:28.967Z'
      numEdits: 7
      reactions: []
    id: 6498e275a8a2c9873abf551b
    type: comment
  author: deleted
  content: This comment has been hidden
  created_at: 2023-06-25 23:57:25+00:00
  edited: true
  hidden: true
  id: 6498e275a8a2c9873abf551b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
      fullname: Francisco
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Panchovix
      type: user
    createdAt: '2023-06-26T00:58:30.000Z'
    data:
      edited: true
      editors:
      - Panchovix
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8661231398582458
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f52a8d866ed36aba5000ac8d0ef5bc96.svg
          fullname: Francisco
          isHf: false
          isPro: false
          name: Panchovix
          type: user
        html: '<p>I just did a PR which adds these values into ooba to be able to
          set them.</p>

          <p><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/pull/2876">https://github.com/oobabooga/text-generation-webui/pull/2876</a></p>

          <p>Edit: ignore that PR, ooba just did it now as well lol <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui/pull/2875">https://github.com/oobabooga/text-generation-webui/pull/2875</a></p>

          <p>Edit2: it''s merged</p>

          '
        raw: 'I just did a PR which adds these values into ooba to be able to set
          them.


          https://github.com/oobabooga/text-generation-webui/pull/2876


          Edit: ignore that PR, ooba just did it now as well lol https://github.com/oobabooga/text-generation-webui/pull/2875


          Edit2: it''s merged'
        updatedAt: '2023-06-26T02:06:20.857Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nacs
    id: 6498e2b63436cfb6bf6add49
    type: comment
  author: Panchovix
  content: 'I just did a PR which adds these values into ooba to be able to set them.


    https://github.com/oobabooga/text-generation-webui/pull/2876


    Edit: ignore that PR, ooba just did it now as well lol https://github.com/oobabooga/text-generation-webui/pull/2875


    Edit2: it''s merged'
  created_at: 2023-06-25 23:58:30+00:00
  edited: true
  hidden: false
  id: 6498e2b63436cfb6bf6add49
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/392fb33720d1f2f9bddc7f1b18d59340.svg
      fullname: Ichigo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ichigo2899
      type: user
    createdAt: '2023-06-26T05:55:51.000Z'
    data:
      edited: false
      editors:
      - Ichigo2899
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7672135829925537
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/392fb33720d1f2f9bddc7f1b18d59340.svg
          fullname: Ichigo
          isHf: false
          isPro: false
          name: Ichigo2899
          type: user
        html: '<p>Im using huggingface text-generation inference and running the fp16
          version. How do i apply the patching to that?</p>

          '
        raw: 'Im using huggingface text-generation inference and running the fp16
          version. How do i apply the patching to that?

          '
        updatedAt: '2023-06-26T05:55:51.754Z'
      numEdits: 0
      reactions: []
    id: 649928676117099581f7ced6
    type: comment
  author: Ichigo2899
  content: 'Im using huggingface text-generation inference and running the fp16 version.
    How do i apply the patching to that?

    '
  created_at: 2023-06-26 04:55:51+00:00
  edited: false
  hidden: false
  id: 649928676117099581f7ced6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: flashvenom/Airoboros-13B-SuperHOT-8K-4bit-GPTQ
repo_type: model
status: open
target_branch: null
title: Can you be more specific how to use 8k context with ExLLama in Oobabooga?
