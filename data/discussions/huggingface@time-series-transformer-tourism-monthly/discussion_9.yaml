!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yuvalarbel
conflicting_files: null
created_at: 2023-03-01 23:42:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9ea36afef077e8cee59df9acebf0133.svg
      fullname: Yuval Arbel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuvalarbel
      type: user
    createdAt: '2023-03-01T23:42:32.000Z'
    data:
      edited: false
      editors:
      - yuvalarbel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9ea36afef077e8cee59df9acebf0133.svg
          fullname: Yuval Arbel
          isHf: false
          isPro: false
          name: yuvalarbel
          type: user
        html: '<p>Hello!</p>

          <p>I followed the blog and created my own <code>TimeSeriesTransformerForPrediction</code>
          model, using preprocessing with GluonTS, and training using Pytorch Lightning.<br>For
          some reason, on some of my iterations and evaluations for my model I''m
          getting a negative <code>outputs.loss</code>. As this is negative log likelihood,
          this doesn''t make sense.</p>

          <p>Any idea why this can happen? Or maybe point me in a direction to figure
          out my problem?</p>

          <p>I''m using multi-variate prediction (input_size=5).<br>My data is normalized
          (so in particular can have negative values). Of course, the output distribution
          is <code>AffineTransformed(Independent(StudentT()))</code> - and the AffineTransformed
          has <code>loc=0.0</code> (single float), and <code>scale</code> is a torch
          tensor of ones, of size <code>torch.Size([256, 1, 5])</code>. (Hence - normalized
          batch of size 256)</p>

          <p>Any ideas would be great. Thanks!</p>

          '
        raw: "Hello!\r\n\r\nI followed the blog and created my own `TimeSeriesTransformerForPrediction`\
          \ model, using preprocessing with GluonTS, and training using Pytorch Lightning.\r\
          \nFor some reason, on some of my iterations and evaluations for my model\
          \ I'm getting a negative `outputs.loss`. As this is negative log likelihood,\
          \ this doesn't make sense.\r\n\r\nAny idea why this can happen? Or maybe\
          \ point me in a direction to figure out my problem?\r\n\r\nI'm using multi-variate\
          \ prediction (input_size=5).\r\nMy data is normalized (so in particular\
          \ can have negative values). Of course, the output distribution is `AffineTransformed(Independent(StudentT()))`\
          \ - and the AffineTransformed has `loc=0.0` (single float), and `scale`\
          \ is a torch tensor of ones, of size `torch.Size([256, 1, 5])`. (Hence -\
          \ normalized batch of size 256)\r\n\r\nAny ideas would be great. Thanks!"
        updatedAt: '2023-03-01T23:42:32.303Z'
      numEdits: 0
      reactions: []
    id: 63ffe2e8fe6383d50b2fa905
    type: comment
  author: yuvalarbel
  content: "Hello!\r\n\r\nI followed the blog and created my own `TimeSeriesTransformerForPrediction`\
    \ model, using preprocessing with GluonTS, and training using Pytorch Lightning.\r\
    \nFor some reason, on some of my iterations and evaluations for my model I'm getting\
    \ a negative `outputs.loss`. As this is negative log likelihood, this doesn't\
    \ make sense.\r\n\r\nAny idea why this can happen? Or maybe point me in a direction\
    \ to figure out my problem?\r\n\r\nI'm using multi-variate prediction (input_size=5).\r\
    \nMy data is normalized (so in particular can have negative values). Of course,\
    \ the output distribution is `AffineTransformed(Independent(StudentT()))` - and\
    \ the AffineTransformed has `loc=0.0` (single float), and `scale` is a torch tensor\
    \ of ones, of size `torch.Size([256, 1, 5])`. (Hence - normalized batch of size\
    \ 256)\r\n\r\nAny ideas would be great. Thanks!"
  created_at: 2023-03-01 23:42:32+00:00
  edited: false
  hidden: false
  id: 63ffe2e8fe6383d50b2fa905
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
      fullname: Kashif Rasul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kashif
      type: user
    createdAt: '2023-03-02T08:55:24.000Z'
    data:
      edited: true
      editors:
      - kashif
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
          fullname: Kashif Rasul
          isHf: true
          isPro: false
          name: kashif
          type: user
        html: "<p>thanks, <span data-props=\"{&quot;user&quot;:&quot;yuvalarbel&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yuvalarbel\"\
          >@<span class=\"underline\">yuvalarbel</span></a></span>\n\n\t</span></span>\
          \ so the loss is <code>- distrb.log_prob(ground-truth)</code> and so as\
          \ the model gets better and better this value will tend to negative infinity...\
          \ thus negative values are actually good. Note that this is not the proper\
          \ likelihood but rather the likelihood from the distribution and the important\
          \ thing is that it gets smaller and smaller.</p>\n"
        raw: thanks, @yuvalarbel so the loss is `- distrb.log_prob(ground-truth)`
          and so as the model gets better and better this value will tend to negative
          infinity... thus negative values are actually good. Note that this is not
          the proper likelihood but rather the likelihood from the distribution and
          the important thing is that it gets smaller and smaller.
        updatedAt: '2023-03-02T09:03:52.152Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yuvalarbel
    id: 6400647c7945fccb941766ec
    type: comment
  author: kashif
  content: thanks, @yuvalarbel so the loss is `- distrb.log_prob(ground-truth)` and
    so as the model gets better and better this value will tend to negative infinity...
    thus negative values are actually good. Note that this is not the proper likelihood
    but rather the likelihood from the distribution and the important thing is that
    it gets smaller and smaller.
  created_at: 2023-03-02 08:55:24+00:00
  edited: true
  hidden: false
  id: 6400647c7945fccb941766ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9ea36afef077e8cee59df9acebf0133.svg
      fullname: Yuval Arbel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yuvalarbel
      type: user
    createdAt: '2023-03-02T11:41:37.000Z'
    data:
      edited: false
      editors:
      - yuvalarbel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9ea36afef077e8cee59df9acebf0133.svg
          fullname: Yuval Arbel
          isHf: false
          isPro: false
          name: yuvalarbel
          type: user
        html: '<p>Great, thank you!</p>

          '
        raw: Great, thank you!
        updatedAt: '2023-03-02T11:41:37.758Z'
      numEdits: 0
      reactions: []
    id: 64008b712a6805d272d80483
    type: comment
  author: yuvalarbel
  content: Great, thank you!
  created_at: 2023-03-02 11:41:37+00:00
  edited: false
  hidden: false
  id: 64008b712a6805d272d80483
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: huggingface/time-series-transformer-tourism-monthly
repo_type: model
status: open
target_branch: null
title: Getting a negative NLL from outputs.loss - with multi-target normalized data
