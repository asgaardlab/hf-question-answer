!!python/object:huggingface_hub.community.DiscussionWithDetails
author: studygold
conflicting_files: null
created_at: 2023-03-14 18:59:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
      fullname: Josh Goldberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: studygold
      type: user
    createdAt: '2023-03-14T19:59:59.000Z'
    data:
      edited: false
      editors:
      - studygold
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
          fullname: Josh Goldberg
          isHf: false
          isPro: false
          name: studygold
          type: user
        html: "<p>Following the blog post, I want to understand the inference step\
          \ better and how to extend the horizon of the forecast.</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1678823524699-6410b03fe28eb8449fa2f975.png\"\
          ><img alt=\"Screen Shot 2023-03-14 at 12.51.18 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1678823524699-6410b03fe28eb8449fa2f975.png\"\
          ></a></p>\n<p>If we wanted the orange line to go beyond the blue now, what\
          \ is the best way to do that? The model in the image above is from the <a\
          \ href=\"https://huggingface.co/blog/time-series-transformers\">blog post</a>.\
          \ It was trained with the following configuration:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> TimeSeriesTransformerConfig,\
          \ TimeSeriesTransformerForPrediction\n\nconfig = TimeSeriesTransformerConfig(\n\
          \    prediction_length=prediction_length, <span class=\"hljs-comment\">#\
          \ 24 months</span>\n    context_length=prediction_length*<span class=\"\
          hljs-number\">6</span>, <span class=\"hljs-comment\"># context length</span>\n\
          \    lags_sequence=lags_sequence,\n    num_time_features=<span class=\"\
          hljs-built_in\">len</span>(time_features) + <span class=\"hljs-number\"\
          >1</span>, <span class=\"hljs-comment\"># we'll add 2 time features (\"\
          month of year\" and \"age\", see further)</span>\n    num_static_categorical_features=<span\
          \ class=\"hljs-number\">1</span>, <span class=\"hljs-comment\"># we have\
          \ a single static categorical feature, namely time series ID</span>\n  \
          \  cardinality=[<span class=\"hljs-built_in\">len</span>(train_dataset)],\
          \ <span class=\"hljs-comment\"># it has 366 possible values</span>\n   \
          \ embedding_dimension=[<span class=\"hljs-number\">2</span>], <span class=\"\
          hljs-comment\"># the model will learn an embedding of size 2 for each of\
          \ the 366 possible values</span>\n    encoder_layers=<span class=\"hljs-number\"\
          >4</span>, \n    decoder_layers=<span class=\"hljs-number\">4</span>,\n\
          )\n</code></pre>\n<p>So the model is trained to predict 24 months into the\
          \ future. At inference time, we use the <code>.generate()</code> method.\
          \ Only past data is passed because this is a test set.</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">for</span> batch <span class=\"\
          hljs-keyword\">in</span> test_dataloader:\n    outputs = model.generate(\n\
          \        static_categorical_features=batch[<span class=\"hljs-string\">\"\
          static_categorical_features\"</span>].to(device),\n        static_real_features=batch[<span\
          \ class=\"hljs-string\">\"static_real_features\"</span>].to(device),\n \
          \       past_time_features=batch[<span class=\"hljs-string\">\"past_time_features\"\
          </span>].to(device),\n        past_values=batch[<span class=\"hljs-string\"\
          >\"past_values\"</span>].to(device), <span class=\"hljs-comment\"># only\
          \ past values, no future values</span>\n        future_time_features=batch[<span\
          \ class=\"hljs-string\">\"future_time_features\"</span>].to(device),\n \
          \       past_observed_mask=batch[<span class=\"hljs-string\">\"past_observed_mask\"\
          </span>].to(device),\n    )\n</code></pre>\n<p>But these outputs will only\
          \ be for historical data where we have the ground truth. How do we forecast\
          \ beyond our training data? One thought is to recursively forecast by taking\
          \ the outputs from the above model.generate(), and append that to the test\
          \ data, then forecast again. Surely the extension will not be as good as\
          \ the forecast where we have labels. But the end goal of a forecasting model\
          \ is to provide predictions into the future (in this example, that would\
          \ be projecting into the future by the <code>prediction_length</code>: 24\
          \ months). How do we achieve this in an efficient way given this modeling\
          \ package on HuggingFace? Or is it easier to use gluonTS directly for these\
          \ purposes?</p>\n"
        raw: "Following the blog post, I want to understand the inference step better\
          \ and how to extend the horizon of the forecast.\r\n\r\n![Screen Shot 2023-03-14\
          \ at 12.51.18 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1678823524699-6410b03fe28eb8449fa2f975.png)\r\
          \n\r\nIf we wanted the orange line to go beyond the blue now, what is the\
          \ best way to do that? The model in the image above is from the [blog post](https://huggingface.co/blog/time-series-transformers).\
          \ It was trained with the following configuration:\r\n\r\n```python\r\n\
          from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\r\
          \n\r\nconfig = TimeSeriesTransformerConfig(\r\n    prediction_length=prediction_length,\
          \ # 24 months\r\n    context_length=prediction_length*6, # context length\r\
          \n    lags_sequence=lags_sequence,\r\n    num_time_features=len(time_features)\
          \ + 1, # we'll add 2 time features (\"month of year\" and \"age\", see further)\r\
          \n    num_static_categorical_features=1, # we have a single static categorical\
          \ feature, namely time series ID\r\n    cardinality=[len(train_dataset)],\
          \ # it has 366 possible values\r\n    embedding_dimension=[2], # the model\
          \ will learn an embedding of size 2 for each of the 366 possible values\r\
          \n    encoder_layers=4, \r\n    decoder_layers=4,\r\n)\r\n```\r\nSo the\
          \ model is trained to predict 24 months into the future. At inference time,\
          \ we use the `.generate()` method. Only past data is passed because this\
          \ is a test set.\r\n\r\n```python\r\nfor batch in test_dataloader:\r\n \
          \   outputs = model.generate(\r\n        static_categorical_features=batch[\"\
          static_categorical_features\"].to(device),\r\n        static_real_features=batch[\"\
          static_real_features\"].to(device),\r\n        past_time_features=batch[\"\
          past_time_features\"].to(device),\r\n        past_values=batch[\"past_values\"\
          ].to(device), # only past values, no future values\r\n        future_time_features=batch[\"\
          future_time_features\"].to(device),\r\n        past_observed_mask=batch[\"\
          past_observed_mask\"].to(device),\r\n    )\r\n```\r\nBut these outputs will\
          \ only be for historical data where we have the ground truth. How do we\
          \ forecast beyond our training data? One thought is to recursively forecast\
          \ by taking the outputs from the above model.generate(), and append that\
          \ to the test data, then forecast again. Surely the extension will not be\
          \ as good as the forecast where we have labels. But the end goal of a forecasting\
          \ model is to provide predictions into the future (in this example, that\
          \ would be projecting into the future by the `prediction_length`: 24 months).\
          \ How do we achieve this in an efficient way given this modeling package\
          \ on HuggingFace? Or is it easier to use gluonTS directly for these purposes?\r\
          \n"
        updatedAt: '2023-03-14T19:59:59.349Z'
      numEdits: 0
      reactions: []
    id: 6410d23f8573c51c0456fe15
    type: comment
  author: studygold
  content: "Following the blog post, I want to understand the inference step better\
    \ and how to extend the horizon of the forecast.\r\n\r\n![Screen Shot 2023-03-14\
    \ at 12.51.18 PM.png](https://cdn-uploads.huggingface.co/production/uploads/1678823524699-6410b03fe28eb8449fa2f975.png)\r\
    \n\r\nIf we wanted the orange line to go beyond the blue now, what is the best\
    \ way to do that? The model in the image above is from the [blog post](https://huggingface.co/blog/time-series-transformers).\
    \ It was trained with the following configuration:\r\n\r\n```python\r\nfrom transformers\
    \ import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction\r\n\r\
    \nconfig = TimeSeriesTransformerConfig(\r\n    prediction_length=prediction_length,\
    \ # 24 months\r\n    context_length=prediction_length*6, # context length\r\n\
    \    lags_sequence=lags_sequence,\r\n    num_time_features=len(time_features)\
    \ + 1, # we'll add 2 time features (\"month of year\" and \"age\", see further)\r\
    \n    num_static_categorical_features=1, # we have a single static categorical\
    \ feature, namely time series ID\r\n    cardinality=[len(train_dataset)], # it\
    \ has 366 possible values\r\n    embedding_dimension=[2], # the model will learn\
    \ an embedding of size 2 for each of the 366 possible values\r\n    encoder_layers=4,\
    \ \r\n    decoder_layers=4,\r\n)\r\n```\r\nSo the model is trained to predict\
    \ 24 months into the future. At inference time, we use the `.generate()` method.\
    \ Only past data is passed because this is a test set.\r\n\r\n```python\r\nfor\
    \ batch in test_dataloader:\r\n    outputs = model.generate(\r\n        static_categorical_features=batch[\"\
    static_categorical_features\"].to(device),\r\n        static_real_features=batch[\"\
    static_real_features\"].to(device),\r\n        past_time_features=batch[\"past_time_features\"\
    ].to(device),\r\n        past_values=batch[\"past_values\"].to(device), # only\
    \ past values, no future values\r\n        future_time_features=batch[\"future_time_features\"\
    ].to(device),\r\n        past_observed_mask=batch[\"past_observed_mask\"].to(device),\r\
    \n    )\r\n```\r\nBut these outputs will only be for historical data where we\
    \ have the ground truth. How do we forecast beyond our training data? One thought\
    \ is to recursively forecast by taking the outputs from the above model.generate(),\
    \ and append that to the test data, then forecast again. Surely the extension\
    \ will not be as good as the forecast where we have labels. But the end goal of\
    \ a forecasting model is to provide predictions into the future (in this example,\
    \ that would be projecting into the future by the `prediction_length`: 24 months).\
    \ How do we achieve this in an efficient way given this modeling package on HuggingFace?\
    \ Or is it easier to use gluonTS directly for these purposes?\r\n"
  created_at: 2023-03-14 18:59:59+00:00
  edited: false
  hidden: false
  id: 6410d23f8573c51c0456fe15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
      fullname: Kashif Rasul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kashif
      type: user
    createdAt: '2023-03-15T11:22:10.000Z'
    data:
      edited: true
      editors:
      - kashif
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
          fullname: Kashif Rasul
          isHf: true
          isPro: false
          name: kashif
          type: user
        html: "<p>so <span data-props=\"{&quot;user&quot;:&quot;studygold&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/studygold\"\
          >@<span class=\"underline\">studygold</span></a></span>\n\n\t</span></span>\
          \ the <code>batch[\"past_values\"]</code> is indeed from the test data but\
          \ is the last context window before the very last prediction window (which\
          \ is what the test-splitter does). We could have also used the very last\
          \ context window from the training set (but the training dataloader has\
          \ the random window splitter). The test splitter as mentioned will return\
          \ the very last window before the prediction window since that is the way\
          \ the test set was created. </p>\n<pre><code>[ .....  train data ....  \
          \ [context][predict] ....             ]\n[ ...... test data .......... \
          \                        [context]|[prediction window]]\n              \
          \                                         |\n                          \
          \                             L-&gt; batch[\"past_values\"] in generate\n\
          \n[ .... time features .........                                 |[time\
          \ feat        ]]\n                                                     \
          \                |\n                                                   \
          \                  L-&gt; batch[\"future_time_features\"] in generate\n\
          </code></pre>\n<p>Thus the model actually only uses the <code>batch[\"future_time_features\"\
          ]</code> from the \"test\" data which are time features made from the unseen\
          \ future date-times. </p>\n<p>Thus to create predictions for which you have\
          \ no ground truth, you will need to provide the generate function the very\
          \ last context window after which you would like predictions, together with\
          \ the appropriate <code>batch[\"future_time_features\"]</code> for the future.</p>\n\
          <p>Let me know if that makes sense? To iterate, we are only using the unseen\
          \ ground-truth data for plotting and evaluation metrics, the <code>generate</code>\
          \ does not use it.</p>\n"
        raw: "so @studygold the `batch[\"past_values\"]` is indeed from the test data\
          \ but is the last context window before the very last prediction window\
          \ (which is what the test-splitter does). We could have also used the very\
          \ last context window from the training set (but the training dataloader\
          \ has the random window splitter). The test splitter as mentioned will return\
          \ the very last window before the prediction window since that is the way\
          \ the test set was created. \n\n```\n[ .....  train data ....   [context][predict]\
          \ ....             ]\n[ ...... test data ..........                    \
          \     [context]|[prediction window]]\n                                 \
          \                      |\n                                             \
          \          L-> batch[\"past_values\"] in generate\n\n[ .... time features\
          \ .........                                 |[time feat        ]]\n    \
          \                                                                 |\n  \
          \                                                                   L->\
          \ batch[\"future_time_features\"] in generate\n```\n\nThus the model actually\
          \ only uses the `batch[\"future_time_features\"]` from the \"test\" data\
          \ which are time features made from the unseen future date-times. \n\nThus\
          \ to create predictions for which you have no ground truth, you will need\
          \ to provide the generate function the very last context window after which\
          \ you would like predictions, together with the appropriate `batch[\"future_time_features\"\
          ]` for the future.\n\nLet me know if that makes sense? To iterate, we are\
          \ only using the unseen ground-truth data for plotting and evaluation metrics,\
          \ the `generate` does not use it."
        updatedAt: '2023-03-16T22:05:40.867Z'
      numEdits: 23
      reactions: []
    id: 6411aa62d7792bd687fa1177
    type: comment
  author: kashif
  content: "so @studygold the `batch[\"past_values\"]` is indeed from the test data\
    \ but is the last context window before the very last prediction window (which\
    \ is what the test-splitter does). We could have also used the very last context\
    \ window from the training set (but the training dataloader has the random window\
    \ splitter). The test splitter as mentioned will return the very last window before\
    \ the prediction window since that is the way the test set was created. \n\n```\n\
    [ .....  train data ....   [context][predict] ....             ]\n[ ...... test\
    \ data ..........                         [context]|[prediction window]]\n   \
    \                                                    |\n                     \
    \                                  L-> batch[\"past_values\"] in generate\n\n\
    [ .... time features .........                                 |[time feat   \
    \     ]]\n                                                                   \
    \  |\n                                                                     L->\
    \ batch[\"future_time_features\"] in generate\n```\n\nThus the model actually\
    \ only uses the `batch[\"future_time_features\"]` from the \"test\" data which\
    \ are time features made from the unseen future date-times. \n\nThus to create\
    \ predictions for which you have no ground truth, you will need to provide the\
    \ generate function the very last context window after which you would like predictions,\
    \ together with the appropriate `batch[\"future_time_features\"]` for the future.\n\
    \nLet me know if that makes sense? To iterate, we are only using the unseen ground-truth\
    \ data for plotting and evaluation metrics, the `generate` does not use it."
  created_at: 2023-03-15 10:22:10+00:00
  edited: true
  hidden: false
  id: 6411aa62d7792bd687fa1177
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
      fullname: Josh Goldberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: studygold
      type: user
    createdAt: '2023-03-16T01:21:27.000Z'
    data:
      edited: true
      editors:
      - studygold
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
          fullname: Josh Goldberg
          isHf: false
          isPro: false
          name: studygold
          type: user
        html: "<blockquote>\n<p>Thus to create predictions for which you have no ground\
          \ truth, you will need to provide the generate function the very last context\
          \ window after which you would like predictions, together with the appropriate\
          \ batch[\"future_time_features\"] for the future.</p>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;kashif&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/kashif\">@<span class=\"underline\"\
          >kashif</span></a></span>\n\n\t</span></span>  How do I do this? The two\
          \ pieces of code to modify seem to be:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> gluonts.transform.sampler <span\
          \ class=\"hljs-keyword\">import</span> InstanceSampler\n<span class=\"hljs-keyword\"\
          >from</span> typing <span class=\"hljs-keyword\">import</span> <span class=\"\
          hljs-type\">Optional</span>\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">create_instance_splitter</span>(<span class=\"\
          hljs-params\">config: PretrainedConfig, mode: <span class=\"hljs-built_in\"\
          >str</span>, train_sampler: <span class=\"hljs-type\">Optional</span>[InstanceSampler]\
          \ = <span class=\"hljs-literal\">None</span>,</span>\n<span class=\"hljs-params\"\
          >    validation_sampler: <span class=\"hljs-type\">Optional</span>[InstanceSampler]\
          \ = <span class=\"hljs-literal\">None</span>,</span>) -&gt; Transformation:\n\
          \    <span class=\"hljs-keyword\">assert</span> mode <span class=\"hljs-keyword\"\
          >in</span> [<span class=\"hljs-string\">\"train\"</span>, <span class=\"\
          hljs-string\">\"validation\"</span>, <span class=\"hljs-string\">\"test\"\
          </span>]\n\n    instance_sampler = {\n        <span class=\"hljs-string\"\
          >\"train\"</span>: train_sampler <span class=\"hljs-keyword\">or</span>\
          \ ExpectedNumInstanceSampler(\n            num_instances=<span class=\"\
          hljs-number\">1.0</span>, min_future=config.prediction_length\n        ),\n\
          \        <span class=\"hljs-string\">\"validation\"</span>:  validation_sampler\
          \ <span class=\"hljs-keyword\">or</span> ValidationSplitSampler(\n     \
          \       min_future=config.prediction_length\n        ),\n        <span class=\"\
          hljs-string\">\"test\"</span>: TestSplitSampler(),\n    }[mode]\n\n    <span\
          \ class=\"hljs-keyword\">return</span> InstanceSplitter(\n        target_field=<span\
          \ class=\"hljs-string\">\"values\"</span>,\n        is_pad_field=FieldName.IS_PAD,\n\
          \        start_field=FieldName.START,\n        forecast_start_field=FieldName.FORECAST_START,\n\
          \        instance_sampler=instance_sampler,\n        past_length=config.context_length\
          \ + <span class=\"hljs-built_in\">max</span>(config.lags_sequence),\n  \
          \      future_length=config.prediction_length,\n        time_series_fields=[\n\
          \            <span class=\"hljs-string\">\"time_features\"</span>,\n   \
          \         <span class=\"hljs-string\">\"observed_mask\"</span>,\n      \
          \  ],\n    )\n</code></pre>\n<p>and</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >create_test_dataloader</span>(<span class=\"hljs-params\"></span>\n<span\
          \ class=\"hljs-params\">    config: PretrainedConfig,</span>\n<span class=\"\
          hljs-params\">    freq,</span>\n<span class=\"hljs-params\">    data,</span>\n\
          <span class=\"hljs-params\">    batch_size: <span class=\"hljs-built_in\"\
          >int</span>,</span>\n<span class=\"hljs-params\">    **kwargs,</span>\n\
          <span class=\"hljs-params\"></span>):\n    PREDICTION_INPUT_NAMES = [\n\
          \        <span class=\"hljs-string\">\"static_categorical_features\"</span>,\n\
          \        <span class=\"hljs-string\">\"static_real_features\"</span>,\n\
          \        <span class=\"hljs-string\">\"past_time_features\"</span>,\n  \
          \      <span class=\"hljs-string\">\"past_values\"</span>,\n        <span\
          \ class=\"hljs-string\">\"past_observed_mask\"</span>,\n        <span class=\"\
          hljs-string\">\"future_time_features\"</span>,\n        ]\n    \n    transformation\
          \ = create_transformation(freq, config)\n    transformed_data = transformation.apply(data,\
          \ is_train=<span class=\"hljs-literal\">False</span>)\n    \n    <span class=\"\
          hljs-comment\"># we create a Test Instance splitter which will sample the\
          \ very last </span>\n    <span class=\"hljs-comment\"># context window seen\
          \ during training only for the encoder.</span>\n    instance_sampler = create_instance_splitter(\n\
          \        config, <span class=\"hljs-string\">\"test\"</span>\n    ) + SelectFields(PREDICTION_INPUT_NAMES)\n\
          \    \n    <span class=\"hljs-comment\"># we apply the transformations in\
          \ test mode</span>\n    testing_instances = instance_sampler.apply(transformed_data,\
          \ is_train=<span class=\"hljs-literal\">False</span>)\n    \n    <span class=\"\
          hljs-comment\"># This returns a Dataloader which will go over the dataset\
          \ once.</span>\n    <span class=\"hljs-keyword\">return</span> DataLoader(IterableDataset(testing_instances),\
          \ batch_size=batch_size, **kwargs)\n</code></pre>\n<p>Though the test loader\
          \ may not be usable and we need to make an \"out of sample data loader?\"\
          \ I am not sure how to do that though.</p>\n"
        raw: "> Thus to create predictions for which you have no ground truth, you\
          \ will need to provide the generate function the very last context window\
          \ after which you would like predictions, together with the appropriate\
          \ batch[\"future_time_features\"] for the future.\n\n@kashif  How do I do\
          \ this? The two pieces of code to modify seem to be:\n\n```python\nfrom\
          \ gluonts.transform.sampler import InstanceSampler\nfrom typing import Optional\n\
          \ndef create_instance_splitter(config: PretrainedConfig, mode: str, train_sampler:\
          \ Optional[InstanceSampler] = None,\n    validation_sampler: Optional[InstanceSampler]\
          \ = None,) -> Transformation:\n    assert mode in [\"train\", \"validation\"\
          , \"test\"]\n\n    instance_sampler = {\n        \"train\": train_sampler\
          \ or ExpectedNumInstanceSampler(\n            num_instances=1.0, min_future=config.prediction_length\n\
          \        ),\n        \"validation\":  validation_sampler or ValidationSplitSampler(\n\
          \            min_future=config.prediction_length\n        ),\n        \"\
          test\": TestSplitSampler(),\n    }[mode]\n\n    return InstanceSplitter(\n\
          \        target_field=\"values\",\n        is_pad_field=FieldName.IS_PAD,\n\
          \        start_field=FieldName.START,\n        forecast_start_field=FieldName.FORECAST_START,\n\
          \        instance_sampler=instance_sampler,\n        past_length=config.context_length\
          \ + max(config.lags_sequence),\n        future_length=config.prediction_length,\n\
          \        time_series_fields=[\n            \"time_features\",\n        \
          \    \"observed_mask\",\n        ],\n    )\n```\n\nand\n\n```python\ndef\
          \ create_test_dataloader(\n    config: PretrainedConfig,\n    freq,\n  \
          \  data,\n    batch_size: int,\n    **kwargs,\n):\n    PREDICTION_INPUT_NAMES\
          \ = [\n        \"static_categorical_features\",\n        \"static_real_features\"\
          ,\n        \"past_time_features\",\n        \"past_values\",\n        \"\
          past_observed_mask\",\n        \"future_time_features\",\n        ]\n  \
          \  \n    transformation = create_transformation(freq, config)\n    transformed_data\
          \ = transformation.apply(data, is_train=False)\n    \n    # we create a\
          \ Test Instance splitter which will sample the very last \n    # context\
          \ window seen during training only for the encoder.\n    instance_sampler\
          \ = create_instance_splitter(\n        config, \"test\"\n    ) + SelectFields(PREDICTION_INPUT_NAMES)\n\
          \    \n    # we apply the transformations in test mode\n    testing_instances\
          \ = instance_sampler.apply(transformed_data, is_train=False)\n    \n   \
          \ # This returns a Dataloader which will go over the dataset once.\n   \
          \ return DataLoader(IterableDataset(testing_instances), batch_size=batch_size,\
          \ **kwargs)\n```\n\nThough the test loader may not be usable and we need\
          \ to make an \"out of sample data loader?\" I am not sure how to do that\
          \ though."
        updatedAt: '2023-03-16T01:22:52.105Z'
      numEdits: 1
      reactions: []
    id: 64126f17ed3c91c5e9f2f531
    type: comment
  author: studygold
  content: "> Thus to create predictions for which you have no ground truth, you will\
    \ need to provide the generate function the very last context window after which\
    \ you would like predictions, together with the appropriate batch[\"future_time_features\"\
    ] for the future.\n\n@kashif  How do I do this? The two pieces of code to modify\
    \ seem to be:\n\n```python\nfrom gluonts.transform.sampler import InstanceSampler\n\
    from typing import Optional\n\ndef create_instance_splitter(config: PretrainedConfig,\
    \ mode: str, train_sampler: Optional[InstanceSampler] = None,\n    validation_sampler:\
    \ Optional[InstanceSampler] = None,) -> Transformation:\n    assert mode in [\"\
    train\", \"validation\", \"test\"]\n\n    instance_sampler = {\n        \"train\"\
    : train_sampler or ExpectedNumInstanceSampler(\n            num_instances=1.0,\
    \ min_future=config.prediction_length\n        ),\n        \"validation\":  validation_sampler\
    \ or ValidationSplitSampler(\n            min_future=config.prediction_length\n\
    \        ),\n        \"test\": TestSplitSampler(),\n    }[mode]\n\n    return\
    \ InstanceSplitter(\n        target_field=\"values\",\n        is_pad_field=FieldName.IS_PAD,\n\
    \        start_field=FieldName.START,\n        forecast_start_field=FieldName.FORECAST_START,\n\
    \        instance_sampler=instance_sampler,\n        past_length=config.context_length\
    \ + max(config.lags_sequence),\n        future_length=config.prediction_length,\n\
    \        time_series_fields=[\n            \"time_features\",\n            \"\
    observed_mask\",\n        ],\n    )\n```\n\nand\n\n```python\ndef create_test_dataloader(\n\
    \    config: PretrainedConfig,\n    freq,\n    data,\n    batch_size: int,\n \
    \   **kwargs,\n):\n    PREDICTION_INPUT_NAMES = [\n        \"static_categorical_features\"\
    ,\n        \"static_real_features\",\n        \"past_time_features\",\n      \
    \  \"past_values\",\n        \"past_observed_mask\",\n        \"future_time_features\"\
    ,\n        ]\n    \n    transformation = create_transformation(freq, config)\n\
    \    transformed_data = transformation.apply(data, is_train=False)\n    \n   \
    \ # we create a Test Instance splitter which will sample the very last \n    #\
    \ context window seen during training only for the encoder.\n    instance_sampler\
    \ = create_instance_splitter(\n        config, \"test\"\n    ) + SelectFields(PREDICTION_INPUT_NAMES)\n\
    \    \n    # we apply the transformations in test mode\n    testing_instances\
    \ = instance_sampler.apply(transformed_data, is_train=False)\n    \n    # This\
    \ returns a Dataloader which will go over the dataset once.\n    return DataLoader(IterableDataset(testing_instances),\
    \ batch_size=batch_size, **kwargs)\n```\n\nThough the test loader may not be usable\
    \ and we need to make an \"out of sample data loader?\" I am not sure how to do\
    \ that though."
  created_at: 2023-03-16 00:21:27+00:00
  edited: true
  hidden: false
  id: 64126f17ed3c91c5e9f2f531
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
      fullname: Kashif Rasul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kashif
      type: user
    createdAt: '2023-03-16T12:35:52.000Z'
    data:
      edited: false
      editors:
      - kashif
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
          fullname: Kashif Rasul
          isHf: true
          isPro: false
          name: kashif
          type: user
        html: '<p>one quick way to test things out is to make a test-set by adding
          dummy values to the end of the target array and then the gluonts helpers
          etc as in blog post can be used to make the appropriate tensors for the
          generate function.</p>

          '
        raw: one quick way to test things out is to make a test-set by adding dummy
          values to the end of the target array and then the gluonts helpers etc as
          in blog post can be used to make the appropriate tensors for the generate
          function.
        updatedAt: '2023-03-16T12:35:52.662Z'
      numEdits: 0
      reactions: []
    id: 64130d28b9ae8e8d066bacb7
    type: comment
  author: kashif
  content: one quick way to test things out is to make a test-set by adding dummy
    values to the end of the target array and then the gluonts helpers etc as in blog
    post can be used to make the appropriate tensors for the generate function.
  created_at: 2023-03-16 11:35:52+00:00
  edited: false
  hidden: false
  id: 64130d28b9ae8e8d066bacb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
      fullname: Josh Goldberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: studygold
      type: user
    createdAt: '2023-03-16T17:47:10.000Z'
    data:
      edited: false
      editors:
      - studygold
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
          fullname: Josh Goldberg
          isHf: false
          isPro: false
          name: studygold
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kashif&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kashif\">@<span class=\"\
          underline\">kashif</span></a></span>\n\n\t</span></span> What do you mean\
          \ by dummy values (I have no categorical data)? Do you mean extend the target\
          \ array into the future with missing values? I think predicting out of sample\
          \ is a standard desire for anyone making a forecast model. Could you provide\
          \ a code example for how to do this? It seems like it should be supported.\
          \ For example, if you fit a model in gluonTS, to predict out of sample you\
          \ just need to:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-comment\"># Train a DeepAR model on all data but the last 36 months</span>\n\
          training_data, test_gen = split(dataset, offset=-<span class=\"hljs-number\"\
          >36</span>)\nmodel = DeepAREstimator(\n    prediction_length=<span class=\"\
          hljs-number\">12</span>, \n    freq=<span class=\"hljs-string\">\"M\"</span>,\
          \ \n    trainer=Trainer(epochs=<span class=\"hljs-number\">5</span>)\n).train(training_data)\n\
          </code></pre>\n<p>Then predict out of sample like this:</p>\n<pre><code\
          \ class=\"language-python\">future_pred = <span class=\"hljs-built_in\"\
          >list</span>(model.predict(dataset))\n</code></pre>\n<pre><code class=\"\
          language-python\">df[<span class=\"hljs-string\">\"#Passengers\"</span>].plot(color=<span\
          \ class=\"hljs-string\">\"black\"</span>, figsize=(<span class=\"hljs-number\"\
          >15</span>, <span class=\"hljs-number\">3</span>))\n<span class=\"hljs-keyword\"\
          >for</span> f <span class=\"hljs-keyword\">in</span> future_pred:\n    f.plot()\n\
          \    plt.show()\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1678988764558-6410b03fe28eb8449fa2f975.png\"\
          ><img alt=\"Screen Shot 2023-03-16 at 10.45.49 AM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1678988764558-6410b03fe28eb8449fa2f975.png\"\
          ></a></p>\n<p>I was hoping there was an easy way to do it with this Transformer\
          \ on Hugging face. Otherwise, wouldn't it be easier to use the Transformer\
          \ on <a rel=\"nofollow\" href=\"https://github.com/awslabs/gluonts/blob/dev/src/gluonts/mx/model/transformer/_estimator.py\"\
          >gluonTS</a>?</p>\n"
        raw: "@kashif What do you mean by dummy values (I have no categorical data)?\
          \ Do you mean extend the target array into the future with missing values?\
          \ I think predicting out of sample is a standard desire for anyone making\
          \ a forecast model. Could you provide a code example for how to do this?\
          \ It seems like it should be supported. For example, if you fit a model\
          \ in gluonTS, to predict out of sample you just need to:\n\n```python\n\
          # Train a DeepAR model on all data but the last 36 months\ntraining_data,\
          \ test_gen = split(dataset, offset=-36)\nmodel = DeepAREstimator(\n    prediction_length=12,\
          \ \n    freq=\"M\", \n    trainer=Trainer(epochs=5)\n).train(training_data)\n\
          ```\n\nThen predict out of sample like this:\n```python\nfuture_pred = list(model.predict(dataset))\n\
          ```\n\n```python\ndf[\"#Passengers\"].plot(color=\"black\", figsize=(15,\
          \ 3))\nfor f in future_pred:\n    f.plot()\n    plt.show()\n```\n![Screen\
          \ Shot 2023-03-16 at 10.45.49 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1678988764558-6410b03fe28eb8449fa2f975.png)\n\
          \nI was hoping there was an easy way to do it with this Transformer on Hugging\
          \ face. Otherwise, wouldn't it be easier to use the Transformer on [gluonTS](https://github.com/awslabs/gluonts/blob/dev/src/gluonts/mx/model/transformer/_estimator.py)?"
        updatedAt: '2023-03-16T17:47:10.793Z'
      numEdits: 0
      reactions: []
    id: 6413561e2f36ddb7e2cb8679
    type: comment
  author: studygold
  content: "@kashif What do you mean by dummy values (I have no categorical data)?\
    \ Do you mean extend the target array into the future with missing values? I think\
    \ predicting out of sample is a standard desire for anyone making a forecast model.\
    \ Could you provide a code example for how to do this? It seems like it should\
    \ be supported. For example, if you fit a model in gluonTS, to predict out of\
    \ sample you just need to:\n\n```python\n# Train a DeepAR model on all data but\
    \ the last 36 months\ntraining_data, test_gen = split(dataset, offset=-36)\nmodel\
    \ = DeepAREstimator(\n    prediction_length=12, \n    freq=\"M\", \n    trainer=Trainer(epochs=5)\n\
    ).train(training_data)\n```\n\nThen predict out of sample like this:\n```python\n\
    future_pred = list(model.predict(dataset))\n```\n\n```python\ndf[\"#Passengers\"\
    ].plot(color=\"black\", figsize=(15, 3))\nfor f in future_pred:\n    f.plot()\n\
    \    plt.show()\n```\n![Screen Shot 2023-03-16 at 10.45.49 AM.png](https://cdn-uploads.huggingface.co/production/uploads/1678988764558-6410b03fe28eb8449fa2f975.png)\n\
    \nI was hoping there was an easy way to do it with this Transformer on Hugging\
    \ face. Otherwise, wouldn't it be easier to use the Transformer on [gluonTS](https://github.com/awslabs/gluonts/blob/dev/src/gluonts/mx/model/transformer/_estimator.py)?"
  created_at: 2023-03-16 16:47:10+00:00
  edited: false
  hidden: false
  id: 6413561e2f36ddb7e2cb8679
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
      fullname: Kashif Rasul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kashif
      type: user
    createdAt: '2023-03-16T18:15:14.000Z'
    data:
      edited: false
      editors:
      - kashif
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
          fullname: Kashif Rasul
          isHf: true
          isPro: false
          name: kashif
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;studygold&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/studygold\">@<span class=\"\
          underline\">studygold</span></a></span>\n\n\t</span></span> what I meant\
          \ was to append an array of say <code>[-1, ... -1]</code> to the end of\
          \ your time series <code>target</code> values, where the length of this\
          \ array is your prediction length, and then use this resulting dataset as\
          \ the test-set as per the blog. This will result in the generate function\
          \ predicting the time points corresponding to the dummy values you appended.\
          \  I can check if I can add this to the blog.</p>\n"
        raw: '@studygold what I meant was to append an array of say `[-1, ... -1]`
          to the end of your time series `target` values, where the length of this
          array is your prediction length, and then use this resulting dataset as
          the test-set as per the blog. This will result in the generate function
          predicting the time points corresponding to the dummy values you appended.  I
          can check if I can add this to the blog.'
        updatedAt: '2023-03-16T18:15:14.956Z'
      numEdits: 0
      reactions: []
    id: 64135cb24c23691ebe20fe46
    type: comment
  author: kashif
  content: '@studygold what I meant was to append an array of say `[-1, ... -1]` to
    the end of your time series `target` values, where the length of this array is
    your prediction length, and then use this resulting dataset as the test-set as
    per the blog. This will result in the generate function predicting the time points
    corresponding to the dummy values you appended.  I can check if I can add this
    to the blog.'
  created_at: 2023-03-16 17:15:14+00:00
  edited: false
  hidden: false
  id: 64135cb24c23691ebe20fe46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
      fullname: Josh Goldberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: studygold
      type: user
    createdAt: '2023-03-16T21:33:39.000Z'
    data:
      edited: false
      editors:
      - studygold
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a5e2142440eb7118c595515f6b8b6e3.svg
          fullname: Josh Goldberg
          isHf: false
          isPro: false
          name: studygold
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kashif&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kashif\">@<span class=\"\
          underline\">kashif</span></a></span>\n\n\t</span></span> I see. So you have\
          \ to extend the data set into the future so the generate function predicts\
          \ those values. How do you make it ignore the -1 dummy values?</p>\n"
        raw: '@kashif I see. So you have to extend the data set into the future so
          the generate function predicts those values. How do you make it ignore the
          -1 dummy values?'
        updatedAt: '2023-03-16T21:33:39.368Z'
      numEdits: 0
      reactions: []
    id: 64138b332f36ddb7e2db6d19
    type: comment
  author: studygold
  content: '@kashif I see. So you have to extend the data set into the future so the
    generate function predicts those values. How do you make it ignore the -1 dummy
    values?'
  created_at: 2023-03-16 20:33:39+00:00
  edited: false
  hidden: false
  id: 64138b332f36ddb7e2db6d19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
      fullname: Kashif Rasul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kashif
      type: user
    createdAt: '2023-03-16T21:41:32.000Z'
    data:
      edited: false
      editors:
      - kashif
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
          fullname: Kashif Rasul
          isHf: true
          isPro: false
          name: kashif
          type: user
        html: '<p>as mentioned, the test-splitter helper does that for you... that
          is what is happening with the actual test set as well... as shown in the
          crappy diagram above</p>

          '
        raw: as mentioned, the test-splitter helper does that for you... that is what
          is happening with the actual test set as well... as shown in the crappy
          diagram above
        updatedAt: '2023-03-16T21:41:32.519Z'
      numEdits: 0
      reactions: []
    id: 64138d0c6c4cf03e0e49096e
    type: comment
  author: kashif
  content: as mentioned, the test-splitter helper does that for you... that is what
    is happening with the actual test set as well... as shown in the crappy diagram
    above
  created_at: 2023-03-16 20:41:32+00:00
  edited: false
  hidden: false
  id: 64138d0c6c4cf03e0e49096e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: huggingface/time-series-transformer-tourism-monthly
repo_type: model
status: open
target_branch: null
title: Extending Horizon and Understanding Inference Step
