!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nisseb
conflicting_files: null
created_at: 2022-11-11 10:41:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f642c8141d504622f2ecfef36da1db9.svg
      fullname: "Nils B\xE4ckstr\xF6m"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nisseb
      type: user
    createdAt: '2022-11-11T10:41:54.000Z'
    data:
      edited: false
      editors:
      - nisseb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f642c8141d504622f2ecfef36da1db9.svg
          fullname: "Nils B\xE4ckstr\xF6m"
          isHf: false
          isPro: false
          name: nisseb
          type: user
        html: '<p>Hello, thank you for contributing an interesting model.</p>

          <p>I saw in another post that you plan to release a notebook, however I
          am not very patient :D.</p>

          <p>If you have the time to answer, I would like to ask a question about
          dimensionality of the inputs. I see that the model always expects the past_input''s
          shape to be (batch_size, context_length+max(lags_sequence)). Does this mean
          that I have to provide #context_length additional lag steps in my inputs?</p>

          '
        raw: "Hello, thank you for contributing an interesting model.\r\n\r\nI saw\
          \ in another post that you plan to release a notebook, however I am not\
          \ very patient :D.\r\n\r\nIf you have the time to answer, I would like to\
          \ ask a question about dimensionality of the inputs. I see that the model\
          \ always expects the past_input's shape to be (batch_size, context_length+max(lags_sequence)).\
          \ Does this mean that I have to provide #context_length additional lag steps\
          \ in my inputs?"
        updatedAt: '2022-11-11T10:41:54.324Z'
      numEdits: 0
      reactions: []
    id: 636e26f29b61600b6298c2fb
    type: comment
  author: nisseb
  content: "Hello, thank you for contributing an interesting model.\r\n\r\nI saw in\
    \ another post that you plan to release a notebook, however I am not very patient\
    \ :D.\r\n\r\nIf you have the time to answer, I would like to ask a question about\
    \ dimensionality of the inputs. I see that the model always expects the past_input's\
    \ shape to be (batch_size, context_length+max(lags_sequence)). Does this mean\
    \ that I have to provide #context_length additional lag steps in my inputs?"
  created_at: 2022-11-11 10:41:54+00:00
  edited: false
  hidden: false
  id: 636e26f29b61600b6298c2fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
      fullname: Kashif Rasul
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: kashif
      type: user
    createdAt: '2022-11-22T11:45:00.000Z'
    data:
      edited: true
      editors:
      - kashif
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg?w=200&h=200&f=face
          fullname: Kashif Rasul
          isHf: true
          isPro: false
          name: kashif
          type: user
        html: '<p>yes I believe so... do have a look at the blog post here for further
          details: <a href="https://huggingface.co/blog/time-series-transformers">https://huggingface.co/blog/time-series-transformers</a></p>

          '
        raw: 'yes I believe so... do have a look at the blog post here for further
          details: https://huggingface.co/blog/time-series-transformers'
        updatedAt: '2022-12-02T08:36:43.538Z'
      numEdits: 2
      reactions: []
    id: 637cb63c6281ea1fcc45f864
    type: comment
  author: kashif
  content: 'yes I believe so... do have a look at the blog post here for further details:
    https://huggingface.co/blog/time-series-transformers'
  created_at: 2022-11-22 11:45:00+00:00
  edited: true
  hidden: false
  id: 637cb63c6281ea1fcc45f864
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-12-02T08:35:47.000Z'
    data:
      status: closed
    id: 6389b8e356f8a656a3c66a89
    type: status-change
  author: nielsr
  created_at: 2022-12-02 08:35:47+00:00
  id: 6389b8e356f8a656a3c66a89
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-12-02T08:36:04.000Z'
    data:
      edited: true
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>The way lags are created was actually a bit confusing for me at first
          - let me explain.</p>

          <p>So for instance when training on the "tourism-monthly" dataset, the frequency
          of the data is "monthly", and as explained in the <a href="https://huggingface.co/blog/time-series-transformers">blog</a>,
          we use the default lags provided by GluonTS:</p>

          <pre><code>from gluonts.time_feature import get_lags_for_frequency


          freq = "1M"


          lags_sequence = get_lags_for_frequency(freq)

          print(lags_sequence)


          &gt;&gt;&gt; [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 23, 24, 25, 35, 36, 37]

          </code></pre>

          <p>This means that, for each value of a time series that we feed to the
          Transformer, we will add the value of the month before that, the value of
          2 months before that, ... until the value of 37 months before that as additional
          features.</p>

          <p>So if you have a value for a particular month, then we will add the previous
          37 values as one vector to the model, in addition to the value itself. This
          means that you actually feed a vector of size 38 to the model. This is the
          embedding vector that will go through the Transformer. </p>

          <p>The inputs that we feed to the model (past_values) may contain these
          lags, but internally the model will squash them into the feature dimension,
          to make sure the sequence length is still equal to the context length.</p>

          <p>So let''s say we use a context length of 24 months, and we use the 37
          lags, then this means that the past_values we feed to the model are of shape
          (batch_size, context_length + max(lags_sequence)) = (batch_size, 24 + 37)
          = (batch_size, 61). The model will internally make sure that this tensor
          is turned into a tensor of shape (batch_size, sequence_length, hidden_size)
          = (batch_size, 24, 38) - assuming the lags are the only "features" we add
          besides the real values. This is the tensor that will go through the Transformer
          encoder.</p>

          <p>Hope that makes it clear!</p>

          '
        raw: "Hi,\n\nThe way lags are created was actually a bit confusing for me\
          \ at first - let me explain.\n\nSo for instance when training on the \"\
          tourism-monthly\" dataset, the frequency of the data is \"monthly\", and\
          \ as explained in the [blog](https://huggingface.co/blog/time-series-transformers),\
          \ we use the default lags provided by GluonTS:\n\n```\nfrom gluonts.time_feature\
          \ import get_lags_for_frequency\n\nfreq = \"1M\"\n\nlags_sequence = get_lags_for_frequency(freq)\n\
          print(lags_sequence)\n\n>>> [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 23, 24, 25,\
          \ 35, 36, 37]\n```\n\nThis means that, for each value of a time series that\
          \ we feed to the Transformer, we will add the value of the month before\
          \ that, the value of 2 months before that, ... until the value of 37 months\
          \ before that as additional features.\n\nSo if you have a value for a particular\
          \ month, then we will add the previous 37 values as one vector to the model,\
          \ in addition to the value itself. This means that you actually feed a vector\
          \ of size 38 to the model. This is the embedding vector that will go through\
          \ the Transformer. \n\nThe inputs that we feed to the model (past_values)\
          \ may contain these lags, but internally the model will squash them into\
          \ the feature dimension, to make sure the sequence length is still equal\
          \ to the context length.\n\nSo let's say we use a context length of 24 months,\
          \ and we use the 37 lags, then this means that the past_values we feed to\
          \ the model are of shape (batch_size, context_length + max(lags_sequence))\
          \ = (batch_size, 24 + 37) = (batch_size, 61). The model will internally\
          \ make sure that this tensor is turned into a tensor of shape (batch_size,\
          \ sequence_length, hidden_size) = (batch_size, 24, 38) - assuming the lags\
          \ are the only \"features\" we add besides the real values. This is the\
          \ tensor that will go through the Transformer encoder.\n\nHope that makes\
          \ it clear!"
        updatedAt: '2022-12-02T08:45:37.668Z'
      numEdits: 4
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - ketyi
        - elisim
        - aadityaverma
      relatedEventId: 6389b8f456f8a656a3c66af6
    id: 6389b8f456f8a656a3c66af5
    type: comment
  author: nielsr
  content: "Hi,\n\nThe way lags are created was actually a bit confusing for me at\
    \ first - let me explain.\n\nSo for instance when training on the \"tourism-monthly\"\
    \ dataset, the frequency of the data is \"monthly\", and as explained in the [blog](https://huggingface.co/blog/time-series-transformers),\
    \ we use the default lags provided by GluonTS:\n\n```\nfrom gluonts.time_feature\
    \ import get_lags_for_frequency\n\nfreq = \"1M\"\n\nlags_sequence = get_lags_for_frequency(freq)\n\
    print(lags_sequence)\n\n>>> [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 23, 24, 25, 35,\
    \ 36, 37]\n```\n\nThis means that, for each value of a time series that we feed\
    \ to the Transformer, we will add the value of the month before that, the value\
    \ of 2 months before that, ... until the value of 37 months before that as additional\
    \ features.\n\nSo if you have a value for a particular month, then we will add\
    \ the previous 37 values as one vector to the model, in addition to the value\
    \ itself. This means that you actually feed a vector of size 38 to the model.\
    \ This is the embedding vector that will go through the Transformer. \n\nThe inputs\
    \ that we feed to the model (past_values) may contain these lags, but internally\
    \ the model will squash them into the feature dimension, to make sure the sequence\
    \ length is still equal to the context length.\n\nSo let's say we use a context\
    \ length of 24 months, and we use the 37 lags, then this means that the past_values\
    \ we feed to the model are of shape (batch_size, context_length + max(lags_sequence))\
    \ = (batch_size, 24 + 37) = (batch_size, 61). The model will internally make sure\
    \ that this tensor is turned into a tensor of shape (batch_size, sequence_length,\
    \ hidden_size) = (batch_size, 24, 38) - assuming the lags are the only \"features\"\
    \ we add besides the real values. This is the tensor that will go through the\
    \ Transformer encoder.\n\nHope that makes it clear!"
  created_at: 2022-12-02 08:36:04+00:00
  edited: true
  hidden: false
  id: 6389b8f456f8a656a3c66af5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2022-12-02T08:36:04.000Z'
    data:
      status: open
    id: 6389b8f456f8a656a3c66af6
    type: status-change
  author: nielsr
  created_at: 2022-12-02 08:36:04+00:00
  id: 6389b8f456f8a656a3c66af6
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: huggingface/time-series-transformer-tourism-monthly
repo_type: model
status: open
target_branch: null
title: Question about context_length and lags_sequence
