!!python/object:huggingface_hub.community.DiscussionWithDetails
author: edmond
conflicting_files: null
created_at: 2023-04-17 14:53:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2023-04-17T15:53:55.000Z'
    data:
      edited: false
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: '<p>How come, no matter my learning rate, I end up having a prediction
          giving nans ?<br>My inputs'' max and min values are okay, the output''s
          too, but for some reason I end up having nans.<br>I even instantly have
          nans if I train with a soft prompt self.trans(inputs_embeds=patch_emb) (min
          an max are okay).<br>When i predict, before training, the values are fine
          too. And if i train on bert and inject the information of the soft prompt
          by adding the same embedding on all the sequence it works fine.</p>

          '
        raw: "How come, no matter my learning rate, I end up having a prediction giving\
          \ nans ?\r\nMy inputs' max and min values are okay, the output's too, but\
          \ for some reason I end up having nans. \r\nI even instantly have nans if\
          \ I train with a soft prompt self.trans(inputs_embeds=patch_emb) (min an\
          \ max are okay).\r\nWhen i predict, before training, the values are fine\
          \ too. And if i train on bert and inject the information of the soft prompt\
          \ by adding the same embedding on all the sequence it works fine."
        updatedAt: '2023-04-17T15:53:55.983Z'
      numEdits: 0
      reactions: []
    id: 643d6b9312a352fee90abe57
    type: comment
  author: edmond
  content: "How come, no matter my learning rate, I end up having a prediction giving\
    \ nans ?\r\nMy inputs' max and min values are okay, the output's too, but for\
    \ some reason I end up having nans. \r\nI even instantly have nans if I train\
    \ with a soft prompt self.trans(inputs_embeds=patch_emb) (min an max are okay).\r\
    \nWhen i predict, before training, the values are fine too. And if i train on\
    \ bert and inject the information of the soft prompt by adding the same embedding\
    \ on all the sequence it works fine."
  created_at: 2023-04-17 14:53:55+00:00
  edited: false
  hidden: false
  id: 643d6b9312a352fee90abe57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8356b08d33303d2a374f7ab4557af8c.svg
      fullname: Ryan Burke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rburke45
      type: user
    createdAt: '2023-04-26T19:48:10.000Z'
    data:
      edited: false
      editors:
      - rburke45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8356b08d33303d2a374f7ab4557af8c.svg
          fullname: Ryan Burke
          isHf: false
          isPro: false
          name: rburke45
          type: user
        html: '<p>I''m experiencing the same issue. Did you find a solution?</p>

          '
        raw: I'm experiencing the same issue. Did you find a solution?
        updatedAt: '2023-04-26T19:48:10.974Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - edmond
    id: 64497ffab3cd701e0a078fea
    type: comment
  author: rburke45
  content: I'm experiencing the same issue. Did you find a solution?
  created_at: 2023-04-26 18:48:10+00:00
  edited: false
  hidden: false
  id: 64497ffab3cd701e0a078fea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2023-04-26T20:04:18.000Z'
    data:
      edited: true
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: "<p>Good to know, no it failed and I do not think its my fault as I\
          \ had amazing results with using a masked decoder only <a href=\"https://huggingface.co/microsoft/layoutlmv3-large\"\
          >https://huggingface.co/microsoft/layoutlmv3-large</a>, but its not supposed\
          \ to work good as its not trained on natural images.<br>I have no time to\
          \ try <a href=\"https://huggingface.co/bigscience/mt0-base\">https://huggingface.co/bigscience/mt0-base</a>\
          \ (I know bigscience are serious people as bloomz works amazinlgy for me)\
          \ which might not be buggy, please tell me if you have any result with it.\
          \ </p>\n<blockquote>\n<p>I'm experiencing the same issue. Did you find a\
          \ solution? <span data-props=\"{&quot;user&quot;:&quot;rburke45&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rburke45\"\
          >@<span class=\"underline\">rburke45</span></a></span>\n\n\t</span></span></p>\n\
          </blockquote>\n"
        raw: "Good to know, no it failed and I do not think its my fault as I had\
          \ amazing results with using a masked decoder only https://huggingface.co/microsoft/layoutlmv3-large,\
          \ but its not supposed to work good as its not trained on natural images.\n\
          I have no time to try https://huggingface.co/bigscience/mt0-base (I know\
          \ bigscience are serious people as bloomz works amazinlgy for me) which\
          \ might not be buggy, please tell me if you have any result with it. \n\
          > I'm experiencing the same issue. Did you find a solution? @rburke45"
        updatedAt: '2023-04-26T20:04:42.983Z'
      numEdits: 1
      reactions: []
    id: 644983c2b3cd701e0a07daff
    type: comment
  author: edmond
  content: "Good to know, no it failed and I do not think its my fault as I had amazing\
    \ results with using a masked decoder only https://huggingface.co/microsoft/layoutlmv3-large,\
    \ but its not supposed to work good as its not trained on natural images.\nI have\
    \ no time to try https://huggingface.co/bigscience/mt0-base (I know bigscience\
    \ are serious people as bloomz works amazinlgy for me) which might not be buggy,\
    \ please tell me if you have any result with it. \n> I'm experiencing the same\
    \ issue. Did you find a solution? @rburke45"
  created_at: 2023-04-26 19:04:18+00:00
  edited: true
  hidden: false
  id: 644983c2b3cd701e0a07daff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8356b08d33303d2a374f7ab4557af8c.svg
      fullname: Ryan Burke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rburke45
      type: user
    createdAt: '2023-04-26T21:53:30.000Z'
    data:
      edited: false
      editors:
      - rburke45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8356b08d33303d2a374f7ab4557af8c.svg
          fullname: Ryan Burke
          isHf: false
          isPro: false
          name: rburke45
          type: user
        html: '<p>Were you using a reduced precision version of the model? Both the
          FP16 &amp; INT8 models output nans when training, but the full precision
          model is training fine for me now.</p>

          '
        raw: Were you using a reduced precision version of the model? Both the FP16
          & INT8 models output nans when training, but the full precision model is
          training fine for me now.
        updatedAt: '2023-04-26T21:53:30.700Z'
      numEdits: 0
      reactions: []
    id: 64499d5ab3cd701e0a09e2e1
    type: comment
  author: rburke45
  content: Were you using a reduced precision version of the model? Both the FP16
    & INT8 models output nans when training, but the full precision model is training
    fine for me now.
  created_at: 2023-04-26 20:53:30+00:00
  edited: false
  hidden: false
  id: 64499d5ab3cd701e0a09e2e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2023-04-27T06:31:55.000Z'
    data:
      edited: true
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: "<p>Yes i am using fp16 but I read in a thread william falcon saying\
          \ using fp32 will only postpone the phenomenon.<br>Maybe he was wrong, did\
          \ you succeed training it till overfitting ? <span data-props=\"{&quot;user&quot;:&quot;rburke45&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rburke45\"\
          >@<span class=\"underline\">rburke45</span></a></span>\n\n\t</span></span></p>\n"
        raw: 'Yes i am using fp16 but I read in a thread william falcon saying using
          fp32 will only postpone the phenomenon.

          Maybe he was wrong, did you succeed training it till overfitting ? @rburke45'
        updatedAt: '2023-04-27T06:33:11.019Z'
      numEdits: 1
      reactions: []
    id: 644a16dbf3ff2cb84eb062f7
    type: comment
  author: edmond
  content: 'Yes i am using fp16 but I read in a thread william falcon saying using
    fp32 will only postpone the phenomenon.

    Maybe he was wrong, did you succeed training it till overfitting ? @rburke45'
  created_at: 2023-04-27 05:31:55+00:00
  edited: true
  hidden: false
  id: 644a16dbf3ff2cb84eb062f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d8356b08d33303d2a374f7ab4557af8c.svg
      fullname: Ryan Burke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rburke45
      type: user
    createdAt: '2023-05-01T19:26:05.000Z'
    data:
      edited: false
      editors:
      - rburke45
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d8356b08d33303d2a374f7ab4557af8c.svg
          fullname: Ryan Burke
          isHf: false
          isPro: false
          name: rburke45
          type: user
        html: '<p>Yep, trained a full 40 epochs with overfitting starting around epoch
          20. Not sure what issue William Falcon has, but I''m not seeing it here.</p>

          '
        raw: Yep, trained a full 40 epochs with overfitting starting around epoch
          20. Not sure what issue William Falcon has, but I'm not seeing it here.
        updatedAt: '2023-05-01T19:26:05.491Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - edmond
        - yuuhan
    id: 6450124d577838187e00b820
    type: comment
  author: rburke45
  content: Yep, trained a full 40 epochs with overfitting starting around epoch 20.
    Not sure what issue William Falcon has, but I'm not seeing it here.
  created_at: 2023-05-01 18:26:05+00:00
  edited: false
  hidden: false
  id: 6450124d577838187e00b820
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2023-05-29T11:12:01.000Z'
    data:
      edited: false
      editors:
      - edmond
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
          fullname: Edmond Jacoupeau
          isHf: false
          isPro: false
          name: edmond
          type: user
        html: '<p>Thanks ok</p>

          '
        raw: Thanks ok
        updatedAt: '2023-05-29T11:12:01.475Z'
      numEdits: 0
      reactions: []
    id: 64748881d56974d0c05a71aa
    type: comment
  author: edmond
  content: Thanks ok
  created_at: 2023-05-29 10:12:01+00:00
  edited: false
  hidden: false
  id: 64748881d56974d0c05a71aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/2bdb4a26fde4cbe5b4673e53e0d44540.svg
      fullname: Edmond Jacoupeau
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edmond
      type: user
    createdAt: '2023-05-29T11:12:04.000Z'
    data:
      status: closed
    id: 647488845ada8510bc3e5ef8
    type: status-change
  author: edmond
  created_at: 2023-05-29 10:12:04+00:00
  id: 647488845ada8510bc3e5ef8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: google/flan-t5-base
repo_type: model
status: closed
target_branch: null
title: nans while fine tuning
