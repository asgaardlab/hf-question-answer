!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gigant
conflicting_files: null
created_at: 2023-09-09 11:49:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
      fullname: "Th\xE9o Gigant"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gigant
      type: user
    createdAt: '2023-09-09T12:49:08.000Z'
    data:
      edited: false
      editors:
      - gigant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4552752375602722
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
          fullname: "Th\xE9o Gigant"
          isHf: false
          isPro: false
          name: gigant
          type: user
        html: "<p>Hello,</p>\n<p>The Flax version of this T5 checkpoint with float16\
          \ dtype outputs exclusively nan. In float32 it works.</p>\n<p>To reproduce\
          \ the error:</p>\n<pre><code>from transformers import T5Tokenizer, FlaxT5ForConditionalGeneration\n\
          \ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\nmodel\
          \ = FlaxT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\"\
          , dtype=\"float16\")\n\nmodel_module = __import__(model.__module__, fromlist=[\"\
          shift_tokens_tight\"])\nshift_tokens_right_fn = getattr(model_module, \"\
          shift_tokens_right\")\n\npad_token_id=model.config.pad_token_id\ndecoder_start_token_id=model.config.decoder_start_token_id\n\
          \n\ninput_text = \"translate English to German: How old are you?\"\n\ninput_ids\
          \ = tokenizer(input_text, return_tensors=\"np\")\n\ndecoder_input_ids =\
          \ shift_tokens_right_fn(\n    input_ids[\"input_ids\"], pad_token_id, decoder_start_token_id\n\
          )\n\noutputs = model.generate(**input_ids)\nprint(tokenizer.decode(outputs[0][0],\
          \ skip_special_tokens=True))\nprint(model(**input_ids, decoder_input_ids=decoder_input_ids))\n\
          </code></pre>\n<p>The generation results is just empty and the model output\
          \ is filled with nan.</p>\n<p>Is there a way to prevent that / a correct\
          \ way to translate the float32 weights to float16 that will not give this\
          \ error?</p>\n<p>Thank you</p>\n"
        raw: "Hello,\r\n\r\nThe Flax version of this T5 checkpoint with float16 dtype\
          \ outputs exclusively nan. In float32 it works.\r\n\r\nTo reproduce the\
          \ error:\r\n\r\n```\r\nfrom transformers import T5Tokenizer, FlaxT5ForConditionalGeneration\r\
          \n\r\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\r\
          \nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\"\
          , dtype=\"float16\")\r\n\r\nmodel_module = __import__(model.__module__,\
          \ fromlist=[\"shift_tokens_tight\"])\r\nshift_tokens_right_fn = getattr(model_module,\
          \ \"shift_tokens_right\")\r\n\r\npad_token_id=model.config.pad_token_id\r\
          \ndecoder_start_token_id=model.config.decoder_start_token_id\r\n\r\n\r\n\
          input_text = \"translate English to German: How old are you?\"\r\n\r\ninput_ids\
          \ = tokenizer(input_text, return_tensors=\"np\")\r\n\r\ndecoder_input_ids\
          \ = shift_tokens_right_fn(\r\n    input_ids[\"input_ids\"], pad_token_id,\
          \ decoder_start_token_id\r\n)\r\n\r\noutputs = model.generate(**input_ids)\r\
          \nprint(tokenizer.decode(outputs[0][0], skip_special_tokens=True))\r\nprint(model(**input_ids,\
          \ decoder_input_ids=decoder_input_ids))\r\n```\r\n\r\nThe generation results\
          \ is just empty and the model output is filled with nan.\r\n\r\nIs there\
          \ a way to prevent that / a correct way to translate the float32 weights\
          \ to float16 that will not give this error?\r\n\r\nThank you"
        updatedAt: '2023-09-09T12:49:08.817Z'
      numEdits: 0
      reactions: []
    id: 64fc69c4365e3069d73e76a2
    type: comment
  author: gigant
  content: "Hello,\r\n\r\nThe Flax version of this T5 checkpoint with float16 dtype\
    \ outputs exclusively nan. In float32 it works.\r\n\r\nTo reproduce the error:\r\
    \n\r\n```\r\nfrom transformers import T5Tokenizer, FlaxT5ForConditionalGeneration\r\
    \n\r\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\r\nmodel\
    \ = FlaxT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", dtype=\"\
    float16\")\r\n\r\nmodel_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"\
    ])\r\nshift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\r\n\
    \r\npad_token_id=model.config.pad_token_id\r\ndecoder_start_token_id=model.config.decoder_start_token_id\r\
    \n\r\n\r\ninput_text = \"translate English to German: How old are you?\"\r\n\r\
    \ninput_ids = tokenizer(input_text, return_tensors=\"np\")\r\n\r\ndecoder_input_ids\
    \ = shift_tokens_right_fn(\r\n    input_ids[\"input_ids\"], pad_token_id, decoder_start_token_id\r\
    \n)\r\n\r\noutputs = model.generate(**input_ids)\r\nprint(tokenizer.decode(outputs[0][0],\
    \ skip_special_tokens=True))\r\nprint(model(**input_ids, decoder_input_ids=decoder_input_ids))\r\
    \n```\r\n\r\nThe generation results is just empty and the model output is filled\
    \ with nan.\r\n\r\nIs there a way to prevent that / a correct way to translate\
    \ the float32 weights to float16 that will not give this error?\r\n\r\nThank you"
  created_at: 2023-09-09 11:49:08+00:00
  edited: false
  hidden: false
  id: 64fc69c4365e3069d73e76a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-09-11T20:37:34.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.988025426864624
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>,\
          \ would you know what might be happening?</p>\n"
        raw: '@sanchit-gandhi, would you know what might be happening?'
        updatedAt: '2023-09-11T20:37:34.358Z'
      numEdits: 0
      reactions: []
    id: 64ff7a8e4b7e0cd1b7b4c796
    type: comment
  author: lysandre
  content: '@sanchit-gandhi, would you know what might be happening?'
  created_at: 2023-09-11 19:37:34+00:00
  edited: false
  hidden: false
  id: 64ff7a8e4b7e0cd1b7b4c796
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-09-14T15:55:59.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8204190731048584
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;gigant&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gigant\">@<span class=\"\
          underline\">gigant</span></a></span>\n\n\t</span></span> - I ran the codesnippet\
          \ with a few settings, and believe this phenomenon is an artefact of the\
          \ float16 dynamic range, rather than an issue with the model. E.g. if we\
          \ run the model with a dtype of <code>jnp.float32</code> or <code>jnp.bfloat16</code>\
          \ (larger dynamic range) we get sensible outputs. If we switch to a dtype\
          \ of <code>jnp.float16</code> (smaller dynamic range), we get nan's as you\
          \ have described =&gt; this suggests that the model requires the larger\
          \ dynamic range to operate, and exceeds the dynamic range of <code>float16</code>.\
          \ This might come as a result of it being trained in <code>bfloat16</code>\
          \ directly.</p>\n<p>Note that the <code>dtype</code> argument only changes\
          \ the dtype of the model computation (forward pass), not the parameters.\
          \ This gets you memory and latency improvements  since the model operations\
          \ are run in a lower dtype, but the parameters themselves are un-changes.</p>\n\
          <p> If you want to change the dtype of the parameters, you can set:</p>\n\
          <pre><code class=\"language-python\">model = FlaxT5ForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"google/flan-t5-base\"</span>)\nmodel.params =\
          \ model.to_fp16(model.params)\n</code></pre>\n<p>=&gt; this won't speed\
          \ up the forward pass (since you still run it in a dtype of <code>float32</code>),\
          \ but will give you a memory saving by downcasting the params and thus reducing\
          \ their memory footprint. You can also downcast them to <code>bfloat16</code>\
          \ in much the same way:</p>\n<pre><code class=\"language-python\">model\
          \ = FlaxT5ForConditionalGeneration.from_pretrained(<span class=\"hljs-string\"\
          >\"google/flan-t5-base\"</span>)\nmodel.params = model.to_bf16(model.params)\n\
          </code></pre>\n<p><strong>TLDR:</strong> for fast inference and memory savings,\
          \ set <code>dtype=jnp.bfloat16</code>. For just memory savings, downcast\
          \ the params. For max performance, do both!</p>\n<pre><code class=\"language-python\"\
          >model = FlaxT5ForConditionalGeneration.from_pretrained(<span class=\"hljs-string\"\
          >\"google/flan-t5-base\"</span>, dtype=jnp.bfloat16)\nmodel.params = model.to_bf16(model.params)\n\
          </code></pre>\n"
        raw: "Hey @gigant - I ran the codesnippet with a few settings, and believe\
          \ this phenomenon is an artefact of the float16 dynamic range, rather than\
          \ an issue with the model. E.g. if we run the model with a dtype of `jnp.float32`\
          \ or `jnp.bfloat16` (larger dynamic range) we get sensible outputs. If we\
          \ switch to a dtype of `jnp.float16` (smaller dynamic range), we get nan's\
          \ as you have described => this suggests that the model requires the larger\
          \ dynamic range to operate, and exceeds the dynamic range of `float16`.\
          \ This might come as a result of it being trained in `bfloat16` directly.\n\
          \nNote that the `dtype` argument only changes the dtype of the model computation\
          \ (forward pass), not the parameters. This gets you memory and latency improvements\
          \  since the model operations are run in a lower dtype, but the parameters\
          \ themselves are un-changes.\n\n If you want to change the dtype of the\
          \ parameters, you can set:\n```python\nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"\
          google/flan-t5-base\")\nmodel.params = model.to_fp16(model.params)\n```\n\
          \n=> this won't speed up the forward pass (since you still run it in a dtype\
          \ of `float32`), but will give you a memory saving by downcasting the params\
          \ and thus reducing their memory footprint. You can also downcast them to\
          \ `bfloat16` in much the same way:\n\n```python\nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"\
          google/flan-t5-base\")\nmodel.params = model.to_bf16(model.params)\n```\n\
          \n**TLDR:** for fast inference and memory savings, set `dtype=jnp.bfloat16`.\
          \ For just memory savings, downcast the params. For max performance, do\
          \ both!\n\n```python\nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"\
          google/flan-t5-base\", dtype=jnp.bfloat16)\nmodel.params = model.to_bf16(model.params)\n\
          ```"
        updatedAt: '2023-09-14T15:56:28.010Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - gigant
        - lysandre
    id: 65032d0f2bfd1e86cdba71ec
    type: comment
  author: sanchit-gandhi
  content: "Hey @gigant - I ran the codesnippet with a few settings, and believe this\
    \ phenomenon is an artefact of the float16 dynamic range, rather than an issue\
    \ with the model. E.g. if we run the model with a dtype of `jnp.float32` or `jnp.bfloat16`\
    \ (larger dynamic range) we get sensible outputs. If we switch to a dtype of `jnp.float16`\
    \ (smaller dynamic range), we get nan's as you have described => this suggests\
    \ that the model requires the larger dynamic range to operate, and exceeds the\
    \ dynamic range of `float16`. This might come as a result of it being trained\
    \ in `bfloat16` directly.\n\nNote that the `dtype` argument only changes the dtype\
    \ of the model computation (forward pass), not the parameters. This gets you memory\
    \ and latency improvements  since the model operations are run in a lower dtype,\
    \ but the parameters themselves are un-changes.\n\n If you want to change the\
    \ dtype of the parameters, you can set:\n```python\nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"\
    google/flan-t5-base\")\nmodel.params = model.to_fp16(model.params)\n```\n\n=>\
    \ this won't speed up the forward pass (since you still run it in a dtype of `float32`),\
    \ but will give you a memory saving by downcasting the params and thus reducing\
    \ their memory footprint. You can also downcast them to `bfloat16` in much the\
    \ same way:\n\n```python\nmodel = FlaxT5ForConditionalGeneration.from_pretrained(\"\
    google/flan-t5-base\")\nmodel.params = model.to_bf16(model.params)\n```\n\n**TLDR:**\
    \ for fast inference and memory savings, set `dtype=jnp.bfloat16`. For just memory\
    \ savings, downcast the params. For max performance, do both!\n\n```python\nmodel\
    \ = FlaxT5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", dtype=jnp.bfloat16)\n\
    model.params = model.to_bf16(model.params)\n```"
  created_at: 2023-09-14 14:55:59+00:00
  edited: true
  hidden: false
  id: 65032d0f2bfd1e86cdba71ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
      fullname: "Th\xE9o Gigant"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gigant
      type: user
    createdAt: '2023-09-15T13:04:02.000Z'
    data:
      edited: false
      editors:
      - gigant
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9742518663406372
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
          fullname: "Th\xE9o Gigant"
          isHf: false
          isPro: false
          name: gigant
          type: user
        html: '<p>Thank you very much for the comprehensive answer! Its much clearer
          now</p>

          '
        raw: Thank you very much for the comprehensive answer! Its much clearer now
        updatedAt: '2023-09-15T13:04:02.727Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6504564257cc1e5d753790b1
    id: 6504564257cc1e5d753790af
    type: comment
  author: gigant
  content: Thank you very much for the comprehensive answer! Its much clearer now
  created_at: 2023-09-15 12:04:02+00:00
  edited: false
  hidden: false
  id: 6504564257cc1e5d753790af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1645712223620-60d35154d7b174177faabd55.jpeg?w=200&h=200&f=face
      fullname: "Th\xE9o Gigant"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gigant
      type: user
    createdAt: '2023-09-15T13:04:02.000Z'
    data:
      status: closed
    id: 6504564257cc1e5d753790b1
    type: status-change
  author: gigant
  created_at: 2023-09-15 12:04:02+00:00
  id: 6504564257cc1e5d753790b1
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: google/flan-t5-base
repo_type: model
status: closed
target_branch: null
title: Flax checkpoint with float16 dtype outputs nan
