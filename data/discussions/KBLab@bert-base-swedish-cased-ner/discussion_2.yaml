!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tomsoderlund
conflicting_files: null
created_at: 2022-12-17 14:59:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663598645497-63287f0bce50f1253749f7a3.jpeg?w=200&h=200&f=face
      fullname: "Tom S\xF6derlund"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomsoderlund
      type: user
    createdAt: '2022-12-17T14:59:51.000Z'
    data:
      edited: true
      editors:
      - tomsoderlund
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663598645497-63287f0bce50f1253749f7a3.jpeg?w=200&h=200&f=face
          fullname: "Tom S\xF6derlund"
          isHf: false
          isPro: false
          name: tomsoderlund
          type: user
        html: "<p>Great model, thank you for your hard work <strong>KBLab</strong>!\
          \ I'm struggling a bit though:</p>\n<p>The string <code>\"Groens malmg\xE5\
          rd \xE4r en av Stockholms malmg\xE5rdar, bel\xE4gen vid Malmg\xE5rdsv\xE4\
          gen 53 p\xE5 S\xF6dermalm i Stockholm.\"</code></p>\n<p>generates the following\
          \ output:</p>\n<pre><code>{\n  \"entities\": [\n    {\n      \"entity\"\
          : \"LOC\",\n      \"score\": 0.4733002483844757,\n      \"index\": 1,\n\
          \      \"word\": \"Gro\",\n      \"start\": 0,\n      \"end\": 3\n    },\n\
          \    {\n      \"entity\": \"LOC\",\n      \"score\": 0.7171409130096436,\n\
          \      \"index\": 2,\n      \"word\": \"##ens\",\n      \"start\": 3,\n\
          \      \"end\": 6\n    },\n    {\n      \"entity\": \"LOC\",\n      \"score\"\
          : 0.5204557180404663,\n      \"index\": 3,\n      \"word\": \"malm\",\n\
          \      \"start\": 7,\n      \"end\": 11\n    },\n    {\n      \"entity\"\
          : \"LOC\",\n      \"score\": 0.9973670840263367,\n      \"index\": 8,\n\
          \      \"word\": \"Stockholms\",\n      \"start\": 25,\n      \"end\": 35\n\
          \    },\n    {\n      \"entity\": \"LOC\",\n      \"score\": 0.9990962743759155,\n\
          \      \"index\": 14,\n      \"word\": \"Malm\",\n      \"start\": 60,\n\
          \      \"end\": 64\n    },\n    {\n      \"entity\": \"LOC\",\n      \"\
          score\": 0.9991210103034973,\n      \"index\": 15,\n      \"word\": \"##g\xE5\
          rds\",\n      \"start\": 64,\n      \"end\": 69\n    },\n    {\n      \"\
          entity\": \"LOC\",\n      \"score\": 0.9988952875137329,\n      \"index\"\
          : 16,\n      \"word\": \"##v\xE4gen\",\n      \"start\": 69,\n      \"end\"\
          : 74\n    },\n    {\n      \"entity\": \"LOC\",\n      \"score\": 0.9987678527832031,\n\
          \      \"index\": 17,\n      \"word\": \"53\",\n      \"start\": 75,\n \
          \     \"end\": 77\n    },\n    {\n      \"entity\": \"LOC\",\n      \"score\"\
          : 0.9986610412597656,\n      \"index\": 19,\n      \"word\": \"S\xF6dermalm\"\
          ,\n      \"start\": 81,\n      \"end\": 90\n    },\n    {\n      \"entity\"\
          : \"LOC\",\n      \"score\": 0.9985471367835999,\n      \"index\": 21,\n\
          \      \"word\": \"Stockholm\",\n      \"start\": 93,\n      \"end\": 102\n\
          \    }\n  ]\n}\n</code></pre>\n<p>I'm surprised to find <code>\"Stockholms\"\
          </code> as a word \u2013&nbsp;is pluralization a problem?</p>\n"
        raw: "Great model, thank you for your hard work **KBLab**! I'm struggling\
          \ a bit though:\n\nThe string `\"Groens malmg\xE5rd \xE4r en av Stockholms\
          \ malmg\xE5rdar, bel\xE4gen vid Malmg\xE5rdsv\xE4gen 53 p\xE5 S\xF6dermalm\
          \ i Stockholm.\"`\n\ngenerates the following output:\n\n    {\n      \"\
          entities\": [\n        {\n          \"entity\": \"LOC\",\n          \"score\"\
          : 0.4733002483844757,\n          \"index\": 1,\n          \"word\": \"Gro\"\
          ,\n          \"start\": 0,\n          \"end\": 3\n        },\n        {\n\
          \          \"entity\": \"LOC\",\n          \"score\": 0.7171409130096436,\n\
          \          \"index\": 2,\n          \"word\": \"##ens\",\n          \"start\"\
          : 3,\n          \"end\": 6\n        },\n        {\n          \"entity\"\
          : \"LOC\",\n          \"score\": 0.5204557180404663,\n          \"index\"\
          : 3,\n          \"word\": \"malm\",\n          \"start\": 7,\n         \
          \ \"end\": 11\n        },\n        {\n          \"entity\": \"LOC\",\n \
          \         \"score\": 0.9973670840263367,\n          \"index\": 8,\n    \
          \      \"word\": \"Stockholms\",\n          \"start\": 25,\n          \"\
          end\": 35\n        },\n        {\n          \"entity\": \"LOC\",\n     \
          \     \"score\": 0.9990962743759155,\n          \"index\": 14,\n       \
          \   \"word\": \"Malm\",\n          \"start\": 60,\n          \"end\": 64\n\
          \        },\n        {\n          \"entity\": \"LOC\",\n          \"score\"\
          : 0.9991210103034973,\n          \"index\": 15,\n          \"word\": \"\
          ##g\xE5rds\",\n          \"start\": 64,\n          \"end\": 69\n       \
          \ },\n        {\n          \"entity\": \"LOC\",\n          \"score\": 0.9988952875137329,\n\
          \          \"index\": 16,\n          \"word\": \"##v\xE4gen\",\n       \
          \   \"start\": 69,\n          \"end\": 74\n        },\n        {\n     \
          \     \"entity\": \"LOC\",\n          \"score\": 0.9987678527832031,\n \
          \         \"index\": 17,\n          \"word\": \"53\",\n          \"start\"\
          : 75,\n          \"end\": 77\n        },\n        {\n          \"entity\"\
          : \"LOC\",\n          \"score\": 0.9986610412597656,\n          \"index\"\
          : 19,\n          \"word\": \"S\xF6dermalm\",\n          \"start\": 81,\n\
          \          \"end\": 90\n        },\n        {\n          \"entity\": \"\
          LOC\",\n          \"score\": 0.9985471367835999,\n          \"index\": 21,\n\
          \          \"word\": \"Stockholm\",\n          \"start\": 93,\n        \
          \  \"end\": 102\n        }\n      ]\n    }\n\nI'm surprised to find `\"\
          Stockholms\"` as a word \u2013\_is pluralization a problem?"
        updatedAt: '2022-12-17T15:52:49.415Z'
      numEdits: 1
      reactions: []
    id: 639dd967f87da5e2eb11c9c0
    type: comment
  author: tomsoderlund
  content: "Great model, thank you for your hard work **KBLab**! I'm struggling a\
    \ bit though:\n\nThe string `\"Groens malmg\xE5rd \xE4r en av Stockholms malmg\xE5\
    rdar, bel\xE4gen vid Malmg\xE5rdsv\xE4gen 53 p\xE5 S\xF6dermalm i Stockholm.\"\
    `\n\ngenerates the following output:\n\n    {\n      \"entities\": [\n       \
    \ {\n          \"entity\": \"LOC\",\n          \"score\": 0.4733002483844757,\n\
    \          \"index\": 1,\n          \"word\": \"Gro\",\n          \"start\": 0,\n\
    \          \"end\": 3\n        },\n        {\n          \"entity\": \"LOC\",\n\
    \          \"score\": 0.7171409130096436,\n          \"index\": 2,\n         \
    \ \"word\": \"##ens\",\n          \"start\": 3,\n          \"end\": 6\n      \
    \  },\n        {\n          \"entity\": \"LOC\",\n          \"score\": 0.5204557180404663,\n\
    \          \"index\": 3,\n          \"word\": \"malm\",\n          \"start\":\
    \ 7,\n          \"end\": 11\n        },\n        {\n          \"entity\": \"LOC\"\
    ,\n          \"score\": 0.9973670840263367,\n          \"index\": 8,\n       \
    \   \"word\": \"Stockholms\",\n          \"start\": 25,\n          \"end\": 35\n\
    \        },\n        {\n          \"entity\": \"LOC\",\n          \"score\": 0.9990962743759155,\n\
    \          \"index\": 14,\n          \"word\": \"Malm\",\n          \"start\"\
    : 60,\n          \"end\": 64\n        },\n        {\n          \"entity\": \"\
    LOC\",\n          \"score\": 0.9991210103034973,\n          \"index\": 15,\n \
    \         \"word\": \"##g\xE5rds\",\n          \"start\": 64,\n          \"end\"\
    : 69\n        },\n        {\n          \"entity\": \"LOC\",\n          \"score\"\
    : 0.9988952875137329,\n          \"index\": 16,\n          \"word\": \"##v\xE4\
    gen\",\n          \"start\": 69,\n          \"end\": 74\n        },\n        {\n\
    \          \"entity\": \"LOC\",\n          \"score\": 0.9987678527832031,\n  \
    \        \"index\": 17,\n          \"word\": \"53\",\n          \"start\": 75,\n\
    \          \"end\": 77\n        },\n        {\n          \"entity\": \"LOC\",\n\
    \          \"score\": 0.9986610412597656,\n          \"index\": 19,\n        \
    \  \"word\": \"S\xF6dermalm\",\n          \"start\": 81,\n          \"end\": 90\n\
    \        },\n        {\n          \"entity\": \"LOC\",\n          \"score\": 0.9985471367835999,\n\
    \          \"index\": 21,\n          \"word\": \"Stockholm\",\n          \"start\"\
    : 93,\n          \"end\": 102\n        }\n      ]\n    }\n\nI'm surprised to find\
    \ `\"Stockholms\"` as a word \u2013\_is pluralization a problem?"
  created_at: 2022-12-17 14:59:51+00:00
  edited: true
  hidden: false
  id: 639dd967f87da5e2eb11c9c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663598645497-63287f0bce50f1253749f7a3.jpeg?w=200&h=200&f=face
      fullname: "Tom S\xF6derlund"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomsoderlund
      type: user
    createdAt: '2022-12-17T15:53:15.000Z'
    data:
      from: Why is the model struggling with Swedish words, and what does `##` mean?
      to: Why is the model struggling with pluralization of Swedish words?
    id: 639de5eb7145123e0d4c5046
    type: title-change
  author: tomsoderlund
  created_at: 2022-12-17 15:53:15+00:00
  id: 639de5eb7145123e0d4c5046
  new_title: Why is the model struggling with pluralization of Swedish words?
  old_title: Why is the model struggling with Swedish words, and what does `##` mean?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678254967312-6167f557ddd96c73b929a4ff.jpeg?w=200&h=200&f=face
      fullname: Peter Krantz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: peterkz
      type: user
    createdAt: '2022-12-17T16:10:36.000Z'
    data:
      edited: true
      editors:
      - peterkz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678254967312-6167f557ddd96c73b929a4ff.jpeg?w=200&h=200&f=face
          fullname: Peter Krantz
          isHf: false
          isPro: false
          name: peterkz
          type: user
        html: "<p>(Update - replied to the original comment)</p>\n<p>The documentation\
          \ README says:</p>\n<blockquote>\n<p>The BERT tokenizer often splits words\
          \ into multiple tokens, with the subparts starting with ##, for example\
          \ the string Engelbert k\xF6r Volvo till Herr\xE4ngens fotbollsklubb gets\
          \ tokenized as Engel ##bert k\xF6r Volvo till Herr ##\xE4ngens fotbolls\
          \ ##klubb. </p>\n</blockquote>\n<p>There is also a short code example on\
          \ how to glue them together.</p>\n"
        raw: "(Update - replied to the original comment)\n\nThe documentation README\
          \ says:\n\n> The BERT tokenizer often splits words into multiple tokens,\
          \ with the subparts starting with ##, for example the string Engelbert k\xF6\
          r Volvo till Herr\xE4ngens fotbollsklubb gets tokenized as Engel ##bert\
          \ k\xF6r Volvo till Herr ##\xE4ngens fotbolls ##klubb. \n\nThere is also\
          \ a short code example on how to glue them together."
        updatedAt: '2022-12-17T16:11:49.306Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Lauler
    id: 639de9fc7145123e0d4cb4b2
    type: comment
  author: peterkz
  content: "(Update - replied to the original comment)\n\nThe documentation README\
    \ says:\n\n> The BERT tokenizer often splits words into multiple tokens, with\
    \ the subparts starting with ##, for example the string Engelbert k\xF6r Volvo\
    \ till Herr\xE4ngens fotbollsklubb gets tokenized as Engel ##bert k\xF6r Volvo\
    \ till Herr ##\xE4ngens fotbolls ##klubb. \n\nThere is also a short code example\
    \ on how to glue them together."
  created_at: 2022-12-17 16:10:36+00:00
  edited: true
  hidden: false
  id: 639de9fc7145123e0d4cb4b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663598645497-63287f0bce50f1253749f7a3.jpeg?w=200&h=200&f=face
      fullname: "Tom S\xF6derlund"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tomsoderlund
      type: user
    createdAt: '2022-12-17T16:12:32.000Z'
    data:
      edited: false
      editors:
      - tomsoderlund
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663598645497-63287f0bce50f1253749f7a3.jpeg?w=200&h=200&f=face
          fullname: "Tom S\xF6derlund"
          isHf: false
          isPro: false
          name: tomsoderlund
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;peterkz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/peterkz\"\
          >@<span class=\"underline\">peterkz</span></a></span>\n\n\t</span></span>,\
          \ I was too quick/lazy and missed it in the docs.</p>\n"
        raw: Thank you @peterkz, I was too quick/lazy and missed it in the docs.
        updatedAt: '2022-12-17T16:12:32.744Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Lauler
    id: 639dea70f87da5e2eb134ae2
    type: comment
  author: tomsoderlund
  content: Thank you @peterkz, I was too quick/lazy and missed it in the docs.
  created_at: 2022-12-17 16:12:32+00:00
  edited: false
  hidden: false
  id: 639dea70f87da5e2eb134ae2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fedebb29532e2fb1648814a291a96d4.svg
      fullname: Faton Rekathati
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Lauler
      type: user
    createdAt: '2022-12-17T16:17:25.000Z'
    data:
      edited: true
      editors:
      - Lauler
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fedebb29532e2fb1648814a291a96d4.svg
          fullname: Faton Rekathati
          isHf: false
          isPro: false
          name: Lauler
          type: user
        html: "<p>Yes, it is a partial word (a token). <code>##</code> means it is\
          \ a suffix. </p>\n<p>If a token does not have a <code>##</code>, that means\
          \ it is either its own word unit, or alternatively a starting prefix of\
          \ a longer word (the latter is the case when the following token indices\
          \ have tokens starting with <code>##</code>). </p>\n<p>The model card of\
          \ this model has an example of how to concatenate words that are broken\
          \ up when tokenized:</p>\n<pre><code>text = 'Engelbert tar Volvon till Tele2\
          \ Arena f\xF6r att titta p\xE5 Djurg\xE5rden IF ' +\\\n       'som spelar\
          \ fotboll i VM klockan tv\xE5 p\xE5 kv\xE4llen.'\n\nl = []\nfor token in\
          \ nlp(text):\n    if token['word'].startswith('##'):\n        l[-1]['word']\
          \ += token['word'][2:]\n    else:\n        l += [ token ]\n\nprint(l)\n\
          </code></pre>\n<p>Transformer language models have fixed size vocabularies.\
          \ The wordpiece chunks that constitute its vocabulary are called \"tokens\"\
          . Generally, before pre-training a transformer language model, we first\
          \ train a <code>tokenizer</code> to efficiently encode text in a given language\
          \ into a fixed number of discrete vocabulary units (tokens). </p>\n<p>The\
          \ reason we need these is because we cannot actually input words or text\
          \ into the model, but rather input numeric vector representations (embeddings)\
          \ of words or pieces of words (tokens). If each individual word and each\
          \ word inflection in a language would be its own token, we would need millions\
          \ of tokens and millions of these numeric representations for each token\
          \ to be able to represent a language as its unieuq set of possible word\
          \ units. Instead, we train a tokenizer with a fixed vocabulary size (e.g.\
          \ this model has about 52K token vocab size), and the purpose of one of\
          \ these tokenizer models is to </p>\n<ol>\n<li>Encode as much text as possible\
          \ efficiently into as few discrete token units as possible.</li>\n<li>Have\
          \ a mapping of each token to an integer, where the integer is used to select\
          \ the correct numeric vector representation of the token to be supplied\
          \ as input to the model. </li>\n<li>Perform the reverse mapping from integer\
          \ to text tokens once the model has processed the text.</li>\n</ol>\n<p>Video\
          \ explanation: <a rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=qpv6ms_t_1A\"\
          >https://www.youtube.com/watch?v=qpv6ms_t_1A</a></p>\n"
        raw: "Yes, it is a partial word (a token). `##` means it is a suffix. \n\n\
          If a token does not have a `##`, that means it is either its own word unit,\
          \ or alternatively a starting prefix of a longer word (the latter is the\
          \ case when the following token indices have tokens starting with `##`).\
          \ \n\nThe model card of this model has an example of how to concatenate\
          \ words that are broken up when tokenized:\n\n```\ntext = 'Engelbert tar\
          \ Volvon till Tele2 Arena f\xF6r att titta p\xE5 Djurg\xE5rden IF ' +\\\n\
          \       'som spelar fotboll i VM klockan tv\xE5 p\xE5 kv\xE4llen.'\n\nl\
          \ = []\nfor token in nlp(text):\n    if token['word'].startswith('##'):\n\
          \        l[-1]['word'] += token['word'][2:]\n    else:\n        l += [ token\
          \ ]\n\nprint(l)\n```\n\nTransformer language models have fixed size vocabularies.\
          \ The wordpiece chunks that constitute its vocabulary are called \"tokens\"\
          . Generally, before pre-training a transformer language model, we first\
          \ train a `tokenizer` to efficiently encode text in a given language into\
          \ a fixed number of discrete vocabulary units (tokens). \n\nThe reason we\
          \ need these is because we cannot actually input words or text into the\
          \ model, but rather input numeric vector representations (embeddings) of\
          \ words or pieces of words (tokens). If each individual word and each word\
          \ inflection in a language would be its own token, we would need millions\
          \ of tokens and millions of these numeric representations for each token\
          \ to be able to represent a language as its unieuq set of possible word\
          \ units. Instead, we train a tokenizer with a fixed vocabulary size (e.g.\
          \ this model has about 52K token vocab size), and the purpose of one of\
          \ these tokenizer models is to \n\n1. Encode as much text as possible efficiently\
          \ into as few discrete token units as possible.\n2. Have a mapping of each\
          \ token to an integer, where the integer is used to select the correct numeric\
          \ vector representation of the token to be supplied as input to the model.\
          \ \n3. Perform the reverse mapping from integer to text tokens once the\
          \ model has processed the text.\n\nVideo explanation: https://www.youtube.com/watch?v=qpv6ms_t_1A"
        updatedAt: '2022-12-17T16:20:24.654Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - peterkz
    id: 639deb9572706670111aaf07
    type: comment
  author: Lauler
  content: "Yes, it is a partial word (a token). `##` means it is a suffix. \n\nIf\
    \ a token does not have a `##`, that means it is either its own word unit, or\
    \ alternatively a starting prefix of a longer word (the latter is the case when\
    \ the following token indices have tokens starting with `##`). \n\nThe model card\
    \ of this model has an example of how to concatenate words that are broken up\
    \ when tokenized:\n\n```\ntext = 'Engelbert tar Volvon till Tele2 Arena f\xF6\
    r att titta p\xE5 Djurg\xE5rden IF ' +\\\n       'som spelar fotboll i VM klockan\
    \ tv\xE5 p\xE5 kv\xE4llen.'\n\nl = []\nfor token in nlp(text):\n    if token['word'].startswith('##'):\n\
    \        l[-1]['word'] += token['word'][2:]\n    else:\n        l += [ token ]\n\
    \nprint(l)\n```\n\nTransformer language models have fixed size vocabularies. The\
    \ wordpiece chunks that constitute its vocabulary are called \"tokens\". Generally,\
    \ before pre-training a transformer language model, we first train a `tokenizer`\
    \ to efficiently encode text in a given language into a fixed number of discrete\
    \ vocabulary units (tokens). \n\nThe reason we need these is because we cannot\
    \ actually input words or text into the model, but rather input numeric vector\
    \ representations (embeddings) of words or pieces of words (tokens). If each individual\
    \ word and each word inflection in a language would be its own token, we would\
    \ need millions of tokens and millions of these numeric representations for each\
    \ token to be able to represent a language as its unieuq set of possible word\
    \ units. Instead, we train a tokenizer with a fixed vocabulary size (e.g. this\
    \ model has about 52K token vocab size), and the purpose of one of these tokenizer\
    \ models is to \n\n1. Encode as much text as possible efficiently into as few\
    \ discrete token units as possible.\n2. Have a mapping of each token to an integer,\
    \ where the integer is used to select the correct numeric vector representation\
    \ of the token to be supplied as input to the model. \n3. Perform the reverse\
    \ mapping from integer to text tokens once the model has processed the text.\n\
    \nVideo explanation: https://www.youtube.com/watch?v=qpv6ms_t_1A"
  created_at: 2022-12-17 16:17:25+00:00
  edited: true
  hidden: false
  id: 639deb9572706670111aaf07
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: KBLab/bert-base-swedish-cased-ner
repo_type: model
status: open
target_branch: null
title: Why is the model struggling with pluralization of Swedish words?
