!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fkov
conflicting_files: null
created_at: 2023-07-29 02:15:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a17e6120a0e504c9d8e95147efaadb7.svg
      fullname: FK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fkov
      type: user
    createdAt: '2023-07-29T03:15:39.000Z'
    data:
      edited: false
      editors:
      - fkov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9614647626876831
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a17e6120a0e504c9d8e95147efaadb7.svg
          fullname: FK
          isHf: false
          isPro: false
          name: fkov
          type: user
        html: "<p>Hello \U0001F44B<br>I was trying to lower the validation loss without\
          \ success. I tried with different dropouts eg.: --attention_dropout 0.1\
          \ --dropout 0 \u2014activation_dropout 0 and other variations. However what\
          \ I observed is that with any dropout the training loss stops decreasing,\
          \ and the evaluation loss reaches similar values as the training loss (they\
          \ are both still high(60-80%)) however the accuracy is constant with low\
          \ value (~50-60%).  Do you have any suggestions on which training parameters\
          \ might help decrease the evaluation loss? </p>\n"
        raw: "Hello \U0001F44B \r\nI was trying to lower the validation loss without\
          \ success. I tried with different dropouts eg.: --attention_dropout 0.1\
          \ --dropout 0 \u2014activation_dropout 0 and other variations. However what\
          \ I observed is that with any dropout the training loss stops decreasing,\
          \ and the evaluation loss reaches similar values as the training loss (they\
          \ are both still high(60-80%)) however the accuracy is constant with low\
          \ value (~50-60%).  Do you have any suggestions on which training parameters\
          \ might help decrease the evaluation loss? "
        updatedAt: '2023-07-29T03:15:39.385Z'
      numEdits: 0
      reactions: []
    id: 64c4845b3f3387bcfa0a7aeb
    type: comment
  author: fkov
  content: "Hello \U0001F44B \r\nI was trying to lower the validation loss without\
    \ success. I tried with different dropouts eg.: --attention_dropout 0.1 --dropout\
    \ 0 \u2014activation_dropout 0 and other variations. However what I observed is\
    \ that with any dropout the training loss stops decreasing, and the evaluation\
    \ loss reaches similar values as the training loss (they are both still high(60-80%))\
    \ however the accuracy is constant with low value (~50-60%).  Do you have any\
    \ suggestions on which training parameters might help decrease the evaluation\
    \ loss? "
  created_at: 2023-07-29 02:15:39+00:00
  edited: false
  hidden: false
  id: 64c4845b3f3387bcfa0a7aeb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: sanchit-gandhi/whisper-medium-fleurs-lang-id
repo_type: model
status: open
target_branch: null
title: decreasing validation loss?
