!!python/object:huggingface_hub.community.DiscussionWithDetails
author: patrickvonplaten
conflicting_files: null
created_at: 2022-06-08 15:07:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-06-08T16:07:06.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;ccdv&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ccdv\">@<span class=\"\
          underline\">ccdv</span></a></span>\n\n\t</span></span> - amazing job on\
          \ this BART-Base implementation! That's a really impressive performance\
          \ :-) </p>\n<p>With <span data-props=\"{&quot;user&quot;:&quot;Stancld&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Stancld\"\
          >@<span class=\"underline\">Stancld</span></a></span>\n\n\t</span></span>\
          \  we are working on LongT5: <a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2112.07916.pdf\"\
          >https://arxiv.org/pdf/2112.07916.pdf</a> and are currently fine-tuning\
          \ the model on PubMed - do you have any link to a loss curve by any chance\
          \ we could look into? </p>\n"
        raw: "Hey @ccdv - amazing job on this BART-Base implementation! That's a really\
          \ impressive performance :-) \r\n\r\nWith @Stancld  we are working on LongT5:\
          \ https://arxiv.org/pdf/2112.07916.pdf and are currently fine-tuning the\
          \ model on PubMed - do you have any link to a loss curve by any chance we\
          \ could look into? "
        updatedAt: '2022-06-08T16:07:06.262Z'
      numEdits: 0
      reactions: []
    id: 62a0c92a8498753ef042f082
    type: comment
  author: patrickvonplaten
  content: "Hey @ccdv - amazing job on this BART-Base implementation! That's a really\
    \ impressive performance :-) \r\n\r\nWith @Stancld  we are working on LongT5:\
    \ https://arxiv.org/pdf/2112.07916.pdf and are currently fine-tuning the model\
    \ on PubMed - do you have any link to a loss curve by any chance we could look\
    \ into? "
  created_at: 2022-06-08 15:07:06+00:00
  edited: false
  hidden: false
  id: 62a0c92a8498753ef042f082
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659877476012-616f3a4cf92cbd8e03c1d541.jpeg?w=200&h=200&f=face
      fullname: ccdv
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ccdv
      type: user
    createdAt: '2022-06-09T08:26:44.000Z'
    data:
      edited: true
      editors:
      - ccdv
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659877476012-616f3a4cf92cbd8e03c1d541.jpeg?w=200&h=200&f=face
          fullname: ccdv
          isHf: false
          isPro: false
          name: ccdv
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;patrickvonplaten&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/patrickvonplaten\"\
          >@<span class=\"underline\">patrickvonplaten</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Sadly I dont have the loss curve (empty tensorboard) but I have\
          \ the final training loss and training params (eval_loss and eval metrics\
          \ are on the test set).<br>Training is done using the summarization script\
          \ (with few tweaks) from Transformers.<br>Note that the model has been trained\
          \ on 4096 length (<a href=\"https://huggingface.co/ccdv/lsg-bart-base-4096-pubmed\"\
          >see</a>) during 8 epochs, then converted and finetuned during 1 epoch on\
          \ 16384 length to reduce overall computation.<br>To convert BART-base and\
          \ to increase max model length, I rely on a <a rel=\"nofollow\" href=\"\
          https://github.com/ccdv-ai/convert_checkpoint_to_lsg\">conversion script</a>\
          \ also able to convert BERT, RoBERTa and DistilBERT checkpoints (attention\
          \ is replaced and global tokens added).</p>\n<p>The objective of the LSG\
          \ approach is to significantly reduce computation costs (tight budget) for\
          \ standard and encoder-decoder models on long sequences using plain PyTorch.<br>You\
          \ can get about x2 training speed compared to Longformer/BigBird if you\
          \ convert RoBERTa to 4096 length (replacing the attention mecanism only).</p>\n\
          <p>Final losses:<br>On 4096 length:</p>\n<pre><code>{\n    \"epoch\": 8.0,\n\
          \    \"train_loss\": 1.727705581620244,\n    \"train_samples\": 119924,\n\
          \    \"eval_gen_len\": 353.9225, \n    \"eval_loss\": 1.5759657621383667,\n\
          \    \"eval_samples\": 6658\n}\n</code></pre>\n<p>On 16394 length:</p>\n\
          <pre><code>{\n    \"epoch\": 1.0,\n    \"train_loss\": 1.5442171282598995,\n\
          \    \"train_samples\": 119924,\n    \"eval_gen_len\": 337.5673, \n    \"\
          eval_loss\": 1.505071759223938,\n    \"eval_samples\": 6658\n}\n</code></pre>\n\
          <p>Training params:</p>\n<ul>\n<li>total batch_size: 32</li>\n<li>lr: 8e-5</li>\n\
          <li>warmup_ratio: 0.1</li>\n<li>epochs (4096 length): 8</li>\n<li>epochs\
          \ (16834 length): 1</li>\n<li>lr linear decay</li>\n</ul>\n<p>This model\
          \ only has 145M params, a different batch size/lr may be required for larger\
          \ models, especially for LongT5 (smallest model has about 220M params, largest\
          \ 3B).<br>The setup is similar (less epochs) for the ArXiv summarization\
          \ dataset, see <a href=\"https://huggingface.co/ccdv/lsg-bart-base-16384-arxiv\"\
          >lsg-bart-base-16384-arxiv</a></p>\n"
        raw: "Hi @patrickvonplaten \n\nSadly I dont have the loss curve (empty tensorboard)\
          \ but I have the final training loss and training params (eval_loss and\
          \ eval metrics are on the test set). \nTraining is done using the summarization\
          \ script (with few tweaks) from Transformers.\nNote that the model has been\
          \ trained on 4096 length ([see](https://huggingface.co/ccdv/lsg-bart-base-4096-pubmed))\
          \ during 8 epochs, then converted and finetuned during 1 epoch on 16384\
          \ length to reduce overall computation.\nTo convert BART-base and to increase\
          \ max model length, I rely on a [conversion script](https://github.com/ccdv-ai/convert_checkpoint_to_lsg)\
          \ also able to convert BERT, RoBERTa and DistilBERT checkpoints (attention\
          \ is replaced and global tokens added).\n\nThe objective of the LSG approach\
          \ is to significantly reduce computation costs (tight budget) for standard\
          \ and encoder-decoder models on long sequences using plain PyTorch. \nYou\
          \ can get about x2 training speed compared to Longformer/BigBird if you\
          \ convert RoBERTa to 4096 length (replacing the attention mecanism only).\n\
          \nFinal losses:\nOn 4096 length:\n```\n{\n    \"epoch\": 8.0,\n    \"train_loss\"\
          : 1.727705581620244,\n    \"train_samples\": 119924,\n    \"eval_gen_len\"\
          : 353.9225, \n    \"eval_loss\": 1.5759657621383667,\n    \"eval_samples\"\
          : 6658\n}\n```\n\nOn 16394 length:\n```\n{\n    \"epoch\": 1.0,\n    \"\
          train_loss\": 1.5442171282598995,\n    \"train_samples\": 119924,\n    \"\
          eval_gen_len\": 337.5673, \n    \"eval_loss\": 1.505071759223938,\n    \"\
          eval_samples\": 6658\n}\n```\n\nTraining params:\n- total batch_size: 32\n\
          - lr: 8e-5\n- warmup_ratio: 0.1\n- epochs (4096 length): 8\n- epochs (16834\
          \ length): 1\n- lr linear decay\n\nThis model only has 145M params, a different\
          \ batch size/lr may be required for larger models, especially for LongT5\
          \ (smallest model has about 220M params, largest 3B).\nThe setup is similar\
          \ (less epochs) for the ArXiv summarization dataset, see [lsg-bart-base-16384-arxiv](https://huggingface.co/ccdv/lsg-bart-base-16384-arxiv)"
        updatedAt: '2022-06-09T09:19:36.362Z'
      numEdits: 2
      reactions: []
    id: 62a1aec4073fae54662319cd
    type: comment
  author: ccdv
  content: "Hi @patrickvonplaten \n\nSadly I dont have the loss curve (empty tensorboard)\
    \ but I have the final training loss and training params (eval_loss and eval metrics\
    \ are on the test set). \nTraining is done using the summarization script (with\
    \ few tweaks) from Transformers.\nNote that the model has been trained on 4096\
    \ length ([see](https://huggingface.co/ccdv/lsg-bart-base-4096-pubmed)) during\
    \ 8 epochs, then converted and finetuned during 1 epoch on 16384 length to reduce\
    \ overall computation.\nTo convert BART-base and to increase max model length,\
    \ I rely on a [conversion script](https://github.com/ccdv-ai/convert_checkpoint_to_lsg)\
    \ also able to convert BERT, RoBERTa and DistilBERT checkpoints (attention is\
    \ replaced and global tokens added).\n\nThe objective of the LSG approach is to\
    \ significantly reduce computation costs (tight budget) for standard and encoder-decoder\
    \ models on long sequences using plain PyTorch. \nYou can get about x2 training\
    \ speed compared to Longformer/BigBird if you convert RoBERTa to 4096 length (replacing\
    \ the attention mecanism only).\n\nFinal losses:\nOn 4096 length:\n```\n{\n  \
    \  \"epoch\": 8.0,\n    \"train_loss\": 1.727705581620244,\n    \"train_samples\"\
    : 119924,\n    \"eval_gen_len\": 353.9225, \n    \"eval_loss\": 1.5759657621383667,\n\
    \    \"eval_samples\": 6658\n}\n```\n\nOn 16394 length:\n```\n{\n    \"epoch\"\
    : 1.0,\n    \"train_loss\": 1.5442171282598995,\n    \"train_samples\": 119924,\n\
    \    \"eval_gen_len\": 337.5673, \n    \"eval_loss\": 1.505071759223938,\n   \
    \ \"eval_samples\": 6658\n}\n```\n\nTraining params:\n- total batch_size: 32\n\
    - lr: 8e-5\n- warmup_ratio: 0.1\n- epochs (4096 length): 8\n- epochs (16834 length):\
    \ 1\n- lr linear decay\n\nThis model only has 145M params, a different batch size/lr\
    \ may be required for larger models, especially for LongT5 (smallest model has\
    \ about 220M params, largest 3B).\nThe setup is similar (less epochs) for the\
    \ ArXiv summarization dataset, see [lsg-bart-base-16384-arxiv](https://huggingface.co/ccdv/lsg-bart-base-16384-arxiv)"
  created_at: 2022-06-09 07:26:44+00:00
  edited: true
  hidden: false
  id: 62a1aec4073fae54662319cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-06-10T17:22:52.000Z'
    data:
      edited: true
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Super cool - thanks a lot for posting this! cc <span data-props=\"\
          {&quot;user&quot;:&quot;Stancld&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Stancld\">@<span class=\"underline\">Stancld</span></a></span>\n\
          \n\t</span></span></p>\n"
        raw: Super cool - thanks a lot for posting this! cc @Stancld
        updatedAt: '2022-06-10T17:23:01.583Z'
      numEdits: 1
      reactions: []
    id: 62a37dec5e32017b931aaa41
    type: comment
  author: patrickvonplaten
  content: Super cool - thanks a lot for posting this! cc @Stancld
  created_at: 2022-06-10 16:22:52+00:00
  edited: true
  hidden: false
  id: 62a37dec5e32017b931aaa41
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ccdv/lsg-bart-base-16384-pubmed
repo_type: model
status: open
target_branch: null
title: Training loss curves
