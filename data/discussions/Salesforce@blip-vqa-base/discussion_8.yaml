!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AhmadMujtaba200210
conflicting_files: null
created_at: 2023-12-06 07:39:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oHb7Fzflihuo80jDaem5o.jpeg?w=200&h=200&f=face
      fullname: Ahmad Mujtaba
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AhmadMujtaba200210
      type: user
    createdAt: '2023-12-06T07:39:56.000Z'
    data:
      edited: false
      editors:
      - AhmadMujtaba200210
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6696348190307617
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/oHb7Fzflihuo80jDaem5o.jpeg?w=200&h=200&f=face
          fullname: Ahmad Mujtaba
          isHf: false
          isPro: false
          name: AhmadMujtaba200210
          type: user
        html: '<p>I am using this model but I am unable to generate the response in
          more than  a word, for example,<br>my question is describe this picture
          it response me, No.</p>

          <p>here is code,</p>

          <p>import requests<br>from PIL import Image<br>import matplotlib.pyplot
          as plt<br>from transformers import BlipProcessor, BlipForQuestionAnswering</p>

          <p>processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")<br>model
          = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to("cuda")</p>

          <p>img_url = ''<a rel="nofollow" href="https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg''">https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg''</a><br>raw_image
          = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')</p>

          <h1 id="display-the-image">Display the image</h1>

          <p>plt.imshow(raw_image)<br>plt.axis(''off'')<br>plt.show()</p>

          <p>question = "how many dogs are in the picture?"<br>inputs = processor(raw_image,
          question, return_tensors="pt").to("cuda")</p>

          <p>out = model.generate(**inputs)</p>

          <h1 id="decode-and-print-every-possible-output">Decode and print every possible
          output</h1>

          <p>for i in range(out.shape[0]):<br>    answer = processor.decode(out[i],
          skip_special_tokens=True)<br>    print(f"Answer {i + 1}: {answer}")</p>

          '
        raw: "I am using this model but I am unable to generate the response in more\
          \ than  a word, for example,\r\nmy question is describe this picture it\
          \ response me, No.\r\n\r\nhere is code,\r\n\r\nimport requests\r\nfrom PIL\
          \ import Image\r\nimport matplotlib.pyplot as plt\r\nfrom transformers import\
          \ BlipProcessor, BlipForQuestionAnswering\r\n\r\nprocessor = BlipProcessor.from_pretrained(\"\
          Salesforce/blip-vqa-base\")\r\nmodel = BlipForQuestionAnswering.from_pretrained(\"\
          Salesforce/blip-vqa-base\").to(\"cuda\")\r\n\r\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\
          \ \r\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\r\
          \n\r\n# Display the image\r\nplt.imshow(raw_image)\r\nplt.axis('off')\r\n\
          plt.show()\r\n\r\nquestion = \"how many dogs are in the picture?\"\r\ninputs\
          \ = processor(raw_image, question, return_tensors=\"pt\").to(\"cuda\")\r\
          \n\r\nout = model.generate(**inputs)\r\n\r\n# Decode and print every possible\
          \ output\r\nfor i in range(out.shape[0]):\r\n    answer = processor.decode(out[i],\
          \ skip_special_tokens=True)\r\n    print(f\"Answer {i + 1}: {answer}\")\r\
          \n"
        updatedAt: '2023-12-06T07:39:56.935Z'
      numEdits: 0
      reactions: []
    id: 6570254c64a66b9f4728981e
    type: comment
  author: AhmadMujtaba200210
  content: "I am using this model but I am unable to generate the response in more\
    \ than  a word, for example,\r\nmy question is describe this picture it response\
    \ me, No.\r\n\r\nhere is code,\r\n\r\nimport requests\r\nfrom PIL import Image\r\
    \nimport matplotlib.pyplot as plt\r\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\r\
    \n\r\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\r\
    \nmodel = BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\"\
    ).to(\"cuda\")\r\n\r\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\
    \ \r\nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\r\
    \n\r\n# Display the image\r\nplt.imshow(raw_image)\r\nplt.axis('off')\r\nplt.show()\r\
    \n\r\nquestion = \"how many dogs are in the picture?\"\r\ninputs = processor(raw_image,\
    \ question, return_tensors=\"pt\").to(\"cuda\")\r\n\r\nout = model.generate(**inputs)\r\
    \n\r\n# Decode and print every possible output\r\nfor i in range(out.shape[0]):\r\
    \n    answer = processor.decode(out[i], skip_special_tokens=True)\r\n    print(f\"\
    Answer {i + 1}: {answer}\")\r\n"
  created_at: 2023-12-06 07:39:56+00:00
  edited: false
  hidden: false
  id: 6570254c64a66b9f4728981e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-07T09:17:02.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.686360776424408
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;AhmadMujtaba200210&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/AhmadMujtaba200210\"\
          >@<span class=\"underline\">AhmadMujtaba200210</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for the issue, </p>\n<p>I think this is expected, per my understanding\
          \ this model is trained to generate short output / sentences. </p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">import</span> requests\n\
          <span class=\"hljs-keyword\">from</span> PIL <span class=\"hljs-keyword\"\
          >import</span> Image\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> BlipProcessor, BlipForQuestionAnswering\n\
          \nprocessor = BlipProcessor.from_pretrained(<span class=\"hljs-string\"\
          >\"Salesforce/blip-vqa-base\"</span>)\nmodel = BlipForQuestionAnswering.from_pretrained(<span\
          \ class=\"hljs-string\">\"Salesforce/blip-vqa-base\"</span>).to(<span class=\"\
          hljs-string\">\"cuda\"</span>)\n\nimg_url = <span class=\"hljs-string\"\
          >'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'</span>\
          \ \nraw_image = Image.<span class=\"hljs-built_in\">open</span>(requests.get(img_url,\
          \ stream=<span class=\"hljs-literal\">True</span>).raw).convert(<span class=\"\
          hljs-string\">'RGB'</span>)\n\nquestion = <span class=\"hljs-string\">\"\
          how many dogs are in the picture?\"</span>\ninputs = processor(raw_image,\
          \ question, return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(<span\
          \ class=\"hljs-string\">\"cuda\"</span>)\n\nout = model.generate(**inputs)\n\
          <span class=\"hljs-built_in\">print</span>(processor.decode(out[<span class=\"\
          hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span><span class=\"\
          hljs-number\">1</span>\n</code></pre>\n<p>To generate longer sentences you\
          \ can either use the image captioning models, or use other architectures,\
          \ such as <code>llava</code>: <a href=\"https://huggingface.co/llava-hf\"\
          >https://huggingface.co/llava-hf</a></p>\n"
        raw: "Hi @AhmadMujtaba200210 \nThanks for the issue, \n\nI think this is expected,\
          \ per my understanding this model is trained to generate short output /\
          \ sentences. \n\n```python\nimport requests\nfrom PIL import Image\nfrom\
          \ transformers import BlipProcessor, BlipForQuestionAnswering\n\nprocessor\
          \ = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\nmodel =\
          \ BlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\"\
          ).to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\
          \ \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\
          \nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image,\
          \ question, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\n\
          print(processor.decode(out[0], skip_special_tokens=True))\n>>> 1\n```\n\n\
          To generate longer sentences you can either use the image captioning models,\
          \ or use other architectures, such as `llava`: https://huggingface.co/llava-hf"
        updatedAt: '2023-12-07T09:17:02.050Z'
      numEdits: 0
      reactions: []
    id: 65718d8ed5a82cf89b082aa4
    type: comment
  author: ybelkada
  content: "Hi @AhmadMujtaba200210 \nThanks for the issue, \n\nI think this is expected,\
    \ per my understanding this model is trained to generate short output / sentences.\
    \ \n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import\
    \ BlipProcessor, BlipForQuestionAnswering\n\nprocessor = BlipProcessor.from_pretrained(\"\
    Salesforce/blip-vqa-base\")\nmodel = BlipForQuestionAnswering.from_pretrained(\"\
    Salesforce/blip-vqa-base\").to(\"cuda\")\n\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\
    \ \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\
    \nquestion = \"how many dogs are in the picture?\"\ninputs = processor(raw_image,\
    \ question, return_tensors=\"pt\").to(\"cuda\")\n\nout = model.generate(**inputs)\n\
    print(processor.decode(out[0], skip_special_tokens=True))\n>>> 1\n```\n\nTo generate\
    \ longer sentences you can either use the image captioning models, or use other\
    \ architectures, such as `llava`: https://huggingface.co/llava-hf"
  created_at: 2023-12-07 09:17:02+00:00
  edited: false
  hidden: false
  id: 65718d8ed5a82cf89b082aa4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Salesforce/blip-vqa-base
repo_type: model
status: open
target_branch: null
title: How I can generate the ttext more than a single word?
