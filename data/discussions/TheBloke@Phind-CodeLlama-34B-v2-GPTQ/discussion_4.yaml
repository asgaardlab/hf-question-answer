!!python/object:huggingface_hub.community.DiscussionWithDetails
author: m9e
conflicting_files: null
created_at: 2023-09-11 22:29:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
      fullname: Matthew Wallace
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m9e
      type: user
    createdAt: '2023-09-11T23:29:21.000Z'
    data:
      edited: false
      editors:
      - m9e
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7969467639923096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
          fullname: Matthew Wallace
          isHf: false
          isPro: false
          name: m9e
          type: user
        html: '<p>Just finished testing this against HumanEval, using this config:</p>

          <p>{<br>  "_from_model_config": true,<br>  "bos_token_id": 1,<br>  "do_sample":
          true,<br>  "pad_token_id": 2,<br>  "eos_token_id": 2,<br>  "max_new_tokens":
          384,<br>  "temperature": 0.1,<br>  "top_p": 0.75,<br>   "top_k": 40,<br>  "transformers_version":
          "4.33.1"<br>}</p>

          <p>which I believe ~matches the Phind config.</p>

          <p>on the :gptq-4bit-32g-actorder_True branch</p>

          <p>results</p>

          <p>{''pass@1'': 0.725609756097561}</p>

          <p>which I believe means my first test on humaneval the model got 119/164
          correct, vs 121/164 for the full precision according to the model card,
          although given it''s not fully deterministic can''t say for sure that''s
          the quantization.</p>

          <p>Still, nice to see it come in extremely tight on humaneval.</p>

          '
        raw: "Just finished testing this against HumanEval, using this config:\r\n\
          \r\n{\r\n  \"_from_model_config\": true,\r\n  \"bos_token_id\": 1,\r\n \
          \ \"do_sample\": true,\r\n  \"pad_token_id\": 2,\r\n  \"eos_token_id\":\
          \ 2,\r\n  \"max_new_tokens\": 384,\r\n  \"temperature\": 0.1,\r\n  \"top_p\"\
          : 0.75,\r\n   \"top_k\": 40,\r\n  \"transformers_version\": \"4.33.1\"\r\
          \n}\r\n\r\nwhich I believe ~matches the Phind config.\r\n\r\non the :gptq-4bit-32g-actorder_True\
          \ branch\r\n\r\nresults\r\n\r\n{'pass@1': 0.725609756097561}\r\n\r\nwhich\
          \ I believe means my first test on humaneval the model got 119/164 correct,\
          \ vs 121/164 for the full precision according to the model card, although\
          \ given it's not fully deterministic can't say for sure that's the quantization.\r\
          \n\r\nStill, nice to see it come in extremely tight on humaneval."
        updatedAt: '2023-09-11T23:29:21.276Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - radm
        - gnomealone
        - Tianh
    id: 64ffa2d1efd273eec76dce04
    type: comment
  author: m9e
  content: "Just finished testing this against HumanEval, using this config:\r\n\r\
    \n{\r\n  \"_from_model_config\": true,\r\n  \"bos_token_id\": 1,\r\n  \"do_sample\"\
    : true,\r\n  \"pad_token_id\": 2,\r\n  \"eos_token_id\": 2,\r\n  \"max_new_tokens\"\
    : 384,\r\n  \"temperature\": 0.1,\r\n  \"top_p\": 0.75,\r\n   \"top_k\": 40,\r\
    \n  \"transformers_version\": \"4.33.1\"\r\n}\r\n\r\nwhich I believe ~matches\
    \ the Phind config.\r\n\r\non the :gptq-4bit-32g-actorder_True branch\r\n\r\n\
    results\r\n\r\n{'pass@1': 0.725609756097561}\r\n\r\nwhich I believe means my first\
    \ test on humaneval the model got 119/164 correct, vs 121/164 for the full precision\
    \ according to the model card, although given it's not fully deterministic can't\
    \ say for sure that's the quantization.\r\n\r\nStill, nice to see it come in extremely\
    \ tight on humaneval."
  created_at: 2023-09-11 22:29:21+00:00
  edited: false
  hidden: false
  id: 64ffa2d1efd273eec76dce04
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
      fullname: Matthew Wallace
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: m9e
      type: user
    createdAt: '2023-09-12T13:20:26.000Z'
    data:
      edited: false
      editors:
      - m9e
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8334154486656189
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a81f2ea42e958c3dea3c729eb210e34e.svg
          fullname: Matthew Wallace
          isHf: false
          isPro: false
          name: m9e
          type: user
        html: '<p>as a followup, I extended the context window using these settings:</p>

          <p>{<br>  "_from_model_config": true,<br>  "bos_token_id": 1,<br>  "do_sample":
          true,<br>  "pad_token_id": 2,<br>  "eos_token_id": 2,<br>  "max_new_tokens":
          384,<br>  "temperature": 0.1,<br>  "top_p": 0.75,<br>  "top_k": 40,<br>  "max_seq_length":
          16384,<br>  "rope_freq_base": 1000000,<br>  "compress_pos_emb": 4,<br>  "gpu_split":
          "19,23",<br>  "transformers_version": "4.33.1"<br>}</p>

          <p>Fumbling around a bit from various sources on the proper way to do the
          longer sequence with the CodeLlama model, but it was able to digest a 1000-line/35k
          character file and analyze the code in a way that wasn''t crazy in text-generation-webui
          with these settings, so I spawned them in my server version (which is just
          a fastapi wrapper around AutoModelForCausalLM.from_pretrained basically)</p>

          <p>and re-tested</p>

          <p>{''pass@1'': 0.7134146341463414}</p>

          <p>again hard to say where the variance is, and my impression is that the
          compress_pos_emb setting needed to extend the context properly has a small
          negative effect, so this wasn''t surprising but it still seemed competent.</p>

          '
        raw: "as a followup, I extended the context window using these settings:\n\
          \n{\n  \"_from_model_config\": true,\n  \"bos_token_id\": 1,\n  \"do_sample\"\
          : true,\n  \"pad_token_id\": 2,\n  \"eos_token_id\": 2,\n  \"max_new_tokens\"\
          : 384,\n  \"temperature\": 0.1,\n  \"top_p\": 0.75,\n  \"top_k\": 40,\n\
          \  \"max_seq_length\": 16384,\n  \"rope_freq_base\": 1000000,\n  \"compress_pos_emb\"\
          : 4,\n  \"gpu_split\": \"19,23\",\n  \"transformers_version\": \"4.33.1\"\
          \n}\n\nFumbling around a bit from various sources on the proper way to do\
          \ the longer sequence with the CodeLlama model, but it was able to digest\
          \ a 1000-line/35k character file and analyze the code in a way that wasn't\
          \ crazy in text-generation-webui with these settings, so I spawned them\
          \ in my server version (which is just a fastapi wrapper around AutoModelForCausalLM.from_pretrained\
          \ basically)\n\nand re-tested\n\n{'pass@1': 0.7134146341463414}\n\nagain\
          \ hard to say where the variance is, and my impression is that the compress_pos_emb\
          \ setting needed to extend the context properly has a small negative effect,\
          \ so this wasn't surprising but it still seemed competent."
        updatedAt: '2023-09-12T13:20:26.331Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - radm
    id: 6500659a0e8369f6a8ec9138
    type: comment
  author: m9e
  content: "as a followup, I extended the context window using these settings:\n\n\
    {\n  \"_from_model_config\": true,\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n\
    \  \"pad_token_id\": 2,\n  \"eos_token_id\": 2,\n  \"max_new_tokens\": 384,\n\
    \  \"temperature\": 0.1,\n  \"top_p\": 0.75,\n  \"top_k\": 40,\n  \"max_seq_length\"\
    : 16384,\n  \"rope_freq_base\": 1000000,\n  \"compress_pos_emb\": 4,\n  \"gpu_split\"\
    : \"19,23\",\n  \"transformers_version\": \"4.33.1\"\n}\n\nFumbling around a bit\
    \ from various sources on the proper way to do the longer sequence with the CodeLlama\
    \ model, but it was able to digest a 1000-line/35k character file and analyze\
    \ the code in a way that wasn't crazy in text-generation-webui with these settings,\
    \ so I spawned them in my server version (which is just a fastapi wrapper around\
    \ AutoModelForCausalLM.from_pretrained basically)\n\nand re-tested\n\n{'pass@1':\
    \ 0.7134146341463414}\n\nagain hard to say where the variance is, and my impression\
    \ is that the compress_pos_emb setting needed to extend the context properly has\
    \ a small negative effect, so this wasn't surprising but it still seemed competent."
  created_at: 2023-09-12 12:20:26+00:00
  edited: false
  hidden: false
  id: 6500659a0e8369f6a8ec9138
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Phind-CodeLlama-34B-v2-GPTQ
repo_type: model
status: open
target_branch: null
title: HumanEval Results
