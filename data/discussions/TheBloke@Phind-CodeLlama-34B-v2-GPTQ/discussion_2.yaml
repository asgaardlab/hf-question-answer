!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GirishSharma
conflicting_files: null
created_at: 2023-09-07 23:08:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/310ad54d26901816b5d91535d908a58d.svg
      fullname: Girish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GirishSharma
      type: user
    createdAt: '2023-09-08T00:08:18.000Z'
    data:
      edited: false
      editors:
      - GirishSharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49306291341781616
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/310ad54d26901816b5d91535d908a58d.svg
          fullname: Girish
          isHf: false
          isPro: false
          name: GirishSharma
          type: user
        html: '<p>(.env) girish@mypc:/media/girish/Lekha/Phind-CodeLlama-34B-v2-GPTQ/oobabooga_linux/one-click-installers/text-generation-webui$
          python server.py<br>2023-09-08 05:31:34.088192: I tensorflow/core/platform/cpu_feature_guard.cc:182]
          This TensorFlow binary is optimized to use available CPU instructions in
          performance-critical operations.<br>To enable the following instructions:
          AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler
          flags.<br>2023-09-08 05:31:35.359894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]
          TF-TRT Warning: Could not find TensorRT<br>2023-09-08 05:31:37 INFO:Loading
          the extension "gallery"...<br>Running on local URL:  <a rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.<br>2023-09-08
          05:32:06 INFO:Loading TheBloke_Phind-CodeLlama-34B-v2-GPTQ_gptq-4bit-32g-actorder_True...<br>2023-09-08
          05:32:07 INFO:The AutoGPTQ params are: {''model_basename'': ''model'', ''device'':
          ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'': True, ''inject_fused_mlp'':
          True, ''use_safetensors'': True, ''trust_remote_code'': False, ''max_memory'':
          None, ''quantize_config'': None, ''use_cuda_fp16'': True, ''disable_exllama'':
          False}<br>''AsyncRequest'' object has no attribute ''_json_response_data''<br>Killed</p>

          <p>Python version 3.11.4<br>Ubuntu 23.04</p>

          <p>When I tries to add the model, it get failed by above error message.</p>

          '
        raw: "(.env) girish@mypc:/media/girish/Lekha/Phind-CodeLlama-34B-v2-GPTQ/oobabooga_linux/one-click-installers/text-generation-webui$\
          \ python server.py\r\n2023-09-08 05:31:34.088192: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
          \ This TensorFlow binary is optimized to use available CPU instructions\
          \ in performance-critical operations.\r\nTo enable the following instructions:\
          \ AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate\
          \ compiler flags.\r\n2023-09-08 05:31:35.359894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38]\
          \ TF-TRT Warning: Could not find TensorRT\r\n2023-09-08 05:31:37 INFO:Loading\
          \ the extension \"gallery\"...\r\nRunning on local URL:  http://127.0.0.1:7860\r\
          \n\r\nTo create a public link, set `share=True` in `launch()`.\r\n2023-09-08\
          \ 05:32:06 INFO:Loading TheBloke_Phind-CodeLlama-34B-v2-GPTQ_gptq-4bit-32g-actorder_True...\r\
          \n2023-09-08 05:32:07 INFO:The AutoGPTQ params are: {'model_basename': 'model',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None, 'use_cuda_fp16': True,\
          \ 'disable_exllama': False}\r\n'AsyncRequest' object has no attribute '_json_response_data'\r\
          \nKilled\r\n\r\nPython version 3.11.4\r\nUbuntu 23.04\r\n\r\nWhen I tries\
          \ to add the model, it get failed by above error message."
        updatedAt: '2023-09-08T00:08:18.322Z'
      numEdits: 0
      reactions: []
    id: 64fa65f27eeb6dbf40710bf4
    type: comment
  author: GirishSharma
  content: "(.env) girish@mypc:/media/girish/Lekha/Phind-CodeLlama-34B-v2-GPTQ/oobabooga_linux/one-click-installers/text-generation-webui$\
    \ python server.py\r\n2023-09-08 05:31:34.088192: I tensorflow/core/platform/cpu_feature_guard.cc:182]\
    \ This TensorFlow binary is optimized to use available CPU instructions in performance-critical\
    \ operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations,\
    \ rebuild TensorFlow with the appropriate compiler flags.\r\n2023-09-08 05:31:35.359894:\
    \ W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could\
    \ not find TensorRT\r\n2023-09-08 05:31:37 INFO:Loading the extension \"gallery\"\
    ...\r\nRunning on local URL:  http://127.0.0.1:7860\r\n\r\nTo create a public\
    \ link, set `share=True` in `launch()`.\r\n2023-09-08 05:32:06 INFO:Loading TheBloke_Phind-CodeLlama-34B-v2-GPTQ_gptq-4bit-32g-actorder_True...\r\
    \n2023-09-08 05:32:07 INFO:The AutoGPTQ params are: {'model_basename': 'model',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None, 'use_cuda_fp16': True, 'disable_exllama': False}\r\n\
    'AsyncRequest' object has no attribute '_json_response_data'\r\nKilled\r\n\r\n\
    Python version 3.11.4\r\nUbuntu 23.04\r\n\r\nWhen I tries to add the model, it\
    \ get failed by above error message."
  created_at: 2023-09-07 23:08:18+00:00
  edited: false
  hidden: false
  id: 64fa65f27eeb6dbf40710bf4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Phind-CodeLlama-34B-v2-GPTQ
repo_type: model
status: open
target_branch: null
title: '''AsyncRequest'' object has no attribute ''_json_response_data'''
