!!python/object:huggingface_hub.community.DiscussionWithDetails
author: soorooshak
conflicting_files: null
created_at: 2023-12-21 15:55:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79e06d91f125560bbdf5683359833b91.svg
      fullname: Soroush Seifi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: soorooshak
      type: user
    createdAt: '2023-12-21T15:55:45.000Z'
    data:
      edited: true
      editors:
      - soorooshak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5384016633033752
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79e06d91f125560bbdf5683359833b91.svg
          fullname: Soroush Seifi
          isHf: false
          isPro: false
          name: soorooshak
          type: user
        html: "<p>Hi,<br>Is there a way to easily get patch embeddings with dataparallel\
          \ ?<br>1)Forward function outputs only the cls token embedding.<br>2)Accessing\
          \ .module.forward_features removes the parallelism (OOM error on larger\
          \ batch sizes)<br>3)Replacing the forward function with forward_features\
          \ causes a device mismatch:</p>\n<pre><code>    dino_model = timm.create_model('vit_large_patch14_dinov2.lvd142m',\
          \ pretrained=True, num_classes=0).to(args.device)\n    dino_model=torch.nn.DataParallel(dino_model,device_ids=args.device_ids)\n\
          \    dino_model.forward=copy.deepcopy(dino_model.forward_features)\n   \
          \ self.dino_model=torch.nn.DataParallel(dino_model,device_ids=args.device_ids)\n\
          \    patch_ft = self.dino_model(imgs)\n</code></pre>\n<p>RuntimeError: Expected\
          \ all tensors to be on the same device, but found at least two devices,\
          \ cuda:5 and cuda:3! (when checking argument for argument weight in method\
          \ wrapper__cudnn_convolution)</p>\n<p>this happens in a convolution layer\
          \ in the patch_embed funtion.</p>\n<p>Thanks for your help !</p>\n"
        raw: "Hi,\nIs there a way to easily get patch embeddings with dataparallel\
          \ ?\n1)Forward function outputs only the cls token embedding.\n2)Accessing\
          \ .module.forward_features removes the parallelism (OOM error on larger\
          \ batch sizes)\n3)Replacing the forward function with forward_features causes\
          \ a device mismatch:\n\n        dino_model = timm.create_model('vit_large_patch14_dinov2.lvd142m',\
          \ pretrained=True, num_classes=0).to(args.device)\n        dino_model=torch.nn.DataParallel(dino_model,device_ids=args.device_ids)\n\
          \        dino_model.forward=copy.deepcopy(dino_model.forward_features)\n\
          \        self.dino_model=torch.nn.DataParallel(dino_model,device_ids=args.device_ids)\n\
          \        patch_ft = self.dino_model(imgs)\n\nRuntimeError: Expected all\
          \ tensors to be on the same device, but found at least two devices, cuda:5\
          \ and cuda:3! (when checking argument for argument weight in method wrapper__cudnn_convolution)\n\
          \nthis happens in a convolution layer in the patch_embed funtion.\n\nThanks\
          \ for your help !\n"
        updatedAt: '2023-12-21T16:17:52.224Z'
      numEdits: 2
      reactions: []
    id: 65846001948899c453bc4556
    type: comment
  author: soorooshak
  content: "Hi,\nIs there a way to easily get patch embeddings with dataparallel ?\n\
    1)Forward function outputs only the cls token embedding.\n2)Accessing .module.forward_features\
    \ removes the parallelism (OOM error on larger batch sizes)\n3)Replacing the forward\
    \ function with forward_features causes a device mismatch:\n\n        dino_model\
    \ = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True, num_classes=0).to(args.device)\n\
    \        dino_model=torch.nn.DataParallel(dino_model,device_ids=args.device_ids)\n\
    \        dino_model.forward=copy.deepcopy(dino_model.forward_features)\n     \
    \   self.dino_model=torch.nn.DataParallel(dino_model,device_ids=args.device_ids)\n\
    \        patch_ft = self.dino_model(imgs)\n\nRuntimeError: Expected all tensors\
    \ to be on the same device, but found at least two devices, cuda:5 and cuda:3!\
    \ (when checking argument for argument weight in method wrapper__cudnn_convolution)\n\
    \nthis happens in a convolution layer in the patch_embed funtion.\n\nThanks for\
    \ your help !\n"
  created_at: 2023-12-21 15:55:45+00:00
  edited: true
  hidden: false
  id: 65846001948899c453bc4556
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
      fullname: Ross Wightman
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: rwightman
      type: user
    createdAt: '2023-12-21T17:46:22.000Z'
    data:
      edited: false
      editors:
      - rwightman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7678868770599365
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667002643224-604a5184dca2c7ac7508b849.jpeg?w=200&h=200&f=face
          fullname: Ross Wightman
          isHf: true
          isPro: false
          name: rwightman
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;soorooshak&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/soorooshak\">@<span class=\"\
          underline\">soorooshak</span></a></span>\n\n\t</span></span> DataParallel\
          \ wraps forward() methods, you might want to try doing the .forward patch\
          \ you have there before wrapping? Or maybe further wrapping the vit model\
          \ in a nn.Module that calls foward_features()? </p>\n<p>Otherwise, the equivalent\
          \ of foward features should be  to set a null pooling on the model <code>dino_model\
          \ = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True,\
          \ num_classes=0, global_pool='')</code> </p>\n"
        raw: "@soorooshak DataParallel wraps forward() methods, you might want to\
          \ try doing the .forward patch you have there before wrapping? Or maybe\
          \ further wrapping the vit model in a nn.Module that calls foward_features()?\
          \ \n\nOtherwise, the equivalent of foward features should be  to set a null\
          \ pooling on the model `dino_model = timm.create_model('vit_large_patch14_dinov2.lvd142m',\
          \ pretrained=True, num_classes=0, global_pool='')` "
        updatedAt: '2023-12-21T17:46:22.847Z'
      numEdits: 0
      reactions: []
    id: 658479ee8615630cb3c54c04
    type: comment
  author: rwightman
  content: "@soorooshak DataParallel wraps forward() methods, you might want to try\
    \ doing the .forward patch you have there before wrapping? Or maybe further wrapping\
    \ the vit model in a nn.Module that calls foward_features()? \n\nOtherwise, the\
    \ equivalent of foward features should be  to set a null pooling on the model\
    \ `dino_model = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True,\
    \ num_classes=0, global_pool='')` "
  created_at: 2023-12-21 17:46:22+00:00
  edited: false
  hidden: false
  id: 658479ee8615630cb3c54c04
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: timm/vit_large_patch14_dinov2.lvd142m
repo_type: model
status: open
target_branch: null
title: Forward_features data parallel
