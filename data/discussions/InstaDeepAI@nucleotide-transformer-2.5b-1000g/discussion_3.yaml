!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JayceCeleste
conflicting_files: null
created_at: 2023-12-21 05:20:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/H-cznO5GhRoL65AhquqB7.jpeg?w=200&h=200&f=face
      fullname: Zzz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JayceCeleste
      type: user
    createdAt: '2023-12-21T05:20:00.000Z'
    data:
      edited: true
      editors:
      - JayceCeleste
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9226899147033691
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/H-cznO5GhRoL65AhquqB7.jpeg?w=200&h=200&f=face
          fullname: Zzz
          isHf: false
          isPro: false
          name: JayceCeleste
          type: user
        html: '<p>Hello InstaDeepAI and community members,</p>

          <p>I am currently delving into the fascinating work presented in the nucleotide-transformer
          paper, particularly the use of data from the 1000 Genomes Project. The research
          is incredibly insightful, and I have a few questions that I hope the community
          or the authors can help clarify:</p>

          <ol>

          <li><p><strong>Data Interpretation:</strong> The paper mentions the use
          of 3202 high-coverage human genomes, totaling 20.5 trillion nucleotides.
          From my understanding, considering that each human genome is about 3.2 billion
          nucleotides, and accounting for diploid genomes, the calculation, 3.2B *
          2 * 3202 =  20,492.8B = 20.5T, seems to match up with your figures. Can
          anyone confirm if this interpretation is correct or if there''s something
          I''m missing?</p>

          </li>

          <li><p><strong>Handling Large Data Volumes:</strong> In handling data from
          the 1000 Genomes Project, I''ve encountered the challenge of managing large
          volumes of data. With each genome being about 3GB and considering diploidy,
          the data for 3202 samples would be roughly 18.76TB (= 3GB * 2 * 3202) .
          How did the team manage such a vast amount of training data? Did you utilize
          the entire dataset directly for training, or were there specific preprocessing
          or reduction techniques applied?</p>

          </li>

          <li><p><strong>FASTA File Generation Issues:</strong> I''ve been facing
          difficulties in generating FASTA files from VCF files, with processes taking
          an excessive amount of time even on high-spec servers. I''ve tried using
          tools like <code>bcftools consensus</code> and <code>gatk FastaAlternateReferenceMaker</code>
          but to no avail. I''m curious about the tools or methods the team used for
          this conversion process and would appreciate any suggestions or recommendations.</p>

          </li>

          </ol>

          <p>Any insights or guidance on these matters would be greatly appreciated
          and would significantly enhance my understanding and application of this
          data.</p>

          <p>Thank you for your time and contributions to this exciting field!</p>

          <p>Best regards</p>

          '
        raw: 'Hello InstaDeepAI and community members,


          I am currently delving into the fascinating work presented in the nucleotide-transformer
          paper, particularly the use of data from the 1000 Genomes Project. The research
          is incredibly insightful, and I have a few questions that I hope the community
          or the authors can help clarify:


          1. **Data Interpretation:** The paper mentions the use of 3202 high-coverage
          human genomes, totaling 20.5 trillion nucleotides. From my understanding,
          considering that each human genome is about 3.2 billion nucleotides, and
          accounting for diploid genomes, the calculation, 3.2B * 2 * 3202 =  20,492.8B
          = 20.5T, seems to match up with your figures. Can anyone confirm if this
          interpretation is correct or if there''s something I''m missing?


          2. **Handling Large Data Volumes:** In handling data from the 1000 Genomes
          Project, I''ve encountered the challenge of managing large volumes of data.
          With each genome being about 3GB and considering diploidy, the data for
          3202 samples would be roughly 18.76TB (= 3GB * 2 * 3202) . How did the team
          manage such a vast amount of training data? Did you utilize the entire dataset
          directly for training, or were there specific preprocessing or reduction
          techniques applied?


          3. **FASTA File Generation Issues:** I''ve been facing difficulties in generating
          FASTA files from VCF files, with processes taking an excessive amount of
          time even on high-spec servers. I''ve tried using tools like `bcftools consensus`
          and `gatk FastaAlternateReferenceMaker` but to no avail. I''m curious about
          the tools or methods the team used for this conversion process and would
          appreciate any suggestions or recommendations.


          Any insights or guidance on these matters would be greatly appreciated and
          would significantly enhance my understanding and application of this data.


          Thank you for your time and contributions to this exciting field!


          Best regards'
        updatedAt: '2023-12-21T05:39:25.396Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - JayceCeleste
    id: 6583cb0064188479685670ae
    type: comment
  author: JayceCeleste
  content: 'Hello InstaDeepAI and community members,


    I am currently delving into the fascinating work presented in the nucleotide-transformer
    paper, particularly the use of data from the 1000 Genomes Project. The research
    is incredibly insightful, and I have a few questions that I hope the community
    or the authors can help clarify:


    1. **Data Interpretation:** The paper mentions the use of 3202 high-coverage human
    genomes, totaling 20.5 trillion nucleotides. From my understanding, considering
    that each human genome is about 3.2 billion nucleotides, and accounting for diploid
    genomes, the calculation, 3.2B * 2 * 3202 =  20,492.8B = 20.5T, seems to match
    up with your figures. Can anyone confirm if this interpretation is correct or
    if there''s something I''m missing?


    2. **Handling Large Data Volumes:** In handling data from the 1000 Genomes Project,
    I''ve encountered the challenge of managing large volumes of data. With each genome
    being about 3GB and considering diploidy, the data for 3202 samples would be roughly
    18.76TB (= 3GB * 2 * 3202) . How did the team manage such a vast amount of training
    data? Did you utilize the entire dataset directly for training, or were there
    specific preprocessing or reduction techniques applied?


    3. **FASTA File Generation Issues:** I''ve been facing difficulties in generating
    FASTA files from VCF files, with processes taking an excessive amount of time
    even on high-spec servers. I''ve tried using tools like `bcftools consensus` and
    `gatk FastaAlternateReferenceMaker` but to no avail. I''m curious about the tools
    or methods the team used for this conversion process and would appreciate any
    suggestions or recommendations.


    Any insights or guidance on these matters would be greatly appreciated and would
    significantly enhance my understanding and application of this data.


    Thank you for your time and contributions to this exciting field!


    Best regards'
  created_at: 2023-12-21 05:20:00+00:00
  edited: true
  hidden: false
  id: 6583cb0064188479685670ae
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: InstaDeepAI/nucleotide-transformer-2.5b-1000g
repo_type: model
status: open
target_branch: null
title: Questions about Nucleotide-Transformer and 1000 Genomes Project Data
