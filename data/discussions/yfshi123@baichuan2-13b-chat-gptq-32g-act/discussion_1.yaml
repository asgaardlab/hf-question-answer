!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xuan0126
conflicting_files: null
created_at: 2023-09-08 09:30:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ee94c6f0aafb49959e4f326867b47d8.svg
      fullname: xuan liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xuan0126
      type: user
    createdAt: '2023-09-08T10:30:39.000Z'
    data:
      edited: false
      editors:
      - xuan0126
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8186837434768677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ee94c6f0aafb49959e4f326867b47d8.svg
          fullname: xuan liu
          isHf: false
          isPro: false
          name: xuan0126
          type: user
        html: '<p>I want to convert a baichuan2_base,can I get your help?</p>

          '
        raw: I want to convert a baichuan2_base,can I get your help?
        updatedAt: '2023-09-08T10:30:39.688Z'
      numEdits: 0
      reactions: []
    id: 64faf7cffd212fdfdefc66ed
    type: comment
  author: xuan0126
  content: I want to convert a baichuan2_base,can I get your help?
  created_at: 2023-09-08 09:30:39+00:00
  edited: false
  hidden: false
  id: 64faf7cffd212fdfdefc66ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e94543e550a76e0a57dce834891a6bf3.svg
      fullname: John Smith
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yfshi123
      type: user
    createdAt: '2023-09-08T12:47:58.000Z'
    data:
      edited: false
      editors:
      - yfshi123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4468734860420227
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e94543e550a76e0a57dce834891a6bf3.svg
          fullname: John Smith
          isHf: false
          isPro: false
          name: yfshi123
          type: user
        html: "<p>You can deploy a pod on <a rel=\"nofollow\" href=\"https://www.runpod.io/\"\
          >https://www.runpod.io/</a></p>\n<p>I use this for quantizing the model\
          \ and uploading.</p>\n<p>First install:</p>\n<pre><code>mkdir baichuan2-13b-chat\n\
          cd baichuan2-13b-chat\nwget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/config.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/configuration_baichuan.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/generation_config.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/generation_utils.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/modeling_baichuan.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00001-of-00003.bin\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00002-of-00003.bin\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00003-of-00003.bin\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model.bin.index.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/quantizer.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/special_tokens_map.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenization_baichuan.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenizer.model\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenizer_config.json\n\
          cd ..\n\napt update\napt install python3.10-venv -y\npython -m venv venv\
          \ --system-site-packages\nsource venv/bin/activate\n\npip install auto-gptq\n\
          pip install sentencepiece\npip install protobuf==3.20.0\n\nwget https://github.com/gururise/AlpacaDataCleaned/raw/main/alpaca_data_cleaned.json\n\
          </code></pre>\n<p>And run this:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> sys\nsys.path.insert(<span class=\"\
          hljs-number\">0</span>, <span class=\"hljs-string\">'/workspace/venv/lib/python3.10/site-packages/'</span>)\n\
          \n<span class=\"hljs-keyword\">import</span> transformers\n\n<span class=\"\
          hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\">import</span>\
          \ AutoTokenizer\n<span class=\"hljs-keyword\">from</span> auto_gptq <span\
          \ class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> logging\n\npretrained_model_dir\
          \ = <span class=\"hljs-string\">\"./baichuan2-13b-chat/\"</span>\nquantized_model_dir\
          \ = <span class=\"hljs-string\">\"./baichuan2-13b-chat-gptq-32g-act/\"</span>\n\
          \ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir)\n\nquantize_config\
          \ = BaseQuantizeConfig(\n    bits=<span class=\"hljs-number\">4</span>,\
          \  <span class=\"hljs-comment\"># quantize model to 4-bit</span>\n    group_size=<span\
          \ class=\"hljs-number\">32</span>,  <span class=\"hljs-comment\"># it is\
          \ recommended to set the value to 128</span>\n    desc_act=<span class=\"\
          hljs-literal\">True</span>,  <span class=\"hljs-comment\"># set to False\
          \ can significantly speed up inference but the perplexity may slightly bad\
          \ </span>\n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\n<span class=\"hljs-keyword\">import</span> json\n\
          <span class=\"hljs-keyword\">import</span> random\n<span class=\"hljs-keyword\"\
          >import</span> time\n\n<span class=\"hljs-keyword\">import</span> torch\n\
          <span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\"\
          >import</span> Dataset\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">load_data</span>(<span class=\"hljs-params\"\
          >data_path, tokenizer, n_samples</span>):\n    <span class=\"hljs-keyword\"\
          >with</span> <span class=\"hljs-built_in\">open</span>(data_path, <span\
          \ class=\"hljs-string\">\"r\"</span>, encoding=<span class=\"hljs-string\"\
          >\"utf-8\"</span>) <span class=\"hljs-keyword\">as</span> f:\n        raw_data\
          \ = json.load(f)\n\n    raw_data = random.sample(raw_data, k=<span class=\"\
          hljs-built_in\">min</span>(n_samples, <span class=\"hljs-built_in\">len</span>(raw_data)))\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >dummy_gen</span>():\n        <span class=\"hljs-keyword\">return</span>\
          \ raw_data\n\n    <span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">tokenize</span>(<span class=\"hljs-params\">examples</span>):\n\
          \        instructions = examples[<span class=\"hljs-string\">\"instruction\"\
          </span>]\n        inputs = examples[<span class=\"hljs-string\">\"input\"\
          </span>]\n        outputs = examples[<span class=\"hljs-string\">\"output\"\
          </span>]\n\n        prompts = []\n        texts = []\n        input_ids\
          \ = []\n        attention_mask = []\n        <span class=\"hljs-keyword\"\
          >for</span> istr, inp, opt <span class=\"hljs-keyword\">in</span> <span\
          \ class=\"hljs-built_in\">zip</span>(instructions, inputs, outputs):\n \
          \           <span class=\"hljs-keyword\">if</span> inp:\n              \
          \  prompt = <span class=\"hljs-string\">f\"Instruction:\\n<span class=\"\
          hljs-subst\">{istr}</span>\\nInput:\\n<span class=\"hljs-subst\">{inp}</span>\\\
          nOutput:\\n\"</span>\n                text = prompt + opt\n            <span\
          \ class=\"hljs-keyword\">else</span>:\n                prompt = <span class=\"\
          hljs-string\">f\"Instruction:\\n<span class=\"hljs-subst\">{istr}</span>\\\
          nOutput:\\n\"</span>\n                text = prompt + opt\n            <span\
          \ class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(tokenizer(prompt)[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>]) &gt;= tokenizer.model_max_length:\n\
          \                <span class=\"hljs-keyword\">continue</span>\n\n      \
          \      tokenized_data = tokenizer(text)\n\n            input_ids.append(tokenized_data[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>][: tokenizer.model_max_length])\n\
          \            attention_mask.append(tokenized_data[<span class=\"hljs-string\"\
          >\"attention_mask\"</span>][: tokenizer.model_max_length])\n           \
          \ prompts.append(prompt)\n            texts.append(text)\n\n        <span\
          \ class=\"hljs-keyword\">return</span> {\n            <span class=\"hljs-string\"\
          >\"input_ids\"</span>: input_ids,\n            <span class=\"hljs-string\"\
          >\"attention_mask\"</span>: attention_mask,\n            <span class=\"\
          hljs-string\">\"prompt\"</span>: prompts\n        }\n\n    dataset = Dataset.from_generator(dummy_gen)\n\
          \n    dataset = dataset.<span class=\"hljs-built_in\">map</span>(\n    \
          \    tokenize,\n        batched=<span class=\"hljs-literal\">True</span>,\n\
          \        batch_size=<span class=\"hljs-built_in\">len</span>(dataset),\n\
          \        num_proc=<span class=\"hljs-number\">1</span>,\n        keep_in_memory=<span\
          \ class=\"hljs-literal\">True</span>,\n        load_from_cache_file=<span\
          \ class=\"hljs-literal\">False</span>,\n        remove_columns=[<span class=\"\
          hljs-string\">\"instruction\"</span>, <span class=\"hljs-string\">\"input\"\
          </span>]\n    )\n\n    dataset = dataset.to_list()\n\n    <span class=\"\
          hljs-keyword\">for</span> sample <span class=\"hljs-keyword\">in</span>\
          \ dataset:\n        sample[<span class=\"hljs-string\">\"input_ids\"</span>]\
          \ = torch.LongTensor(sample[<span class=\"hljs-string\">\"input_ids\"</span>])\n\
          \        sample[<span class=\"hljs-string\">\"attention_mask\"</span>] =\
          \ torch.LongTensor(sample[<span class=\"hljs-string\">\"attention_mask\"\
          </span>])\n\n    <span class=\"hljs-keyword\">return</span> dataset\n  \
          \  \nexamples = load_data(<span class=\"hljs-string\">\"alpaca_data_cleaned.json\"\
          </span>, tokenizer, <span class=\"hljs-number\">128</span>)\n\nmodel.quantize(examples,\
          \ batch_size=<span class=\"hljs-number\">1</span>)\n\nmodel.save_quantized(quantized_model_dir)\n\
          \nmodel.push_to_hub(repo_id=<span class=\"hljs-string\">'baichuan2-13b-chat-gptq-32g-act'</span>,\
          \ use_safetensors=<span class=\"hljs-literal\">True</span>, private=<span\
          \ class=\"hljs-literal\">False</span>, token=<span class=\"hljs-string\"\
          >''</span>)\n</code></pre>\n"
        raw: "You can deploy a pod on https://www.runpod.io/\n\nI use this for quantizing\
          \ the model and uploading.\n\nFirst install:\n\n```\nmkdir baichuan2-13b-chat\n\
          cd baichuan2-13b-chat\nwget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/config.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/configuration_baichuan.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/generation_config.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/generation_utils.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/modeling_baichuan.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00001-of-00003.bin\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00002-of-00003.bin\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00003-of-00003.bin\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model.bin.index.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/quantizer.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/special_tokens_map.json\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenization_baichuan.py\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenizer.model\n\
          wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenizer_config.json\n\
          cd ..\n\napt update\napt install python3.10-venv -y\npython -m venv venv\
          \ --system-site-packages\nsource venv/bin/activate\n\npip install auto-gptq\n\
          pip install sentencepiece\npip install protobuf==3.20.0\n\nwget https://github.com/gururise/AlpacaDataCleaned/raw/main/alpaca_data_cleaned.json\n\
          ```\n\nAnd run this:\n\n```python\nimport sys\nsys.path.insert(0, '/workspace/venv/lib/python3.10/site-packages/')\n\
          \nimport transformers\n\nfrom transformers import AutoTokenizer\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport logging\n\npretrained_model_dir\
          \ = \"./baichuan2-13b-chat/\"\nquantized_model_dir = \"./baichuan2-13b-chat-gptq-32g-act/\"\
          \n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir)\n\n\
          quantize_config = BaseQuantizeConfig(\n    bits=4,  # quantize model to\
          \ 4-bit\n    group_size=32,  # it is recommended to set the value to 128\n\
          \    desc_act=True,  # set to False can significantly speed up inference\
          \ but the perplexity may slightly bad \n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir,\
          \ quantize_config)\n\nimport json\nimport random\nimport time\n\nimport\
          \ torch\nfrom datasets import Dataset\n\ndef load_data(data_path, tokenizer,\
          \ n_samples):\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n\
          \        raw_data = json.load(f)\n\n    raw_data = random.sample(raw_data,\
          \ k=min(n_samples, len(raw_data)))\n\n    def dummy_gen():\n        return\
          \ raw_data\n\n    def tokenize(examples):\n        instructions = examples[\"\
          instruction\"]\n        inputs = examples[\"input\"]\n        outputs =\
          \ examples[\"output\"]\n\n        prompts = []\n        texts = []\n   \
          \     input_ids = []\n        attention_mask = []\n        for istr, inp,\
          \ opt in zip(instructions, inputs, outputs):\n            if inp:\n    \
          \            prompt = f\"Instruction:\\n{istr}\\nInput:\\n{inp}\\nOutput:\\\
          n\"\n                text = prompt + opt\n            else:\n          \
          \      prompt = f\"Instruction:\\n{istr}\\nOutput:\\n\"\n              \
          \  text = prompt + opt\n            if len(tokenizer(prompt)[\"input_ids\"\
          ]) >= tokenizer.model_max_length:\n                continue\n\n        \
          \    tokenized_data = tokenizer(text)\n\n            input_ids.append(tokenized_data[\"\
          input_ids\"][: tokenizer.model_max_length])\n            attention_mask.append(tokenized_data[\"\
          attention_mask\"][: tokenizer.model_max_length])\n            prompts.append(prompt)\n\
          \            texts.append(text)\n\n        return {\n            \"input_ids\"\
          : input_ids,\n            \"attention_mask\": attention_mask,\n        \
          \    \"prompt\": prompts\n        }\n\n    dataset = Dataset.from_generator(dummy_gen)\n\
          \n    dataset = dataset.map(\n        tokenize,\n        batched=True,\n\
          \        batch_size=len(dataset),\n        num_proc=1,\n        keep_in_memory=True,\n\
          \        load_from_cache_file=False,\n        remove_columns=[\"instruction\"\
          , \"input\"]\n    )\n\n    dataset = dataset.to_list()\n\n    for sample\
          \ in dataset:\n        sample[\"input_ids\"] = torch.LongTensor(sample[\"\
          input_ids\"])\n        sample[\"attention_mask\"] = torch.LongTensor(sample[\"\
          attention_mask\"])\n\n    return dataset\n    \nexamples = load_data(\"\
          alpaca_data_cleaned.json\", tokenizer, 128)\n\nmodel.quantize(examples,\
          \ batch_size=1)\n\nmodel.save_quantized(quantized_model_dir)\n\nmodel.push_to_hub(repo_id='baichuan2-13b-chat-gptq-32g-act',\
          \ use_safetensors=True, private=False, token='')\n```"
        updatedAt: '2023-09-08T12:47:58.251Z'
      numEdits: 0
      reactions: []
    id: 64fb17fe9ecd05d5bf38327d
    type: comment
  author: yfshi123
  content: "You can deploy a pod on https://www.runpod.io/\n\nI use this for quantizing\
    \ the model and uploading.\n\nFirst install:\n\n```\nmkdir baichuan2-13b-chat\n\
    cd baichuan2-13b-chat\nwget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/config.json\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/configuration_baichuan.py\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/generation_config.json\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/generation_utils.py\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/modeling_baichuan.py\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00001-of-00003.bin\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00002-of-00003.bin\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model-00003-of-00003.bin\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/pytorch_model.bin.index.json\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/quantizer.py\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/special_tokens_map.json\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenization_baichuan.py\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenizer.model\n\
    wget https://huggingface.co/baichuan-inc/Baichuan2-13B-Chat/resolve/main/tokenizer_config.json\n\
    cd ..\n\napt update\napt install python3.10-venv -y\npython -m venv venv --system-site-packages\n\
    source venv/bin/activate\n\npip install auto-gptq\npip install sentencepiece\n\
    pip install protobuf==3.20.0\n\nwget https://github.com/gururise/AlpacaDataCleaned/raw/main/alpaca_data_cleaned.json\n\
    ```\n\nAnd run this:\n\n```python\nimport sys\nsys.path.insert(0, '/workspace/venv/lib/python3.10/site-packages/')\n\
    \nimport transformers\n\nfrom transformers import AutoTokenizer\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport logging\n\npretrained_model_dir\
    \ = \"./baichuan2-13b-chat/\"\nquantized_model_dir = \"./baichuan2-13b-chat-gptq-32g-act/\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir)\n\nquantize_config\
    \ = BaseQuantizeConfig(\n    bits=4,  # quantize model to 4-bit\n    group_size=32,\
    \  # it is recommended to set the value to 128\n    desc_act=True,  # set to False\
    \ can significantly speed up inference but the perplexity may slightly bad \n\
    )\n\nmodel = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)\n\
    \nimport json\nimport random\nimport time\n\nimport torch\nfrom datasets import\
    \ Dataset\n\ndef load_data(data_path, tokenizer, n_samples):\n    with open(data_path,\
    \ \"r\", encoding=\"utf-8\") as f:\n        raw_data = json.load(f)\n\n    raw_data\
    \ = random.sample(raw_data, k=min(n_samples, len(raw_data)))\n\n    def dummy_gen():\n\
    \        return raw_data\n\n    def tokenize(examples):\n        instructions\
    \ = examples[\"instruction\"]\n        inputs = examples[\"input\"]\n        outputs\
    \ = examples[\"output\"]\n\n        prompts = []\n        texts = []\n       \
    \ input_ids = []\n        attention_mask = []\n        for istr, inp, opt in zip(instructions,\
    \ inputs, outputs):\n            if inp:\n                prompt = f\"Instruction:\\\
    n{istr}\\nInput:\\n{inp}\\nOutput:\\n\"\n                text = prompt + opt\n\
    \            else:\n                prompt = f\"Instruction:\\n{istr}\\nOutput:\\\
    n\"\n                text = prompt + opt\n            if len(tokenizer(prompt)[\"\
    input_ids\"]) >= tokenizer.model_max_length:\n                continue\n\n   \
    \         tokenized_data = tokenizer(text)\n\n            input_ids.append(tokenized_data[\"\
    input_ids\"][: tokenizer.model_max_length])\n            attention_mask.append(tokenized_data[\"\
    attention_mask\"][: tokenizer.model_max_length])\n            prompts.append(prompt)\n\
    \            texts.append(text)\n\n        return {\n            \"input_ids\"\
    : input_ids,\n            \"attention_mask\": attention_mask,\n            \"\
    prompt\": prompts\n        }\n\n    dataset = Dataset.from_generator(dummy_gen)\n\
    \n    dataset = dataset.map(\n        tokenize,\n        batched=True,\n     \
    \   batch_size=len(dataset),\n        num_proc=1,\n        keep_in_memory=True,\n\
    \        load_from_cache_file=False,\n        remove_columns=[\"instruction\"\
    , \"input\"]\n    )\n\n    dataset = dataset.to_list()\n\n    for sample in dataset:\n\
    \        sample[\"input_ids\"] = torch.LongTensor(sample[\"input_ids\"])\n   \
    \     sample[\"attention_mask\"] = torch.LongTensor(sample[\"attention_mask\"\
    ])\n\n    return dataset\n    \nexamples = load_data(\"alpaca_data_cleaned.json\"\
    , tokenizer, 128)\n\nmodel.quantize(examples, batch_size=1)\n\nmodel.save_quantized(quantized_model_dir)\n\
    \nmodel.push_to_hub(repo_id='baichuan2-13b-chat-gptq-32g-act', use_safetensors=True,\
    \ private=False, token='')\n```"
  created_at: 2023-09-08 11:47:58+00:00
  edited: false
  hidden: false
  id: 64fb17fe9ecd05d5bf38327d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yfshi123/baichuan2-13b-chat-gptq-32g-act
repo_type: model
status: open
target_branch: null
title: How does that translate?
