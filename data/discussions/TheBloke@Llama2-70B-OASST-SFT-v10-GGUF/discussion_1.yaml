!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rambocoder
conflicting_files: null
created_at: 2023-09-21 02:43:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
      fullname: Alex K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rambocoder
      type: user
    createdAt: '2023-09-21T03:43:58.000Z'
    data:
      edited: false
      editors:
      - rambocoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3727402091026306
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
          fullname: Alex K
          isHf: false
          isPro: false
          name: rambocoder
          type: user
        html: '<p>When running <code>./main -ngl 32  -gqa 8 -m /tmp/llama2-70b-oasst-sft-v10.Q8_0.gguf
          --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "&lt;|im_start|&gt;system\n{system_message}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{prompt}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant"</code></p>

          <p>error occurs:<br>error loading model: create_tensor: tensor ''token_embd.weight''
          has wrong shape; expected  8192, 32007, got  8192, 32128,     1,     1<br>llama_load_model_from_file:
          failed to load model<br>llama_init_from_gpt_params: error: failed to load
          model ''/tmp/llama2-70b-oasst-sft-v10.Q8_0.gguf''<br>main: error: unable
          to load model</p>

          <p>Any ideas how to get around this error?</p>

          '
        raw: "When running `./main -ngl 32  -gqa 8 -m /tmp/llama2-70b-oasst-sft-v10.Q8_0.gguf\
          \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\\\
          n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\
          `\r\n\r\nerror occurs:\r\nerror loading model: create_tensor: tensor 'token_embd.weight'\
          \ has wrong shape; expected  8192, 32007, got  8192, 32128,     1,     1\r\
          \nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
          \ error: failed to load model '/tmp/llama2-70b-oasst-sft-v10.Q8_0.gguf'\r\
          \nmain: error: unable to load model\r\n\r\nAny ideas how to get around this\
          \ error?"
        updatedAt: '2023-09-21T03:43:58.758Z'
      numEdits: 0
      reactions: []
    id: 650bbbfebe6db1ec21377105
    type: comment
  author: rambocoder
  content: "When running `./main -ngl 32  -gqa 8 -m /tmp/llama2-70b-oasst-sft-v10.Q8_0.gguf\
    \ --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|im_start|>system\\\
    n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\"\
    `\r\n\r\nerror occurs:\r\nerror loading model: create_tensor: tensor 'token_embd.weight'\
    \ has wrong shape; expected  8192, 32007, got  8192, 32128,     1,     1\r\nllama_load_model_from_file:\
    \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load model\
    \ '/tmp/llama2-70b-oasst-sft-v10.Q8_0.gguf'\r\nmain: error: unable to load model\r\
    \n\r\nAny ideas how to get around this error?"
  created_at: 2023-09-21 02:43:58+00:00
  edited: false
  hidden: false
  id: 650bbbfebe6db1ec21377105
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
      fullname: Alex K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rambocoder
      type: user
    createdAt: '2023-09-21T04:10:04.000Z'
    data:
      edited: false
      editors:
      - rambocoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3262855112552643
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
          fullname: Alex K
          isHf: false
          isPro: false
          name: rambocoder
          type: user
        html: '<p>The GGML version has these attributes:<br>llama_model_load_internal:
          n_vocab    = 32007<br>llama_model_load_internal: n_embd     = 8192</p>

          '
        raw: 'The GGML version has these attributes:

          llama_model_load_internal: n_vocab    = 32007

          llama_model_load_internal: n_embd     = 8192'
        updatedAt: '2023-09-21T04:10:04.248Z'
      numEdits: 0
      reactions: []
    id: 650bc21c2feef65026c868a7
    type: comment
  author: rambocoder
  content: 'The GGML version has these attributes:

    llama_model_load_internal: n_vocab    = 32007

    llama_model_load_internal: n_embd     = 8192'
  created_at: 2023-09-21 03:10:04+00:00
  edited: false
  hidden: false
  id: 650bc21c2feef65026c868a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-21T07:13:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9751965403556824
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Ah I know what this is. Andreas did something to the model to pad
          it to 128 tokens, but the result is the model is actually broken.  I had
          the same issue when I made the AWQ the other day - I had to go back to an
          earlier commit on the source model, before he did that padding.  See here:
          <a href="https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10/discussions/5">https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10/discussions/5</a></p>

          <p>I''ll have to re-make the GGUF as well I guess</p>

          '
        raw: 'Ah I know what this is. Andreas did something to the model to pad it
          to 128 tokens, but the result is the model is actually broken.  I had the
          same issue when I made the AWQ the other day - I had to go back to an earlier
          commit on the source model, before he did that padding.  See here: https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10/discussions/5


          I''ll have to re-make the GGUF as well I guess'
        updatedAt: '2023-09-21T07:13:42.914Z'
      numEdits: 0
      reactions: []
    id: 650bed265510464e85b11676
    type: comment
  author: TheBloke
  content: 'Ah I know what this is. Andreas did something to the model to pad it to
    128 tokens, but the result is the model is actually broken.  I had the same issue
    when I made the AWQ the other day - I had to go back to an earlier commit on the
    source model, before he did that padding.  See here: https://huggingface.co/OpenAssistant/llama2-70b-oasst-sft-v10/discussions/5


    I''ll have to re-make the GGUF as well I guess'
  created_at: 2023-09-21 06:13:42+00:00
  edited: false
  hidden: false
  id: 650bed265510464e85b11676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
      fullname: Alex K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rambocoder
      type: user
    createdAt: '2023-09-21T15:11:34.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
          fullname: Alex K
          isHf: false
          isPro: false
          name: rambocoder
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-21T15:56:12.643Z'
      numEdits: 0
      reactions: []
    id: 650c5d2644ab290c357a4b97
    type: comment
  author: rambocoder
  content: This comment has been hidden
  created_at: 2023-09-21 14:11:34+00:00
  edited: true
  hidden: true
  id: 650c5d2644ab290c357a4b97
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08a922bc2056520ded010d3f1bc4c643.svg
      fullname: Anja Kuzev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anjakuzev
      type: user
    createdAt: '2023-09-22T16:51:46.000Z'
    data:
      edited: true
      editors:
      - anjakuzev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8060243725776672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08a922bc2056520ded010d3f1bc4c643.svg
          fullname: Anja Kuzev
          isHf: false
          isPro: false
          name: anjakuzev
          type: user
        html: '<p>How can i convert a finetuned model with LoRA to GGUF?</p>

          <p>when i try with the convert.py i get this error:<br>Loading model file
          lora/adapter_model.bin<br>Traceback (most recent call last):<br>  File "convert.py",
          line 1208, in <br>    main()<br>  File "convert.py", line 1157, in main<br>    params
          = Params.load(model_plus)<br>  File "convert.py", line 292, in load<br>    params
          = Params.guessed(model_plus.model)<br>  File "convert.py", line 166, in
          guessed<br>    n_vocab, n_embd = model["model.embed_tokens.weight"].shape
          if "model.embed_tokens.weight" in model else model["tok_embeddings.weight"].shape<br>KeyError:
          ''tok_embeddings.weight</p>

          '
        raw: "How can i convert a finetuned model with LoRA to GGUF?\n\nwhen i try\
          \ with the convert.py i get this error:\nLoading model file lora/adapter_model.bin\n\
          Traceback (most recent call last):\n  File \"convert.py\", line 1208, in\
          \ <module>\n    main()\n  File \"convert.py\", line 1157, in main\n    params\
          \ = Params.load(model_plus)\n  File \"convert.py\", line 292, in load\n\
          \    params = Params.guessed(model_plus.model)\n  File \"convert.py\", line\
          \ 166, in guessed\n    n_vocab, n_embd = model[\"model.embed_tokens.weight\"\
          ].shape if \"model.embed_tokens.weight\" in model else model[\"tok_embeddings.weight\"\
          ].shape\nKeyError: 'tok_embeddings.weight"
        updatedAt: '2023-09-22T17:11:10.519Z'
      numEdits: 1
      reactions: []
    id: 650dc6228607fa78b0ffffb9
    type: comment
  author: anjakuzev
  content: "How can i convert a finetuned model with LoRA to GGUF?\n\nwhen i try with\
    \ the convert.py i get this error:\nLoading model file lora/adapter_model.bin\n\
    Traceback (most recent call last):\n  File \"convert.py\", line 1208, in <module>\n\
    \    main()\n  File \"convert.py\", line 1157, in main\n    params = Params.load(model_plus)\n\
    \  File \"convert.py\", line 292, in load\n    params = Params.guessed(model_plus.model)\n\
    \  File \"convert.py\", line 166, in guessed\n    n_vocab, n_embd = model[\"model.embed_tokens.weight\"\
    ].shape if \"model.embed_tokens.weight\" in model else model[\"tok_embeddings.weight\"\
    ].shape\nKeyError: 'tok_embeddings.weight"
  created_at: 2023-09-22 15:51:46+00:00
  edited: true
  hidden: false
  id: 650dc6228607fa78b0ffffb9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
      fullname: Viorel Mirea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmirea
      type: user
    createdAt: '2023-10-25T13:45:17.000Z'
    data:
      edited: true
      editors:
      - vmirea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6094835996627808
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
          fullname: Viorel Mirea
          isHf: false
          isPro: false
          name: vmirea
          type: user
        html: '<blockquote>

          <p>How can i convert a finetuned model with LoRA to GGUF?</p>

          <p>when i try with the convert.py i get this error:<br>Loading model file
          lora/adapter_model.bin<br>Traceback (most recent call last):<br>  File "convert.py",
          line 1208, in <br>    main()<br>  File "convert.py", line 1157, in main<br>    params
          = Params.load(model_plus)<br>  File "convert.py", line 292, in load<br>    params
          = Params.guessed(model_plus.model)<br>  File "convert.py", line 166, in
          guessed<br>    n_vocab, n_embd = model["model.embed_tokens.weight"].shape
          if "model.embed_tokens.weight" in model else model["tok_embeddings.weight"].shape<br>KeyError:
          ''tok_embeddings.weight</p>

          </blockquote>

          <p>Have you succeeded in fixing this? I get the same error on convert.py</p>

          '
        raw: "> How can i convert a finetuned model with LoRA to GGUF?\n> \n> when\
          \ i try with the convert.py i get this error:\n> Loading model file lora/adapter_model.bin\n\
          > Traceback (most recent call last):\n>   File \"convert.py\", line 1208,\
          \ in <module>\n>     main()\n>   File \"convert.py\", line 1157, in main\n\
          >     params = Params.load(model_plus)\n>   File \"convert.py\", line 292,\
          \ in load\n>     params = Params.guessed(model_plus.model)\n>   File \"\
          convert.py\", line 166, in guessed\n>     n_vocab, n_embd = model[\"model.embed_tokens.weight\"\
          ].shape if \"model.embed_tokens.weight\" in model else model[\"tok_embeddings.weight\"\
          ].shape\n> KeyError: 'tok_embeddings.weight\n\nHave you succeeded in fixing\
          \ this? I get the same error on convert.py"
        updatedAt: '2023-10-25T13:45:46.351Z'
      numEdits: 1
      reactions: []
    id: 65391beda693d907fab69469
    type: comment
  author: vmirea
  content: "> How can i convert a finetuned model with LoRA to GGUF?\n> \n> when i\
    \ try with the convert.py i get this error:\n> Loading model file lora/adapter_model.bin\n\
    > Traceback (most recent call last):\n>   File \"convert.py\", line 1208, in <module>\n\
    >     main()\n>   File \"convert.py\", line 1157, in main\n>     params = Params.load(model_plus)\n\
    >   File \"convert.py\", line 292, in load\n>     params = Params.guessed(model_plus.model)\n\
    >   File \"convert.py\", line 166, in guessed\n>     n_vocab, n_embd = model[\"\
    model.embed_tokens.weight\"].shape if \"model.embed_tokens.weight\" in model else\
    \ model[\"tok_embeddings.weight\"].shape\n> KeyError: 'tok_embeddings.weight\n\
    \nHave you succeeded in fixing this? I get the same error on convert.py"
  created_at: 2023-10-25 12:45:17+00:00
  edited: true
  hidden: false
  id: 65391beda693d907fab69469
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/327c1c38c5a67cf2fec718128bc151bc.svg
      fullname: News Letter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: newsletter
      type: user
    createdAt: '2023-11-13T10:43:11.000Z'
    data:
      edited: false
      editors:
      - newsletter
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7116390466690063
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/327c1c38c5a67cf2fec718128bc151bc.svg
          fullname: News Letter
          isHf: false
          isPro: false
          name: newsletter
          type: user
        html: '<p>It is still not usable, apparently: Using llama2-70b-oasst-sft-v10.Q5_K_M.gguf
          in oobabooga/text-generation-webui fails with the following error:<br><code>error
          loading model: create_tensor: tensor ''token_embd.weight'' has wrong shape;
          expected  8192, 32007, got  8192, 32128,     1,     1</code></p>

          <p>Is there any way to fix this?</p>

          '
        raw: 'It is still not usable, apparently: Using llama2-70b-oasst-sft-v10.Q5_K_M.gguf
          in oobabooga/text-generation-webui fails with the following error:

          ```error loading model: create_tensor: tensor ''token_embd.weight'' has
          wrong shape; expected  8192, 32007, got  8192, 32128,     1,     1```


          Is there any way to fix this?'
        updatedAt: '2023-11-13T10:43:11.634Z'
      numEdits: 0
      reactions: []
    id: 6551fdbfe5f380aca832d70b
    type: comment
  author: newsletter
  content: 'It is still not usable, apparently: Using llama2-70b-oasst-sft-v10.Q5_K_M.gguf
    in oobabooga/text-generation-webui fails with the following error:

    ```error loading model: create_tensor: tensor ''token_embd.weight'' has wrong
    shape; expected  8192, 32007, got  8192, 32128,     1,     1```


    Is there any way to fix this?'
  created_at: 2023-11-13 10:43:11+00:00
  edited: false
  hidden: false
  id: 6551fdbfe5f380aca832d70b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama2-70B-OASST-SFT-v10-GGUF
repo_type: model
status: open
target_branch: null
title: tensor 'token_embd.weight' has wrong shape
