!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MohamedNumair
conflicting_files: null
created_at: 2023-04-04 12:08:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ba62fedccef4949deb1b4c42a99f1b9.svg
      fullname: Mohamed Numair
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MohamedNumair
      type: user
    createdAt: '2023-04-04T13:08:32.000Z'
    data:
      edited: false
      editors:
      - MohamedNumair
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ba62fedccef4949deb1b4c42a99f1b9.svg
          fullname: Mohamed Numair
          isHf: false
          isPro: false
          name: MohamedNumair
          type: user
        html: "<p>Hi there I am just trying to experiment with question answering\
          \ however when I execute the following code:</p>\n<pre><code>from transformers\
          \ import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name\
          \ = \"deepset/xlm-roberta-large-squad2\"\n\n# a) Get predictions\nnlp =\
          \ pipeline('question-answering', model=model_name, tokenizer=model_name)\n\
          </code></pre>\n<p>I get the following error message, any ideas why?</p>\n\
          <pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[49], line 8\n      5 model_name = \"deepset/xlm-roberta-large-squad2\"\
          \n      7 # a) Get predictions\n----&gt; 8 nlp = pipeline('question-answering',\
          \ model=model_name, tokenizer=model_name)\n\nFile c:\\Users\\mnumai200\\\
          AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\\
          pipelines\\__init__.py:727, in pipeline(task, model, config, tokenizer,\
          \ feature_extractor, framework, revision, use_fast, use_auth_token, device,\
          \ device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class,\
          \ **kwargs)\n    723 # Infer the framework from the model\n    724 # Forced\
          \ if framework already defined, inferred if it's None\n    725 # Will load\
          \ the correct model if possible\n    726 model_classes = {\"tf\": targeted_task[\"\
          tf\"], \"pt\": targeted_task[\"pt\"]}\n--&gt; 727 framework, model = infer_framework_load_model(\n\
          \    728     model,\n    729     model_classes=model_classes,\n    730 \
          \    config=config,\n    731     framework=framework,\n    732     task=task,\n\
          \    733     **hub_kwargs,\n    734     **model_kwargs,\n    735 )\n   \
          \ 737 model_config = model.config\n    738 hub_kwargs[\"_commit_hash\"]\
          \ = model.config._commit_hash\n\nFile c:\\Users\\mnumai200\\AppData\\Local\\\
          Programs\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\\
          base.py:266, in infer_framework_load_model(model, config, model_classes,\
          \ task, framework, **model_kwargs)\n    263             continue\n    265\
          \     if isinstance(model, str):\n--&gt; 266         raise ValueError(f\"\
          Could not load model {model} with any of the following classes: {class_tuple}.\"\
          )\n    268 framework = \"tf\" if model.__class__.__name__.startswith(\"\
          TF\") else \"pt\"\n    269 return framework, model\n\nValueError: Could\
          \ not load model deepset/xlm-roberta-large-squad2 with any of the following\
          \ classes: (, ).\n</code></pre>\n"
        raw: "Hi there I am just trying to experiment with question answering however\
          \ when I execute the following code:\r\n\r\n```\r\nfrom transformers import\
          \ AutoModelForQuestionAnswering, AutoTokenizer, pipeline\r\n\r\nmodel_name\
          \ = \"deepset/xlm-roberta-large-squad2\"\r\n\r\n# a) Get predictions\r\n\
          nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\r\
          \n```\r\n\r\nI get the following error message, any ideas why?\r\n\r\n```\r\
          \n---------------------------------------------------------------------------\r\
          \nValueError                                Traceback (most recent call\
          \ last)\r\nCell In[49], line 8\r\n      5 model_name = \"deepset/xlm-roberta-large-squad2\"\
          \r\n      7 # a) Get predictions\r\n----> 8 nlp = pipeline('question-answering',\
          \ model=model_name, tokenizer=model_name)\r\n\r\nFile c:\\Users\\mnumai200\\\
          AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\\
          pipelines\\__init__.py:727, in pipeline(task, model, config, tokenizer,\
          \ feature_extractor, framework, revision, use_fast, use_auth_token, device,\
          \ device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class,\
          \ **kwargs)\r\n    723 # Infer the framework from the model\r\n    724 #\
          \ Forced if framework already defined, inferred if it's None\r\n    725\
          \ # Will load the correct model if possible\r\n    726 model_classes = {\"\
          tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\r\n--> 727 framework,\
          \ model = infer_framework_load_model(\r\n    728     model,\r\n    729 \
          \    model_classes=model_classes,\r\n    730     config=config,\r\n    731\
          \     framework=framework,\r\n    732     task=task,\r\n    733     **hub_kwargs,\r\
          \n    734     **model_kwargs,\r\n    735 )\r\n    737 model_config = model.config\r\
          \n    738 hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\r\n\r\
          \nFile c:\\Users\\mnumai200\\AppData\\Local\\Programs\\Python\\Python38\\\
          lib\\site-packages\\transformers\\pipelines\\base.py:266, in infer_framework_load_model(model,\
          \ config, model_classes, task, framework, **model_kwargs)\r\n    263   \
          \          continue\r\n    265     if isinstance(model, str):\r\n--> 266\
          \         raise ValueError(f\"Could not load model {model} with any of the\
          \ following classes: {class_tuple}.\")\r\n    268 framework = \"tf\" if\
          \ model.__class__.__name__.startswith(\"TF\") else \"pt\"\r\n    269 return\
          \ framework, model\r\n\r\nValueError: Could not load model deepset/xlm-roberta-large-squad2\
          \ with any of the following classes: (, ).\r\n```"
        updatedAt: '2023-04-04T13:08:32.673Z'
      numEdits: 0
      reactions: []
    id: 642c2150fa6e17ba1e29535f
    type: comment
  author: MohamedNumair
  content: "Hi there I am just trying to experiment with question answering however\
    \ when I execute the following code:\r\n\r\n```\r\nfrom transformers import AutoModelForQuestionAnswering,\
    \ AutoTokenizer, pipeline\r\n\r\nmodel_name = \"deepset/xlm-roberta-large-squad2\"\
    \r\n\r\n# a) Get predictions\r\nnlp = pipeline('question-answering', model=model_name,\
    \ tokenizer=model_name)\r\n```\r\n\r\nI get the following error message, any ideas\
    \ why?\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
    \nValueError                                Traceback (most recent call last)\r\
    \nCell In[49], line 8\r\n      5 model_name = \"deepset/xlm-roberta-large-squad2\"\
    \r\n      7 # a) Get predictions\r\n----> 8 nlp = pipeline('question-answering',\
    \ model=model_name, tokenizer=model_name)\r\n\r\nFile c:\\Users\\mnumai200\\AppData\\\
    Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\pipelines\\\
    __init__.py:727, in pipeline(task, model, config, tokenizer, feature_extractor,\
    \ framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype,\
    \ trust_remote_code, model_kwargs, pipeline_class, **kwargs)\r\n    723 # Infer\
    \ the framework from the model\r\n    724 # Forced if framework already defined,\
    \ inferred if it's None\r\n    725 # Will load the correct model if possible\r\
    \n    726 model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"\
    pt\"]}\r\n--> 727 framework, model = infer_framework_load_model(\r\n    728  \
    \   model,\r\n    729     model_classes=model_classes,\r\n    730     config=config,\r\
    \n    731     framework=framework,\r\n    732     task=task,\r\n    733     **hub_kwargs,\r\
    \n    734     **model_kwargs,\r\n    735 )\r\n    737 model_config = model.config\r\
    \n    738 hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\r\n\r\nFile\
    \ c:\\Users\\mnumai200\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\\
    transformers\\pipelines\\base.py:266, in infer_framework_load_model(model, config,\
    \ model_classes, task, framework, **model_kwargs)\r\n    263             continue\r\
    \n    265     if isinstance(model, str):\r\n--> 266         raise ValueError(f\"\
    Could not load model {model} with any of the following classes: {class_tuple}.\"\
    )\r\n    268 framework = \"tf\" if model.__class__.__name__.startswith(\"TF\"\
    ) else \"pt\"\r\n    269 return framework, model\r\n\r\nValueError: Could not\
    \ load model deepset/xlm-roberta-large-squad2 with any of the following classes:\
    \ (, ).\r\n```"
  created_at: 2023-04-04 12:08:32+00:00
  edited: false
  hidden: false
  id: 642c2150fa6e17ba1e29535f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: deepset/xlm-roberta-large-squad2
repo_type: model
status: open
target_branch: null
title: I get an error when using the model with transformers
