!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kelheor
conflicting_files: null
created_at: 2023-05-27 08:45:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
      fullname: Kelheor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kelheor
      type: user
    createdAt: '2023-05-27T09:45:26.000Z'
    data:
      edited: false
      editors:
      - Kelheor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/77821edef9e95d38793da3c8ab5eae58.svg
          fullname: Kelheor
          isHf: false
          isPro: false
          name: Kelheor
          type: user
        html: '<p>Is it possible in future to post version with Groupsize = None?
          So it will be possible to fit full context on consumer grade GPU, like 4090
          24Gb? Version with 128g gives out of memory error when context almost full.</p>

          <p>Example command from other model to visualise what I mean:<br>python
          llama.py /workspace/models/ehartford_WizardLM-30B-Uncensored  wikitext2
          --wbits 4 --true-sequential --act-order  --save_safetensors /workspace/eric-30B/gptq/WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors</p>

          '
        raw: "Is it possible in future to post version with Groupsize = None? So it\
          \ will be possible to fit full context on consumer grade GPU, like 4090\
          \ 24Gb? Version with 128g gives out of memory error when context almost\
          \ full.\r\n\r\nExample command from other model to visualise what I mean:\r\
          \npython llama.py /workspace/models/ehartford_WizardLM-30B-Uncensored  wikitext2\
          \ --wbits 4 --true-sequential --act-order  --save_safetensors /workspace/eric-30B/gptq/WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors"
        updatedAt: '2023-05-27T09:45:26.431Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ADHDINO
    id: 6471d136a2b0a376b8b68729
    type: comment
  author: Kelheor
  content: "Is it possible in future to post version with Groupsize = None? So it\
    \ will be possible to fit full context on consumer grade GPU, like 4090 24Gb?\
    \ Version with 128g gives out of memory error when context almost full.\r\n\r\n\
    Example command from other model to visualise what I mean:\r\npython llama.py\
    \ /workspace/models/ehartford_WizardLM-30B-Uncensored  wikitext2 --wbits 4 --true-sequential\
    \ --act-order  --save_safetensors /workspace/eric-30B/gptq/WizardLM-30B-Uncensored-GPTQ-4bit.act-order.safetensors"
  created_at: 2023-05-27 08:45:26+00:00
  edited: false
  hidden: false
  id: 6471d136a2b0a376b8b68729
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
      fullname: reeducator
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: reeducator
      type: user
    createdAt: '2023-05-27T11:35:16.000Z'
    data:
      edited: false
      editors:
      - reeducator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/37bfa79033dfa10085b474ae84598081.svg
          fullname: reeducator
          isHf: false
          isPro: false
          name: reeducator
          type: user
        html: '<p>Yes, I can do that. There should be an update to the model itself
          soon, so I will run the conversions for that then.</p>

          '
        raw: Yes, I can do that. There should be an update to the model itself soon,
          so I will run the conversions for that then.
        updatedAt: '2023-05-27T11:35:16.710Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - ADHDINO
        - Cytho
        - AliCat2
    id: 6471eaf45afd6a69657d5eee
    type: comment
  author: reeducator
  content: Yes, I can do that. There should be an update to the model itself soon,
    so I will run the conversions for that then.
  created_at: 2023-05-27 10:35:16+00:00
  edited: false
  hidden: false
  id: 6471eaf45afd6a69657d5eee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03e141d9f57876e34a1e45ce261da7e.svg
      fullname: Ali
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AliCat2
      type: user
    createdAt: '2023-07-05T14:51:00.000Z'
    data:
      edited: false
      editors:
      - AliCat2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957259476184845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03e141d9f57876e34a1e45ce261da7e.svg
          fullname: Ali
          isHf: false
          isPro: false
          name: AliCat2
          type: user
        html: '<blockquote>

          <p>Yes, I can do that. There should be an update to the model itself soon,
          so I will run the conversions for that then.</p>

          </blockquote>

          <p>Thank you! That would also allow us to use 4096 context size with NTK
          w/ 24GB VRAM, so it''d be quite nice!</p>

          '
        raw: '> Yes, I can do that. There should be an update to the model itself
          soon, so I will run the conversions for that then.


          Thank you! That would also allow us to use 4096 context size with NTK w/
          24GB VRAM, so it''d be quite nice!

          '
        updatedAt: '2023-07-05T14:51:00.552Z'
      numEdits: 0
      reactions: []
    id: 64a58354c568c6ebe0f41f0f
    type: comment
  author: AliCat2
  content: '> Yes, I can do that. There should be an update to the model itself soon,
    so I will run the conversions for that then.


    Thank you! That would also allow us to use 4096 context size with NTK w/ 24GB
    VRAM, so it''d be quite nice!

    '
  created_at: 2023-07-05 13:51:00+00:00
  edited: false
  hidden: false
  id: 64a58354c568c6ebe0f41f0f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: reeducator/bluemoonrp-30b
repo_type: model
status: open
target_branch: null
title: Version with Groupsize = None
