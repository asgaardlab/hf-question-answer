!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hpnyaggerman
conflicting_files: null
created_at: 2023-05-31 20:09:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
      fullname: Ivan Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hpnyaggerman
      type: user
    createdAt: '2023-05-31T21:09:30.000Z'
    data:
      edited: false
      editors:
      - hpnyaggerman
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8dd2a237b0b2e1644fb9e924332b7496.svg
          fullname: Ivan Ivanov
          isHf: false
          isPro: false
          name: hpnyaggerman
          type: user
        html: '<p>Output generated in 208.93 seconds (0.53 tokens/s, 111 tokens, context
          125, seed 1431550975)<br>^^^ This is bluemoonrp-30b with --pre_layer 30
          60<br>Output generated in 9.68 seconds (20.36 tokens/s, 197 tokens, context
          125, seed 394076319)<br>^^^ This is llama-30b-4bit-128g with --pre_layer
          30 60</p>

          <p>I must note, I am executing bluemoonrp-30b with upstream GPTQ, while
          llama I am executing on ooba''s fork. But I must be doing something terribly
          wrong if the speeds are this bad, it can''t be that the new cuda branch
          slows stuff down by the factor of 40. What am I doing wrong?</p>

          '
        raw: "Output generated in 208.93 seconds (0.53 tokens/s, 111 tokens, context\
          \ 125, seed 1431550975)\r\n^^^ This is bluemoonrp-30b with --pre_layer 30\
          \ 60\r\nOutput generated in 9.68 seconds (20.36 tokens/s, 197 tokens, context\
          \ 125, seed 394076319)\r\n^^^ This is llama-30b-4bit-128g with --pre_layer\
          \ 30 60\r\n\r\nI must note, I am executing bluemoonrp-30b with upstream\
          \ GPTQ, while llama I am executing on ooba's fork. But I must be doing something\
          \ terribly wrong if the speeds are this bad, it can't be that the new cuda\
          \ branch slows stuff down by the factor of 40. What am I doing wrong?"
        updatedAt: '2023-05-31T21:09:30.355Z'
      numEdits: 0
      reactions: []
    id: 6477b78af32a4117fd16288c
    type: comment
  author: hpnyaggerman
  content: "Output generated in 208.93 seconds (0.53 tokens/s, 111 tokens, context\
    \ 125, seed 1431550975)\r\n^^^ This is bluemoonrp-30b with --pre_layer 30 60\r\
    \nOutput generated in 9.68 seconds (20.36 tokens/s, 197 tokens, context 125, seed\
    \ 394076319)\r\n^^^ This is llama-30b-4bit-128g with --pre_layer 30 60\r\n\r\n\
    I must note, I am executing bluemoonrp-30b with upstream GPTQ, while llama I am\
    \ executing on ooba's fork. But I must be doing something terribly wrong if the\
    \ speeds are this bad, it can't be that the new cuda branch slows stuff down by\
    \ the factor of 40. What am I doing wrong?"
  created_at: 2023-05-31 20:09:30+00:00
  edited: false
  hidden: false
  id: 6477b78af32a4117fd16288c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: reeducator/bluemoonrp-30b
repo_type: model
status: open
target_branch: null
title: Low t/s on 2 4090's
