!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yramshev
conflicting_files: null
created_at: 2023-05-15 10:07:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8618f4cfe3fa60534230a3e302826b5.svg
      fullname: Yordan Ramshev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yramshev
      type: user
    createdAt: '2023-05-15T11:07:48.000Z'
    data:
      edited: false
      editors:
      - yramshev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8618f4cfe3fa60534230a3e302826b5.svg
          fullname: Yordan Ramshev
          isHf: false
          isPro: false
          name: yramshev
          type: user
        html: '<p>What are the hardware requirements for this? I am running out of
          memory on my RTX3060 Ti :o<br>PC:<br> 16gB Ram<br>NVIDIA 3060Ti<br>AMD Ryzen5
          3600</p>

          '
        raw: "What are the hardware requirements for this? I am running out of memory\
          \ on my RTX3060 Ti :o\r\nPC:\r\n 16gB Ram\r\nNVIDIA 3060Ti\r\nAMD Ryzen5\
          \ 3600"
        updatedAt: '2023-05-15T11:07:48.580Z'
      numEdits: 0
      reactions: []
    id: 64621284b7fb9d68086119f7
    type: comment
  author: yramshev
  content: "What are the hardware requirements for this? I am running out of memory\
    \ on my RTX3060 Ti :o\r\nPC:\r\n 16gB Ram\r\nNVIDIA 3060Ti\r\nAMD Ryzen5 3600"
  created_at: 2023-05-15 10:07:48+00:00
  edited: false
  hidden: false
  id: 64621284b7fb9d68086119f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba75fa36ec4101c6f45905a1103d34ab.svg
      fullname: wrzaskun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wrzaskun
      type: user
    createdAt: '2023-05-15T16:05:17.000Z'
    data:
      edited: false
      editors:
      - wrzaskun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba75fa36ec4101c6f45905a1103d34ab.svg
          fullname: wrzaskun
          isHf: false
          isPro: false
          name: wrzaskun
          type: user
        html: '<p>I guess 8.7GB VRam, you have 8 on 3060TI, but model vicuna-7B  should
          works fine.</p>

          '
        raw: I guess 8.7GB VRam, you have 8 on 3060TI, but model vicuna-7B  should
          works fine.
        updatedAt: '2023-05-15T16:05:17.433Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yramshev
    id: 6462583dd290a75bd98d302d
    type: comment
  author: wrzaskun
  content: I guess 8.7GB VRam, you have 8 on 3060TI, but model vicuna-7B  should works
    fine.
  created_at: 2023-05-15 15:05:17+00:00
  edited: false
  hidden: false
  id: 6462583dd290a75bd98d302d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T21:55:33.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah I don''t think you can get away with using a 13B model on an
          8GB card.  A 7B model should be fine.</p>

          <p>And/or, check out the GGML version of this model and try it with the
          new GPU-accelerated llama.cpp.  That allows you to offload as many layers
          to GPU as you have VRAM for, and the rest is done on CPU.  Early reports
          are that it''s performing very well.  And the new GPU inference is now supported
          in text-generation-webui.</p>

          '
        raw: 'Yeah I don''t think you can get away with using a 13B model on an 8GB
          card.  A 7B model should be fine.


          And/or, check out the GGML version of this model and try it with the new
          GPU-accelerated llama.cpp.  That allows you to offload as many layers to
          GPU as you have VRAM for, and the rest is done on CPU.  Early reports are
          that it''s performing very well.  And the new GPU inference is now supported
          in text-generation-webui.'
        updatedAt: '2023-05-15T21:55:33.009Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - yramshev
    id: 6462aa5502de01c83c0e2f71
    type: comment
  author: TheBloke
  content: 'Yeah I don''t think you can get away with using a 13B model on an 8GB
    card.  A 7B model should be fine.


    And/or, check out the GGML version of this model and try it with the new GPU-accelerated
    llama.cpp.  That allows you to offload as many layers to GPU as you have VRAM
    for, and the rest is done on CPU.  Early reports are that it''s performing very
    well.  And the new GPU inference is now supported in text-generation-webui.'
  created_at: 2023-05-15 20:55:33+00:00
  edited: false
  hidden: false
  id: 6462aa5502de01c83c0e2f71
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: What are the hardware requirements for this? I am running out of memory on
  my RTX3060 Ti :o
