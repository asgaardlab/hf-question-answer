!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Blue-Devil
conflicting_files: null
created_at: 2023-04-29 17:54:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Oqk6T_U52gaDWUQITg-jb.jpeg?w=200&h=200&f=face
      fullname: Hansen Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Blue-Devil
      type: user
    createdAt: '2023-04-29T18:54:49.000Z'
    data:
      edited: false
      editors:
      - Blue-Devil
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Oqk6T_U52gaDWUQITg-jb.jpeg?w=200&h=200&f=face
          fullname: Hansen Zhang
          isHf: false
          isPro: false
          name: Blue-Devil
          type: user
        html: "<p>Sir, after installing this model directly with oobabooga webui,\
          \ I have got the following errors. Could you please share with me any suggestions\
          \ or advice you have? Thank you so much!</p>\n<p>Traceback (most recent\
          \ call last):<br>File \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\u201D, line 442, in load_state_dict<br>return torch.load(checkpoint_file,\
          \ map_location=\u201Ccpu\u201D)<br>File \u201CC:\\Users\\hanse\\OneDrive\\\
          Desktop\\test\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 791, in load<br>with _open_file_like(f,\
          \ \u2018rb\u2019) as opened_file:<br>File \u201CC:\\Users\\hanse\\OneDrive\\\
          Desktop\\test\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\u201D, line 271, in _open_file_like<br>return _open_file(name_or_buffer,\
          \ mode)<br>File \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
          , line 252, in init<br>super().init(open(name, mode))<br>FileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models\\TheBloke_stable-vicuna-13B-GPTQ\\\
          pytorch_model-00001-of-00003.bin\u2019</p>\n<p>During handling of the above\
          \ exception, another exception occurred:</p>\n<p>Traceback (most recent\
          \ call last):<br>File \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\\
          oobabooga_windows\\text-generation-webui\\server.py\u201D, line 102, in\
          \ load_model_wrapper<br>shared.model, shared.tokenizer = load_model(shared.model_name)<br>File\
          \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 84, in load_model<br>model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=trust_remote_code)<br>File\
          \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 471, in from_pretrained<br>return model_class.from_pretrained(<br>File\
          \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2795,\
          \ in from_pretrained<br>) = cls._load_pretrained_model(<br>File \u201CC:\\\
          Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 3109,\
          \ in _load_pretrained_model<br>state_dict = load_state_dict(shard_file)<br>File\
          \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 445,\
          \ in load_state_dict<br>with open(checkpoint_file) as f:<br>FileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models\\TheBloke_stable-vicuna-13B-GPTQ\\\
          pytorch_model-00001-of-00003.bin\u2019</p>\n"
        raw: "Sir, after installing this model directly with oobabooga webui, I have\
          \ got the following errors. Could you please share with me any suggestions\
          \ or advice you have? Thank you so much!\r\n\r\nTraceback (most recent call\
          \ last):\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 442, in load_state_dict\r\nreturn torch.load(checkpoint_file, map_location=\u201C\
          cpu\u201D)\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
          , line 791, in load\r\nwith _open_file_like(f, \u2018rb\u2019) as opened_file:\r\
          \nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
          , line 271, in _open_file_like\r\nreturn _open_file(name_or_buffer, mode)\r\
          \nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\u201D\
          , line 252, in init\r\nsuper().init(open(name, mode))\r\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models\\TheBloke_stable-vicuna-13B-GPTQ\\\
          pytorch_model-00001-of-00003.bin\u2019\r\n\r\nDuring handling of the above\
          \ exception, another exception occurred:\r\n\r\nTraceback (most recent call\
          \ last):\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 102, in load_model_wrapper\r\
          \nshared.model, shared.tokenizer = load_model(shared.model_name)\r\nFile\
          \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 84, in load_model\r\nmodel = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=trust_remote_code)\r\
          \nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 471, in from_pretrained\r\nreturn model_class.from_pretrained(\r\n\
          File \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 2795, in from_pretrained\r\n) = cls._load_pretrained_model(\r\nFile\
          \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 3109,\
          \ in _load_pretrained_model\r\nstate_dict = load_state_dict(shard_file)\r\
          \nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D\
          , line 445, in load_state_dict\r\nwith open(checkpoint_file) as f:\r\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: \u2018models\\TheBloke_stable-vicuna-13B-GPTQ\\\
          pytorch_model-00001-of-00003.bin\u2019\r\n"
        updatedAt: '2023-04-29T18:54:49.994Z'
      numEdits: 0
      reactions: []
    id: 644d67f916703fd670299d25
    type: comment
  author: Blue-Devil
  content: "Sir, after installing this model directly with oobabooga webui, I have\
    \ got the following errors. Could you please share with me any suggestions or\
    \ advice you have? Thank you so much!\r\n\r\nTraceback (most recent call last):\r\
    \nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 442, in load_state_dict\r\
    \nreturn torch.load(checkpoint_file, map_location=\u201Ccpu\u201D)\r\nFile \u201C\
    C:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\serialization.py\u201D, line 791, in load\r\n\
    with _open_file_like(f, \u2018rb\u2019) as opened_file:\r\nFile \u201CC:\\Users\\\
    hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\serialization.py\u201D, line 271, in _open_file_like\r\n\
    return _open_file(name_or_buffer, mode)\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\\
    Desktop\\test\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    serialization.py\u201D, line 252, in init\r\nsuper().init(open(name, mode))\r\n\
    FileNotFoundError: [Errno 2] No such file or directory: \u2018models\\TheBloke_stable-vicuna-13B-GPTQ\\\
    pytorch_model-00001-of-00003.bin\u2019\r\n\r\nDuring handling of the above exception,\
    \ another exception occurred:\r\n\r\nTraceback (most recent call last):\r\nFile\
    \ \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\text-generation-webui\\\
    server.py\u201D, line 102, in load_model_wrapper\r\nshared.model, shared.tokenizer\
    \ = load_model(shared.model_name)\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\\
    test\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D, line\
    \ 84, in load_model\r\nmodel = LoaderClass.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}\"\
    ), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if shared.args.bf16 else\
    \ torch.float16, trust_remote_code=trust_remote_code)\r\nFile \u201CC:\\Users\\\
    hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line 471, in\
    \ from_pretrained\r\nreturn model_class.from_pretrained(\r\nFile \u201CC:\\Users\\\
    hanse\\OneDrive\\Desktop\\test\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\transformers\\modeling_utils.py\u201D, line 2795, in from_pretrained\r\
    \n) = cls._load_pretrained_model(\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\\
    test\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\u201D, line 3109, in _load_pretrained_model\r\nstate_dict =\
    \ load_state_dict(shard_file)\r\nFile \u201CC:\\Users\\hanse\\OneDrive\\Desktop\\\
    test\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\u201D, line 445, in load_state_dict\r\nwith open(checkpoint_file)\
    \ as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: \u2018models\\\
    TheBloke_stable-vicuna-13B-GPTQ\\pytorch_model-00001-of-00003.bin\u2019\r\n"
  created_at: 2023-04-29 17:54:49+00:00
  edited: false
  hidden: false
  id: 644d67f916703fd670299d25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T19:25:10.000Z'
    data:
      edited: true
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>Rename the folder to TheBloke_stable-vicuna-13B-4bit-128g-GPTQ</p>

          '
        raw: Rename the folder to TheBloke_stable-vicuna-13B-4bit-128g-GPTQ
        updatedAt: '2023-04-29T19:27:05.404Z'
      numEdits: 2
      reactions:
      - count: 9
        reaction: "\u2764\uFE0F"
        users:
        - 7SMVnBb3
        - userdiffuser
        - sefsef
        - leonyourheart
        - SynthezMC
        - Jefp
        - Dronkie
        - TheRobberPanda
        - Hamstorm87
      - count: 5
        reaction: "\U0001F44D"
        users:
        - telemak
        - leonyourheart
        - SynthezMC
        - Dronkie
        - dzupin
    id: 644d6f16328c1aa30e4aef0c
    type: comment
  author: Squish42
  content: Rename the folder to TheBloke_stable-vicuna-13B-4bit-128g-GPTQ
  created_at: 2023-04-29 18:25:10+00:00
  edited: true
  hidden: false
  id: 644d6f16328c1aa30e4aef0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5e0271f4c128d6f3ff574826682c026d.svg
      fullname: JML
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: srm80
      type: user
    createdAt: '2023-04-29T19:26:58.000Z'
    data:
      edited: false
      editors:
      - srm80
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5e0271f4c128d6f3ff574826682c026d.svg
          fullname: JML
          isHf: false
          isPro: false
          name: srm80
          type: user
        html: '<p>I was having this same issue. I managed to get it to run by launching
          it with a .bat file with the line:<br>call python server.py --auto-devices
          --chat --wbits 4 --groupsize 128 --model TheBloke_stable-vicuna-13B-GPTQ</p>

          <p>That loaded it into the webui and then I saved settings for the model,
          and after that I was able to load it via the webui normally. For whatever
          reason, sometimes Ooga seems to glitch out on trying to load a new model
          at the same time as you are setting the wbits/groupsize in the webui. You
          also can try making sure the wbits and groupsize are set right in the Ooga
          webui and reloading it a few times.</p>

          '
        raw: 'I was having this same issue. I managed to get it to run by launching
          it with a .bat file with the line:

          call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --model
          TheBloke_stable-vicuna-13B-GPTQ


          That loaded it into the webui and then I saved settings for the model, and
          after that I was able to load it via the webui normally. For whatever reason,
          sometimes Ooga seems to glitch out on trying to load a new model at the
          same time as you are setting the wbits/groupsize in the webui. You also
          can try making sure the wbits and groupsize are set right in the Ooga webui
          and reloading it a few times.'
        updatedAt: '2023-04-29T19:26:58.876Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Squish42
    id: 644d6f82fa94e93b0ecb435d
    type: comment
  author: srm80
  content: 'I was having this same issue. I managed to get it to run by launching
    it with a .bat file with the line:

    call python server.py --auto-devices --chat --wbits 4 --groupsize 128 --model
    TheBloke_stable-vicuna-13B-GPTQ


    That loaded it into the webui and then I saved settings for the model, and after
    that I was able to load it via the webui normally. For whatever reason, sometimes
    Ooga seems to glitch out on trying to load a new model at the same time as you
    are setting the wbits/groupsize in the webui. You also can try making sure the
    wbits and groupsize are set right in the Ooga webui and reloading it a few times.'
  created_at: 2023-04-29 18:26:58+00:00
  edited: false
  hidden: false
  id: 644d6f82fa94e93b0ecb435d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T19:33:35.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>text-generation-webui can check the folder name for the 4bit-128g
          config. Then it''s fine without the extra command line arguments.<br>If
          you use the UI to make a config entry instead, make sure to actually click
          "Save settings for this model" after it loads.</p>

          '
        raw: 'text-generation-webui can check the folder name for the 4bit-128g config.
          Then it''s fine without the extra command line arguments.

          If you use the UI to make a config entry instead, make sure to actually
          click "Save settings for this model" after it loads.'
        updatedAt: '2023-04-29T19:33:35.812Z'
      numEdits: 0
      reactions: []
    id: 644d710f97a3b0904a577ba9
    type: comment
  author: Squish42
  content: 'text-generation-webui can check the folder name for the 4bit-128g config.
    Then it''s fine without the extra command line arguments.

    If you use the UI to make a config entry instead, make sure to actually click
    "Save settings for this model" after it loads.'
  created_at: 2023-04-29 18:33:35+00:00
  edited: false
  hidden: false
  id: 644d710f97a3b0904a577ba9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T20:20:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks, Squish</p>

          <p>The error is annoying and maybe I should just rename the repo. But it
          can be ignored. If you enter the params and then Reload The Model it will
          work.</p>

          <p>Maybe I should make that clearer in the instructions. Or just rename
          it!</p>

          '
        raw: 'Thanks, Squish


          The error is annoying and maybe I should just rename the repo. But it can
          be ignored. If you enter the params and then Reload The Model it will work.


          Maybe I should make that clearer in the instructions. Or just rename it!'
        updatedAt: '2023-04-29T20:20:16.014Z'
      numEdits: 0
      reactions: []
    id: 644d7c000dc952d245a5f93b
    type: comment
  author: TheBloke
  content: 'Thanks, Squish


    The error is annoying and maybe I should just rename the repo. But it can be ignored.
    If you enter the params and then Reload The Model it will work.


    Maybe I should make that clearer in the instructions. Or just rename it!'
  created_at: 2023-04-29 19:20:16+00:00
  edited: false
  hidden: false
  id: 644d7c000dc952d245a5f93b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T20:50:19.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>Yea I think it is just confusing mostly because people are still
          figuring out how to load all these different formats and tools. I probably
          wouldn''t rename it to cater to one tool in the current ecosystem. It''s
          a few clicks for us and probably not something you should worry much about.</p>

          '
        raw: Yea I think it is just confusing mostly because people are still figuring
          out how to load all these different formats and tools. I probably wouldn't
          rename it to cater to one tool in the current ecosystem. It's a few clicks
          for us and probably not something you should worry much about.
        updatedAt: '2023-04-29T20:50:19.209Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - Vidada
    id: 644d830bfa94e93b0ecd0256
    type: comment
  author: Squish42
  content: Yea I think it is just confusing mostly because people are still figuring
    out how to load all these different formats and tools. I probably wouldn't rename
    it to cater to one tool in the current ecosystem. It's a few clicks for us and
    probably not something you should worry much about.
  created_at: 2023-04-29 19:50:19+00:00
  edited: false
  hidden: false
  id: 644d830bfa94e93b0ecd0256
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b3b75b83d424fca00455b4c16646a5b1.svg
      fullname: leon benzene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: leonyourheart
      type: user
    createdAt: '2023-04-30T13:59:38.000Z'
    data:
      edited: false
      editors:
      - leonyourheart
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b3b75b83d424fca00455b4c16646a5b1.svg
          fullname: leon benzene
          isHf: false
          isPro: false
          name: leonyourheart
          type: user
        html: '<blockquote>

          <p>Rename the folder to TheBloke_stable-vicuna-13B-4bit-128g-GPTQ</p>

          </blockquote>

          <p>Ok, it works, but why?</p>

          '
        raw: '> Rename the folder to TheBloke_stable-vicuna-13B-4bit-128g-GPTQ


          Ok, it works, but why?'
        updatedAt: '2023-04-30T13:59:38.472Z'
      numEdits: 0
      reactions: []
    id: 644e744aa00f4b11d3924ee0
    type: comment
  author: leonyourheart
  content: '> Rename the folder to TheBloke_stable-vicuna-13B-4bit-128g-GPTQ


    Ok, it works, but why?'
  created_at: 2023-04-30 12:59:38+00:00
  edited: false
  hidden: false
  id: 644e744aa00f4b11d3924ee0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-30T15:03:10.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Because if you rename the directory, ooba text-gen-ui can automatically
          fill in the GPTQ parameters:<br>bits = 4<br>groupsize = 128<br>model_type
          = llama</p>

          <p>You can also fill them in manually at any time, and details are in the
          README - step 8:</p>

          <h2 id="how-to-easily-download-and-use-this-model-in-text-generation-webui">How
          to easily download and use this model in text-generation-webui</h2>

          <p>Open the text-generation-webui UI as normal.</p>

          <ol>

          <li>Click the <strong>Model tab</strong>.</li>

          <li>Under <strong>Download custom model or LoRA</strong>, enter <code>TheBloke/stable-vicuna-13B-GPTQ</code>.</li>

          <li>Click <strong>Download</strong>.</li>

          <li>Wait until it says it''s finished downloading.</li>

          <li>Click the <strong>Refresh</strong> icon next to <strong>Model</strong>
          in the top left.</li>

          <li>In the <strong>Model drop-down</strong>: choose the model you just downloaded,<code>stable-vicuna-13B-GPTQ</code>.</li>

          <li>If you see an error in the bottom right, ignore it - it''s temporary.</li>

          <li>Fill out the <code>GPTQ parameters</code> on the right: <code>Bits =
          4</code>, <code>Groupsize = 128</code>, <code>model_type = Llama</code></li>

          <li>Click <strong>Save settings for this model</strong> in the top right.</li>

          <li>Click <strong>Reload the Model</strong> in the top right.</li>

          <li>Once it says it''s loaded, click the <strong>Text Generation tab</strong>
          and enter a prompt!</li>

          </ol>

          '
        raw: 'Because if you rename the directory, ooba text-gen-ui can automatically
          fill in the GPTQ parameters:

          bits = 4

          groupsize = 128

          model_type = llama


          You can also fill them in manually at any time, and details are in the README
          - step 8:


          ## How to easily download and use this model in text-generation-webui


          Open the text-generation-webui UI as normal.


          1. Click the **Model tab**.

          2. Under **Download custom model or LoRA**, enter `TheBloke/stable-vicuna-13B-GPTQ`.

          3. Click **Download**.

          4. Wait until it says it''s finished downloading.

          5. Click the **Refresh** icon next to **Model** in the top left.

          6. In the **Model drop-down**: choose the model you just downloaded,`stable-vicuna-13B-GPTQ`.

          7. If you see an error in the bottom right, ignore it - it''s temporary.

          8. Fill out the `GPTQ parameters` on the right: `Bits = 4`, `Groupsize =
          128`, `model_type = Llama`

          9. Click **Save settings for this model** in the top right.

          10. Click **Reload the Model** in the top right.

          11. Once it says it''s loaded, click the **Text Generation tab** and enter
          a prompt!'
        updatedAt: '2023-04-30T15:03:24.657Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dzupin
    id: 644e832eddf20748b058c01a
    type: comment
  author: TheBloke
  content: 'Because if you rename the directory, ooba text-gen-ui can automatically
    fill in the GPTQ parameters:

    bits = 4

    groupsize = 128

    model_type = llama


    You can also fill them in manually at any time, and details are in the README
    - step 8:


    ## How to easily download and use this model in text-generation-webui


    Open the text-generation-webui UI as normal.


    1. Click the **Model tab**.

    2. Under **Download custom model or LoRA**, enter `TheBloke/stable-vicuna-13B-GPTQ`.

    3. Click **Download**.

    4. Wait until it says it''s finished downloading.

    5. Click the **Refresh** icon next to **Model** in the top left.

    6. In the **Model drop-down**: choose the model you just downloaded,`stable-vicuna-13B-GPTQ`.

    7. If you see an error in the bottom right, ignore it - it''s temporary.

    8. Fill out the `GPTQ parameters` on the right: `Bits = 4`, `Groupsize = 128`,
    `model_type = Llama`

    9. Click **Save settings for this model** in the top right.

    10. Click **Reload the Model** in the top right.

    11. Once it says it''s loaded, click the **Text Generation tab** and enter a prompt!'
  created_at: 2023-04-30 14:03:10+00:00
  edited: true
  hidden: false
  id: 644e832eddf20748b058c01a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/W9maLAuthxuaU6N3wTx-o.png?w=200&h=200&f=face
      fullname: Tomasz Bekus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: drtombek
      type: user
    createdAt: '2023-05-06T20:00:49.000Z'
    data:
      edited: false
      editors:
      - drtombek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/W9maLAuthxuaU6N3wTx-o.png?w=200&h=200&f=face
          fullname: Tomasz Bekus
          isHf: false
          isPro: false
          name: drtombek
          type: user
        html: '<p>How to fix it?<br>Traceback (most recent call last):<br>  File "C:\LLM\text-generation-webui\text-generation-webui\server.py",
          line 872, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\LLM\text-generation-webui\text-generation-webui\modules\models.py",
          line 85, in load_model<br>    model = LoaderClass.from_pretrained(Path(f"{shared.args.model_dir}/{model_name}"),
          low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if shared.args.bf16 else
          torch.float16, trust_remote_code=trust_remote_code)<br>  File "C:\LLM\text-generation-webui\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 471, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "C:\LLM\text-generation-webui\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 2405, in from_pretrained<br>    raise EnvironmentError(<br>OSError:
          Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or
          flax_model.msgpack found in directory models\TheBloke_stable-vicuna-13B-GPTQ.</p>

          '
        raw: "How to fix it?\nTraceback (most recent call last):\n  File \"C:\\LLM\\\
          text-generation-webui\\text-generation-webui\\server.py\", line 872, in\
          \ <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"C:\\LLM\\text-generation-webui\\text-generation-webui\\modules\\\
          models.py\", line 85, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=trust_remote_code)\n\
          \  File \"C:\\LLM\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\n\
          \    return model_class.from_pretrained(\n  File \"C:\\LLM\\text-generation-webui\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 2405, in from_pretrained\n    raise EnvironmentError(\nOSError: Error\
          \ no file named pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke_stable-vicuna-13B-GPTQ."
        updatedAt: '2023-05-06T20:00:49.652Z'
      numEdits: 0
      reactions: []
    id: 6456b1f1cd6567f52fba18e2
    type: comment
  author: drtombek
  content: "How to fix it?\nTraceback (most recent call last):\n  File \"C:\\LLM\\\
    text-generation-webui\\text-generation-webui\\server.py\", line 872, in <module>\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name)\n  File \"\
    C:\\LLM\\text-generation-webui\\text-generation-webui\\modules\\models.py\", line\
    \ 85, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}\"\
    ), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if shared.args.bf16 else\
    \ torch.float16, trust_remote_code=trust_remote_code)\n  File \"C:\\LLM\\text-generation-webui\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
    , line 471, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"C:\\LLM\\text-generation-webui\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 2405, in from_pretrained\n    raise EnvironmentError(\n\
    OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index\
    \ or flax_model.msgpack found in directory models\\TheBloke_stable-vicuna-13B-GPTQ."
  created_at: 2023-05-06 19:00:49+00:00
  edited: false
  hidden: false
  id: 6456b1f1cd6567f52fba18e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-06T20:16:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please see the README - you need to fill out the GPTQ parameters,
          then "save settings for this model" and "reload the model"</p>

          '
        raw: Please see the README - you need to fill out the GPTQ parameters, then
          "save settings for this model" and "reload the model"
        updatedAt: '2023-05-06T20:16:52.096Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - drtombek
        - gweber
    id: 6456b5b4cd6567f52fba4f0a
    type: comment
  author: TheBloke
  content: Please see the README - you need to fill out the GPTQ parameters, then
    "save settings for this model" and "reload the model"
  created_at: 2023-05-06 19:16:52+00:00
  edited: false
  hidden: false
  id: 6456b5b4cd6567f52fba4f0a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: "No such file or directory: \u2018models\\TheBloke_stable-vicuna-13B-GPTQ\\\
  pytorch_model-00001-of-00003.bin\u2019"
