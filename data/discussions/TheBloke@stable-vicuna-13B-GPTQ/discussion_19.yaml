!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joshlevy89
conflicting_files: null
created_at: 2023-05-13 18:09:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
      fullname: Joshua Levy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joshlevy89
      type: user
    createdAt: '2023-05-13T19:09:13.000Z'
    data:
      edited: true
      editors:
      - joshlevy89
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
          fullname: Joshua Levy
          isHf: false
          isPro: false
          name: joshlevy89
          type: user
        html: "<p>I'm trying to load the pretrained model using AutoGPTQ but getting\
          \ error.<br>I'm on Colab (T4)</p>\n<p>Update: I am still getting the error\
          \ below for this model, and the non-RLHF'ed, safetensors version of vicuna\
          \ (vicuna-13B-1.1-GPTQ-4bit-128g.latest.safetensors).<br>However, I did\
          \ get the no-act-order, pt version of the latter to work (vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order.pt).<br>So\
          \ I guess it's something to do with compatibility with certain types of\
          \ quantizations but not others...maybe some configurations arguments need\
          \ to be passed to make it work?</p>\n<p>Code:<br>!pip install auto-gptq[llama]<br>from\
          \ auto_gptq import AutoGPTQForCausalLM<br>device = \"cuda:0\"<br>MODEL_DIR\
          \ = \"/content/drive/MyDrive/AI/projects/lie_detector/TheBloke/stable-vicuna-13B-GPTQ\"\
          <br>MODEL_FILE = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\
          <br>model = AutoGPTQForCausalLM.from_quantized(MODEL_DIR, model_basename=MODEL_FILE,\
          \ use_triton=True)</p>\n<p>Note: I had to append .bin to the model file\
          \ name because it seems that AutoGPTQ does this automatically.</p>\n<p>Error:<br>UnpicklingError:\
          \ invalid load key, '\\xa8'.</p>\n<p>Full Trace:<br>WARNING:auto_gptq.modeling._base:use_triton\
          \ will force moving the whole model to GPU, make sure you have enough VRAM.<br>\u256D\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last)\
          \ \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502 in &lt;cell\
          \ line: 8&gt;:8                                                        \
          \                      \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:63\
          \ in from_quantized          \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502   60 \u2502   \u2502   trust_remote_code: bool = False\
          \                                                     \u2502<br>\u2502 \
          \  61 \u2502   ) -&gt; BaseGPTQForCausalLM:                            \
          \                                   \u2502<br>\u2502   62 \u2502   \u2502\
          \   model_type = check_and_get_model_type(save_dir)                    \
          \                 \u2502<br>\u2502 \u2771 63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
          \                         \u2502<br>\u2502   64 \u2502   \u2502   \u2502\
          \   save_dir=save_dir,                                                 \
          \             \u2502<br>\u2502   65 \u2502   \u2502   \u2502   device=device,\
          \                                                                  \u2502\
          <br>\u2502   66 \u2502   \u2502   \u2502   use_safetensors=use_safetensors,\
          \                                                \u2502<br>\u2502      \
          \                                                                      \
          \                      \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:544\
          \ in from_quantized        \u2502<br>\u2502                            \
          \                                                                      \u2502\
          <br>\u2502   541 \u2502   \u2502   if not max_memory and not device_map:\
          \                                              \u2502<br>\u2502   542 \u2502\
          \   \u2502   \u2502   device_map = {\"\": device}                      \
          \                                \u2502<br>\u2502   543 \u2502   \u2502\
          \                                                                      \
          \                \u2502<br>\u2502 \u2771 544 \u2502   \u2502   model = accelerate.load_checkpoint_and_dispatch(\
          \                                   \u2502<br>\u2502   545 \u2502   \u2502\
          \   \u2502   model, model_save_name, device_map, max_memory, no_split_module_classes=[cls\
          \   \u2502<br>\u2502   546 \u2502   \u2502   )                         \
          \                                                         \u2502<br>\u2502\
          \   547                                                                \
          \                            \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:479\
          \ in                        \u2502<br>\u2502 load_checkpoint_and_dispatch\
          \                                                                     \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502   476 \u2502   \u2502\
          \   )                                                                  \
          \                \u2502<br>\u2502   477 \u2502   if offload_state_dict is\
          \ None and device_map is not None and \"disk\" in device_map.va   \u2502\
          <br>\u2502   478 \u2502   \u2502   offload_state_dict = True           \
          \                                               \u2502<br>\u2502 \u2771\
          \ 479 \u2502   load_checkpoint_in_model(                               \
          \                               \u2502<br>\u2502   480 \u2502   \u2502 \
          \  model,                                                              \
          \               \u2502<br>\u2502   481 \u2502   \u2502   checkpoint,   \
          \                                                                     \u2502\
          <br>\u2502   482 \u2502   \u2502   device_map=device_map,              \
          \                                               \u2502<br>\u2502       \
          \                                                                      \
          \                     \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:971\
          \ in                      \u2502<br>\u2502 load_checkpoint_in_model    \
          \                                                                     \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502    968 \u2502   buffer_names\
          \ = [name for name, _ in model.named_buffers()]                        \
          \    \u2502<br>\u2502    969 \u2502                                    \
          \                                                     \u2502<br>\u2502 \
          \   970 \u2502   for checkpoint_file in checkpoint_files:              \
          \                                \u2502<br>\u2502 \u2771  971 \u2502   \u2502\
          \   checkpoint = load_state_dict(checkpoint_file, device_map=device_map)\
          \              \u2502<br>\u2502    972 \u2502   \u2502   if device_map is\
          \ None:                                                            \u2502\
          <br>\u2502    973 \u2502   \u2502   \u2502   model.load_state_dict(checkpoint,\
          \ strict=False)                               \u2502<br>\u2502    974 \u2502\
          \   \u2502   else:                                                     \
          \                        \u2502<br>\u2502                              \
          \                                                                    \u2502\
          <br>\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:873\
          \ in load_state_dict      \u2502<br>\u2502                             \
          \                                                                     \u2502\
          <br>\u2502    870 \u2502   \u2502   \u2502                             \
          \                                                    \u2502<br>\u2502  \
          \  871 \u2502   \u2502   \u2502   return tensors                       \
          \                                         \u2502<br>\u2502    872 \u2502\
          \   else:                                                              \
          \                   \u2502<br>\u2502 \u2771  873 \u2502   \u2502   return\
          \ torch.load(checkpoint_file)                                          \
          \      \u2502<br>\u2502    874                                         \
          \                                                  \u2502<br>\u2502    875\
          \                                                                      \
          \                     \u2502<br>\u2502    876 def load_checkpoint_in_model(\
          \                                                             \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/torch/serialization.py:815\
          \ in load                       \u2502<br>\u2502                       \
          \                                                                      \
          \     \u2502<br>\u2502    812 \u2502   \u2502   \u2502   \u2502   return\
          \ _legacy_load(opened_file, map_location, _weights_only_unpickler,   \u2502\
          <br>\u2502    813 \u2502   \u2502   \u2502   except RuntimeError as e: \
          \                                                    \u2502<br>\u2502  \
          \  814 \u2502   \u2502   \u2502   \u2502   raise pickle.UnpicklingError(UNSAFE_MESSAGE\
          \ + str(e)) from None           \u2502<br>\u2502 \u2771  815 \u2502   \u2502\
          \   return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args\
          \  \u2502<br>\u2502    816                                             \
          \                                              \u2502<br>\u2502    817 \
          \                                                                      \
          \                    \u2502<br>\u2502    818 # Register pickling support\
          \ for layout instances such as                                  \u2502<br>\u2502\
          \                                                                      \
          \                            \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/torch/serialization.py:1033\
          \ in _legacy_load              \u2502<br>\u2502                        \
          \                                                                      \
          \    \u2502<br>\u2502   1030 \u2502   \u2502   \u2502   f\"Received object\
          \ of type \"{type(f)}\". Please update to Python 3.8.2 or ne  \u2502<br>\u2502\
          \   1031 \u2502   \u2502   \u2502   \"functionality.\")                \
          \                                             \u2502<br>\u2502   1032 \u2502\
          \                                                                      \
          \                   \u2502<br>\u2502 \u2771 1033 \u2502   magic_number =\
          \ pickle_module.load(f, **pickle_load_args)                            \
          \  \u2502<br>\u2502   1034 \u2502   if magic_number != MAGIC_NUMBER:   \
          \                                                   \u2502<br>\u2502   1035\
          \ \u2502   \u2502   raise RuntimeError(\"Invalid magic number; corrupt file?\"\
          )                         \u2502<br>\u2502   1036 \u2502   protocol_version\
          \ = pickle_module.load(f, **pickle_load_args)<br>UnpicklingError: invalid\
          \ load key, '\\xa8'.</p>\n"
        raw: "I'm trying to load the pretrained model using AutoGPTQ but getting error.\n\
          I'm on Colab (T4)\n\nUpdate: I am still getting the error below for this\
          \ model, and the non-RLHF'ed, safetensors version of vicuna (vicuna-13B-1.1-GPTQ-4bit-128g.latest.safetensors).\
          \  \nHowever, I did get the no-act-order, pt version of the latter to work\
          \ (vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order.pt).  \nSo I guess\
          \ it's something to do with compatibility with certain types of quantizations\
          \ but not others...maybe some configurations arguments need to be passed\
          \ to make it work?\n\nCode: \n!pip install auto-gptq[llama]\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM\ndevice = \"cuda:0\"\nMODEL_DIR = \"/content/drive/MyDrive/AI/projects/lie_detector/TheBloke/stable-vicuna-13B-GPTQ\"\
          \nMODEL_FILE = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\
          \nmodel = AutoGPTQForCausalLM.from_quantized(MODEL_DIR, model_basename=MODEL_FILE,\
          \ use_triton=True)\n\nNote: I had to append .bin to the model file name\
          \ because it seems that AutoGPTQ does this automatically.\n\nError:\nUnpicklingError:\
          \ invalid load key, '\\xa8'.\n\nFull Trace:\nWARNING:auto_gptq.modeling._base:use_triton\
          \ will force moving the whole model to GPU, make sure you have enough VRAM.\n\
          \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ in <cell line: 8>:8                                                  \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:63\
          \ in from_quantized          \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   60 \u2502   \u2502   trust_remote_code: bool = False        \
          \                                             \u2502\n\u2502   61 \u2502\
          \   ) -> BaseGPTQForCausalLM:                                          \
          \                     \u2502\n\u2502   62 \u2502   \u2502   model_type =\
          \ check_and_get_model_type(save_dir)                                   \
          \  \u2502\n\u2502 \u2771 63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
          \                         \u2502\n\u2502   64 \u2502   \u2502   \u2502 \
          \  save_dir=save_dir,                                                  \
          \            \u2502\n\u2502   65 \u2502   \u2502   \u2502   device=device,\
          \                                                                  \u2502\
          \n\u2502   66 \u2502   \u2502   \u2502   use_safetensors=use_safetensors,\
          \                                                \u2502\n\u2502        \
          \                                                                      \
          \                    \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:544\
          \ in from_quantized        \u2502\n\u2502                              \
          \                                                                    \u2502\
          \n\u2502   541 \u2502   \u2502   if not max_memory and not device_map: \
          \                                             \u2502\n\u2502   542 \u2502\
          \   \u2502   \u2502   device_map = {\"\": device}                      \
          \                                \u2502\n\u2502   543 \u2502   \u2502  \
          \                                                                      \
          \              \u2502\n\u2502 \u2771 544 \u2502   \u2502   model = accelerate.load_checkpoint_and_dispatch(\
          \                                   \u2502\n\u2502   545 \u2502   \u2502\
          \   \u2502   model, model_save_name, device_map, max_memory, no_split_module_classes=[cls\
          \   \u2502\n\u2502   546 \u2502   \u2502   )                           \
          \                                                       \u2502\n\u2502 \
          \  547                                                                 \
          \                           \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:479\
          \ in                        \u2502\n\u2502 load_checkpoint_and_dispatch\
          \                                                                     \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502   476 \u2502   \u2502\
          \   )                                                                  \
          \                \u2502\n\u2502   477 \u2502   if offload_state_dict is\
          \ None and device_map is not None and \"disk\" in device_map.va   \u2502\
          \n\u2502   478 \u2502   \u2502   offload_state_dict = True             \
          \                                             \u2502\n\u2502 \u2771 479\
          \ \u2502   load_checkpoint_in_model(                                   \
          \                           \u2502\n\u2502   480 \u2502   \u2502   model,\
          \                                                                      \
          \       \u2502\n\u2502   481 \u2502   \u2502   checkpoint,             \
          \                                                           \u2502\n\u2502\
          \   482 \u2502   \u2502   device_map=device_map,                       \
          \                                      \u2502\n\u2502                  \
          \                                                                      \
          \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:971\
          \ in                      \u2502\n\u2502 load_checkpoint_in_model      \
          \                                                                   \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502    968 \u2502   buffer_names\
          \ = [name for name, _ in model.named_buffers()]                        \
          \    \u2502\n\u2502    969 \u2502                                      \
          \                                                   \u2502\n\u2502    970\
          \ \u2502   for checkpoint_file in checkpoint_files:                    \
          \                          \u2502\n\u2502 \u2771  971 \u2502   \u2502  \
          \ checkpoint = load_state_dict(checkpoint_file, device_map=device_map) \
          \             \u2502\n\u2502    972 \u2502   \u2502   if device_map is None:\
          \                                                            \u2502\n\u2502\
          \    973 \u2502   \u2502   \u2502   model.load_state_dict(checkpoint, strict=False)\
          \                               \u2502\n\u2502    974 \u2502   \u2502  \
          \ else:                                                                \
          \             \u2502\n\u2502                                           \
          \                                                       \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:873\
          \ in load_state_dict      \u2502\n\u2502                               \
          \                                                                   \u2502\
          \n\u2502    870 \u2502   \u2502   \u2502                               \
          \                                                  \u2502\n\u2502    871\
          \ \u2502   \u2502   \u2502   return tensors                            \
          \                                    \u2502\n\u2502    872 \u2502   else:\
          \                                                                      \
          \           \u2502\n\u2502 \u2771  873 \u2502   \u2502   return torch.load(checkpoint_file)\
          \                                                \u2502\n\u2502    874 \
          \                                                                      \
          \                    \u2502\n\u2502    875                             \
          \                                                              \u2502\n\u2502\
          \    876 def load_checkpoint_in_model(                                 \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/torch/serialization.py:815\
          \ in load                       \u2502\n\u2502                         \
          \                                                                      \
          \   \u2502\n\u2502    812 \u2502   \u2502   \u2502   \u2502   return _legacy_load(opened_file,\
          \ map_location, _weights_only_unpickler,   \u2502\n\u2502    813 \u2502\
          \   \u2502   \u2502   except RuntimeError as e:                        \
          \                             \u2502\n\u2502    814 \u2502   \u2502   \u2502\
          \   \u2502   raise pickle.UnpicklingError(UNSAFE_MESSAGE + str(e)) from\
          \ None           \u2502\n\u2502 \u2771  815 \u2502   \u2502   return _legacy_load(opened_file,\
          \ map_location, pickle_module, **pickle_load_args  \u2502\n\u2502    816\
          \                                                                      \
          \                     \u2502\n\u2502    817                            \
          \                                                               \u2502\n\
          \u2502    818 # Register pickling support for layout instances such as \
          \                                 \u2502\n\u2502                       \
          \                                                                      \
          \     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/serialization.py:1033\
          \ in _legacy_load              \u2502\n\u2502                          \
          \                                                                      \
          \  \u2502\n\u2502   1030 \u2502   \u2502   \u2502   f\"Received object of\
          \ type \\\"{type(f)}\\\". Please update to Python 3.8.2 or ne  \u2502\n\u2502\
          \   1031 \u2502   \u2502   \u2502   \"functionality.\")                \
          \                                             \u2502\n\u2502   1032 \u2502\
          \                                                                      \
          \                   \u2502\n\u2502 \u2771 1033 \u2502   magic_number = pickle_module.load(f,\
          \ **pickle_load_args)                              \u2502\n\u2502   1034\
          \ \u2502   if magic_number != MAGIC_NUMBER:                            \
          \                          \u2502\n\u2502   1035 \u2502   \u2502   raise\
          \ RuntimeError(\"Invalid magic number; corrupt file?\")                \
          \         \u2502\n\u2502   1036 \u2502   protocol_version = pickle_module.load(f,\
          \ **pickle_load_args)    \nUnpicklingError: invalid load key, '\\xa8'."
        updatedAt: '2023-05-13T19:37:47.003Z'
      numEdits: 2
      reactions: []
    id: 645fe059335c21d19f3bb84f
    type: comment
  author: joshlevy89
  content: "I'm trying to load the pretrained model using AutoGPTQ but getting error.\n\
    I'm on Colab (T4)\n\nUpdate: I am still getting the error below for this model,\
    \ and the non-RLHF'ed, safetensors version of vicuna (vicuna-13B-1.1-GPTQ-4bit-128g.latest.safetensors).\
    \  \nHowever, I did get the no-act-order, pt version of the latter to work (vicuna-13B-1.1-GPTQ-4bit-128g.compat.no-act-order.pt).\
    \  \nSo I guess it's something to do with compatibility with certain types of\
    \ quantizations but not others...maybe some configurations arguments need to be\
    \ passed to make it work?\n\nCode: \n!pip install auto-gptq[llama]\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM\ndevice = \"cuda:0\"\nMODEL_DIR = \"/content/drive/MyDrive/AI/projects/lie_detector/TheBloke/stable-vicuna-13B-GPTQ\"\
    \nMODEL_FILE = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\
    \nmodel = AutoGPTQForCausalLM.from_quantized(MODEL_DIR, model_basename=MODEL_FILE,\
    \ use_triton=True)\n\nNote: I had to append .bin to the model file name because\
    \ it seems that AutoGPTQ does this automatically.\n\nError:\nUnpicklingError:\
    \ invalid load key, '\\xa8'.\n\nFull Trace:\nWARNING:auto_gptq.modeling._base:use_triton\
    \ will force moving the whole model to GPU, make sure you have enough VRAM.\n\u256D\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u256E\n\u2502 in <cell line: 8>:8                         \
    \                                                     \u2502\n\u2502         \
    \                                                                            \
    \             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:63\
    \ in from_quantized          \u2502\n\u2502                                  \
    \                                                                \u2502\n\u2502\
    \   60 \u2502   \u2502   trust_remote_code: bool = False                     \
    \                                \u2502\n\u2502   61 \u2502   ) -> BaseGPTQForCausalLM:\
    \                                                               \u2502\n\u2502\
    \   62 \u2502   \u2502   model_type = check_and_get_model_type(save_dir)     \
    \                                \u2502\n\u2502 \u2771 63 \u2502   \u2502   return\
    \ GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(                       \
    \  \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,         \
    \                                                     \u2502\n\u2502   65 \u2502\
    \   \u2502   \u2502   device=device,                                         \
    \                         \u2502\n\u2502   66 \u2502   \u2502   \u2502   use_safetensors=use_safetensors,\
    \                                                \u2502\n\u2502              \
    \                                                                            \
    \        \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py:544\
    \ in from_quantized        \u2502\n\u2502                                    \
    \                                                              \u2502\n\u2502\
    \   541 \u2502   \u2502   if not max_memory and not device_map:              \
    \                                \u2502\n\u2502   542 \u2502   \u2502   \u2502\
    \   device_map = {\"\": device}                                              \
    \        \u2502\n\u2502   543 \u2502   \u2502                                \
    \                                                      \u2502\n\u2502 \u2771 544\
    \ \u2502   \u2502   model = accelerate.load_checkpoint_and_dispatch(         \
    \                          \u2502\n\u2502   545 \u2502   \u2502   \u2502   model,\
    \ model_save_name, device_map, max_memory, no_split_module_classes=[cls   \u2502\
    \n\u2502   546 \u2502   \u2502   )                                           \
    \                                       \u2502\n\u2502   547                 \
    \                                                                           \u2502\
    \n\u2502                                                                     \
    \                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py:479\
    \ in                        \u2502\n\u2502 load_checkpoint_and_dispatch      \
    \                                                               \u2502\n\u2502\
    \                                                                            \
    \                      \u2502\n\u2502   476 \u2502   \u2502   )              \
    \                                                                    \u2502\n\u2502\
    \   477 \u2502   if offload_state_dict is None and device_map is not None and\
    \ \"disk\" in device_map.va   \u2502\n\u2502   478 \u2502   \u2502   offload_state_dict\
    \ = True                                                          \u2502\n\u2502\
    \ \u2771 479 \u2502   load_checkpoint_in_model(                              \
    \                                \u2502\n\u2502   480 \u2502   \u2502   model,\
    \                                                                            \
    \ \u2502\n\u2502   481 \u2502   \u2502   checkpoint,                         \
    \                                               \u2502\n\u2502   482 \u2502  \
    \ \u2502   device_map=device_map,                                            \
    \                 \u2502\n\u2502                                             \
    \                                                     \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:971\
    \ in                      \u2502\n\u2502 load_checkpoint_in_model            \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \                     \u2502\n\u2502    968 \u2502   buffer_names = [name for\
    \ name, _ in model.named_buffers()]                            \u2502\n\u2502\
    \    969 \u2502                                                              \
    \                           \u2502\n\u2502    970 \u2502   for checkpoint_file\
    \ in checkpoint_files:                                              \u2502\n\u2502\
    \ \u2771  971 \u2502   \u2502   checkpoint = load_state_dict(checkpoint_file,\
    \ device_map=device_map)              \u2502\n\u2502    972 \u2502   \u2502  \
    \ if device_map is None:                                                     \
    \       \u2502\n\u2502    973 \u2502   \u2502   \u2502   model.load_state_dict(checkpoint,\
    \ strict=False)                               \u2502\n\u2502    974 \u2502   \u2502\
    \   else:                                                                    \
    \         \u2502\n\u2502                                                     \
    \                                             \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py:873\
    \ in load_state_dict      \u2502\n\u2502                                     \
    \                                                             \u2502\n\u2502 \
    \   870 \u2502   \u2502   \u2502                                             \
    \                                    \u2502\n\u2502    871 \u2502   \u2502   \u2502\
    \   return tensors                                                           \
    \     \u2502\n\u2502    872 \u2502   else:                                   \
    \                                              \u2502\n\u2502 \u2771  873 \u2502\
    \   \u2502   return torch.load(checkpoint_file)                              \
    \                  \u2502\n\u2502    874                                     \
    \                                                      \u2502\n\u2502    875 \
    \                                                                            \
    \              \u2502\n\u2502    876 def load_checkpoint_in_model(           \
    \                                                  \u2502\n\u2502            \
    \                                                                            \
    \          \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/serialization.py:815\
    \ in load                       \u2502\n\u2502                               \
    \                                                                   \u2502\n\u2502\
    \    812 \u2502   \u2502   \u2502   \u2502   return _legacy_load(opened_file,\
    \ map_location, _weights_only_unpickler,   \u2502\n\u2502    813 \u2502   \u2502\
    \   \u2502   except RuntimeError as e:                                       \
    \              \u2502\n\u2502    814 \u2502   \u2502   \u2502   \u2502   raise\
    \ pickle.UnpicklingError(UNSAFE_MESSAGE + str(e)) from None           \u2502\n\
    \u2502 \u2771  815 \u2502   \u2502   return _legacy_load(opened_file, map_location,\
    \ pickle_module, **pickle_load_args  \u2502\n\u2502    816                   \
    \                                                                        \u2502\
    \n\u2502    817                                                              \
    \                             \u2502\n\u2502    818 # Register pickling support\
    \ for layout instances such as                                  \u2502\n\u2502\
    \                                                                            \
    \                      \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/serialization.py:1033\
    \ in _legacy_load              \u2502\n\u2502                                \
    \                                                                  \u2502\n\u2502\
    \   1030 \u2502   \u2502   \u2502   f\"Received object of type \\\"{type(f)}\\\
    \". Please update to Python 3.8.2 or ne  \u2502\n\u2502   1031 \u2502   \u2502\
    \   \u2502   \"functionality.\")                                             \
    \                \u2502\n\u2502   1032 \u2502                                \
    \                                                         \u2502\n\u2502 \u2771\
    \ 1033 \u2502   magic_number = pickle_module.load(f, **pickle_load_args)     \
    \                         \u2502\n\u2502   1034 \u2502   if magic_number != MAGIC_NUMBER:\
    \                                                      \u2502\n\u2502   1035 \u2502\
    \   \u2502   raise RuntimeError(\"Invalid magic number; corrupt file?\")     \
    \                    \u2502\n\u2502   1036 \u2502   protocol_version = pickle_module.load(f,\
    \ **pickle_load_args)    \nUnpicklingError: invalid load key, '\\xa8'."
  created_at: 2023-05-13 18:09:13+00:00
  edited: true
  hidden: false
  id: 645fe059335c21d19f3bb84f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T21:25:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK firstly please switch to the <code>faster-llama</code> branch\
          \ of AutoGPTQ, ie do the following:</p>\n<pre><code>git clone https://github.com/PanQiWei/AutoGPTQ\n\
          cd AutoGPTQ\ngit checkout faster-llama\npip install .\n</code></pre>\n<p>Then\
          \ test this model with this code:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, pipeline, logging\n<span class=\"hljs-keyword\"\
          >from</span> auto_gptq <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\n<span class=\"hljs-keyword\">import</span> argparse\n\
          \nquantized_model_dir = <span class=\"hljs-string\">\"/workspace/models/TheBloke_stable-vicuna-13B-GPTQ\"\
          </span>\n\nmodel_basename = <span class=\"hljs-string\">\"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\
          </span>\n\nuse_strict = <span class=\"hljs-literal\">False</span>\n\nuse_triton\
          \ = <span class=\"hljs-literal\">False</span>\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=<span class=\"hljs-literal\">True</span>)\n\nquantize_config\
          \ = BaseQuantizeConfig(\n        bits=<span class=\"hljs-number\">4</span>,\n\
          \        group_size=<span class=\"hljs-number\">128</span>,\n        desc_act=<span\
          \ class=\"hljs-literal\">False</span>\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     strict=use_strict,\n        model_basename=model_basename,\n     \
          \   device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n        use_triton=use_triton,\n\
          \        quantize_config=quantize_config)\n\n<span class=\"hljs-comment\"\
          ># Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = <span\
          \ class=\"hljs-string\">\"Tell me about AI\"</span>\nprompt_template=<span\
          \ class=\"hljs-string\">f'''### Human: <span class=\"hljs-subst\">{prompt}</span></span>\n\
          <span class=\"hljs-string\">### Assistant:'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe\
          \ = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"</span>,\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"\
          hljs-number\">512</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n</code></pre>\n<p>Example output:</p>\n\
          <pre><code>root@d2264a66ab0b:/workspace# python simple_gptq_anon.py\n***\
          \ Pipeline:\n### Human: Tell me about AI\n### Assistant: Artificial Intelligence\
          \ (AI) is a field of computer science that focuses on creating intelligent\
          \ machines that can think and act like humans. It involves the development\
          \ of algorithms, models, and systems that enable computers to learn from\
          \ data, make decisions, and perform tasks without human intervention. Some\
          \ examples of AI include natural language processing, machine learning,\
          \ robotics, and computer vision. The potential applications of AI are vast\
          \ and range from healthcare and finance to transportation and entertainment.\
          \ However, there are also concerns around the impact of AI on employment\
          \ and privacy.\n\n\n*** Generate:\n&lt;s&gt; ### Human: Tell me about AI\n\
          ### Assistant: AI stands for Artificial Intelligence. It is a field of computer\
          \ science that focuses on creating intelligent machines that can think and\
          \ act like humans. AI involves the development of algorithms and models\
          \ that enable computers to learn from data, make predictions, and take actions\
          \ based on that data. AI is used in a variety of applications, including\
          \ natural language processing, computer vision, robotics, and more.\n\n\
          Some of the key areas of AI research include:\n\n- Machine learning: This\
          \ involves the development of algorithms that enable computers to learn\
          \ from data and make predictions or take actions based on that data.\n\n\
          - Natural language processing: This involves the development of algorithms\
          \ that enable computers to understand and generate human language.\n\n-\
          \ Computer vision: This involves the development of algorithms that enable\
          \ computers to interpret and analyze visual data, such as images and videos.\n\
          \n- Robotics: This involves the development of algorithms and models that\
          \ enable robots to perform tasks autonomously.\n\n- AI ethics: This involves\
          \ the study of the ethical implications of AI, such as the potential impact\
          \ of AI on society and the need for ethical guidelines in AI development.\n\
          \nOverall, AI has the potential to revolutionize many industries and transform\
          \ the way we live and work. However, there are also concerns about the potential\
          \ negative impacts of AI, such as job displacement and the potential for\
          \ AI to be used for malicious purposes.&lt;/s&gt;\n</code></pre>\n<p>Same\
          \ should work for the old Vicuna model - but yes you have to rename to .bin\
          \ (or symlink).  I will eventually do that in the repo itself - or probably\
          \ just replace the model with a .safetensors instead.</p>\n"
        raw: "OK firstly please switch to the `faster-llama` branch of AutoGPTQ, ie\
          \ do the following:\n\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\n\
          cd AutoGPTQ\ngit checkout faster-llama\npip install .\n```\n\nThen test\
          \ this model with this code:\n```python\nfrom transformers import AutoTokenizer,\
          \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          import argparse\n\nquantized_model_dir = \"/workspace/models/TheBloke_stable-vicuna-13B-GPTQ\"\
          \n\nmodel_basename = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\
          \n\nuse_strict = False\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\nquantize_config = BaseQuantizeConfig(\n        bits=4,\n\
          \        group_size=128,\n        desc_act=False\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        strict=use_strict,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=quantize_config)\n\
          \n# Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me\
          \ about AI\"\nprompt_template=f'''### Human: {prompt}\n### Assistant:'''\n\
          \nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n\
          \    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n \
          \   temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\
          print(pipe(prompt_template)[0]['generated_text'])\n\nprint(\"\\n\\n*** Generate:\"\
          )\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
          print(tokenizer.decode(output[0]))\n```\n\nExample output:\n```\nroot@d2264a66ab0b:/workspace#\
          \ python simple_gptq_anon.py\n*** Pipeline:\n### Human: Tell me about AI\n\
          ### Assistant: Artificial Intelligence (AI) is a field of computer science\
          \ that focuses on creating intelligent machines that can think and act like\
          \ humans. It involves the development of algorithms, models, and systems\
          \ that enable computers to learn from data, make decisions, and perform\
          \ tasks without human intervention. Some examples of AI include natural\
          \ language processing, machine learning, robotics, and computer vision.\
          \ The potential applications of AI are vast and range from healthcare and\
          \ finance to transportation and entertainment. However, there are also concerns\
          \ around the impact of AI on employment and privacy.\n\n\n*** Generate:\n\
          <s> ### Human: Tell me about AI\n### Assistant: AI stands for Artificial\
          \ Intelligence. It is a field of computer science that focuses on creating\
          \ intelligent machines that can think and act like humans. AI involves the\
          \ development of algorithms and models that enable computers to learn from\
          \ data, make predictions, and take actions based on that data. AI is used\
          \ in a variety of applications, including natural language processing, computer\
          \ vision, robotics, and more.\n\nSome of the key areas of AI research include:\n\
          \n- Machine learning: This involves the development of algorithms that enable\
          \ computers to learn from data and make predictions or take actions based\
          \ on that data.\n\n- Natural language processing: This involves the development\
          \ of algorithms that enable computers to understand and generate human language.\n\
          \n- Computer vision: This involves the development of algorithms that enable\
          \ computers to interpret and analyze visual data, such as images and videos.\n\
          \n- Robotics: This involves the development of algorithms and models that\
          \ enable robots to perform tasks autonomously.\n\n- AI ethics: This involves\
          \ the study of the ethical implications of AI, such as the potential impact\
          \ of AI on society and the need for ethical guidelines in AI development.\n\
          \nOverall, AI has the potential to revolutionize many industries and transform\
          \ the way we live and work. However, there are also concerns about the potential\
          \ negative impacts of AI, such as job displacement and the potential for\
          \ AI to be used for malicious purposes.</s>\n```\n\nSame should work for\
          \ the old Vicuna model - but yes you have to rename to .bin (or symlink).\
          \  I will eventually do that in the repo itself - or probably just replace\
          \ the model with a .safetensors instead."
        updatedAt: '2023-05-13T21:25:32.054Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
        - joshlevy89
      relatedEventId: 6460004cd27fa024aa88013d
    id: 6460004cd27fa024aa88013c
    type: comment
  author: TheBloke
  content: "OK firstly please switch to the `faster-llama` branch of AutoGPTQ, ie\
    \ do the following:\n\n```\ngit clone https://github.com/PanQiWei/AutoGPTQ\ncd\
    \ AutoGPTQ\ngit checkout faster-llama\npip install .\n```\n\nThen test this model\
    \ with this code:\n```python\nfrom transformers import AutoTokenizer, pipeline,\
    \ logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport\
    \ argparse\n\nquantized_model_dir = \"/workspace/models/TheBloke_stable-vicuna-13B-GPTQ\"\
    \n\nmodel_basename = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order\"\n\nuse_strict\
    \ = False\n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n\nquantize_config = BaseQuantizeConfig(\n        bits=4,\n \
    \       group_size=128,\n        desc_act=False\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        strict=use_strict,\n        model_basename=model_basename,\n\
    \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=quantize_config)\n\
    \n# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n\
    logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''###\
    \ Human: {prompt}\n### Assistant:'''\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n```\n\nExample output:\n```\nroot@d2264a66ab0b:/workspace#\
    \ python simple_gptq_anon.py\n*** Pipeline:\n### Human: Tell me about AI\n###\
    \ Assistant: Artificial Intelligence (AI) is a field of computer science that\
    \ focuses on creating intelligent machines that can think and act like humans.\
    \ It involves the development of algorithms, models, and systems that enable computers\
    \ to learn from data, make decisions, and perform tasks without human intervention.\
    \ Some examples of AI include natural language processing, machine learning, robotics,\
    \ and computer vision. The potential applications of AI are vast and range from\
    \ healthcare and finance to transportation and entertainment. However, there are\
    \ also concerns around the impact of AI on employment and privacy.\n\n\n*** Generate:\n\
    <s> ### Human: Tell me about AI\n### Assistant: AI stands for Artificial Intelligence.\
    \ It is a field of computer science that focuses on creating intelligent machines\
    \ that can think and act like humans. AI involves the development of algorithms\
    \ and models that enable computers to learn from data, make predictions, and take\
    \ actions based on that data. AI is used in a variety of applications, including\
    \ natural language processing, computer vision, robotics, and more.\n\nSome of\
    \ the key areas of AI research include:\n\n- Machine learning: This involves the\
    \ development of algorithms that enable computers to learn from data and make\
    \ predictions or take actions based on that data.\n\n- Natural language processing:\
    \ This involves the development of algorithms that enable computers to understand\
    \ and generate human language.\n\n- Computer vision: This involves the development\
    \ of algorithms that enable computers to interpret and analyze visual data, such\
    \ as images and videos.\n\n- Robotics: This involves the development of algorithms\
    \ and models that enable robots to perform tasks autonomously.\n\n- AI ethics:\
    \ This involves the study of the ethical implications of AI, such as the potential\
    \ impact of AI on society and the need for ethical guidelines in AI development.\n\
    \nOverall, AI has the potential to revolutionize many industries and transform\
    \ the way we live and work. However, there are also concerns about the potential\
    \ negative impacts of AI, such as job displacement and the potential for AI to\
    \ be used for malicious purposes.</s>\n```\n\nSame should work for the old Vicuna\
    \ model - but yes you have to rename to .bin (or symlink).  I will eventually\
    \ do that in the repo itself - or probably just replace the model with a .safetensors\
    \ instead."
  created_at: 2023-05-13 20:25:32+00:00
  edited: false
  hidden: false
  id: 6460004cd27fa024aa88013c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T21:25:32.000Z'
    data:
      status: closed
    id: 6460004cd27fa024aa88013d
    type: status-change
  author: TheBloke
  created_at: 2023-05-13 20:25:32+00:00
  id: 6460004cd27fa024aa88013d
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T21:25:35.000Z'
    data:
      status: open
    id: 6460004f4357049a57f79b1d
    type: status-change
  author: TheBloke
  created_at: 2023-05-13 20:25:35+00:00
  id: 6460004f4357049a57f79b1d
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T21:25:47.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>(sorry didn''t mean to close the thread there)</p>

          '
        raw: (sorry didn't mean to close the thread there)
        updatedAt: '2023-05-13T21:25:47.255Z'
      numEdits: 0
      reactions: []
    id: 6460005b4357049a57f79b99
    type: comment
  author: TheBloke
  content: (sorry didn't mean to close the thread there)
  created_at: 2023-05-13 20:25:47+00:00
  edited: false
  hidden: false
  id: 6460005b4357049a57f79b99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e97b8ff5a966164091ad5195341c8e5d.svg
      fullname: Nikita
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nikitastaf1996
      type: user
    createdAt: '2023-05-14T07:04:59.000Z'
    data:
      edited: false
      editors:
      - nikitastaf1996
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e97b8ff5a966164091ad5195341c8e5d.svg
          fullname: Nikita
          isHf: false
          isPro: false
          name: nikitastaf1996
          type: user
        html: '<p>Thank you. Been encountering same mistake.</p>

          '
        raw: Thank you. Been encountering same mistake.
        updatedAt: '2023-05-14T07:04:59.193Z'
      numEdits: 0
      reactions: []
    id: 6460881b7735f76a4a4cf227
    type: comment
  author: nikitastaf1996
  content: Thank you. Been encountering same mistake.
  created_at: 2023-05-14 06:04:59+00:00
  edited: false
  hidden: false
  id: 6460881b7735f76a4a4cf227
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
      fullname: Joshua Levy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joshlevy89
      type: user
    createdAt: '2023-05-15T19:25:16.000Z'
    data:
      edited: true
      editors:
      - joshlevy89
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
          fullname: Joshua Levy
          isHf: false
          isPro: false
          name: joshlevy89
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Major thanks!!\
          \ It worked</p>\n<p>A few notes for future readers: (1) faster-llama has\
          \ been merged to mainline (2) if using use_safetensors, you now don't need\
          \ to append .bin to the actual file name...you do need to remove \".safetensors\"\
          \ from the model_basename because the code auto adds it (3) to get vanilla\
          \ vicuna (i.e. not stable-vicuna) to work, I had to change desc_act to True\
          \ in BaseQuantizeConfig...this is a variable to mindful of.</p>\n"
        raw: '@TheBloke Major thanks!! It worked


          A few notes for future readers: (1) faster-llama has been merged to mainline
          (2) if using use_safetensors, you now don''t need to append .bin to the
          actual file name...you do need to remove ".safetensors" from the model_basename
          because the code auto adds it (3) to get vanilla vicuna (i.e. not stable-vicuna)
          to work, I had to change desc_act to True in BaseQuantizeConfig...this is
          a variable to mindful of.'
        updatedAt: '2023-05-15T19:25:32.716Z'
      numEdits: 1
      reactions: []
    id: 6462871c02de01c83c0d1db0
    type: comment
  author: joshlevy89
  content: '@TheBloke Major thanks!! It worked


    A few notes for future readers: (1) faster-llama has been merged to mainline (2)
    if using use_safetensors, you now don''t need to append .bin to the actual file
    name...you do need to remove ".safetensors" from the model_basename because the
    code auto adds it (3) to get vanilla vicuna (i.e. not stable-vicuna) to work,
    I had to change desc_act to True in BaseQuantizeConfig...this is a variable to
    mindful of.'
  created_at: 2023-05-15 18:25:16+00:00
  edited: true
  hidden: false
  id: 6462871c02de01c83c0d1db0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-15T19:29:14.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah re 3 that''s my fault for having two model files in one repo,
          with only one quantize_config.json.  On all my recent repos I use separate
          branches for separate model files.</p>

          <p>I keep meaning to clean that up and will do soon.</p>

          <p>Glad you got it working!</p>

          '
        raw: 'Yeah re 3 that''s my fault for having two model files in one repo, with
          only one quantize_config.json.  On all my recent repos I use separate branches
          for separate model files.


          I keep meaning to clean that up and will do soon.


          Glad you got it working!'
        updatedAt: '2023-05-15T19:29:14.358Z'
      numEdits: 0
      reactions: []
    id: 6462880a48e13890ea54911a
    type: comment
  author: TheBloke
  content: 'Yeah re 3 that''s my fault for having two model files in one repo, with
    only one quantize_config.json.  On all my recent repos I use separate branches
    for separate model files.


    I keep meaning to clean that up and will do soon.


    Glad you got it working!'
  created_at: 2023-05-15 18:29:14+00:00
  edited: false
  hidden: false
  id: 6462880a48e13890ea54911a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
      fullname: Joshua Levy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joshlevy89
      type: user
    createdAt: '2023-05-15T19:35:22.000Z'
    data:
      status: closed
    id: 6462897a02de01c83c0d2f63
    type: status-change
  author: joshlevy89
  created_at: 2023-05-15 18:35:22+00:00
  id: 6462897a02de01c83c0d2f63
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'Success using AutoGPTQ? '
