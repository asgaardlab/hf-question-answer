!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Andyrasika
conflicting_files: null
created_at: 2023-08-15 02:19:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
      fullname: Ankush Singal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Andyrasika
      type: user
    createdAt: '2023-08-15T03:19:19.000Z'
    data:
      edited: false
      editors:
      - Andyrasika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9181908369064331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6290ec00a29097b211b94f0f/7OYU-yroOUxJryJkl33L6.png?w=200&h=200&f=face
          fullname: Ankush Singal
          isHf: false
          isPro: false
          name: Andyrasika
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ ,<br>     Thanks for the amazing model. Is there a way to quatize this\
          \ model to run on 4 bit?<br>Looking forward to hearing from you.<br>Thanks,<br>Andy</p>\n"
        raw: "Hey @TheBloke ,\r\n     Thanks for the amazing model. Is there a way\
          \ to quatize this model to run on 4 bit?\r\nLooking forward to hearing from\
          \ you.\r\nThanks,\r\nAndy"
        updatedAt: '2023-08-15T03:19:19.151Z'
      numEdits: 0
      reactions: []
    id: 64daeeb75f144aa29fee56b4
    type: comment
  author: Andyrasika
  content: "Hey @TheBloke ,\r\n     Thanks for the amazing model. Is there a way to\
    \ quatize this model to run on 4 bit?\r\nLooking forward to hearing from you.\r\
    \nThanks,\r\nAndy"
  created_at: 2023-08-15 02:19:19+00:00
  edited: false
  hidden: false
  id: 64daeeb75f144aa29fee56b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-15T07:10:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9433451890945435
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Well yes - this repo IS a 4-bit quantisation :)  So you can use
          this repo with AutoGPTQ or (very soon) Transformers, when it adds native
          GPTQ support.   </p>

          <p>Or you can use my stable-vicuna-13B-HF repo with bitsandbytes and <code>load_in_4bit=True</code>
          for on-the-fly 4bit quantisation.</p>

          <p>Or you can use one of the files in my stable-vicuna-13B-GGML repo with
          llama-cpp-python or ctransformers - there are multiple quantisation sizes
          in that repo, including 4-bit and bigger and smaller sizes.</p>

          <p>All that said, this is an old model and there are much better models
          released since then. All the recent Llama models I''ve uploaded are based
          on Llama 2 so they are commercially licensed, have greater context size
          (4096 vs 2048), and their base models were trained on twice as many tokens
          (2T vs 1T).  So I no longer recommend this much older Llama 1 model.</p>

          '
        raw: "Well yes - this repo IS a 4-bit quantisation :)  So you can use this\
          \ repo with AutoGPTQ or (very soon) Transformers, when it adds native GPTQ\
          \ support.   \n\nOr you can use my stable-vicuna-13B-HF repo with bitsandbytes\
          \ and `load_in_4bit=True` for on-the-fly 4bit quantisation.\n\nOr you can\
          \ use one of the files in my stable-vicuna-13B-GGML repo with llama-cpp-python\
          \ or ctransformers - there are multiple quantisation sizes in that repo,\
          \ including 4-bit and bigger and smaller sizes.\n\nAll that said, this is\
          \ an old model and there are much better models released since then. All\
          \ the recent Llama models I've uploaded are based on Llama 2 so they are\
          \ commercially licensed, have greater context size (4096 vs 2048), and their\
          \ base models were trained on twice as many tokens (2T vs 1T).  So I no\
          \ longer recommend this much older Llama 1 model."
        updatedAt: '2023-08-15T07:10:24.227Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Andyrasika
    id: 64db24e095da0aa95cfda598
    type: comment
  author: TheBloke
  content: "Well yes - this repo IS a 4-bit quantisation :)  So you can use this repo\
    \ with AutoGPTQ or (very soon) Transformers, when it adds native GPTQ support.\
    \   \n\nOr you can use my stable-vicuna-13B-HF repo with bitsandbytes and `load_in_4bit=True`\
    \ for on-the-fly 4bit quantisation.\n\nOr you can use one of the files in my stable-vicuna-13B-GGML\
    \ repo with llama-cpp-python or ctransformers - there are multiple quantisation\
    \ sizes in that repo, including 4-bit and bigger and smaller sizes.\n\nAll that\
    \ said, this is an old model and there are much better models released since then.\
    \ All the recent Llama models I've uploaded are based on Llama 2 so they are commercially\
    \ licensed, have greater context size (4096 vs 2048), and their base models were\
    \ trained on twice as many tokens (2T vs 1T).  So I no longer recommend this much\
    \ older Llama 1 model."
  created_at: 2023-08-15 06:10:24+00:00
  edited: false
  hidden: false
  id: 64db24e095da0aa95cfda598
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 28
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: How to run qlora with stable vicuna?
