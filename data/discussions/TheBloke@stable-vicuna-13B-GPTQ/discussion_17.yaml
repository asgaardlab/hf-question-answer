!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yramshev
conflicting_files: null
created_at: 2023-05-11 13:35:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8618f4cfe3fa60534230a3e302826b5.svg
      fullname: Yordan Ramshev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yramshev
      type: user
    createdAt: '2023-05-11T14:35:45.000Z'
    data:
      edited: false
      editors:
      - yramshev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8618f4cfe3fa60534230a3e302826b5.svg
          fullname: Yordan Ramshev
          isHf: false
          isPro: false
          name: yramshev
          type: user
        html: '<p>loaded the model successfully, however prompts generate the following
          error: AttributeError: ''Offload_LlamaModel'' object has no attribute ''preload''</p>

          '
        raw: 'loaded the model successfully, however prompts generate the following
          error: AttributeError: ''Offload_LlamaModel'' object has no attribute ''preload'''
        updatedAt: '2023-05-11T14:35:45.327Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - wingzx3
        - keldun
    id: 645cfd4158f9ee3151450fdc
    type: comment
  author: yramshev
  content: 'loaded the model successfully, however prompts generate the following
    error: AttributeError: ''Offload_LlamaModel'' object has no attribute ''preload'''
  created_at: 2023-05-11 13:35:45+00:00
  edited: false
  hidden: false
  id: 645cfd4158f9ee3151450fdc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8618f4cfe3fa60534230a3e302826b5.svg
      fullname: Yordan Ramshev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yramshev
      type: user
    createdAt: '2023-05-11T14:42:20.000Z'
    data:
      edited: false
      editors:
      - yramshev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8618f4cfe3fa60534230a3e302826b5.svg
          fullname: Yordan Ramshev
          isHf: false
          isPro: false
          name: yramshev
          type: user
        html: '<p>Traceback (most recent call last):<br>  File "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\callbacks.py",
          line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 259, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\text-generation-webui\repositories\GPTQ-for-LLaMa\llama_inference_offload.py",
          line 135, in forward<br>    if idx &lt;= (self.preload - 1):<br>  File "D:\Documents\DEVELOPMENT\LanguageModels\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1614, in <strong>getattr</strong><br>    raise AttributeError("''{}''
          object has no attribute ''{}''".format(<br>AttributeError: ''Offload_LlamaModel''
          object has no attribute ''preload''</p>

          '
        raw: "Traceback (most recent call last):\n  File \"D:\\Documents\\DEVELOPMENT\\\
          LanguageModels\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 73, in gentask\n    ret = self.mfunc(callback=_callback,\
          \ **self.kwargs)\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 259, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\\
          _contextlib.py\", line 115, in decorate_context\n    return func(*args,\
          \ **kwargs)\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 1485, in generate\n    return self.sample(\n\
          \  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          generation\\utils.py\", line 2524, in sample\n    outputs = self(\n  File\
          \ \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\", line 687, in forward\n    outputs =\
          \ self.model(\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
          llama_inference_offload.py\", line 135, in forward\n    if idx <= (self.preload\
          \ - 1):\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1614, in __getattr__\n    raise AttributeError(\"\
          '{}' object has no attribute '{}'\".format(\nAttributeError: 'Offload_LlamaModel'\
          \ object has no attribute 'preload'"
        updatedAt: '2023-05-11T14:42:20.379Z'
      numEdits: 0
      reactions: []
    id: 645cfeccf36ed281fabc6c44
    type: comment
  author: yramshev
  content: "Traceback (most recent call last):\n  File \"D:\\Documents\\DEVELOPMENT\\\
    LanguageModels\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    modules\\callbacks.py\", line 73, in gentask\n    ret = self.mfunc(callback=_callback,\
    \ **self.kwargs)\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\\
    oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\", line\
    \ 259, in generate_with_callback\n    shared.model.generate(**kwargs)\n  File\
    \ \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line\
    \ 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"D:\\Documents\\\
    DEVELOPMENT\\LanguageModels\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1485, in generate\n\
    \    return self.sample(\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\\
    oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 2524, in sample\n    outputs = self(\n\
    \  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"D:\\\
    Documents\\DEVELOPMENT\\LanguageModels\\oobabooga_windows\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 687, in forward\n    outputs = self.model(\n  File \"D:\\Documents\\DEVELOPMENT\\\
    LanguageModels\\oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n   \
    \ return forward_call(*args, **kwargs)\n  File \"D:\\Documents\\DEVELOPMENT\\\
    LanguageModels\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\", line 135, in forward\n\
    \    if idx <= (self.preload - 1):\n  File \"D:\\Documents\\DEVELOPMENT\\LanguageModels\\\
    oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1614, in __getattr__\n    raise AttributeError(\"\
    '{}' object has no attribute '{}'\".format(\nAttributeError: 'Offload_LlamaModel'\
    \ object has no attribute 'preload'"
  created_at: 2023-05-11 13:42:20+00:00
  edited: false
  hidden: false
  id: 645cfeccf36ed281fabc6c44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e889702b747899cad6655bcf94df1dcf.svg
      fullname: Vladivillian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VladCorvi
      type: user
    createdAt: '2023-05-17T03:15:48.000Z'
    data:
      edited: false
      editors:
      - VladCorvi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e889702b747899cad6655bcf94df1dcf.svg
          fullname: Vladivillian
          isHf: false
          isPro: false
          name: VladCorvi
          type: user
        html: '<p>The same. Need a solution =/</p>

          '
        raw: The same. Need a solution =/
        updatedAt: '2023-05-17T03:15:48.990Z'
      numEdits: 0
      reactions: []
    id: 646446e4dae3c8a327c3dca5
    type: comment
  author: VladCorvi
  content: The same. Need a solution =/
  created_at: 2023-05-17 02:15:48+00:00
  edited: false
  hidden: false
  id: 646446e4dae3c8a327c3dca5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4e825943d37a1f000b59f97b80dbe2.svg
      fullname: Alexandre Ochiai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alochiai
      type: user
    createdAt: '2023-05-17T03:21:42.000Z'
    data:
      edited: false
      editors:
      - alochiai
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4e825943d37a1f000b59f97b80dbe2.svg
          fullname: Alexandre Ochiai
          isHf: false
          isPro: false
          name: alochiai
          type: user
        html: '<p>found a solution... change the GPTQ repository for the old-cuda
          branch, install it again.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63cc63d4336d9d561711b4b2/E6aYy-DL4NoyLlhHKvdTN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63cc63d4336d9d561711b4b2/E6aYy-DL4NoyLlhHKvdTN.png"></a></p>

          '
        raw: 'found a solution... change the GPTQ repository for the old-cuda branch,
          install it again.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63cc63d4336d9d561711b4b2/E6aYy-DL4NoyLlhHKvdTN.png)'
        updatedAt: '2023-05-17T03:21:42.298Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - VladCorvi
    id: 646448466dad99445def2479
    type: comment
  author: alochiai
  content: 'found a solution... change the GPTQ repository for the old-cuda branch,
    install it again.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63cc63d4336d9d561711b4b2/E6aYy-DL4NoyLlhHKvdTN.png)'
  created_at: 2023-05-17 02:21:42+00:00
  edited: false
  hidden: false
  id: 646448466dad99445def2479
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/037daa7709e3513c9a91a754d2a5200b.svg
      fullname: illia kr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ilyxa3d
      type: user
    createdAt: '2023-06-01T18:26:28.000Z'
    data:
      edited: false
      editors:
      - ilyxa3d
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/037daa7709e3513c9a91a754d2a5200b.svg
          fullname: illia kr
          isHf: false
          isPro: false
          name: ilyxa3d
          type: user
        html: '<p>If you are using <strong>oobabooga/one-click-installers</strong>,
          you can change GPTQ  branch in <strong>webui.py</strong> file, from "-b
          cuda" to "-b old-cuda", and run update script</p>

          '
        raw: If you are using **oobabooga/one-click-installers**, you can change GPTQ  branch
          in **webui.py** file, from "-b cuda" to "-b old-cuda", and run update script
        updatedAt: '2023-06-01T18:26:28.856Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Hellsadvocate
    id: 6478e2d442b1805ae2ada6c0
    type: comment
  author: ilyxa3d
  content: If you are using **oobabooga/one-click-installers**, you can change GPTQ  branch
    in **webui.py** file, from "-b cuda" to "-b old-cuda", and run update script
  created_at: 2023-06-01 17:26:28+00:00
  edited: false
  hidden: false
  id: 6478e2d442b1805ae2ada6c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c0d30f466356dd57ac1d863a7259693.svg
      fullname: Gideon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AIDesignerTester
      type: user
    createdAt: '2023-06-15T16:21:10.000Z'
    data:
      edited: false
      editors:
      - AIDesignerTester
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.38694652915000916
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c0d30f466356dd57ac1d863a7259693.svg
          fullname: Gideon
          isHf: false
          isPro: false
          name: AIDesignerTester
          type: user
        html: '<p>I''ve tried the "-b old-cuda" still doesn''t work. I did get it
          to work once after that getting this error:</p>

          <p>Traceback (most recent call last):<br>  File "C:\oobabooga_windows\text-generation-webui\modules\callbacks.py",
          line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "C:\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 263, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "C:\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\modeling_base.py",
          line 423, in generate<br>    return self.model.generate(**kwargs)<br>  File
          "C:\oobabooga_windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "C:\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1572, in generate<br>    return self.sample(<br>  File "C:\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2619, in sample<br>    outputs = self(<br>  File "C:\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl    return forward_call(*args, **kwargs)<br>  File
          "C:\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 688, in forward<br>    outputs = self.model(<br>  File "C:\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl    return forward_call(*args, **kwargs)<br>  File
          "C:\oobabooga_windows\text-generation-webui\repositories\GPTQ-for-LLaMa\llama_inference_offload.py",
          line 135, in forward<br>    if idx &lt;= (self.preload - 1):<br>  File "C:\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1614, in <strong>getattr</strong><br>    raise AttributeError("''{}''
          object has no attribute ''{}''".format(<br>AttributeError: ''Offload_LlamaModel''
          object has no attribute ''preload''<br>Output generated in 0.43 seconds
          (0.00 tokens/s, 0 tokens, context 1397, seed 2017233876)</p>

          '
        raw: "I've tried the \"-b old-cuda\" still doesn't work. I did get it to work\
          \ once after that getting this error:\n\nTraceback (most recent call last):\n\
          \  File \"C:\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
          , line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"C:\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 263, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          auto_gptq\\modeling\\_base.py\", line 423, in generate\n    return self.model.generate(**kwargs)\n\
          \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return\
          \ func(*args, **kwargs)\n  File \"C:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1572,\
          \ in generate\n    return self.sample(\n  File \"C:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"C:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl    return forward_call(*args, **kwargs)\n  File\
          \ \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\", line 688, in forward\n    outputs =\
          \ self.model(\n  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\
          \    return forward_call(*args, **kwargs)\n  File \"C:\\oobabooga_windows\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\"\
          , line 135, in forward\n    if idx <= (self.preload - 1):\n  File \"C:\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1614, in __getattr__\n    raise AttributeError(\"\
          '{}' object has no attribute '{}'\".format(\nAttributeError: 'Offload_LlamaModel'\
          \ object has no attribute 'preload'\nOutput generated in 0.43 seconds (0.00\
          \ tokens/s, 0 tokens, context 1397, seed 2017233876)"
        updatedAt: '2023-06-15T16:21:10.513Z'
      numEdits: 0
      reactions: []
    id: 648b3a76dd8c4d00e256fbbe
    type: comment
  author: AIDesignerTester
  content: "I've tried the \"-b old-cuda\" still doesn't work. I did get it to work\
    \ once after that getting this error:\n\nTraceback (most recent call last):\n\
    \  File \"C:\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
    , line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
    \  File \"C:\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 263, in generate_with_callback\n    shared.model.generate(**kwargs)\n \
    \ File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
    modeling\\_base.py\", line 423, in generate\n    return self.model.generate(**kwargs)\n\
    \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
    \ **kwargs)\n  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 1572, in generate\n    return self.sample(\n\
    \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 2619, in sample\n    outputs = self(\n  File \"C:\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl    return forward_call(*args, **kwargs)\n\
    \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 688, in forward\n    outputs = self.model(\n\
    \  File \"C:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl    return forward_call(*args,\
    \ **kwargs)\n  File \"C:\\oobabooga_windows\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\llama_inference_offload.py\", line 135, in forward\n    if idx\
    \ <= (self.preload - 1):\n  File \"C:\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1614, in __getattr__\n\
    \    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\nAttributeError:\
    \ 'Offload_LlamaModel' object has no attribute 'preload'\nOutput generated in\
    \ 0.43 seconds (0.00 tokens/s, 0 tokens, context 1397, seed 2017233876)"
  created_at: 2023-06-15 15:21:10+00:00
  edited: false
  hidden: false
  id: 648b3a76dd8c4d00e256fbbe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-15T19:13:21.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9133284091949463
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You should update to the latest version of text-generation-webui,
          and then launch it.  That is enough to get the model working.</p>

          <p>These errors indicate you''re either running a really old version of
          textgen, or else you''re trying to use GPTQ-for-LLaMA still, eg with <code>--wbits
          4 --model_type llama --groupsize 128</code> arguments, or by setting bits
          = 4, model_type = llama in the UI</p>

          <p>None of that is needed any more as now AutoGPTQ is default and that will
          auto load the model</p>

          <p>I hadn''t updated the README of this model to reflect that, so it was
          still telling people to set GPTQ params. I''ve edited that now</p>

          '
        raw: 'You should update to the latest version of text-generation-webui, and
          then launch it.  That is enough to get the model working.


          These errors indicate you''re either running a really old version of textgen,
          or else you''re trying to use GPTQ-for-LLaMA still, eg with `--wbits 4 --model_type
          llama --groupsize 128` arguments, or by setting bits = 4, model_type = llama
          in the UI


          None of that is needed any more as now AutoGPTQ is default and that will
          auto load the model


          I hadn''t updated the README of this model to reflect that, so it was still
          telling people to set GPTQ params. I''ve edited that now'
        updatedAt: '2023-06-15T19:13:21.196Z'
      numEdits: 0
      reactions: []
    id: 648b62d1df4710674c6384d9
    type: comment
  author: TheBloke
  content: 'You should update to the latest version of text-generation-webui, and
    then launch it.  That is enough to get the model working.


    These errors indicate you''re either running a really old version of textgen,
    or else you''re trying to use GPTQ-for-LLaMA still, eg with `--wbits 4 --model_type
    llama --groupsize 128` arguments, or by setting bits = 4, model_type = llama in
    the UI


    None of that is needed any more as now AutoGPTQ is default and that will auto
    load the model


    I hadn''t updated the README of this model to reflect that, so it was still telling
    people to set GPTQ params. I''ve edited that now'
  created_at: 2023-06-15 18:13:21+00:00
  edited: false
  hidden: false
  id: 648b62d1df4710674c6384d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-06-17T23:30:52.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3473556637763977
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: '<p>I updated Ooba today and I still get this error. Loading the model
          works, but after sending an input, this error comes up.</p>

          <p>2023-06-18 00:28:58 INFO:Loaded the model in 3.20 seconds.</p>

          <p>Traceback (most recent call last):<br>  File "F:\Programme\oobabooga_windows\text-generation-webui\modules\callbacks.py",
          line 74, in gentask<br>    ret = self.mfunc(callback=_callback, *args, **self.kwargs)<br>  File
          "F:\Programme\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 263, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\modeling_base.py",
          line 423, in generate<br>    return self.model.generate(**kwargs)<br>  File
          "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1572, in generate<br>    return self.sample(<br>  File "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2619, in sample<br>    outputs = self(<br>  File "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 688, in forward<br>    outputs = self.model(<br>  File "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "F:\Programme\oobabooga_windows\text-generation-webui\repositories\GPTQ-for-LLaMa\llama_inference_offload.py",
          line 135, in forward<br>    if idx &lt;= (self.preload - 1):<br>  File "F:\Programme\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1614, in <strong>getattr</strong><br>    raise AttributeError("''{}''
          object has no attribute ''{}''".format(<br>AttributeError: ''Offload_LlamaModel''
          object has no attribute ''preload''<br>Output generated in 0.27 seconds
          (0.00 tokens/s, 0 tokens, context 1074, seed 2123655593)</p>

          '
        raw: "I updated Ooba today and I still get this error. Loading the model works,\
          \ but after sending an input, this error comes up.\n\n2023-06-18 00:28:58\
          \ INFO:Loaded the model in 3.20 seconds.\n\nTraceback (most recent call\
          \ last):\n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 74, in gentask\n    ret = self.mfunc(callback=_callback,\
          \ *args, **self.kwargs)\n  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\\
          modules\\text_generation.py\", line 263, in generate_with_callback\n   \
          \ shared.model.generate(**kwargs)\n  File \"F:\\Programme\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling\\_base.py\"\
          , line 423, in generate\n    return self.model.generate(**kwargs)\n  File\
          \ \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return\
          \ func(*args, **kwargs)\n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1572,\
          \ in generate\n    return self.sample(\n  File \"F:\\Programme\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"F:\\Programme\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 688, in forward\n\
          \    outputs = self.model(\n  File \"F:\\Programme\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\repositories\\\
          GPTQ-for-LLaMa\\llama_inference_offload.py\", line 135, in forward\n   \
          \ if idx <= (self.preload - 1):\n  File \"F:\\Programme\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1614, in __getattr__\n    raise AttributeError(\"'{}' object has\
          \ no attribute '{}'\".format(\nAttributeError: 'Offload_LlamaModel' object\
          \ has no attribute 'preload'\nOutput generated in 0.27 seconds (0.00 tokens/s,\
          \ 0 tokens, context 1074, seed 2123655593)\n"
        updatedAt: '2023-06-17T23:30:52.644Z'
      numEdits: 0
      reactions: []
    id: 648e422cef352f79d8a40be8
    type: comment
  author: StableDiffusion69
  content: "I updated Ooba today and I still get this error. Loading the model works,\
    \ but after sending an input, this error comes up.\n\n2023-06-18 00:28:58 INFO:Loaded\
    \ the model in 3.20 seconds.\n\nTraceback (most recent call last):\n  File \"\
    F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
    , line 74, in gentask\n    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\n\
    \  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 263, in generate_with_callback\n    shared.model.generate(**kwargs)\n \
    \ File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    auto_gptq\\modeling\\_base.py\", line 423, in generate\n    return self.model.generate(**kwargs)\n\
    \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
    \ **kwargs)\n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\generation\\utils.py\", line 1572, in generate\n\
    \    return self.sample(\n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2619, in sample\n\
    \    outputs = self(\n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
    \    return forward_call(*args, **kwargs)\n  File \"F:\\Programme\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 688, in forward\n    outputs = self.model(\n  File \"F:\\Programme\\oobabooga_windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"F:\\\
    Programme\\oobabooga_windows\\text-generation-webui\\repositories\\GPTQ-for-LLaMa\\\
    llama_inference_offload.py\", line 135, in forward\n    if idx <= (self.preload\
    \ - 1):\n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1614, in __getattr__\n  \
    \  raise AttributeError(\"'{}' object has no attribute '{}'\".format(\nAttributeError:\
    \ 'Offload_LlamaModel' object has no attribute 'preload'\nOutput generated in\
    \ 0.27 seconds (0.00 tokens/s, 0 tokens, context 1074, seed 2123655593)\n"
  created_at: 2023-06-17 22:30:52+00:00
  edited: false
  hidden: false
  id: 648e422cef352f79d8a40be8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T10:13:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9206844568252563
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Somehow it''s still trying to load GPTQ-for-LLaMA.  Show a screenshot
          of your Models tab</p>

          '
        raw: Somehow it's still trying to load GPTQ-for-LLaMA.  Show a screenshot
          of your Models tab
        updatedAt: '2023-06-20T10:13:54.102Z'
      numEdits: 0
      reactions: []
    id: 64917be2d0aa14078dec6f26
    type: comment
  author: TheBloke
  content: Somehow it's still trying to load GPTQ-for-LLaMA.  Show a screenshot of
    your Models tab
  created_at: 2023-06-20 09:13:54+00:00
  edited: false
  hidden: false
  id: 64917be2d0aa14078dec6f26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-06-20T13:17:42.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7756924629211426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: '<blockquote>

          <p>Somehow it''s still trying to load GPTQ-for-LLaMA.  Show a screenshot
          of your Models tab</p>

          </blockquote>

          <p>OK, here ... thanks for looking at it.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/dfoWWNwcsBZIYN1RtoyNK.png"><img
          alt="Zwischenablagebild (3).png" src="https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/dfoWWNwcsBZIYN1RtoyNK.png"></a></p>

          '
        raw: '> Somehow it''s still trying to load GPTQ-for-LLaMA.  Show a screenshot
          of your Models tab


          OK, here ... thanks for looking at it.


          ![Zwischenablagebild (3).png](https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/dfoWWNwcsBZIYN1RtoyNK.png)

          '
        updatedAt: '2023-06-20T13:17:42.704Z'
      numEdits: 0
      reactions: []
    id: 6491a6f67eba9e8abf34d3f1
    type: comment
  author: StableDiffusion69
  content: '> Somehow it''s still trying to load GPTQ-for-LLaMA.  Show a screenshot
    of your Models tab


    OK, here ... thanks for looking at it.


    ![Zwischenablagebild (3).png](https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/dfoWWNwcsBZIYN1RtoyNK.png)

    '
  created_at: 2023-06-20 12:17:42+00:00
  edited: false
  hidden: false
  id: 6491a6f67eba9e8abf34d3f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-20T13:20:23.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8735259175300598
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry I meant the whole of the Models tab, not just the Model dropdown.  Please
          select this model in the Model dropdown, and then show me a full screenshot
          of everything you see on screen on that tab.</p>

          '
        raw: Sorry I meant the whole of the Models tab, not just the Model dropdown.  Please
          select this model in the Model dropdown, and then show me a full screenshot
          of everything you see on screen on that tab.
        updatedAt: '2023-06-20T13:20:23.061Z'
      numEdits: 0
      reactions: []
    id: 6491a7977eba9e8abf34e00c
    type: comment
  author: TheBloke
  content: Sorry I meant the whole of the Models tab, not just the Model dropdown.  Please
    select this model in the Model dropdown, and then show me a full screenshot of
    everything you see on screen on that tab.
  created_at: 2023-06-20 12:20:23+00:00
  edited: false
  hidden: false
  id: 6491a7977eba9e8abf34e00c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-06-21T16:25:51.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.583215057849884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: '<p>Ah, OK, no problem, here it is:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/YEc4LAxMU9Tyc9rIWOvIu.png"><img
          alt="Zwischenablagebild.png" src="https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/YEc4LAxMU9Tyc9rIWOvIu.png"></a></p>

          <p>In the console this comes up:<br>To create a public link, set <code>share=True</code>
          in <code>launch()</code>.<br>2023-06-21 17:21:37 INFO:Loading TheBloke_stable-vicuna-13B-GPTQ...<br>2023-06-21
          17:21:37 INFO:The AutoGPTQ params are: {''model_basename'': ''stable-vicuna-13b-gptq-4bit.compat.no-act-order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': None, ''quantize_config'': None}<br>2023-06-21 17:21:38
          WARNING:The model weights are not tied. Please use the <code>tie_weights</code>
          method before using the <code>infer_auto_device</code> function.<br>2023-06-21
          17:21:38 WARNING:The safetensors archive passed at models\TheBloke_stable-vicuna-13B-GPTQ\stable-vicuna-13b-gptq-4bit.compat.no-act-order.safetensors
          does not contain metadata. Make sure to save your model with the <code>save_pretrained</code>
          method. Defaulting to ''pt'' metadata.</p>

          <p>I maybe should mention, that I have a 8GB VRAM card. I''m not sure if
          this is relevant for this error.</p>

          '
        raw: 'Ah, OK, no problem, here it is:


          ![Zwischenablagebild.png](https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/YEc4LAxMU9Tyc9rIWOvIu.png)


          In the console this comes up:

          To create a public link, set `share=True` in `launch()`.

          2023-06-21 17:21:37 INFO:Loading TheBloke_stable-vicuna-13B-GPTQ...

          2023-06-21 17:21:37 INFO:The AutoGPTQ params are: {''model_basename'': ''stable-vicuna-13b-gptq-4bit.compat.no-act-order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          True, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': None, ''quantize_config'': None}

          2023-06-21 17:21:38 WARNING:The model weights are not tied. Please use the
          `tie_weights` method before using the `infer_auto_device` function.

          2023-06-21 17:21:38 WARNING:The safetensors archive passed at models\TheBloke_stable-vicuna-13B-GPTQ\stable-vicuna-13b-gptq-4bit.compat.no-act-order.safetensors
          does not contain metadata. Make sure to save your model with the `save_pretrained`
          method. Defaulting to ''pt'' metadata.


          I maybe should mention, that I have a 8GB VRAM card. I''m not sure if this
          is relevant for this error.'
        updatedAt: '2023-06-21T16:25:51.601Z'
      numEdits: 0
      reactions: []
    id: 6493248f5075760130eaa12e
    type: comment
  author: StableDiffusion69
  content: 'Ah, OK, no problem, here it is:


    ![Zwischenablagebild.png](https://cdn-uploads.huggingface.co/production/uploads/63b1b9129d21227b914746ae/YEc4LAxMU9Tyc9rIWOvIu.png)


    In the console this comes up:

    To create a public link, set `share=True` in `launch()`.

    2023-06-21 17:21:37 INFO:Loading TheBloke_stable-vicuna-13B-GPTQ...

    2023-06-21 17:21:37 INFO:The AutoGPTQ params are: {''model_basename'': ''stable-vicuna-13b-gptq-4bit.compat.no-act-order'',
    ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'': True,
    ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
    False, ''max_memory'': None, ''quantize_config'': None}

    2023-06-21 17:21:38 WARNING:The model weights are not tied. Please use the `tie_weights`
    method before using the `infer_auto_device` function.

    2023-06-21 17:21:38 WARNING:The safetensors archive passed at models\TheBloke_stable-vicuna-13B-GPTQ\stable-vicuna-13b-gptq-4bit.compat.no-act-order.safetensors
    does not contain metadata. Make sure to save your model with the `save_pretrained`
    method. Defaulting to ''pt'' metadata.


    I maybe should mention, that I have a 8GB VRAM card. I''m not sure if this is
    relevant for this error.'
  created_at: 2023-06-21 15:25:51+00:00
  edited: false
  hidden: false
  id: 6493248f5075760130eaa12e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-21T16:33:41.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7872452139854431
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>That''s a different error now!</p>

          <p>Now you are simply running out of VRAM.  And yes, 8GB card is very relevant.  8GB
          is not enough to load a 13B model, you need 12GB minimum.  8GB isn''t even
          enough to load a 7B model using AutoGPTQ.</p>

          <p>Two options:</p>

          <ol>

          <li><p>Use a 7B GPTQ model, with the new ExLlama loader. This is now supported
          under text-generation-webui, though you need to install ExLlama manually:
          <a rel="nofollow" href="https://github.com/turboderp/exllama">https://github.com/turboderp/exllama</a></p>

          </li>

          <li><p>Or, try a 13B GGML instead.  With GGML 13B, eg a q4_K_M.bin file,
          you can use the llama.cpp loader.  By default it is only for CPU inference,
          but you can enable GPU acceleration by compiling llama-cpp-python with CUDA
          support by following these instructions: <a rel="nofollow" href="https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal">https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal</a></p>

          </li>

          </ol>

          <p>First you will need to get CUDA toolkit installed.</p>

          <p>Then for Windows, the commands are:</p>

          <pre><code>pip uninstall -y llama-cpp-python

          set CMAKE_ARGS="-DLLAMA_CUBLAS=on"

          set FORCE_CMAKE=1

          pip install llama-cpp-python --no-cache-dir

          </code></pre>

          <p>Once this is done successfully, you can:</p>

          <ul>

          <li>Download a GGML model file, like stable-vicuna.ggmlv3.q4_K_M.bin</li>

          <li>In text-gen, choose the llama.cpp loader</li>

          <li>Set "n-gpu-layers" to 40  (if this gives another CUDA out of memory
          error, try 35 instead)</li>

          <li>Set Threads to 8</li>

          </ul>

          '
        raw: 'That''s a different error now!


          Now you are simply running out of VRAM.  And yes, 8GB card is very relevant.  8GB
          is not enough to load a 13B model, you need 12GB minimum.  8GB isn''t even
          enough to load a 7B model using AutoGPTQ.


          Two options:


          1. Use a 7B GPTQ model, with the new ExLlama loader. This is now supported
          under text-generation-webui, though you need to install ExLlama manually:
          https://github.com/turboderp/exllama


          2. Or, try a 13B GGML instead.  With GGML 13B, eg a q4_K_M.bin file, you
          can use the llama.cpp loader.  By default it is only for CPU inference,
          but you can enable GPU acceleration by compiling llama-cpp-python with CUDA
          support by following these instructions: https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal


          First you will need to get CUDA toolkit installed.


          Then for Windows, the commands are:

          ```

          pip uninstall -y llama-cpp-python

          set CMAKE_ARGS="-DLLAMA_CUBLAS=on"

          set FORCE_CMAKE=1

          pip install llama-cpp-python --no-cache-dir

          ```


          Once this is done successfully, you can:

          - Download a GGML model file, like stable-vicuna.ggmlv3.q4_K_M.bin

          - In text-gen, choose the llama.cpp loader

          - Set "n-gpu-layers" to 40  (if this gives another CUDA out of memory error,
          try 35 instead)

          - Set Threads to 8


          '
        updatedAt: '2023-06-21T16:33:41.006Z'
      numEdits: 0
      reactions: []
    id: 64932665a7f8d3fb53b617bc
    type: comment
  author: TheBloke
  content: 'That''s a different error now!


    Now you are simply running out of VRAM.  And yes, 8GB card is very relevant.  8GB
    is not enough to load a 13B model, you need 12GB minimum.  8GB isn''t even enough
    to load a 7B model using AutoGPTQ.


    Two options:


    1. Use a 7B GPTQ model, with the new ExLlama loader. This is now supported under
    text-generation-webui, though you need to install ExLlama manually: https://github.com/turboderp/exllama


    2. Or, try a 13B GGML instead.  With GGML 13B, eg a q4_K_M.bin file, you can use
    the llama.cpp loader.  By default it is only for CPU inference, but you can enable
    GPU acceleration by compiling llama-cpp-python with CUDA support by following
    these instructions: https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal


    First you will need to get CUDA toolkit installed.


    Then for Windows, the commands are:

    ```

    pip uninstall -y llama-cpp-python

    set CMAKE_ARGS="-DLLAMA_CUBLAS=on"

    set FORCE_CMAKE=1

    pip install llama-cpp-python --no-cache-dir

    ```


    Once this is done successfully, you can:

    - Download a GGML model file, like stable-vicuna.ggmlv3.q4_K_M.bin

    - In text-gen, choose the llama.cpp loader

    - Set "n-gpu-layers" to 40  (if this gives another CUDA out of memory error, try
    35 instead)

    - Set Threads to 8


    '
  created_at: 2023-06-21 15:33:41+00:00
  edited: false
  hidden: false
  id: 64932665a7f8d3fb53b617bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-06-21T17:14:58.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6752999424934387
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: "<blockquote>\n<p>Now you are simply running out of VRAM. </p>\n</blockquote>\n\
          <p>Ah, you're right. I hadn't looked at this, sorry. \U0001F92A<br>I tried\
          \ using CPU, then it loads. But after input a text I get:</p>\n<p>To create\
          \ a public link, set <code>share=True</code> in <code>launch()</code>.<br>2023-06-21\
          \ 18:04:08 INFO:Loading TheBloke_stable-vicuna-13B-GPTQ...<br>2023-06-21\
          \ 18:04:09 INFO:The AutoGPTQ params are: {'model_basename': 'stable-vicuna-13b-gptq-4bit.compat.no-act-order',\
          \ 'device': 'cpu', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None}<br>2023-06-21 18:04:10\
          \ WARNING:The model weights are not tied. Please use the <code>tie_weights</code>\
          \ method before using the <code>infer_auto_device</code> function.<br>2023-06-21\
          \ 18:04:10 WARNING:The safetensors archive passed at models\\TheBloke_stable-vicuna-13B-GPTQ\\\
          stable-vicuna-13b-gptq-4bit.compat.no-act-order.safetensors does not contain\
          \ metadata. Make sure to save your model with the <code>save_pretrained</code>\
          \ method. Defaulting to 'pt' metadata.<br>2023-06-21 18:05:52 WARNING:skip\
          \ module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.<br>2023-06-21 18:05:53 INFO:Loaded the model in 104.87\
          \ seconds.<br>============================================================<br>loading\
          \ character assistant<br>Traceback (most recent call last):<br>  File \"\
          F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\"\
          , line 73, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          text_generation.py\", line 263, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling_base.py\", line 422, in generate<br>\
          \    with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling_base.py\", line 411, in device<br>  \
          \  device = [d for d in self.hf_device_map.values() if d not in {'cpu',\
          \ 'disk'}][0]<br>IndexError: list index out of range<br>Output generated\
          \ in 0.33 seconds (0.00 tokens/s, 0 tokens, context 42, seed 540771681)</p>\n\
          <blockquote>\n<p>8GB is not enough to load a 13B model, you need 12GB minimum.\
          \ 8GB isn't even enough to load a 7B model using AutoGPTQ.</p>\n</blockquote>\n\
          <p>Well, your model is 6.75GB. I thought this is OK. I always thought it's\
          \ about the filesize.<br>I still haven't found out, what models can/should\
          \ be used depending on VRAM. I currently still don't understand, why I can\
          \ use the 12.4GB facbook model, or the 11.2GB pygmalion model from AlekseyKorshuk\
          \ or even the 12.5GB pygmalion model from Imablank, but not your way smaller\
          \ model. \U0001F914<br>Maybe I may ask you for a brief guide on what to\
          \ look for?</p>\n<p>And many thanks for the detailed information, I will\
          \ look into it. \U0001F44D\U0001F642</p>\n"
        raw: "> Now you are simply running out of VRAM. \n\nAh, you're right. I hadn't\
          \ looked at this, sorry. \U0001F92A\nI tried using CPU, then it loads. But\
          \ after input a text I get:\n\nTo create a public link, set `share=True`\
          \ in `launch()`.\n2023-06-21 18:04:08 INFO:Loading TheBloke_stable-vicuna-13B-GPTQ...\n\
          2023-06-21 18:04:09 INFO:The AutoGPTQ params are: {'model_basename': 'stable-vicuna-13b-gptq-4bit.compat.no-act-order',\
          \ 'device': 'cpu', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': None}\n2023-06-21 18:04:10\
          \ WARNING:The model weights are not tied. Please use the `tie_weights` method\
          \ before using the `infer_auto_device` function.\n2023-06-21 18:04:10 WARNING:The\
          \ safetensors archive passed at models\\TheBloke_stable-vicuna-13B-GPTQ\\\
          stable-vicuna-13b-gptq-4bit.compat.no-act-order.safetensors does not contain\
          \ metadata. Make sure to save your model with the `save_pretrained` method.\
          \ Defaulting to 'pt' metadata.\n2023-06-21 18:05:52 WARNING:skip module\
          \ injection for FusedLlamaMLPForQuantizedModel not support integrate without\
          \ triton yet.\n2023-06-21 18:05:53 INFO:Loaded the model in 104.87 seconds.\n\
          ============================================================\nloading character\
          \ assistant\nTraceback (most recent call last):\n  File \"F:\\Programme\\\
          oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\", line\
          \ 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
          \  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\\
          text_generation.py\", line 263, in generate_with_callback\n    shared.model.generate(**kwargs)\n\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling\\_base.py\", line 422, in generate\n\
          \    with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\n\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling\\_base.py\", line 411, in device\n  \
          \  device = [d for d in self.hf_device_map.values() if d not in {'cpu',\
          \ 'disk'}][0]\nIndexError: list index out of range\nOutput generated in\
          \ 0.33 seconds (0.00 tokens/s, 0 tokens, context 42, seed 540771681)\n\n\
          > 8GB is not enough to load a 13B model, you need 12GB minimum. 8GB isn't\
          \ even enough to load a 7B model using AutoGPTQ.\n\nWell, your model is\
          \ 6.75GB. I thought this is OK. I always thought it's about the filesize.\n\
          I still haven't found out, what models can/should be used depending on VRAM.\
          \ I currently still don't understand, why I can use the 12.4GB facbook model,\
          \ or the 11.2GB pygmalion model from AlekseyKorshuk or even the 12.5GB pygmalion\
          \ model from Imablank, but not your way smaller model. \U0001F914\nMaybe\
          \ I may ask you for a brief guide on what to look for?\n\nAnd many thanks\
          \ for the detailed information, I will look into it. \U0001F44D\U0001F642"
        updatedAt: '2023-06-21T17:14:58.974Z'
      numEdits: 0
      reactions: []
    id: 64933012bba143e85f433870
    type: comment
  author: StableDiffusion69
  content: "> Now you are simply running out of VRAM. \n\nAh, you're right. I hadn't\
    \ looked at this, sorry. \U0001F92A\nI tried using CPU, then it loads. But after\
    \ input a text I get:\n\nTo create a public link, set `share=True` in `launch()`.\n\
    2023-06-21 18:04:08 INFO:Loading TheBloke_stable-vicuna-13B-GPTQ...\n2023-06-21\
    \ 18:04:09 INFO:The AutoGPTQ params are: {'model_basename': 'stable-vicuna-13b-gptq-4bit.compat.no-act-order',\
    \ 'device': 'cpu', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': None}\n2023-06-21 18:04:10 WARNING:The model weights are\
    \ not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
    \ function.\n2023-06-21 18:04:10 WARNING:The safetensors archive passed at models\\\
    TheBloke_stable-vicuna-13B-GPTQ\\stable-vicuna-13b-gptq-4bit.compat.no-act-order.safetensors\
    \ does not contain metadata. Make sure to save your model with the `save_pretrained`\
    \ method. Defaulting to 'pt' metadata.\n2023-06-21 18:05:52 WARNING:skip module\
    \ injection for FusedLlamaMLPForQuantizedModel not support integrate without triton\
    \ yet.\n2023-06-21 18:05:53 INFO:Loaded the model in 104.87 seconds.\n============================================================\n\
    loading character assistant\nTraceback (most recent call last):\n  File \"F:\\\
    Programme\\oobabooga_windows\\text-generation-webui\\modules\\callbacks.py\",\
    \ line 73, in gentask\n    ret = self.mfunc(callback=_callback, **self.kwargs)\n\
    \  File \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 263, in generate_with_callback\n    shared.model.generate(**kwargs)\n \
    \ File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    auto_gptq\\modeling\\_base.py\", line 422, in generate\n    with torch.inference_mode(),\
    \ torch.amp.autocast(device_type=self.device.type):\n  File \"F:\\Programme\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling\\\
    _base.py\", line 411, in device\n    device = [d for d in self.hf_device_map.values()\
    \ if d not in {'cpu', 'disk'}][0]\nIndexError: list index out of range\nOutput\
    \ generated in 0.33 seconds (0.00 tokens/s, 0 tokens, context 42, seed 540771681)\n\
    \n> 8GB is not enough to load a 13B model, you need 12GB minimum. 8GB isn't even\
    \ enough to load a 7B model using AutoGPTQ.\n\nWell, your model is 6.75GB. I thought\
    \ this is OK. I always thought it's about the filesize.\nI still haven't found\
    \ out, what models can/should be used depending on VRAM. I currently still don't\
    \ understand, why I can use the 12.4GB facbook model, or the 11.2GB pygmalion\
    \ model from AlekseyKorshuk or even the 12.5GB pygmalion model from Imablank,\
    \ but not your way smaller model. \U0001F914\nMaybe I may ask you for a brief\
    \ guide on what to look for?\n\nAnd many thanks for the detailed information,\
    \ I will look into it. \U0001F44D\U0001F642"
  created_at: 2023-06-21 16:14:58+00:00
  edited: false
  hidden: false
  id: 64933012bba143e85f433870
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'AttributeError: ''Offload_LlamaModel'' object has no attribute ''preload'''
