!!python/object:huggingface_hub.community.DiscussionWithDetails
author: thefaheem
conflicting_files: null
created_at: 2023-05-04 13:49:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-04T14:49:09.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>How to Use This Model With Transformers Library?</p>

          <p>While Trying I Got The Following Error!</p>

          <p>OSError: TheBloke/stable-vicuna-13B-GPTQ does not appear to have a file
          named pytorch_model.bin, tf_model.h5,<br>model.ckpt or flax_model.msgpack.</p>

          '
        raw: "How to Use This Model With Transformers Library?\r\n\r\nWhile Trying\
          \ I Got The Following Error!\r\n\r\nOSError: TheBloke/stable-vicuna-13B-GPTQ\
          \ does not appear to have a file named pytorch_model.bin, tf_model.h5, \r\
          \nmodel.ckpt or flax_model.msgpack.\r\n"
        updatedAt: '2023-05-04T14:49:09.385Z'
      numEdits: 0
      reactions: []
    id: 6453c5e5096d57ae122c3e50
    type: comment
  author: thefaheem
  content: "How to Use This Model With Transformers Library?\r\n\r\nWhile Trying I\
    \ Got The Following Error!\r\n\r\nOSError: TheBloke/stable-vicuna-13B-GPTQ does\
    \ not appear to have a file named pytorch_model.bin, tf_model.h5, \r\nmodel.ckpt\
    \ or flax_model.msgpack.\r\n"
  created_at: 2023-05-04 13:49:09+00:00
  edited: false
  hidden: false
  id: 6453c5e5096d57ae122c3e50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-04T15:00:55.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can''t use this model with transformers directly. For that you
          need the - HF model, but it will need 30GB VRAM or 15gb in 8bit.</p>

          <p>You can use this model with text generation UI as described in the readme.
          Or you can use it with AutoGPTQ which is an extension for HF format. AutoGPTQ
          is still in development and not yet fully stable but people are using it
          successfully with models like this one.</p>

          '
        raw: 'You can''t use this model with transformers directly. For that you need
          the - HF model, but it will need 30GB VRAM or 15gb in 8bit.


          You can use this model with text generation UI as described in the readme.
          Or you can use it with AutoGPTQ which is an extension for HF format. AutoGPTQ
          is still in development and not yet fully stable but people are using it
          successfully with models like this one.'
        updatedAt: '2023-05-04T15:00:55.512Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ThisUserNameWillNotExist
    id: 6453c8a772d331dec8996168
    type: comment
  author: TheBloke
  content: 'You can''t use this model with transformers directly. For that you need
    the - HF model, but it will need 30GB VRAM or 15gb in 8bit.


    You can use this model with text generation UI as described in the readme. Or
    you can use it with AutoGPTQ which is an extension for HF format. AutoGPTQ is
    still in development and not yet fully stable but people are using it successfully
    with models like this one.'
  created_at: 2023-05-04 14:00:55+00:00
  edited: false
  hidden: false
  id: 6453c8a772d331dec8996168
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-04T16:45:55.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<blockquote>

          <p>You can''t use this model with transformers directly. For that you need
          the - HF model, but it will need 30GB VRAM or 15gb in 8bit.</p>

          <p>You can use this model with text generation UI as described in the readme.
          Or you can use it with AutoGPTQ which is an extension for HF format. AutoGPTQ
          is still in development and not yet fully stable but people are using it
          successfully with models like this one.</p>

          </blockquote>

          <p>I''ll Try it For Sure and Thanks For Your Lightning Reply btw...</p>

          '
        raw: "> You can't use this model with transformers directly. For that you\
          \ need the - HF model, but it will need 30GB VRAM or 15gb in 8bit.\n> \n\
          > You can use this model with text generation UI as described in the readme.\
          \ Or you can use it with AutoGPTQ which is an extension for HF format. AutoGPTQ\
          \ is still in development and not yet fully stable but people are using\
          \ it successfully with models like this one.\n\nI'll Try it For Sure and\
          \ Thanks For Your Lightning Reply btw..."
        updatedAt: '2023-05-04T16:45:55.638Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 6453e143b8c58783d665c2c7
    type: comment
  author: thefaheem
  content: "> You can't use this model with transformers directly. For that you need\
    \ the - HF model, but it will need 30GB VRAM or 15gb in 8bit.\n> \n> You can use\
    \ this model with text generation UI as described in the readme. Or you can use\
    \ it with AutoGPTQ which is an extension for HF format. AutoGPTQ is still in development\
    \ and not yet fully stable but people are using it successfully with models like\
    \ this one.\n\nI'll Try it For Sure and Thanks For Your Lightning Reply btw..."
  created_at: 2023-05-04 15:45:55+00:00
  edited: false
  hidden: false
  id: 6453e143b8c58783d665c2c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: using with transformers
