!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joshlevy89
conflicting_files: null
created_at: 2023-05-10 18:13:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
      fullname: Joshua Levy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joshlevy89
      type: user
    createdAt: '2023-05-10T19:13:23.000Z'
    data:
      edited: true
      editors:
      - joshlevy89
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
          fullname: Joshua Levy
          isHf: false
          isPro: false
          name: joshlevy89
          type: user
        html: '<p>I used the code on discussion <a href="/TheBloke/stable-vicuna-13B-GPTQ/discussions/1">#1</a>
          (<a href="https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/1#644cc8af97a3b0904a481e3e">https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/1#644cc8af97a3b0904a481e3e</a>)
          but modified it to output hidden_states and attentions.  The textual output
          looks good, and the hidden_states look good, but each element of the attention
          tensors are None.  The output size is 1007 (generated length) x 40 (layer
          number) but the tensors within are all None.  Any ideas why this would be?</p>

          <p>Generate updated to output internals:</p>

          <p>with torch.no_grad():<br>    model_output = model.generate(<br>        input_ids,<br>        do_sample=False,  #
          Set to False for now<br>        min_length=min_length,<br>        max_length=max_length,<br>        top_p=top_p,<br>        temperature=temperature,<br>        return_dict_in_generate=True,<br>        output_scores=True,<br>        output_attentions=True,<br>        output_hidden_states=True<br>    )</p>

          '
        raw: "I used the code on discussion #1 (https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/1#644cc8af97a3b0904a481e3e)\
          \ but modified it to output hidden_states and attentions.  The textual output\
          \ looks good, and the hidden_states look good, but each element of the attention\
          \ tensors are None.  The output size is 1007 (generated length) x 40 (layer\
          \ number) but the tensors within are all None.  Any ideas why this would\
          \ be?\n\nGenerate updated to output internals:\n\nwith torch.no_grad():\n\
          \    model_output = model.generate(\n        input_ids,\n        do_sample=False,\
          \  # Set to False for now\n        min_length=min_length,\n        max_length=max_length,\n\
          \        top_p=top_p,\n        temperature=temperature,\n        return_dict_in_generate=True,\n\
          \        output_scores=True,\n        output_attentions=True,\n        output_hidden_states=True\n\
          \    )"
        updatedAt: '2023-05-10T19:15:14.200Z'
      numEdits: 2
      reactions: []
    id: 645becd308d9a18f91ed8e37
    type: comment
  author: joshlevy89
  content: "I used the code on discussion #1 (https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ/discussions/1#644cc8af97a3b0904a481e3e)\
    \ but modified it to output hidden_states and attentions.  The textual output\
    \ looks good, and the hidden_states look good, but each element of the attention\
    \ tensors are None.  The output size is 1007 (generated length) x 40 (layer number)\
    \ but the tensors within are all None.  Any ideas why this would be?\n\nGenerate\
    \ updated to output internals:\n\nwith torch.no_grad():\n    model_output = model.generate(\n\
    \        input_ids,\n        do_sample=False,  # Set to False for now\n      \
    \  min_length=min_length,\n        max_length=max_length,\n        top_p=top_p,\n\
    \        temperature=temperature,\n        return_dict_in_generate=True,\n   \
    \     output_scores=True,\n        output_attentions=True,\n        output_hidden_states=True\n\
    \    )"
  created_at: 2023-05-10 18:13:23+00:00
  edited: true
  hidden: false
  id: 645becd308d9a18f91ed8e37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T20:08:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry, no, no clue. Never tried to do that before.</p>

          <p>What is the point of outputting the hidden states and attentions?</p>

          '
        raw: 'Sorry, no, no clue. Never tried to do that before.


          What is the point of outputting the hidden states and attentions?'
        updatedAt: '2023-05-10T20:08:19.740Z'
      numEdits: 0
      reactions: []
    id: 645bf9b3b396a40a8493c29e
    type: comment
  author: TheBloke
  content: 'Sorry, no, no clue. Never tried to do that before.


    What is the point of outputting the hidden states and attentions?'
  created_at: 2023-05-10 19:08:19+00:00
  edited: false
  hidden: false
  id: 645bf9b3b396a40a8493c29e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
      fullname: Joshua Levy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joshlevy89
      type: user
    createdAt: '2023-05-10T20:32:07.000Z'
    data:
      edited: false
      editors:
      - joshlevy89
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d96bd8ba5f1d60a17ea00386444f383.svg
          fullname: Joshua Levy
          isHf: false
          isPro: false
          name: joshlevy89
          type: user
        html: '<p>Thanks for the quick reply.</p>

          <p>Many different applications, e.g. <a rel="nofollow" href="https://openai.com/research/language-models-can-explain-neurons-in-language-models">https://openai.com/research/language-models-can-explain-neurons-in-language-models</a></p>

          '
        raw: 'Thanks for the quick reply.


          Many different applications, e.g. https://openai.com/research/language-models-can-explain-neurons-in-language-models'
        updatedAt: '2023-05-10T20:32:07.705Z'
      numEdits: 0
      reactions: []
    id: 645bff47337b2ccf07fe0485
    type: comment
  author: joshlevy89
  content: 'Thanks for the quick reply.


    Many different applications, e.g. https://openai.com/research/language-models-can-explain-neurons-in-language-models'
  created_at: 2023-05-10 19:32:07+00:00
  edited: false
  hidden: false
  id: 645bff47337b2ccf07fe0485
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Attentions are all None
