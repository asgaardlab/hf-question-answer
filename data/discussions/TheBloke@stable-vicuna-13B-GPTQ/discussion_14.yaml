!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Squeezitgirdle
conflicting_files: null
created_at: 2023-05-07 07:25:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squeezitgirdle
      type: user
    createdAt: '2023-05-07T08:25:56.000Z'
    data:
      edited: true
      editors:
      - Squeezitgirdle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
          fullname: David
          isHf: false
          isPro: false
          name: Squeezitgirdle
          type: user
        html: "<p>First attempted to download via but get the following error:</p>\n\
          <pre><code>Traceback (most recent call last):\nFile \u201C...\\AIchatbot\\\
          oogabooga\\oobabooga-windows\\text-generation-webui\\server.py\u201D, line\
          \ 172, in download_model_wrapper\nlinks, sha256, is_lora = downloader.get_download_links_from_huggingface(model,\
          \ branch, text_only=False)\nFile \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\\
          text-generation-webui\\download-model.py\u201D, line 103, in get_download_links_from_huggingface\n\
          fname = dict[i][\u2018path\u2019]\nKeyError: 0\n</code></pre>\n<p>So I downloaded\
          \ manually instead. Added the model in models/TheBloke/stable-vicuna-13B-GPTQ</p>\n\
          <p>I now get this error:</p>\n<p><code>Traceback (most recent call last):\
          \ File \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\text-generation-webui\\\
          server.py\u201D, line 85, in load_model_wrapper shared.model, shared.tokenizer\
          \ = load_model(shared.model_name) File \u201C...\\AIchatbot\\oogabooga\\\
          oobabooga-windows\\text-generation-webui\\modules\\models.py\u201D, line\
          \ 175, in load_model model = LoaderClass.from_pretrained(checkpoint, **params)\
          \ File \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 471, in from_pretrained return model_class.from_pretrained( File\
          \ \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2405,\
          \ in from_pretrained raise EnvironmentError( OSError: Error no file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke.</code></p>\n<p>I have also tried\
          \ ignoring this error and entering the GPTQ parameters Bits = 4, Groupsize\
          \ 128, Llama<br>however this does not change the error I receive.</p>\n\
          <p>Thanks</p>\n<p>Note, I am intending to run via GPU on a 4090. Additionally\
          \ other models do work.</p>\n"
        raw: "First attempted to download via but get the following error:\n```\n\
          Traceback (most recent call last):\nFile \u201C...\\AIchatbot\\oogabooga\\\
          oobabooga-windows\\text-generation-webui\\server.py\u201D, line 172, in\
          \ download_model_wrapper\nlinks, sha256, is_lora = downloader.get_download_links_from_huggingface(model,\
          \ branch, text_only=False)\nFile \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\\
          text-generation-webui\\download-model.py\u201D, line 103, in get_download_links_from_huggingface\n\
          fname = dict[i][\u2018path\u2019]\nKeyError: 0\n```\n\nSo I downloaded manually\
          \ instead. Added the model in models/TheBloke/stable-vicuna-13B-GPTQ\n\n\
          I now get this error:\n```Traceback (most recent call last):\nFile \u201C\
          ...\\AIchatbot\\oogabooga\\oobabooga-windows\\text-generation-webui\\server.py\u201D\
          , line 85, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
          File \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\text-generation-webui\\\
          modules\\models.py\u201D, line 175, in load_model\nmodel = LoaderClass.from_pretrained(checkpoint,\
          \ **params)\nFile \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D\
          , line 471, in from_pretrained\nreturn model_class.from_pretrained(\nFile\
          \ \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\u201D, line 2405,\
          \ in from_pretrained\nraise EnvironmentError(\nOSError: Error no file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt.index or flax_model.msgpack\
          \ found in directory models\\TheBloke.```\n\nI have also tried ignoring\
          \ this error and entering the GPTQ parameters Bits = 4, Groupsize 128, Llama\n\
          however this does not change the error I receive.\n\nThanks\n\nNote, I am\
          \ intending to run via GPU on a 4090. Additionally other models do work."
        updatedAt: '2023-05-07T08:26:42.634Z'
      numEdits: 1
      reactions: []
    id: 6457609490bf61c87f9ff593
    type: comment
  author: Squeezitgirdle
  content: "First attempted to download via but get the following error:\n```\nTraceback\
    \ (most recent call last):\nFile \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\\
    text-generation-webui\\server.py\u201D, line 172, in download_model_wrapper\n\
    links, sha256, is_lora = downloader.get_download_links_from_huggingface(model,\
    \ branch, text_only=False)\nFile \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\\
    text-generation-webui\\download-model.py\u201D, line 103, in get_download_links_from_huggingface\n\
    fname = dict[i][\u2018path\u2019]\nKeyError: 0\n```\n\nSo I downloaded manually\
    \ instead. Added the model in models/TheBloke/stable-vicuna-13B-GPTQ\n\nI now\
    \ get this error:\n```Traceback (most recent call last):\nFile \u201C...\\AIchatbot\\\
    oogabooga\\oobabooga-windows\\text-generation-webui\\server.py\u201D, line 85,\
    \ in load_model_wrapper\nshared.model, shared.tokenizer = load_model(shared.model_name)\n\
    File \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\text-generation-webui\\\
    modules\\models.py\u201D, line 175, in load_model\nmodel = LoaderClass.from_pretrained(checkpoint,\
    \ **params)\nFile \u201C...\\AIchatbot\\oogabooga\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\u201D, line\
    \ 471, in from_pretrained\nreturn model_class.from_pretrained(\nFile \u201C...\\\
    AIchatbot\\oogabooga\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\u201D, line 2405, in from_pretrained\nraise EnvironmentError(\n\
    OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index\
    \ or flax_model.msgpack found in directory models\\TheBloke.```\n\nI have also\
    \ tried ignoring this error and entering the GPTQ parameters Bits = 4, Groupsize\
    \ 128, Llama\nhowever this does not change the error I receive.\n\nThanks\n\n\
    Note, I am intending to run via GPU on a 4090. Additionally other models do work."
  created_at: 2023-05-07 07:25:56+00:00
  edited: true
  hidden: false
  id: 6457609490bf61c87f9ff593
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-07T08:30:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please read the README. You missed steps 5 - 9:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png"></a></p>

          '
        raw: 'Please read the README. You missed steps 5 - 9:



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png)'
        updatedAt: '2023-05-07T08:30:00.780Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - OtherBarry
      - count: 1
        reaction: "\U0001F91D"
        users:
        - OtherBarry
    id: 64576188cf099a9dd149fd78
    type: comment
  author: TheBloke
  content: 'Please read the README. You missed steps 5 - 9:



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png)'
  created_at: 2023-05-07 07:30:00+00:00
  edited: false
  hidden: false
  id: 64576188cf099a9dd149fd78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squeezitgirdle
      type: user
    createdAt: '2023-05-08T04:58:17.000Z'
    data:
      edited: false
      editors:
      - Squeezitgirdle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
          fullname: David
          isHf: false
          isPro: false
          name: Squeezitgirdle
          type: user
        html: '<blockquote>

          <p>Please read the README. You missed steps 5 - 9:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png"></a></p>

          </blockquote>

          <p>Thanks for the reply, but I did not miss steps 5 - 9.<br>I''m happy to
          provide any information that may help, but I repeated steps 5 - 9 multiple
          times with no luck.</p>

          '
        raw: "> Please read the README. You missed steps 5 - 9:\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png)\n\
          \nThanks for the reply, but I did not miss steps 5 - 9. \nI'm happy to provide\
          \ any information that may help, but I repeated steps 5 - 9 multiple times\
          \ with no luck."
        updatedAt: '2023-05-08T04:58:17.530Z'
      numEdits: 0
      reactions: []
    id: 645881690332c1fb59fce431
    type: comment
  author: Squeezitgirdle
  content: "> Please read the README. You missed steps 5 - 9:\n> \n> \n> ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/ekiz2f1K7gOWvQW3Ymcp3.png)\n\
    \nThanks for the reply, but I did not miss steps 5 - 9. \nI'm happy to provide\
    \ any information that may help, but I repeated steps 5 - 9 multiple times with\
    \ no luck."
  created_at: 2023-05-08 03:58:17+00:00
  edited: false
  hidden: false
  id: 645881690332c1fb59fce431
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-08T07:26:06.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK please show me the contents of your <code>models</code> folder
          and the contents of  the model folder (which I think is called<code>models\TheBloke</code>
          ? based on the errror message)  .  </p>

          <p>If you''ve called the model folder <code>TheBloke</code> then the model
          files need to be directly in that folder.</p>

          <p>Eg if you had <code>models/TheBloke/stable-vicuna-13B-GPTQ/&lt;model
          files here&gt;</code> that would not work.</p>

          '
        raw: "OK please show me the contents of your `models` folder and the contents\
          \ of  the model folder (which I think is called`models\\TheBloke` ? based\
          \ on the errror message)  .  \n\nIf you've called the model folder `TheBloke`\
          \ then the model files need to be directly in that folder.\n\nEg if you\
          \ had `models/TheBloke/stable-vicuna-13B-GPTQ/<model files here>` that would\
          \ not work."
        updatedAt: '2023-05-08T07:26:06.855Z'
      numEdits: 0
      reactions: []
    id: 6458a40e39e6aea69cb5c3b8
    type: comment
  author: TheBloke
  content: "OK please show me the contents of your `models` folder and the contents\
    \ of  the model folder (which I think is called`models\\TheBloke` ? based on the\
    \ errror message)  .  \n\nIf you've called the model folder `TheBloke` then the\
    \ model files need to be directly in that folder.\n\nEg if you had `models/TheBloke/stable-vicuna-13B-GPTQ/<model\
    \ files here>` that would not work."
  created_at: 2023-05-08 06:26:06+00:00
  edited: false
  hidden: false
  id: 6458a40e39e6aea69cb5c3b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4626499ebf9ee0097eb9cd5a967afebd.svg
      fullname: 1 1
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Monochromancer
      type: user
    createdAt: '2023-05-09T03:00:11.000Z'
    data:
      edited: false
      editors:
      - Monochromancer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4626499ebf9ee0097eb9cd5a967afebd.svg
          fullname: 1 1
          isHf: false
          isPro: false
          name: Monochromancer
          type: user
        html: '<p>Same issue here, multiple reinstalls:<br>\text-generation-webui\models\TheBloke_stable-vicuna-13B-GPTQ<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/MBx8nUs45Q3V-SPxj1u0o.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/MBx8nUs45Q3V-SPxj1u0o.png"></a></p>

          <p>Models:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/kqC0HDg3qTl3Vp4RIfCBN.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/kqC0HDg3qTl3Vp4RIfCBN.png"></a></p>

          '
        raw: 'Same issue here, multiple reinstalls:

          \text-generation-webui\models\TheBloke_stable-vicuna-13B-GPTQ

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/MBx8nUs45Q3V-SPxj1u0o.png)


          Models:



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/kqC0HDg3qTl3Vp4RIfCBN.png)'
        updatedAt: '2023-05-09T03:00:11.394Z'
      numEdits: 0
      reactions: []
    id: 6459b73bf92601affa3d2bb7
    type: comment
  author: Monochromancer
  content: 'Same issue here, multiple reinstalls:

    \text-generation-webui\models\TheBloke_stable-vicuna-13B-GPTQ

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/MBx8nUs45Q3V-SPxj1u0o.png)


    Models:



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6307b142a642348dc5e60133/kqC0HDg3qTl3Vp4RIfCBN.png)'
  created_at: 2023-05-09 02:00:11+00:00
  edited: false
  hidden: false
  id: 6459b73bf92601affa3d2bb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-09T10:48:40.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Someone on Discord recently told me this :</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bKslNOGDlacUCvMJ8IYAl.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bKslNOGDlacUCvMJ8IYAl.png"></a></p>

          <p>So please try that advice: go to the Models screen, manually set wbits
          = 4, groupsize = 128, model_type = llama, then reload model.  Does that
          work?</p>

          <p>If it does, please try clicking "Save settings for this model", then
          restart the UI and see if it works then.  If it does, then you''re fine
          (and didn''t follow the README!).  If it doesn''t, then you''re hitting
          this same issue, which must be a bug in text-generation-webui.</p>

          '
        raw: 'Someone on Discord recently told me this :


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bKslNOGDlacUCvMJ8IYAl.png)


          So please try that advice: go to the Models screen, manually set wbits =
          4, groupsize = 128, model_type = llama, then reload model.  Does that work?


          If it does, please try clicking "Save settings for this model", then restart
          the UI and see if it works then.  If it does, then you''re fine (and didn''t
          follow the README!).  If it doesn''t, then you''re hitting this same issue,
          which must be a bug in text-generation-webui.'
        updatedAt: '2023-05-09T10:48:40.363Z'
      numEdits: 0
      reactions: []
    id: 645a2508761b359d06e62402
    type: comment
  author: TheBloke
  content: 'Someone on Discord recently told me this :


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/bKslNOGDlacUCvMJ8IYAl.png)


    So please try that advice: go to the Models screen, manually set wbits = 4, groupsize
    = 128, model_type = llama, then reload model.  Does that work?


    If it does, please try clicking "Save settings for this model", then restart the
    UI and see if it works then.  If it does, then you''re fine (and didn''t follow
    the README!).  If it doesn''t, then you''re hitting this same issue, which must
    be a bug in text-generation-webui.'
  created_at: 2023-05-09 09:48:40+00:00
  edited: false
  hidden: false
  id: 645a2508761b359d06e62402
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5a2cf01434a982d795801ec242ab7b7.svg
      fullname: Deca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: decagrog
      type: user
    createdAt: '2023-05-09T20:25:27.000Z'
    data:
      edited: false
      editors:
      - decagrog
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5a2cf01434a982d795801ec242ab7b7.svg
          fullname: Deca
          isHf: false
          isPro: false
          name: decagrog
          type: user
        html: '<p>Same error, coming from a fresh oogabooga install</p>

          '
        raw: Same error, coming from a fresh oogabooga install
        updatedAt: '2023-05-09T20:25:27.858Z'
      numEdits: 0
      reactions: []
    id: 645aac37214503ad17ac7b40
    type: comment
  author: decagrog
  content: Same error, coming from a fresh oogabooga install
  created_at: 2023-05-09 19:25:27+00:00
  edited: false
  hidden: false
  id: 645aac37214503ad17ac7b40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squeezitgirdle
      type: user
    createdAt: '2023-05-09T21:30:55.000Z'
    data:
      edited: false
      editors:
      - Squeezitgirdle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fd41a323d2f05840fcf3d3fd2d703710.svg
          fullname: David
          isHf: false
          isPro: false
          name: Squeezitgirdle
          type: user
        html: '<p>I have the model + other files directly in this path:</p>

          <p>...\AIchatbot\oogabooga\oobabooga-windows\text-generation-webui\models\TheBloke</p>

          <p>I also have this on my start-webui.bat:<br><code>call python server.py
          --auto-devices --chat --wbits 4 --gpu-memory 22 --groupsize 128</code><br>Which
          potentially could have been causing the issue, though the settings are technically
          correct. </p>

          <p>The only difference I did this time was set the settings before choosing
          the model. After choosing the model it reset the settings again, but I chose
          them again and saved.</p>

          <p>I don''t think choosing the settings before the model made any difference
          but somehow I got it working now.</p>

          '
        raw: "I have the model + other files directly in this path:\n\n...\\AIchatbot\\\
          oogabooga\\oobabooga-windows\\text-generation-webui\\models\\TheBloke\n\n\
          I also have this on my start-webui.bat:\n```call python server.py --auto-devices\
          \ --chat --wbits 4 --gpu-memory 22 --groupsize 128```\nWhich potentially\
          \ could have been causing the issue, though the settings are technically\
          \ correct. \n\nThe only difference I did this time was set the settings\
          \ before choosing the model. After choosing the model it reset the settings\
          \ again, but I chose them again and saved.\n\nI don't think choosing the\
          \ settings before the model made any difference but somehow I got it working\
          \ now."
        updatedAt: '2023-05-09T21:30:55.674Z'
      numEdits: 0
      reactions: []
    id: 645abb8f16dd9c078262ae37
    type: comment
  author: Squeezitgirdle
  content: "I have the model + other files directly in this path:\n\n...\\AIchatbot\\\
    oogabooga\\oobabooga-windows\\text-generation-webui\\models\\TheBloke\n\nI also\
    \ have this on my start-webui.bat:\n```call python server.py --auto-devices --chat\
    \ --wbits 4 --gpu-memory 22 --groupsize 128```\nWhich potentially could have been\
    \ causing the issue, though the settings are technically correct. \n\nThe only\
    \ difference I did this time was set the settings before choosing the model. After\
    \ choosing the model it reset the settings again, but I chose them again and saved.\n\
    \nI don't think choosing the settings before the model made any difference but\
    \ somehow I got it working now."
  created_at: 2023-05-09 20:30:55+00:00
  edited: false
  hidden: false
  id: 645abb8f16dd9c078262ae37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664936604750-noauth.jpeg?w=200&h=200&f=face
      fullname: Peter O'Loughlin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ninja-Ferret
      type: user
    createdAt: '2023-05-11T17:25:58.000Z'
    data:
      edited: true
      editors:
      - Ninja-Ferret
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664936604750-noauth.jpeg?w=200&h=200&f=face
          fullname: Peter O'Loughlin
          isHf: false
          isPro: false
          name: Ninja-Ferret
          type: user
        html: '<p>make a .json file in your TheBloke_stable-vicuna-13B-GPTQ named
          pytorch_model.bin.index with this text </p>

          <p>{<br>  "metadata": {<br>    "total_size": 26031759360<br>  },<br>  "weight_map":
          {<br>    "lm_head.weight": "pytorch_model-00003-of-00003.bin",<br>    "model.embed_tokens.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.0.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.1.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.10.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.11.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.12.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.13.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.14.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.15.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.15.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.15.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.15.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.16.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.17.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.18.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.19.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.2.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.2.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.20.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.20.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.21.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.22.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.23.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.24.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.25.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.26.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.27.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.28.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.input_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.mlp.up_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.post_attention_layernorm.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.29.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.3.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.3.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.30.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.30.mlp.down_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.30.mlp.gate_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.30.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.30.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.30.self_attn.k_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.30.self_attn.o_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.30.self_attn.q_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.30.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.30.self_attn.v_proj.weight":
          "pytorch_model-00002-of-00003.bin",<br>    "model.layers.31.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.31.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.32.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.33.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.34.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.35.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.36.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.37.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.38.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.input_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.mlp.down_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.mlp.gate_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.mlp.up_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.post_attention_layernorm.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.self_attn.k_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.self_attn.o_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.self_attn.q_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.39.self_attn.v_proj.weight":
          "pytorch_model-00003-of-00003.bin",<br>    "model.layers.4.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.4.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.5.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.6.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.7.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.8.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.input_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.mlp.down_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.mlp.gate_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.mlp.up_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.post_attention_layernorm.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.self_attn.k_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.self_attn.o_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.self_attn.q_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.self_attn.rotary_emb.inv_freq":
          "pytorch_model-00001-of-00003.bin",<br>    "model.layers.9.self_attn.v_proj.weight":
          "pytorch_model-00001-of-00003.bin",<br>    "model.norm.weight": "pytorch_model-00003-of-00003.bin"<br>  }<br>}</p>

          '
        raw: "make a .json file in your TheBloke_stable-vicuna-13B-GPTQ named pytorch_model.bin.index\
          \ with this text \n\n{\n  \"metadata\": {\n    \"total_size\": 26031759360\n\
          \  },\n  \"weight_map\": {\n    \"lm_head.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.embed_tokens.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.0.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.1.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.10.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.11.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.12.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.13.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.14.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.15.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.15.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.15.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.15.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.16.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.17.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.18.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.19.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.2.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.2.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.20.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.20.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.21.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.22.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.23.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.24.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.25.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.26.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.27.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.28.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.29.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.3.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.3.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.30.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.30.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.30.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.30.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.30.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.30.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.30.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.30.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.30.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.30.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
          ,\n    \"model.layers.31.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.31.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.32.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.33.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.34.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.35.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.36.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.37.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.38.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.39.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
          ,\n    \"model.layers.4.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.4.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.5.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.6.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.7.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.8.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.layers.9.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
          ,\n    \"model.norm.weight\": \"pytorch_model-00003-of-00003.bin\"\n  }\n\
          }"
        updatedAt: '2023-05-11T17:27:01.403Z'
      numEdits: 1
      reactions: []
    id: 645d2526f1e3b219cb0c4dbd
    type: comment
  author: Ninja-Ferret
  content: "make a .json file in your TheBloke_stable-vicuna-13B-GPTQ named pytorch_model.bin.index\
    \ with this text \n\n{\n  \"metadata\": {\n    \"total_size\": 26031759360\n \
    \ },\n  \"weight_map\": {\n    \"lm_head.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.embed_tokens.weight\": \"pytorch_model-00001-of-00003.bin\",\n\
    \    \"model.layers.0.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.1.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.10.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.11.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.12.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.13.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.14.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.15.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.15.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.15.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.15.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.16.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.17.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.18.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.19.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.2.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.2.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.20.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.20.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.21.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.22.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.23.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.24.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.25.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.26.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.27.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.28.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.input_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.mlp.up_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.post_attention_layernorm.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.29.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.3.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.3.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.30.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.30.mlp.down_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.30.mlp.gate_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.30.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.30.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.30.self_attn.k_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.30.self_attn.o_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.30.self_attn.q_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.30.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.30.self_attn.v_proj.weight\": \"pytorch_model-00002-of-00003.bin\"\
    ,\n    \"model.layers.31.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.31.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.32.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.33.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.34.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.35.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.36.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.37.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.38.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.input_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.mlp.down_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.mlp.gate_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.mlp.up_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.post_attention_layernorm.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.self_attn.k_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.self_attn.o_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.self_attn.q_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.39.self_attn.v_proj.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.layers.4.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.4.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.5.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.6.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.7.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.8.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.mlp.gate_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.mlp.up_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.post_attention_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.self_attn.k_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.self_attn.o_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.self_attn.q_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.self_attn.rotary_emb.inv_freq\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.9.self_attn.v_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.norm.weight\": \"pytorch_model-00003-of-00003.bin\"\n  }\n}"
  created_at: 2023-05-11 16:25:58+00:00
  edited: true
  hidden: false
  id: 645d2526f1e3b219cb0c4dbd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-12T07:46:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You don''t need a pytorch_model.bin.json for a GPTQ model. It doesn''t
          do anything.</p>

          <p>The error is caused by not having GPTQ settings chosen for the model.  There
          was recently a bug in text-generation-webui that meant that GPTQ params
          weren''t being saved properly. It''s now been fixed, so update your text-gen-webui
          to the latest version if you''re still having problems loading the model.</p>

          '
        raw: 'You don''t need a pytorch_model.bin.json for a GPTQ model. It doesn''t
          do anything.


          The error is caused by not having GPTQ settings chosen for the model.  There
          was recently a bug in text-generation-webui that meant that GPTQ params
          weren''t being saved properly. It''s now been fixed, so update your text-gen-webui
          to the latest version if you''re still having problems loading the model.'
        updatedAt: '2023-05-12T07:46:27.909Z'
      numEdits: 0
      reactions: []
    id: 645deed3fa43f072e93d64ee
    type: comment
  author: TheBloke
  content: 'You don''t need a pytorch_model.bin.json for a GPTQ model. It doesn''t
    do anything.


    The error is caused by not having GPTQ settings chosen for the model.  There was
    recently a bug in text-generation-webui that meant that GPTQ params weren''t being
    saved properly. It''s now been fixed, so update your text-gen-webui to the latest
    version if you''re still having problems loading the model.'
  created_at: 2023-05-12 06:46:27+00:00
  edited: false
  hidden: false
  id: 645deed3fa43f072e93d64ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664936604750-noauth.jpeg?w=200&h=200&f=face
      fullname: Peter O'Loughlin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ninja-Ferret
      type: user
    createdAt: '2023-05-12T15:43:25.000Z'
    data:
      edited: false
      editors:
      - Ninja-Ferret
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664936604750-noauth.jpeg?w=200&h=200&f=face
          fullname: Peter O'Loughlin
          isHf: false
          isPro: false
          name: Ninja-Ferret
          type: user
        html: '<p>Ok, thank you for the update :)</p>

          '
        raw: Ok, thank you for the update :)
        updatedAt: '2023-05-12T15:43:25.955Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 645e5e9d144c62420f2947e9
    type: comment
  author: Ninja-Ferret
  content: Ok, thank you for the update :)
  created_at: 2023-05-12 14:43:25+00:00
  edited: false
  hidden: false
  id: 645e5e9d144c62420f2947e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fd2bf3a84b6a517d1c929ace3de25ff.svg
      fullname: Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narfel
      type: user
    createdAt: '2023-05-22T11:37:52.000Z'
    data:
      edited: true
      editors:
      - Narfel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fd2bf3a84b6a517d1c929ace3de25ff.svg
          fullname: Chen
          isHf: false
          isPro: false
          name: Narfel
          type: user
        html: '<p>Quick and dirty on a fresh install:</p>

          <ol>

          <li>Install ooba then update ooba (you need gcc installed, e.g. "apt install
          build-essential" or "Developer Tools" on Fedora)</li>

          <li>at the end use "Manually specify a Hugging Face model" to download "TheBloke/stable-vicuna-13B-GPTQ"
          and quit the installer, do not attempt to run it</li>

          <li>edit webui.py to use CMD_FLAGS = ''--auto-devices --chat --wbits 4 --gpu-memory
          22 --groupsize 128'' instead of "CMD_FLAGS = ''--chat --model-menu''" and
          start ooba</li>

          <li>set settings as outlined above or in README and reload model<br>You
          can then edit the webui.py back to what it was or what you prefer.</li>

          </ol>

          <p>Possible pitfalls:</p>

          <ul>

          <li>you have not updated ooba</li>

          <li>you have no gcc installed</li>

          <li>you read this in a month or more from now</li>

          <li>you''re lost in dependecy hell and noone can hear you scream</li>

          </ul>

          <p>This was tested on a fresh install of ubuntu 22.04 wsl, gtx 3060 12GB,
          amd ryzen 5 3600</p>

          <p>#edit: copy&amp;paste error in flags</p>

          '
        raw: 'Quick and dirty on a fresh install:

          1. Install ooba then update ooba (you need gcc installed, e.g. "apt install
          build-essential" or "Developer Tools" on Fedora)

          2. at the end use "Manually specify a Hugging Face model" to download "TheBloke/stable-vicuna-13B-GPTQ"
          and quit the installer, do not attempt to run it

          4. edit webui.py to use CMD_FLAGS = ''--auto-devices --chat --wbits 4 --gpu-memory
          22 --groupsize 128'' instead of "CMD_FLAGS = ''--chat --model-menu''" and
          start ooba

          5. set settings as outlined above or in README and reload model

          You can then edit the webui.py back to what it was or what you prefer.


          Possible pitfalls:

          - you have not updated ooba

          - you have no gcc installed

          - you read this in a month or more from now

          - you''re lost in dependecy hell and noone can hear you scream


          This was tested on a fresh install of ubuntu 22.04 wsl, gtx 3060 12GB, amd
          ryzen 5 3600


          #edit: copy&paste error in flags'
        updatedAt: '2023-05-22T13:21:47.196Z'
      numEdits: 1
      reactions: []
    id: 646b5410f85ebf65c5298bdb
    type: comment
  author: Narfel
  content: 'Quick and dirty on a fresh install:

    1. Install ooba then update ooba (you need gcc installed, e.g. "apt install build-essential"
    or "Developer Tools" on Fedora)

    2. at the end use "Manually specify a Hugging Face model" to download "TheBloke/stable-vicuna-13B-GPTQ"
    and quit the installer, do not attempt to run it

    4. edit webui.py to use CMD_FLAGS = ''--auto-devices --chat --wbits 4 --gpu-memory
    22 --groupsize 128'' instead of "CMD_FLAGS = ''--chat --model-menu''" and start
    ooba

    5. set settings as outlined above or in README and reload model

    You can then edit the webui.py back to what it was or what you prefer.


    Possible pitfalls:

    - you have not updated ooba

    - you have no gcc installed

    - you read this in a month or more from now

    - you''re lost in dependecy hell and noone can hear you scream


    This was tested on a fresh install of ubuntu 22.04 wsl, gtx 3060 12GB, amd ryzen
    5 3600


    #edit: copy&paste error in flags'
  created_at: 2023-05-22 10:37:52+00:00
  edited: true
  hidden: false
  id: 646b5410f85ebf65c5298bdb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-22T11:40:24.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Narfel&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Narfel\">@<span class=\"\
          underline\">Narfel</span></a></span>\n\n\t</span></span> Why is this needed?\
          \ The instructions in the README describe how to download a model and apply\
          \ and save the settings.  Did something change in text-gen-ui to make the\
          \ README instructions no longer work?</p>\n<p>Also I don't understand step\
          \ 3 - your before and after edits are the same?</p>\n"
        raw: '@Narfel Why is this needed? The instructions in the README describe
          how to download a model and apply and save the settings.  Did something
          change in text-gen-ui to make the README instructions no longer work?


          Also I don''t understand step 3 - your before and after edits are the same?'
        updatedAt: '2023-05-22T11:40:24.618Z'
      numEdits: 0
      reactions: []
    id: 646b54a8db697c798a374faf
    type: comment
  author: TheBloke
  content: '@Narfel Why is this needed? The instructions in the README describe how
    to download a model and apply and save the settings.  Did something change in
    text-gen-ui to make the README instructions no longer work?


    Also I don''t understand step 3 - your before and after edits are the same?'
  created_at: 2023-05-22 10:40:24+00:00
  edited: false
  hidden: false
  id: 646b54a8db697c798a374faf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fd2bf3a84b6a517d1c929ace3de25ff.svg
      fullname: Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Narfel
      type: user
    createdAt: '2023-05-22T13:56:00.000Z'
    data:
      edited: true
      editors:
      - Narfel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fd2bf3a84b6a517d1c929ace3de25ff.svg
          fullname: Chen
          isHf: false
          isPro: false
          name: Narfel
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> This is more\
          \ a potential error guide for new users that might come from a ooba install\
          \ guide somewhere and have not downloaded or ran a model or if (in my case)\
          \ you start troubleshooting from a clean slate. The reasoning behind this\
          \ is that the ui won't start and you would never reach the settings inside\
          \ the ui.<br>If stable-vicuna-13B-GPTQ is the first ever model downloaded\
          \ the ui will not load. Normally you should be able to start the ui by selecting\
          \ the \"m\" option, but that is currently not working (\"you must select\
          \ a model...\"). Even if it would load the ui, it wouldnot immediately obvious\
          \ that you would need to select this option after erroring out. That's all.\
          \ It is however producing this exact error, so I wrote it here for posterity\
          \ :)</p>\n<p>Oh and the step 3 was stupidity on my side. I have since edited\
          \ it to reflect the actual values.</p>\n"
        raw: "@TheBloke This is more a potential error guide for new users that might\
          \ come from a ooba install guide somewhere and have not downloaded or ran\
          \ a model or if (in my case) you start troubleshooting from a clean slate.\
          \ The reasoning behind this is that the ui won't start and you would never\
          \ reach the settings inside the ui. \nIf stable-vicuna-13B-GPTQ is the first\
          \ ever model downloaded the ui will not load. Normally you should be able\
          \ to start the ui by selecting the \"m\" option, but that is currently not\
          \ working (\"you must select a model...\"). Even if it would load the ui,\
          \ it wouldnot immediately obvious that you would need to select this option\
          \ after erroring out. That's all. It is however producing this exact error,\
          \ so I wrote it here for posterity :)\n\nOh and the step 3 was stupidity\
          \ on my side. I have since edited it to reflect the actual values."
        updatedAt: '2023-05-22T13:57:27.849Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 646b7470ed228272132dac4b
    type: comment
  author: Narfel
  content: "@TheBloke This is more a potential error guide for new users that might\
    \ come from a ooba install guide somewhere and have not downloaded or ran a model\
    \ or if (in my case) you start troubleshooting from a clean slate. The reasoning\
    \ behind this is that the ui won't start and you would never reach the settings\
    \ inside the ui. \nIf stable-vicuna-13B-GPTQ is the first ever model downloaded\
    \ the ui will not load. Normally you should be able to start the ui by selecting\
    \ the \"m\" option, but that is currently not working (\"you must select a model...\"\
    ). Even if it would load the ui, it wouldnot immediately obvious that you would\
    \ need to select this option after erroring out. That's all. It is however producing\
    \ this exact error, so I wrote it here for posterity :)\n\nOh and the step 3 was\
    \ stupidity on my side. I have since edited it to reflect the actual values."
  created_at: 2023-05-22 12:56:00+00:00
  edited: true
  hidden: false
  id: 646b7470ed228272132dac4b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'OSError: Error no file named pytorch_model.bin, tf_model.h5, model.ckpt.index
  or flax_model.msgpack found in directory models\TheBlokestable-vicuna-13B-GPTQ.'
