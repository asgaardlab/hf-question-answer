!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vmajor
conflicting_files: null
created_at: 2023-04-29 04:49:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-29T05:49:21.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<p>When attempting to load stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\
          \ using the standard python code that I use to test all other GPTQ models\
          \ I get this:</p>\n<pre><code>raise RuntimeError('Error(s) in loading state_dict\
          \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM:\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([40, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 640]).\n        size mismatch\
          \ for model.layers.0.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([40, 5120]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 5120]).\n...\n</code></pre>\n"
        raw: "When attempting to load stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\
          \ using the standard python code that I use to test all other GPTQ models\
          \ I get this:\r\n```\r\nraise RuntimeError('Error(s) in loading state_dict\
          \ for {}:\\n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM:\r\n        size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([40, 640]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 640]).\r\n        size mismatch\
          \ for model.layers.0.self_attn.k_proj.scales: copying a param with shape\
          \ torch.Size([40, 5120]) from checkpoint, the shape in current model is\
          \ torch.Size([1, 5120]).\r\n...\r\n```"
        updatedAt: '2023-04-29T05:49:21.963Z'
      numEdits: 0
      reactions: []
    id: 644cafe1f30d4ab8ea13ce06
    type: comment
  author: vmajor
  content: "When attempting to load stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\
    \ using the standard python code that I use to test all other GPTQ models I get\
    \ this:\r\n```\r\nraise RuntimeError('Error(s) in loading state_dict for {}:\\\
    n\\t{}'.format(\r\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\r\
    \n        size mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a\
    \ param with shape torch.Size([40, 640]) from checkpoint, the shape in current\
    \ model is torch.Size([1, 640]).\r\n        size mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([40, 5120]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 5120]).\r\n...\r\n```"
  created_at: 2023-04-29 04:49:21+00:00
  edited: false
  hidden: false
  id: 644cafe1f30d4ab8ea13ce06
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-29T07:35:11.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>First thing to do is double check the file downloaded OK - sha256sum\
          \ it.</p>\n<pre><code>root@6d4bbc85231a:~/gptq-llama# sha256sum /workspace/stable-vicuna-13B-GPTQ/stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\n\
          442d71b56bc16721d28aeb2d5e0ba07cf04bfb61cc7af47993d5f0a15133b520  /workspace/stable-vicuna-13B-GPTQ/stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\n\
          </code></pre>\n<p>I ran your single loop test code and it runs OK (bit of\
          \ a response problem here, but no errors!):</p>\n<pre><code>root@6d4bbc85231a:~/gptq-llama#\
          \ python do_gptq_inf.py /workspace/stable-vicuna-13B-GPTQ stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\
          \ --text \"### Human: write a story about llamas\n### Assistant:\"\nLoading\
          \ model ...\nFound 3 unique KN Linear values.\nWarming up autotune cache\
          \ ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:47&lt;00:00,\
          \  3.97s/it]\nFound 1 unique fused mlp KN values.\nWarming up autotune cache\
          \ ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:41&lt;00:00,\
          \  3.46s/it]\nDone.\nOutput: &lt;/s&gt;### Human: write a story about llamas\n\
          ### Assistant: Once upon a time, there lived a llama named Llama. He was\
          \ a special creature, with a heart of gold and a love for all things equine.\
          \ Llama was a horse with a dream, to ride across the land and be a star.\n\
          ### He loved to sing and dance, with a twinkle in his eye.\n### He was a\
          \ horse of a different color, with a heart of gold and a love for all.\n\
          ### Llama was a horse of a different color, with a heart of gold and a love\
          \ for all.\n### He loved to ride and sing, with a twinkle in his eye.\n\
          ### Llama was a horse of a different color, with a heart of gold and a love\
          \ for all.\n### He was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### Llama was a horse of a different color, with\
          \ a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n###\n</code></pre>\n<p>Code used to test:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">import</span> torch.nn <span class=\"hljs-keyword\"\
          >as</span> nn\n<span class=\"hljs-keyword\">import</span> quant\n<span class=\"\
          hljs-keyword\">from</span> gptq <span class=\"hljs-keyword\">import</span>\
          \ GPTQ\n<span class=\"hljs-keyword\">from</span> utils <span class=\"hljs-keyword\"\
          >import</span> find_layers, DEV, set_seed, get_wikitext2, get_ptb, get_c4,\
          \ get_ptb_new, get_c4_new, get_loaders\n<span class=\"hljs-keyword\">import</span>\
          \ transformers\n<span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> AutoTokenizer\n<span class=\"hljs-keyword\"\
          >import</span> argparse\n<span class=\"hljs-keyword\">import</span> warnings\n\
          \n<span class=\"hljs-comment\"># Suppress warnings from the specified modules</span>\n\
          warnings.filterwarnings(<span class=\"hljs-string\">\"ignore\"</span>, module=<span\
          \ class=\"hljs-string\">\"safetensors\"</span>)\nwarnings.filterwarnings(<span\
          \ class=\"hljs-string\">\"ignore\"</span>, module=<span class=\"hljs-string\"\
          >\"torch\"</span>)\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">get_llama</span>(<span class=\"hljs-params\">model</span>):\n\
          \n    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >skip</span>(<span class=\"hljs-params\">*args, **kwargs</span>):\n    \
          \    <span class=\"hljs-keyword\">pass</span>\n\n    torch.nn.init.kaiming_uniform_\
          \ = skip\n    torch.nn.init.uniform_ = skip\n    torch.nn.init.normal_ =\
          \ skip\n    <span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype=<span class=\"hljs-string\">'auto'</span>)\n    model.seqlen\
          \ = <span class=\"hljs-number\">2048</span>\n    <span class=\"hljs-keyword\"\
          >return</span> model\n\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">load_quant</span>(<span class=\"hljs-params\"\
          >model, checkpoint, wbits, groupsize=-<span class=\"hljs-number\">1</span>,\
          \ fused_mlp=<span class=\"hljs-literal\">True</span>, <span class=\"hljs-built_in\"\
          >eval</span>=<span class=\"hljs-literal\">True</span>, warmup_autotune=<span\
          \ class=\"hljs-literal\">True</span></span>):\n    <span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> LlamaConfig,\
          \ LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\n\
          \    <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >noop</span>(<span class=\"hljs-params\">*args, **kwargs</span>):\n    \
          \    <span class=\"hljs-keyword\">pass</span>\n\n    torch.nn.init.kaiming_uniform_\
          \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ =\
          \ noop\n\n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = <span class=\"hljs-literal\">False</span>\n    torch.set_default_dtype(torch.half)\n\
          \    model = LlamaForCausalLM(config)\n    torch.set_default_dtype(torch.<span\
          \ class=\"hljs-built_in\">float</span>)\n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">eval</span>:\n        model = model.<span\
          \ class=\"hljs-built_in\">eval</span>()\n    layers = find_layers(model)\n\
          \    <span class=\"hljs-keyword\">for</span> name <span class=\"hljs-keyword\"\
          >in</span> [<span class=\"hljs-string\">'lm_head'</span>]:\n        <span\
          \ class=\"hljs-keyword\">if</span> name <span class=\"hljs-keyword\">in</span>\
          \ layers:\n            <span class=\"hljs-keyword\">del</span> layers[name]\n\
          \    quant.make_quant_linear(model, layers, wbits, groupsize)\n\n    <span\
          \ class=\"hljs-keyword\">del</span> layers\n\n    <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">'Loading model ...'</span>)\n\
          \    <span class=\"hljs-keyword\">if</span> checkpoint.endswith(<span class=\"\
          hljs-string\">'.safetensors'</span>):\n        <span class=\"hljs-keyword\"\
          >from</span> safetensors.torch <span class=\"hljs-keyword\">import</span>\
          \ load_file <span class=\"hljs-keyword\">as</span> safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=<span class=\"hljs-literal\">False</span>)\n    <span class=\"\
          hljs-keyword\">else</span>:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=<span class=\"hljs-literal\">False</span>)\n\n    quant.make_quant_attn(model)\n\
          \    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >eval</span> <span class=\"hljs-keyword\">and</span> fused_mlp:\n      \
          \  quant.make_fused_mlp(model)\n\n    <span class=\"hljs-keyword\">if</span>\
          \ warmup_autotune:\n        quant.autotune_warmup_linear(model, transpose=<span\
          \ class=\"hljs-keyword\">not</span> (<span class=\"hljs-built_in\">eval</span>))\n\
          \        <span class=\"hljs-keyword\">if</span> <span class=\"hljs-built_in\"\
          >eval</span> <span class=\"hljs-keyword\">and</span> fused_mlp:\n      \
          \      quant.autotune_warmup_fused(model)\n    model.seqlen = <span class=\"\
          hljs-number\">2048</span>\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">'Done.'</span>)\n\n    <span class=\"hljs-keyword\"\
          >return</span> model\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">run_llama_inference</span>(<span class=\"hljs-params\"\
          ></span>\n<span class=\"hljs-params\">    model_path,</span>\n<span class=\"\
          hljs-params\">    wbits=<span class=\"hljs-number\">4</span>,</span>\n<span\
          \ class=\"hljs-params\">    groupsize=-<span class=\"hljs-number\">1</span>,</span>\n\
          <span class=\"hljs-params\">    load_path=<span class=\"hljs-string\">\"\
          \"</span>,</span>\n<span class=\"hljs-params\">    text=<span class=\"hljs-string\"\
          >\"\"</span>,</span>\n<span class=\"hljs-params\">    min_length=<span class=\"\
          hljs-number\">10</span>,</span>\n<span class=\"hljs-params\">    max_length=<span\
          \ class=\"hljs-number\">1024</span>,</span>\n<span class=\"hljs-params\"\
          >    top_p=<span class=\"hljs-number\">0.7</span>,</span>\n<span class=\"\
          hljs-params\">    temperature=<span class=\"hljs-number\">0.8</span>,</span>\n\
          <span class=\"hljs-params\">    device=<span class=\"hljs-number\">0</span>,</span>\n\
          <span class=\"hljs-params\"></span>):\n\n    <span class=\"hljs-keyword\"\
          >if</span> load_path:\n        model = load_quant(model_path, load_path,\
          \ wbits, groupsize)\n    <span class=\"hljs-keyword\">else</span>:\n   \
          \     model = get_llama(model_path)\n        model.<span class=\"hljs-built_in\"\
          >eval</span>()\n\n    model.to(DEV)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=<span class=\"hljs-literal\">False</span>)\n    input_ids = tokenizer.encode(text,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(DEV)\n\n\
          \    <span class=\"hljs-keyword\">with</span> torch.no_grad():\n       \
          \ generated_ids = model.generate(\n            input_ids,\n            do_sample=<span\
          \ class=\"hljs-literal\">True</span>,\n            min_length=min_length,\n\
          \            max_length=max_length,\n            top_p=top_p,\n        \
          \    temperature=temperature,\n        )\n    <span class=\"hljs-keyword\"\
          >return</span> tokenizer.decode([el.item() <span class=\"hljs-keyword\"\
          >for</span> el <span class=\"hljs-keyword\">in</span> generated_ids[<span\
          \ class=\"hljs-number\">0</span>]])\n\n<span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">main</span>():\n    parser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">\"Summarize an article using Vicuna.\"</span>)\n\
          \    parser.add_argument(<span class=\"hljs-string\">'model_dir'</span>,\
          \ <span class=\"hljs-built_in\">type</span>=<span class=\"hljs-built_in\"\
          >str</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'The model dir to load from'</span>)\n    parser.add_argument(<span class=\"\
          hljs-string\">'model_file'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\">help</span>=<span\
          \ class=\"hljs-string\">'The model file to load'</span>)\n    parser.add_argument(<span\
          \ class=\"hljs-string\">'--text'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">str</span>, required=<span class=\"hljs-literal\"\
          >True</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'The text to summarize.'</span>)\n    parser.add_argument(<span class=\"\
          hljs-string\">'--wbits'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">int</span>, default=<span class=\"hljs-number\"\
          >4</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'The model file to load'</span>)\n    parser.add_argument(<span class=\"\
          hljs-string\">'--groupsize'</span>, <span class=\"hljs-built_in\">type</span>=<span\
          \ class=\"hljs-built_in\">int</span>, default=<span class=\"hljs-number\"\
          >128</span>, <span class=\"hljs-built_in\">help</span>=<span class=\"hljs-string\"\
          >'The model file to load'</span>)\n    args = parser.parse_args()\n\n  \
          \  output = run_llama_inference(\n        args.model_dir,\n        wbits=args.wbits,\n\
          \        groupsize=args.groupsize,\n        load_path=<span class=\"hljs-string\"\
          >f\"<span class=\"hljs-subst\">{args.model_dir}</span>/<span class=\"hljs-subst\"\
          >{args.model_file}</span>\"</span>,\n        text=args.text,\n    )\n\n\
          \    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\"\
          >open</span>(<span class=\"hljs-string\">\"output.txt\"</span>, <span class=\"\
          hljs-string\">\"a\"</span>, encoding=<span class=\"hljs-string\">\"utf-8\"\
          </span>) <span class=\"hljs-keyword\">as</span> f:\n        f.write(<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{args.text}</span>\\\
          n<span class=\"hljs-subst\">{output}</span>\\n\"</span>)\n\n    <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Output: <span\
          \ class=\"hljs-subst\">{output}</span>\"</span>)\n\n<span class=\"hljs-keyword\"\
          >if</span> __name__ == <span class=\"hljs-string\">\"__main__\"</span>:\n\
          \    main()\n</code></pre>\n"
        raw: "First thing to do is double check the file downloaded OK - sha256sum\
          \ it.\n\n```\nroot@6d4bbc85231a:~/gptq-llama# sha256sum /workspace/stable-vicuna-13B-GPTQ/stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\n\
          442d71b56bc16721d28aeb2d5e0ba07cf04bfb61cc7af47993d5f0a15133b520  /workspace/stable-vicuna-13B-GPTQ/stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\n\
          ```\n\nI ran your single loop test code and it runs OK (bit of a response\
          \ problem here, but no errors!):\n```\nroot@6d4bbc85231a:~/gptq-llama# python\
          \ do_gptq_inf.py /workspace/stable-vicuna-13B-GPTQ stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\
          \ --text \"### Human: write a story about llamas\n### Assistant:\"\nLoading\
          \ model ...\nFound 3 unique KN Linear values.\nWarming up autotune cache\
          \ ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:47<00:00,  3.97s/it]\n\
          Found 1 unique fused mlp KN values.\nWarming up autotune cache ...\n100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588| 12/12 [00:41<00:00,  3.46s/it]\nDone.\n\
          Output: </s>### Human: write a story about llamas\n### Assistant: Once upon\
          \ a time, there lived a llama named Llama. He was a special creature, with\
          \ a heart of gold and a love for all things equine. Llama was a horse with\
          \ a dream, to ride across the land and be a star.\n### He loved to sing\
          \ and dance, with a twinkle in his eye.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ loved to ride and sing, with a twinkle in his eye.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n### Llama was a horse\
          \ of a different color, with a heart of gold and a love for all.\n### He\
          \ was a horse of a different color, with a heart of gold and a love for\
          \ all.\n### Llama was a horse of a different color, with a heart of gold\
          \ and a love for all.\n### He was a horse of a different color, with a heart\
          \ of gold and a love for all.\n### Llama was a horse of a different color,\
          \ with a heart of gold and a love for all.\n### He was a horse of a different\
          \ color, with a heart of gold and a love for all.\n###\n```\n\nCode used\
          \ to test:\n```python\nimport torch\nimport torch.nn as nn\nimport quant\n\
          from gptq import GPTQ\nfrom utils import find_layers, DEV, set_seed, get_wikitext2,\
          \ get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders\nimport transformers\n\
          from transformers import AutoTokenizer\nimport argparse\nimport warnings\n\
          \n# Suppress warnings from the specified modules\nwarnings.filterwarnings(\"\
          ignore\", module=\"safetensors\")\nwarnings.filterwarnings(\"ignore\", module=\"\
          torch\")\n\ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n  \
          \      pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_\
          \ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import\
          \ LlamaForCausalLM\n    model = LlamaForCausalLM.from_pretrained(model,\
          \ torch_dtype='auto')\n    model.seqlen = 2048\n    return model\n\n\ndef\
          \ load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True,\
          \ warmup_autotune=True):\n    from transformers import LlamaConfig, LlamaForCausalLM\n\
          \    config = LlamaConfig.from_pretrained(model)\n\n    def noop(*args,\
          \ **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_ = noop\n\
          \    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\n\
          \    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
          \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
          \    torch.set_default_dtype(torch.float)\n    if eval:\n        model =\
          \ model.eval()\n    layers = find_layers(model)\n    for name in ['lm_head']:\n\
          \        if name in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
          \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model\
          \ ...')\n    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch\
          \ import load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
          \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
          \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
          \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n      \
          \  quant.autotune_warmup_linear(model, transpose=not (eval))\n        if\
          \ eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
          \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef\
          \ run_llama_inference(\n    model_path,\n    wbits=4,\n    groupsize=-1,\n\
          \    load_path=\"\",\n    text=\"\",\n    min_length=10,\n    max_length=1024,\n\
          \    top_p=0.7,\n    temperature=0.8,\n    device=0,\n):\n\n    if load_path:\n\
          \        model = load_quant(model_path, load_path, wbits, groupsize)\n \
          \   else:\n        model = get_llama(model_path)\n        model.eval()\n\
          \n    model.to(DEV)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ use_fast=False)\n    input_ids = tokenizer.encode(text, return_tensors=\"\
          pt\").to(DEV)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n\
          \            input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
          \            max_length=max_length,\n            top_p=top_p,\n        \
          \    temperature=temperature,\n        )\n    return tokenizer.decode([el.item()\
          \ for el in generated_ids[0]])\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"\
          Summarize an article using Vicuna.\")\n    parser.add_argument('model_dir',\
          \ type=str, help='The model dir to load from')\n    parser.add_argument('model_file',\
          \ type=str, help='The model file to load')\n    parser.add_argument('--text',\
          \ type=str, required=True, help='The text to summarize.')\n    parser.add_argument('--wbits',\
          \ type=int, default=4, help='The model file to load')\n    parser.add_argument('--groupsize',\
          \ type=int, default=128, help='The model file to load')\n    args = parser.parse_args()\n\
          \n    output = run_llama_inference(\n        args.model_dir,\n        wbits=args.wbits,\n\
          \        groupsize=args.groupsize,\n        load_path=f\"{args.model_dir}/{args.model_file}\"\
          ,\n        text=args.text,\n    )\n\n    with open(\"output.txt\", \"a\"\
          , encoding=\"utf-8\") as f:\n        f.write(f\"{args.text}\\n{output}\\\
          n\")\n\n    print(f\"Output: {output}\")\n\nif __name__ == \"__main__\"\
          :\n    main()\n```"
        updatedAt: '2023-04-29T07:36:57.375Z'
      numEdits: 1
      reactions: []
    id: 644cc8af97a3b0904a481e3e
    type: comment
  author: TheBloke
  content: "First thing to do is double check the file downloaded OK - sha256sum it.\n\
    \n```\nroot@6d4bbc85231a:~/gptq-llama# sha256sum /workspace/stable-vicuna-13B-GPTQ/stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\n\
    442d71b56bc16721d28aeb2d5e0ba07cf04bfb61cc7af47993d5f0a15133b520  /workspace/stable-vicuna-13B-GPTQ/stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\n\
    ```\n\nI ran your single loop test code and it runs OK (bit of a response problem\
    \ here, but no errors!):\n```\nroot@6d4bbc85231a:~/gptq-llama# python do_gptq_inf.py\
    \ /workspace/stable-vicuna-13B-GPTQ stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\
    \ --text \"### Human: write a story about llamas\n### Assistant:\"\nLoading model\
    \ ...\nFound 3 unique KN Linear values.\nWarming up autotune cache ...\n100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588| 12/12 [00:47<00:00,  3.97s/it]\nFound 1 unique fused mlp KN\
    \ values.\nWarming up autotune cache ...\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12/12 [00:41<00:00,\
    \  3.46s/it]\nDone.\nOutput: </s>### Human: write a story about llamas\n### Assistant:\
    \ Once upon a time, there lived a llama named Llama. He was a special creature,\
    \ with a heart of gold and a love for all things equine. Llama was a horse with\
    \ a dream, to ride across the land and be a star.\n### He loved to sing and dance,\
    \ with a twinkle in his eye.\n### He was a horse of a different color, with a\
    \ heart of gold and a love for all.\n### Llama was a horse of a different color,\
    \ with a heart of gold and a love for all.\n### He loved to ride and sing, with\
    \ a twinkle in his eye.\n### Llama was a horse of a different color, with a heart\
    \ of gold and a love for all.\n### He was a horse of a different color, with a\
    \ heart of gold and a love for all.\n### Llama was a horse of a different color,\
    \ with a heart of gold and a love for all.\n### He was a horse of a different\
    \ color, with a heart of gold and a love for all.\n### Llama was a horse of a\
    \ different color, with a heart of gold and a love for all.\n### He was a horse\
    \ of a different color, with a heart of gold and a love for all.\n### Llama was\
    \ a horse of a different color, with a heart of gold and a love for all.\n###\
    \ He was a horse of a different color, with a heart of gold and a love for all.\n\
    ### Llama was a horse of a different color, with a heart of gold and a love for\
    \ all.\n### He was a horse of a different color, with a heart of gold and a love\
    \ for all.\n### Llama was a horse of a different color, with a heart of gold and\
    \ a love for all.\n### He was a horse of a different color, with a heart of gold\
    \ and a love for all.\n### Llama was a horse of a different color, with a heart\
    \ of gold and a love for all.\n### He was a horse of a different color, with a\
    \ heart of gold and a love for all.\n### Llama was a horse of a different color,\
    \ with a heart of gold and a love for all.\n### He was a horse of a different\
    \ color, with a heart of gold and a love for all.\n### Llama was a horse of a\
    \ different color, with a heart of gold and a love for all.\n### He was a horse\
    \ of a different color, with a heart of gold and a love for all.\n### Llama was\
    \ a horse of a different color, with a heart of gold and a love for all.\n###\
    \ He was a horse of a different color, with a heart of gold and a love for all.\n\
    ### Llama was a horse of a different color, with a heart of gold and a love for\
    \ all.\n### He was a horse of a different color, with a heart of gold and a love\
    \ for all.\n### Llama was a horse of a different color, with a heart of gold and\
    \ a love for all.\n### He was a horse of a different color, with a heart of gold\
    \ and a love for all.\n### Llama was a horse of a different color, with a heart\
    \ of gold and a love for all.\n### He was a horse of a different color, with a\
    \ heart of gold and a love for all.\n### Llama was a horse of a different color,\
    \ with a heart of gold and a love for all.\n### He was a horse of a different\
    \ color, with a heart of gold and a love for all.\n### Llama was a horse of a\
    \ different color, with a heart of gold and a love for all.\n### He was a horse\
    \ of a different color, with a heart of gold and a love for all.\n### Llama was\
    \ a horse of a different color, with a heart of gold and a love for all.\n###\
    \ He was a horse of a different color, with a heart of gold and a love for all.\n\
    ### Llama was a horse of a different color, with a heart of gold and a love for\
    \ all.\n### He was a horse of a different color, with a heart of gold and a love\
    \ for all.\n### Llama was a horse of a different color, with a heart of gold and\
    \ a love for all.\n### He was a horse of a different color, with a heart of gold\
    \ and a love for all.\n###\n```\n\nCode used to test:\n```python\nimport torch\n\
    import torch.nn as nn\nimport quant\nfrom gptq import GPTQ\nfrom utils import\
    \ find_layers, DEV, set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new, get_c4_new,\
    \ get_loaders\nimport transformers\nfrom transformers import AutoTokenizer\nimport\
    \ argparse\nimport warnings\n\n# Suppress warnings from the specified modules\n\
    warnings.filterwarnings(\"ignore\", module=\"safetensors\")\nwarnings.filterwarnings(\"\
    ignore\", module=\"torch\")\n\ndef get_llama(model):\n\n    def skip(*args, **kwargs):\n\
    \        pass\n\n    torch.nn.init.kaiming_uniform_ = skip\n    torch.nn.init.uniform_\
    \ = skip\n    torch.nn.init.normal_ = skip\n    from transformers import LlamaForCausalLM\n\
    \    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n   \
    \ model.seqlen = 2048\n    return model\n\n\ndef load_quant(model, checkpoint,\
    \ wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n   \
    \ from transformers import LlamaConfig, LlamaForCausalLM\n    config = LlamaConfig.from_pretrained(model)\n\
    \n    def noop(*args, **kwargs):\n        pass\n\n    torch.nn.init.kaiming_uniform_\
    \ = noop\n    torch.nn.init.uniform_ = noop\n    torch.nn.init.normal_ = noop\n\
    \n    torch.set_default_dtype(torch.half)\n    transformers.modeling_utils._init_weights\
    \ = False\n    torch.set_default_dtype(torch.half)\n    model = LlamaForCausalLM(config)\n\
    \    torch.set_default_dtype(torch.float)\n    if eval:\n        model = model.eval()\n\
    \    layers = find_layers(model)\n    for name in ['lm_head']:\n        if name\
    \ in layers:\n            del layers[name]\n    quant.make_quant_linear(model,\
    \ layers, wbits, groupsize)\n\n    del layers\n\n    print('Loading model ...')\n\
    \    if checkpoint.endswith('.safetensors'):\n        from safetensors.torch import\
    \ load_file as safe_load\n        model.load_state_dict(safe_load(checkpoint),\
    \ strict=False)\n    else:\n        model.load_state_dict(torch.load(checkpoint),\
    \ strict=False)\n\n    quant.make_quant_attn(model)\n    if eval and fused_mlp:\n\
    \        quant.make_fused_mlp(model)\n\n    if warmup_autotune:\n        quant.autotune_warmup_linear(model,\
    \ transpose=not (eval))\n        if eval and fused_mlp:\n            quant.autotune_warmup_fused(model)\n\
    \    model.seqlen = 2048\n    print('Done.')\n\n    return model\n\ndef run_llama_inference(\n\
    \    model_path,\n    wbits=4,\n    groupsize=-1,\n    load_path=\"\",\n    text=\"\
    \",\n    min_length=10,\n    max_length=1024,\n    top_p=0.7,\n    temperature=0.8,\n\
    \    device=0,\n):\n\n    if load_path:\n        model = load_quant(model_path,\
    \ load_path, wbits, groupsize)\n    else:\n        model = get_llama(model_path)\n\
    \        model.eval()\n\n    model.to(DEV)\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ use_fast=False)\n    input_ids = tokenizer.encode(text, return_tensors=\"pt\"\
    ).to(DEV)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n\
    \            input_ids,\n            do_sample=True,\n            min_length=min_length,\n\
    \            max_length=max_length,\n            top_p=top_p,\n            temperature=temperature,\n\
    \        )\n    return tokenizer.decode([el.item() for el in generated_ids[0]])\n\
    \ndef main():\n    parser = argparse.ArgumentParser(description=\"Summarize an\
    \ article using Vicuna.\")\n    parser.add_argument('model_dir', type=str, help='The\
    \ model dir to load from')\n    parser.add_argument('model_file', type=str, help='The\
    \ model file to load')\n    parser.add_argument('--text', type=str, required=True,\
    \ help='The text to summarize.')\n    parser.add_argument('--wbits', type=int,\
    \ default=4, help='The model file to load')\n    parser.add_argument('--groupsize',\
    \ type=int, default=128, help='The model file to load')\n    args = parser.parse_args()\n\
    \n    output = run_llama_inference(\n        args.model_dir,\n        wbits=args.wbits,\n\
    \        groupsize=args.groupsize,\n        load_path=f\"{args.model_dir}/{args.model_file}\"\
    ,\n        text=args.text,\n    )\n\n    with open(\"output.txt\", \"a\", encoding=\"\
    utf-8\") as f:\n        f.write(f\"{args.text}\\n{output}\\n\")\n\n    print(f\"\
    Output: {output}\")\n\nif __name__ == \"__main__\":\n    main()\n```"
  created_at: 2023-04-29 06:35:11+00:00
  edited: true
  hidden: false
  id: 644cc8af97a3b0904a481e3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-29T13:37:41.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<p>Yea... sorry for the noise. It indeed works. The issue is that I\
          \ have several test mules and this one was set to groupsize = -1 at invocation.\
          \ This model wants groupsize=128 otherwise it fails with the error I reported.</p>\n\
          <p>But there is indeed something wrong with it or with the way we are interfacing\
          \ with it because the output is not useful. Do you think something happened\
          \ to it during merge? Even when I change the input section to this and just\
          \ press enter to default to the preset I get a bad result. It does not infer.\
          \ To me it looks like a reconstructed tokenizer output, rather than model\
          \ inference result, and like in your example, it also repeats. I got something\
          \ similar from the Alpacino13b 4bit.safetensors model - it just repeated\
          \ the input text as output. Do you know what may be causing this?</p>\n\
          <pre><code>prompt = \"\"\"\\\n    ### Human: Please provide a concise summary\
          \ of the following news article, capturing the key information and stating\
          \ company ticker symbols, and government entity abbreviations, whenever\
          \ possible: Credit Suisse logged asset outflows of more than $68 billion\
          \ during first-quarter collapse. Credit Suisse on Monday revealed that it\
          \ suffered net asset outflows of 61.2 billion Swiss francs ($68.6 billion)\
          \ during the first-quarter collapse that culminated in its emergency rescue\
          \ by domestic rival UBS. The stricken Swiss lender posted a one-off 12.43\
          \ billion Swiss franc profit for the first quarter of 2023, due to the controversial\
          \ write-off of 15 billion Swiss francs of AT1 bonds by the Swiss regulator\
          \ as part of the deal. The adjusted pre-tax loss for the quarter came in\
          \ at 1.3 billion Swiss francs. Swiss authorities brokered the controversial\
          \ 3 billion Swiss franc rescue over the course of a weekend in late March,\
          \ following a collapse in Credit Suisse's deposits and share price amid\
          \ fears of a global banking crisis triggered by the fall of U.S. lender\
          \ Silicon Valley Bank. In Monday's earnings report, which could be the last\
          \ in its 167-year history, Credit Suisse said it experienced significant\
          \ net asset outflows, particularly in the second half of March 2023, which\
          \ have \"moderated but have not yet reversed as of April 24, 2023.\" First-quarter\
          \ net outflows totaled 61.2 billion, 5% of the group's assets under management\
          \ as of the end of 2022. Deposit outflows represented 57% of the net asset\
          \ outflows from Credit Suisse's wealth management unit and Swiss bank for\
          \ the quarter.\n    ### Assistant:\\\n    \"\"\"\n    text = input(f\"Enter\
          \ input text (default prompt will be used if left empty): \")\n    if not\
          \ text:\n        text = prompt\n</code></pre>\n<p>Output:</p>\n<pre><code>Swiss\
          \ bank Credit Suisse on Monday revealed that it had suffered net asset outflows\
          \ of more than $68 billion during the first three months of 2023.\n\nThe\
          \ controversial write-off of 15 billion Swiss francs of AT1 bonds by the\
          \ Swiss regulator as part of the deal was the primary reason for the bank's\
          \ collapse, which has left it with a net asset outflow of more than $68\
          \ billion.\n\nCredit Suisse's share price has been on a rollercoaster since\
          \ the start of the year, falling from more than $50 to below $20 in just\
          \ a few months.\n\nThe bank's wealth management unit, which manages the\
          \ assets of more than 1 million clients, posted a one-off 12.43 billion\
          \ Swiss franc profit for the first quarter of 2023, due to the controversial\
          \ write-off of 15 billion Swiss francs of AT1 bonds by the Swiss regulator\
          \ as part of the deal.\n\nCredit Suisse on Monday revealed that it had experienced\
          \ significant net asset outflows, particularly in the second half of March\
          \ 2023, which totalled 61.2 billion Swiss francs, or 5% of the group's assets\
          \ under management as of the end of 2022.\n\nThe bank's collapse has left\
          \ it with a net asset outflow of more than $68 billion, which it has been\
          \ trying to manage since the start of the year.\n\nCredit Suisse's share\
          \ price has been on a rollercoaster since the start of the year, falling\
          \ from more than $50 to below $20 in just a few months.\n\nThe bank's wealth\
          \ management unit, which manages the assets of more than 1 million clients,\
          \ posted a one-off 12.43 billion Swiss franc profit for the first quarter\
          \ of 2023, due to the controversial write-off of 15 billion Swiss francs\
          \ of AT1 bonds by the Swiss regulator as part of the deal.\n\nCredit Suisse\
          \ on Monday revealed that it had experienced significant net asset outflows,\
          \ particularly in the second half of March 2023, which totalled 61.2 billion\
          \ Swiss francs, or 5% of the group's assets under management as of the end\
          \ of 2022.\n\nThe bank's share price has been on a rollercoaster since the\
          \ start of the year, falling from more than $50 to below $20 in just a few\
          \ months.\n\nCredit Suisse on Monday revealed that it had experienced significant\
          \ net asset outflows, particularly in the second half of March 2023, which\
          \ totalled 61.2 billion Swiss francs, or 5%\n</code></pre>\n"
        raw: "Yea... sorry for the noise. It indeed works. The issue is that I have\
          \ several test mules and this one was set to groupsize = -1 at invocation.\
          \ This model wants groupsize=128 otherwise it fails with the error I reported.\n\
          \nBut there is indeed something wrong with it or with the way we are interfacing\
          \ with it because the output is not useful. Do you think something happened\
          \ to it during merge? Even when I change the input section to this and just\
          \ press enter to default to the preset I get a bad result. It does not infer.\
          \ To me it looks like a reconstructed tokenizer output, rather than model\
          \ inference result, and like in your example, it also repeats. I got something\
          \ similar from the Alpacino13b 4bit.safetensors model - it just repeated\
          \ the input text as output. Do you know what may be causing this?\n```\n\
          prompt = \"\"\"\\\n    ### Human: Please provide a concise summary of the\
          \ following news article, capturing the key information and stating company\
          \ ticker symbols, and government entity abbreviations, whenever possible:\
          \ Credit Suisse logged asset outflows of more than $68 billion during first-quarter\
          \ collapse. Credit Suisse on Monday revealed that it suffered net asset\
          \ outflows of 61.2 billion Swiss francs ($68.6 billion) during the first-quarter\
          \ collapse that culminated in its emergency rescue by domestic rival UBS.\
          \ The stricken Swiss lender posted a one-off 12.43 billion Swiss franc profit\
          \ for the first quarter of 2023, due to the controversial write-off of 15\
          \ billion Swiss francs of AT1 bonds by the Swiss regulator as part of the\
          \ deal. The adjusted pre-tax loss for the quarter came in at 1.3 billion\
          \ Swiss francs. Swiss authorities brokered the controversial 3 billion Swiss\
          \ franc rescue over the course of a weekend in late March, following a collapse\
          \ in Credit Suisse's deposits and share price amid fears of a global banking\
          \ crisis triggered by the fall of U.S. lender Silicon Valley Bank. In Monday's\
          \ earnings report, which could be the last in its 167-year history, Credit\
          \ Suisse said it experienced significant net asset outflows, particularly\
          \ in the second half of March 2023, which have \"moderated but have not\
          \ yet reversed as of April 24, 2023.\" First-quarter net outflows totaled\
          \ 61.2 billion, 5% of the group's assets under management as of the end\
          \ of 2022. Deposit outflows represented 57% of the net asset outflows from\
          \ Credit Suisse's wealth management unit and Swiss bank for the quarter.\n\
          \    ### Assistant:\\\n    \"\"\"\n    text = input(f\"Enter input text\
          \ (default prompt will be used if left empty): \")\n    if not text:\n \
          \       text = prompt\n```\n\nOutput:\n\n```\nSwiss bank Credit Suisse on\
          \ Monday revealed that it had suffered net asset outflows of more than $68\
          \ billion during the first three months of 2023.\n\nThe controversial write-off\
          \ of 15 billion Swiss francs of AT1 bonds by the Swiss regulator as part\
          \ of the deal was the primary reason for the bank's collapse, which has\
          \ left it with a net asset outflow of more than $68 billion.\n\nCredit Suisse's\
          \ share price has been on a rollercoaster since the start of the year, falling\
          \ from more than $50 to below $20 in just a few months.\n\nThe bank's wealth\
          \ management unit, which manages the assets of more than 1 million clients,\
          \ posted a one-off 12.43 billion Swiss franc profit for the first quarter\
          \ of 2023, due to the controversial write-off of 15 billion Swiss francs\
          \ of AT1 bonds by the Swiss regulator as part of the deal.\n\nCredit Suisse\
          \ on Monday revealed that it had experienced significant net asset outflows,\
          \ particularly in the second half of March 2023, which totalled 61.2 billion\
          \ Swiss francs, or 5% of the group's assets under management as of the end\
          \ of 2022.\n\nThe bank's collapse has left it with a net asset outflow of\
          \ more than $68 billion, which it has been trying to manage since the start\
          \ of the year.\n\nCredit Suisse's share price has been on a rollercoaster\
          \ since the start of the year, falling from more than $50 to below $20 in\
          \ just a few months.\n\nThe bank's wealth management unit, which manages\
          \ the assets of more than 1 million clients, posted a one-off 12.43 billion\
          \ Swiss franc profit for the first quarter of 2023, due to the controversial\
          \ write-off of 15 billion Swiss francs of AT1 bonds by the Swiss regulator\
          \ as part of the deal.\n\nCredit Suisse on Monday revealed that it had experienced\
          \ significant net asset outflows, particularly in the second half of March\
          \ 2023, which totalled 61.2 billion Swiss francs, or 5% of the group's assets\
          \ under management as of the end of 2022.\n\nThe bank's share price has\
          \ been on a rollercoaster since the start of the year, falling from more\
          \ than $50 to below $20 in just a few months.\n\nCredit Suisse on Monday\
          \ revealed that it had experienced significant net asset outflows, particularly\
          \ in the second half of March 2023, which totalled 61.2 billion Swiss francs,\
          \ or 5%\n```"
        updatedAt: '2023-04-29T13:38:17.286Z'
      numEdits: 1
      reactions: []
    id: 644d1da50dc952d2459ccb20
    type: comment
  author: vmajor
  content: "Yea... sorry for the noise. It indeed works. The issue is that I have\
    \ several test mules and this one was set to groupsize = -1 at invocation. This\
    \ model wants groupsize=128 otherwise it fails with the error I reported.\n\n\
    But there is indeed something wrong with it or with the way we are interfacing\
    \ with it because the output is not useful. Do you think something happened to\
    \ it during merge? Even when I change the input section to this and just press\
    \ enter to default to the preset I get a bad result. It does not infer. To me\
    \ it looks like a reconstructed tokenizer output, rather than model inference\
    \ result, and like in your example, it also repeats. I got something similar from\
    \ the Alpacino13b 4bit.safetensors model - it just repeated the input text as\
    \ output. Do you know what may be causing this?\n```\nprompt = \"\"\"\\\n    ###\
    \ Human: Please provide a concise summary of the following news article, capturing\
    \ the key information and stating company ticker symbols, and government entity\
    \ abbreviations, whenever possible: Credit Suisse logged asset outflows of more\
    \ than $68 billion during first-quarter collapse. Credit Suisse on Monday revealed\
    \ that it suffered net asset outflows of 61.2 billion Swiss francs ($68.6 billion)\
    \ during the first-quarter collapse that culminated in its emergency rescue by\
    \ domestic rival UBS. The stricken Swiss lender posted a one-off 12.43 billion\
    \ Swiss franc profit for the first quarter of 2023, due to the controversial write-off\
    \ of 15 billion Swiss francs of AT1 bonds by the Swiss regulator as part of the\
    \ deal. The adjusted pre-tax loss for the quarter came in at 1.3 billion Swiss\
    \ francs. Swiss authorities brokered the controversial 3 billion Swiss franc rescue\
    \ over the course of a weekend in late March, following a collapse in Credit Suisse's\
    \ deposits and share price amid fears of a global banking crisis triggered by\
    \ the fall of U.S. lender Silicon Valley Bank. In Monday's earnings report, which\
    \ could be the last in its 167-year history, Credit Suisse said it experienced\
    \ significant net asset outflows, particularly in the second half of March 2023,\
    \ which have \"moderated but have not yet reversed as of April 24, 2023.\" First-quarter\
    \ net outflows totaled 61.2 billion, 5% of the group's assets under management\
    \ as of the end of 2022. Deposit outflows represented 57% of the net asset outflows\
    \ from Credit Suisse's wealth management unit and Swiss bank for the quarter.\n\
    \    ### Assistant:\\\n    \"\"\"\n    text = input(f\"Enter input text (default\
    \ prompt will be used if left empty): \")\n    if not text:\n        text = prompt\n\
    ```\n\nOutput:\n\n```\nSwiss bank Credit Suisse on Monday revealed that it had\
    \ suffered net asset outflows of more than $68 billion during the first three\
    \ months of 2023.\n\nThe controversial write-off of 15 billion Swiss francs of\
    \ AT1 bonds by the Swiss regulator as part of the deal was the primary reason\
    \ for the bank's collapse, which has left it with a net asset outflow of more\
    \ than $68 billion.\n\nCredit Suisse's share price has been on a rollercoaster\
    \ since the start of the year, falling from more than $50 to below $20 in just\
    \ a few months.\n\nThe bank's wealth management unit, which manages the assets\
    \ of more than 1 million clients, posted a one-off 12.43 billion Swiss franc profit\
    \ for the first quarter of 2023, due to the controversial write-off of 15 billion\
    \ Swiss francs of AT1 bonds by the Swiss regulator as part of the deal.\n\nCredit\
    \ Suisse on Monday revealed that it had experienced significant net asset outflows,\
    \ particularly in the second half of March 2023, which totalled 61.2 billion Swiss\
    \ francs, or 5% of the group's assets under management as of the end of 2022.\n\
    \nThe bank's collapse has left it with a net asset outflow of more than $68 billion,\
    \ which it has been trying to manage since the start of the year.\n\nCredit Suisse's\
    \ share price has been on a rollercoaster since the start of the year, falling\
    \ from more than $50 to below $20 in just a few months.\n\nThe bank's wealth management\
    \ unit, which manages the assets of more than 1 million clients, posted a one-off\
    \ 12.43 billion Swiss franc profit for the first quarter of 2023, due to the controversial\
    \ write-off of 15 billion Swiss francs of AT1 bonds by the Swiss regulator as\
    \ part of the deal.\n\nCredit Suisse on Monday revealed that it had experienced\
    \ significant net asset outflows, particularly in the second half of March 2023,\
    \ which totalled 61.2 billion Swiss francs, or 5% of the group's assets under\
    \ management as of the end of 2022.\n\nThe bank's share price has been on a rollercoaster\
    \ since the start of the year, falling from more than $50 to below $20 in just\
    \ a few months.\n\nCredit Suisse on Monday revealed that it had experienced significant\
    \ net asset outflows, particularly in the second half of March 2023, which totalled\
    \ 61.2 billion Swiss francs, or 5%\n```"
  created_at: 2023-04-29 12:37:41+00:00
  edited: true
  hidden: false
  id: 644d1da50dc952d2459ccb20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-29T16:54:07.000Z'
    data:
      edited: true
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<pre><code>Credit Suisse reported a massive $68 billion in asset outflows
          during Q1 2023, resulting in a one-time gain of $12.43 billion due to the
          write-off of AT1 bonds by the Swiss regulator. Despite this, the company
          still recorded a pre-tax loss of $1.3 billion for the quarter. Net outflows
          were primarily driven by withdrawals from the wealth management and Swiss
          bank units, with deposit outflows representing 57% of these losses. These
          outflows have moderated slightly since the beginning of April, but have
          not fully reversed yet.


          Company Ticker Symbols:


          Credit Suisse - CSGNY (New York Stock Exchange)


          UBS - UBSG (Swiss Exchange)


          Government Entity Abbreviations:


          Switzerland - CHE (ISO 3166-1 alpha-3 code)

          </code></pre>

          <p>I seem to get good results in text-generation-webui. Maybe it is only
          your parameters. The parameters preset used for the above output is labeled
          Llama-Precise:</p>

          <pre><code>temp: 0.7

          top_p: 0.1

          top_k: 40

          typical_p: 1

          repetition_penalty: 1.18

          encoder_reptition_penalty: 1

          no_repeat_ngram_size: 0

          min_length: 0

          </code></pre>

          '
        raw: '```

          Credit Suisse reported a massive $68 billion in asset outflows during Q1
          2023, resulting in a one-time gain of $12.43 billion due to the write-off
          of AT1 bonds by the Swiss regulator. Despite this, the company still recorded
          a pre-tax loss of $1.3 billion for the quarter. Net outflows were primarily
          driven by withdrawals from the wealth management and Swiss bank units, with
          deposit outflows representing 57% of these losses. These outflows have moderated
          slightly since the beginning of April, but have not fully reversed yet.


          Company Ticker Symbols:


          Credit Suisse - CSGNY (New York Stock Exchange)


          UBS - UBSG (Swiss Exchange)


          Government Entity Abbreviations:


          Switzerland - CHE (ISO 3166-1 alpha-3 code)

          ```


          I seem to get good results in text-generation-webui. Maybe it is only your
          parameters. The parameters preset used for the above output is labeled Llama-Precise:

          ```

          temp: 0.7

          top_p: 0.1

          top_k: 40

          typical_p: 1

          repetition_penalty: 1.18

          encoder_reptition_penalty: 1

          no_repeat_ngram_size: 0

          min_length: 0

          ```'
        updatedAt: '2023-04-29T17:52:01.405Z'
      numEdits: 2
      reactions: []
    id: 644d4baf0dc952d245a1c93a
    type: comment
  author: Squish42
  content: '```

    Credit Suisse reported a massive $68 billion in asset outflows during Q1 2023,
    resulting in a one-time gain of $12.43 billion due to the write-off of AT1 bonds
    by the Swiss regulator. Despite this, the company still recorded a pre-tax loss
    of $1.3 billion for the quarter. Net outflows were primarily driven by withdrawals
    from the wealth management and Swiss bank units, with deposit outflows representing
    57% of these losses. These outflows have moderated slightly since the beginning
    of April, but have not fully reversed yet.


    Company Ticker Symbols:


    Credit Suisse - CSGNY (New York Stock Exchange)


    UBS - UBSG (Swiss Exchange)


    Government Entity Abbreviations:


    Switzerland - CHE (ISO 3166-1 alpha-3 code)

    ```


    I seem to get good results in text-generation-webui. Maybe it is only your parameters.
    The parameters preset used for the above output is labeled Llama-Precise:

    ```

    temp: 0.7

    top_p: 0.1

    top_k: 40

    typical_p: 1

    repetition_penalty: 1.18

    encoder_reptition_penalty: 1

    no_repeat_ngram_size: 0

    min_length: 0

    ```'
  created_at: 2023-04-29 15:54:07+00:00
  edited: true
  hidden: false
  id: 644d4baf0dc952d245a1c93a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-30T03:48:35.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Interesting, and that is indeed a great result. I now need to look
          at their source to see how they are invoking the model and passing the parameters.
          I passed the same parameters to my implementation and got a nonsensical
          result. I wonder if they are using generate() from transformers or something
          else.</p>

          '
        raw: Interesting, and that is indeed a great result. I now need to look at
          their source to see how they are invoking the model and passing the parameters.
          I passed the same parameters to my implementation and got a nonsensical
          result. I wonder if they are using generate() from transformers or something
          else.
        updatedAt: '2023-04-30T03:48:35.651Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AbdouS
    id: 644de513fa94e93b0ed46a62
    type: comment
  author: vmajor
  content: Interesting, and that is indeed a great result. I now need to look at their
    source to see how they are invoking the model and passing the parameters. I passed
    the same parameters to my implementation and got a nonsensical result. I wonder
    if they are using generate() from transformers or something else.
  created_at: 2023-04-30 02:48:35+00:00
  edited: false
  hidden: false
  id: 644de513fa94e93b0ed46a62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
      fullname: Squish
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Squish42
      type: user
    createdAt: '2023-04-30T04:15:23.000Z'
    data:
      edited: false
      editors:
      - Squish42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae79b716e530022eeb67b31bd335c3ac.svg
          fullname: Squish
          isHf: false
          isPro: false
          name: Squish42
          type: user
        html: '<p>I believe text-generation-webui includes "This is a conversation
          with your Assistant. The Assistant is very helpful and is eager to chat
          with you and answer your questions." as part of the context before the prompt.
          This might have some effect.<br>When prompted without that context, my responses
          weren''t nearly as coherent.</p>

          '
        raw: 'I believe text-generation-webui includes "This is a conversation with
          your Assistant. The Assistant is very helpful and is eager to chat with
          you and answer your questions." as part of the context before the prompt.
          This might have some effect.

          When prompted without that context, my responses weren''t nearly as coherent.'
        updatedAt: '2023-04-30T04:15:23.978Z'
      numEdits: 0
      reactions: []
    id: 644deb5b97a3b0904a5fef6d
    type: comment
  author: Squish42
  content: 'I believe text-generation-webui includes "This is a conversation with
    your Assistant. The Assistant is very helpful and is eager to chat with you and
    answer your questions." as part of the context before the prompt. This might have
    some effect.

    When prompted without that context, my responses weren''t nearly as coherent.'
  created_at: 2023-04-30 03:15:23+00:00
  edited: false
  hidden: false
  id: 644deb5b97a3b0904a5fef6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-30T04:44:20.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>I just tried it with several different input templates. The "Alpaca"
          template does not produce any output, "Alpaca-with-input" generates an output
          but it is low quality, not the amazing one that you got. Could you tell
          me exactly what prompt you used, in its entirety? Also, the exact model.
          I am finding this process of deploying self hosted LLMs like the proverbial
          cat herding.</p>

          '
        raw: I just tried it with several different input templates. The "Alpaca"
          template does not produce any output, "Alpaca-with-input" generates an output
          but it is low quality, not the amazing one that you got. Could you tell
          me exactly what prompt you used, in its entirety? Also, the exact model.
          I am finding this process of deploying self hosted LLMs like the proverbial
          cat herding.
        updatedAt: '2023-04-30T04:44:20.163Z'
      numEdits: 0
      reactions: []
    id: 644df22497a3b0904a6066f0
    type: comment
  author: vmajor
  content: I just tried it with several different input templates. The "Alpaca" template
    does not produce any output, "Alpaca-with-input" generates an output but it is
    low quality, not the amazing one that you got. Could you tell me exactly what
    prompt you used, in its entirety? Also, the exact model. I am finding this process
    of deploying self hosted LLMs like the proverbial cat herding.
  created_at: 2023-04-30 03:44:20+00:00
  edited: false
  hidden: false
  id: 644df22497a3b0904a6066f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-30T07:57:48.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Ok I got it to work, thank you for your hint regarding webui. I
          looked at their code and there is nothing magical that they are doing to
          the input, or the output besides making it better presented for the webui
          gradio output. The key was indeed the prompt structure and webui makes it
          easy to test which prompt format works best for which model. stable-vicuna
          is doing really well for my needs with a fixed seed that I found:</p>

          <h3 id="response">Response:</h3>

          <h3 id="assistant-companies-mentioned-credit-suisse-cs-ubs-ubsgy-government-entities-mentioned-swiss-regulators-summary-credit-suisse-reported-a-one-time-gain-of-chf-1243-billion-due-to-the-write-off-of-chf-15-billion-worth-of-at1-bonds-by-the-swiss-regulators-as-part-of-the-rescue-package-net-asset-outflows-amounted-to-chf-612-billion-primarily-driven-by-withdrawals-from-the-wealth-management-and-swiss-bank-units-despite-efforts-to-moderate-the-outflows-they-have-not-fully-reversed-as-of-april-24th-2023">Assistant:
          Companies Mentioned: Credit Suisse (CS), UBS (UBSGY). Government Entities
          Mentioned: Swiss Regulators. Summary: Credit Suisse reported a one-time
          gain of CHF 12.43 billion due to the write-off of CHF 15 billion worth of
          AT1 bonds by the Swiss regulators as part of the rescue package. Net asset
          outflows amounted to CHF 61.2 billion, primarily driven by withdrawals from
          the wealth management and Swiss bank units. Despite efforts to moderate
          the outflows, they have not fully reversed as of April 24th, 2023.</h3>

          '
        raw: 'Ok I got it to work, thank you for your hint regarding webui. I looked
          at their code and there is nothing magical that they are doing to the input,
          or the output besides making it better presented for the webui gradio output.
          The key was indeed the prompt structure and webui makes it easy to test
          which prompt format works best for which model. stable-vicuna is doing really
          well for my needs with a fixed seed that I found:



          ### Response:

          ### Assistant: Companies Mentioned: Credit Suisse (CS), UBS (UBSGY). Government
          Entities Mentioned: Swiss Regulators. Summary: Credit Suisse reported a
          one-time gain of CHF 12.43 billion due to the write-off of CHF 15 billion
          worth of AT1 bonds by the Swiss regulators as part of the rescue package.
          Net asset outflows amounted to CHF 61.2 billion, primarily driven by withdrawals
          from the wealth management and Swiss bank units. Despite efforts to moderate
          the outflows, they have not fully reversed as of April 24th, 2023.'
        updatedAt: '2023-04-30T07:58:04.477Z'
      numEdits: 1
      reactions: []
    id: 644e1f7ca00f4b11d38abb67
    type: comment
  author: vmajor
  content: 'Ok I got it to work, thank you for your hint regarding webui. I looked
    at their code and there is nothing magical that they are doing to the input, or
    the output besides making it better presented for the webui gradio output. The
    key was indeed the prompt structure and webui makes it easy to test which prompt
    format works best for which model. stable-vicuna is doing really well for my needs
    with a fixed seed that I found:



    ### Response:

    ### Assistant: Companies Mentioned: Credit Suisse (CS), UBS (UBSGY). Government
    Entities Mentioned: Swiss Regulators. Summary: Credit Suisse reported a one-time
    gain of CHF 12.43 billion due to the write-off of CHF 15 billion worth of AT1
    bonds by the Swiss regulators as part of the rescue package. Net asset outflows
    amounted to CHF 61.2 billion, primarily driven by withdrawals from the wealth
    management and Swiss bank units. Despite efforts to moderate the outflows, they
    have not fully reversed as of April 24th, 2023.'
  created_at: 2023-04-30 06:57:48+00:00
  edited: true
  hidden: false
  id: 644e1f7ca00f4b11d38abb67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-04-30T16:08:39.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p> as there is no delete. Still looking into this.</p>

          '
        raw: <edit> as there is no delete. Still looking into this.
        updatedAt: '2023-04-30T16:11:14.967Z'
      numEdits: 1
      reactions: []
    id: 644e9287a00f4b11d3953947
    type: comment
  author: vmajor
  content: <edit> as there is no delete. Still looking into this.
  created_at: 2023-04-30 15:08:39+00:00
  edited: true
  hidden: false
  id: 644e9287a00f4b11d3953947
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-02T12:56:57.000Z'
    data:
      edited: false
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: '<p>Hello vmajor</p>

          <p>I am trying to do the exact same thing as you but reading and looping
          through my email. I also used <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ">https://github.com/PanQiWei/AutoGPTQ</a>
          in order to load the quantized model.</p>

          <p>Two question please: </p>

          <ul>

          <li>How were you able to pass the  --instruct argument and Vicuna-V0 instruction
          to the model through python?</li>

          <li>How much Vram does your model consume after loading and during inference?</li>

          </ul>

          <p>Many thanks.</p>

          '
        raw: "Hello vmajor\n\nI am trying to do the exact same thing as you but reading\
          \ and looping through my email. I also used https://github.com/PanQiWei/AutoGPTQ\
          \ in order to load the quantized model.\n\nTwo question please: \n\n - How\
          \ were you able to pass the  --instruct argument and Vicuna-V0 instruction\
          \ to the model through python?\n- How much Vram does your model consume\
          \ after loading and during inference?\n\nMany thanks."
        updatedAt: '2023-05-02T12:56:57.457Z'
      numEdits: 0
      reactions: []
    id: 6451089941f3c769b9095e78
    type: comment
  author: AbdouS
  content: "Hello vmajor\n\nI am trying to do the exact same thing as you but reading\
    \ and looping through my email. I also used https://github.com/PanQiWei/AutoGPTQ\
    \ in order to load the quantized model.\n\nTwo question please: \n\n - How were\
    \ you able to pass the  --instruct argument and Vicuna-V0 instruction to the model\
    \ through python?\n- How much Vram does your model consume after loading and during\
    \ inference?\n\nMany thanks."
  created_at: 2023-05-02 11:56:57+00:00
  edited: false
  hidden: false
  id: 6451089941f3c769b9095e78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-05-02T15:14:27.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>There are two entire programs that I shared above, take a look.
          They both run, but the problem is that the results are unstable (literally
          unstable with stable-vicuna because the model quits seemingly randomly)
          because the inference may work once or three times, but fail intermittently
          when you least expect it. As I suggested in the other thread, work with
          other models, not 13b GPTQ. If you keep trying with 13b GPTQ and get them
          to work reliably I''d love to hear how you did it. I could not even get
          them to work reliably with webui, so it was not my code that was causing
          issues.</p>

          <p>EDIT: regarding VRAM, 13b GPTQ models occupy a bit over 10GB of VRAM.<br>EDIT
          2: Sorry, I now see that this is a different thread. Try The Bloke''s version
          of the code. That one has a better prompt than my original code and has
          additional print statement</p>

          '
        raw: 'There are two entire programs that I shared above, take a look. They
          both run, but the problem is that the results are unstable (literally unstable
          with stable-vicuna because the model quits seemingly randomly) because the
          inference may work once or three times, but fail intermittently when you
          least expect it. As I suggested in the other thread, work with other models,
          not 13b GPTQ. If you keep trying with 13b GPTQ and get them to work reliably
          I''d love to hear how you did it. I could not even get them to work reliably
          with webui, so it was not my code that was causing issues.


          EDIT: regarding VRAM, 13b GPTQ models occupy a bit over 10GB of VRAM.

          EDIT 2: Sorry, I now see that this is a different thread. Try The Bloke''s
          version of the code. That one has a better prompt than my original code
          and has additional print statement'
        updatedAt: '2023-05-02T15:24:18.751Z'
      numEdits: 2
      reactions: []
    id: 645128d35fb40b9f50aacb34
    type: comment
  author: vmajor
  content: 'There are two entire programs that I shared above, take a look. They both
    run, but the problem is that the results are unstable (literally unstable with
    stable-vicuna because the model quits seemingly randomly) because the inference
    may work once or three times, but fail intermittently when you least expect it.
    As I suggested in the other thread, work with other models, not 13b GPTQ. If you
    keep trying with 13b GPTQ and get them to work reliably I''d love to hear how
    you did it. I could not even get them to work reliably with webui, so it was not
    my code that was causing issues.


    EDIT: regarding VRAM, 13b GPTQ models occupy a bit over 10GB of VRAM.

    EDIT 2: Sorry, I now see that this is a different thread. Try The Bloke''s version
    of the code. That one has a better prompt than my original code and has additional
    print statement'
  created_at: 2023-05-02 14:14:27+00:00
  edited: true
  hidden: false
  id: 645128d35fb40b9f50aacb34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-02T17:13:21.000Z'
    data:
      edited: false
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vmajor&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vmajor\">@<span class=\"\
          underline\">vmajor</span></a></span>\n\n\t</span></span> About VRAM indeed\
          \ it uses 10Gb when loaded but whenever I send a request for a prompt or\
          \ chunks of a prompt the memory usage skyrockets. I don't know if you had\
          \ this kind of behavior before.</p>\n<p>Did you try the --instruct argument\
          \ and Vicuna-V0 parameter in the webui?</p>\n"
        raw: '@vmajor About VRAM indeed it uses 10Gb when loaded but whenever I send
          a request for a prompt or chunks of a prompt the memory usage skyrockets.
          I don''t know if you had this kind of behavior before.


          Did you try the --instruct argument and Vicuna-V0 parameter in the webui?'
        updatedAt: '2023-05-02T17:13:21.772Z'
      numEdits: 0
      reactions: []
    id: 645144b141f3c769b910a2c9
    type: comment
  author: AbdouS
  content: '@vmajor About VRAM indeed it uses 10Gb when loaded but whenever I send
    a request for a prompt or chunks of a prompt the memory usage skyrockets. I don''t
    know if you had this kind of behavior before.


    Did you try the --instruct argument and Vicuna-V0 parameter in the webui?'
  created_at: 2023-05-02 16:13:21+00:00
  edited: false
  hidden: false
  id: 645144b141f3c769b910a2c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-05-02T17:26:25.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>The only time I saw the ram spike uncontrollably is when I use native
          HF models with transformers, and when using that beam search thing. I do
          not remember the exact wording because I do not have webui running right
          now. I do not recall trying the --instruct or Vicuna-V0 parameters in webui.</p>

          '
        raw: The only time I saw the ram spike uncontrollably is when I use native
          HF models with transformers, and when using that beam search thing. I do
          not remember the exact wording because I do not have webui running right
          now. I do not recall trying the --instruct or Vicuna-V0 parameters in webui.
        updatedAt: '2023-05-02T17:26:25.605Z'
      numEdits: 0
      reactions: []
    id: 645147c1b3f75261a7d867f8
    type: comment
  author: vmajor
  content: The only time I saw the ram spike uncontrollably is when I use native HF
    models with transformers, and when using that beam search thing. I do not remember
    the exact wording because I do not have webui running right now. I do not recall
    trying the --instruct or Vicuna-V0 parameters in webui.
  created_at: 2023-05-02 16:26:25+00:00
  edited: false
  hidden: false
  id: 645147c1b3f75261a7d867f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-02T19:19:19.000Z'
    data:
      edited: false
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: '<p>You have to try --instruct and Vicuna-V0 this changes everything
          when running it on webui. No more giberrish generation. Much more stable</p>

          '
        raw: You have to try --instruct and Vicuna-V0 this changes everything when
          running it on webui. No more giberrish generation. Much more stable
        updatedAt: '2023-05-02T19:19:19.298Z'
      numEdits: 0
      reactions: []
    id: 645162375fb40b9f50b0ed59
    type: comment
  author: AbdouS
  content: You have to try --instruct and Vicuna-V0 this changes everything when running
    it on webui. No more giberrish generation. Much more stable
  created_at: 2023-05-02 18:19:19+00:00
  edited: false
  hidden: false
  id: 645162375fb40b9f50b0ed59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-05-03T03:12:15.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>I cannot see these flags available here: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a></p>

          <p>Where did you set them?</p>

          '
        raw: 'I cannot see these flags available here: https://github.com/oobabooga/text-generation-webui


          Where did you set them?'
        updatedAt: '2023-05-03T03:12:15.817Z'
      numEdits: 0
      reactions: []
    id: 6451d10f41f3c769b920e67b
    type: comment
  author: vmajor
  content: 'I cannot see these flags available here: https://github.com/oobabooga/text-generation-webui


    Where did you set them?'
  created_at: 2023-05-03 02:12:15+00:00
  edited: false
  hidden: false
  id: 6451d10f41f3c769b920e67b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-03T06:02:11.000Z'
    data:
      edited: false
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vmajor&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vmajor\">@<span class=\"\
          underline\">vmajor</span></a></span>\n\n\t</span></span> check minute 5\
          \ of this video <a rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=QeBmeHg8s5Y&amp;t=301s\"\
          >https://www.youtube.com/watch?v=QeBmeHg8s5Y&amp;t=301s</a></p>\n"
        raw: '@vmajor check minute 5 of this video https://www.youtube.com/watch?v=QeBmeHg8s5Y&t=301s'
        updatedAt: '2023-05-03T06:02:11.544Z'
      numEdits: 0
      reactions: []
    id: 6451f8e3efe8578bb343925a
    type: comment
  author: AbdouS
  content: '@vmajor check minute 5 of this video https://www.youtube.com/watch?v=QeBmeHg8s5Y&t=301s'
  created_at: 2023-05-03 05:02:11+00:00
  edited: false
  hidden: false
  id: 6451f8e3efe8578bb343925a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-04T14:56:16.000Z'
    data:
      edited: false
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;vmajor&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vmajor\"\
          >@<span class=\"underline\">vmajor</span></a></span>\n\n\t</span></span>\
          \ would you mind sharing the code your using with the 65B model? thanks</p>\n"
        raw: Hello @vmajor would you mind sharing the code your using with the 65B
          model? thanks
        updatedAt: '2023-05-04T14:56:16.722Z'
      numEdits: 0
      reactions: []
    id: 6453c79068cbb276cb4f8ad7
    type: comment
  author: AbdouS
  content: Hello @vmajor would you mind sharing the code your using with the 65B model?
    thanks
  created_at: 2023-05-04 13:56:16+00:00
  edited: false
  hidden: false
  id: 6453c79068cbb276cb4f8ad7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-05-04T15:07:12.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: "<p>OK sure, you can use this as a basic example for instruction based\
          \ inference. </p>\n<pre><code>import argparse\nfrom llama_cpp import Llama\n\
          \nparser = argparse.ArgumentParser()\nparser.add_argument(\"-m\", \"--model\"\
          , type=str, default=\"/home/vmajor/models/alpaca-lora-65B-GGML/alpaca-lora-65B.ggml.q5_1.bin\"\
          )\nargs = parser.parse_args()\n\nllm = Llama(model_path=args.model, n_threads=12,\
          \ n_ctx=2048, seed=1000, n_batch=128, last_n_tokens_size=150)\n\ncontext\
          \ = \"Below is an instruction that describes a task, paired with an input\
          \ that provides further context. Write a response that appropriately completes\
          \ the request.\\n\"\n\ninstruction = \"### Instruction: Please provide a\
          \ concise summary of the following news article, capturing the key information\
          \ and stating company ticker symbols, and government entity abbreviations,\
          \ whenever possible: \\n\"\n\ninput = \"\"\"### Input: Major Wall Street\
          \ firm sees a breakout in luxury stocks \u2014 and lists three reasons why\
          \ ETFs are a great way to play it. As luxury stocks make waves overseas,\
          \ State Street Global Advisors believes investors should consider European\
          \ ETFs if they want to capture the gains from their outperformance. Matt\
          \ Bartolini, the firm's head of SPDR Americas research, finds three reasons\
          \ why the backdrop is becoming particularly attractive. First and second\
          \ on his list: valuations and earnings upgrades. \"That's completely different\
          \ than what we saw for U.S. firms,\" he told CNBC's Bob Pisani on \"ETF\
          \ Edge\" this week. His remarks come as LVMH became the first European company\
          \ to surpass $500 billion in market value earlier this week. Bartolini lists\
          \ price momentum as a third driver of the investor shift. \\n\"\"\"\n\n\
          output = \"### Response: \\n\"\n\nprint(context + instruction + input +\
          \ output)\nsummary_output = llm(\n    context + instruction + input + output,\n\
          \    max_tokens=1024,\n    stop=None,\n    temperature=1.0,\n    repeat_penalty=1.1,\n\
          \    top_k=160,\n    top_p=0.5,\n    echo=True,\n)\n\nsummary_text = summary_output[\"\
          choices\"][0][\"text\"]\n# Split the output by the keyword Answer:\nparts\
          \ = summary_text.split(\"### Response:\")\n\n# Check if there are two parts\
          \ after splitting\nif len(parts) == 2:\n    # Get the second part which\
          \ contains the answer\n    answer = parts[1]\n\n    # Strip any leading\
          \ or trailing whitespace from the answer\n    answer = answer.strip()\n\n\
          \    # Save the answer to a file\n    with open(\"summary.txt\", \"w\")\
          \ as f:\n        f.write(answer)\nelse:\n    # Print an error message if\
          \ there is no answer in the output\n    print(\"No answer found in the output.\"\
          )\n</code></pre>\n"
        raw: "OK sure, you can use this as a basic example for instruction based inference.\
          \ \n```\nimport argparse\nfrom llama_cpp import Llama\n\nparser = argparse.ArgumentParser()\n\
          parser.add_argument(\"-m\", \"--model\", type=str, default=\"/home/vmajor/models/alpaca-lora-65B-GGML/alpaca-lora-65B.ggml.q5_1.bin\"\
          )\nargs = parser.parse_args()\n\nllm = Llama(model_path=args.model, n_threads=12,\
          \ n_ctx=2048, seed=1000, n_batch=128, last_n_tokens_size=150)\n\ncontext\
          \ = \"Below is an instruction that describes a task, paired with an input\
          \ that provides further context. Write a response that appropriately completes\
          \ the request.\\n\"\n\ninstruction = \"### Instruction: Please provide a\
          \ concise summary of the following news article, capturing the key information\
          \ and stating company ticker symbols, and government entity abbreviations,\
          \ whenever possible: \\n\"\n\ninput = \"\"\"### Input: Major Wall Street\
          \ firm sees a breakout in luxury stocks \u2014 and lists three reasons why\
          \ ETFs are a great way to play it. As luxury stocks make waves overseas,\
          \ State Street Global Advisors believes investors should consider European\
          \ ETFs if they want to capture the gains from their outperformance. Matt\
          \ Bartolini, the firm's head of SPDR Americas research, finds three reasons\
          \ why the backdrop is becoming particularly attractive. First and second\
          \ on his list: valuations and earnings upgrades. \"That's completely different\
          \ than what we saw for U.S. firms,\" he told CNBC's Bob Pisani on \"ETF\
          \ Edge\" this week. His remarks come as LVMH became the first European company\
          \ to surpass $500 billion in market value earlier this week. Bartolini lists\
          \ price momentum as a third driver of the investor shift. \\n\"\"\"\n\n\
          output = \"### Response: \\n\"\n\nprint(context + instruction + input +\
          \ output)\nsummary_output = llm(\n    context + instruction + input + output,\n\
          \    max_tokens=1024,\n    stop=None,\n    temperature=1.0,\n    repeat_penalty=1.1,\n\
          \    top_k=160,\n    top_p=0.5,\n    echo=True,\n)\n\nsummary_text = summary_output[\"\
          choices\"][0][\"text\"]\n# Split the output by the keyword Answer:\nparts\
          \ = summary_text.split(\"### Response:\")\n\n# Check if there are two parts\
          \ after splitting\nif len(parts) == 2:\n    # Get the second part which\
          \ contains the answer\n    answer = parts[1]\n\n    # Strip any leading\
          \ or trailing whitespace from the answer\n    answer = answer.strip()\n\n\
          \    # Save the answer to a file\n    with open(\"summary.txt\", \"w\")\
          \ as f:\n        f.write(answer)\nelse:\n    # Print an error message if\
          \ there is no answer in the output\n    print(\"No answer found in the output.\"\
          )\n```"
        updatedAt: '2023-05-04T15:07:12.879Z'
      numEdits: 0
      reactions: []
    id: 6453ca20dd49b82d7afa6d8c
    type: comment
  author: vmajor
  content: "OK sure, you can use this as a basic example for instruction based inference.\
    \ \n```\nimport argparse\nfrom llama_cpp import Llama\n\nparser = argparse.ArgumentParser()\n\
    parser.add_argument(\"-m\", \"--model\", type=str, default=\"/home/vmajor/models/alpaca-lora-65B-GGML/alpaca-lora-65B.ggml.q5_1.bin\"\
    )\nargs = parser.parse_args()\n\nllm = Llama(model_path=args.model, n_threads=12,\
    \ n_ctx=2048, seed=1000, n_batch=128, last_n_tokens_size=150)\n\ncontext = \"\
    Below is an instruction that describes a task, paired with an input that provides\
    \ further context. Write a response that appropriately completes the request.\\\
    n\"\n\ninstruction = \"### Instruction: Please provide a concise summary of the\
    \ following news article, capturing the key information and stating company ticker\
    \ symbols, and government entity abbreviations, whenever possible: \\n\"\n\ninput\
    \ = \"\"\"### Input: Major Wall Street firm sees a breakout in luxury stocks \u2014\
    \ and lists three reasons why ETFs are a great way to play it. As luxury stocks\
    \ make waves overseas, State Street Global Advisors believes investors should\
    \ consider European ETFs if they want to capture the gains from their outperformance.\
    \ Matt Bartolini, the firm's head of SPDR Americas research, finds three reasons\
    \ why the backdrop is becoming particularly attractive. First and second on his\
    \ list: valuations and earnings upgrades. \"That's completely different than what\
    \ we saw for U.S. firms,\" he told CNBC's Bob Pisani on \"ETF Edge\" this week.\
    \ His remarks come as LVMH became the first European company to surpass $500 billion\
    \ in market value earlier this week. Bartolini lists price momentum as a third\
    \ driver of the investor shift. \\n\"\"\"\n\noutput = \"### Response: \\n\"\n\n\
    print(context + instruction + input + output)\nsummary_output = llm(\n    context\
    \ + instruction + input + output,\n    max_tokens=1024,\n    stop=None,\n    temperature=1.0,\n\
    \    repeat_penalty=1.1,\n    top_k=160,\n    top_p=0.5,\n    echo=True,\n)\n\n\
    summary_text = summary_output[\"choices\"][0][\"text\"]\n# Split the output by\
    \ the keyword Answer:\nparts = summary_text.split(\"### Response:\")\n\n# Check\
    \ if there are two parts after splitting\nif len(parts) == 2:\n    # Get the second\
    \ part which contains the answer\n    answer = parts[1]\n\n    # Strip any leading\
    \ or trailing whitespace from the answer\n    answer = answer.strip()\n\n    #\
    \ Save the answer to a file\n    with open(\"summary.txt\", \"w\") as f:\n   \
    \     f.write(answer)\nelse:\n    # Print an error message if there is no answer\
    \ in the output\n    print(\"No answer found in the output.\")\n```"
  created_at: 2023-05-04 14:07:12+00:00
  edited: false
  hidden: false
  id: 6453ca20dd49b82d7afa6d8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-04T16:20:31.000Z'
    data:
      edited: true
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: "<p>Thank I will try it with the 30B model asap. If you are curious\
          \ this is the code that I was trying to implement a cleaning and summary\
          \ for financial emails. Not working of course and I don't know how to leverage\
          \ my 4090 with any other model.</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\"\
          >as</span> pd\n<span class=\"hljs-keyword\">from</span> msal <span class=\"\
          hljs-keyword\">import</span> PublicClientApplication\n<span class=\"hljs-keyword\"\
          >from</span> bs4 <span class=\"hljs-keyword\">import</span> BeautifulSoup\n\
          <span class=\"hljs-keyword\">import</span> re\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ pipeline, logging\n<span class=\"hljs-keyword\">from</span> auto_gptq\
          \ <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\
          <span class=\"hljs-keyword\">import</span> torch\n\n<span class=\"hljs-comment\"\
          >#Loading summurizer</span>\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"Loading summarizer...\"</span>)\nsummarizer =\
          \ pipeline(<span class=\"hljs-string\">\"summarization\"</span>)\n\n<span\
          \ class=\"hljs-comment\">#Emplacement du mod\xE8le sur le disque dur</span>\n\
          quantized_model_dir = <span class=\"hljs-string\">\"D:\\Clone\\TheBloke\
          \ stable vicuna 13B GPTQ\"</span>\n\n\n<span class=\"hljs-comment\">#Mise\
          \ en place du tokenizer pour envoyer les requetes au modele</span>\ntokenizer\
          \ = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\n<span class=\"hljs-comment\"># Global variable\
          \ to store the loaded model</span>\nloaded_model = <span class=\"hljs-literal\"\
          >None</span>\n\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\
          \ function_\">load_gpt_model</span>():\n    <span class=\"hljs-keyword\"\
          >global</span> loaded_model\n    <span class=\"hljs-keyword\">if</span>\
          \ loaded_model <span class=\"hljs-keyword\">is</span> <span class=\"hljs-literal\"\
          >None</span>:\n        loaded_model = get_model(<span class=\"hljs-string\"\
          >\"stable-vicuna-13B-GPTQ-4bit\"</span>, triton=<span class=\"hljs-literal\"\
          >False</span>, model_has_desc_act=<span class=\"hljs-literal\">False</span>)\n\
          \    <span class=\"hljs-keyword\">return</span> loaded_model\n\n<span class=\"\
          hljs-comment\">#configuration du mod\xE8le GPTQ</span>\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">get_config</span>(<span\
          \ class=\"hljs-params\">has_desc_act</span>):\n    )\n<span class=\"hljs-comment\"\
          >#fonction pour r\xE9cup\xE8rer un certain type de mod\xE8le selon la fin\
          \ du fichier</span>\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">get_model</span>(<span class=\"hljs-params\">model_base,\
          \ triton, model_has_desc_act</span>):\n    <span class=\"hljs-keyword\"\
          >if</span> model_has_desc_act:\n        model_suffix=<span class=\"hljs-string\"\
          >\"latest.act-order\"</span>\n    <span class=\"hljs-keyword\">else</span>:\n\
          \        model_suffix=<span class=\"hljs-string\">\"compat.no-act-order\"\
          </span>\n    <span class=\"hljs-keyword\">return</span> AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ use_safetensors=<span class=\"hljs-literal\">True</span>, model_basename=<span\
          \ class=\"hljs-string\">f\"<span class=\"hljs-subst\">{model_base}</span>.<span\
          \ class=\"hljs-subst\">{model_suffix}</span>\"</span>, device=<span class=\"\
          hljs-string\">\"cuda:0\"</span>, use_triton=triton, quantize_config=get_config(model_has_desc_act))\n\
          \n<span class=\"hljs-comment\"># Prevent printing spurious transformers\
          \ error</span>\nlogging.set_verbosity(logging.CRITICAL)\n<span class=\"\
          hljs-comment\">#enclenchement de la function pour r\xE9cup\xE9rer le mod\xE8\
          le</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"Loading GPTQ model...\"</span>)\nmodel = load_gpt_model()\n<span class=\"\
          hljs-comment\">#Sensibilit\xE9 du mod\xE8le</span>\npipe = pipeline(\n \
          \   <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    max_length=<span class=\"hljs-number\">512</span>,\n\
          \    temperature=<span class=\"hljs-number\">0.7</span>,\n    top_p=<span\
          \ class=\"hljs-number\">0.95</span>,\n    repetition_penalty=<span class=\"\
          hljs-number\">1.15</span>\n)\n\n\n<span class=\"hljs-comment\"># Functions\
          \ to clean text and extract paragraphs</span>\n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">extract_text</span>(<span\
          \ class=\"hljs-params\">html_content</span>):\n    soup = BeautifulSoup(html_content,\
          \ <span class=\"hljs-string\">\"html.parser\"</span>)\n    text_parts =\
          \ []\n\n    <span class=\"hljs-keyword\">for</span> tag <span class=\"hljs-keyword\"\
          >in</span> soup.find_all([<span class=\"hljs-string\">'p'</span>, <span\
          \ class=\"hljs-string\">'h1'</span>, <span class=\"hljs-string\">'h2'</span>,\
          \ <span class=\"hljs-string\">'h3'</span>, <span class=\"hljs-string\">'h4'</span>,\
          \ <span class=\"hljs-string\">'h5'</span>, <span class=\"hljs-string\">'h6'</span>,\
          \ <span class=\"hljs-string\">'li'</span>]):\n        text_parts.append(tag.get_text(separator=<span\
          \ class=\"hljs-string\">' '</span>))\n\n    <span class=\"hljs-keyword\"\
          >return</span> <span class=\"hljs-string\">' '</span>.join(text_parts).strip()\n\
          \n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >clean_email_text</span>(<span class=\"hljs-params\">text</span>):\n   \
          \ <span class=\"hljs-comment\"># Remove email addresses</span>\n    text\
          \ = re.sub(<span class=\"hljs-string\">r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\
          .[A-Z|a-z]{2,}\\b'</span>, <span class=\"hljs-string\">''</span>, text)\n\
          \n    <span class=\"hljs-comment\"># Remove phone numbers</span>\n    text\
          \ = re.sub(<span class=\"hljs-string\">r'\\+?\\d[\\d -]{7,}\\d'</span>,\
          \ <span class=\"hljs-string\">''</span>, text)\n\n    <span class=\"hljs-comment\"\
          ># Remove URLs</span>\n    text = re.sub(<span class=\"hljs-string\">r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\\
          \\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'</span>, <span class=\"hljs-string\"\
          >''</span>, text)\n\n    <span class=\"hljs-comment\"># Remove \"unsubscribe\"\
          \ and \"disclaimer\" sections</span>\n    text = re.sub(<span class=\"hljs-string\"\
          >r'((Un)?subscribe|Disclaimer)[\\s\\S]*'</span>, <span class=\"hljs-string\"\
          >''</span>, text, flags=re.IGNORECASE)\n\n    <span class=\"hljs-comment\"\
          ># Remove excessive whitespace</span>\n    text = re.sub(<span class=\"\
          hljs-string\">r'\\s+'</span>, <span class=\"hljs-string\">' '</span>, text).strip()\n\
          \n    <span class=\"hljs-keyword\">return</span> text\n\n\n<span class=\"\
          hljs-comment\">##################################################################################################</span>\n\
          <span class=\"hljs-comment\">##################################################################################################</span>\n\
          <span class=\"hljs-comment\">##########################Clean with VICUNA\
          \ 13B ##################################################</span>\n<span class=\"\
          hljs-comment\">##################################################################################################</span>\n\
          <span class=\"hljs-comment\">##################################################################################################</span>\n\
          <span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >clean_text_with_gptq_model</span>(<span class=\"hljs-params\">cleaned_relevant_text,\
          \ max_tokens=<span class=\"hljs-number\">2048</span></span>):\n    <span\
          \ class=\"hljs-comment\"># Adjust the number to reserve tokens for the prompt</span>\n\
          \    reserved_tokens = <span class=\"hljs-number\">1000</span>\n    max_chunk_tokens\
          \ = max_tokens - reserved_tokens\n\n    <span class=\"hljs-comment\"># Tokenize\
          \ the text using the model's tokenizer</span>\n    tokens = tokenizer.encode_plus(\n\
          \        cleaned_relevant_text,\n        max_length=max_chunk_tokens,\n\
          \        return_overflowing_tokens=<span class=\"hljs-literal\">True</span>,\n\
          \        truncation=<span class=\"hljs-literal\">True</span>,\n        padding=<span\
          \ class=\"hljs-string\">'max_length'</span>,\n        stride=<span class=\"\
          hljs-number\">0</span>\n    )\n\n    chunks = [tokens[<span class=\"hljs-string\"\
          >'input_ids'</span>]]\n    <span class=\"hljs-keyword\">if</span> <span\
          \ class=\"hljs-string\">'overflowing_tokens'</span> <span class=\"hljs-keyword\"\
          >in</span> tokens:\n        <span class=\"hljs-comment\"># Convert overflowing\
          \ tokens to lists of integers</span>\n        chunks.extend(tokens[<span\
          \ class=\"hljs-string\">'overflowing_tokens'</span>])\n\n    cleaned_text_chunks\
          \ = []\n    <span class=\"hljs-keyword\">for</span> chunk <span class=\"\
          hljs-keyword\">in</span> chunks:\n        <span class=\"hljs-comment\">#\
          \ Flatten the list of lists into a single list of integers</span>\n    \
          \    flat_chunk = [item <span class=\"hljs-keyword\">for</span> sublist\
          \ <span class=\"hljs-keyword\">in</span> chunk <span class=\"hljs-keyword\"\
          >for</span> item <span class=\"hljs-keyword\">in</span> sublist]\n     \
          \   decoded_chunk = tokenizer.decode(flat_chunk, skip_special_tokens=<span\
          \ class=\"hljs-literal\">True</span>)\n        prompt = <span class=\"hljs-string\"\
          >f\"Please remove disclaimers and any irrelevant information from the following\
          \ text:\\n\\n<span class=\"hljs-subst\">{decoded_chunk}</span>\\n\\nCleaned\
          \ text:\"</span>\n        \n        generated_text = pipe(prompt)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>]\n\
          \        response = generated_text.split(<span class=\"hljs-string\">\"\
          Cleaned text:\"</span>)[-<span class=\"hljs-number\">1</span>].strip()\n\
          \        \n        cleaned_text_chunks.append(response)\n\n        <span\
          \ class=\"hljs-comment\"># Free up GPU memory</span>\n        torch.cuda.empty_cache()\n\
          \n    <span class=\"hljs-keyword\">return</span> <span class=\"hljs-string\"\
          >\" \"</span>.join(cleaned_text_chunks)\n\n<span class=\"hljs-comment\"\
          >##################################################################################################</span>\n\
          <span class=\"hljs-comment\">###############Grabbing messages and looping\
          \ them cleaning and sending mail#######################</span>\n<span class=\"\
          hljs-comment\">##################################################################################################</span>\n\
          <span class=\"hljs-comment\">##################################################################################################</span>\n\
          \n<span class=\"hljs-keyword\">if</span> response.status_code == <span class=\"\
          hljs-number\">200</span>:\n    messages = response.json()[<span class=\"\
          hljs-string\">\"value\"</span>]\n    <span class=\"hljs-keyword\">for</span>\
          \ message <span class=\"hljs-keyword\">in</span> messages:\n        subject\
          \ = message[<span class=\"hljs-string\">\"subject\"</span>]\n        content\
          \ = message[<span class=\"hljs-string\">\"body\"</span>][<span class=\"\
          hljs-string\">\"content\"</span>]\n\n        <span class=\"hljs-comment\"\
          ># 1 Extract text from the email content, utilise beautifulsoup, fournit\
          \ le message qui est en html et lui le reprend en texte</span>\n       \
          \ text = extract_text(content)\n        stock_found = <span class=\"hljs-literal\"\
          >None</span>\n        relevant_text = <span class=\"hljs-literal\">None</span>\n\
          \n        <span class=\"hljs-comment\"># 2 Search for stock names in the\
          \ text if found it's then a stock found and text is relevant </span>\n \
          \       <span class=\"hljs-keyword\">for</span> stock_name <span class=\"\
          hljs-keyword\">in</span> stock_names:\n            <span class=\"hljs-keyword\"\
          >if</span> stock_name.lower() <span class=\"hljs-keyword\">in</span> text.lower():\n\
          \                stock_found = stock_name\n                relevant_text\
          \ = text\n                <span class=\"hljs-keyword\">break</span>\n\n\
          \        <span class=\"hljs-keyword\">if</span> stock_found:\n         \
          \   <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Found text mentioning <span class=\"hljs-subst\">{stock_found}</span>:\
          \ <span class=\"hljs-subst\">{relevant_text}</span>\"</span>)\n\n      \
          \      <span class=\"hljs-comment\"># 3 Clean the relevant text using BeautifulSoup\
          \ </span>\n            cleaned_relevant_text = clean_email_text(relevant_text)\n\
          \            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"Cleaned relevant text using BeautifulSoup.\"</span>)\n            <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Cleaned\
          \ text length: <span class=\"hljs-subst\">{<span class=\"hljs-built_in\"\
          >len</span>(cleaned_relevant_text)}</span>\"</span>)\n            <span\
          \ class=\"hljs-comment\"># 4 Send the cleaned relevant text for further\
          \ cleaning</span>\n            cleaned_relevant_text = clean_text_with_gptq_model(cleaned_relevant_text)\n\
          \            <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"Cleaned relevant text using GPTQ model.\"</span>)\n            \n   \
          \         <span class=\"hljs-keyword\">try</span>:\n                summarized_text\
          \ = summarizer(cleaned_relevant_text, max_length=<span class=\"hljs-number\"\
          >600</span>, min_length=<span class=\"hljs-number\">10</span>, do_sample=<span\
          \ class=\"hljs-literal\">False</span>)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">\"summary_text\"</span>]\n                summary\
          \ = summary.append({<span class=\"hljs-string\">\"Stock\"</span>: stock_found,\
          \ <span class=\"hljs-string\">\"Subject\"</span>: subject, <span class=\"\
          hljs-string\">\"Summary\"</span>: summarized_text}, ignore_index=<span class=\"\
          hljs-literal\">True</span>)\n                <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">f\"Summarized text for <span class=\"\
          hljs-subst\">{stock_found}</span>.\"</span>)\n            <span class=\"\
          hljs-keyword\">except</span> IndexError:\n                <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Error summarizing\
          \ the text mentioning <span class=\"hljs-subst\">{stock_found}</span>.\"\
          </span>)\n\n    <span class=\"hljs-comment\"># Send email with the summary</span>\n\
          \    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> summary.empty:\n        send_email(summary, access_token)\n\
          \    <span class=\"hljs-keyword\">else</span>:\n        <span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"No emails mentioning stocks\
          \ found\"</span>)\n\n<span class=\"hljs-keyword\">else</span>:\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Error\
          \ fetching messages: <span class=\"hljs-subst\">{response.status_code}</span>,\
          \ <span class=\"hljs-subst\">{response.text}</span>\"</span>)\n</code></pre>\n"
        raw: "Thank I will try it with the 30B model asap. If you are curious this\
          \ is the code that I was trying to implement a cleaning and summary for\
          \ financial emails. Not working of course and I don't know how to leverage\
          \ my 4090 with any other model.\n\n``` python\nimport pandas as pd\nfrom\
          \ msal import PublicClientApplication\nfrom bs4 import BeautifulSoup\nimport\
          \ re\nfrom transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport torch\n\n#Loading\
          \ summurizer\nprint(\"Loading summarizer...\")\nsummarizer = pipeline(\"\
          summarization\")\n\n#Emplacement du mod\xE8le sur le disque dur\nquantized_model_dir\
          \ = \"D:\\Clone\\TheBloke stable vicuna 13B GPTQ\"\n\n\n#Mise en place du\
          \ tokenizer pour envoyer les requetes au modele\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\n# Global variable to store the loaded model\nloaded_model\
          \ = None\n\ndef load_gpt_model():\n    global loaded_model\n    if loaded_model\
          \ is None:\n        loaded_model = get_model(\"stable-vicuna-13B-GPTQ-4bit\"\
          , triton=False, model_has_desc_act=False)\n    return loaded_model\n\n#configuration\
          \ du mod\xE8le GPTQ\ndef get_config(has_desc_act):\n    )\n#fonction pour\
          \ r\xE9cup\xE8rer un certain type de mod\xE8le selon la fin du fichier\n\
          def get_model(model_base, triton, model_has_desc_act):\n    if model_has_desc_act:\n\
          \        model_suffix=\"latest.act-order\"\n    else:\n        model_suffix=\"\
          compat.no-act-order\"\n    return AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\
          \ use_safetensors=True, model_basename=f\"{model_base}.{model_suffix}\"\
          , device=\"cuda:0\", use_triton=triton, quantize_config=get_config(model_has_desc_act))\n\
          \n# Prevent printing spurious transformers error\nlogging.set_verbosity(logging.CRITICAL)\n\
          #enclenchement de la function pour r\xE9cup\xE9rer le mod\xE8le\nprint(\"\
          Loading GPTQ model...\")\nmodel = load_gpt_model()\n#Sensibilit\xE9 du mod\xE8\
          le\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_length=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\n\n# Functions to clean text and extract paragraphs\n\ndef extract_text(html_content):\n\
          \    soup = BeautifulSoup(html_content, \"html.parser\")\n    text_parts\
          \ = []\n\n    for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5',\
          \ 'h6', 'li']):\n        text_parts.append(tag.get_text(separator=' '))\n\
          \n    return ' '.join(text_parts).strip()\n\ndef clean_email_text(text):\n\
          \    # Remove email addresses\n    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\
          .[A-Z|a-z]{2,}\\b', '', text)\n\n    # Remove phone numbers\n    text =\
          \ re.sub(r'\\+?\\d[\\d -]{7,}\\d', '', text)\n\n    # Remove URLs\n    text\
          \ = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\
          \ '', text)\n\n    # Remove \"unsubscribe\" and \"disclaimer\" sections\n\
          \    text = re.sub(r'((Un)?subscribe|Disclaimer)[\\s\\S]*', '', text, flags=re.IGNORECASE)\n\
          \n    # Remove excessive whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\
          \n    return text\n\n\n##################################################################################################\n\
          ##################################################################################################\n\
          ##########################Clean with VICUNA 13B ##################################################\n\
          ##################################################################################################\n\
          ##################################################################################################\n\
          def clean_text_with_gptq_model(cleaned_relevant_text, max_tokens=2048):\n\
          \    # Adjust the number to reserve tokens for the prompt\n    reserved_tokens\
          \ = 1000\n    max_chunk_tokens = max_tokens - reserved_tokens\n\n    # Tokenize\
          \ the text using the model's tokenizer\n    tokens = tokenizer.encode_plus(\n\
          \        cleaned_relevant_text,\n        max_length=max_chunk_tokens,\n\
          \        return_overflowing_tokens=True,\n        truncation=True,\n   \
          \     padding='max_length',\n        stride=0\n    )\n\n    chunks = [tokens['input_ids']]\n\
          \    if 'overflowing_tokens' in tokens:\n        # Convert overflowing tokens\
          \ to lists of integers\n        chunks.extend(tokens['overflowing_tokens'])\n\
          \n    cleaned_text_chunks = []\n    for chunk in chunks:\n        # Flatten\
          \ the list of lists into a single list of integers\n        flat_chunk =\
          \ [item for sublist in chunk for item in sublist]\n        decoded_chunk\
          \ = tokenizer.decode(flat_chunk, skip_special_tokens=True)\n        prompt\
          \ = f\"Please remove disclaimers and any irrelevant information from the\
          \ following text:\\n\\n{decoded_chunk}\\n\\nCleaned text:\"\n        \n\
          \        generated_text = pipe(prompt)[0]['generated_text']\n        response\
          \ = generated_text.split(\"Cleaned text:\")[-1].strip()\n        \n    \
          \    cleaned_text_chunks.append(response)\n\n        # Free up GPU memory\n\
          \        torch.cuda.empty_cache()\n\n    return \" \".join(cleaned_text_chunks)\n\
          \n##################################################################################################\n\
          ###############Grabbing messages and looping them cleaning and sending mail#######################\n\
          ##################################################################################################\n\
          ##################################################################################################\n\
          \nif response.status_code == 200:\n    messages = response.json()[\"value\"\
          ]\n    for message in messages:\n        subject = message[\"subject\"]\n\
          \        content = message[\"body\"][\"content\"]\n\n        # 1 Extract\
          \ text from the email content, utilise beautifulsoup, fournit le message\
          \ qui est en html et lui le reprend en texte\n        text = extract_text(content)\n\
          \        stock_found = None\n        relevant_text = None\n\n        # 2\
          \ Search for stock names in the text if found it's then a stock found and\
          \ text is relevant \n        for stock_name in stock_names:\n          \
          \  if stock_name.lower() in text.lower():\n                stock_found =\
          \ stock_name\n                relevant_text = text\n                break\n\
          \n        if stock_found:\n            print(f\"Found text mentioning {stock_found}:\
          \ {relevant_text}\")\n\n            # 3 Clean the relevant text using BeautifulSoup\
          \ \n            cleaned_relevant_text = clean_email_text(relevant_text)\n\
          \            print(\"Cleaned relevant text using BeautifulSoup.\")\n   \
          \         print(f\"Cleaned text length: {len(cleaned_relevant_text)}\")\n\
          \            # 4 Send the cleaned relevant text for further cleaning\n \
          \           cleaned_relevant_text = clean_text_with_gptq_model(cleaned_relevant_text)\n\
          \            print(\"Cleaned relevant text using GPTQ model.\")\n      \
          \      \n            try:\n                summarized_text = summarizer(cleaned_relevant_text,\
          \ max_length=600, min_length=10, do_sample=False)[0][\"summary_text\"]\n\
          \                summary = summary.append({\"Stock\": stock_found, \"Subject\"\
          : subject, \"Summary\": summarized_text}, ignore_index=True)\n         \
          \       print(f\"Summarized text for {stock_found}.\")\n            except\
          \ IndexError:\n                print(f\"Error summarizing the text mentioning\
          \ {stock_found}.\")\n\n    # Send email with the summary\n    if not summary.empty:\n\
          \        send_email(summary, access_token)\n    else:\n        print(\"\
          No emails mentioning stocks found\")\n\nelse:\n    print(f\"Error fetching\
          \ messages: {response.status_code}, {response.text}\")\n```"
        updatedAt: '2023-05-04T16:47:17.130Z'
      numEdits: 2
      reactions: []
    id: 6453db4f72d331dec89b7719
    type: comment
  author: AbdouS
  content: "Thank I will try it with the 30B model asap. If you are curious this is\
    \ the code that I was trying to implement a cleaning and summary for financial\
    \ emails. Not working of course and I don't know how to leverage my 4090 with\
    \ any other model.\n\n``` python\nimport pandas as pd\nfrom msal import PublicClientApplication\n\
    from bs4 import BeautifulSoup\nimport re\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n\
    import torch\n\n#Loading summurizer\nprint(\"Loading summarizer...\")\nsummarizer\
    \ = pipeline(\"summarization\")\n\n#Emplacement du mod\xE8le sur le disque dur\n\
    quantized_model_dir = \"D:\\Clone\\TheBloke stable vicuna 13B GPTQ\"\n\n\n#Mise\
    \ en place du tokenizer pour envoyer les requetes au modele\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n\n# Global variable to store the loaded model\nloaded_model\
    \ = None\n\ndef load_gpt_model():\n    global loaded_model\n    if loaded_model\
    \ is None:\n        loaded_model = get_model(\"stable-vicuna-13B-GPTQ-4bit\",\
    \ triton=False, model_has_desc_act=False)\n    return loaded_model\n\n#configuration\
    \ du mod\xE8le GPTQ\ndef get_config(has_desc_act):\n    )\n#fonction pour r\xE9\
    cup\xE8rer un certain type de mod\xE8le selon la fin du fichier\ndef get_model(model_base,\
    \ triton, model_has_desc_act):\n    if model_has_desc_act:\n        model_suffix=\"\
    latest.act-order\"\n    else:\n        model_suffix=\"compat.no-act-order\"\n\
    \    return AutoGPTQForCausalLM.from_quantized(quantized_model_dir, use_safetensors=True,\
    \ model_basename=f\"{model_base}.{model_suffix}\", device=\"cuda:0\", use_triton=triton,\
    \ quantize_config=get_config(model_has_desc_act))\n\n# Prevent printing spurious\
    \ transformers error\nlogging.set_verbosity(logging.CRITICAL)\n#enclenchement\
    \ de la function pour r\xE9cup\xE9rer le mod\xE8le\nprint(\"Loading GPTQ model...\"\
    )\nmodel = load_gpt_model()\n#Sensibilit\xE9 du mod\xE8le\npipe = pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=512,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n)\n\n\n#\
    \ Functions to clean text and extract paragraphs\n\ndef extract_text(html_content):\n\
    \    soup = BeautifulSoup(html_content, \"html.parser\")\n    text_parts = []\n\
    \n    for tag in soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n\
    \        text_parts.append(tag.get_text(separator=' '))\n\n    return ' '.join(text_parts).strip()\n\
    \ndef clean_email_text(text):\n    # Remove email addresses\n    text = re.sub(r'\\\
    b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n\n    # Remove\
    \ phone numbers\n    text = re.sub(r'\\+?\\d[\\d -]{7,}\\d', '', text)\n\n   \
    \ # Remove URLs\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\
    \\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n\n    # Remove \"unsubscribe\"\
    \ and \"disclaimer\" sections\n    text = re.sub(r'((Un)?subscribe|Disclaimer)[\\\
    s\\S]*', '', text, flags=re.IGNORECASE)\n\n    # Remove excessive whitespace\n\
    \    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n\n\n##################################################################################################\n\
    ##################################################################################################\n\
    ##########################Clean with VICUNA 13B ##################################################\n\
    ##################################################################################################\n\
    ##################################################################################################\n\
    def clean_text_with_gptq_model(cleaned_relevant_text, max_tokens=2048):\n    #\
    \ Adjust the number to reserve tokens for the prompt\n    reserved_tokens = 1000\n\
    \    max_chunk_tokens = max_tokens - reserved_tokens\n\n    # Tokenize the text\
    \ using the model's tokenizer\n    tokens = tokenizer.encode_plus(\n        cleaned_relevant_text,\n\
    \        max_length=max_chunk_tokens,\n        return_overflowing_tokens=True,\n\
    \        truncation=True,\n        padding='max_length',\n        stride=0\n \
    \   )\n\n    chunks = [tokens['input_ids']]\n    if 'overflowing_tokens' in tokens:\n\
    \        # Convert overflowing tokens to lists of integers\n        chunks.extend(tokens['overflowing_tokens'])\n\
    \n    cleaned_text_chunks = []\n    for chunk in chunks:\n        # Flatten the\
    \ list of lists into a single list of integers\n        flat_chunk = [item for\
    \ sublist in chunk for item in sublist]\n        decoded_chunk = tokenizer.decode(flat_chunk,\
    \ skip_special_tokens=True)\n        prompt = f\"Please remove disclaimers and\
    \ any irrelevant information from the following text:\\n\\n{decoded_chunk}\\n\\\
    nCleaned text:\"\n        \n        generated_text = pipe(prompt)[0]['generated_text']\n\
    \        response = generated_text.split(\"Cleaned text:\")[-1].strip()\n    \
    \    \n        cleaned_text_chunks.append(response)\n\n        # Free up GPU memory\n\
    \        torch.cuda.empty_cache()\n\n    return \" \".join(cleaned_text_chunks)\n\
    \n##################################################################################################\n\
    ###############Grabbing messages and looping them cleaning and sending mail#######################\n\
    ##################################################################################################\n\
    ##################################################################################################\n\
    \nif response.status_code == 200:\n    messages = response.json()[\"value\"]\n\
    \    for message in messages:\n        subject = message[\"subject\"]\n      \
    \  content = message[\"body\"][\"content\"]\n\n        # 1 Extract text from the\
    \ email content, utilise beautifulsoup, fournit le message qui est en html et\
    \ lui le reprend en texte\n        text = extract_text(content)\n        stock_found\
    \ = None\n        relevant_text = None\n\n        # 2 Search for stock names in\
    \ the text if found it's then a stock found and text is relevant \n        for\
    \ stock_name in stock_names:\n            if stock_name.lower() in text.lower():\n\
    \                stock_found = stock_name\n                relevant_text = text\n\
    \                break\n\n        if stock_found:\n            print(f\"Found\
    \ text mentioning {stock_found}: {relevant_text}\")\n\n            # 3 Clean the\
    \ relevant text using BeautifulSoup \n            cleaned_relevant_text = clean_email_text(relevant_text)\n\
    \            print(\"Cleaned relevant text using BeautifulSoup.\")\n         \
    \   print(f\"Cleaned text length: {len(cleaned_relevant_text)}\")\n          \
    \  # 4 Send the cleaned relevant text for further cleaning\n            cleaned_relevant_text\
    \ = clean_text_with_gptq_model(cleaned_relevant_text)\n            print(\"Cleaned\
    \ relevant text using GPTQ model.\")\n            \n            try:\n       \
    \         summarized_text = summarizer(cleaned_relevant_text, max_length=600,\
    \ min_length=10, do_sample=False)[0][\"summary_text\"]\n                summary\
    \ = summary.append({\"Stock\": stock_found, \"Subject\": subject, \"Summary\"\
    : summarized_text}, ignore_index=True)\n                print(f\"Summarized text\
    \ for {stock_found}.\")\n            except IndexError:\n                print(f\"\
    Error summarizing the text mentioning {stock_found}.\")\n\n    # Send email with\
    \ the summary\n    if not summary.empty:\n        send_email(summary, access_token)\n\
    \    else:\n        print(\"No emails mentioning stocks found\")\n\nelse:\n  \
    \  print(f\"Error fetching messages: {response.status_code}, {response.text}\"\
    )\n```"
  created_at: 2023-05-04 15:20:31+00:00
  edited: true
  hidden: false
  id: 6453db4f72d331dec89b7719
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-05-04T16:49:20.000Z'
    data:
      edited: true
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>OK I looked at the code just quickly, and have a few observations:</p>

          <ol>

          <li><p>Regex did not work for me at all for symbol lookup. In fact the only
          thing that worked was the Alpaca 65B, most of the time, and GPT-3.5 and
          GPT-4 every time. So if you are already using API calls to GPT-3.5 asking
          it to reverse lookup symbols would be money well spent.</p>

          </li>

          <li><p>Make sure that your prompt is constructed exactly as per the original
          model documentation. These smaller models, and llama in particular are extremely
          sensitive to how you format the prompt. Including the context, ### Instruction:
          ### Input: and ### Response: are not optional if you want a consistent response.</p>

          </li>

          <li><p>Buy more RAM and don''t waste time on smaller models...or do and
          then share your success story. It would be groundbreaking and I for one
          would love to read about how you made the sub 65B model work for your use
          case (similar to my use case), consistently and often enough to be genuinely
          usable.</p>

          </li>

          </ol>

          <p>...and I just got an email from OpenAI telling me that they (finally)
          granted me GPT-4 API access, yey :)</p>

          '
        raw: 'OK I looked at the code just quickly, and have a few observations:


          1. Regex did not work for me at all for symbol lookup. In fact the only
          thing that worked was the Alpaca 65B, most of the time, and GPT-3.5 and
          GPT-4 every time. So if you are already using API calls to GPT-3.5 asking
          it to reverse lookup symbols would be money well spent.


          2. Make sure that your prompt is constructed exactly as per the original
          model documentation. These smaller models, and llama in particular are extremely
          sensitive to how you format the prompt. Including the context, ### Instruction:
          ### Input: and ### Response: are not optional if you want a consistent response.


          3. Buy more RAM and don''t waste time on smaller models...or do and then
          share your success story. It would be groundbreaking and I for one would
          love to read about how you made the sub 65B model work for your use case
          (similar to my use case), consistently and often enough to be genuinely
          usable.


          ...and I just got an email from OpenAI telling me that they (finally) granted
          me GPT-4 API access, yey :)'
        updatedAt: '2023-05-04T16:49:59.814Z'
      numEdits: 1
      reactions: []
    id: 6453e21068cbb276cb525ee9
    type: comment
  author: vmajor
  content: 'OK I looked at the code just quickly, and have a few observations:


    1. Regex did not work for me at all for symbol lookup. In fact the only thing
    that worked was the Alpaca 65B, most of the time, and GPT-3.5 and GPT-4 every
    time. So if you are already using API calls to GPT-3.5 asking it to reverse lookup
    symbols would be money well spent.


    2. Make sure that your prompt is constructed exactly as per the original model
    documentation. These smaller models, and llama in particular are extremely sensitive
    to how you format the prompt. Including the context, ### Instruction: ### Input:
    and ### Response: are not optional if you want a consistent response.


    3. Buy more RAM and don''t waste time on smaller models...or do and then share
    your success story. It would be groundbreaking and I for one would love to read
    about how you made the sub 65B model work for your use case (similar to my use
    case), consistently and often enough to be genuinely usable.


    ...and I just got an email from OpenAI telling me that they (finally) granted
    me GPT-4 API access, yey :)'
  created_at: 2023-05-04 15:49:20+00:00
  edited: true
  hidden: false
  id: 6453e21068cbb276cb525ee9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
      fullname: Abdoullah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AbdouS
      type: user
    createdAt: '2023-05-04T16:59:28.000Z'
    data:
      edited: false
      editors:
      - AbdouS
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ddec073513879d336b6ed5c918a98e4e.svg
          fullname: Abdoullah
          isHf: false
          isPro: false
          name: AbdouS
          type: user
        html: '<p>Congratulation! on Api access but I would guess that the api call
          cost will be tremendous :D.</p>

          <p>For my use case regex is looking for names or part of stock names and
          it works quite well actually, if there''s any email where a specific stock
          is mentionned take the whole email clean it and feed it to the LLM.</p>

          <p>I will buy more ram but I dont think my 5800x3D will be enough for inference.
          I dont know.</p>

          <p>Is there a forum or discord for noobs like me to discuss these topics?
          It was a pleasure talking to you btw.</p>

          '
        raw: 'Congratulation! on Api access but I would guess that the api call cost
          will be tremendous :D.


          For my use case regex is looking for names or part of stock names and it
          works quite well actually, if there''s any email where a specific stock
          is mentionned take the whole email clean it and feed it to the LLM.


          I will buy more ram but I dont think my 5800x3D will be enough for inference.
          I dont know.


          Is there a forum or discord for noobs like me to discuss these topics? It
          was a pleasure talking to you btw.'
        updatedAt: '2023-05-04T16:59:28.220Z'
      numEdits: 0
      reactions: []
    id: 6453e470b8c58783d6661ce5
    type: comment
  author: AbdouS
  content: 'Congratulation! on Api access but I would guess that the api call cost
    will be tremendous :D.


    For my use case regex is looking for names or part of stock names and it works
    quite well actually, if there''s any email where a specific stock is mentionned
    take the whole email clean it and feed it to the LLM.


    I will buy more ram but I dont think my 5800x3D will be enough for inference.
    I dont know.


    Is there a forum or discord for noobs like me to discuss these topics? It was
    a pleasure talking to you btw.'
  created_at: 2023-05-04 15:59:28+00:00
  edited: false
  hidden: false
  id: 6453e470b8c58783d6661ce5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-05-05T04:48:52.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>OK, maybe I will take a closer look at you regex and see if I can
          use it. Your CPU will work if you can increase your system RAM. I have 128
          GB and it is a decent balance between the largest models that I want to
          work with given the size and speed of inference. Meaning, if I get more
          RAM I could use larger models but I do not want to wait a day for the inference
          to finish.</p>

          <p>Forum...not sure. I usually follow reddit and the respositories on GitHub.
          Since you are using GPTQ you may get good answers there.</p>

          '
        raw: 'OK, maybe I will take a closer look at you regex and see if I can use
          it. Your CPU will work if you can increase your system RAM. I have 128 GB
          and it is a decent balance between the largest models that I want to work
          with given the size and speed of inference. Meaning, if I get more RAM I
          could use larger models but I do not want to wait a day for the inference
          to finish.


          Forum...not sure. I usually follow reddit and the respositories on GitHub.
          Since you are using GPTQ you may get good answers there.'
        updatedAt: '2023-05-05T04:48:52.837Z'
      numEdits: 0
      reactions: []
    id: 64548ab46b61332c0e4ef6c2
    type: comment
  author: vmajor
  content: 'OK, maybe I will take a closer look at you regex and see if I can use
    it. Your CPU will work if you can increase your system RAM. I have 128 GB and
    it is a decent balance between the largest models that I want to work with given
    the size and speed of inference. Meaning, if I get more RAM I could use larger
    models but I do not want to wait a day for the inference to finish.


    Forum...not sure. I usually follow reddit and the respositories on GitHub. Since
    you are using GPTQ you may get good answers there.'
  created_at: 2023-05-05 03:48:52+00:00
  edited: false
  hidden: false
  id: 64548ab46b61332c0e4ef6c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T07:37:35.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>There''s quite a number of Discords for discussing this stuff. I
          hang out on several.  Here you go:</p>

          <ul>

          <li>Alpaca Lora (lots of discussion of fine tuning and training especially,
          and inference and coding too): <a rel="nofollow" href="https://discord.gg/ZMHkCGy9">https://discord.gg/ZMHkCGy9</a></li>

          <li>LmSys (people who released Vicuna. Discussion on inference and fine
          tuning/training): <a rel="nofollow" href="https://discord.gg/CPz84krv">https://discord.gg/CPz84krv</a></li>

          <li>GPT4ALL (a company called Nomic, who release models and have a simple
          local UI for inference. Not much discussion of fine tuning, some coding
          talk, lots of inference talk): <a rel="nofollow" href="https://discord.gg/sfWUbDKH">https://discord.gg/sfWUbDKH</a></li>

          </ul>

          '
        raw: 'There''s quite a number of Discords for discussing this stuff. I hang
          out on several.  Here you go:


          * Alpaca Lora (lots of discussion of fine tuning and training especially,
          and inference and coding too): https://discord.gg/ZMHkCGy9

          * LmSys (people who released Vicuna. Discussion on inference and fine tuning/training):
          https://discord.gg/CPz84krv

          * GPT4ALL (a company called Nomic, who release models and have a simple
          local UI for inference. Not much discussion of fine tuning, some coding
          talk, lots of inference talk): https://discord.gg/sfWUbDKH'
        updatedAt: '2023-05-05T07:37:35.774Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vmajor
    id: 6454b23fd55525a4fedd6461
    type: comment
  author: TheBloke
  content: 'There''s quite a number of Discords for discussing this stuff. I hang
    out on several.  Here you go:


    * Alpaca Lora (lots of discussion of fine tuning and training especially, and
    inference and coding too): https://discord.gg/ZMHkCGy9

    * LmSys (people who released Vicuna. Discussion on inference and fine tuning/training):
    https://discord.gg/CPz84krv

    * GPT4ALL (a company called Nomic, who release models and have a simple local
    UI for inference. Not much discussion of fine tuning, some coding talk, lots of
    inference talk): https://discord.gg/sfWUbDKH'
  created_at: 2023-05-05 06:37:35+00:00
  edited: false
  hidden: false
  id: 6454b23fd55525a4fedd6461
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-07T11:13:14.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8272093534469604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: "<p>Sorry to necro this thread a bit, but is the prompting style below\
          \ with ### Instruct:/### Response:, something that will rectify Stable Vicuna\
          \ conversing with itself, and can it be used in a langchain prompt template?\
          \ I'm having the toughest time stopping the model doing that with  ### Human:/###\
          \ Assistant:, no matter if I'm using custom_stopping_strings, or eos_token_id's,\
          \ etc. Nothing stops it. I end up stripping the rest of the conversation\
          \ out, and it extends inference time. Any guidance would be appreciated!</p>\n\
          <blockquote>\n<p>OK sure, you can use this as a basic example for instruction\
          \ based inference. </p>\n<pre><code>import argparse\nfrom llama_cpp import\
          \ Llama\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-m\"\
          , \"--model\", type=str, default=\"/home/vmajor/models/alpaca-lora-65B-GGML/alpaca-lora-65B.ggml.q5_1.bin\"\
          )\nargs = parser.parse_args()\n\nllm = Llama(model_path=args.model, n_threads=12,\
          \ n_ctx=2048, seed=1000, n_batch=128, last_n_tokens_size=150)\n\ncontext\
          \ = \"Below is an instruction that describes a task, paired with an input\
          \ that provides further context. Write a response that appropriately completes\
          \ the request.\\n\"\n\ninstruction = \"### Instruction: Please provide a\
          \ concise summary of the following news article, capturing the key information\
          \ and stating company ticker symbols, and government entity abbreviations,\
          \ whenever possible: \\n\"\n\ninput = \"\"\"### Input: Major Wall Street\
          \ firm sees a breakout in luxury stocks \u2014 and lists three reasons why\
          \ ETFs are a great way to play it. As luxury stocks make waves overseas,\
          \ State Street Global Advisors believes investors should consider European\
          \ ETFs if they want to capture the gains from their outperformance. Matt\
          \ Bartolini, the firm's head of SPDR Americas research, finds three reasons\
          \ why the backdrop is becoming particularly attractive. First and second\
          \ on his list: valuations and earnings upgrades. \"That's completely different\
          \ than what we saw for U.S. firms,\" he told CNBC's Bob Pisani on \"ETF\
          \ Edge\" this week. His remarks come as LVMH became the first European company\
          \ to surpass $500 billion in market value earlier this week. Bartolini lists\
          \ price momentum as a third driver of the investor shift. \\n\"\"\"\n\n\
          output = \"### Response: \\n\"\n\nprint(context + instruction + input +\
          \ output)\nsummary_output = llm(\n    context + instruction + input + output,\n\
          \    max_tokens=1024,\n    stop=None,\n    temperature=1.0,\n    repeat_penalty=1.1,\n\
          \    top_k=160,\n    top_p=0.5,\n    echo=True,\n)\n\nsummary_text = summary_output[\"\
          choices\"][0][\"text\"]\n# Split the output by the keyword Answer:\nparts\
          \ = summary_text.split(\"### Response:\")\n\n# Check if there are two parts\
          \ after splitting\nif len(parts) == 2:\n    # Get the second part which\
          \ contains the answer\n    answer = parts[1]\n\n    # Strip any leading\
          \ or trailing whitespace from the answer\n    answer = answer.strip()\n\
          </code></pre>\n</blockquote>\n"
        raw: "Sorry to necro this thread a bit, but is the prompting style below with\
          \ ### Instruct:/### Response:, something that will rectify Stable Vicuna\
          \ conversing with itself, and can it be used in a langchain prompt template?\
          \ I'm having the toughest time stopping the model doing that with  ### Human:/###\
          \ Assistant:, no matter if I'm using custom_stopping_strings, or eos_token_id's,\
          \ etc. Nothing stops it. I end up stripping the rest of the conversation\
          \ out, and it extends inference time. Any guidance would be appreciated!\n\
          \n> OK sure, you can use this as a basic example for instruction based inference.\
          \ \n> ```\n> import argparse\n> from llama_cpp import Llama\n> \n> parser\
          \ = argparse.ArgumentParser()\n> parser.add_argument(\"-m\", \"--model\"\
          , type=str, default=\"/home/vmajor/models/alpaca-lora-65B-GGML/alpaca-lora-65B.ggml.q5_1.bin\"\
          )\n> args = parser.parse_args()\n> \n> llm = Llama(model_path=args.model,\
          \ n_threads=12, n_ctx=2048, seed=1000, n_batch=128, last_n_tokens_size=150)\n\
          > \n> context = \"Below is an instruction that describes a task, paired\
          \ with an input that provides further context. Write a response that appropriately\
          \ completes the request.\\n\"\n> \n> instruction = \"### Instruction: Please\
          \ provide a concise summary of the following news article, capturing the\
          \ key information and stating company ticker symbols, and government entity\
          \ abbreviations, whenever possible: \\n\"\n> \n> input = \"\"\"### Input:\
          \ Major Wall Street firm sees a breakout in luxury stocks \u2014 and lists\
          \ three reasons why ETFs are a great way to play it. As luxury stocks make\
          \ waves overseas, State Street Global Advisors believes investors should\
          \ consider European ETFs if they want to capture the gains from their outperformance.\
          \ Matt Bartolini, the firm's head of SPDR Americas research, finds three\
          \ reasons why the backdrop is becoming particularly attractive. First and\
          \ second on his list: valuations and earnings upgrades. \"That's completely\
          \ different than what we saw for U.S. firms,\" he told CNBC's Bob Pisani\
          \ on \"ETF Edge\" this week. His remarks come as LVMH became the first European\
          \ company to surpass $500 billion in market value earlier this week. Bartolini\
          \ lists price momentum as a third driver of the investor shift. \\n\"\"\"\
          \n> \n> output = \"### Response: \\n\"\n> \n> print(context + instruction\
          \ + input + output)\n> summary_output = llm(\n>     context + instruction\
          \ + input + output,\n>     max_tokens=1024,\n>     stop=None,\n>     temperature=1.0,\n\
          >     repeat_penalty=1.1,\n>     top_k=160,\n>     top_p=0.5,\n>     echo=True,\n\
          > )\n> \n> summary_text = summary_output[\"choices\"][0][\"text\"]\n> #\
          \ Split the output by the keyword Answer:\n> parts = summary_text.split(\"\
          ### Response:\")\n> \n> # Check if there are two parts after splitting\n\
          > if len(parts) == 2:\n>     # Get the second part which contains the answer\n\
          >     answer = parts[1]\n> \n>     # Strip any leading or trailing whitespace\
          \ from the answer\n>     answer = answer.strip()\n>"
        updatedAt: '2023-06-07T11:13:14.175Z'
      numEdits: 0
      reactions: []
    id: 6480664a9aafd41918a6005c
    type: comment
  author: Satya93
  content: "Sorry to necro this thread a bit, but is the prompting style below with\
    \ ### Instruct:/### Response:, something that will rectify Stable Vicuna conversing\
    \ with itself, and can it be used in a langchain prompt template? I'm having the\
    \ toughest time stopping the model doing that with  ### Human:/### Assistant:,\
    \ no matter if I'm using custom_stopping_strings, or eos_token_id's, etc. Nothing\
    \ stops it. I end up stripping the rest of the conversation out, and it extends\
    \ inference time. Any guidance would be appreciated!\n\n> OK sure, you can use\
    \ this as a basic example for instruction based inference. \n> ```\n> import argparse\n\
    > from llama_cpp import Llama\n> \n> parser = argparse.ArgumentParser()\n> parser.add_argument(\"\
    -m\", \"--model\", type=str, default=\"/home/vmajor/models/alpaca-lora-65B-GGML/alpaca-lora-65B.ggml.q5_1.bin\"\
    )\n> args = parser.parse_args()\n> \n> llm = Llama(model_path=args.model, n_threads=12,\
    \ n_ctx=2048, seed=1000, n_batch=128, last_n_tokens_size=150)\n> \n> context =\
    \ \"Below is an instruction that describes a task, paired with an input that provides\
    \ further context. Write a response that appropriately completes the request.\\\
    n\"\n> \n> instruction = \"### Instruction: Please provide a concise summary of\
    \ the following news article, capturing the key information and stating company\
    \ ticker symbols, and government entity abbreviations, whenever possible: \\n\"\
    \n> \n> input = \"\"\"### Input: Major Wall Street firm sees a breakout in luxury\
    \ stocks \u2014 and lists three reasons why ETFs are a great way to play it. As\
    \ luxury stocks make waves overseas, State Street Global Advisors believes investors\
    \ should consider European ETFs if they want to capture the gains from their outperformance.\
    \ Matt Bartolini, the firm's head of SPDR Americas research, finds three reasons\
    \ why the backdrop is becoming particularly attractive. First and second on his\
    \ list: valuations and earnings upgrades. \"That's completely different than what\
    \ we saw for U.S. firms,\" he told CNBC's Bob Pisani on \"ETF Edge\" this week.\
    \ His remarks come as LVMH became the first European company to surpass $500 billion\
    \ in market value earlier this week. Bartolini lists price momentum as a third\
    \ driver of the investor shift. \\n\"\"\"\n> \n> output = \"### Response: \\n\"\
    \n> \n> print(context + instruction + input + output)\n> summary_output = llm(\n\
    >     context + instruction + input + output,\n>     max_tokens=1024,\n>     stop=None,\n\
    >     temperature=1.0,\n>     repeat_penalty=1.1,\n>     top_k=160,\n>     top_p=0.5,\n\
    >     echo=True,\n> )\n> \n> summary_text = summary_output[\"choices\"][0][\"\
    text\"]\n> # Split the output by the keyword Answer:\n> parts = summary_text.split(\"\
    ### Response:\")\n> \n> # Check if there are two parts after splitting\n> if len(parts)\
    \ == 2:\n>     # Get the second part which contains the answer\n>     answer =\
    \ parts[1]\n> \n>     # Strip any leading or trailing whitespace from the answer\n\
    >     answer = answer.strip()\n>"
  created_at: 2023-06-07 10:13:14+00:00
  edited: false
  hidden: false
  id: 6480664a9aafd41918a6005c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
      fullname: Victor Major
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmajor
      type: user
    createdAt: '2023-06-07T11:33:32.000Z'
    data:
      edited: false
      editors:
      - vmajor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9728946685791016
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63992e59afe0d224cf2b6bf1/q2JeqTcIb5j6fUg1SWGzL.jpeg?w=200&h=200&f=face
          fullname: Victor Major
          isHf: false
          isPro: false
          name: vmajor
          type: user
        html: '<p>Can''t help with stable vicuna but that prompt method works with
          all llama models I tried. </p>

          <p>I should note that no prompt worked reliably with smaller models. The
          smallest model that I used and is reliable is the new 30B Wizard.</p>

          '
        raw: "Can't help with stable vicuna but that prompt method works with all\
          \ llama models I tried. \n\nI should note that no prompt worked reliably\
          \ with smaller models. The smallest model that I used and is reliable is\
          \ the new 30B Wizard."
        updatedAt: '2023-06-07T11:33:32.425Z'
      numEdits: 0
      reactions: []
    id: 64806b0c40facadc556b49b0
    type: comment
  author: vmajor
  content: "Can't help with stable vicuna but that prompt method works with all llama\
    \ models I tried. \n\nI should note that no prompt worked reliably with smaller\
    \ models. The smallest model that I used and is reliable is the new 30B Wizard."
  created_at: 2023-06-07 10:33:32+00:00
  edited: false
  hidden: false
  id: 64806b0c40facadc556b49b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
      fullname: Gordon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya93
      type: user
    createdAt: '2023-06-07T15:36:09.000Z'
    data:
      edited: false
      editors:
      - Satya93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9641232490539551
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1f35d36f235ab6987526943c7c51d7a.svg
          fullname: Gordon
          isHf: false
          isPro: false
          name: Satya93
          type: user
        html: '<p>Ok, thanks for the clarification! The model works quite well in
          the text-generation-ui with the Vicuna0 template in Instruct mode. I just
          couldn''t find out exactly how the ui was feeding that in to generation.</p>

          '
        raw: Ok, thanks for the clarification! The model works quite well in the text-generation-ui
          with the Vicuna0 template in Instruct mode. I just couldn't find out exactly
          how the ui was feeding that in to generation.
        updatedAt: '2023-06-07T15:36:09.264Z'
      numEdits: 0
      reactions: []
    id: 6480a3e940facadc556f98aa
    type: comment
  author: Satya93
  content: Ok, thanks for the clarification! The model works quite well in the text-generation-ui
    with the Vicuna0 template in Instruct mode. I just couldn't find out exactly how
    the ui was feeding that in to generation.
  created_at: 2023-06-07 14:36:09+00:00
  edited: false
  hidden: false
  id: 6480a3e940facadc556f98aa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors not compatible
  with "standard" settings
