!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abhinavkulkarni
conflicting_files: null
created_at: 2023-05-13 06:13:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-05-13T07:13:42.000Z'
    data:
      edited: true
      editors:
      - abhinavkulkarni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: '<p>Hi,</p>

          <p>Pardon me for asking this, but I have a very basic question about 4-bit
          quantization. How are these 4-bit quantized weights loaded in PyTorch (through
          HF <code>AutoModelForCausalLM</code> API) when PyTorch doesn''t natively
          support <code>int4</code>?</p>

          <p>For e.g., I understand how 4-bit quantized vectors (or matrixes) and
          the corresponding fp32 scaling factor and zero points can be stored contiguously
          as is explained <a rel="nofollow" href="https://acl.inf.ethz.ch/research/Clover/">here</a>,
          however, I am not clear about how the computations are being done in PyTorch
          when it doesn''t support a native <code>int4</code> data type.</p>

          <p>Thanks!</p>

          '
        raw: 'Hi,


          Pardon me for asking this, but I have a very basic question about 4-bit
          quantization. How are these 4-bit quantized weights loaded in PyTorch (through
          HF `AutoModelForCausalLM` API) when PyTorch doesn''t natively support `int4`?


          For e.g., I understand how 4-bit quantized vectors (or matrixes) and the
          corresponding fp32 scaling factor and zero points can be stored contiguously
          as is explained [here](https://acl.inf.ethz.ch/research/Clover/), however,
          I am not clear about how the computations are being done in PyTorch when
          it doesn''t support a native `int4` data type.


          Thanks!'
        updatedAt: '2023-05-13T08:09:51.518Z'
      numEdits: 1
      reactions: []
    id: 645f38a6a40b7e364c3f0dca
    type: comment
  author: abhinavkulkarni
  content: 'Hi,


    Pardon me for asking this, but I have a very basic question about 4-bit quantization.
    How are these 4-bit quantized weights loaded in PyTorch (through HF `AutoModelForCausalLM`
    API) when PyTorch doesn''t natively support `int4`?


    For e.g., I understand how 4-bit quantized vectors (or matrixes) and the corresponding
    fp32 scaling factor and zero points can be stored contiguously as is explained
    [here](https://acl.inf.ethz.ch/research/Clover/), however, I am not clear about
    how the computations are being done in PyTorch when it doesn''t support a native
    `int4` data type.


    Thanks!'
  created_at: 2023-05-13 06:13:42+00:00
  edited: true
  hidden: false
  id: 645f38a6a40b7e364c3f0dca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-13T11:11:27.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can''t load 4bit models in native transformers at the moment.  You
          may be able to do so soon, when <code>bitsandbytes</code> releases its new
          4bit mode.  However then you would use the base float16 model, with something
          like <code>load_in_4bit=True</code> (not sure exactly as it''s not released
          yet) - same principle as their current 8bit quantisations.</p>

          <p>To load GPTQ 4bit models you need to use compatible code.</p>

          <p>There''s a relatively new repo called <a rel="nofollow" href="https://github.com/PanQiWei/AutoGPTQ">AutoGPTQ</a>
          which aims to make it as easy as possible to load GPTQ models and then use
          them with standard transformers code.  You still don''t use <code>AutoModelForCausalLM</code>
          - instead you use <code>AutoGPTQForCausalLM</code> - but once the model
          is loaded, then you can use any normal transformers code.</p>

          '
        raw: 'You can''t load 4bit models in native transformers at the moment.  You
          may be able to do so soon, when `bitsandbytes` releases its new 4bit mode.  However
          then you would use the base float16 model, with something like `load_in_4bit=True`
          (not sure exactly as it''s not released yet) - same principle as their current
          8bit quantisations.


          To load GPTQ 4bit models you need to use compatible code.


          There''s a relatively new repo called [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
          which aims to make it as easy as possible to load GPTQ models and then use
          them with standard transformers code.  You still don''t use `AutoModelForCausalLM`
          - instead you use `AutoGPTQForCausalLM` - but once the model is loaded,
          then you can use any normal transformers code.'
        updatedAt: '2023-05-13T11:11:27.705Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
        - laek
        - abhinavkulkarni
    id: 645f705f25a4075bcf99f79a
    type: comment
  author: TheBloke
  content: 'You can''t load 4bit models in native transformers at the moment.  You
    may be able to do so soon, when `bitsandbytes` releases its new 4bit mode.  However
    then you would use the base float16 model, with something like `load_in_4bit=True`
    (not sure exactly as it''s not released yet) - same principle as their current
    8bit quantisations.


    To load GPTQ 4bit models you need to use compatible code.


    There''s a relatively new repo called [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)
    which aims to make it as easy as possible to load GPTQ models and then use them
    with standard transformers code.  You still don''t use `AutoModelForCausalLM`
    - instead you use `AutoGPTQForCausalLM` - but once the model is loaded, then you
    can use any normal transformers code.'
  created_at: 2023-05-13 10:11:27+00:00
  edited: false
  hidden: false
  id: 645f705f25a4075bcf99f79a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-05-15T05:35:59.000Z'
    data:
      edited: false
      editors:
      - abhinavkulkarni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
          fullname: Abhinav Kulkarni
          isHf: false
          isPro: false
          name: abhinavkulkarni
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ for the reply and all the great work you do in providing quantized 4-bit\
          \ models to the community.</p>\n"
        raw: 'Thanks @TheBloke for the reply and all the great work you do in providing
          quantized 4-bit models to the community.

          '
        updatedAt: '2023-05-15T05:35:59.673Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - BrianTin
      relatedEventId: 6461c4bf3cc3259ce05f0013
    id: 6461c4bf3cc3259ce05f0012
    type: comment
  author: abhinavkulkarni
  content: 'Thanks @TheBloke for the reply and all the great work you do in providing
    quantized 4-bit models to the community.

    '
  created_at: 2023-05-15 04:35:59+00:00
  edited: false
  hidden: false
  id: 6461c4bf3cc3259ce05f0012
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1643883553153-noauth.jpeg?w=200&h=200&f=face
      fullname: Abhinav Kulkarni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhinavkulkarni
      type: user
    createdAt: '2023-05-15T05:35:59.000Z'
    data:
      status: closed
    id: 6461c4bf3cc3259ce05f0013
    type: status-change
  author: abhinavkulkarni
  created_at: 2023-05-15 04:35:59+00:00
  id: 6461c4bf3cc3259ce05f0013
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Basic question about 4-bit quantization
