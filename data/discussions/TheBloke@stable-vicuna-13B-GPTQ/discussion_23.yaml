!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Depsi
conflicting_files: null
created_at: 2023-05-20 08:37:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
      fullname: Depsi Pill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Depsi
      type: user
    createdAt: '2023-05-20T09:37:30.000Z'
    data:
      edited: false
      editors:
      - Depsi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
          fullname: Depsi Pill
          isHf: false
          isPro: false
          name: Depsi
          type: user
        html: '<p>I have installed everything, and it all went well without any errors.
          Now, when I run the start_windows.bat file, it asks me to press any key
          to continue. When I do, the cmd window closes but nothing happens. I hope
          the screenshot below helps.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64688fcc5d701566394cb3d0/5cVjtJaUYN531FFzUN_l2.png"><img
          alt="2023-05-20.png" src="https://cdn-uploads.huggingface.co/production/uploads/64688fcc5d701566394cb3d0/5cVjtJaUYN531FFzUN_l2.png"></a></p>

          '
        raw: "I have installed everything, and it all went well without any errors.\
          \ Now, when I run the start_windows.bat file, it asks me to press any key\
          \ to continue. When I do, the cmd window closes but nothing happens. I hope\
          \ the screenshot below helps.\r\n![2023-05-20.png](https://cdn-uploads.huggingface.co/production/uploads/64688fcc5d701566394cb3d0/5cVjtJaUYN531FFzUN_l2.png)\r\
          \n"
        updatedAt: '2023-05-20T09:37:30.600Z'
      numEdits: 0
      reactions: []
    id: 646894dae134d050a589872b
    type: comment
  author: Depsi
  content: "I have installed everything, and it all went well without any errors.\
    \ Now, when I run the start_windows.bat file, it asks me to press any key to continue.\
    \ When I do, the cmd window closes but nothing happens. I hope the screenshot\
    \ below helps.\r\n![2023-05-20.png](https://cdn-uploads.huggingface.co/production/uploads/64688fcc5d701566394cb3d0/5cVjtJaUYN531FFzUN_l2.png)\r\
    \n"
  created_at: 2023-05-20 08:37:30+00:00
  edited: false
  hidden: false
  id: 646894dae134d050a589872b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-20T10:15:28.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>How much RAM do you have?</p>

          '
        raw: How much RAM do you have?
        updatedAt: '2023-05-20T10:15:28.044Z'
      numEdits: 0
      reactions: []
    id: 64689dc099182de178455bce
    type: comment
  author: TheBloke
  content: How much RAM do you have?
  created_at: 2023-05-20 09:15:28+00:00
  edited: false
  hidden: false
  id: 64689dc099182de178455bce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
      fullname: Depsi Pill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Depsi
      type: user
    createdAt: '2023-05-22T07:33:20.000Z'
    data:
      edited: false
      editors:
      - Depsi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
          fullname: Depsi Pill
          isHf: false
          isPro: false
          name: Depsi
          type: user
        html: '<blockquote>

          <p>How much RAM do you have?</p>

          </blockquote>

          <p>32</p>

          '
        raw: '> How much RAM do you have?


          32'
        updatedAt: '2023-05-22T07:33:20.252Z'
      numEdits: 0
      reactions: []
    id: 646b1ac0db697c798a2cc62a
    type: comment
  author: Depsi
  content: '> How much RAM do you have?


    32'
  created_at: 2023-05-22 06:33:20+00:00
  edited: false
  hidden: false
  id: 646b1ac0db697c798a2cc62a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-24T08:27:14.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK I''ve finally learned what causes this.  Yes it is RAM related.  The
          model needs more than 32GB of RAM while loading onto the GPU.</p>

          <p>The fix is to increase your pagefile size - you may need as much as 90GB
          pagefile.  That will give it enough room to load the model onto VRAM, and
          then it will work fine and run exclusively from the GPU.</p>

          '
        raw: 'OK I''ve finally learned what causes this.  Yes it is RAM related.  The
          model needs more than 32GB of RAM while loading onto the GPU.


          The fix is to increase your pagefile size - you may need as much as 90GB
          pagefile.  That will give it enough room to load the model onto VRAM, and
          then it will work fine and run exclusively from the GPU.'
        updatedAt: '2023-05-24T08:27:25.844Z'
      numEdits: 1
      reactions: []
    id: 646dca62e34b2ec2d2d301de
    type: comment
  author: TheBloke
  content: 'OK I''ve finally learned what causes this.  Yes it is RAM related.  The
    model needs more than 32GB of RAM while loading onto the GPU.


    The fix is to increase your pagefile size - you may need as much as 90GB pagefile.  That
    will give it enough room to load the model onto VRAM, and then it will work fine
    and run exclusively from the GPU.'
  created_at: 2023-05-24 07:27:14+00:00
  edited: true
  hidden: false
  id: 646dca62e34b2ec2d2d301de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
      fullname: Depsi Pill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Depsi
      type: user
    createdAt: '2023-05-24T11:56:24.000Z'
    data:
      edited: false
      editors:
      - Depsi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
          fullname: Depsi Pill
          isHf: false
          isPro: false
          name: Depsi
          type: user
        html: '<p>Got it! much appreciated.<br>Any GPUs you would recommend maybe
          from the upper midrange variety? Unfortunately, I''m not a computer geek,
          so if you give me the specifications, I''ll know what to look for when shopping.<br>Thanks
          again, and have a good day!</p>

          '
        raw: "Got it! much appreciated. \nAny GPUs you would recommend maybe from\
          \ the upper midrange variety? Unfortunately, I'm not a computer geek, so\
          \ if you give me the specifications, I'll know what to look for when shopping.\n\
          Thanks again, and have a good day!"
        updatedAt: '2023-05-24T11:56:24.638Z'
      numEdits: 0
      reactions: []
    id: 646dfb684bbb6117ee306b21
    type: comment
  author: Depsi
  content: "Got it! much appreciated. \nAny GPUs you would recommend maybe from the\
    \ upper midrange variety? Unfortunately, I'm not a computer geek, so if you give\
    \ me the specifications, I'll know what to look for when shopping.\nThanks again,\
    \ and have a good day!"
  created_at: 2023-05-24 10:56:24+00:00
  edited: false
  hidden: false
  id: 646dfb684bbb6117ee306b21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-26T11:34:58.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>The 4090 is the current king of consumer GPUs. It has 24GB VRAM
          which is very useful - meaning you can load a 30B 4bit model completely
          into VRAM - and it''s extremely fast.</p>

          <p>But the 4090 is also very expensive.  A compromise would be a 3090, perhaps
          a used one.   If buying used you can get a 3090 for less than half the price
          of a 4090, but it has the same amount of VRAM and is no more than ~20-30%
          slower for LLM inference.</p>

          <p>If a 3090 is too much, then look for the best card you can afford with
          12GB VRAM.  A 4070 is one option. Or if that''s still too much, there are
          a number of 3060s that have 12GB and are very affordable, like 1/3 the price
          of a used 3090.</p>

          '
        raw: 'The 4090 is the current king of consumer GPUs. It has 24GB VRAM which
          is very useful - meaning you can load a 30B 4bit model completely into VRAM
          - and it''s extremely fast.


          But the 4090 is also very expensive.  A compromise would be a 3090, perhaps
          a used one.   If buying used you can get a 3090 for less than half the price
          of a 4090, but it has the same amount of VRAM and is no more than ~20-30%
          slower for LLM inference.


          If a 3090 is too much, then look for the best card you can afford with 12GB
          VRAM.  A 4070 is one option. Or if that''s still too much, there are a number
          of 3060s that have 12GB and are very affordable, like 1/3 the price of a
          used 3090.'
        updatedAt: '2023-05-26T11:37:09.641Z'
      numEdits: 2
      reactions: []
    id: 64709962806c7d87fa15947f
    type: comment
  author: TheBloke
  content: 'The 4090 is the current king of consumer GPUs. It has 24GB VRAM which
    is very useful - meaning you can load a 30B 4bit model completely into VRAM -
    and it''s extremely fast.


    But the 4090 is also very expensive.  A compromise would be a 3090, perhaps a
    used one.   If buying used you can get a 3090 for less than half the price of
    a 4090, but it has the same amount of VRAM and is no more than ~20-30% slower
    for LLM inference.


    If a 3090 is too much, then look for the best card you can afford with 12GB VRAM.  A
    4070 is one option. Or if that''s still too much, there are a number of 3060s
    that have 12GB and are very affordable, like 1/3 the price of a used 3090.'
  created_at: 2023-05-26 10:34:58+00:00
  edited: true
  hidden: false
  id: 64709962806c7d87fa15947f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
      fullname: Depsi Pill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Depsi
      type: user
    createdAt: '2023-05-26T13:15:12.000Z'
    data:
      edited: false
      editors:
      - Depsi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c8e165932d5884e47a9c678118603fff.svg
          fullname: Depsi Pill
          isHf: false
          isPro: false
          name: Depsi
          type: user
        html: '<p>Thank you very much for your time. This was really helpful. The
          3090 seems very enticing!<br>Have a great day!</p>

          '
        raw: 'Thank you very much for your time. This was really helpful. The 3090
          seems very enticing!

          Have a great day!'
        updatedAt: '2023-05-26T13:15:12.028Z'
      numEdits: 0
      reactions: []
    id: 6470b0e01f0e7ee7fb1b1955
    type: comment
  author: Depsi
  content: 'Thank you very much for your time. This was really helpful. The 3090 seems
    very enticing!

    Have a great day!'
  created_at: 2023-05-26 12:15:12+00:00
  edited: false
  hidden: false
  id: 6470b0e01f0e7ee7fb1b1955
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/971061b5c124eaf52851fb7951ee5f67.svg
      fullname: legao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: legao26
      type: user
    createdAt: '2023-05-28T08:03:40.000Z'
    data:
      edited: false
      editors:
      - legao26
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/971061b5c124eaf52851fb7951ee5f67.svg
          fullname: legao
          isHf: false
          isPro: false
          name: legao26
          type: user
        html: '<p>robot</p>

          '
        raw: robot
        updatedAt: '2023-05-28T08:03:40.478Z'
      numEdits: 0
      reactions: []
    id: 64730adc6cff2f8672fe91b1
    type: comment
  author: legao26
  content: robot
  created_at: 2023-05-28 07:03:40+00:00
  edited: false
  hidden: false
  id: 64730adc6cff2f8672fe91b1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Issue starting the webui
