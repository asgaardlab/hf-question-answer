!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AB00k
conflicting_files: null
created_at: 2023-07-12 10:12:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c282fd8cc87cf0c057420d/kcH8CPemTkTAPMA-o7hRT.png?w=200&h=200&f=face
      fullname: Abdul Basit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AB00k
      type: user
    createdAt: '2023-07-12T11:12:54.000Z'
    data:
      edited: true
      editors:
      - AB00k
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49472925066947937
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c282fd8cc87cf0c057420d/kcH8CPemTkTAPMA-o7hRT.png?w=200&h=200&f=face
          fullname: Abdul Basit
          isHf: false
          isPro: false
          name: AB00k
          type: user
        html: '<p>Could not find model in TheBloke/stable-vicuna-13B-GPTQ</p>

          <p>I''m having above error when trying to load it using following code is
          it not et available?<br>from transformers import AutoTokenizer, pipeline,
          logging<br>from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig<br>import
          argparse</p>

          <h1 id="change-this-path-to-match-where-you-downloaded-the-model">change
          this path to match where you downloaded the model</h1>

          <p>quantized_model_dir = "TheBloke/stable-vicuna-13B-GPTQ"</p>

          <p>model_basename = "stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(quantized_model_dir, use_fast=True)</p>

          <p>quantize_config = BaseQuantizeConfig(<br>        bits=4,<br>        group_size=128,<br>        desc_act=False<br>    )</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,<br>        use_safetensors=True,<br>        model_basename=model_basename,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=quantize_config)</p>

          '
        raw: "Could not find model in TheBloke/stable-vicuna-13B-GPTQ\n\n\nI'm having\
          \ above error when trying to load it using following code is it not et available?\n\
          from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\n# change\
          \ this path to match where you downloaded the model\nquantized_model_dir\
          \ = \"TheBloke/stable-vicuna-13B-GPTQ\"\n\nmodel_basename = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
          \ use_fast=True)\n\nquantize_config = BaseQuantizeConfig(\n        bits=4,\n\
          \        group_size=128,\n        desc_act=False\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
          \        use_safetensors=True,\n        model_basename=model_basename,\n\
          \        device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=quantize_config)"
        updatedAt: '2023-07-12T11:14:33.107Z'
      numEdits: 1
      reactions: []
    id: 64ae8ab660ecf778883c6c6a
    type: comment
  author: AB00k
  content: "Could not find model in TheBloke/stable-vicuna-13B-GPTQ\n\n\nI'm having\
    \ above error when trying to load it using following code is it not et available?\n\
    from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq import\
    \ AutoGPTQForCausalLM, BaseQuantizeConfig\nimport argparse\n\n# change this path\
    \ to match where you downloaded the model\nquantized_model_dir = \"TheBloke/stable-vicuna-13B-GPTQ\"\
    \n\nmodel_basename = \"stable-vicuna-13B-GPTQ-4bit.compat.no-act-order.safetensors\"\
    \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(quantized_model_dir,\
    \ use_fast=True)\n\nquantize_config = BaseQuantizeConfig(\n        bits=4,\n \
    \       group_size=128,\n        desc_act=False\n    )\n\nmodel = AutoGPTQForCausalLM.from_quantized(quantized_model_dir,\n\
    \        use_safetensors=True,\n        model_basename=model_basename,\n     \
    \   device=\"cuda:0\",\n        use_triton=use_triton,\n        quantize_config=quantize_config)"
  created_at: 2023-07-12 10:12:54+00:00
  edited: true
  hidden: false
  id: 64ae8ab660ecf778883c6c6a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c282fd8cc87cf0c057420d/kcH8CPemTkTAPMA-o7hRT.png?w=200&h=200&f=face
      fullname: Abdul Basit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AB00k
      type: user
    createdAt: '2023-07-12T11:16:24.000Z'
    data:
      edited: false
      editors:
      - AB00k
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8258452415466309
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c282fd8cc87cf0c057420d/kcH8CPemTkTAPMA-o7hRT.png?w=200&h=200&f=face
          fullname: Abdul Basit
          isHf: false
          isPro: false
          name: AB00k
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> can you please\
          \ clarify this</p>\n"
        raw: '@TheBloke can you please clarify this

          '
        updatedAt: '2023-07-12T11:16:24.661Z'
      numEdits: 0
      reactions: []
    id: 64ae8b88a3331885d48923f8
    type: comment
  author: AB00k
  content: '@TheBloke can you please clarify this

    '
  created_at: 2023-07-12 10:16:24+00:00
  edited: false
  hidden: false
  id: 64ae8b88a3331885d48923f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-12T11:45:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6391960978507996
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Don''t put <code>.safetensors</code> on the end of model basename</p>

          <pre><code>model_basename = "stable-vicuna-13B-GPTQ-4bit.compat.no-act-order"

          </code></pre>

          '
        raw: 'Don''t put `.safetensors` on the end of model basename


          ```

          model_basename = "stable-vicuna-13B-GPTQ-4bit.compat.no-act-order"

          ```'
        updatedAt: '2023-07-12T11:45:19.714Z'
      numEdits: 0
      reactions: []
    id: 64ae924f076b0a518b373343
    type: comment
  author: TheBloke
  content: 'Don''t put `.safetensors` on the end of model basename


    ```

    model_basename = "stable-vicuna-13B-GPTQ-4bit.compat.no-act-order"

    ```'
  created_at: 2023-07-12 10:45:19+00:00
  edited: false
  hidden: false
  id: 64ae924f076b0a518b373343
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c282fd8cc87cf0c057420d/kcH8CPemTkTAPMA-o7hRT.png?w=200&h=200&f=face
      fullname: Abdul Basit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AB00k
      type: user
    createdAt: '2023-07-12T12:11:04.000Z'
    data:
      edited: false
      editors:
      - AB00k
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.973612368106842
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c282fd8cc87cf0c057420d/kcH8CPemTkTAPMA-o7hRT.png?w=200&h=200&f=face
          fullname: Abdul Basit
          isHf: false
          isPro: false
          name: AB00k
          type: user
        html: '<p>Thanks it worked!</p>

          '
        raw: 'Thanks it worked!

          '
        updatedAt: '2023-07-12T12:11:04.261Z'
      numEdits: 0
      reactions: []
    id: 64ae9858efd22ea69bddba78
    type: comment
  author: AB00k
  content: 'Thanks it worked!

    '
  created_at: 2023-07-12 11:11:04+00:00
  edited: false
  hidden: false
  id: 64ae9858efd22ea69bddba78
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Could not find model in TheBloke/stable-vicuna-13B-GPTQ
