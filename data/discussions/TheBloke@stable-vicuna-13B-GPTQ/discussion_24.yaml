!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 2themaxx
conflicting_files: null
created_at: 2023-05-24 13:12:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4ebf5c09820f832479de642dd077858.svg
      fullname: Samuel Mantravadi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2themaxx
      type: user
    createdAt: '2023-05-24T14:12:23.000Z'
    data:
      edited: true
      editors:
      - 2themaxx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4ebf5c09820f832479de642dd077858.svg
          fullname: Samuel Mantravadi
          isHf: false
          isPro: false
          name: 2themaxx
          type: user
        html: "<p>Was looking at various llm quant options and came across: <a rel=\"\
          nofollow\" href=\"https://github.com/artidoro/qlora\">https://github.com/artidoro/qlora</a>\
          \ </p>\n<p>It mentions load_in_4bit in the <a rel=\"nofollow\" href=\"https://github.com/artidoro/qlora#quantization\"\
          >README.md</a>, and I hadn't heard of that being available. Apparently they\
          \ built a new datatype for it (not sure how performant it is). After a bit\
          \ of looking around, it's apparently now part of the huggingface <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/peft/blob/d6015bc11fc9a194c54ed9337513716ac6554437/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py\"\
          >peft</a> library. Tim Dettmers (working on bitsandbytes) is also a contributor\
          \ to the qlora repo (likely also part of his research), but it looks like\
          \ you can now load straight models into frozen 4-bit at least for training,\
          \ and it should be able to be work in inference as well one would assume\
          \ just by following the instructions on the readme.</p>\n<p>I stumbled onto\
          \ the closed conversation here by TheBloke mentioning \"when it is released\
          \ it will look like... load_in_4bit\" or some such, and thought I'd post\
          \ this in case it's useful to anyone.</p>\n<p>Wonder if this could also\
          \ be used on the CPU easily and how that might perform\U0001F914</p>\n"
        raw: "Was looking at various llm quant options and came across: https://github.com/artidoro/qlora\
          \ \n\nIt mentions load_in_4bit in the [README.md](https://github.com/artidoro/qlora#quantization),\
          \ and I hadn't heard of that being available. Apparently they built a new\
          \ datatype for it (not sure how performant it is). After a bit of looking\
          \ around, it's apparently now part of the huggingface [peft](https://github.com/huggingface/peft/blob/d6015bc11fc9a194c54ed9337513716ac6554437/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py)\
          \ library. Tim Dettmers (working on bitsandbytes) is also a contributor\
          \ to the qlora repo (likely also part of his research), but it looks like\
          \ you can now load straight models into frozen 4-bit at least for training,\
          \ and it should be able to be work in inference as well one would assume\
          \ just by following the instructions on the readme.\n\nI stumbled onto the\
          \ closed conversation here by TheBloke mentioning \"when it is released\
          \ it will look like... load_in_4bit\" or some such, and thought I'd post\
          \ this in case it's useful to anyone.\n\nWonder if this could also be used\
          \ on the CPU easily and how that might perform\U0001F914"
        updatedAt: '2023-05-24T14:14:27.609Z'
      numEdits: 1
      reactions: []
    id: 646e1b47075bbcc48ddf0c00
    type: comment
  author: 2themaxx
  content: "Was looking at various llm quant options and came across: https://github.com/artidoro/qlora\
    \ \n\nIt mentions load_in_4bit in the [README.md](https://github.com/artidoro/qlora#quantization),\
    \ and I hadn't heard of that being available. Apparently they built a new datatype\
    \ for it (not sure how performant it is). After a bit of looking around, it's\
    \ apparently now part of the huggingface [peft](https://github.com/huggingface/peft/blob/d6015bc11fc9a194c54ed9337513716ac6554437/examples/fp4_finetuning/finetune_fp4_opt_bnb_peft.py)\
    \ library. Tim Dettmers (working on bitsandbytes) is also a contributor to the\
    \ qlora repo (likely also part of his research), but it looks like you can now\
    \ load straight models into frozen 4-bit at least for training, and it should\
    \ be able to be work in inference as well one would assume just by following the\
    \ instructions on the readme.\n\nI stumbled onto the closed conversation here\
    \ by TheBloke mentioning \"when it is released it will look like... load_in_4bit\"\
    \ or some such, and thought I'd post this in case it's useful to anyone.\n\nWonder\
    \ if this could also be used on the CPU easily and how that might perform\U0001F914"
  created_at: 2023-05-24 13:12:23+00:00
  edited: true
  hidden: false
  id: 646e1b47075bbcc48ddf0c00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-24T14:18:52.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Thanks.  It''s not fully released yet. The code is in PEFT and transformers
          (or will be soon). But the actual 4bit bitsandbytes library is not yet released.  I''m
          sure it''ll be out very soon.</p>

          <p>I wouldn''t hold your hopes up for good performance.  8bit bitsandbytes
          performs much worse than other methods, and early indications seem to be
          that this may be the same for 4bit.</p>

          <p>The huge benefit that bitsandbytes has is how easy it is to use. You
          can download any HF model and with one parameter, load it in 8bit instead,
          and soon 4bit as well.  But that does bring with it a cost to performance.
          I wouldn''t expect it to be usable on CPU at all; for that you want GGML
          q4 or q5.</p>

          '
        raw: 'Thanks.  It''s not fully released yet. The code is in PEFT and transformers
          (or will be soon). But the actual 4bit bitsandbytes library is not yet released.  I''m
          sure it''ll be out very soon.


          I wouldn''t hold your hopes up for good performance.  8bit bitsandbytes
          performs much worse than other methods, and early indications seem to be
          that this may be the same for 4bit.


          The huge benefit that bitsandbytes has is how easy it is to use. You can
          download any HF model and with one parameter, load it in 8bit instead, and
          soon 4bit as well.  But that does bring with it a cost to performance. I
          wouldn''t expect it to be usable on CPU at all; for that you want GGML q4
          or q5.'
        updatedAt: '2023-05-24T14:18:52.253Z'
      numEdits: 0
      reactions: []
    id: 646e1ccce34b2ec2d2dfd89d
    type: comment
  author: TheBloke
  content: 'Thanks.  It''s not fully released yet. The code is in PEFT and transformers
    (or will be soon). But the actual 4bit bitsandbytes library is not yet released.  I''m
    sure it''ll be out very soon.


    I wouldn''t hold your hopes up for good performance.  8bit bitsandbytes performs
    much worse than other methods, and early indications seem to be that this may
    be the same for 4bit.


    The huge benefit that bitsandbytes has is how easy it is to use. You can download
    any HF model and with one parameter, load it in 8bit instead, and soon 4bit as
    well.  But that does bring with it a cost to performance. I wouldn''t expect it
    to be usable on CPU at all; for that you want GGML q4 or q5.'
  created_at: 2023-05-24 13:18:52+00:00
  edited: false
  hidden: false
  id: 646e1ccce34b2ec2d2dfd89d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f4ebf5c09820f832479de642dd077858.svg
      fullname: Samuel Mantravadi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2themaxx
      type: user
    createdAt: '2023-05-25T00:49:22.000Z'
    data:
      edited: true
      editors:
      - 2themaxx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f4ebf5c09820f832479de642dd077858.svg
          fullname: Samuel Mantravadi
          isHf: false
          isPro: false
          name: 2themaxx
          type: user
        html: '<p>Not claiming anything about performance, but it looks like it''s
          at least in alpha release after the hugging face blog post on it today.
          </p>

          <p><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a></p>

          <p>They claim it works in inference.</p>

          '
        raw: "Not claiming anything about performance, but it looks like it's at least\
          \ in alpha release after the hugging face blog post on it today. \n\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\n\
          \nThey claim it works in inference."
        updatedAt: '2023-05-25T00:49:50.675Z'
      numEdits: 1
      reactions: []
    id: 646eb0927942c36e9daf0a6b
    type: comment
  author: 2themaxx
  content: "Not claiming anything about performance, but it looks like it's at least\
    \ in alpha release after the hugging face blog post on it today. \n\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\n\
    \nThey claim it works in inference."
  created_at: 2023-05-24 23:49:22+00:00
  edited: true
  hidden: false
  id: 646eb0927942c36e9daf0a6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-25T07:08:21.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah it''s out for training - but not yet ready for inference  <a
          rel="nofollow" href="https://twitter.com/Tim_Dettmers/status/1661617478865395712?s=20">https://twitter.com/Tim_Dettmers/status/1661617478865395712?s=20</a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F4gDfJSO5Matg77q79d0q.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F4gDfJSO5Matg77q79d0q.png"></a></p>

          '
        raw: 'Yeah it''s out for training - but not yet ready for inference  https://twitter.com/Tim_Dettmers/status/1661617478865395712?s=20



          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F4gDfJSO5Matg77q79d0q.png)'
        updatedAt: '2023-05-25T07:08:39.272Z'
      numEdits: 1
      reactions: []
    id: 646f0965de1b1c1780a8f6c7
    type: comment
  author: TheBloke
  content: 'Yeah it''s out for training - but not yet ready for inference  https://twitter.com/Tim_Dettmers/status/1661617478865395712?s=20



    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/F4gDfJSO5Matg77q79d0q.png)'
  created_at: 2023-05-25 06:08:21+00:00
  edited: true
  hidden: false
  id: 646f0965de1b1c1780a8f6c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: TheBloke/stable-vicuna-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: HF/bitsandbytes load_in_4bit is now apparently live apparently! (in peft)
