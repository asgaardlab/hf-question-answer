!!python/object:huggingface_hub.community.DiscussionWithDetails
author: evilperson068
conflicting_files: null
created_at: 2023-07-29 12:29:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c576c21faad556d112daee522a2aa29e.svg
      fullname: Evil Person
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: evilperson068
      type: user
    createdAt: '2023-07-29T13:29:19.000Z'
    data:
      edited: false
      editors:
      - evilperson068
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5316191911697388
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c576c21faad556d112daee522a2aa29e.svg
          fullname: Evil Person
          isHf: false
          isPro: false
          name: evilperson068
          type: user
        html: "<pre><code>    layer_outputs = decoder_layer(\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1505, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1514, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1505, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1514, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n\
          \  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in forward\n    query_states = [F.linear(hidden_states, query_slices[i])\
          \ for i in range(self.pretraining_tp)]\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in &lt;listcomp&gt;\n    query_states = [F.linear(hidden_states,\
          \ query_slices[i]) for i in range(self.pretraining_tp)]\nRuntimeError: mat1\
          \ and mat2 shapes cannot be multiplied (25x5120 and 1x2560)\n</code></pre>\n\
          <p>Anyone please help with this issue? Thanks. </p>\n"
        raw: "```\r\n    layer_outputs = decoder_layer(\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1505, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1514, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1505, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1514, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/accelerate/hooks.py\"\
          , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
          \n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in forward\r\n    query_states = [F.linear(hidden_states, query_slices[i])\
          \ for i in range(self.pretraining_tp)]\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 295, in <listcomp>\r\n    query_states = [F.linear(hidden_states,\
          \ query_slices[i]) for i in range(self.pretraining_tp)]\r\nRuntimeError:\
          \ mat1 and mat2 shapes cannot be multiplied (25x5120 and 1x2560)\r\n```\r\
          \nAnyone please help with this issue? Thanks. "
        updatedAt: '2023-07-29T13:29:19.060Z'
      numEdits: 0
      reactions: []
    id: 64c5142ffafa16b5144d57ee
    type: comment
  author: evilperson068
  content: "```\r\n    layer_outputs = decoder_layer(\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1505, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1514, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 408, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1505, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1514, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/accelerate/hooks.py\"\
    , line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
    \ \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 295, in forward\r\n    query_states = [F.linear(hidden_states, query_slices[i])\
    \ for i in range(self.pretraining_tp)]\r\n  File \"/opt/miniconda3/envs/torch2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 295, in <listcomp>\r\n    query_states = [F.linear(hidden_states, query_slices[i])\
    \ for i in range(self.pretraining_tp)]\r\nRuntimeError: mat1 and mat2 shapes cannot\
    \ be multiplied (25x5120 and 1x2560)\r\n```\r\nAnyone please help with this issue?\
    \ Thanks. "
  created_at: 2023-07-29 12:29:19+00:00
  edited: false
  hidden: false
  id: 64c5142ffafa16b5144d57ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: YeungNLP/firefly-llama2-13b
repo_type: model
status: open
target_branch: null
title: Inference Failure
