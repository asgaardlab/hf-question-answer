!!python/object:huggingface_hub.community.DiscussionWithDetails
author: phosseini
conflicting_files: null
created_at: 2023-04-20 18:13:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639031241540-5e1cf9e5fcf41d740b69969f.png?w=200&h=200&f=face
      fullname: Pedram Hosseini
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phosseini
      type: user
    createdAt: '2023-04-20T19:13:40.000Z'
    data:
      edited: false
      editors:
      - phosseini
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1639031241540-5e1cf9e5fcf41d740b69969f.png?w=200&h=200&f=face
          fullname: Pedram Hosseini
          isHf: false
          isPro: false
          name: phosseini
          type: user
        html: "<p>Hi, I'm running the provided example in the model card and I'm getting\
          \ the following error:</p>\n<pre><code>---------------------------------------------------------------------------\n\
          IndexError                                Traceback (most recent call last)\n\
          &lt;ipython-input-8-5181f41be2fc&gt; in &lt;cell line: 1&gt;()\n----&gt;\
          \ 1 torch_outs = model(\n      2     tokens_ids,\n      3     attention_mask=attention_mask,\n\
          \      4     encoder_attention_mask=attention_mask,\n      5     output_hidden_states=True\n\
          \n8 frames\n/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\
          \ in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
          \ sparse)\n   2208         # remove once script supports set_grad_enabled\n\
          \   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n\
          -&gt; 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)\n   2211 \n   2212 \n\nIndexError: index out of range in self\n\
          </code></pre>\n<p>Looking at the model and the tokenizer's vocab sizes,\
          \ I see a mismatch. Could that be the problem or I'm missing something else?</p>\n\
          <pre><code>model.config.vocab_size\n&gt; 4105\n\ntokenizer.vocab_size\n\
          &gt; 4107\n</code></pre>\n"
        raw: "Hi, I'm running the provided example in the model card and I'm getting\
          \ the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
          \nIndexError                                Traceback (most recent call\
          \ last)\r\n<ipython-input-8-5181f41be2fc> in <cell line: 1>()\r\n----> 1\
          \ torch_outs = model(\r\n      2     tokens_ids,\r\n      3     attention_mask=attention_mask,\r\
          \n      4     encoder_attention_mask=attention_mask,\r\n      5     output_hidden_states=True\r\
          \n\r\n8 frames\r\n/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\
          \ in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
          \ sparse)\r\n   2208         # remove once script supports set_grad_enabled\r\
          \n   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\
          \n-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
          \ sparse)\r\n   2211 \r\n   2212 \r\n\r\nIndexError: index out of range\
          \ in self\r\n```\r\n\r\nLooking at the model and the tokenizer's vocab sizes,\
          \ I see a mismatch. Could that be the problem or I'm missing something else?\r\
          \n\r\n```\r\nmodel.config.vocab_size\r\n> 4105\r\n\r\ntokenizer.vocab_size\r\
          \n> 4107\r\n```"
        updatedAt: '2023-04-20T19:13:40.541Z'
      numEdits: 0
      reactions: []
    id: 64418ee4cea37249a0ff7008
    type: comment
  author: phosseini
  content: "Hi, I'm running the provided example in the model card and I'm getting\
    \ the following error:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\
    \nIndexError                                Traceback (most recent call last)\r\
    \n<ipython-input-8-5181f41be2fc> in <cell line: 1>()\r\n----> 1 torch_outs = model(\r\
    \n      2     tokens_ids,\r\n      3     attention_mask=attention_mask,\r\n  \
    \    4     encoder_attention_mask=attention_mask,\r\n      5     output_hidden_states=True\r\
    \n\r\n8 frames\r\n/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\
    \ in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq,\
    \ sparse)\r\n   2208         # remove once script supports set_grad_enabled\r\n\
    \   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\r\
    \n-> 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq,\
    \ sparse)\r\n   2211 \r\n   2212 \r\n\r\nIndexError: index out of range in self\r\
    \n```\r\n\r\nLooking at the model and the tokenizer's vocab sizes, I see a mismatch.\
    \ Could that be the problem or I'm missing something else?\r\n\r\n```\r\nmodel.config.vocab_size\r\
    \n> 4105\r\n\r\ntokenizer.vocab_size\r\n> 4107\r\n```"
  created_at: 2023-04-20 18:13:40+00:00
  edited: false
  hidden: false
  id: 64418ee4cea37249a0ff7008
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
      fullname: Matthew Carrigan
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rocketknight1
      type: user
    createdAt: '2023-04-21T15:40:43.000Z'
    data:
      edited: false
      editors:
      - Rocketknight1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
          fullname: Matthew Carrigan
          isHf: true
          isPro: false
          name: Rocketknight1
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;phosseini&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/phosseini\"\
          >@<span class=\"underline\">phosseini</span></a></span>\n\n\t</span></span>\
          \ - this error is fixed by a PR we pushed to <code>transformers</code>,\
          \ but which is unfortunately only available on <code>main</code> right now.\
          \ Please try installing from main with <code>pip install --upgrade git+https://github.com/huggingface/transformers.git</code>\
          \ and see if that fixes your issue!</p>\n"
        raw: Hi @phosseini - this error is fixed by a PR we pushed to `transformers`,
          but which is unfortunately only available on `main` right now. Please try
          installing from main with `pip install --upgrade git+https://github.com/huggingface/transformers.git`
          and see if that fixes your issue!
        updatedAt: '2023-04-21T15:40:43.172Z'
      numEdits: 0
      reactions: []
    id: 6442ae7bf8b647fa4f51b6df
    type: comment
  author: Rocketknight1
  content: Hi @phosseini - this error is fixed by a PR we pushed to `transformers`,
    but which is unfortunately only available on `main` right now. Please try installing
    from main with `pip install --upgrade git+https://github.com/huggingface/transformers.git`
    and see if that fixes your issue!
  created_at: 2023-04-21 14:40:43+00:00
  edited: false
  hidden: false
  id: 6442ae7bf8b647fa4f51b6df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435d96cf81a16e74365fcd2/W0NraBPfBj0_1MVFXZ-lV.jpeg?w=200&h=200&f=face
      fullname: krb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: esko2213
      type: user
    createdAt: '2023-05-18T23:32:39.000Z'
    data:
      edited: true
      editors:
      - esko2213
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435d96cf81a16e74365fcd2/W0NraBPfBj0_1MVFXZ-lV.jpeg?w=200&h=200&f=face
          fullname: krb
          isHf: false
          isPro: false
          name: esko2213
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Rocketknight1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Rocketknight1\"\
          >@<span class=\"underline\">Rocketknight1</span></a></span>\n\n\t</span></span>,</p>\n\
          <p>I've been tooling around trying to fine-tune this model to a classifier.\
          \ I was able to get the model through the trainer but when I go to run inference,\
          \ I am getting the same issue listed above.<br>The model was trained with\
          \ this config:</p>\n<pre><code class=\"language-_name_or_path:&quot;InstaDeepAI/nucleotide-transformer-500m-1000g&quot;\"\
          >architectures:\n0: \"EsmForSequenceClassification\"\nattention_probs_dropout_prob:0\n\
          emb_layer_norm_before:false\nesmfold_config:null\nhidden_dropout_prob:0\n\
          hidden_size:1280\nid2label:\n0:\"LABEL_0\"\n1:\"LABEL_1\"\n2:\"LABEL_2\"\
          \n3:\"LABEL_3\"\n4:\"LABEL_4\"\n5:\"LABEL_5\"\n6:\"LABEL_6\"\ninitializer_range:0.02\n\
          intermediate_size:5120\nis_folding_model:false\nlabel2id:\nLABEL_0:0\nLABEL_1:1\n\
          LABEL_2:2\nLABEL_3:3\nLABEL_4:4\nLABEL_5:5\nLABEL_6:6\nlayer_norm_eps:1e-12\n\
          mask_token_id:2\nmax_position_embeddings:1002\nmodel_type:\"esm\"\nnum_attention_heads:20\n\
          num_hidden_layers:24\npad_token_id:1\nposition_embedding_type:\"absolute\"\
          \nproblem_type:\"single_label_classification\"\ntie_word_embeddings:false\n\
          token_dropout:true\ntorch_dtype:\"float32\"\ntransformers_version:\"4.30.0.dev0\"\
          \nuse_cache:false\nvocab_list:null\nvocab_size:4105\n</code></pre>\n<p>I\
          \ am using the <code>InstaDeepAI/nucleotide-transformer-500m-1000g</code>\
          \ tokenizer and have the same transformer version <code>4.30.0.dev0</code>\
          \ loaded in my notebook which I can run the fill-mask model from the card\
          \ successfully. </p>\n<p>Any ideas would be appreciated.</p>\n"
        raw: "Hi @Rocketknight1,\n\nI've been tooling around trying to fine-tune this\
          \ model to a classifier. I was able to get the model through the trainer\
          \ but when I go to run inference, I am getting the same issue listed above.\
          \ \nThe model was trained with this config:\n```_name_or_path:\"InstaDeepAI/nucleotide-transformer-500m-1000g\"\
          \narchitectures:\n0: \"EsmForSequenceClassification\"\nattention_probs_dropout_prob:0\n\
          emb_layer_norm_before:false\nesmfold_config:null\nhidden_dropout_prob:0\n\
          hidden_size:1280\nid2label:\n0:\"LABEL_0\"\n1:\"LABEL_1\"\n2:\"LABEL_2\"\
          \n3:\"LABEL_3\"\n4:\"LABEL_4\"\n5:\"LABEL_5\"\n6:\"LABEL_6\"\ninitializer_range:0.02\n\
          intermediate_size:5120\nis_folding_model:false\nlabel2id:\nLABEL_0:0\nLABEL_1:1\n\
          LABEL_2:2\nLABEL_3:3\nLABEL_4:4\nLABEL_5:5\nLABEL_6:6\nlayer_norm_eps:1e-12\n\
          mask_token_id:2\nmax_position_embeddings:1002\nmodel_type:\"esm\"\nnum_attention_heads:20\n\
          num_hidden_layers:24\npad_token_id:1\nposition_embedding_type:\"absolute\"\
          \nproblem_type:\"single_label_classification\"\ntie_word_embeddings:false\n\
          token_dropout:true\ntorch_dtype:\"float32\"\ntransformers_version:\"4.30.0.dev0\"\
          \nuse_cache:false\nvocab_list:null\nvocab_size:4105\n\n```\n\nI am using\
          \ the `InstaDeepAI/nucleotide-transformer-500m-1000g` tokenizer and have\
          \ the same transformer version `4.30.0.dev0` loaded in my notebook which\
          \ I can run the fill-mask model from the card successfully. \n\nAny ideas\
          \ would be appreciated."
        updatedAt: '2023-05-18T23:35:23.765Z'
      numEdits: 2
      reactions: []
    id: 6466b597310be9cb7645e752
    type: comment
  author: esko2213
  content: "Hi @Rocketknight1,\n\nI've been tooling around trying to fine-tune this\
    \ model to a classifier. I was able to get the model through the trainer but when\
    \ I go to run inference, I am getting the same issue listed above. \nThe model\
    \ was trained with this config:\n```_name_or_path:\"InstaDeepAI/nucleotide-transformer-500m-1000g\"\
    \narchitectures:\n0: \"EsmForSequenceClassification\"\nattention_probs_dropout_prob:0\n\
    emb_layer_norm_before:false\nesmfold_config:null\nhidden_dropout_prob:0\nhidden_size:1280\n\
    id2label:\n0:\"LABEL_0\"\n1:\"LABEL_1\"\n2:\"LABEL_2\"\n3:\"LABEL_3\"\n4:\"LABEL_4\"\
    \n5:\"LABEL_5\"\n6:\"LABEL_6\"\ninitializer_range:0.02\nintermediate_size:5120\n\
    is_folding_model:false\nlabel2id:\nLABEL_0:0\nLABEL_1:1\nLABEL_2:2\nLABEL_3:3\n\
    LABEL_4:4\nLABEL_5:5\nLABEL_6:6\nlayer_norm_eps:1e-12\nmask_token_id:2\nmax_position_embeddings:1002\n\
    model_type:\"esm\"\nnum_attention_heads:20\nnum_hidden_layers:24\npad_token_id:1\n\
    position_embedding_type:\"absolute\"\nproblem_type:\"single_label_classification\"\
    \ntie_word_embeddings:false\ntoken_dropout:true\ntorch_dtype:\"float32\"\ntransformers_version:\"\
    4.30.0.dev0\"\nuse_cache:false\nvocab_list:null\nvocab_size:4105\n\n```\n\nI am\
    \ using the `InstaDeepAI/nucleotide-transformer-500m-1000g` tokenizer and have\
    \ the same transformer version `4.30.0.dev0` loaded in my notebook which I can\
    \ run the fill-mask model from the card successfully. \n\nAny ideas would be appreciated."
  created_at: 2023-05-18 22:32:39+00:00
  edited: true
  hidden: false
  id: 6466b597310be9cb7645e752
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
      fullname: Matthew Carrigan
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rocketknight1
      type: user
    createdAt: '2023-05-19T14:47:55.000Z'
    data:
      edited: false
      editors:
      - Rocketknight1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
          fullname: Matthew Carrigan
          isHf: true
          isPro: false
          name: Rocketknight1
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;esko2213&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/esko2213\"\
          >@<span class=\"underline\">esko2213</span></a></span>\n\n\t</span></span>,\
          \ can you send me some code to reproduce the issue?</p>\n"
        raw: Hi @esko2213, can you send me some code to reproduce the issue?
        updatedAt: '2023-05-19T14:47:55.204Z'
      numEdits: 0
      reactions: []
    id: 64678c1bab75d9cb3c485c65
    type: comment
  author: Rocketknight1
  content: Hi @esko2213, can you send me some code to reproduce the issue?
  created_at: 2023-05-19 13:47:55+00:00
  edited: false
  hidden: false
  id: 64678c1bab75d9cb3c485c65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435d96cf81a16e74365fcd2/W0NraBPfBj0_1MVFXZ-lV.jpeg?w=200&h=200&f=face
      fullname: krb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: esko2213
      type: user
    createdAt: '2023-05-20T00:39:21.000Z'
    data:
      edited: true
      editors:
      - esko2213
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6435d96cf81a16e74365fcd2/W0NraBPfBj0_1MVFXZ-lV.jpeg?w=200&h=200&f=face
          fullname: krb
          isHf: false
          isPro: false
          name: esko2213
          type: user
        html: "<p>Thanks for the quick response.</p>\n<p>I had been deploying the\
          \ the model through SageMaker and calling it as such....</p>\n<pre><code>predictor\
          \ = huggingface_estimator.deploy(1, \"ml.m5.xlarge\")\ninput_sequence= {\"\
          inputs\":\"CAGCATTTTGAATTTGAATACCAGACCAAAGTGGATGGTGAGATAATCCTTCATCTTTATGACAAAGGAGGAATTGAGCAAACAATTTGTATGTTGGATGGTGTGTTTGCATTTGTTTTACTGGATACTGCCAATAAGAAAGTGTTCCTGGGTAGAGATACATATGGAGTCAGACCTTTGTTTAAAGCAATGACAGAAGATGGATTTTTGGCTGTATGTTCAGAAGCTAAAGGTCTTGTTACATTGAAGCACTCCGCGACTCCCTTTTTAAAAGTGGAGCCTTTTCTTCCTGGACACTATGAAGTTTTGGATTTAAAGCCAAATGGCAAAGTTGCATCCGTGGAAATGGTTAAATATCATCACTGTCGGGATGTACCCCTGCACGCCCTCTATGACAATGTGGAGAAACTCTTTCCAGGTTTTGAGATAGAAACTGTGAAGAACAACCTCAGGATCCTTTTTAATAATGCTGTAAAGAAACGTTTGATGACAGACAGAAGGATTGGCTGCCTTTTATCAGGGGGCTTGGACTCCAGCTTGGTTGCTGCCACTCTGTTGAAGCAGCTGAAAGAAGCCCAAGTACAGTATCCTCTCCAGACATTTGCAATTGGCATGGAAGACAGCCCCGATTTACTGGCTGCTAGAAAGGTGGCAGATCATATTGGAAGTGAACATTATGAAGTCCTTTTTAACTCTGAGGAAGGCATTCAGGCTCTGGATGAAGTCATATTTTCCTTGGAAACTTATGACATTACAACAGTTCGTGCTTCAGTAGGTATGTATTTAATTTCCAAGTATATTCGGAAGAACACAGATAGCGTGGTGATCTTCTCTGGAGAAGGATCAGATGAACTTACGCAGGGTTACATATATTTTCACAAGGCTCCTTCTCCTGAAAAAGCCGAGGAGGAGAGTGAGAGGCTTCTGAGGGAACTCTATTTGTTTGATGTTCTCCGCGCAGATCGAACTACTGCTGCCCATGGTCTTGAACTGAGAGTCCCATTTCTAGATCATCGATTTTCTTCCTATTACTTGTCTCTGCCACCAGAAATGAGAATTCCAAAGAATGGGATAGAAAAACATCTCCTGAGAGAGACGTTTGAGGATTCCAATCTGATACCCAAAGAGATTCTCTGGCGACCAAAAGAAGCCTTCAGTGATGGAATAACTTCAGTTAAGAATTCCTGGTTTAAGATTTTACAGGAATACGTTGAACATCAGGTTGATGATGCAATGATGGCAAATGCAGCCCAGAAATTTCCCTTCAATACTCCTAAAACCAAAGAAGGATATTACTACCGTCAAGTCTTTGAACGCCATTACCCAGGCCGGGCTGACTGGCTGAGCCATTACTGGATGCCCAAGTGGATCAATGCCACTGACCCTTCTGCCCGCACGCTGACCCACTACAAGTCAGCTGTCAAAGCTTAG\"\
          }\npredictor.predict(input_sequence)\n\nIndexError: index out of range in\
          \ self\n</code></pre>\n<p>which produced the same error reported above.\
          \ I've tried it with a single string and an array of strings as input, both\
          \ throw the error.  </p>\n<p>I have not pushed it to the Hub yet but wanted\
          \ to see what happens if I run it locally first...</p>\n<p>Running it locally\
          \ via pipeline - </p>\n<pre><code>\nfrom transformers import pipeline\n\
          pipeline = pipeline(task=\"text-classification\", model=\"./nucl_class_model/\"\
          )\nfor x in pipeline(\"CAGCATTTTGAATTTGAATACCAGACCAAAGTGGATGGTGAGATAATCCTTCATCTTTATGACAAAGGAGGAATTGAGCAAACAATTTGTATGTTGGATGGTGTGTTTGCATTTGTTTTACTGGATACTGCCAATAAGAAAGTGTTCCTGGGTAGAGATACATATGGAGTCAGACCTTTGTTTAAAGCAATGACAGAAGATGGATTTTTGGCTGTATGTTCAGAAGCTAAAGGTCTTGTTACATTGAAGCACTCCGCGACTCCCTTTTTAAAAGTGGAGCCTTTTCTTCCTGGACACTATGAAGTTTTGGATTTAAAGCCAAATGGCAAAGTTGCATCCGTGGAAATGGTTAAA\"\
          ):\n    print(x)\n\n\nIndexError: index out of range in self\n</code></pre>\n\
          <p>However, when I run it via AutoModel and remove the encoder_attention_mask</p>\n\
          <pre><code># Import the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(\"\
          InstaDeepAI/nucleotide-transformer-500m-1000g\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
          ./nucl_class_model/\", local_files_only=True, )\n\nsequences = ['ATGCCCCAACTAAATACTACCGTATGGCCCACCATAATTACCCCCATACTCCTTACACTATTCCTCATCACCCAACTAAAAATATTAAACACAAACTACCACCTACCTCCCTCACCAAAGCCCATAAAAATAAAAAATTATAACAAACCCTGAGAACCAAAATGAACGAAAATCTGTTCGCTTCATTCATTGCCCCCACAATCCTAG']\n\
          tokens_ids = tokenizer.batch_encode_plus(sequences, return_tensors=\"pt\"\
          )[\"input_ids\"]\nattention_mask = tokens_ids != tokenizer.mask_token_id\n\
          \ntorch_outs = model(\n    tokens_ids,\n    attention_mask=attention_mask,\n\
          \    #encoder_attention_mask=attention_mask,\n    output_hidden_states=True\n\
          )\n\nprobs = torch.softmax(torch_outs.logits, dim=1)\n</code></pre>\n<p>I\
          \ can get probabilities </p>\n<pre><code>tensor([[2.8926e-04, 2.9352e-04,\
          \ 3.5966e-04, 1.9120e-04, 1.0060e-03, 4.8940e-05,\n         9.9781e-01]],\
          \ grad_fn=&lt;SoftmaxBackward0&gt;)\n</code></pre>\n<p>Let me know if you\
          \ want me to push up the model.</p>\n"
        raw: "Thanks for the quick response.\n\nI had been deploying the the model\
          \ through SageMaker and calling it as such....\n\n```\npredictor = huggingface_estimator.deploy(1,\
          \ \"ml.m5.xlarge\")\ninput_sequence= {\"inputs\":\"CAGCATTTTGAATTTGAATACCAGACCAAAGTGGATGGTGAGATAATCCTTCATCTTTATGACAAAGGAGGAATTGAGCAAACAATTTGTATGTTGGATGGTGTGTTTGCATTTGTTTTACTGGATACTGCCAATAAGAAAGTGTTCCTGGGTAGAGATACATATGGAGTCAGACCTTTGTTTAAAGCAATGACAGAAGATGGATTTTTGGCTGTATGTTCAGAAGCTAAAGGTCTTGTTACATTGAAGCACTCCGCGACTCCCTTTTTAAAAGTGGAGCCTTTTCTTCCTGGACACTATGAAGTTTTGGATTTAAAGCCAAATGGCAAAGTTGCATCCGTGGAAATGGTTAAATATCATCACTGTCGGGATGTACCCCTGCACGCCCTCTATGACAATGTGGAGAAACTCTTTCCAGGTTTTGAGATAGAAACTGTGAAGAACAACCTCAGGATCCTTTTTAATAATGCTGTAAAGAAACGTTTGATGACAGACAGAAGGATTGGCTGCCTTTTATCAGGGGGCTTGGACTCCAGCTTGGTTGCTGCCACTCTGTTGAAGCAGCTGAAAGAAGCCCAAGTACAGTATCCTCTCCAGACATTTGCAATTGGCATGGAAGACAGCCCCGATTTACTGGCTGCTAGAAAGGTGGCAGATCATATTGGAAGTGAACATTATGAAGTCCTTTTTAACTCTGAGGAAGGCATTCAGGCTCTGGATGAAGTCATATTTTCCTTGGAAACTTATGACATTACAACAGTTCGTGCTTCAGTAGGTATGTATTTAATTTCCAAGTATATTCGGAAGAACACAGATAGCGTGGTGATCTTCTCTGGAGAAGGATCAGATGAACTTACGCAGGGTTACATATATTTTCACAAGGCTCCTTCTCCTGAAAAAGCCGAGGAGGAGAGTGAGAGGCTTCTGAGGGAACTCTATTTGTTTGATGTTCTCCGCGCAGATCGAACTACTGCTGCCCATGGTCTTGAACTGAGAGTCCCATTTCTAGATCATCGATTTTCTTCCTATTACTTGTCTCTGCCACCAGAAATGAGAATTCCAAAGAATGGGATAGAAAAACATCTCCTGAGAGAGACGTTTGAGGATTCCAATCTGATACCCAAAGAGATTCTCTGGCGACCAAAAGAAGCCTTCAGTGATGGAATAACTTCAGTTAAGAATTCCTGGTTTAAGATTTTACAGGAATACGTTGAACATCAGGTTGATGATGCAATGATGGCAAATGCAGCCCAGAAATTTCCCTTCAATACTCCTAAAACCAAAGAAGGATATTACTACCGTCAAGTCTTTGAACGCCATTACCCAGGCCGGGCTGACTGGCTGAGCCATTACTGGATGCCCAAGTGGATCAATGCCACTGACCCTTCTGCCCGCACGCTGACCCACTACAAGTCAGCTGTCAAAGCTTAG\"\
          }\npredictor.predict(input_sequence)\n\nIndexError: index out of range in\
          \ self\n```\n\nwhich produced the same error reported above. I've tried\
          \ it with a single string and an array of strings as input, both throw the\
          \ error.  \n\n\n\nI have not pushed it to the Hub yet but wanted to see\
          \ what happens if I run it locally first...\n\nRunning it locally via pipeline\
          \ - \n\n```\n\nfrom transformers import pipeline\npipeline = pipeline(task=\"\
          text-classification\", model=\"./nucl_class_model/\")\nfor x in pipeline(\"\
          CAGCATTTTGAATTTGAATACCAGACCAAAGTGGATGGTGAGATAATCCTTCATCTTTATGACAAAGGAGGAATTGAGCAAACAATTTGTATGTTGGATGGTGTGTTTGCATTTGTTTTACTGGATACTGCCAATAAGAAAGTGTTCCTGGGTAGAGATACATATGGAGTCAGACCTTTGTTTAAAGCAATGACAGAAGATGGATTTTTGGCTGTATGTTCAGAAGCTAAAGGTCTTGTTACATTGAAGCACTCCGCGACTCCCTTTTTAAAAGTGGAGCCTTTTCTTCCTGGACACTATGAAGTTTTGGATTTAAAGCCAAATGGCAAAGTTGCATCCGTGGAAATGGTTAAA\"\
          ):\n    print(x)\n\n\nIndexError: index out of range in self\n```\n\n\n\
          However, when I run it via AutoModel and remove the encoder_attention_mask\n\
          \n\n```\n# Import the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(\"\
          InstaDeepAI/nucleotide-transformer-500m-1000g\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
          ./nucl_class_model/\", local_files_only=True, )\n\nsequences = ['ATGCCCCAACTAAATACTACCGTATGGCCCACCATAATTACCCCCATACTCCTTACACTATTCCTCATCACCCAACTAAAAATATTAAACACAAACTACCACCTACCTCCCTCACCAAAGCCCATAAAAATAAAAAATTATAACAAACCCTGAGAACCAAAATGAACGAAAATCTGTTCGCTTCATTCATTGCCCCCACAATCCTAG']\n\
          tokens_ids = tokenizer.batch_encode_plus(sequences, return_tensors=\"pt\"\
          )[\"input_ids\"]\nattention_mask = tokens_ids != tokenizer.mask_token_id\n\
          \ntorch_outs = model(\n    tokens_ids,\n    attention_mask=attention_mask,\n\
          \    #encoder_attention_mask=attention_mask,\n    output_hidden_states=True\n\
          )\n\nprobs = torch.softmax(torch_outs.logits, dim=1)\n\n```\n\nI can get\
          \ probabilities \n\n```\ntensor([[2.8926e-04, 2.9352e-04, 3.5966e-04, 1.9120e-04,\
          \ 1.0060e-03, 4.8940e-05,\n         9.9781e-01]], grad_fn=<SoftmaxBackward0>)\n\
          \n```\n\nLet me know if you want me to push up the model."
        updatedAt: '2023-05-20T00:41:29.123Z'
      numEdits: 1
      reactions: []
    id: 646816b93a7c8dda230e33ec
    type: comment
  author: esko2213
  content: "Thanks for the quick response.\n\nI had been deploying the the model through\
    \ SageMaker and calling it as such....\n\n```\npredictor = huggingface_estimator.deploy(1,\
    \ \"ml.m5.xlarge\")\ninput_sequence= {\"inputs\":\"CAGCATTTTGAATTTGAATACCAGACCAAAGTGGATGGTGAGATAATCCTTCATCTTTATGACAAAGGAGGAATTGAGCAAACAATTTGTATGTTGGATGGTGTGTTTGCATTTGTTTTACTGGATACTGCCAATAAGAAAGTGTTCCTGGGTAGAGATACATATGGAGTCAGACCTTTGTTTAAAGCAATGACAGAAGATGGATTTTTGGCTGTATGTTCAGAAGCTAAAGGTCTTGTTACATTGAAGCACTCCGCGACTCCCTTTTTAAAAGTGGAGCCTTTTCTTCCTGGACACTATGAAGTTTTGGATTTAAAGCCAAATGGCAAAGTTGCATCCGTGGAAATGGTTAAATATCATCACTGTCGGGATGTACCCCTGCACGCCCTCTATGACAATGTGGAGAAACTCTTTCCAGGTTTTGAGATAGAAACTGTGAAGAACAACCTCAGGATCCTTTTTAATAATGCTGTAAAGAAACGTTTGATGACAGACAGAAGGATTGGCTGCCTTTTATCAGGGGGCTTGGACTCCAGCTTGGTTGCTGCCACTCTGTTGAAGCAGCTGAAAGAAGCCCAAGTACAGTATCCTCTCCAGACATTTGCAATTGGCATGGAAGACAGCCCCGATTTACTGGCTGCTAGAAAGGTGGCAGATCATATTGGAAGTGAACATTATGAAGTCCTTTTTAACTCTGAGGAAGGCATTCAGGCTCTGGATGAAGTCATATTTTCCTTGGAAACTTATGACATTACAACAGTTCGTGCTTCAGTAGGTATGTATTTAATTTCCAAGTATATTCGGAAGAACACAGATAGCGTGGTGATCTTCTCTGGAGAAGGATCAGATGAACTTACGCAGGGTTACATATATTTTCACAAGGCTCCTTCTCCTGAAAAAGCCGAGGAGGAGAGTGAGAGGCTTCTGAGGGAACTCTATTTGTTTGATGTTCTCCGCGCAGATCGAACTACTGCTGCCCATGGTCTTGAACTGAGAGTCCCATTTCTAGATCATCGATTTTCTTCCTATTACTTGTCTCTGCCACCAGAAATGAGAATTCCAAAGAATGGGATAGAAAAACATCTCCTGAGAGAGACGTTTGAGGATTCCAATCTGATACCCAAAGAGATTCTCTGGCGACCAAAAGAAGCCTTCAGTGATGGAATAACTTCAGTTAAGAATTCCTGGTTTAAGATTTTACAGGAATACGTTGAACATCAGGTTGATGATGCAATGATGGCAAATGCAGCCCAGAAATTTCCCTTCAATACTCCTAAAACCAAAGAAGGATATTACTACCGTCAAGTCTTTGAACGCCATTACCCAGGCCGGGCTGACTGGCTGAGCCATTACTGGATGCCCAAGTGGATCAATGCCACTGACCCTTCTGCCCGCACGCTGACCCACTACAAGTCAGCTGTCAAAGCTTAG\"\
    }\npredictor.predict(input_sequence)\n\nIndexError: index out of range in self\n\
    ```\n\nwhich produced the same error reported above. I've tried it with a single\
    \ string and an array of strings as input, both throw the error.  \n\n\n\nI have\
    \ not pushed it to the Hub yet but wanted to see what happens if I run it locally\
    \ first...\n\nRunning it locally via pipeline - \n\n```\n\nfrom transformers import\
    \ pipeline\npipeline = pipeline(task=\"text-classification\", model=\"./nucl_class_model/\"\
    )\nfor x in pipeline(\"CAGCATTTTGAATTTGAATACCAGACCAAAGTGGATGGTGAGATAATCCTTCATCTTTATGACAAAGGAGGAATTGAGCAAACAATTTGTATGTTGGATGGTGTGTTTGCATTTGTTTTACTGGATACTGCCAATAAGAAAGTGTTCCTGGGTAGAGATACATATGGAGTCAGACCTTTGTTTAAAGCAATGACAGAAGATGGATTTTTGGCTGTATGTTCAGAAGCTAAAGGTCTTGTTACATTGAAGCACTCCGCGACTCCCTTTTTAAAAGTGGAGCCTTTTCTTCCTGGACACTATGAAGTTTTGGATTTAAAGCCAAATGGCAAAGTTGCATCCGTGGAAATGGTTAAA\"\
    ):\n    print(x)\n\n\nIndexError: index out of range in self\n```\n\n\nHowever,\
    \ when I run it via AutoModel and remove the encoder_attention_mask\n\n\n```\n\
    # Import the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(\"\
    InstaDeepAI/nucleotide-transformer-500m-1000g\")\nmodel = AutoModelForSequenceClassification.from_pretrained(\"\
    ./nucl_class_model/\", local_files_only=True, )\n\nsequences = ['ATGCCCCAACTAAATACTACCGTATGGCCCACCATAATTACCCCCATACTCCTTACACTATTCCTCATCACCCAACTAAAAATATTAAACACAAACTACCACCTACCTCCCTCACCAAAGCCCATAAAAATAAAAAATTATAACAAACCCTGAGAACCAAAATGAACGAAAATCTGTTCGCTTCATTCATTGCCCCCACAATCCTAG']\n\
    tokens_ids = tokenizer.batch_encode_plus(sequences, return_tensors=\"pt\")[\"\
    input_ids\"]\nattention_mask = tokens_ids != tokenizer.mask_token_id\n\ntorch_outs\
    \ = model(\n    tokens_ids,\n    attention_mask=attention_mask,\n    #encoder_attention_mask=attention_mask,\n\
    \    output_hidden_states=True\n)\n\nprobs = torch.softmax(torch_outs.logits,\
    \ dim=1)\n\n```\n\nI can get probabilities \n\n```\ntensor([[2.8926e-04, 2.9352e-04,\
    \ 3.5966e-04, 1.9120e-04, 1.0060e-03, 4.8940e-05,\n         9.9781e-01]], grad_fn=<SoftmaxBackward0>)\n\
    \n```\n\nLet me know if you want me to push up the model."
  created_at: 2023-05-19 23:39:21+00:00
  edited: true
  hidden: false
  id: 646816b93a7c8dda230e33ec
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: InstaDeepAI/nucleotide-transformer-500m-1000g
repo_type: model
status: open
target_branch: null
title: 'IndexError: index out of range in self'
