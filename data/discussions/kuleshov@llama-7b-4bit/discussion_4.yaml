!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rrkotik
conflicting_files: null
created_at: 2023-05-03 16:36:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ffd61e1d205245ace313f779abae1456.svg
      fullname: RR Kotik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rrkotik
      type: user
    createdAt: '2023-05-03T17:36:25.000Z'
    data:
      edited: false
      editors:
      - rrkotik
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ffd61e1d205245ace313f779abae1456.svg
          fullname: RR Kotik
          isHf: false
          isPro: false
          name: rrkotik
          type: user
        html: '<p>Hello, can you provide how to run inference for it? </p>

          <p>i tried something like this:</p>

          <p><code>model = transformers.LlamaForCausalLM.from_pretrained("kuleshov/llama-7b-4bit",
          load_in_8bit=True, device_map=''auto'')</code></p>

          <p>I receive error:</p>

          <pre><code>ValueError: weight is on the meta device, we need a `value` to
          put in on 0.

          </code></pre>

          '
        raw: "Hello, can you provide how to run inference for it? \r\n\r\ni tried\
          \ something like this:\r\n\r\n```model = transformers.LlamaForCausalLM.from_pretrained(\"\
          kuleshov/llama-7b-4bit\", load_in_8bit=True, device_map='auto')```\r\n\r\
          \nI receive error:\r\n\r\n```\r\nValueError: weight is on the meta device,\
          \ we need a `value` to put in on 0.\r\n```\r\n\r\n"
        updatedAt: '2023-05-03T17:36:25.012Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Kofc
    id: 64529b99515bd8f741785514
    type: comment
  author: rrkotik
  content: "Hello, can you provide how to run inference for it? \r\n\r\ni tried something\
    \ like this:\r\n\r\n```model = transformers.LlamaForCausalLM.from_pretrained(\"\
    kuleshov/llama-7b-4bit\", load_in_8bit=True, device_map='auto')```\r\n\r\nI receive\
    \ error:\r\n\r\n```\r\nValueError: weight is on the meta device, we need a `value`\
    \ to put in on 0.\r\n```\r\n\r\n"
  created_at: 2023-05-03 16:36:25+00:00
  edited: false
  hidden: false
  id: 64529b99515bd8f741785514
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1bd626f14ebe6d5573f1527df524247.svg
      fullname: zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zongqiang
      type: user
    createdAt: '2023-06-08T10:54:33.000Z'
    data:
      edited: false
      editors:
      - zongqiang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9664174318313599
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1bd626f14ebe6d5573f1527df524247.svg
          fullname: zhang
          isHf: false
          isPro: false
          name: zongqiang
          type: user
        html: '<p>The lines of code run well in the env,  But I am confused that its
          gpu memory usage is about to 8GB, which is the same with llama-7b-int8 model.</p>

          '
        raw: The lines of code run well in the env,  But I am confused that its gpu
          memory usage is about to 8GB, which is the same with llama-7b-int8 model.
        updatedAt: '2023-06-08T10:54:33.100Z'
      numEdits: 0
      reactions: []
    id: 6481b3696f283a746863b13a
    type: comment
  author: zongqiang
  content: The lines of code run well in the env,  But I am confused that its gpu
    memory usage is about to 8GB, which is the same with llama-7b-int8 model.
  created_at: 2023-06-08 09:54:33+00:00
  edited: false
  hidden: false
  id: 6481b3696f283a746863b13a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: kuleshov/llama-7b-4bit
repo_type: model
status: open
target_branch: null
title: inference example
