!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zokica
conflicting_files: null
created_at: 2023-04-17 04:20:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-04-17T05:20:03.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>Hello,<br> Thanks for the model.</p>

          <p>Do you think it is worth to finetune this version (just 4bit), and is
          there any example in notebook how to do so,  even iI think it is pretty
          similar to GPT2 code for fine tuning.</p>

          <p>I guess it is much better to finetune model in fp16 and then quantize
          it.</p>

          <p>Thanks</p>

          '
        raw: "Hello,\n Thanks for the model.\n\nDo you think it is worth to finetune\
          \ this version (just 4bit), and is there any example in notebook how to\
          \ do so,  even iI think it is pretty similar to GPT2 code for fine tuning.\n\
          \nI guess it is much better to finetune model in fp16 and then quantize\
          \ it.\n\nThanks"
        updatedAt: '2023-04-17T05:21:17.133Z'
      numEdits: 1
      reactions: []
    id: 643cd7036eeb746f5ad90e11
    type: comment
  author: zokica
  content: "Hello,\n Thanks for the model.\n\nDo you think it is worth to finetune\
    \ this version (just 4bit), and is there any example in notebook how to do so,\
    \  even iI think it is pretty similar to GPT2 code for fine tuning.\n\nI guess\
    \ it is much better to finetune model in fp16 and then quantize it.\n\nThanks"
  created_at: 2023-04-17 04:20:03+00:00
  edited: true
  hidden: false
  id: 643cd7036eeb746f5ad90e11
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: kuleshov/llama-7b-4bit
repo_type: model
status: open
target_branch: null
title: Finetunig the model
