!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krokoko
conflicting_files: null
created_at: 2023-05-16 16:46:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e7c13d6715be3c13d020c457a8eb2392.svg
      fullname: krokoko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krokoko
      type: user
    createdAt: '2023-05-16T17:46:54.000Z'
    data:
      edited: false
      editors:
      - krokoko
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e7c13d6715be3c13d020c457a8eb2392.svg
          fullname: krokoko
          isHf: false
          isPro: false
          name: krokoko
          type: user
        html: "<p>Hi, I'm trying to deploy the model to a SageMaker endpoint using\
          \ the <a rel=\"nofollow\" href=\"https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html\"\
          >SDK</a>. I extended the latest available hugging face DLC to install the\
          \ correct version of the transformers library (4.28.0). I'm deploying the\
          \ model with:</p>\n<pre><code>hub = {\n    'HF_MODEL_ID':'nomic-ai/gpt4all-13b-snoozy',\n\
          \    'HF_TASK':'text-generation'\n}\n\n# create Hugging Face Model Class\n\
          huggingface_model_snoozy = HuggingFaceModel(\n    image_uri=ecr_image,\n\
          \    transformers_version='4.28.0',\n    pytorch_version='1.13.1',\n   \
          \ py_version='py39',\n    env=hub,\n    role=role, \n)\n</code></pre>\n\
          <p>When running a prediction, I get:</p>\n<pre><code>ModelError: An error\
          \ occurred (ModelError) when calling the InvokeEndpoint operation: Received\
          \ client error (400) from primary with message \"{\n  \"code\": 400,\n \
          \ \"type\": \"InternalServerException\",\n  \"message\": \"Could not load\
          \ model /.sagemaker/mms/models/nomic-ai__gpt4all-13b-snoozy with any of\
          \ the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.llama.modeling_llama.LlamaForCausalLM\\\
          u0027\\u003e).\"\n}\n</code></pre>\n<p>Any idea what could be happening\
          \ ? Thanks !</p>\n"
        raw: "Hi, I'm trying to deploy the model to a SageMaker endpoint using the\
          \ [SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html).\
          \ I extended the latest available hugging face DLC to install the correct\
          \ version of the transformers library (4.28.0). I'm deploying the model\
          \ with:\r\n\r\n```\r\nhub = {\r\n\t'HF_MODEL_ID':'nomic-ai/gpt4all-13b-snoozy',\r\
          \n\t'HF_TASK':'text-generation'\r\n}\r\n\r\n# create Hugging Face Model\
          \ Class\r\nhuggingface_model_snoozy = HuggingFaceModel(\r\n    image_uri=ecr_image,\r\
          \n\ttransformers_version='4.28.0',\r\n\tpytorch_version='1.13.1',\r\n\t\
          py_version='py39',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\n\r\nWhen\
          \ running a prediction, I get:\r\n\r\n```\r\nModelError: An error occurred\
          \ (ModelError) when calling the InvokeEndpoint operation: Received client\
          \ error (400) from primary with message \"{\r\n  \"code\": 400,\r\n  \"\
          type\": \"InternalServerException\",\r\n  \"message\": \"Could not load\
          \ model /.sagemaker/mms/models/nomic-ai__gpt4all-13b-snoozy with any of\
          \ the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.llama.modeling_llama.LlamaForCausalLM\\\
          u0027\\u003e).\"\r\n}\r\n```\r\n\r\nAny idea what could be happening ? Thanks\
          \ !"
        updatedAt: '2023-05-16T17:46:54.858Z'
      numEdits: 0
      reactions: []
    id: 6463c18e7572c66a8e62a8c8
    type: comment
  author: krokoko
  content: "Hi, I'm trying to deploy the model to a SageMaker endpoint using the [SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/index.html).\
    \ I extended the latest available hugging face DLC to install the correct version\
    \ of the transformers library (4.28.0). I'm deploying the model with:\r\n\r\n\
    ```\r\nhub = {\r\n\t'HF_MODEL_ID':'nomic-ai/gpt4all-13b-snoozy',\r\n\t'HF_TASK':'text-generation'\r\
    \n}\r\n\r\n# create Hugging Face Model Class\r\nhuggingface_model_snoozy = HuggingFaceModel(\r\
    \n    image_uri=ecr_image,\r\n\ttransformers_version='4.28.0',\r\n\tpytorch_version='1.13.1',\r\
    \n\tpy_version='py39',\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n```\r\n\r\nWhen\
    \ running a prediction, I get:\r\n\r\n```\r\nModelError: An error occurred (ModelError)\
    \ when calling the InvokeEndpoint operation: Received client error (400) from\
    \ primary with message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
    ,\r\n  \"message\": \"Could not load model /.sagemaker/mms/models/nomic-ai__gpt4all-13b-snoozy\
    \ with any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
    u0027\\u003e, \\u003cclass \\u0027transformers.models.llama.modeling_llama.LlamaForCausalLM\\\
    u0027\\u003e).\"\r\n}\r\n```\r\n\r\nAny idea what could be happening ? Thanks\
    \ !"
  created_at: 2023-05-16 16:46:54+00:00
  edited: false
  hidden: false
  id: 6463c18e7572c66a8e62a8c8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: nomic-ai/gpt4all-13b-snoozy
repo_type: model
status: open
target_branch: null
title: Runtime issue when deploying on a SageMaker endpoint
