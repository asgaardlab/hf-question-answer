!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBloke
conflicting_files: null
created_at: 2023-05-05 13:56:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-05T14:56:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve done a 4bit GPTQ conversion of this model, which is available
          here:  <a href="https://huggingface.co/TheBloke/GPT4All-13B-snoozy-GPTQ">https://huggingface.co/TheBloke/GPT4All-13B-snoozy-GPTQ</a></p>

          '
        raw: 'I''ve done a 4bit GPTQ conversion of this model, which is available
          here:  https://huggingface.co/TheBloke/GPT4All-13B-snoozy-GPTQ'
        updatedAt: '2023-05-05T14:56:10.616Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - zpn
        - Sab1
      - count: 2
        reaction: "\U0001F44D"
        users:
        - dzupin
        - jgm3109
    id: 6455190a02912fad3f229228
    type: comment
  author: TheBloke
  content: 'I''ve done a 4bit GPTQ conversion of this model, which is available here:  https://huggingface.co/TheBloke/GPT4All-13B-snoozy-GPTQ'
  created_at: 2023-05-05 13:56:10+00:00
  edited: false
  hidden: false
  id: 6455190a02912fad3f229228
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/188067aa1e3f8e69fe252a0346209f28.svg
      fullname: Zach Nussbaum
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: zpn
      type: user
    createdAt: '2023-05-05T16:45:40.000Z'
    data:
      edited: false
      editors:
      - zpn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/188067aa1e3f8e69fe252a0346209f28.svg
          fullname: Zach Nussbaum
          isHf: false
          isPro: false
          name: zpn
          type: user
        html: '<p>awesome thanks so much!!</p>

          '
        raw: awesome thanks so much!!
        updatedAt: '2023-05-05T16:45:40.669Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 645532b4a473375be573f5de
    type: comment
  author: zpn
  content: awesome thanks so much!!
  created_at: 2023-05-05 15:45:40+00:00
  edited: false
  hidden: false
  id: 645532b4a473375be573f5de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663879535365-noauth.png?w=200&h=200&f=face
      fullname: Carl Moebsi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: moebis
      type: user
    createdAt: '2023-06-21T12:45:08.000Z'
    data:
      edited: false
      editors:
      - moebis
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594727754592896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663879535365-noauth.png?w=200&h=200&f=face
          fullname: Carl Moebsi
          isHf: false
          isPro: false
          name: moebis
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> is it possible\
          \ to add your own additional training to \"gpt4all-13b-snoozy\"? So lets\
          \ say I have my own data I want to train and give a heavier weight and merge\
          \ back with this, is that possible?</p>\n"
        raw: '@TheBloke is it possible to add your own additional training to "gpt4all-13b-snoozy"?
          So lets say I have my own data I want to train and give a heavier weight
          and merge back with this, is that possible?'
        updatedAt: '2023-06-21T12:45:08.944Z'
      numEdits: 0
      reactions: []
    id: 6492f0d44487bf82fded2143
    type: comment
  author: moebis
  content: '@TheBloke is it possible to add your own additional training to "gpt4all-13b-snoozy"?
    So lets say I have my own data I want to train and give a heavier weight and merge
    back with this, is that possible?'
  created_at: 2023-06-21 11:45:08+00:00
  edited: false
  hidden: false
  id: 6492f0d44487bf82fded2143
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-21T13:10:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9472976326942444
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ is it possible to add your own additional training to \"gpt4all-13b-snoozy\"\
          ? So lets say I have my own data I want to train and give a heavier weight\
          \ and merge back with this, is that possible?</p>\n</blockquote>\n<p>Yes\
          \ it is possible to apply additional training on top of a trained model\
          \ like this.</p>\n<p>There are four methods:</p>\n<ol>\n<li>Get the unquantised\
          \ model from this repo, apply a new full training on top of it - ie similar\
          \ to what GPT4All did to train this model in the first place, but using\
          \ their model as the base instead of raw Llama;</li>\n<li>Get the unquantised\
          \ model from this repo, apply a LoRA  fine tuning;</li>\n<li>Get the unquantised\
          \ model from this repo, apply a QLoRA fine tuning;</li>\n<li>Get the quantised\
          \ GPTQ model from my repo, apply a LoRA training using the new AutoGPTQ\
          \ PEFT code added in version 0.3.0 - not yet released, so need to compile\
          \ from source.</li>\n</ol>\n<p>Method 1 is arguably the ideal method in\
          \ terms of quality of training, but also by far the most expensive. To do\
          \ that you'd need 4 x A100 40GB, or similar hardware.</p>\n<p>Method 2 could\
          \ be done on a single A100 80GB, or probably a single A6000 48GB.</p>\n\
          <p>Method 3 could be done on a consumer GPU, like a 24GB 3090 or 4090, or\
          \ possibly even a 16GB GPU.  </p>\n<p>Method 4 could also be done on a consumer\
          \ GPU and may be a bit faster than method 3. It has the advantage that you\
          \ don't need to download the full 26GB base model, but only the 4bit GPTQ.\
          \  I don't know how quality compares to method 3.</p>\n<p>Generally speaking\
          \ I believe methods 2, 3 and 4 will all have a similar training quality.\
          \ Lower than method 1, but definitely acceptable.</p>\n<p>A lot of people\
          \ are using QLoRA now, since it came out a few weeks ago. If you browse\
          \ Hugging Face Hub a lot you may well have seen mention of QLoRA in a number\
          \ of repos.  There's a number of tutorial videos on it, like this one I\
          \ can recommend: <a rel=\"nofollow\" href=\"https://youtu.be/8vmWGX1nfNM\"\
          >https://youtu.be/8vmWGX1nfNM</a> .  And here's a blog post tutorial on\
          \ it: <a rel=\"nofollow\" href=\"https://animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/\"\
          >https://animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/</a></p>\n\
          <p>AutoGPTQ PEFT is brand new and not many people are using it yet, but\
          \ it just got integrated into text-generation-webui so it's easy to try\
          \ it in that UI.</p>\n"
        raw: "\n> @TheBloke is it possible to add your own additional training to\
          \ \"gpt4all-13b-snoozy\"? So lets say I have my own data I want to train\
          \ and give a heavier weight and merge back with this, is that possible?\n\
          \nYes it is possible to apply additional training on top of a trained model\
          \ like this.\n\nThere are four methods:\n\n1. Get the unquantised model\
          \ from this repo, apply a new full training on top of it - ie similar to\
          \ what GPT4All did to train this model in the first place, but using their\
          \ model as the base instead of raw Llama;\n2. Get the unquantised model\
          \ from this repo, apply a LoRA  fine tuning;\n3. Get the unquantised model\
          \ from this repo, apply a QLoRA fine tuning;\n3. Get the quantised GPTQ\
          \ model from my repo, apply a LoRA training using the new AutoGPTQ PEFT\
          \ code added in version 0.3.0 - not yet released, so need to compile from\
          \ source.\n\nMethod 1 is arguably the ideal method in terms of quality of\
          \ training, but also by far the most expensive. To do that you'd need 4\
          \ x A100 40GB, or similar hardware.\n\nMethod 2 could be done on a single\
          \ A100 80GB, or probably a single A6000 48GB.\n\nMethod 3 could be done\
          \ on a consumer GPU, like a 24GB 3090 or 4090, or possibly even a 16GB GPU.\
          \  \n\nMethod 4 could also be done on a consumer GPU and may be a bit faster\
          \ than method 3. It has the advantage that you don't need to download the\
          \ full 26GB base model, but only the 4bit GPTQ.  I don't know how quality\
          \ compares to method 3.\n\nGenerally speaking I believe methods 2, 3 and\
          \ 4 will all have a similar training quality. Lower than method 1, but definitely\
          \ acceptable.\n\nA lot of people are using QLoRA now, since it came out\
          \ a few weeks ago. If you browse Hugging Face Hub a lot you may well have\
          \ seen mention of QLoRA in a number of repos.  There's a number of tutorial\
          \ videos on it, like this one I can recommend: https://youtu.be/8vmWGX1nfNM\
          \ .  And here's a blog post tutorial on it: https://animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/\n\
          \nAutoGPTQ PEFT is brand new and not many people are using it yet, but it\
          \ just got integrated into text-generation-webui so it's easy to try it\
          \ in that UI."
        updatedAt: '2023-06-21T13:10:00.085Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - moebis
        - dillfrescott
        - Dipakbariya308
    id: 6492f6a8463c76d6c65ca33c
    type: comment
  author: TheBloke
  content: "\n> @TheBloke is it possible to add your own additional training to \"\
    gpt4all-13b-snoozy\"? So lets say I have my own data I want to train and give\
    \ a heavier weight and merge back with this, is that possible?\n\nYes it is possible\
    \ to apply additional training on top of a trained model like this.\n\nThere are\
    \ four methods:\n\n1. Get the unquantised model from this repo, apply a new full\
    \ training on top of it - ie similar to what GPT4All did to train this model in\
    \ the first place, but using their model as the base instead of raw Llama;\n2.\
    \ Get the unquantised model from this repo, apply a LoRA  fine tuning;\n3. Get\
    \ the unquantised model from this repo, apply a QLoRA fine tuning;\n3. Get the\
    \ quantised GPTQ model from my repo, apply a LoRA training using the new AutoGPTQ\
    \ PEFT code added in version 0.3.0 - not yet released, so need to compile from\
    \ source.\n\nMethod 1 is arguably the ideal method in terms of quality of training,\
    \ but also by far the most expensive. To do that you'd need 4 x A100 40GB, or\
    \ similar hardware.\n\nMethod 2 could be done on a single A100 80GB, or probably\
    \ a single A6000 48GB.\n\nMethod 3 could be done on a consumer GPU, like a 24GB\
    \ 3090 or 4090, or possibly even a 16GB GPU.  \n\nMethod 4 could also be done\
    \ on a consumer GPU and may be a bit faster than method 3. It has the advantage\
    \ that you don't need to download the full 26GB base model, but only the 4bit\
    \ GPTQ.  I don't know how quality compares to method 3.\n\nGenerally speaking\
    \ I believe methods 2, 3 and 4 will all have a similar training quality. Lower\
    \ than method 1, but definitely acceptable.\n\nA lot of people are using QLoRA\
    \ now, since it came out a few weeks ago. If you browse Hugging Face Hub a lot\
    \ you may well have seen mention of QLoRA in a number of repos.  There's a number\
    \ of tutorial videos on it, like this one I can recommend: https://youtu.be/8vmWGX1nfNM\
    \ .  And here's a blog post tutorial on it: https://animal-machine.com/posts/fine-tuning-llama-models-with-qlora-and-axolotl/\n\
    \nAutoGPTQ PEFT is brand new and not many people are using it yet, but it just\
    \ got integrated into text-generation-webui so it's easy to try it in that UI."
  created_at: 2023-06-21 12:10:00+00:00
  edited: false
  hidden: false
  id: 6492f6a8463c76d6c65ca33c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: nomic-ai/gpt4all-13b-snoozy
repo_type: model
status: open
target_branch: null
title: 4bit GPTQ model available for anyone interested
