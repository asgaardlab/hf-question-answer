!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Crocha
conflicting_files: null
created_at: 2023-06-09 12:50:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/61470978588c1fcdc8ef191efd6f60c8.svg
      fullname: Caio Rocha de Goes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Crocha
      type: user
    createdAt: '2023-06-09T13:50:53.000Z'
    data:
      edited: false
      editors:
      - Crocha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8070241212844849
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/61470978588c1fcdc8ef191efd6f60c8.svg
          fullname: Caio Rocha de Goes
          isHf: false
          isPro: false
          name: Crocha
          type: user
        html: '<p>Hello everyone, I''m loading the model to be used with gpu, I have
          the 6 x 3090 layout.<br>I''m using this form to load the model, I use load_in_8bit
          due to the amount of memory on the machines</p>

          <p><code>import transformers from transformers import AutoTokenizer, LlamaForCausalLM
          model = LlamaForCausalLM.from_pretrained("nomic-ai/gpt4all-13b-snoozy",
          device_map= ''auto'',low_cpu_mem_usage=True,load_in_8bit=True) tokenizer
          = AutoTokenizer.from_pretrained("nomic-ai/gpt4all-13b-snoozy", device_map=
          ''auto'', low_cpu_mem_usage=True,load_in_8bit=True)</code></p>

          <p>I see that even using the GPU the model still uses a little of the CPU
          cores, in the image to see that it is mainly limited to one core at 100%,
          would it be possible to make the model stop using the CPU or force it to
          use all the nuclei at the time of processing ?<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/vJhMxM4gPoJK-QkljnS19.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/vJhMxM4gPoJK-QkljnS19.png"></a></p>

          <p>Another question is whether it is normal for him to alternately use the
          GPUs, it is a little clearer in this image<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/lmoouDRz5dwsWmiBve2cH.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/lmoouDRz5dwsWmiBve2cH.png"></a></p>

          <p>Can you see that it uses some gpus but the others are stopped temporarily,
          is it a normal behavior of the model?</p>

          '
        raw: "Hello everyone, I'm loading the model to be used with gpu, I have the\
          \ 6 x 3090 layout.\r\nI'm using this form to load the model, I use load_in_8bit\
          \ due to the amount of memory on the machines\r\n\r\n`import transformers\r\
          \nfrom transformers import AutoTokenizer, LlamaForCausalLM\r\nmodel = LlamaForCausalLM.from_pretrained(\"\
          nomic-ai/gpt4all-13b-snoozy\", device_map= 'auto',low_cpu_mem_usage=True,load_in_8bit=True)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/gpt4all-13b-snoozy\"\
          , device_map= 'auto', low_cpu_mem_usage=True,load_in_8bit=True)`\r\n\r\n\
          I see that even using the GPU the model still uses a little of the CPU cores,\
          \ in the image to see that it is mainly limited to one core at 100%, would\
          \ it be possible to make the model stop using the CPU or force it to use\
          \ all the nuclei at the time of processing ?\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/vJhMxM4gPoJK-QkljnS19.png)\r\
          \n\r\nAnother question is whether it is normal for him to alternately use\
          \ the GPUs, it is a little clearer in this image\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/lmoouDRz5dwsWmiBve2cH.png)\r\
          \n\r\nCan you see that it uses some gpus but the others are stopped temporarily,\
          \ is it a normal behavior of the model?"
        updatedAt: '2023-06-09T13:50:53.005Z'
      numEdits: 0
      reactions: []
    id: 64832e3df190aedbb4bab2f5
    type: comment
  author: Crocha
  content: "Hello everyone, I'm loading the model to be used with gpu, I have the\
    \ 6 x 3090 layout.\r\nI'm using this form to load the model, I use load_in_8bit\
    \ due to the amount of memory on the machines\r\n\r\n`import transformers\r\n\
    from transformers import AutoTokenizer, LlamaForCausalLM\r\nmodel = LlamaForCausalLM.from_pretrained(\"\
    nomic-ai/gpt4all-13b-snoozy\", device_map= 'auto',low_cpu_mem_usage=True,load_in_8bit=True)\r\
    \ntokenizer = AutoTokenizer.from_pretrained(\"nomic-ai/gpt4all-13b-snoozy\", device_map=\
    \ 'auto', low_cpu_mem_usage=True,load_in_8bit=True)`\r\n\r\nI see that even using\
    \ the GPU the model still uses a little of the CPU cores, in the image to see\
    \ that it is mainly limited to one core at 100%, would it be possible to make\
    \ the model stop using the CPU or force it to use all the nuclei at the time of\
    \ processing ?\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/vJhMxM4gPoJK-QkljnS19.png)\r\
    \n\r\nAnother question is whether it is normal for him to alternately use the\
    \ GPUs, it is a little clearer in this image\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6483255797a133e1f2285f97/lmoouDRz5dwsWmiBve2cH.png)\r\
    \n\r\nCan you see that it uses some gpus but the others are stopped temporarily,\
    \ is it a normal behavior of the model?"
  created_at: 2023-06-09 12:50:53+00:00
  edited: false
  hidden: false
  id: 64832e3df190aedbb4bab2f5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: nomic-ai/gpt4all-13b-snoozy
repo_type: model
status: open
target_branch: null
title: Model usage with GPU and CPU cores usage
