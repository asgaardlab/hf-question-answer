!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zippity
conflicting_files: null
created_at: 2023-04-29 15:38:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94ae712e1be235e4f6b4c42eb635de4d.svg
      fullname: Bit Shifter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zippity
      type: user
    createdAt: '2023-04-29T16:38:09.000Z'
    data:
      edited: false
      editors:
      - Zippity
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94ae712e1be235e4f6b4c42eb635de4d.svg
          fullname: Bit Shifter
          isHf: false
          isPro: false
          name: Zippity
          type: user
        html: '<p>I downloaded a model and when I tried to use it in gpt4all it erred
          with the following message:</p>

          <p>main: seed = 1682785585<br>llama_model_load: loading model from ''gpt4all-lora-quantized-ggfm.bin''
          - please wait ...<br>llama_model_load: invalid model file ''gpt4all-lora-quantized-ggfm.bin''
          (bad magic)<br>main: failed to load model from ''gpt4all-lora-quantized-ggfm.bin''</p>

          <p>Other models do work:</p>

          <p>main: seed = 1682786177<br>llama_model_load: loading model from ''gpt4all-lora-unfiltered-quantized.bin''
          - please wait ...<br>llama_model_load: ggml ctx size = 6065.35 MB<br>llama_model_load:
          memory_size =  2048.00 MB, n_mem = 65536<br>llama_model_load: loading model
          part 1/1 from ''gpt4all-lora-unfiltered-quantized.bin''<br>llama_model_load:
          .................................... done<br>llama_model_load: model size
          =  4017.27 MB / num tensors = 291</p>

          <p>system_info: n_threads = 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA
          = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 |
          BLAS = 0 | SSE3 = 1 | VSX = 0 |<br>main: interactive mode on.<br>sampling
          parameters: temp = 0.100000, top_k = 40, top_p = 0.950000, repeat_last_n
          = 64, repeat_penalty = 1.300000</p>

          <p>I was hoping to try your models, but don''t know how to make them work.</p>

          '
        raw: "I downloaded a model and when I tried to use it in gpt4all it erred\
          \ with the following message:\r\n\r\nmain: seed = 1682785585\r\nllama_model_load:\
          \ loading model from 'gpt4all-lora-quantized-ggfm.bin' - please wait ...\r\
          \nllama_model_load: invalid model file 'gpt4all-lora-quantized-ggfm.bin'\
          \ (bad magic)\r\nmain: failed to load model from 'gpt4all-lora-quantized-ggfm.bin'\r\
          \n\r\nOther models do work:\r\n\r\nmain: seed = 1682786177\r\nllama_model_load:\
          \ loading model from 'gpt4all-lora-unfiltered-quantized.bin' - please wait\
          \ ...\r\nllama_model_load: ggml ctx size = 6065.35 MB\r\nllama_model_load:\
          \ memory_size =  2048.00 MB, n_mem = 65536\r\nllama_model_load: loading\
          \ model part 1/1 from 'gpt4all-lora-unfiltered-quantized.bin'\r\nllama_model_load:\
          \ .................................... done\r\nllama_model_load: model size\
          \ =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads = 4 /\
          \ 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA =\
          \ 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX\
          \ = 0 | \r\nmain: interactive mode on.\r\nsampling parameters: temp = 0.100000,\
          \ top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\
          \n\r\nI was hoping to try your models, but don't know how to make them work.\r\
          \n"
        updatedAt: '2023-04-29T16:38:09.129Z'
      numEdits: 0
      reactions: []
    id: 644d47f16d84e25fad75eda9
    type: comment
  author: Zippity
  content: "I downloaded a model and when I tried to use it in gpt4all it erred with\
    \ the following message:\r\n\r\nmain: seed = 1682785585\r\nllama_model_load: loading\
    \ model from 'gpt4all-lora-quantized-ggfm.bin' - please wait ...\r\nllama_model_load:\
    \ invalid model file 'gpt4all-lora-quantized-ggfm.bin' (bad magic)\r\nmain: failed\
    \ to load model from 'gpt4all-lora-quantized-ggfm.bin'\r\n\r\nOther models do\
    \ work:\r\n\r\nmain: seed = 1682786177\r\nllama_model_load: loading model from\
    \ 'gpt4all-lora-unfiltered-quantized.bin' - please wait ...\r\nllama_model_load:\
    \ ggml ctx size = 6065.35 MB\r\nllama_model_load: memory_size =  2048.00 MB, n_mem\
    \ = 65536\r\nllama_model_load: loading model part 1/1 from 'gpt4all-lora-unfiltered-quantized.bin'\r\
    \nllama_model_load: .................................... done\r\nllama_model_load:\
    \ model size =  4017.27 MB / num tensors = 291\r\n\r\nsystem_info: n_threads =\
    \ 4 / 12 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0\
    \ | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \r\
    \nmain: interactive mode on.\r\nsampling parameters: temp = 0.100000, top_k =\
    \ 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\r\n\r\n\
    I was hoping to try your models, but don't know how to make them work.\r\n"
  created_at: 2023-04-29 15:38:09+00:00
  edited: false
  hidden: false
  id: 644d47f16d84e25fad75eda9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Jiliby/GPT4All-7B-4bit
repo_type: model
status: open
target_branch: null
title: Is this broken?
