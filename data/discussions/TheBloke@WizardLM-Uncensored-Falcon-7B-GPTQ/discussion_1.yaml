!!python/object:huggingface_hub.community.DiscussionWithDetails
author: thefaheem
conflicting_files: null
created_at: 2023-06-01 17:01:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-01T18:01:31.000Z'
    data:
      edited: true
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>This is what my code is </p>\n<pre><code class=\"language-import\"\
          >from transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\n\
          \n# Download the model from HF and store it locally, then reference its\
          \ location here:\n#quantized_model_dir = model_path\n\nfrom transformers\
          \ import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\"\
          , use_fast=False)\n\nmodel = AutoGPTQForCausalLM.from_quantized(\"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\"\
          , device=\"cuda:1\", use_triton=False, use_safetensors=True, trust_remote_code=True)\n\
          </code></pre>\n<p>When I Run This, i Got These:</p>\n<pre><code class=\"\
          language-A\">- configuration_RW.py\n. Make sure to double-check they do\
          \ not contain any added malicious code. To avoid downloading new versions\
          \ of the code file, you can pin a revision.\n\u256D\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u256E\n\u2502 in &lt;cell line: 11&gt;:11     \
          \                                                                      \
          \ \u2502\n\u2502                                                       \
          \                                           \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
          \ in from_quantized          \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   59 \u2502   \u2502   model_basename: Optional[str] = None,  \
          \                                             \u2502\n\u2502   60 \u2502\
          \   \u2502   trust_remote_code: bool = False                           \
          \                          \u2502\n\u2502   61 \u2502   ) -&gt; BaseGPTQForCausalLM:\
          \                                                               \u2502\n\
          \u2502 \u2771 62 \u2502   \u2502   model_type = check_and_get_model_type(save_dir)\
          \                                     \u2502\n\u2502   63 \u2502   \u2502\
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(        \
          \                 \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                                              \u2502\n\u2502\
          \   65 \u2502   \u2502   \u2502   device=device,                       \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
          \ in                      \u2502\n\u2502 check_and_get_model_type      \
          \                                                                   \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502   121 def check_and_get_model_type(model_dir):\
          \                                                   \u2502\n\u2502   122\
          \ \u2502   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\
          \                 \u2502\n\u2502   123 \u2502   if config.model_type not\
          \ in SUPPORTED_MODELS:                                          \u2502\n\
          \u2502 \u2771 124 \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                       \u2502\n\u2502   125 \u2502\
          \   model_type = config.model_type                                     \
          \                    \u2502\n\u2502   126 \u2502   return model_type   \
          \                                                                   \u2502\
          \n\u2502   127                                                         \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
          \ RefinedWebModel isn't supported yet.\n</code></pre>\n<p>How Should I Get\
          \ Rid of These?</p>\n<p>Can SomeBody Help?</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \  can you please chime in. Thanks!</p>\n"
        raw: "This is what my code is \n\n```import torch\nfrom transformers import\
          \ AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\n\n# Download\
          \ the model from HF and store it locally, then reference its location here:\n\
          #quantized_model_dir = model_path\n\nfrom transformers import AutoTokenizer\n\
          tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\"\
          , use_fast=False)\n\nmodel = AutoGPTQForCausalLM.from_quantized(\"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\"\
          , device=\"cuda:1\", use_triton=False, use_safetensors=True, trust_remote_code=True)\n\
          ```\n\nWhen I Run This, i Got These:\n\n```A new version of the following\
          \ files was downloaded from https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ:\n\
          - configuration_RW.py\n. Make sure to double-check they do not contain any\
          \ added malicious code. To avoid downloading new versions of the code file,\
          \ you can pin a revision.\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \ Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u256E\n\u2502 in <cell line: 11>:11                       \
          \                                                     \u2502\n\u2502   \
          \                                                                      \
          \                         \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
          \ in from_quantized          \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   59 \u2502   \u2502   model_basename: Optional[str] = None,  \
          \                                             \u2502\n\u2502   60 \u2502\
          \   \u2502   trust_remote_code: bool = False                           \
          \                          \u2502\n\u2502   61 \u2502   ) -> BaseGPTQForCausalLM:\
          \                                                               \u2502\n\
          \u2502 \u2771 62 \u2502   \u2502   model_type = check_and_get_model_type(save_dir)\
          \                                     \u2502\n\u2502   63 \u2502   \u2502\
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(        \
          \                 \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                                              \u2502\n\u2502\
          \   65 \u2502   \u2502   \u2502   device=device,                       \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
          \ in                      \u2502\n\u2502 check_and_get_model_type      \
          \                                                                   \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502   121 def check_and_get_model_type(model_dir):\
          \                                                   \u2502\n\u2502   122\
          \ \u2502   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\
          \                 \u2502\n\u2502   123 \u2502   if config.model_type not\
          \ in SUPPORTED_MODELS:                                          \u2502\n\
          \u2502 \u2771 124 \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                       \u2502\n\u2502   125 \u2502\
          \   model_type = config.model_type                                     \
          \                    \u2502\n\u2502   126 \u2502   return model_type   \
          \                                                                   \u2502\
          \n\u2502   127                                                         \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
          \ RefinedWebModel isn't supported yet.\n```\n\nHow Should I Get Rid of These?\n\
          \nCan SomeBody Help?\n\n@TheBloke  can you please chime in. Thanks!"
        updatedAt: '2023-06-02T06:27:56.509Z'
      numEdits: 1
      reactions: []
    id: 6478dcfb4d959c96feb8dd72
    type: comment
  author: thefaheem
  content: "This is what my code is \n\n```import torch\nfrom transformers import\
    \ AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM\n\n# Download the model\
    \ from HF and store it locally, then reference its location here:\n#quantized_model_dir\
    \ = model_path\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"\
    TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\", use_fast=False)\n\nmodel = AutoGPTQForCausalLM.from_quantized(\"\
    TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\", device=\"cuda:1\", use_triton=False,\
    \ use_safetensors=True, trust_remote_code=True)\n```\n\nWhen I Run This, i Got\
    \ These:\n\n```A new version of the following files was downloaded from https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ:\n\
    - configuration_RW.py\n. Make sure to double-check they do not contain any added\
    \ malicious code. To avoid downloading new versions of the code file, you can\
    \ pin a revision.\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
    \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 in <cell line:\
    \ 11>:11                                                                     \
    \       \u2502\n\u2502                                                       \
    \                                           \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
    \ in from_quantized          \u2502\n\u2502                                  \
    \                                                                \u2502\n\u2502\
    \   59 \u2502   \u2502   model_basename: Optional[str] = None,               \
    \                                \u2502\n\u2502   60 \u2502   \u2502   trust_remote_code:\
    \ bool = False                                                     \u2502\n\u2502\
    \   61 \u2502   ) -> BaseGPTQForCausalLM:                                    \
    \                           \u2502\n\u2502 \u2771 62 \u2502   \u2502   model_type\
    \ = check_and_get_model_type(save_dir)                                     \u2502\
    \n\u2502   63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
    \                         \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                              \u2502\n\u2502\
    \   65 \u2502   \u2502   \u2502   device=device,                             \
    \                                     \u2502\n\u2502                         \
    \                                                                         \u2502\
    \n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
    \ in                      \u2502\n\u2502 check_and_get_model_type            \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \                     \u2502\n\u2502   121 def check_and_get_model_type(model_dir):\
    \                                                   \u2502\n\u2502   122 \u2502\
    \   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)   \
    \              \u2502\n\u2502   123 \u2502   if config.model_type not in SUPPORTED_MODELS:\
    \                                          \u2502\n\u2502 \u2771 124 \u2502  \
    \ \u2502   raise TypeError(f\"{config.model_type} isn't supported yet.\")    \
    \                   \u2502\n\u2502   125 \u2502   model_type = config.model_type\
    \                                                         \u2502\n\u2502   126\
    \ \u2502   return model_type                                                 \
    \                     \u2502\n\u2502   127                                   \
    \                                                         \u2502\n\u2570\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError: RefinedWebModel isn't supported\
    \ yet.\n```\n\nHow Should I Get Rid of These?\n\nCan SomeBody Help?\n\n@TheBloke\
    \  can you please chime in. Thanks!"
  created_at: 2023-06-01 17:01:31+00:00
  edited: true
  hidden: false
  id: 6478dcfb4d959c96feb8dd72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-06-02T09:04:16.000Z'
    data:
      edited: false
      editors:
      - Dxtrmst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: "<p>\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E<br>\u2502\
          \ in &lt;cell line: 11&gt;:11                                          \
          \                                  \u2502<br>\u2502                    \
          \                                                                      \
          \        \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
          \ in from_quantized          \u2502<br>\u2502                          \
          \                                                                      \
          \  \u2502<br>\u2502   59 \u2502   \u2502   model_basename: Optional[str]\
          \ = None,                                               \u2502<br>\u2502\
          \   60 \u2502   \u2502   trust_remote_code: bool = False               \
          \                                      \u2502<br>\u2502   61 \u2502   )\
          \ -&gt; BaseGPTQForCausalLM:                                           \
          \                    \u2502<br>\u2502 \u2771 62 \u2502   \u2502   model_type\
          \ = check_and_get_model_type(save_dir)                                 \
          \    \u2502<br>\u2502   63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
          \                         \u2502<br>\u2502   64 \u2502   \u2502   \u2502\
          \   save_dir=save_dir,                                                 \
          \             \u2502<br>\u2502   65 \u2502   \u2502   \u2502   device=device,\
          \                                                                  \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
          \ in                      \u2502<br>\u2502 check_and_get_model_type    \
          \                                                                     \u2502\
          <br>\u2502                                                             \
          \                                     \u2502<br>\u2502   121 def check_and_get_model_type(model_dir):\
          \                                                   \u2502<br>\u2502   122\
          \ \u2502   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\
          \                 \u2502<br>\u2502   123 \u2502   if config.model_type not\
          \ in SUPPORTED_MODELS:                                          \u2502<br>\u2502\
          \ \u2771 124 \u2502   \u2502   raise TypeError(f\"{config.model_type} isn't\
          \ supported yet.\")                       \u2502<br>\u2502   125 \u2502\
          \   model_type = config.model_type                                     \
          \                    \u2502<br>\u2502   126 \u2502   return model_type \
          \                                                                     \u2502\
          <br>\u2502   127                                                       \
          \                                     \u2502<br>\u2570\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\
          <br>TypeError: RefinedWebModel isn't supported yet.</p>\n"
        raw: "\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ in <cell line: 11>:11                                                \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
          \ in from_quantized          \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502   59 \u2502   \u2502   model_basename: Optional[str] = None,  \
          \                                             \u2502\n\u2502   60 \u2502\
          \   \u2502   trust_remote_code: bool = False                           \
          \                          \u2502\n\u2502   61 \u2502   ) -> BaseGPTQForCausalLM:\
          \                                                               \u2502\n\
          \u2502 \u2771 62 \u2502   \u2502   model_type = check_and_get_model_type(save_dir)\
          \                                     \u2502\n\u2502   63 \u2502   \u2502\
          \   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(        \
          \                 \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
          \                                                              \u2502\n\u2502\
          \   65 \u2502   \u2502   \u2502   device=device,                       \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
          \ in                      \u2502\n\u2502 check_and_get_model_type      \
          \                                                                   \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502   121 def check_and_get_model_type(model_dir):\
          \                                                   \u2502\n\u2502   122\
          \ \u2502   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)\
          \                 \u2502\n\u2502   123 \u2502   if config.model_type not\
          \ in SUPPORTED_MODELS:                                          \u2502\n\
          \u2502 \u2771 124 \u2502   \u2502   raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\")                       \u2502\n\u2502   125 \u2502\
          \   model_type = config.model_type                                     \
          \                    \u2502\n\u2502   126 \u2502   return model_type   \
          \                                                                   \u2502\
          \n\u2502   127                                                         \
          \                                   \u2502\n\u2570\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError:\
          \ RefinedWebModel isn't supported yet."
        updatedAt: '2023-06-02T09:04:16.283Z'
      numEdits: 0
      reactions: []
    id: 6479b0903a17d5e00aca1c28
    type: comment
  author: Dxtrmst
  content: "\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 in <cell line: 11>:11           \
    \                                                                 \u2502\n\u2502\
    \                                                                            \
    \                      \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py:62\
    \ in from_quantized          \u2502\n\u2502                                  \
    \                                                                \u2502\n\u2502\
    \   59 \u2502   \u2502   model_basename: Optional[str] = None,               \
    \                                \u2502\n\u2502   60 \u2502   \u2502   trust_remote_code:\
    \ bool = False                                                     \u2502\n\u2502\
    \   61 \u2502   ) -> BaseGPTQForCausalLM:                                    \
    \                           \u2502\n\u2502 \u2771 62 \u2502   \u2502   model_type\
    \ = check_and_get_model_type(save_dir)                                     \u2502\
    \n\u2502   63 \u2502   \u2502   return GPTQ_CAUSAL_LM_MODEL_MAP[model_type].from_quantized(\
    \                         \u2502\n\u2502   64 \u2502   \u2502   \u2502   save_dir=save_dir,\
    \                                                              \u2502\n\u2502\
    \   65 \u2502   \u2502   \u2502   device=device,                             \
    \                                     \u2502\n\u2502                         \
    \                                                                         \u2502\
    \n\u2502 /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_utils.py:124\
    \ in                      \u2502\n\u2502 check_and_get_model_type            \
    \                                                             \u2502\n\u2502 \
    \                                                                            \
    \                     \u2502\n\u2502   121 def check_and_get_model_type(model_dir):\
    \                                                   \u2502\n\u2502   122 \u2502\
    \   config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)   \
    \              \u2502\n\u2502   123 \u2502   if config.model_type not in SUPPORTED_MODELS:\
    \                                          \u2502\n\u2502 \u2771 124 \u2502  \
    \ \u2502   raise TypeError(f\"{config.model_type} isn't supported yet.\")    \
    \                   \u2502\n\u2502   125 \u2502   model_type = config.model_type\
    \                                                         \u2502\n\u2502   126\
    \ \u2502   return model_type                                                 \
    \                     \u2502\n\u2502   127                                   \
    \                                                         \u2502\n\u2570\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u256F\nTypeError: RefinedWebModel isn't supported\
    \ yet."
  created_at: 2023-06-02 08:04:16+00:00
  edited: false
  hidden: false
  id: 6479b0903a17d5e00aca1c28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:06:07.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>Did You Found Any Solution, My Friend?</p>

          '
        raw: Did You Found Any Solution, My Friend?
        updatedAt: '2023-06-02T09:06:07.213Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F614"
        users:
        - Dxtrmst
        - thefaheem
    id: 6479b0ff1a2aefceecd2fd84
    type: comment
  author: thefaheem
  content: Did You Found Any Solution, My Friend?
  created_at: 2023-06-02 08:06:07+00:00
  edited: false
  hidden: false
  id: 6479b0ff1a2aefceecd2fd84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:21:33.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>Don't Worry <span data-props=\"{&quot;user&quot;:&quot;Dxtrmst&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Dxtrmst\"\
          >@<span class=\"underline\">Dxtrmst</span></a></span>\n\n\t</span></span>\
          \ Tom is Working on This.</p>\n"
        raw: Don't Worry @Dxtrmst Tom is Working on This.
        updatedAt: '2023-06-02T09:21:33.882Z'
      numEdits: 0
      reactions: []
    id: 6479b49d3a17d5e00aca805e
    type: comment
  author: thefaheem
  content: Don't Worry @Dxtrmst Tom is Working on This.
  created_at: 2023-06-02 08:21:33+00:00
  edited: false
  hidden: false
  id: 6479b49d3a17d5e00aca805e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T09:24:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You need to be using auto-gptq version 0.2.0.  For some reason you
          guys don''t have the right version.</p>

          <p>Please try:</p>

          <pre><code>pip uninstall auto-gptq

          pip install auto-gptq==0.2.0

          </code></pre>

          '
        raw: 'You need to be using auto-gptq version 0.2.0.  For some reason you guys
          don''t have the right version.


          Please try:

          ```

          pip uninstall auto-gptq

          pip install auto-gptq==0.2.0

          ```'
        updatedAt: '2023-06-02T09:24:10.361Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lucianosb
    id: 6479b53aa84498f2af44fa8c
    type: comment
  author: TheBloke
  content: 'You need to be using auto-gptq version 0.2.0.  For some reason you guys
    don''t have the right version.


    Please try:

    ```

    pip uninstall auto-gptq

    pip install auto-gptq==0.2.0

    ```'
  created_at: 2023-06-02 08:24:10+00:00
  edited: false
  hidden: false
  id: 6479b53aa84498f2af44fa8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:36:16.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<p>I Showed The Log Here: <a href="https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/discussions/9">https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/discussions/9</a></p>

          '
        raw: 'I Showed The Log Here: https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/discussions/9'
        updatedAt: '2023-06-02T09:36:16.620Z'
      numEdits: 0
      reactions: []
    id: 6479b810f518a860fbc3e5af
    type: comment
  author: thefaheem
  content: 'I Showed The Log Here: https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ/discussions/9'
  created_at: 2023-06-02 08:36:16+00:00
  edited: false
  hidden: false
  id: 6479b810f518a860fbc3e5af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-06-02T09:37:06.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: '<blockquote>

          <p>You need to be using auto-gptq version 0.2.0.  For some reason you guys
          don''t have the right version.</p>

          <p>Please try:</p>

          <pre><code>pip uninstall auto-gptq

          pip install auto-gptq==0.2.0

          </code></pre>

          </blockquote>

          <p>I donno somehow it says it does''nt have version 0.2.0</p>

          '
        raw: "> You need to be using auto-gptq version 0.2.0.  For some reason you\
          \ guys don't have the right version.\n> \n> Please try:\n> ```\n> pip uninstall\
          \ auto-gptq\n> pip install auto-gptq==0.2.0\n> ```\n\nI donno somehow it\
          \ says it does'nt have version 0.2.0"
        updatedAt: '2023-06-02T09:37:06.751Z'
      numEdits: 0
      reactions: []
    id: 6479b842f518a860fbc3eaf4
    type: comment
  author: thefaheem
  content: "> You need to be using auto-gptq version 0.2.0.  For some reason you guys\
    \ don't have the right version.\n> \n> Please try:\n> ```\n> pip uninstall auto-gptq\n\
    > pip install auto-gptq==0.2.0\n> ```\n\nI donno somehow it says it does'nt have\
    \ version 0.2.0"
  created_at: 2023-06-02 08:37:06+00:00
  edited: false
  hidden: false
  id: 6479b842f518a860fbc3eaf4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
      fullname: Jezehel Villaber Franca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dxtrmst
      type: user
    createdAt: '2023-06-02T09:59:18.000Z'
    data:
      edited: true
      editors:
      - Dxtrmst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/FeebGjdFQ4ueuJesSSoX7.png?w=200&h=200&f=face
          fullname: Jezehel Villaber Franca
          isHf: false
          isPro: false
          name: Dxtrmst
          type: user
        html: "<p>thank you <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>this works for me in colab google</p>\n<p>!git clone <a rel=\"\
          nofollow\" href=\"https://github.com/PanQiWei/AutoGPTQ\">https://github.com/PanQiWei/AutoGPTQ</a><br>%cd\
          \ AutoGPTQ<br>!pip install .</p>\n<p>and then download in colab as per below\
          \ sequence.</p>\n<ol>\n<li>!huggingface-cli login --token \"hf_xxxxxxxxx\"\
          </li>\n<li>from huggingface_hub import snapshot_download<br>  snapshot_download(repo_id=\"\
          TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\")</li>\n</ol>\n<p>this wil\
          \ dowdload to the colab instance root folder (this folder will be shown\
          \ after executing step 2 above)</p>\n<p>the rest are the same. however i\
          \ received some warning notifications but it still works.</p>\n"
        raw: "thank you @TheBloke \n\nthis works for me in colab google\n\n!git clone\
          \ https://github.com/PanQiWei/AutoGPTQ\n%cd AutoGPTQ\n!pip install .\n\n\
          and then download in colab as per below sequence.\n\n1. !huggingface-cli\
          \ login --token \"hf_xxxxxxxxx\"\n2. from huggingface_hub import snapshot_download\n\
          \     snapshot_download(repo_id=\"TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\"\
          )\n\nthis wil dowdload to the colab instance root folder (this folder will\
          \ be shown after executing step 2 above)\n\nthe rest are the same. however\
          \ i received some warning notifications but it still works."
        updatedAt: '2023-06-02T10:00:27.826Z'
      numEdits: 1
      reactions: []
    id: 6479bd761a2aefceecd43638
    type: comment
  author: Dxtrmst
  content: "thank you @TheBloke \n\nthis works for me in colab google\n\n!git clone\
    \ https://github.com/PanQiWei/AutoGPTQ\n%cd AutoGPTQ\n!pip install .\n\nand then\
    \ download in colab as per below sequence.\n\n1. !huggingface-cli login --token\
    \ \"hf_xxxxxxxxx\"\n2. from huggingface_hub import snapshot_download\n     snapshot_download(repo_id=\"\
    TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ\")\n\nthis wil dowdload to the colab\
    \ instance root folder (this folder will be shown after executing step 2 above)\n\
    \nthe rest are the same. however i received some warning notifications but it\
    \ still works."
  created_at: 2023-06-02 08:59:18+00:00
  edited: true
  hidden: false
  id: 6479bd761a2aefceecd43638
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-02T10:00:36.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah currently a lot of warnings are printed, stuff like: </p>

          <pre><code>WARNING:RWGPTQForCausalLM hasn''t fused attention module yet,
          will skip inject fused attention.

          WARNING:RWGPTQForCausalLM hasn''t fused mlp module yet, will skip inject
          fused mlp.

          </code></pre>

          <p>This can be ignored. I will suggest to the AutoGPTQ author that these
          warnings should be INFO instead, or not printed</p>

          '
        raw: "Yeah currently a lot of warnings are printed, stuff like: \n```\nWARNING:RWGPTQForCausalLM\
          \ hasn't fused attention module yet, will skip inject fused attention.\n\
          WARNING:RWGPTQForCausalLM hasn't fused mlp module yet, will skip inject\
          \ fused mlp.\n```\n\nThis can be ignored. I will suggest to the AutoGPTQ\
          \ author that these warnings should be INFO instead, or not printed"
        updatedAt: '2023-06-02T10:00:36.335Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Dxtrmst
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - thefaheem
    id: 6479bdc43a17d5e00acb6403
    type: comment
  author: TheBloke
  content: "Yeah currently a lot of warnings are printed, stuff like: \n```\nWARNING:RWGPTQForCausalLM\
    \ hasn't fused attention module yet, will skip inject fused attention.\nWARNING:RWGPTQForCausalLM\
    \ hasn't fused mlp module yet, will skip inject fused mlp.\n```\n\nThis can be\
    \ ignored. I will suggest to the AutoGPTQ author that these warnings should be\
    \ INFO instead, or not printed"
  created_at: 2023-06-02 09:00:36+00:00
  edited: false
  hidden: false
  id: 6479bdc43a17d5e00acb6403
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fd55d402ce3d2cbf2c2a451489f8542.svg
      fullname: Ibrahim.H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bitsnaps
      type: user
    createdAt: '2023-08-15T16:57:41.000Z'
    data:
      edited: false
      editors:
      - bitsnaps
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6966263055801392
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fd55d402ce3d2cbf2c2a451489f8542.svg
          fullname: Ibrahim.H
          isHf: false
          isPro: false
          name: bitsnaps
          type: user
        html: '<p>The only working "old" version of <code>auto-gptq</code> at the
          moment is <code>%pip install auto-gptq==0.2.2</code> on colab.</p>

          '
        raw: The only working "old" version of `auto-gptq` at the moment is `%pip
          install auto-gptq==0.2.2` on colab.
        updatedAt: '2023-08-15T16:57:41.559Z'
      numEdits: 0
      reactions: []
    id: 64dbae85210fa7992a751445
    type: comment
  author: bitsnaps
  content: The only working "old" version of `auto-gptq` at the moment is `%pip install
    auto-gptq==0.2.2` on colab.
  created_at: 2023-08-15 15:57:41+00:00
  edited: false
  hidden: false
  id: 64dbae85210fa7992a751445
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'TypeError: RefinedWebModel isn''t supported yet.'
