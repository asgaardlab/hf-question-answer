!!python/object:huggingface_hub.community.DiscussionWithDetails
author: annamalai-s
conflicting_files: null
created_at: 2023-12-11 11:39:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eec69deff03cbc54a74f080c40d26b6f.svg
      fullname: Annamalai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: annamalai-s
      type: user
    createdAt: '2023-12-11T11:39:37.000Z'
    data:
      edited: false
      editors:
      - annamalai-s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6717565655708313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eec69deff03cbc54a74f080c40d26b6f.svg
          fullname: Annamalai
          isHf: false
          isPro: false
          name: annamalai-s
          type: user
        html: '<p>I''m working on a task to connect database to opensource LLM using
          langchain. I''m trying to connect quantized falcon-7b to SQLDatabase chain.
          This is how I have loaded the model from hugging face<br>repo,</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)<br>model
          = AutoGPTQForCausalLM.from_quantized(model_id, device="cpu", use_triton=False,
          use_safetensors=True, torch_dtype=torch.float16, trust_remote_code=True)</p>

          <p>Note: I have also changed dtype to floatb16, float32 in model class</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6549d71f84ac015f9ee7fc4e/F54kVaxgore78aJartGCq.png"><img
          alt="Screenshot from 2023-12-11 16-38-13.png" src="https://cdn-uploads.huggingface.co/production/uploads/6549d71f84ac015f9ee7fc4e/F54kVaxgore78aJartGCq.png"></a><br>pipe
          = transformers.pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    torch_dtype=torch.float32,<br>    trust_remote_code=True,<br>    device_map=''auto'',<br>    max_new_tokens=2649,<br>    )</p>

          <p>When I run the chain with the input prompt, the code throws the following
          error,<br>chain.run(''How many employees are there?'')</p>

          <p>   File "/home/user/Disk/Langchain/env/lib/python3.10/site-packages/torch/nn/modules/normalization.py",
          line 196, in forward<br>    return F.layer_norm(<br>  File "/home/user/Disk/Langchain/env/lib/python3.10/site-packages/torch/nn/functional.py",
          line 2543, in layer_norm<br>    return torch.layer_norm(input, normalized_shape,
          weight, bias, eps, torch.backends.cudnn.enabled)<br>RuntimeError: "LayerNormKernelImpl"
          not implemented for ''Half''</p>

          <p>These are the package versions I''m using,<br>    langchain  ==  0.0.348<br>    langchain-core  ==
          0.0.12<br>    langchain-experimental ==  0.0.44<br>    langsmith  ==  0.0.69<br>    transformers  ==  4.33.0<br>    accelerate  ==  0.25.0<br>    auto-gptq  ==  0.5.1</p>

          <p>Could someone help with is problem</p>

          '
        raw: "I'm working on a task to connect database to opensource LLM using langchain.\
          \ I'm trying to connect quantized falcon-7b to SQLDatabase chain. This is\
          \ how I have loaded the model from hugging face \r\nrepo,\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_id, use_fast=False)\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_id,\
          \ device=\"cpu\", use_triton=False, use_safetensors=True, torch_dtype=torch.float16,\
          \ trust_remote_code=True)\r\n\r\nNote: I have also changed dtype to floatb16,\
          \ float32 in model class\r\n\r\n![Screenshot from 2023-12-11 16-38-13.png](https://cdn-uploads.huggingface.co/production/uploads/6549d71f84ac015f9ee7fc4e/F54kVaxgore78aJartGCq.png)\r\
          \npipe = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.float32,\r\n    trust_remote_code=True,\r\
          \n    device_map='auto',\r\n    max_new_tokens=2649,\r\n    )\r\n\r\nWhen\
          \ I run the chain with the input prompt, the code throws the following error,\r\
          \nchain.run('How many employees are there?')\r\n\r\n   File \"/home/user/Disk/Langchain/env/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
          , line 196, in forward\r\n    return F.layer_norm(\r\n  File \"/home/user/Disk/Langchain/env/lib/python3.10/site-packages/torch/nn/functional.py\"\
          , line 2543, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
          \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\"\
          \ not implemented for 'Half'\r\n\r\nThese are the package versions I'm using,\r\
          \n    langchain  ==  0.0.348\r\n    langchain-core  == 0.0.12\r\n    langchain-experimental\
          \ ==  0.0.44\r\n    langsmith  ==  0.0.69\r\n    transformers  ==  4.33.0\r\
          \n    accelerate  ==  0.25.0\r\n    auto-gptq  ==  0.5.1\r\n\r\nCould someone\
          \ help with is problem"
        updatedAt: '2023-12-11T11:39:37.914Z'
      numEdits: 0
      reactions: []
    id: 6576f4f98d209fc6a76953a5
    type: comment
  author: annamalai-s
  content: "I'm working on a task to connect database to opensource LLM using langchain.\
    \ I'm trying to connect quantized falcon-7b to SQLDatabase chain. This is how\
    \ I have loaded the model from hugging face \r\nrepo,\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_id,\
    \ use_fast=False)\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_id, device=\"\
    cpu\", use_triton=False, use_safetensors=True, torch_dtype=torch.float16, trust_remote_code=True)\r\
    \n\r\nNote: I have also changed dtype to floatb16, float32 in model class\r\n\r\
    \n![Screenshot from 2023-12-11 16-38-13.png](https://cdn-uploads.huggingface.co/production/uploads/6549d71f84ac015f9ee7fc4e/F54kVaxgore78aJartGCq.png)\r\
    \npipe = transformers.pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
    \n    tokenizer=tokenizer,\r\n    torch_dtype=torch.float32,\r\n    trust_remote_code=True,\r\
    \n    device_map='auto',\r\n    max_new_tokens=2649,\r\n    )\r\n\r\nWhen I run\
    \ the chain with the input prompt, the code throws the following error,\r\nchain.run('How\
    \ many employees are there?')\r\n\r\n   File \"/home/user/Disk/Langchain/env/lib/python3.10/site-packages/torch/nn/modules/normalization.py\"\
    , line 196, in forward\r\n    return F.layer_norm(\r\n  File \"/home/user/Disk/Langchain/env/lib/python3.10/site-packages/torch/nn/functional.py\"\
    , line 2543, in layer_norm\r\n    return torch.layer_norm(input, normalized_shape,\
    \ weight, bias, eps, torch.backends.cudnn.enabled)\r\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'\r\n\r\nThese are the package versions I'm using,\r\
    \n    langchain  ==  0.0.348\r\n    langchain-core  == 0.0.12\r\n    langchain-experimental\
    \ ==  0.0.44\r\n    langsmith  ==  0.0.69\r\n    transformers  ==  4.33.0\r\n\
    \    accelerate  ==  0.25.0\r\n    auto-gptq  ==  0.5.1\r\n\r\nCould someone help\
    \ with is problem"
  created_at: 2023-12-11 11:39:37+00:00
  edited: false
  hidden: false
  id: 6576f4f98d209fc6a76953a5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: 'RuntimeError: "LayerNormKernelImpl" not implemented for ''Half'''
