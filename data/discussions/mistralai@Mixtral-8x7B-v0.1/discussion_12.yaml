!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cchristophe
conflicting_files: null
created_at: 2023-12-12 10:49:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/628e39f4b1596566033b8d7b/-Y807up1cgMmAQsczdOPn.jpeg?w=200&h=200&f=face
      fullname: "Cl\xE9ment Christophe"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cchristophe
      type: user
    createdAt: '2023-12-12T10:49:57.000Z'
    data:
      edited: false
      editors:
      - cchristophe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8457596898078918
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/628e39f4b1596566033b8d7b/-Y807up1cgMmAQsczdOPn.jpeg?w=200&h=200&f=face
          fullname: "Cl\xE9ment Christophe"
          isHf: false
          isPro: false
          name: cchristophe
          type: user
        html: '<p>Hi I''m trying to finetune Mixtral using FSDP framework and I have
          this error during the first backward pass:<br>Exception:  Attention mask
          should be of size (1, 1, 4096, 8192), but is torch.Size([1, 1, 4096, 4096])</p>

          <p>I''m using the same logic and the same data I used to finetune Mistral
          7B...</p>

          '
        raw: "Hi I'm trying to finetune Mixtral using FSDP framework and I have this\
          \ error during the first backward pass:\r\nException:  Attention mask should\
          \ be of size (1, 1, 4096, 8192), but is torch.Size([1, 1, 4096, 4096])\r\
          \n\r\nI'm using the same logic and the same data I used to finetune Mistral\
          \ 7B..."
        updatedAt: '2023-12-12T10:49:57.833Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - orendar
        - gdlangus
        - sean29
        - AhmadShapiro
        - zxchow
    id: 65783ad5ee40482b5c3901fb
    type: comment
  author: cchristophe
  content: "Hi I'm trying to finetune Mixtral using FSDP framework and I have this\
    \ error during the first backward pass:\r\nException:  Attention mask should be\
    \ of size (1, 1, 4096, 8192), but is torch.Size([1, 1, 4096, 4096])\r\n\r\nI'm\
    \ using the same logic and the same data I used to finetune Mistral 7B..."
  created_at: 2023-12-12 10:49:57+00:00
  edited: false
  hidden: false
  id: 65783ad5ee40482b5c3901fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/678158ed18bf608701674248b95ab8c4.svg
      fullname: Geoffrey Angus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gdlangus
      type: user
    createdAt: '2023-12-13T04:41:45.000Z'
    data:
      edited: false
      editors:
      - gdlangus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9849729537963867
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/678158ed18bf608701674248b95ab8c4.svg
          fullname: Geoffrey Angus
          isHf: false
          isPro: false
          name: gdlangus
          type: user
        html: '<p>Getting this error as well.</p>

          '
        raw: Getting this error as well.
        updatedAt: '2023-12-13T04:41:45.884Z'
      numEdits: 0
      reactions: []
    id: 65793609ee40482b5c667407
    type: comment
  author: gdlangus
  content: Getting this error as well.
  created_at: 2023-12-13 04:41:45+00:00
  edited: false
  hidden: false
  id: 65793609ee40482b5c667407
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-13T09:09:01.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950096845626831
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Thanks, could you open an issue on <code>https://huggingface/transformers</code>
          with a full reproducer? </p>

          '
        raw: 'Thanks, could you open an issue on `https://huggingface/transformers`
          with a full reproducer? '
        updatedAt: '2023-12-13T09:09:01.142Z'
      numEdits: 0
      reactions: []
    id: 657974ad7ee8616c7c4a4bc2
    type: comment
  author: ArthurZ
  content: 'Thanks, could you open an issue on `https://huggingface/transformers`
    with a full reproducer? '
  created_at: 2023-12-13 09:09:01+00:00
  edited: false
  hidden: false
  id: 657974ad7ee8616c7c4a4bc2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-05T20:04:24.000Z'
    data:
      edited: false
      editors:
      - rganti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.847328782081604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: '<p>Is there any corresponding issue?</p>

          '
        raw: Is there any corresponding issue?
        updatedAt: '2024-01-05T20:04:24.862Z'
      numEdits: 0
      reactions: []
    id: 659860c8d108fc317f2e28b9
    type: comment
  author: rganti
  content: Is there any corresponding issue?
  created_at: 2024-01-05 20:04:24+00:00
  edited: false
  hidden: false
  id: 659860c8d108fc317f2e28b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-08T08:35:59.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.894403338432312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>I believe it has been recently fixed by: <a rel="nofollow" href="https://github.com/huggingface/transformers/pull/28061">https://github.com/huggingface/transformers/pull/28061</a><br>You
          can use the main branch of transformers, <code>pip install -U git+https://github.com/huggingface/transformers.git</code></p>

          '
        raw: 'I believe it has been recently fixed by: https://github.com/huggingface/transformers/pull/28061

          You can use the main branch of transformers, `pip install -U git+https://github.com/huggingface/transformers.git`'
        updatedAt: '2024-01-08T08:35:59.681Z'
      numEdits: 0
      reactions: []
    id: 659bb3ef21a7431643373b5f
    type: comment
  author: ybelkada
  content: 'I believe it has been recently fixed by: https://github.com/huggingface/transformers/pull/28061

    You can use the main branch of transformers, `pip install -U git+https://github.com/huggingface/transformers.git`'
  created_at: 2024-01-08 08:35:59+00:00
  edited: false
  hidden: false
  id: 659bb3ef21a7431643373b5f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-08T15:43:01.000Z'
    data:
      edited: false
      editors:
      - rganti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9208390116691589
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> I can confirm\
          \ that with moving to the latest HF as mentioned above, I am able to fine\
          \ tune Mixtral using FSDP. :tada:</p>\n"
        raw: '@ybelkada I can confirm that with moving to the latest HF as mentioned
          above, I am able to fine tune Mixtral using FSDP. :tada:'
        updatedAt: '2024-01-08T15:43:01.109Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 659c1805e06aba975d3c518c
    type: comment
  author: rganti
  content: '@ybelkada I can confirm that with moving to the latest HF as mentioned
    above, I am able to fine tune Mixtral using FSDP. :tada:'
  created_at: 2024-01-08 15:43:01+00:00
  edited: false
  hidden: false
  id: 659c1805e06aba975d3c518c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f727dac392d2357dfd4b7192857d535.svg
      fullname: Hrushikesh Mohapatra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hrushikesh1
      type: user
    createdAt: '2024-01-11T18:25:53.000Z'
    data:
      edited: true
      editors:
      - hrushikesh1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6439951062202454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f727dac392d2357dfd4b7192857d535.svg
          fullname: Hrushikesh Mohapatra
          isHf: false
          isPro: false
          name: hrushikesh1
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rganti&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rganti\">@<span class=\"\
          underline\">rganti</span></a></span>\n\n\t</span></span> Can you please\
          \ share your FSDP config ?<br>I am trying a full fine tuning(not LoRA) \
          \ using<br><code>auto_wrap_policy={MixtralDecoderLayer}, activation_checkpointing_policy={MixtralDecoderLayer}</code><br>according\
          \ to <a rel=\"nofollow\" href=\"https://lightning.ai/docs/fabric/stable/advanced/model_parallel/fsdp.html\"\
          >https://lightning.ai/docs/fabric/stable/advanced/model_parallel/fsdp.html</a></p>\n\
          <p>It is giving me recomputed tensor size mismatch error. A detailed bug\
          \ report is <a rel=\"nofollow\" href=\"https://github.com/Lightning-AI/pytorch-lightning/issues/19267\"\
          >here </a><br>FYI: I tried the latest transformer and lightning library\
          \ installed from git+https</p>\n"
        raw: "@rganti Can you please share your FSDP config ?\nI am trying a full\
          \ fine tuning(not LoRA)  using \n`auto_wrap_policy={MixtralDecoderLayer},\
          \ activation_checkpointing_policy={MixtralDecoderLayer}` \naccording to\
          \ https://lightning.ai/docs/fabric/stable/advanced/model_parallel/fsdp.html\n\
          \nIt is giving me recomputed tensor size mismatch error. A detailed bug\
          \ report is [here ](https://github.com/Lightning-AI/pytorch-lightning/issues/19267)\n\
          FYI: I tried the latest transformer and lightning library installed from\
          \ git+https"
        updatedAt: '2024-01-11T18:27:39.376Z'
      numEdits: 1
      reactions: []
    id: 65a032b12df784436610d95c
    type: comment
  author: hrushikesh1
  content: "@rganti Can you please share your FSDP config ?\nI am trying a full fine\
    \ tuning(not LoRA)  using \n`auto_wrap_policy={MixtralDecoderLayer}, activation_checkpointing_policy={MixtralDecoderLayer}`\
    \ \naccording to https://lightning.ai/docs/fabric/stable/advanced/model_parallel/fsdp.html\n\
    \nIt is giving me recomputed tensor size mismatch error. A detailed bug report\
    \ is [here ](https://github.com/Lightning-AI/pytorch-lightning/issues/19267)\n\
    FYI: I tried the latest transformer and lightning library installed from git+https"
  created_at: 2024-01-11 18:25:53+00:00
  edited: true
  hidden: false
  id: 65a032b12df784436610d95c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-11T18:37:47.000Z'
    data:
      edited: false
      editors:
      - rganti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.203581303358078
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: "<pre><code>{\n  \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\"\
          ,\n  \"fsdp_backward_prefetch_policy\": \"BACKWARD_PRE\",\n  \"fsdp_cpu_ram_efficient_loading\"\
          : \"False\",\n  \"fsdp_forward_prefetch\": \"True\",\n  \"fsdp_offload_params\"\
          : \"False\",\n  \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n  \"\
          fsdp_sync_module_states\": \"False\",\n  \"fsdp_transformer_layer_cls_to_wrap\"\
          : \"MixtralDecoderLayer\",\n  \"fsdp_use_orig_params\": \"True\",\n  \"\
          activation_checkpointing\": \"True\"\n}\n</code></pre>\n<p>I am using <code>SFTTrainer</code></p>\n"
        raw: "```\n{\n  \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n \
          \ \"fsdp_backward_prefetch_policy\": \"BACKWARD_PRE\",\n  \"fsdp_cpu_ram_efficient_loading\"\
          : \"False\",\n  \"fsdp_forward_prefetch\": \"True\",\n  \"fsdp_offload_params\"\
          : \"False\",\n  \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n  \"\
          fsdp_sync_module_states\": \"False\",\n  \"fsdp_transformer_layer_cls_to_wrap\"\
          : \"MixtralDecoderLayer\",\n  \"fsdp_use_orig_params\": \"True\",\n  \"\
          activation_checkpointing\": \"True\"\n}\n```\n\nI am using `SFTTrainer`"
        updatedAt: '2024-01-11T18:37:47.165Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - hrushikesh1
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hrushikesh1
    id: 65a0357b26d1e9df4fbeb66f
    type: comment
  author: rganti
  content: "```\n{\n  \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n  \"\
    fsdp_backward_prefetch_policy\": \"BACKWARD_PRE\",\n  \"fsdp_cpu_ram_efficient_loading\"\
    : \"False\",\n  \"fsdp_forward_prefetch\": \"True\",\n  \"fsdp_offload_params\"\
    : \"False\",\n  \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n  \"fsdp_sync_module_states\"\
    : \"False\",\n  \"fsdp_transformer_layer_cls_to_wrap\": \"MixtralDecoderLayer\"\
    ,\n  \"fsdp_use_orig_params\": \"True\",\n  \"activation_checkpointing\": \"True\"\
    \n}\n```\n\nI am using `SFTTrainer`"
  created_at: 2024-01-11 18:37:47+00:00
  edited: false
  hidden: false
  id: 65a0357b26d1e9df4fbeb66f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-11T19:32:58.000Z'
    data:
      edited: false
      editors:
      - rganti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8439139127731323
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: "<p>btw <span data-props=\"{&quot;user&quot;:&quot;hrushikesh1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hrushikesh1\"\
          >@<span class=\"underline\">hrushikesh1</span></a></span>\n\n\t</span></span>\
          \ -- some other model (<code>GPTBigCode</code>) is giving me this trouble\
          \ (size/shape mismatch), it used to work well in the past for me :) </p>\n"
        raw: 'btw @hrushikesh1 -- some other model (`GPTBigCode`) is giving me this
          trouble (size/shape mismatch), it used to work well in the past for me :) '
        updatedAt: '2024-01-11T19:32:58.641Z'
      numEdits: 0
      reactions: []
    id: 65a0426ac2a026427af1a9ff
    type: comment
  author: rganti
  content: 'btw @hrushikesh1 -- some other model (`GPTBigCode`) is giving me this
    trouble (size/shape mismatch), it used to work well in the past for me :) '
  created_at: 2024-01-11 19:32:58+00:00
  edited: false
  hidden: false
  id: 65a0426ac2a026427af1a9ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-12T18:26:10.000Z'
    data:
      edited: false
      editors:
      - rganti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hrushikesh1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hrushikesh1\"\
          >@<span class=\"underline\">hrushikesh1</span></a></span>\n\n\t</span></span>\
          \ To update, it seems to be flaky and dependent on the PyTorch and HF versions\
          \ that are installed. I am still trying to figure out the \"right\" combination,\
          \ but perhaps <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ or someone from HF/PT teams can comment?</p>\n"
        raw: '@hrushikesh1 To update, it seems to be flaky and dependent on the PyTorch
          and HF versions that are installed. I am still trying to figure out the
          "right" combination, but perhaps @ybelkada or someone from HF/PT teams can
          comment?'
        updatedAt: '2024-01-12T18:26:10.560Z'
      numEdits: 0
      reactions: []
    id: 65a184428d74307b9bd722e6
    type: comment
  author: rganti
  content: '@hrushikesh1 To update, it seems to be flaky and dependent on the PyTorch
    and HF versions that are installed. I am still trying to figure out the "right"
    combination, but perhaps @ybelkada or someone from HF/PT teams can comment?'
  created_at: 2024-01-12 18:26:10+00:00
  edited: false
  hidden: false
  id: 65a184428d74307b9bd722e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-12T18:41:27.000Z'
    data:
      edited: true
      editors:
      - rganti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6237497925758362
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: '<p>specifically, using torch version <code>2.2.0.dev20231121+cu118</code>
          and transformers is <code>4.37.0.dev0</code> and python is <code>3.11</code></p>

          '
        raw: specifically, using torch version `2.2.0.dev20231121+cu118` and transformers
          is `4.37.0.dev0` and python is `3.11`
        updatedAt: '2024-01-12T18:42:47.451Z'
      numEdits: 1
      reactions: []
    id: 65a187d71d0037b6cedb57f5
    type: comment
  author: rganti
  content: specifically, using torch version `2.2.0.dev20231121+cu118` and transformers
    is `4.37.0.dev0` and python is `3.11`
  created_at: 2024-01-12 18:41:27+00:00
  edited: true
  hidden: false
  id: 65a187d71d0037b6cedb57f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f727dac392d2357dfd4b7192857d535.svg
      fullname: Hrushikesh Mohapatra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hrushikesh1
      type: user
    createdAt: '2024-01-13T01:07:37.000Z'
    data:
      edited: false
      editors:
      - hrushikesh1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7731427550315857
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f727dac392d2357dfd4b7192857d535.svg
          fullname: Hrushikesh Mohapatra
          isHf: false
          isPro: false
          name: hrushikesh1
          type: user
        html: "<p>Thanks for the info <span data-props=\"{&quot;user&quot;:&quot;rganti&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rganti\"\
          >@<span class=\"underline\">rganti</span></a></span>\n\n\t</span></span>\
          \ !<br>I was able to solve it by explicitly calling<br><code>model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':\
          \ True})</code><br>after  model load <code>AutoModel.from_pretrained()</code>.</p>\n\
          <p>The issue re-appears if I set <code>use_reentrant:False</code> in the\
          \ above call. Lightning library might be defaulting to use_reentrant:False.</p>\n\
          <p>There is lot of notes and warning from pytorch on the renentrant behavior\
          \ <a rel=\"nofollow\" href=\"https://pytorch.org/docs/stable/checkpoint.html\"\
          >here</a><br>As of torch 2.1 it defaults to True, but they plan to move\
          \ to use_reentrant=False as a default in future, that might be causing the\
          \ flakiness you observe across versions</p>\n"
        raw: "Thanks for the info @rganti ! \nI was able to solve it by explicitly\
          \ calling \n`model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':\
          \ True})` \nafter  model load `AutoModel.from_pretrained()`.\n\nThe issue\
          \ re-appears if I set `use_reentrant:False` in the above call. Lightning\
          \ library might be defaulting to use_reentrant:False.\n\nThere is lot of\
          \ notes and warning from pytorch on the renentrant behavior [here](https://pytorch.org/docs/stable/checkpoint.html)\n\
          As of torch 2.1 it defaults to True, but they plan to move to use_reentrant=False\
          \ as a default in future, that might be causing the flakiness you observe\
          \ across versions"
        updatedAt: '2024-01-13T01:07:37.817Z'
      numEdits: 0
      reactions: []
    id: 65a1e259e8fe70d60cc4dd39
    type: comment
  author: hrushikesh1
  content: "Thanks for the info @rganti ! \nI was able to solve it by explicitly calling\
    \ \n`model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant':\
    \ True})` \nafter  model load `AutoModel.from_pretrained()`.\n\nThe issue re-appears\
    \ if I set `use_reentrant:False` in the above call. Lightning library might be\
    \ defaulting to use_reentrant:False.\n\nThere is lot of notes and warning from\
    \ pytorch on the renentrant behavior [here](https://pytorch.org/docs/stable/checkpoint.html)\n\
    As of torch 2.1 it defaults to True, but they plan to move to use_reentrant=False\
    \ as a default in future, that might be causing the flakiness you observe across\
    \ versions"
  created_at: 2024-01-13 01:07:37+00:00
  edited: false
  hidden: false
  id: 65a1e259e8fe70d60cc4dd39
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
      fullname: Raghu Ganti
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rganti
      type: user
    createdAt: '2024-01-16T19:35:21.000Z'
    data:
      edited: false
      editors:
      - rganti
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9118931293487549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7eb33d86f5373a4b3c16b72bfbd4cb0d.svg
          fullname: Raghu Ganti
          isHf: false
          isPro: false
          name: rganti
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hrushikesh1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hrushikesh1\"\
          >@<span class=\"underline\">hrushikesh1</span></a></span>\n\n\t</span></span>\
          \ I was able to Lora tune mixtral on the latest PT nightlies and latest\
          \ HF main after adding the above line, thanks!</p>\n"
        raw: '@hrushikesh1 I was able to Lora tune mixtral on the latest PT nightlies
          and latest HF main after adding the above line, thanks!'
        updatedAt: '2024-01-16T19:35:21.414Z'
      numEdits: 0
      reactions: []
    id: 65a6da791a2b0fa27b15d4cb
    type: comment
  author: rganti
  content: '@hrushikesh1 I was able to Lora tune mixtral on the latest PT nightlies
    and latest HF main after adding the above line, thanks!'
  created_at: 2024-01-16 19:35:21+00:00
  edited: false
  hidden: false
  id: 65a6da791a2b0fa27b15d4cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5f727dac392d2357dfd4b7192857d535.svg
      fullname: Hrushikesh Mohapatra
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hrushikesh1
      type: user
    createdAt: '2024-01-19T00:15:09.000Z'
    data:
      edited: false
      editors:
      - hrushikesh1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9012038707733154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5f727dac392d2357dfd4b7192857d535.svg
          fullname: Hrushikesh Mohapatra
          isHf: false
          isPro: false
          name: hrushikesh1
          type: user
        html: '<p>Wanted to share a note for a future data scientist in trouble:<br>I
          was trying <code>LORA</code> fine tuning of Mistral-7B using <code>FSDP</code>
          strategy and <code>pytorch lighting </code>trainer. It used to get stuck
          at Step-1.<br>Turned out, since there are some frozen parameters without
          gradients, I can not use gradient_clipping.</p>

          '
        raw: "Wanted to share a note for a future data scientist in trouble:\nI was\
          \ trying `LORA` fine tuning of Mistral-7B using `FSDP` strategy and `pytorch\
          \ lighting `trainer. It used to get stuck at Step-1. \nTurned out, since\
          \ there are some frozen parameters without gradients, I can not use gradient_clipping."
        updatedAt: '2024-01-19T00:15:09.003Z'
      numEdits: 0
      reactions: []
    id: 65a9bf0df9e55b5f7548f8da
    type: comment
  author: hrushikesh1
  content: "Wanted to share a note for a future data scientist in trouble:\nI was\
    \ trying `LORA` fine tuning of Mistral-7B using `FSDP` strategy and `pytorch lighting\
    \ `trainer. It used to get stuck at Step-1. \nTurned out, since there are some\
    \ frozen parameters without gradients, I can not use gradient_clipping."
  created_at: 2024-01-19 00:15:09+00:00
  edited: false
  hidden: false
  id: 65a9bf0df9e55b5f7548f8da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: FSDP Finetuning
