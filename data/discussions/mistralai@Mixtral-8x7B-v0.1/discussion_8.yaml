!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mauceric
conflicting_files: null
created_at: 2023-12-11 20:33:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676198180316-5e2034f3691aad406a803a22.jpeg?w=200&h=200&f=face
      fullname: Christian Mauceri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mauceric
      type: user
    createdAt: '2023-12-11T20:33:05.000Z'
    data:
      edited: false
      editors:
      - Mauceric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36108773946762085
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676198180316-5e2034f3691aad406a803a22.jpeg?w=200&h=200&f=face
          fullname: Christian Mauceri
          isHf: false
          isPro: false
          name: Mauceric
          type: user
        html: '<p>The following instructions :</p>

          <blockquote>

          <blockquote>

          <blockquote>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer<br>model_id
          = "mistralai/Mixtral-8x7B-v0.1"<br>model = AutoModelForCausalLM.from_pretrained(model_id)<br>Raise
          the exception :<br>File "", line 1, in <br>  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py",
          line 527, in from_pretrained<br>    config, kwargs = AutoConfig.from_pretrained(<br>                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py",
          line 1048, in from_pretrained<br>    config_class = CONFIG_MAPPING[config_dict["model_type"]]<br>                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py",
          line 743, in <strong>getitem</strong><br>    raise KeyError(key)<br>KeyError:
          ''mixtral''<br>What have I done wrong ?</p>

          </blockquote>

          </blockquote>

          </blockquote>

          '
        raw: "The following instructions :\r\n>>>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\n>>> model_id = \"mistralai/Mixtral-8x7B-v0.1\"\r\n>>>model\
          \ = AutoModelForCausalLM.from_pretrained(model_id)\r\nRaise the exception\
          \ :\r\nFile \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 527, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
          \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 1048, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"\
          model_type\"]]\r\n                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
          , line 743, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'mixtral'\r\
          \nWhat have I done wrong ?\r\n"
        updatedAt: '2023-12-11T20:33:05.793Z'
      numEdits: 0
      reactions: []
    id: 65777201821f439498b9f4da
    type: comment
  author: Mauceric
  content: "The following instructions :\r\n>>>from transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\n>>> model_id = \"mistralai/Mixtral-8x7B-v0.1\"\r\n>>>model\
    \ = AutoModelForCausalLM.from_pretrained(model_id)\r\nRaise the exception :\r\n\
    File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 527, in from_pretrained\r\n    config, kwargs = AutoConfig.from_pretrained(\r\
    \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 1048, in from_pretrained\r\n    config_class = CONFIG_MAPPING[config_dict[\"\
    model_type\"]]\r\n                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\"\
    , line 743, in __getitem__\r\n    raise KeyError(key)\r\nKeyError: 'mixtral'\r\
    \nWhat have I done wrong ?\r\n"
  created_at: 2023-12-11 20:33:05+00:00
  edited: false
  hidden: false
  id: 65777201821f439498b9f4da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-11T20:59:36.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7798667550086975
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>You did not install transformers==4.36.0 .... <code>pip install
          -U transformers</code></p>

          '
        raw: You did not install transformers==4.36.0 .... `pip install -U transformers`
        updatedAt: '2023-12-11T20:59:36.304Z'
      numEdits: 0
      reactions: []
    id: 65777838353869cd6ed8a552
    type: comment
  author: ArthurZ
  content: You did not install transformers==4.36.0 .... `pip install -U transformers`
  created_at: 2023-12-11 20:59:36+00:00
  edited: false
  hidden: false
  id: 65777838353869cd6ed8a552
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/261a7b2ac1495435c8cb228d30ab8c61.svg
      fullname: Dragan Stoll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dragstoll
      type: user
    createdAt: '2023-12-11T21:09:22.000Z'
    data:
      edited: false
      editors:
      - dragstoll
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9390875697135925
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/261a7b2ac1495435c8cb228d30ab8c61.svg
          fullname: Dragan Stoll
          isHf: false
          isPro: false
          name: dragstoll
          type: user
        html: '<p>thank you Arthur, I updated the transformers via github and it didn''t
          work, the dev version is not up to date</p>

          '
        raw: 'thank you Arthur, I updated the transformers via github and it didn''t
          work, the dev version is not up to date

          '
        updatedAt: '2023-12-11T21:09:22.723Z'
      numEdits: 0
      reactions: []
    id: 65777a82dd2996f01ad209a3
    type: comment
  author: dragstoll
  content: 'thank you Arthur, I updated the transformers via github and it didn''t
    work, the dev version is not up to date

    '
  created_at: 2023-12-11 21:09:22+00:00
  edited: false
  hidden: false
  id: 65777a82dd2996f01ad209a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45230d587ae8514807c94833bffdfb2f.svg
      fullname: Sachin Patil
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sacdroid
      type: user
    createdAt: '2023-12-12T00:16:48.000Z'
    data:
      edited: false
      editors:
      - sacdroid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8690519332885742
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45230d587ae8514807c94833bffdfb2f.svg
          fullname: Sachin Patil
          isHf: false
          isPro: false
          name: sacdroid
          type: user
        html: '<p>It is working for me with <code>RUN pip install git+https://github.com/huggingface/transformers.git</code></p>

          '
        raw: It is working for me with `RUN pip install git+https://github.com/huggingface/transformers.git`
        updatedAt: '2023-12-12T00:16:48.711Z'
      numEdits: 0
      reactions: []
    id: 6577a670411e14898b7b49b2
    type: comment
  author: sacdroid
  content: It is working for me with `RUN pip install git+https://github.com/huggingface/transformers.git`
  created_at: 2023-12-12 00:16:48+00:00
  edited: false
  hidden: false
  id: 6577a670411e14898b7b49b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676198180316-5e2034f3691aad406a803a22.jpeg?w=200&h=200&f=face
      fullname: Christian Mauceri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mauceric
      type: user
    createdAt: '2023-12-12T09:26:15.000Z'
    data:
      edited: false
      editors:
      - Mauceric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5416896343231201
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676198180316-5e2034f3691aad406a803a22.jpeg?w=200&h=200&f=face
          fullname: Christian Mauceri
          isHf: false
          isPro: false
          name: Mauceric
          type: user
        html: '<p>Thanks ArthurZ, thanks Sacdroid :)</p>

          '
        raw: Thanks ArthurZ, thanks Sacdroid :)
        updatedAt: '2023-12-12T09:26:15.827Z'
      numEdits: 0
      reactions: []
    id: 6578273758d7a2cc893d60d7
    type: comment
  author: Mauceric
  content: Thanks ArthurZ, thanks Sacdroid :)
  created_at: 2023-12-12 09:26:15+00:00
  edited: false
  hidden: false
  id: 6578273758d7a2cc893d60d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cc768d93be0e14ecb4d3b24dd6ad16d.svg
      fullname: sanjog mohanty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sanjog21
      type: user
    createdAt: '2023-12-12T17:40:00.000Z'
    data:
      edited: false
      editors:
      - sanjog21
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3398388624191284
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cc768d93be0e14ecb4d3b24dd6ad16d.svg
          fullname: sanjog mohanty
          isHf: false
          isPro: false
          name: sanjog21
          type: user
        html: "<hr>\n<p>KeyError                                  Traceback (most\
          \ recent call last)<br>Cell In[5], line 6<br>      3 model_id = \"mistralai/Mixtral-8x7B-v0.1\"\
          <br>      4 tokenizer = AutoTokenizer.from_pretrained(model_id)<br>----&gt;\
          \ 6 model = AutoModelForCausalLM.from_pretrained(model_id)<br>      8 text\
          \ = \"Hello my name is\"<br>      9 inputs = tokenizer(text, return_tensors=\"\
          pt\")</p>\n<p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)<br>    523 if kwargs.get(\"quantization_config\"\
          , None) is not None:<br>    524     _ = kwargs.pop(\"quantization_config\"\
          )<br>--&gt; 526 config, kwargs = AutoConfig.from_pretrained(<br>    527\
          \     pretrained_model_name_or_path,<br>    528     return_unused_kwargs=True,<br>\
          \    529     trust_remote_code=trust_remote_code,<br>    530     code_revision=code_revision,<br>\
          \    531     _commit_hash=commit_hash,<br>    532     **hub_kwargs,<br>\
          \    533     **kwargs,<br>    534 )<br>    536 # if torch_dtype=auto was\
          \ passed here, ensure to pass it on<br>    537 if kwargs_orig.get(\"torch_dtype\"\
          , None) == \"auto\":</p>\n<p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)<br>\
          \   1062     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)<br>   1063 elif \"model_type\" in config_dict:<br>-&gt; 1064\
          \     config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]<br>   1065\
          \     return config_class.from_dict(config_dict, **unused_kwargs)<br>  \
          \ 1066 else:<br>   1067     # Fallback: use pattern matching on the string.<br>\
          \   1068     # We go from longer names to shorter names to catch roberta\
          \ before bert (for instance)</p>\n<p>File /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761,\
          \ in _LazyConfigMapping.<strong>getitem</strong>(self, key)<br>    759 \
          \    return self._extra_content[key]<br>    760 if key not in self._mapping:<br>--&gt;\
          \ 761     raise KeyError(key)<br>    762 value = self._mapping[key]<br>\
          \    763 module_name = model_type_to_module_name(key)</p>\n<h2 id=\"keyerror-mixtral\"\
          >KeyError: 'mixtral'</h2>\n<p>I have tried everything which is mentioned\
          \ in the above comments but still I'm getting this error\n </p>\n"
        raw: "---------------------------------------------------------------------------\n\
          KeyError                                  Traceback (most recent call last)\n\
          Cell In[5], line 6\n      3 model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n\
          \      4 tokenizer = AutoTokenizer.from_pretrained(model_id)\n----> 6 model\
          \ = AutoModelForCausalLM.from_pretrained(model_id)\n      8 text = \"Hello\
          \ my name is\"\n      9 inputs = tokenizer(text, return_tensors=\"pt\")\n\
          \nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    523 if kwargs.get(\"quantization_config\"\
          , None) is not None:\n    524     _ = kwargs.pop(\"quantization_config\"\
          )\n--> 526 config, kwargs = AutoConfig.from_pretrained(\n    527     pretrained_model_name_or_path,\n\
          \    528     return_unused_kwargs=True,\n    529     trust_remote_code=trust_remote_code,\n\
          \    530     code_revision=code_revision,\n    531     _commit_hash=commit_hash,\n\
          \    532     **hub_kwargs,\n    533     **kwargs,\n    534 )\n    536 #\
          \ if torch_dtype=auto was passed here, ensure to pass it on\n    537 if\
          \ kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064,\
          \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
          \   1062     return config_class.from_pretrained(pretrained_model_name_or_path,\
          \ **kwargs)\n   1063 elif \"model_type\" in config_dict:\n-> 1064     config_class\
          \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n   1065     return config_class.from_dict(config_dict,\
          \ **unused_kwargs)\n   1066 else:\n   1067     # Fallback: use pattern matching\
          \ on the string.\n   1068     # We go from longer names to shorter names\
          \ to catch roberta before bert (for instance)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761,\
          \ in _LazyConfigMapping.__getitem__(self, key)\n    759     return self._extra_content[key]\n\
          \    760 if key not in self._mapping:\n--> 761     raise KeyError(key)\n\
          \    762 value = self._mapping[key]\n    763 module_name = model_type_to_module_name(key)\n\
          \nKeyError: 'mixtral'\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
          I have tried everything which is mentioned in the above comments but still\
          \ I'm getting this error\n "
        updatedAt: '2023-12-12T17:40:00.128Z'
      numEdits: 0
      reactions: []
    id: 65789af07f5da1deb68c17ce
    type: comment
  author: sanjog21
  content: "---------------------------------------------------------------------------\n\
    KeyError                                  Traceback (most recent call last)\n\
    Cell In[5], line 6\n      3 model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n     \
    \ 4 tokenizer = AutoTokenizer.from_pretrained(model_id)\n----> 6 model = AutoModelForCausalLM.from_pretrained(model_id)\n\
    \      8 text = \"Hello my name is\"\n      9 inputs = tokenizer(text, return_tensors=\"\
    pt\")\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:526,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n    523 if kwargs.get(\"quantization_config\", None) is not None:\n\
    \    524     _ = kwargs.pop(\"quantization_config\")\n--> 526 config, kwargs =\
    \ AutoConfig.from_pretrained(\n    527     pretrained_model_name_or_path,\n  \
    \  528     return_unused_kwargs=True,\n    529     trust_remote_code=trust_remote_code,\n\
    \    530     code_revision=code_revision,\n    531     _commit_hash=commit_hash,\n\
    \    532     **hub_kwargs,\n    533     **kwargs,\n    534 )\n    536 # if torch_dtype=auto\
    \ was passed here, ensure to pass it on\n    537 if kwargs_orig.get(\"torch_dtype\"\
    , None) == \"auto\":\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1064,\
    \ in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n\
    \   1062     return config_class.from_pretrained(pretrained_model_name_or_path,\
    \ **kwargs)\n   1063 elif \"model_type\" in config_dict:\n-> 1064     config_class\
    \ = CONFIG_MAPPING[config_dict[\"model_type\"]]\n   1065     return config_class.from_dict(config_dict,\
    \ **unused_kwargs)\n   1066 else:\n   1067     # Fallback: use pattern matching\
    \ on the string.\n   1068     # We go from longer names to shorter names to catch\
    \ roberta before bert (for instance)\n\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:761,\
    \ in _LazyConfigMapping.__getitem__(self, key)\n    759     return self._extra_content[key]\n\
    \    760 if key not in self._mapping:\n--> 761     raise KeyError(key)\n    762\
    \ value = self._mapping[key]\n    763 module_name = model_type_to_module_name(key)\n\
    \nKeyError: 'mixtral'\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\
    I have tried everything which is mentioned in the above comments but still I'm\
    \ getting this error\n "
  created_at: 2023-12-12 17:40:00+00:00
  edited: false
  hidden: false
  id: 65789af07f5da1deb68c17ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-12T22:26:53.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8859860301017761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sanjog21&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/sanjog21\">@<span class=\"\
          underline\">sanjog21</span></a></span>\n\n\t</span></span> can you try to\
          \ re-install transformers in a fresh new env?</p>\n"
        raw: '@sanjog21 can you try to re-install transformers in a fresh new env?'
        updatedAt: '2023-12-12T22:26:53.650Z'
      numEdits: 0
      reactions: []
    id: 6578de2de7880298b0e814a3
    type: comment
  author: ybelkada
  content: '@sanjog21 can you try to re-install transformers in a fresh new env?'
  created_at: 2023-12-12 22:26:53+00:00
  edited: false
  hidden: false
  id: 6578de2de7880298b0e814a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dccba59060ce07efab212454077a9bc0.svg
      fullname: Yarden Hochenberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yardenhoch
      type: user
    createdAt: '2023-12-27T09:45:11.000Z'
    data:
      edited: false
      editors:
      - yardenhoch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9088996648788452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dccba59060ce07efab212454077a9bc0.svg
          fullname: Yarden Hochenberg
          isHf: false
          isPro: false
          name: yardenhoch
          type: user
        html: '<p>Hi,</p>

          <p>I am currently trying to load the <code>mistralai/Mixtral-8x7B-v0.1</code>
          model using Hugging Face, but I am encountering some issues. I noticed a
          note stating that "the model cannot (yet) be instantiated with HF." However,
          I have seen that you have managed to load this model despite this comment.
          </p>

          <p>When I attempt to load the model <code>AutoModelForCausalLM.from_pretrained(model_id)</code>,
          I receive the following error:<br><code>OSError: mistralai/Mixtral-8x7B-v0.1
          does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt
          or flax_model.msgpack.</code></p>

          <p>I would appreciate any guidance on how to properly load this model. Is
          there a specific method or workaround to load this model using Hugging Face
          despite the aforementioned comment?</p>

          <p>Thank you in advance for your help.</p>

          '
        raw: "Hi,\n\nI am currently trying to load the `mistralai/Mixtral-8x7B-v0.1`\
          \ model using Hugging Face, but I am encountering some issues. I noticed\
          \ a note stating that \"the model cannot (yet) be instantiated with HF.\"\
          \ However, I have seen that you have managed to load this model despite\
          \ this comment. \n\nWhen I attempt to load the model `AutoModelForCausalLM.from_pretrained(model_id)`,\
          \ I receive the following error:\n`OSError: mistralai/Mixtral-8x7B-v0.1\
          \ does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
          \ or flax_model.msgpack.`\n\nI would appreciate any guidance on how to properly\
          \ load this model. Is there a specific method or workaround to load this\
          \ model using Hugging Face despite the aforementioned comment?\n\nThank\
          \ you in advance for your help."
        updatedAt: '2023-12-27T09:45:11.760Z'
      numEdits: 0
      reactions: []
    id: 658bf2275c6fb5d5e3304330
    type: comment
  author: yardenhoch
  content: "Hi,\n\nI am currently trying to load the `mistralai/Mixtral-8x7B-v0.1`\
    \ model using Hugging Face, but I am encountering some issues. I noticed a note\
    \ stating that \"the model cannot (yet) be instantiated with HF.\" However, I\
    \ have seen that you have managed to load this model despite this comment. \n\n\
    When I attempt to load the model `AutoModelForCausalLM.from_pretrained(model_id)`,\
    \ I receive the following error:\n`OSError: mistralai/Mixtral-8x7B-v0.1 does not\
    \ appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.`\n\
    \nI would appreciate any guidance on how to properly load this model. Is there\
    \ a specific method or workaround to load this model using Hugging Face despite\
    \ the aforementioned comment?\n\nThank you in advance for your help."
  created_at: 2023-12-27 09:45:11+00:00
  edited: false
  hidden: false
  id: 658bf2275c6fb5d5e3304330
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-08T08:37:30.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9114798903465271
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;yardenhoch&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yardenhoch\"\
          >@<span class=\"underline\">yardenhoch</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for the comment! I suspect you most likely have an outdated version of\
          \ transformers that do not support <code>safetensors</code> serialization\
          \ format. Can you try to upgrade your transformers version? <code>pip install\
          \ -U transformers</code> this should solve your issue</p>\n"
        raw: "Hi @yardenhoch \nThanks for the comment! I suspect you most likely have\
          \ an outdated version of transformers that do not support `safetensors`\
          \ serialization format. Can you try to upgrade your transformers version?\
          \ `pip install -U transformers` this should solve your issue"
        updatedAt: '2024-01-08T08:37:30.959Z'
      numEdits: 0
      reactions: []
    id: 659bb44a07198ffcf745b5d4
    type: comment
  author: ybelkada
  content: "Hi @yardenhoch \nThanks for the comment! I suspect you most likely have\
    \ an outdated version of transformers that do not support `safetensors` serialization\
    \ format. Can you try to upgrade your transformers version? `pip install -U transformers`\
    \ this should solve your issue"
  created_at: 2024-01-08 08:37:30+00:00
  edited: false
  hidden: false
  id: 659bb44a07198ffcf745b5d4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: AutoModelForCausalLM does not seem to work for Mixtral
