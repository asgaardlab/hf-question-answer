!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hoioi
conflicting_files: null
created_at: 2023-12-11 16:26:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
      fullname: Hut hiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hoioi
      type: user
    createdAt: '2023-12-11T16:26:41.000Z'
    data:
      edited: false
      editors:
      - Hoioi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9788418412208557
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d380e709fe0e95f8bb1684e961549013.svg
          fullname: Hut hiu
          isHf: false
          isPro: false
          name: Hoioi
          type: user
        html: '<p>I''m not sure if my question makes sense, but I was wondering if
          it would be possible to release an English-only version of the model as
          well, with fewer parameters than 46B, and a smaller size that would be more
          useful for most users.</p>

          '
        raw: I'm not sure if my question makes sense, but I was wondering if it would
          be possible to release an English-only version of the model as well, with
          fewer parameters than 46B, and a smaller size that would be more useful
          for most users.
        updatedAt: '2023-12-11T16:26:41.981Z'
      numEdits: 0
      reactions: []
    id: 6577384152f02732a461af98
    type: comment
  author: Hoioi
  content: I'm not sure if my question makes sense, but I was wondering if it would
    be possible to release an English-only version of the model as well, with fewer
    parameters than 46B, and a smaller size that would be more useful for most users.
  created_at: 2023-12-11 16:26:41+00:00
  edited: false
  hidden: false
  id: 6577384152f02732a461af98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-13T21:34:04.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9508417248725891
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Hoioi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Hoioi\">@<span class=\"\
          underline\">Hoioi</span></a></span>\n\n\t</span></span> I want this to,\
          \ but it will never happen.</p>\n<p>Mistral is a French company looking\
          \ for EU dominance in the AI market, so their smartest path forward is with\
          \ multi-lingual LLMs.</p>\n<p>Plus 46b parameters is already too few for\
          \ the mixture of experts x8 design, which is one of the reasons it performs\
          \ so poorly on Arc despite its size (66 vs GPT3.5's 86). It basically only\
          \ has the intelligence of an ~14b dense (non-MOE) Mistral, and the knowledge\
          \ of an ~20b dense Mistral, yet requires the RAM of a 46b dense Mistral.\
          \ However, they need to move forward with MOE so they can host to more users\
          \ with fewer resources.</p>\n"
        raw: '@Hoioi I want this to, but it will never happen.


          Mistral is a French company looking for EU dominance in the AI market, so
          their smartest path forward is with multi-lingual LLMs.


          Plus 46b parameters is already too few for the mixture of experts x8 design,
          which is one of the reasons it performs so poorly on Arc despite its size
          (66 vs GPT3.5''s 86). It basically only has the intelligence of an ~14b
          dense (non-MOE) Mistral, and the knowledge of an ~20b dense Mistral, yet
          requires the RAM of a 46b dense Mistral. However, they need to move forward
          with MOE so they can host to more users with fewer resources.'
        updatedAt: '2023-12-13T21:34:04.068Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Hoioi
    id: 657a234cf10a98415096bf07
    type: comment
  author: Phil337
  content: '@Hoioi I want this to, but it will never happen.


    Mistral is a French company looking for EU dominance in the AI market, so their
    smartest path forward is with multi-lingual LLMs.


    Plus 46b parameters is already too few for the mixture of experts x8 design, which
    is one of the reasons it performs so poorly on Arc despite its size (66 vs GPT3.5''s
    86). It basically only has the intelligence of an ~14b dense (non-MOE) Mistral,
    and the knowledge of an ~20b dense Mistral, yet requires the RAM of a 46b dense
    Mistral. However, they need to move forward with MOE so they can host to more
    users with fewer resources.'
  created_at: 2023-12-13 21:34:04+00:00
  edited: false
  hidden: false
  id: 657a234cf10a98415096bf07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-13T22:35:39.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.957017719745636
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Phil337&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Phil337\">@<span class=\"\
          underline\">Phil337</span></a></span>\n\n\t</span></span> hmm no it has\
          \ the knowledge of a 47b model? In inference time, you basically use the\
          \ 2 model/layers which are more suited to the task and you should actually\
          \ get better output then a normal 47b model? It beats llama 2 70b which\
          \ was pretty good as well.</p>\n"
        raw: '@Phil337 hmm no it has the knowledge of a 47b model? In inference time,
          you basically use the 2 model/layers which are more suited to the task and
          you should actually get better output then a normal 47b model? It beats
          llama 2 70b which was pretty good as well.'
        updatedAt: '2023-12-13T22:35:39.390Z'
      numEdits: 0
      reactions: []
    id: 657a31bbbdfb77e5c7dac356
    type: comment
  author: YaTharThShaRma999
  content: '@Phil337 hmm no it has the knowledge of a 47b model? In inference time,
    you basically use the 2 model/layers which are more suited to the task and you
    should actually get better output then a normal 47b model? It beats llama 2 70b
    which was pretty good as well.'
  created_at: 2023-12-13 22:35:39+00:00
  edited: false
  hidden: false
  id: 657a31bbbdfb77e5c7dac356
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-13T22:52:41.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9470406770706177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \ MOE, no matter how well it's implemented, will always have significantly\
          \ less information than an equal sized dense LLM.</p>\n<p>This is primarily\
          \ because (1) there's FAR more redundancy within all of the experts. Pretty\
          \ much every expert has the same basic knowledge (and 2) the most ideal\
          \ 2 of 8 experts is not always chosen, so using all 8 experts at once, vs\
          \ just running inference through 2, will return significantly more valid\
          \ information.</p>\n<p>This is all part of the compromise of MOE. Far faster\
          \ inference (=12b) with a small boost in intelligence (<del>=14b), a bigger\
          \ boost in knowledge (=</del>20b), but with far more RAM required (=47b).\
          \ If they ran inference through all 8 at once they can significantly boost\
          \ both intelligence and knowledge (=~26b), but it would run at the speed\
          \ of a 47b LLM.</p>\n<p>The best way to see the knowledge drop is by having\
          \ it recite the lyrics of a popular song. This is hard to store accurately\
          \ within small LLMs. GPT4 is excellent at this, GPT3.5 is good, Llama 2\
          \ 70b &amp; Falcon 180b are OK, Mixtral is bad and Mistral 7b is horrible\
          \ (one line at most).</p>\n<p>And the best way to see the intelligence drop\
          \ is to make it write a joke about two disparate things while forcing a\
          \ header to it doesn't just try to copy an existing joke. For example, \"\
          Write a joke about a horse and computer that begins with On a rainy day.\
          \ Then explain the joke.\" Or just look at the Arc score (only 66 for Mixtral,\
          \ when Mistral 7b scored 60).</p>\n"
        raw: '@YaTharThShaRma999 MOE, no matter how well it''s implemented, will always
          have significantly less information than an equal sized dense LLM.


          This is primarily because (1) there''s FAR more redundancy within all of
          the experts. Pretty much every expert has the same basic knowledge (and
          2) the most ideal 2 of 8 experts is not always chosen, so using all 8 experts
          at once, vs just running inference through 2, will return significantly
          more valid information.


          This is all part of the compromise of MOE. Far faster inference (=12b) with
          a small boost in intelligence (~=14b), a bigger boost in knowledge (=~20b),
          but with far more RAM required (=47b). If they ran inference through all
          8 at once they can significantly boost both intelligence and knowledge (=~26b),
          but it would run at the speed of a 47b LLM.


          The best way to see the knowledge drop is by having it recite the lyrics
          of a popular song. This is hard to store accurately within small LLMs. GPT4
          is excellent at this, GPT3.5 is good, Llama 2 70b & Falcon 180b are OK,
          Mixtral is bad and Mistral 7b is horrible (one line at most).


          And the best way to see the intelligence drop is to make it write a joke
          about two disparate things while forcing a header to it doesn''t just try
          to copy an existing joke. For example, "Write a joke about a horse and computer
          that begins with On a rainy day. Then explain the joke." Or just look at
          the Arc score (only 66 for Mixtral, when Mistral 7b scored 60).


          '
        updatedAt: '2023-12-13T22:52:41.544Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jlzhou
    id: 657a35b9b0e3ccd993478032
    type: comment
  author: Phil337
  content: '@YaTharThShaRma999 MOE, no matter how well it''s implemented, will always
    have significantly less information than an equal sized dense LLM.


    This is primarily because (1) there''s FAR more redundancy within all of the experts.
    Pretty much every expert has the same basic knowledge (and 2) the most ideal 2
    of 8 experts is not always chosen, so using all 8 experts at once, vs just running
    inference through 2, will return significantly more valid information.


    This is all part of the compromise of MOE. Far faster inference (=12b) with a
    small boost in intelligence (~=14b), a bigger boost in knowledge (=~20b), but
    with far more RAM required (=47b). If they ran inference through all 8 at once
    they can significantly boost both intelligence and knowledge (=~26b), but it would
    run at the speed of a 47b LLM.


    The best way to see the knowledge drop is by having it recite the lyrics of a
    popular song. This is hard to store accurately within small LLMs. GPT4 is excellent
    at this, GPT3.5 is good, Llama 2 70b & Falcon 180b are OK, Mixtral is bad and
    Mistral 7b is horrible (one line at most).


    And the best way to see the intelligence drop is to make it write a joke about
    two disparate things while forcing a header to it doesn''t just try to copy an
    existing joke. For example, "Write a joke about a horse and computer that begins
    with On a rainy day. Then explain the joke." Or just look at the Arc score (only
    66 for Mixtral, when Mistral 7b scored 60).


    '
  created_at: 2023-12-13 22:52:41+00:00
  edited: false
  hidden: false
  id: 657a35b9b0e3ccd993478032
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-14T18:21:54.000Z'
    data:
      edited: true
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9721667170524597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Phil337&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Phil337\">@<span class=\"\
          underline\">Phil337</span></a></span>\n\n\t</span></span> alright i see\
          \ your point but I and about everyone else  find it much better then 70b\
          \ sized models and similar sized models as well. Ask it coding tasks, riddles\
          \ and more things and you will see this is a model that can easily compete\
          \ with chatgpt.</p>\n<p>Benchmarks are not everything and even the arc benchmark\
          \ is just really multiple choice which is not everything at all. </p>\n"
        raw: '@Phil337 alright i see your point but I and about everyone else  find
          it much better then 70b sized models and similar sized models as well. Ask
          it coding tasks, riddles and more things and you will see this is a model
          that can easily compete with chatgpt.


          Benchmarks are not everything and even the arc benchmark is just really
          multiple choice which is not everything at all. '
        updatedAt: '2023-12-14T18:22:54.569Z'
      numEdits: 1
      reactions: []
    id: 657b47c26c149b6a964b9d7a
    type: comment
  author: YaTharThShaRma999
  content: '@Phil337 alright i see your point but I and about everyone else  find
    it much better then 70b sized models and similar sized models as well. Ask it
    coding tasks, riddles and more things and you will see this is a model that can
    easily compete with chatgpt.


    Benchmarks are not everything and even the arc benchmark is just really multiple
    choice which is not everything at all. '
  created_at: 2023-12-14 18:21:54+00:00
  edited: true
  hidden: false
  id: 657b47c26c149b6a964b9d7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-14T18:36:25.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956713080406189
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \ Yes, I threw several questions at Mixtral that Falcon 180b, Llama 2 70b,\
          \ Mistral 7b... got wrong, and it got them right. It's undeniably better\
          \ than Falcon 180b or Llama 2 70b. But it's not generally better than GPT3.5.</p>\n\
          <p>For example, it can't adeptly handle complex and tricky prompts which\
          \ require truly adaptable cognition, such as generating jokes about 2 disparate\
          \ things with a forced header so it doesn't just try to copy the format\
          \ of a known joke. This general intelligence is best measured by Arc vs\
          \ any other standardized LLM test (since Arc only requires general knowledge\
          \ that even the smallest LLMs have).</p>\n<p>So claiming GPT3.5 performance\
          \ was almost entirely about Arc. Mistral 7b was already within 3-5 point\
          \ of GPT3.5 on MMLU, WinoGrande, HellaSwag... so Mistral 14b dense would\
          \ have easily matched GPT3.5 on said tests. So it all came down to Arc (60\
          \ for Mistral 7b and 85 for GPT3.5), yet Mixtral only got 66, which is exactly\
          \ what a 14b dense Mistral is predicted to get.</p>\n"
        raw: '@YaTharThShaRma999 Yes, I threw several questions at Mixtral that Falcon
          180b, Llama 2 70b, Mistral 7b... got wrong, and it got them right. It''s
          undeniably better than Falcon 180b or Llama 2 70b. But it''s not generally
          better than GPT3.5.


          For example, it can''t adeptly handle complex and tricky prompts which require
          truly adaptable cognition, such as generating jokes about 2 disparate things
          with a forced header so it doesn''t just try to copy the format of a known
          joke. This general intelligence is best measured by Arc vs any other standardized
          LLM test (since Arc only requires general knowledge that even the smallest
          LLMs have).


          So claiming GPT3.5 performance was almost entirely about Arc. Mistral 7b
          was already within 3-5 point of GPT3.5 on MMLU, WinoGrande, HellaSwag...
          so Mistral 14b dense would have easily matched GPT3.5 on said tests. So
          it all came down to Arc (60 for Mistral 7b and 85 for GPT3.5), yet Mixtral
          only got 66, which is exactly what a 14b dense Mistral is predicted to get.'
        updatedAt: '2023-12-14T18:36:25.896Z'
      numEdits: 0
      reactions: []
    id: 657b4b29504b90a3c5879d0c
    type: comment
  author: Phil337
  content: '@YaTharThShaRma999 Yes, I threw several questions at Mixtral that Falcon
    180b, Llama 2 70b, Mistral 7b... got wrong, and it got them right. It''s undeniably
    better than Falcon 180b or Llama 2 70b. But it''s not generally better than GPT3.5.


    For example, it can''t adeptly handle complex and tricky prompts which require
    truly adaptable cognition, such as generating jokes about 2 disparate things with
    a forced header so it doesn''t just try to copy the format of a known joke. This
    general intelligence is best measured by Arc vs any other standardized LLM test
    (since Arc only requires general knowledge that even the smallest LLMs have).


    So claiming GPT3.5 performance was almost entirely about Arc. Mistral 7b was already
    within 3-5 point of GPT3.5 on MMLU, WinoGrande, HellaSwag... so Mistral 14b dense
    would have easily matched GPT3.5 on said tests. So it all came down to Arc (60
    for Mistral 7b and 85 for GPT3.5), yet Mixtral only got 66, which is exactly what
    a 14b dense Mistral is predicted to get.'
  created_at: 2023-12-14 18:36:25+00:00
  edited: false
  hidden: false
  id: 657b4b29504b90a3c5879d0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-12-15T15:13:44.000Z'
    data:
      edited: true
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9222714900970459
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Phil337&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Phil337\">@<span class=\"\
          underline\">Phil337</span></a></span>\n\n\t</span></span> yeah I agree that\
          \ gpt3.5 does have a slightly better instruction following ability but mixtral\
          \ is pretty near most things. And chatgpt most likely has lots of extra\
          \ stuff  that it\u2019s using with it like wolfram alpha, maybe some hallucinater\
          \ checker or different things.</p>\n<p>Also 8 experts is actually much worse\
          \ than using 2 currently. You can check this perplexity benchmark and it\
          \ shows more experts = higher perplexity. But it is a artificial benchmark\
          \ that\u2019s even worse then normal benchmarks so yeah</p>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/TvjEP14ps7ZUgJ-0-mhIX.png\"\
          >benchmark</a></p>\n"
        raw: "@Phil337 yeah I agree that gpt3.5 does have a slightly better instruction\
          \ following ability but mixtral is pretty near most things. And chatgpt\
          \ most likely has lots of extra stuff  that it\u2019s using with it like\
          \ wolfram alpha, maybe some hallucinater checker or different things.\n\n\
          Also 8 experts is actually much worse than using 2 currently. You can check\
          \ this perplexity benchmark and it shows more experts = higher perplexity.\
          \ But it is a artificial benchmark that\u2019s even worse then normal benchmarks\
          \ so yeah\n\n[benchmark](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/TvjEP14ps7ZUgJ-0-mhIX.png)"
        updatedAt: '2023-12-15T15:14:16.891Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Phil337
        - jlzhou
    id: 657c6d285bc604cae5b64b35
    type: comment
  author: YaTharThShaRma999
  content: "@Phil337 yeah I agree that gpt3.5 does have a slightly better instruction\
    \ following ability but mixtral is pretty near most things. And chatgpt most likely\
    \ has lots of extra stuff  that it\u2019s using with it like wolfram alpha, maybe\
    \ some hallucinater checker or different things.\n\nAlso 8 experts is actually\
    \ much worse than using 2 currently. You can check this perplexity benchmark and\
    \ it shows more experts = higher perplexity. But it is a artificial benchmark\
    \ that\u2019s even worse then normal benchmarks so yeah\n\n[benchmark](https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/TvjEP14ps7ZUgJ-0-mhIX.png)"
  created_at: 2023-12-15 15:13:44+00:00
  edited: true
  hidden: false
  id: 657c6d285bc604cae5b64b35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
      fullname: Phil Foster
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil337
      type: user
    createdAt: '2023-12-15T15:19:23.000Z'
    data:
      edited: false
      editors:
      - Phil337
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445919990539551
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb73653279d1996ec4ecd6477049caea.svg
          fullname: Phil Foster
          isHf: false
          isPro: false
          name: Phil337
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YaTharThShaRma999&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YaTharThShaRma999\"\
          >@<span class=\"underline\">YaTharThShaRma999</span></a></span>\n\n\t</span></span>\
          \ Thanks for that chart. That conclusively shows that using more than 3\
          \ experts is progressively worse.</p>\n"
        raw: '@YaTharThShaRma999 Thanks for that chart. That conclusively shows that
          using more than 3 experts is progressively worse.'
        updatedAt: '2023-12-15T15:19:23.239Z'
      numEdits: 0
      reactions: []
    id: 657c6e7be49e44c26de9b650
    type: comment
  author: Phil337
  content: '@YaTharThShaRma999 Thanks for that chart. That conclusively shows that
    using more than 3 experts is progressively worse.'
  created_at: 2023-12-15 15:19:23+00:00
  edited: false
  hidden: false
  id: 657c6e7be49e44c26de9b650
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: A question
