!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kxgong
conflicting_files: null
created_at: 2024-01-22 13:13:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07158ff6aa1803c846403594c5d55a34.svg
      fullname: Kaixiong Gong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kxgong
      type: user
    createdAt: '2024-01-22T13:13:27.000Z'
    data:
      edited: false
      editors:
      - kxgong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9061684608459473
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07158ff6aa1803c846403594c5d55a34.svg
          fullname: Kaixiong Gong
          isHf: false
          isPro: false
          name: kxgong
          type: user
        html: '<p>Hi, I use the recommended way (from_pretrained(***) )  to load mixtral-8x7B
          but it says out-of-memory.</p>

          <p>I use 8 x A100 GPUs to run this command. What is problem? </p>

          <p>Thank you.</p>

          '
        raw: "Hi, I use the recommended way (from_pretrained(***) )  to load mixtral-8x7B\
          \ but it says out-of-memory.\r\n\r\nI use 8 x A100 GPUs to run this command.\
          \ What is problem? \r\n\r\nThank you."
        updatedAt: '2024-01-22T13:13:27.646Z'
      numEdits: 0
      reactions: []
    id: 65ae69f7fcbe71d6d583af24
    type: comment
  author: kxgong
  content: "Hi, I use the recommended way (from_pretrained(***) )  to load mixtral-8x7B\
    \ but it says out-of-memory.\r\n\r\nI use 8 x A100 GPUs to run this command. What\
    \ is problem? \r\n\r\nThank you."
  created_at: 2024-01-22 13:13:27+00:00
  edited: false
  hidden: false
  id: 65ae69f7fcbe71d6d583af24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-22T16:27:04.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7643829584121704
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kxgong&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kxgong\">@<span class=\"\
          underline\">kxgong</span></a></span>\n\n\t</span></span><br>I suggest to\
          \ load the model in half-precision (<code>torch_dtype=torch.float16</code>)\
          \ or in 4-bit precision <code>load_in_4bit=True</code> in order to load\
          \ your model in the most memory efficient manner possible</p>\n"
        raw: "Hi @kxgong \nI suggest to load the model in half-precision (`torch_dtype=torch.float16`)\
          \ or in 4-bit precision `load_in_4bit=True` in order to load your model\
          \ in the most memory efficient manner possible"
        updatedAt: '2024-01-22T16:27:04.523Z'
      numEdits: 0
      reactions: []
    id: 65ae97581216d50327bc13c7
    type: comment
  author: ybelkada
  content: "Hi @kxgong \nI suggest to load the model in half-precision (`torch_dtype=torch.float16`)\
    \ or in 4-bit precision `load_in_4bit=True` in order to load your model in the\
    \ most memory efficient manner possible"
  created_at: 2024-01-22 16:27:04+00:00
  edited: false
  hidden: false
  id: 65ae97581216d50327bc13c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07158ff6aa1803c846403594c5d55a34.svg
      fullname: Kaixiong Gong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kxgong
      type: user
    createdAt: '2024-01-23T02:53:54.000Z'
    data:
      edited: false
      editors:
      - kxgong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7440857887268066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07158ff6aa1803c846403594c5d55a34.svg
          fullname: Kaixiong Gong
          isHf: false
          isPro: false
          name: kxgong
          type: user
        html: "<blockquote>\n<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kxgong&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kxgong\"\
          >@<span class=\"underline\">kxgong</span></a></span>\n\n\t</span></span><br>I\
          \ suggest to load the model in half-precision (<code>torch_dtype=torch.float16</code>)\
          \ or in 4-bit precision <code>load_in_4bit=True</code> in order to load\
          \ your model in the most memory efficient manner possible</p>\n</blockquote>\n\
          <p>Thank you, I am using mixtral-8x7B for training. I wonder whether using\
          \ 4bit will cause performance drop.</p>\n"
        raw: "> Hi @kxgong \n> I suggest to load the model in half-precision (`torch_dtype=torch.float16`)\
          \ or in 4-bit precision `load_in_4bit=True` in order to load your model\
          \ in the most memory efficient manner possible\n\nThank you, I am using\
          \ mixtral-8x7B for training. I wonder whether using 4bit will cause performance\
          \ drop."
        updatedAt: '2024-01-23T02:53:54.937Z'
      numEdits: 0
      reactions: []
    id: 65af2a42e0ea35ad61a3ee16
    type: comment
  author: kxgong
  content: "> Hi @kxgong \n> I suggest to load the model in half-precision (`torch_dtype=torch.float16`)\
    \ or in 4-bit precision `load_in_4bit=True` in order to load your model in the\
    \ most memory efficient manner possible\n\nThank you, I am using mixtral-8x7B\
    \ for training. I wonder whether using 4bit will cause performance drop."
  created_at: 2024-01-23 02:53:54+00:00
  edited: false
  hidden: false
  id: 65af2a42e0ea35ad61a3ee16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-23T14:17:57.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8337874412536621
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;kxgong&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kxgong\">@<span class=\"\
          underline\">kxgong</span></a></span>\n\n\t</span></span> if you use QLoRA\
          \ you shouldn't expect performance drop with respect to full-finetuning.\
          \ You can read more about QLoRA here: <a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\"\
          >https://huggingface.co/blog/4bit-transformers-bitsandbytes</a> and get\
          \ started with resources on how to run QLoRA with this blogpost for example:\
          \ <a rel=\"nofollow\" href=\"https://pytorch.org/blog/finetune-llms/\">https://pytorch.org/blog/finetune-llms/</a></p>\n"
        raw: '@kxgong if you use QLoRA you shouldn''t expect performance drop with
          respect to full-finetuning. You can read more about QLoRA here: https://huggingface.co/blog/4bit-transformers-bitsandbytes
          and get started with resources on how to run QLoRA with this blogpost for
          example: https://pytorch.org/blog/finetune-llms/'
        updatedAt: '2024-01-23T14:17:57.615Z'
      numEdits: 0
      reactions: []
    id: 65afca953997c4b6d2da190a
    type: comment
  author: ybelkada
  content: '@kxgong if you use QLoRA you shouldn''t expect performance drop with respect
    to full-finetuning. You can read more about QLoRA here: https://huggingface.co/blog/4bit-transformers-bitsandbytes
    and get started with resources on how to run QLoRA with this blogpost for example:
    https://pytorch.org/blog/finetune-llms/'
  created_at: 2024-01-23 14:17:57+00:00
  edited: false
  hidden: false
  id: 65afca953997c4b6d2da190a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: Out of memory issue.
