!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hiyouga
conflicting_files: null
created_at: 2023-12-12 04:40:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-12T04:40:35.000Z'
    data:
      edited: true
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5500767827033997
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>It only requires <strong>28GB</strong> to fine-tune the <strong>8x7B</strong>
          model with <a rel="nofollow" href="https://github.com/hiyouga/LLaMA-Factory">LLaMA
          Factory</a>.</p>

          <p>We adopt 4-bit quantization, LoRA adapters and FlashAttention-2 to save
          the GPU memory.</p>

          <p>Try out <a rel="nofollow" href="https://github.com/hiyouga/LLaMA-Factory">https://github.com/hiyouga/LLaMA-Factory</a>
          </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/8hwRx6cBsK29SLM8JNe0N.png"><img
          alt="GBHtAldbIAAYisZ.png" src="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/8hwRx6cBsK29SLM8JNe0N.png"></a><br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/xoU_xMk1PtTGGOLhr0oll.png"><img
          alt="GBHtHiCbgAA8gVk.png" src="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/xoU_xMk1PtTGGOLhr0oll.png"></a></p>

          '
        raw: "It only requires **28GB** to fine-tune the **8x7B** model with [LLaMA\
          \ Factory](https://github.com/hiyouga/LLaMA-Factory).\n\nWe adopt 4-bit\
          \ quantization, LoRA adapters and FlashAttention-2 to save the GPU memory.\n\
          \nTry out https://github.com/hiyouga/LLaMA-Factory \n\n![GBHtAldbIAAYisZ.png](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/8hwRx6cBsK29SLM8JNe0N.png)\n\
          ![GBHtHiCbgAA8gVk.png](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/xoU_xMk1PtTGGOLhr0oll.png)\n"
        updatedAt: '2023-12-12T04:44:02.776Z'
      numEdits: 1
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - KennethKong
        - mmarianne
        - MaziyarPanahi
        - chenhugging
        - giannigi
        - Benson
        - Morton-with-siri
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - giannigi
    id: 6577e443e390cfd40990deff
    type: comment
  author: hiyouga
  content: "It only requires **28GB** to fine-tune the **8x7B** model with [LLaMA\
    \ Factory](https://github.com/hiyouga/LLaMA-Factory).\n\nWe adopt 4-bit quantization,\
    \ LoRA adapters and FlashAttention-2 to save the GPU memory.\n\nTry out https://github.com/hiyouga/LLaMA-Factory\
    \ \n\n![GBHtAldbIAAYisZ.png](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/8hwRx6cBsK29SLM8JNe0N.png)\n\
    ![GBHtHiCbgAA8gVk.png](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/xoU_xMk1PtTGGOLhr0oll.png)\n"
  created_at: 2023-12-12 04:40:35+00:00
  edited: true
  hidden: false
  id: 6577e443e390cfd40990deff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-12T08:47:53.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.338195264339447
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>Mixtral-8x7B fine-tuned on the Alpaca dataset, preliminary results:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/UKM9ZoA8JgjIZhyNHSzFt.jpeg"><img
          alt="20231212164640.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/UKM9ZoA8JgjIZhyNHSzFt.jpeg"></a></p>

          '
        raw: 'Mixtral-8x7B fine-tuned on the Alpaca dataset, preliminary results:


          ![20231212164640.jpg](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/UKM9ZoA8JgjIZhyNHSzFt.jpeg)

          '
        updatedAt: '2023-12-12T08:47:53.476Z'
      numEdits: 0
      reactions: []
    id: 65781e397f5da1deb674ab5a
    type: comment
  author: hiyouga
  content: 'Mixtral-8x7B fine-tuned on the Alpaca dataset, preliminary results:


    ![20231212164640.jpg](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/UKM9ZoA8JgjIZhyNHSzFt.jpeg)

    '
  created_at: 2023-12-12 08:47:53+00:00
  edited: false
  hidden: false
  id: 65781e397f5da1deb674ab5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-12T08:55:49.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4172190725803375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: '<p>Remarkable reasoning abilities:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/x3p-PwRfBZY2txxuJ1DtB.jpeg"><img
          alt="20231212165413.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/x3p-PwRfBZY2txxuJ1DtB.jpeg"></a></p>

          '
        raw: 'Remarkable reasoning abilities:


          ![20231212165413.jpg](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/x3p-PwRfBZY2txxuJ1DtB.jpeg)

          '
        updatedAt: '2023-12-12T08:55:49.091Z'
      numEdits: 0
      reactions: []
    id: 65782015411e14898b920182
    type: comment
  author: hiyouga
  content: 'Remarkable reasoning abilities:


    ![20231212165413.jpg](https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/x3p-PwRfBZY2txxuJ1DtB.jpeg)

    '
  created_at: 2023-12-12 08:55:49+00:00
  edited: false
  hidden: false
  id: 65782015411e14898b920182
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4809973818f07db013b8bb1893d6a74d.svg
      fullname: Uwe Stoll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kustoll
      type: user
    createdAt: '2023-12-15T00:36:16.000Z'
    data:
      edited: false
      editors:
      - kustoll
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7680973410606384
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4809973818f07db013b8bb1893d6a74d.svg
          fullname: Uwe Stoll
          isHf: false
          isPro: false
          name: kustoll
          type: user
        html: '<p>This sounds great! Could you kindly provide your command line parameters
          and a Deepspeed config to run it on multiple H100s?</p>

          '
        raw: This sounds great! Could you kindly provide your command line parameters
          and a Deepspeed config to run it on multiple H100s?
        updatedAt: '2023-12-15T00:36:16.192Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - giannigi
    id: 657b9f80f634e69165e214b8
    type: comment
  author: kustoll
  content: This sounds great! Could you kindly provide your command line parameters
    and a Deepspeed config to run it on multiple H100s?
  created_at: 2023-12-15 00:36:16+00:00
  edited: false
  hidden: false
  id: 657b9f80f634e69165e214b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678444698553-636b4abeeb076ec3f4098853.jpeg?w=200&h=200&f=face
      fullname: Shahul Es
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shahules786
      type: user
    createdAt: '2023-12-16T12:32:56.000Z'
    data:
      edited: false
      editors:
      - shahules786
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9055433869361877
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678444698553-636b4abeeb076ec3f4098853.jpeg?w=200&h=200&f=face
          fullname: Shahul Es
          isHf: false
          isPro: false
          name: shahules786
          type: user
        html: "<p>This is great <span data-props=\"{&quot;user&quot;:&quot;hiyouga&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/hiyouga\"\
          >@<span class=\"underline\">hiyouga</span></a></span>\n\n\t</span></span>\
          \ . I wonder how efficient the training will be, especially with sparse\
          \ models, and how issues like token dropping will be addressed. </p>\n"
        raw: 'This is great @hiyouga . I wonder how efficient the training will be,
          especially with sparse models, and how issues like token dropping will be
          addressed. '
        updatedAt: '2023-12-16T12:32:56.182Z'
      numEdits: 0
      reactions: []
    id: 657d98f8a982e9093f625418
    type: comment
  author: shahules786
  content: 'This is great @hiyouga . I wonder how efficient the training will be,
    especially with sparse models, and how issues like token dropping will be addressed. '
  created_at: 2023-12-16 12:32:56+00:00
  edited: false
  hidden: false
  id: 657d98f8a982e9093f625418
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/edce18ed5ff73bdd308954fc2fab1ae6.svg
      fullname: syji
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: noforit
      type: user
    createdAt: '2023-12-17T15:52:52.000Z'
    data:
      edited: false
      editors:
      - noforit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7951142191886902
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/edce18ed5ff73bdd308954fc2fab1ae6.svg
          fullname: syji
          isHf: false
          isPro: false
          name: noforit
          type: user
        html: '<p>This is great! Thanks for sharing,  but i have an issue when  adopt
          8-bit quantization, LoRA adapters and FlashAttention-2 with Mixtral 8x7B
          MoE model.<br>there''s an error :       RuntimeError: The size of tensor
          a (32) must match the size of tensor b (8) at non-singleton dimension 0<br>could
          you help me?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6523af370415e1b734ffb84c/3pLGVbfa7wKZmh1dtUhMo.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6523af370415e1b734ffb84c/3pLGVbfa7wKZmh1dtUhMo.png"></a></p>

          '
        raw: "This is great! Thanks for sharing,  but i have an issue when  adopt\
          \ 8-bit quantization, LoRA adapters and FlashAttention-2 with Mixtral 8x7B\
          \ MoE model. \nthere's an error :       RuntimeError: The size of tensor\
          \ a (32) must match the size of tensor b (8) at non-singleton dimension\
          \ 0\ncould you help me?\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6523af370415e1b734ffb84c/3pLGVbfa7wKZmh1dtUhMo.png)\n"
        updatedAt: '2023-12-17T15:52:52.280Z'
      numEdits: 0
      reactions: []
    id: 657f1954138b7e3914a2f377
    type: comment
  author: noforit
  content: "This is great! Thanks for sharing,  but i have an issue when  adopt 8-bit\
    \ quantization, LoRA adapters and FlashAttention-2 with Mixtral 8x7B MoE model.\
    \ \nthere's an error :       RuntimeError: The size of tensor a (32) must match\
    \ the size of tensor b (8) at non-singleton dimension 0\ncould you help me?\n\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/6523af370415e1b734ffb84c/3pLGVbfa7wKZmh1dtUhMo.png)\n"
  created_at: 2023-12-17 15:52:52+00:00
  edited: false
  hidden: false
  id: 657f1954138b7e3914a2f377
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-18T08:43:38.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9130288362503052
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;noforit&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/noforit\">@<span class=\"\
          underline\">noforit</span></a></span>\n\n\t</span></span> it looks like\
          \ there are some issues in 8-bit quantization, we recommend using 4-bit\
          \ quantization instead</p>\n"
        raw: '@noforit it looks like there are some issues in 8-bit quantization,
          we recommend using 4-bit quantization instead'
        updatedAt: '2023-12-18T08:43:38.682Z'
      numEdits: 0
      reactions: []
    id: 6580063acec775bfe0253fd3
    type: comment
  author: hiyouga
  content: '@noforit it looks like there are some issues in 8-bit quantization, we
    recommend using 4-bit quantization instead'
  created_at: 2023-12-18 08:43:38+00:00
  edited: false
  hidden: false
  id: 6580063acec775bfe0253fd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/edce18ed5ff73bdd308954fc2fab1ae6.svg
      fullname: syji
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: noforit
      type: user
    createdAt: '2023-12-19T02:48:17.000Z'
    data:
      edited: false
      editors:
      - noforit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9182221293449402
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/edce18ed5ff73bdd308954fc2fab1ae6.svg
          fullname: syji
          isHf: false
          isPro: false
          name: noforit
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;noforit&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/noforit\"\
          >@<span class=\"underline\">noforit</span></a></span>\n\n\t</span></span>\
          \ it looks like there are some issues in 8-bit quantization, we recommend\
          \ using 4-bit quantization instead</p>\n</blockquote>\n<p>As you say, thanks,\
          \ I use 4-bit instead and it works</p>\n"
        raw: '> @noforit it looks like there are some issues in 8-bit quantization,
          we recommend using 4-bit quantization instead


          As you say, thanks, I use 4-bit instead and it works'
        updatedAt: '2023-12-19T02:48:17.096Z'
      numEdits: 0
      reactions: []
    id: 6581047116eb2b758e890250
    type: comment
  author: noforit
  content: '> @noforit it looks like there are some issues in 8-bit quantization,
    we recommend using 4-bit quantization instead


    As you say, thanks, I use 4-bit instead and it works'
  created_at: 2023-12-19 02:48:17+00:00
  edited: false
  hidden: false
  id: 6581047116eb2b758e890250
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677134452587-616d8d697aa28205c1148fc1.jpeg?w=200&h=200&f=face
      fullname: Baptiste Jamin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baptistejamin
      type: user
    createdAt: '2023-12-20T07:58:24.000Z'
    data:
      edited: false
      editors:
      - baptistejamin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9697704315185547
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677134452587-616d8d697aa28205c1148fc1.jpeg?w=200&h=200&f=face
          fullname: Baptiste Jamin
          isHf: false
          isPro: false
          name: baptistejamin
          type: user
        html: '<p>I saw some comments showing that quantization was an issue to leverage
          with Mixtral MOE.</p>

          <p>Mixtral routes each token to experts. Quantization can reduce probability
          for each token, resulting routing to only go a small portion of experts.
          </p>

          '
        raw: 'I saw some comments showing that quantization was an issue to leverage
          with Mixtral MOE.


          Mixtral routes each token to experts. Quantization can reduce probability
          for each token, resulting routing to only go a small portion of experts. '
        updatedAt: '2023-12-20T07:58:24.289Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - hiyouga
        - jlzhou
        - jensjorisdecorte
        - Morton-with-siri
    id: 65829ea005c177eea3b40b98
    type: comment
  author: baptistejamin
  content: 'I saw some comments showing that quantization was an issue to leverage
    with Mixtral MOE.


    Mixtral routes each token to experts. Quantization can reduce probability for
    each token, resulting routing to only go a small portion of experts. '
  created_at: 2023-12-20 07:58:24+00:00
  edited: false
  hidden: false
  id: 65829ea005c177eea3b40b98
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/152751068011789bfb3389ef09a6d22a.svg
      fullname: fanton
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: giannigi
      type: user
    createdAt: '2023-12-23T11:27:28.000Z'
    data:
      edited: false
      editors:
      - giannigi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8836911916732788
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/152751068011789bfb3389ef09a6d22a.svg
          fullname: fanton
          isHf: false
          isPro: false
          name: giannigi
          type: user
        html: '<p>Can you share a mini-guide on the steps necessary to perform the
          taining, or share the commands and configs used? thanks</p>

          '
        raw: Can you share a mini-guide on the steps necessary to perform the taining,
          or share the commands and configs used? thanks
        updatedAt: '2023-12-23T11:27:28.457Z'
      numEdits: 0
      reactions: []
    id: 6586c420085a5bce6100060c
    type: comment
  author: giannigi
  content: Can you share a mini-guide on the steps necessary to perform the taining,
    or share the commands and configs used? thanks
  created_at: 2023-12-23 11:27:28+00:00
  edited: false
  hidden: false
  id: 6586c420085a5bce6100060c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2023-12-24T11:22:16.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.06820080429315567
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;giannigi&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/giannigi\">@<span class=\"\
          underline\">giannigi</span></a></span>\n\n\t</span></span> </p>\n<pre><code\
          \ class=\"language-bash\">CUDA_VISIBLE_DEVICES=0 python src/train_bash.py\
          \ \\\n    --stage sft \\\n    --do_train \\\n    --model_name_or_path mistralai/Mixtral-8x7B-v0.1\
          \ \\\n    --dataset alpaca_en \\\n    --template mistral \\\n    --finetuning_type\
          \ lora \\\n    --lora_target q_proj,v_proj \\\n    --output_dir mixtral\
          \ \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps\
          \ 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n \
          \   --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs\
          \ 1.0 \\\n    --quantization_bit 4 \\\n    --bf16\n</code></pre>\n"
        raw: "@giannigi \n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py\
          \ \\\n    --stage sft \\\n    --do_train \\\n    --model_name_or_path mistralai/Mixtral-8x7B-v0.1\
          \ \\\n    --dataset alpaca_en \\\n    --template mistral \\\n    --finetuning_type\
          \ lora \\\n    --lora_target q_proj,v_proj \\\n    --output_dir mixtral\
          \ \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps\
          \ 8 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n \
          \   --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs\
          \ 1.0 \\\n    --quantization_bit 4 \\\n    --bf16\n```"
        updatedAt: '2023-12-24T11:22:16.445Z'
      numEdits: 0
      reactions:
      - count: 8
        reaction: "\u2764\uFE0F"
        users:
        - giannigi
        - tjtanaa
        - arthurbm
        - davidpeleg
        - shkim11
        - mofanv
        - Morton-with-siri
        - JoshPurtell
      - count: 3
        reaction: "\U0001F44D"
        users:
        - giannigi
        - shkim11
        - Morton-with-siri
    id: 6588146899ed106ac87a46fd
    type: comment
  author: hiyouga
  content: "@giannigi \n\n```bash\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py\
    \ \\\n    --stage sft \\\n    --do_train \\\n    --model_name_or_path mistralai/Mixtral-8x7B-v0.1\
    \ \\\n    --dataset alpaca_en \\\n    --template mistral \\\n    --finetuning_type\
    \ lora \\\n    --lora_target q_proj,v_proj \\\n    --output_dir mixtral \\\n \
    \   --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\
    \n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps\
    \ 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 1.0 \\\n    --quantization_bit\
    \ 4 \\\n    --bf16\n```"
  created_at: 2023-12-24 11:22:16+00:00
  edited: false
  hidden: false
  id: 6588146899ed106ac87a46fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff9452783ffd8b268e17bc102ad8cf97.svg
      fullname: Sumegh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumegh
      type: user
    createdAt: '2024-01-06T13:55:21.000Z'
    data:
      edited: true
      editors:
      - sumegh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6778408885002136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff9452783ffd8b268e17bc102ad8cf97.svg
          fullname: Sumegh
          isHf: false
          isPro: false
          name: sumegh
          type: user
        html: "<p>i'm following the same and doing a 4-bit LoRA finetuning on a custom\
          \ dataset. Tried changing templates between Alpaca &amp; Mistral. But my\
          \ training loss diverges after 1k steps or so. Any ideas ?</p>\n<p>Reference\
          \ Notebook - <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing\"\
          >https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing</a></p>\n\
          <p>cc - <span data-props=\"{&quot;user&quot;:&quot;hiyouga&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hiyouga\">@<span class=\"\
          underline\">hiyouga</span></a></span>\n\n\t</span></span> </p>\n"
        raw: 'i''m following the same and doing a 4-bit LoRA finetuning on a custom
          dataset. Tried changing templates between Alpaca & Mistral. But my training
          loss diverges after 1k steps or so. Any ideas ?


          Reference Notebook - https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing


          cc - @hiyouga '
        updatedAt: '2024-01-08T06:41:25.174Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - giannigi
    id: 65995bc9539c808e84c36e0f
    type: comment
  author: sumegh
  content: 'i''m following the same and doing a 4-bit LoRA finetuning on a custom
    dataset. Tried changing templates between Alpaca & Mistral. But my training loss
    diverges after 1k steps or so. Any ideas ?


    Reference Notebook - https://colab.research.google.com/drive/1VDa0lIfqiwm16hBlIlEaabGVTNB3dN1A?usp=sharing


    cc - @hiyouga '
  created_at: 2024-01-06 13:55:21+00:00
  edited: true
  hidden: false
  id: 65995bc9539c808e84c36e0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a16462da657327f220a34889b281433.svg
      fullname: Manoj Athreya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Villian7
      type: user
    createdAt: '2024-01-06T22:15:52.000Z'
    data:
      edited: false
      editors:
      - Villian7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8880177736282349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a16462da657327f220a34889b281433.svg
          fullname: Manoj Athreya
          isHf: false
          isPro: false
          name: Villian7
          type: user
        html: '<p>What are the minimum compute resources required to train the model?</p>

          '
        raw: What are the minimum compute resources required to train the model?
        updatedAt: '2024-01-06T22:15:52.494Z'
      numEdits: 0
      reactions: []
    id: 6599d1181b4a26041ad05ea1
    type: comment
  author: Villian7
  content: What are the minimum compute resources required to train the model?
  created_at: 2024-01-06 22:15:52+00:00
  edited: false
  hidden: false
  id: 6599d1181b4a26041ad05ea1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
      fullname: AI Geek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aigeek0x0
      type: user
    createdAt: '2024-01-10T00:29:29.000Z'
    data:
      edited: false
      editors:
      - aigeek0x0
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9254055023193359
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454631806728ff79a3829e8/Im1TjNIXt7J5Am4aW6FZ-.jpeg?w=200&h=200&f=face
          fullname: AI Geek
          isHf: false
          isPro: false
          name: aigeek0x0
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;hiyouga&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/hiyouga\">@<span class=\"\
          underline\">hiyouga</span></a></span>\n\n\t</span></span> </p>\n<p>can you\
          \ please explain why you only targeted \"q_proj,v_proj\" layers? </p>\n\
          <p>i have come across some arguments opposing the idea of fine-tuning all\
          \ linear layers/gates/routers. I would greatly appreciate it if someone\
          \ could provide a more detailed explanation on this matter. </p>\n<p>thank\
          \ you.</p>\n"
        raw: "@hiyouga \n\ncan you please explain why you only targeted \"q_proj,v_proj\"\
          \ layers? \n\ni have come across some arguments opposing the idea of fine-tuning\
          \ all linear layers/gates/routers. I would greatly appreciate it if someone\
          \ could provide a more detailed explanation on this matter. \n\nthank you."
        updatedAt: '2024-01-10T00:29:29.779Z'
      numEdits: 0
      reactions: []
    id: 659de4e91fdbe4c36cce2fe4
    type: comment
  author: aigeek0x0
  content: "@hiyouga \n\ncan you please explain why you only targeted \"q_proj,v_proj\"\
    \ layers? \n\ni have come across some arguments opposing the idea of fine-tuning\
    \ all linear layers/gates/routers. I would greatly appreciate it if someone could\
    \ provide a more detailed explanation on this matter. \n\nthank you."
  created_at: 2024-01-10 00:29:29+00:00
  edited: false
  hidden: false
  id: 659de4e91fdbe4c36cce2fe4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
      fullname: hoshi hiyouga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hiyouga
      type: user
    createdAt: '2024-01-10T01:13:19.000Z'
    data:
      edited: false
      editors:
      - hiyouga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7658281326293945
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png?w=200&h=200&f=face
          fullname: hoshi hiyouga
          isHf: false
          isPro: false
          name: hiyouga
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aigeek0x0&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aigeek0x0\">@<span class=\"\
          underline\">aigeek0x0</span></a></span>\n\n\t</span></span><br>We used <code>q_proj,v_proj</code>\
          \ modules just to estimate the minimum resource usage. It is recommended\
          \ to use all linear layers with LoRA adapters for better fitting.</p>\n"
        raw: "@aigeek0x0 \nWe used `q_proj,v_proj` modules just to estimate the minimum\
          \ resource usage. It is recommended to use all linear layers with LoRA adapters\
          \ for better fitting.\n"
        updatedAt: '2024-01-10T01:13:19.483Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - aigeek0x0
        - Morton-with-siri
    id: 659def2f5bff2252a9a0d57f
    type: comment
  author: hiyouga
  content: "@aigeek0x0 \nWe used `q_proj,v_proj` modules just to estimate the minimum\
    \ resource usage. It is recommended to use all linear layers with LoRA adapters\
    \ for better fitting.\n"
  created_at: 2024-01-10 01:13:19+00:00
  edited: false
  hidden: false
  id: 659def2f5bff2252a9a0d57f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a16462da657327f220a34889b281433.svg
      fullname: Manoj Athreya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Villian7
      type: user
    createdAt: '2024-01-10T01:57:08.000Z'
    data:
      edited: true
      editors:
      - Villian7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9145859479904175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a16462da657327f220a34889b281433.svg
          fullname: Manoj Athreya
          isHf: false
          isPro: false
          name: Villian7
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;aigeek0x0&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/aigeek0x0\">@<span class=\"\
          underline\">aigeek0x0</span></a></span>\n\n\t</span></span><br>You can specify\
          \ and finetune only the linear layers of any LLM model while using LORA.\
          \ When you print(model) you will get the layers in that some people use\
          \ only attention layers such as q, k, v, o in case of Mistral or some people\
          \ use all linear layers. I'm exactly not sure how it affects the performance,\
          \ but it will surely reduce the RAM size of the peft model and it is of\
          \ very small amount.</p>\n"
        raw: "@aigeek0x0 \nYou can specify and finetune only the linear layers of\
          \ any LLM model while using LORA. When you print(model) you will get the\
          \ layers in that some people use only attention layers such as q, k, v,\
          \ o in case of Mistral or some people use all linear layers. I'm exactly\
          \ not sure how it affects the performance, but it will surely reduce the\
          \ RAM size of the peft model and it is of very small amount."
        updatedAt: '2024-01-10T01:57:34.936Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aigeek0x0
    id: 659df974b03a0f0e4f4e577c
    type: comment
  author: Villian7
  content: "@aigeek0x0 \nYou can specify and finetune only the linear layers of any\
    \ LLM model while using LORA. When you print(model) you will get the layers in\
    \ that some people use only attention layers such as q, k, v, o in case of Mistral\
    \ or some people use all linear layers. I'm exactly not sure how it affects the\
    \ performance, but it will surely reduce the RAM size of the peft model and it\
    \ is of very small amount."
  created_at: 2024-01-10 01:57:08+00:00
  edited: true
  hidden: false
  id: 659df974b03a0f0e4f4e577c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6402295a4ad6464af9b3b441/TRe107y6VyOy-7e1D4cIa.jpeg?w=200&h=200&f=face
      fullname: Safouane Chergui
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chsafouane
      type: user
    createdAt: '2024-01-15T13:37:47.000Z'
    data:
      edited: false
      editors:
      - chsafouane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8588828444480896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6402295a4ad6464af9b3b441/TRe107y6VyOy-7e1D4cIa.jpeg?w=200&h=200&f=face
          fullname: Safouane Chergui
          isHf: false
          isPro: false
          name: chsafouane
          type: user
        html: '<p>It''s still not clear for me whether one should also finetune the
          routers. Any resources discussing this ?</p>

          '
        raw: It's still not clear for me whether one should also finetune the routers.
          Any resources discussing this ?
        updatedAt: '2024-01-15T13:37:47.812Z'
      numEdits: 0
      reactions: []
    id: 65a5352b2138495d16f51dbb
    type: comment
  author: chsafouane
  content: It's still not clear for me whether one should also finetune the routers.
    Any resources discussing this ?
  created_at: 2024-01-15 13:37:47+00:00
  edited: false
  hidden: false
  id: 65a5352b2138495d16f51dbb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: Fine-tuning toolkit for Mixtral 8x7B MoE model
