!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dataautogpt3
conflicting_files: null
created_at: 2023-12-11 12:50:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64430e32e255a338677fe9a7/qPn5pCkKOdpmyTHWFve_U.png?w=200&h=200&f=face
      fullname: alexander izquierdo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dataautogpt3
      type: user
    createdAt: '2023-12-11T12:50:50.000Z'
    data:
      edited: false
      editors:
      - dataautogpt3
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9086054563522339
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64430e32e255a338677fe9a7/qPn5pCkKOdpmyTHWFve_U.png?w=200&h=200&f=face
          fullname: alexander izquierdo
          isHf: false
          isPro: false
          name: dataautogpt3
          type: user
        html: '<p>RuntimeError: Expected all tensors to be on the same device, but
          found at least two devices, cuda:0 and cuda:1! (when checking argument for
          argument tensors in method wrapper_CUDA_cat)<br>Output generated in 2.42
          seconds (0.00 tokens/s, 0 tokens, context 65, seed 459973075)<br>seems to
          me like there is a total lack of multi GPU support for inference. </p>

          <p>I would appreciate it if this was addressed.<br>best wishes and thank
          you so much for your hard work! </p>

          '
        raw: "RuntimeError: Expected all tensors to be on the same device, but found\
          \ at least two devices, cuda:0 and cuda:1! (when checking argument for argument\
          \ tensors in method wrapper_CUDA_cat)\r\nOutput generated in 2.42 seconds\
          \ (0.00 tokens/s, 0 tokens, context 65, seed 459973075)\r\nseems to me like\
          \ there is a total lack of multi GPU support for inference. \r\n\r\nI would\
          \ appreciate it if this was addressed. \r\nbest wishes and thank you so\
          \ much for your hard work! "
        updatedAt: '2023-12-11T12:50:50.889Z'
      numEdits: 0
      reactions: []
    id: 657705aa5c5a6c93161130cd
    type: comment
  author: dataautogpt3
  content: "RuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cuda:0 and cuda:1! (when checking argument for argument\
    \ tensors in method wrapper_CUDA_cat)\r\nOutput generated in 2.42 seconds (0.00\
    \ tokens/s, 0 tokens, context 65, seed 459973075)\r\nseems to me like there is\
    \ a total lack of multi GPU support for inference. \r\n\r\nI would appreciate\
    \ it if this was addressed. \r\nbest wishes and thank you so much for your hard\
    \ work! "
  created_at: 2023-12-11 12:50:50+00:00
  edited: false
  hidden: false
  id: 657705aa5c5a6c93161130cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T13:23:48.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7341487407684326
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;dataautogpt3&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dataautogpt3\"\
          >@<span class=\"underline\">dataautogpt3</span></a></span>\n\n\t</span></span><br>Can\
          \ you share a reproducible snippet together with the full traceback of the\
          \ error? thanks</p>\n"
        raw: "hi @dataautogpt3 \nCan you share a reproducible snippet together with\
          \ the full traceback of the error? thanks"
        updatedAt: '2023-12-11T13:23:48.633Z'
      numEdits: 0
      reactions: []
    id: 65770d64eb067b7829137f2d
    type: comment
  author: ybelkada
  content: "hi @dataautogpt3 \nCan you share a reproducible snippet together with\
    \ the full traceback of the error? thanks"
  created_at: 2023-12-11 13:23:48+00:00
  edited: false
  hidden: false
  id: 65770d64eb067b7829137f2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-11T13:32:32.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5151560306549072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: "<p>I'm getting the same issue with the following code:</p>\n<pre><code>from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id =\
          \ \"mistralai/Mixtral-8x7B-v0.1\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\"\
          )\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text, return_tensors=\"\
          pt\").cuda()\n\noutputs = model.generate(**inputs, max_new_tokens=20)\n\
          print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n</code></pre>\n\
          <p>results in:</p>\n<pre><code>Traceback (most recent call last):\n  File\
          \ \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1718, in generate\n    return self.greedy_search(\n  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2579, in greedy_search\n    outputs = self(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\",\
          \ line 164, in new_forward\n    output = module._old_forward(*args, **kwargs)\n\
          \  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 1244, in forward\n    aux_loss = load_balancing_loss_func(\n  File\
          \ \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 98, in load_balancing_loss_func\n    gate_logits = torch.cat(gate_logits,\
          \ dim=0)\nRuntimeError: Expected all tensors to be on the same device, but\
          \ found at least two devices, cuda:0 and cuda:1! (when checking argument\
          \ for argument tensors in method wrapper_CUDA_cat)\n</code></pre>\n"
        raw: "I'm getting the same issue with the following code:\n```\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=\"auto\")\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text,\
          \ return_tensors=\"pt\").cuda()\n\noutputs = model.generate(**inputs, max_new_tokens=20)\n\
          print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\nresults\
          \ in:\n```\nTraceback (most recent call last):\n  File \"<stdin>\", line\
          \ 1, in <module>\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1718, in generate\n    return self.greedy_search(\n  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2579, in greedy_search\n    outputs = self(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\",\
          \ line 164, in new_forward\n    output = module._old_forward(*args, **kwargs)\n\
          \  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 1244, in forward\n    aux_loss = load_balancing_loss_func(\n  File\
          \ \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
          , line 98, in load_balancing_loss_func\n    gate_logits = torch.cat(gate_logits,\
          \ dim=0)\nRuntimeError: Expected all tensors to be on the same device, but\
          \ found at least two devices, cuda:0 and cuda:1! (when checking argument\
          \ for argument tensors in method wrapper_CUDA_cat)\n```\n"
        updatedAt: '2023-12-11T13:32:32.035Z'
      numEdits: 0
      reactions: []
    id: 65770f7017b474ae6b4a182f
    type: comment
  author: bjoernp
  content: "I'm getting the same issue with the following code:\n```\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"mistralai/Mixtral-8x7B-v0.1\"\
    \ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
    \ device_map=\"auto\")\n\ntext = \"Hello my name is\"\ninputs = tokenizer(text,\
    \ return_tensors=\"pt\").cuda()\n\noutputs = model.generate(**inputs, max_new_tokens=20)\n\
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\nresults in:\n\
    ```\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n\
    \  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1718, in generate\n    return self.greedy_search(\n  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2579, in greedy_search\n    outputs = self(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"\
    /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 164, in new_forward\n\
    \    output = module._old_forward(*args, **kwargs)\n  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
    , line 1244, in forward\n    aux_loss = load_balancing_loss_func(\n  File \"/run/determined/pythonuserbase/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py\"\
    , line 98, in load_balancing_loss_func\n    gate_logits = torch.cat(gate_logits,\
    \ dim=0)\nRuntimeError: Expected all tensors to be on the same device, but found\
    \ at least two devices, cuda:0 and cuda:1! (when checking argument for argument\
    \ tensors in method wrapper_CUDA_cat)\n```\n"
  created_at: 2023-12-11 13:32:32+00:00
  edited: false
  hidden: false
  id: 65770f7017b474ae6b4a182f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-11T13:48:17.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9610325694084167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>that for computing the loss, I think the code is not the latest
          cuz I pushed a fix but will check</p>

          '
        raw: 'that for computing the loss, I think the code is not the latest cuz
          I pushed a fix but will check

          '
        updatedAt: '2023-12-11T13:48:17.418Z'
      numEdits: 0
      reactions: []
    id: 65771321039997ad70d1f4ef
    type: comment
  author: ArthurZ
  content: 'that for computing the loss, I think the code is not the latest cuz I
    pushed a fix but will check

    '
  created_at: 2023-12-11 13:48:17+00:00
  edited: false
  hidden: false
  id: 65771321039997ad70d1f4ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T13:49:49.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5785223245620728
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5">https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5</a>
          should fix the issue</p>

          '
        raw: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5 should
          fix the issue
        updatedAt: '2023-12-11T13:49:49.286Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - cchristophe
    id: 6577137d50977341aab51a07
    type: comment
  author: ybelkada
  content: https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/5 should
    fix the issue
  created_at: 2023-12-11 13:49:49+00:00
  edited: false
  hidden: false
  id: 6577137d50977341aab51a07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-11T14:01:31.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3641323447227478
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bjoernp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bjoernp\">@<span class=\"\
          underline\">bjoernp</span></a></span>\n\n\t</span></span> can you try:</p>\n\
          <pre><code class=\"language-py\"><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel_id = <span class=\"hljs-string\">\"mistralai/Mixtral-8x7B-v0.1\"\
          </span>\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>, revision=<span\
          \ class=\"hljs-string\">\"refs/pr/5\"</span>)\n\ntext = <span class=\"hljs-string\"\
          >\"Hello my name is\"</span>\ninputs = tokenizer(text, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).cuda()\n\noutputs = model.generate(**inputs,\
          \ max_new_tokens=<span class=\"hljs-number\">20</span>)\n</code></pre>\n"
        raw: '@bjoernp can you try:


          ```py

          from transformers import AutoModelForCausalLM, AutoTokenizer


          model_id = "mistralai/Mixtral-8x7B-v0.1"

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto",
          revision="refs/pr/5")


          text = "Hello my name is"

          inputs = tokenizer(text, return_tensors="pt").cuda()


          outputs = model.generate(**inputs, max_new_tokens=20)

          ```'
        updatedAt: '2023-12-11T14:01:31.046Z'
      numEdits: 0
      reactions: []
    id: 6577163b8e449026ae37452a
    type: comment
  author: ybelkada
  content: '@bjoernp can you try:


    ```py

    from transformers import AutoModelForCausalLM, AutoTokenizer


    model_id = "mistralai/Mixtral-8x7B-v0.1"

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", revision="refs/pr/5")


    text = "Hello my name is"

    inputs = tokenizer(text, return_tensors="pt").cuda()


    outputs = model.generate(**inputs, max_new_tokens=20)

    ```'
  created_at: 2023-12-11 14:01:31+00:00
  edited: false
  hidden: false
  id: 6577163b8e449026ae37452a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-11T14:55:36.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7702558040618896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Works! Thanks :)</p>

          '
        raw: Works! Thanks :)
        updatedAt: '2023-12-11T14:55:36.072Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 657722e80528126967c73909
    type: comment
  author: bjoernp
  content: Works! Thanks :)
  created_at: 2023-12-11 14:55:36+00:00
  edited: false
  hidden: false
  id: 657722e80528126967c73909
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: No multi GPU inference support?
