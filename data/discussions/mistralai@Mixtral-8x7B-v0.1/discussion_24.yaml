!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ric1732
conflicting_files: null
created_at: 2023-12-21 08:28:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0d51450675951d7c15a95bef8ff23cb.svg
      fullname: ric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ric1732
      type: user
    createdAt: '2023-12-21T08:28:41.000Z'
    data:
      edited: true
      editors:
      - ric1732
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.838761031627655
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0d51450675951d7c15a95bef8ff23cb.svg
          fullname: ric
          isHf: false
          isPro: false
          name: ric1732
          type: user
        html: "<p>I have a code snippet for text generation using Hugging Face's Transformers\
          \ library. I am running inference on a machine with 8 GPUs. However, during\
          \ inference, only 2 or 3 GPUs are being utilized and the GPU utilization\
          \ remains below 32%. I want to optimize my code to utilize the full power\
          \ of all available all 8 GPUs.</p>\n<p>Here is the code I am currently using:</p>\n\
          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import json\n\nbatch_size = 32\n\nif __name__=='__main__':\n    txt_list\
          \ = [\n            \"The King is dead. Long live the Queen.\",\n       \
          \     \"Once there were four children whose names were Peter, Susan, Edmund,\
          \ and Lucy.\",\n            \"The story so far: in the beginning, the universe\
          \ was created.\",\n            \"It was a bright cold day in April, and\
          \ the clocks were striking thirteen.\",\n            \"It is a truth universally\
          \ acknowledged, that a single man in possession of a good fortune, must\
          \ be in want of a wife.\",\n            \"The sweat wis lashing oafay Sick\
          \ Boy; he wis trembling.\",\n            \"124 was spiteful. Full of Baby's\
          \ venom.\",\n            \"As Gregor Samsa awoke one morning from uneasy\
          \ dreams he found himself transformed in his bed into a gigantic insect.\"\
          ,\n            \"I write this sitting in the kitchen sink.\",\n        \
          \    \"We were somewhere around Barstow on the edge of the desert when the\
          \ drugs began to take hold.\",\n        ] * 500\n    lf = len(txt_list)\n\
          \n    tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-v0.1')\n\
          \    tokenizer.pad_token_id = tokenizer.eos_token_id\n    tokenizer.padding_side\
          \ = \"left\"\n    model = AutoModelForCausalLM.from_pretrained('mistralai/Mixtral-8x7B-v0.1',\
          \ device_map='auto')\n\n    out_list = []\n    n_steps = math.ceil(lf/batch_size)\n\
          \n    for btx in range(n_steps):\n        t_sens = txt_list[btx*batch_size:(btx+1)*batch_size]\n\
          \        t_toks = tokenizer(t_sens, return_tensors='pt', padding=True).to('cuda')\n\
          \n        opt = model.generate(**t_toks, max_new_tokens=200)\n\n       \
          \ for jty in range(batch_size):\n            ctxt = tokenizer.decode(opt[jty],\
          \ skip_special_tokens=True)\n            ctxt = ctxt[len(t_sens[jty]):].strip()\n\
          \            out_list.append({'input':t_sens[jty], 'output':ctxt})\n\n \
          \   str_list = [json.dumps(xx) for xx in out_list]\n    otf = open('rrr','w')\n\
          \    otf.write('\\n'.join(str_list))\n    otf.close()\n</code></pre>\n<p>While\
          \ the code is functional, the GPU utilization is restricted to at most 3\
          \ GPUs and not reaching its full potential. How can I modify the code to\
          \ ensure maximum GPU utilization during the inference step?</p>\n<p>Thank\
          \ you!</p>\n"
        raw: "I have a code snippet for text generation using Hugging Face's Transformers\
          \ library. I am running inference on a machine with 8 GPUs. However, during\
          \ inference, only 2 or 3 GPUs are being utilized and the GPU utilization\
          \ remains below 32%. I want to optimize my code to utilize the full power\
          \ of all available all 8 GPUs.\n\nHere is the code I am currently using:\n\
          \n\n\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \    import json\n\n    batch_size = 32\n\n    if __name__=='__main__':\n\
          \        txt_list = [\n                \"The King is dead. Long live the\
          \ Queen.\",\n                \"Once there were four children whose names\
          \ were Peter, Susan, Edmund, and Lucy.\",\n                \"The story so\
          \ far: in the beginning, the universe was created.\",\n                \"\
          It was a bright cold day in April, and the clocks were striking thirteen.\"\
          ,\n                \"It is a truth universally acknowledged, that a single\
          \ man in possession of a good fortune, must be in want of a wife.\",\n \
          \               \"The sweat wis lashing oafay Sick Boy; he wis trembling.\"\
          ,\n                \"124 was spiteful. Full of Baby's venom.\",\n      \
          \          \"As Gregor Samsa awoke one morning from uneasy dreams he found\
          \ himself transformed in his bed into a gigantic insect.\",\n          \
          \      \"I write this sitting in the kitchen sink.\",\n                \"\
          We were somewhere around Barstow on the edge of the desert when the drugs\
          \ began to take hold.\",\n            ] * 500\n        lf = len(txt_list)\n\
          \n        tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-v0.1')\n\
          \        tokenizer.pad_token_id = tokenizer.eos_token_id\n        tokenizer.padding_side\
          \ = \"left\"\n        model = AutoModelForCausalLM.from_pretrained('mistralai/Mixtral-8x7B-v0.1',\
          \ device_map='auto')\n\n        out_list = []\n        n_steps = math.ceil(lf/batch_size)\n\
          \n        for btx in range(n_steps):\n            t_sens = txt_list[btx*batch_size:(btx+1)*batch_size]\n\
          \            t_toks = tokenizer(t_sens, return_tensors='pt', padding=True).to('cuda')\n\
          \n            opt = model.generate(**t_toks, max_new_tokens=200)\n\n   \
          \         for jty in range(batch_size):\n                ctxt = tokenizer.decode(opt[jty],\
          \ skip_special_tokens=True)\n                ctxt = ctxt[len(t_sens[jty]):].strip()\n\
          \                out_list.append({'input':t_sens[jty], 'output':ctxt})\n\
          \n        str_list = [json.dumps(xx) for xx in out_list]\n        otf =\
          \ open('rrr','w')\n        otf.write('\\n'.join(str_list))\n        otf.close()\n\
          \n\nWhile the code is functional, the GPU utilization is restricted to at\
          \ most 3 GPUs and not reaching its full potential. How can I modify the\
          \ code to ensure maximum GPU utilization during the inference step?\n\n\
          Thank you!\n\n"
        updatedAt: '2023-12-21T08:31:39.450Z'
      numEdits: 3
      reactions: []
    id: 6583f739d9ea8286dee14b00
    type: comment
  author: ric1732
  content: "I have a code snippet for text generation using Hugging Face's Transformers\
    \ library. I am running inference on a machine with 8 GPUs. However, during inference,\
    \ only 2 or 3 GPUs are being utilized and the GPU utilization remains below 32%.\
    \ I want to optimize my code to utilize the full power of all available all 8\
    \ GPUs.\n\nHere is the code I am currently using:\n\n\n\n    from transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n    import json\n\n    batch_size\
    \ = 32\n\n    if __name__=='__main__':\n        txt_list = [\n               \
    \ \"The King is dead. Long live the Queen.\",\n                \"Once there were\
    \ four children whose names were Peter, Susan, Edmund, and Lucy.\",\n        \
    \        \"The story so far: in the beginning, the universe was created.\",\n\
    \                \"It was a bright cold day in April, and the clocks were striking\
    \ thirteen.\",\n                \"It is a truth universally acknowledged, that\
    \ a single man in possession of a good fortune, must be in want of a wife.\",\n\
    \                \"The sweat wis lashing oafay Sick Boy; he wis trembling.\",\n\
    \                \"124 was spiteful. Full of Baby's venom.\",\n              \
    \  \"As Gregor Samsa awoke one morning from uneasy dreams he found himself transformed\
    \ in his bed into a gigantic insect.\",\n                \"I write this sitting\
    \ in the kitchen sink.\",\n                \"We were somewhere around Barstow\
    \ on the edge of the desert when the drugs began to take hold.\",\n          \
    \  ] * 500\n        lf = len(txt_list)\n\n        tokenizer = AutoTokenizer.from_pretrained('mistralai/Mixtral-8x7B-v0.1')\n\
    \        tokenizer.pad_token_id = tokenizer.eos_token_id\n        tokenizer.padding_side\
    \ = \"left\"\n        model = AutoModelForCausalLM.from_pretrained('mistralai/Mixtral-8x7B-v0.1',\
    \ device_map='auto')\n\n        out_list = []\n        n_steps = math.ceil(lf/batch_size)\n\
    \n        for btx in range(n_steps):\n            t_sens = txt_list[btx*batch_size:(btx+1)*batch_size]\n\
    \            t_toks = tokenizer(t_sens, return_tensors='pt', padding=True).to('cuda')\n\
    \n            opt = model.generate(**t_toks, max_new_tokens=200)\n\n         \
    \   for jty in range(batch_size):\n                ctxt = tokenizer.decode(opt[jty],\
    \ skip_special_tokens=True)\n                ctxt = ctxt[len(t_sens[jty]):].strip()\n\
    \                out_list.append({'input':t_sens[jty], 'output':ctxt})\n\n   \
    \     str_list = [json.dumps(xx) for xx in out_list]\n        otf = open('rrr','w')\n\
    \        otf.write('\\n'.join(str_list))\n        otf.close()\n\n\nWhile the code\
    \ is functional, the GPU utilization is restricted to at most 3 GPUs and not reaching\
    \ its full potential. How can I modify the code to ensure maximum GPU utilization\
    \ during the inference step?\n\nThank you!\n\n"
  created_at: 2023-12-21 08:28:41+00:00
  edited: true
  hidden: false
  id: 6583f739d9ea8286dee14b00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/a0d51450675951d7c15a95bef8ff23cb.svg
      fullname: ric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ric1732
      type: user
    createdAt: '2023-12-21T08:32:11.000Z'
    data:
      from: 'Question: Maximizing GPU Utilization for Inference with Transformers'
      to: 'Question: Maximizing GPU Utilization for Inference'
    id: 6583f80ba4f176a57fa9c870
    type: title-change
  author: ric1732
  created_at: 2023-12-21 08:32:11+00:00
  id: 6583f80ba4f176a57fa9c870
  new_title: 'Question: Maximizing GPU Utilization for Inference'
  old_title: 'Question: Maximizing GPU Utilization for Inference with Transformers'
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: 'Question: Maximizing GPU Utilization for Inference'
