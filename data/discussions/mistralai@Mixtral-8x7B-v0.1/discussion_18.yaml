!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yixliu1
conflicting_files: null
created_at: 2023-12-14 05:46:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
      fullname: Yixuan Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yixliu1
      type: user
    createdAt: '2023-12-14T05:46:48.000Z'
    data:
      edited: false
      editors:
      - yixliu1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6277399063110352
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
          fullname: Yixuan Liu
          isHf: false
          isPro: false
          name: yixliu1
          type: user
        html: '<p>I tried several ways (using GPU &amp; float 16, using GPU only,
          using CPU, different max token length) to generate answer for <code>1+1</code>.
          Here is the several answer I get:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/MqV97W4TC2l90JURjwY5s.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/MqV97W4TC2l90JURjwY5s.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/SzLl8OKkgViw9huDuD_Jz.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/SzLl8OKkgViw9huDuD_Jz.png"></a></p>

          <p>Here is my code:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/LkxlU6zIbZS6Dul-ronet.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/LkxlU6zIbZS6Dul-ronet.png"></a></p>

          '
        raw: "I tried several ways (using GPU & float 16, using GPU only, using CPU,\
          \ different max token length) to generate answer for `1+1`. Here is the\
          \ several answer I get:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/MqV97W4TC2l90JURjwY5s.png)\r\
          \n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/SzLl8OKkgViw9huDuD_Jz.png)\r\
          \n\r\nHere is my code:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/LkxlU6zIbZS6Dul-ronet.png)\r\
          \n\r\n\r\n"
        updatedAt: '2023-12-14T05:46:48.317Z'
      numEdits: 0
      reactions: []
    id: 657a96c81433ea7d44e67872
    type: comment
  author: yixliu1
  content: "I tried several ways (using GPU & float 16, using GPU only, using CPU,\
    \ different max token length) to generate answer for `1+1`. Here is the several\
    \ answer I get:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/MqV97W4TC2l90JURjwY5s.png)\r\
    \n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/SzLl8OKkgViw9huDuD_Jz.png)\r\
    \n\r\nHere is my code:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64422b76b0328ddab62f7f11/LkxlU6zIbZS6Dul-ronet.png)\r\
    \n\r\n\r\n"
  created_at: 2023-12-14 05:46:48+00:00
  edited: false
  hidden: false
  id: 657a96c81433ea7d44e67872
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
      fullname: nobody
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MuteXX
      type: user
    createdAt: '2023-12-14T08:38:30.000Z'
    data:
      edited: false
      editors:
      - MuteXX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9862212538719177
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
          fullname: nobody
          isHf: false
          isPro: false
          name: MuteXX
          type: user
        html: '<p>LLMs can''t do math reliably without external assistance.</p>

          <p>I don''t know what you expect here. </p>

          '
        raw: 'LLMs can''t do math reliably without external assistance.


          I don''t know what you expect here. '
        updatedAt: '2023-12-14T08:38:30.979Z'
      numEdits: 0
      reactions: []
    id: 657abf06b0e3ccd9935e4661
    type: comment
  author: MuteXX
  content: 'LLMs can''t do math reliably without external assistance.


    I don''t know what you expect here. '
  created_at: 2023-12-14 08:38:30+00:00
  edited: false
  hidden: false
  id: 657abf06b0e3ccd9935e4661
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
      fullname: Yixuan Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yixliu1
      type: user
    createdAt: '2023-12-14T09:29:53.000Z'
    data:
      edited: false
      editors:
      - yixliu1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9686667323112488
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
          fullname: Yixuan Liu
          isHf: false
          isPro: false
          name: yixliu1
          type: user
        html: '<p>Hi,<br>Thanks for your answering. I also tried a few other examples.
          For instance: ask it to generate a prompt based on my need, and ask it to
          answer some questions based on the context I provide. Neither of them shows
          useful results.</p>

          '
        raw: 'Hi,

          Thanks for your answering. I also tried a few other examples. For instance:
          ask it to generate a prompt based on my need, and ask it to answer some
          questions based on the context I provide. Neither of them shows useful results.'
        updatedAt: '2023-12-14T09:29:53.783Z'
      numEdits: 0
      reactions: []
    id: 657acb11fb6d32747a14043d
    type: comment
  author: yixliu1
  content: 'Hi,

    Thanks for your answering. I also tried a few other examples. For instance: ask
    it to generate a prompt based on my need, and ask it to answer some questions
    based on the context I provide. Neither of them shows useful results.'
  created_at: 2023-12-14 09:29:53+00:00
  edited: false
  hidden: false
  id: 657acb11fb6d32747a14043d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
      fullname: nobody
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MuteXX
      type: user
    createdAt: '2023-12-14T09:42:47.000Z'
    data:
      edited: true
      editors:
      - MuteXX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9831951260566711
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
          fullname: nobody
          isHf: false
          isPro: false
          name: MuteXX
          type: user
        html: '<p>You''re asking a base model to solve problems.</p>

          <p>What you want is the Instruct variant. Base isn''t suitable for this.</p>

          '
        raw: 'You''re asking a base model to solve problems.


          What you want is the Instruct variant. Base isn''t suitable for this.'
        updatedAt: '2023-12-14T09:42:58.034Z'
      numEdits: 1
      reactions: []
    id: 657ace1762d7a3ca5119eb7b
    type: comment
  author: MuteXX
  content: 'You''re asking a base model to solve problems.


    What you want is the Instruct variant. Base isn''t suitable for this.'
  created_at: 2023-12-14 09:42:47+00:00
  edited: true
  hidden: false
  id: 657ace1762d7a3ca5119eb7b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
      fullname: Yixuan Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yixliu1
      type: user
    createdAt: '2023-12-14T10:17:01.000Z'
    data:
      edited: false
      editors:
      - yixliu1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8960733413696289
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
          fullname: Yixuan Liu
          isHf: false
          isPro: false
          name: yixliu1
          type: user
        html: "<p>So does base model for further fine tune while instruct model for\
          \ solve problems\uFF1F<br>Thanks for your sharing\uFF01</p>\n"
        raw: "So does base model for further fine tune while instruct model for solve\
          \ problems\uFF1F\nThanks for your sharing\uFF01"
        updatedAt: '2023-12-14T10:17:01.461Z'
      numEdits: 0
      reactions: []
    id: 657ad61dab2698da000640ae
    type: comment
  author: yixliu1
  content: "So does base model for further fine tune while instruct model for solve\
    \ problems\uFF1F\nThanks for your sharing\uFF01"
  created_at: 2023-12-14 10:17:01+00:00
  edited: false
  hidden: false
  id: 657ad61dab2698da000640ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
      fullname: nobody
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MuteXX
      type: user
    createdAt: '2023-12-14T10:24:17.000Z'
    data:
      edited: false
      editors:
      - MuteXX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9590413570404053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
          fullname: nobody
          isHf: false
          isPro: false
          name: MuteXX
          type: user
        html: '<p>The base model is a raw LLM; it ONLY does text completion.<br>Instruct
          has been tuned to respond to you instead.</p>

          '
        raw: 'The base model is a raw LLM; it ONLY does text completion.

          Instruct has been tuned to respond to you instead.'
        updatedAt: '2023-12-14T10:24:17.349Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Nelathan
        - SupPud
    id: 657ad7d1acd6d72f028f951c
    type: comment
  author: MuteXX
  content: 'The base model is a raw LLM; it ONLY does text completion.

    Instruct has been tuned to respond to you instead.'
  created_at: 2023-12-14 10:24:17+00:00
  edited: false
  hidden: false
  id: 657ad7d1acd6d72f028f951c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
      fullname: Yixuan Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yixliu1
      type: user
    createdAt: '2023-12-14T11:05:01.000Z'
    data:
      edited: false
      editors:
      - yixliu1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.316829651594162
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
          fullname: Yixuan Liu
          isHf: false
          isPro: false
          name: yixliu1
          type: user
        html: "<p>Understand. Thx\uFF01</p>\n"
        raw: "Understand. Thx\uFF01"
        updatedAt: '2023-12-14T11:05:01.227Z'
      numEdits: 0
      reactions: []
    id: 657ae15d37d20b27ef318799
    type: comment
  author: yixliu1
  content: "Understand. Thx\uFF01"
  created_at: 2023-12-14 11:05:01+00:00
  edited: false
  hidden: false
  id: 657ae15d37d20b27ef318799
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3d152130bac1ae3ab89d2c3abfbaa76.svg
      fullname: Marek Niemczyk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satoszi
      type: user
    createdAt: '2023-12-21T13:39:25.000Z'
    data:
      edited: false
      editors:
      - Satoszi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9617637395858765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3d152130bac1ae3ab89d2c3abfbaa76.svg
          fullname: Marek Niemczyk
          isHf: false
          isPro: false
          name: Satoszi
          type: user
        html: '<p>"1+1=3" doesn''t necessarily mean that the model was wrong. The
          ''1+1=3'' can mean many different things, such as irony, a metaphor for
          synergy, and it can even be the start of an equation like ''1+1=3-1'' which
          also is correct. </p>

          <p>The issue is NOT that the model is incapable of such a simple operation!
          It''s because it doesn''t understand what you actually want from it. If
          you want to ensure the model knows what you mean, you either have to fine-tune
          it or give an example by prefacing the equation, for example, ''5+3=8 1+1='',
          and now it''s obvious that the expected answer is the sum of 1 and 1.</p>

          <p>Several examples:</p>

          <p>Input: ''5+3=8 1+1='', the model outputs ''2''<br>Input: ''Sum of: 1+1='',
          the model outputs ''2''<br>Input: ''Sum of: 62+16='', the model outputs
          ''78''</p>

          '
        raw: "\"1+1=3\" doesn't necessarily mean that the model was wrong. The '1+1=3'\
          \ can mean many different things, such as irony, a metaphor for synergy,\
          \ and it can even be the start of an equation like '1+1=3-1' which also\
          \ is correct. \n\nThe issue is NOT that the model is incapable of such a\
          \ simple operation! It's because it doesn't understand what you actually\
          \ want from it. If you want to ensure the model knows what you mean, you\
          \ either have to fine-tune it or give an example by prefacing the equation,\
          \ for example, '5+3=8 1+1=', and now it's obvious that the expected answer\
          \ is the sum of 1 and 1.\n\nSeveral examples:\n\nInput: '5+3=8 1+1=', the\
          \ model outputs '2'\nInput: 'Sum of: 1+1=', the model outputs '2'\nInput:\
          \ 'Sum of: 62+16=', the model outputs '78'"
        updatedAt: '2023-12-21T13:39:25.613Z'
      numEdits: 0
      reactions: []
    id: 6584400da76333af9f684588
    type: comment
  author: Satoszi
  content: "\"1+1=3\" doesn't necessarily mean that the model was wrong. The '1+1=3'\
    \ can mean many different things, such as irony, a metaphor for synergy, and it\
    \ can even be the start of an equation like '1+1=3-1' which also is correct. \n\
    \nThe issue is NOT that the model is incapable of such a simple operation! It's\
    \ because it doesn't understand what you actually want from it. If you want to\
    \ ensure the model knows what you mean, you either have to fine-tune it or give\
    \ an example by prefacing the equation, for example, '5+3=8 1+1=', and now it's\
    \ obvious that the expected answer is the sum of 1 and 1.\n\nSeveral examples:\n\
    \nInput: '5+3=8 1+1=', the model outputs '2'\nInput: 'Sum of: 1+1=', the model\
    \ outputs '2'\nInput: 'Sum of: 62+16=', the model outputs '78'"
  created_at: 2023-12-21 13:39:25+00:00
  edited: false
  hidden: false
  id: 6584400da76333af9f684588
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
      fullname: nobody
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MuteXX
      type: user
    createdAt: '2023-12-21T19:13:57.000Z'
    data:
      edited: false
      editors:
      - MuteXX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9361160397529602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb1e72e03430e377d23a57106033c399.svg
          fullname: nobody
          isHf: false
          isPro: false
          name: MuteXX
          type: user
        html: '<p>You can certainly nudge an LLM in the right direction, but they
          are <em>fundamentally</em> incapable of arithmetics or real logical actions
          without external help.<br>Don''t mistake simple calculations being right
          as ability to do maths.</p>

          '
        raw: 'You can certainly nudge an LLM in the right direction, but they are
          *fundamentally* incapable of arithmetics or real logical actions without
          external help.

          Don''t mistake simple calculations being right as ability to do maths.'
        updatedAt: '2023-12-21T19:13:57.513Z'
      numEdits: 0
      reactions: []
    id: 65848e75c8b8bd66f79648f4
    type: comment
  author: MuteXX
  content: 'You can certainly nudge an LLM in the right direction, but they are *fundamentally*
    incapable of arithmetics or real logical actions without external help.

    Don''t mistake simple calculations being right as ability to do maths.'
  created_at: 2023-12-21 19:13:57+00:00
  edited: false
  hidden: false
  id: 65848e75c8b8bd66f79648f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
      fullname: Yixuan Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yixliu1
      type: user
    createdAt: '2023-12-22T03:27:40.000Z'
    data:
      edited: false
      editors:
      - yixliu1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9722630977630615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98156acde9b8a55614269cb7ad89373a.svg
          fullname: Yixuan Liu
          isHf: false
          isPro: false
          name: yixliu1
          type: user
        html: '<blockquote>

          <p>"1+1=3" doesn''t necessarily mean that the model was wrong. The ''1+1=3''
          can mean many different things, such as irony, a metaphor for synergy, and
          it can even be the start of an equation like ''1+1=3-1'' which also is correct.
          </p>

          <p>The issue is NOT that the model is incapable of such a simple operation!
          It''s because it doesn''t understand what you actually want from it. If
          you want to ensure the model knows what you mean, you either have to fine-tune
          it or give an example by prefacing the equation, for example, ''5+3=8 1+1='',
          and now it''s obvious that the expected answer is the sum of 1 and 1.</p>

          <p>Several examples:</p>

          <p>Input: ''5+3=8 1+1='', the model outputs ''2''<br>Input: ''Sum of: 1+1='',
          the model outputs ''2''<br>Input: ''Sum of: 62+16='', the model outputs
          ''78''</p>

          </blockquote>

          <p>Hi Satoszi,<br>Thx for your sharing! I haven''t though from that side.
          I think that''s quite interesting. It''s like LLM has many "capabilities"
          to answer this question but without FT it doesn''t know which one it should
          give. </p>

          '
        raw: "> \"1+1=3\" doesn't necessarily mean that the model was wrong. The '1+1=3'\
          \ can mean many different things, such as irony, a metaphor for synergy,\
          \ and it can even be the start of an equation like '1+1=3-1' which also\
          \ is correct. \n> \n> The issue is NOT that the model is incapable of such\
          \ a simple operation! It's because it doesn't understand what you actually\
          \ want from it. If you want to ensure the model knows what you mean, you\
          \ either have to fine-tune it or give an example by prefacing the equation,\
          \ for example, '5+3=8 1+1=', and now it's obvious that the expected answer\
          \ is the sum of 1 and 1.\n> \n> Several examples:\n> \n> Input: '5+3=8 1+1=',\
          \ the model outputs '2'\n> Input: 'Sum of: 1+1=', the model outputs '2'\n\
          > Input: 'Sum of: 62+16=', the model outputs '78'\n\nHi Satoszi, \nThx for\
          \ your sharing! I haven't though from that side. I think that's quite interesting.\
          \ It's like LLM has many \"capabilities\" to answer this question but without\
          \ FT it doesn't know which one it should give. "
        updatedAt: '2023-12-22T03:27:40.891Z'
      numEdits: 0
      reactions: []
    id: 6585022cb0b0e7e38a2bb4cd
    type: comment
  author: yixliu1
  content: "> \"1+1=3\" doesn't necessarily mean that the model was wrong. The '1+1=3'\
    \ can mean many different things, such as irony, a metaphor for synergy, and it\
    \ can even be the start of an equation like '1+1=3-1' which also is correct. \n\
    > \n> The issue is NOT that the model is incapable of such a simple operation!\
    \ It's because it doesn't understand what you actually want from it. If you want\
    \ to ensure the model knows what you mean, you either have to fine-tune it or\
    \ give an example by prefacing the equation, for example, '5+3=8 1+1=', and now\
    \ it's obvious that the expected answer is the sum of 1 and 1.\n> \n> Several\
    \ examples:\n> \n> Input: '5+3=8 1+1=', the model outputs '2'\n> Input: 'Sum of:\
    \ 1+1=', the model outputs '2'\n> Input: 'Sum of: 62+16=', the model outputs '78'\n\
    \nHi Satoszi, \nThx for your sharing! I haven't though from that side. I think\
    \ that's quite interesting. It's like LLM has many \"capabilities\" to answer\
    \ this question but without FT it doesn't know which one it should give. "
  created_at: 2023-12-22 03:27:40+00:00
  edited: false
  hidden: false
  id: 6585022cb0b0e7e38a2bb4cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3d152130bac1ae3ab89d2c3abfbaa76.svg
      fullname: Marek Niemczyk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satoszi
      type: user
    createdAt: '2023-12-23T18:55:14.000Z'
    data:
      edited: false
      editors:
      - Satoszi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9614413976669312
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3d152130bac1ae3ab89d2c3abfbaa76.svg
          fullname: Marek Niemczyk
          isHf: false
          isPro: false
          name: Satoszi
          type: user
        html: "<blockquote>\n<p>You can certainly nudge an LLM in the right direction,\
          \ but they are <em>fundamentally</em> incapable of arithmetics or real logical\
          \ actions without external help.<br>Don't mistake simple calculations being\
          \ right as ability to do maths.</p>\n</blockquote>\n<p>Of course you are\
          \ right, that LLMs are not good at arithmetics, and they are not built for\
          \ that. We can never trust LLM output in arithmetic problems (and other\
          \ domains too \U0001F642). It'll give approximation that looks legit, but\
          \ for simple operations like sum of small numbers etc that approximation\
          \ should be (usually) correct.</p>\n<blockquote>\n<blockquote>\n<p>\"1+1=3\"\
          \ doesn't necessarily mean that the model was wrong. The '1+1=3' can mean\
          \ many different things, such as irony, a metaphor for synergy, and it can\
          \ even be the start of an equation like '1+1=3-1' which also is correct.\
          \ </p>\n<p>The issue is NOT that the model is incapable of such a simple\
          \ operation! It's because it doesn't understand what you actually want from\
          \ it. If you want to ensure the model knows what you mean, you either have\
          \ to fine-tune it or give an example by prefacing the equation, for example,\
          \ '5+3=8 1+1=', and now it's obvious that the expected answer is the sum\
          \ of 1 and 1.</p>\n<p>Several examples:</p>\n<p>Input: '5+3=8 1+1=', the\
          \ model outputs '2'<br>Input: 'Sum of: 1+1=', the model outputs '2'<br>Input:\
          \ 'Sum of: 62+16=', the model outputs '78'</p>\n</blockquote>\n<p>Hi Satoszi,<br>Thx\
          \ for your sharing! I haven't though from that side. I think that's quite\
          \ interesting. It's like LLM has many \"capabilities\" to answer this question\
          \ but without FT it doesn't know which one it should give.</p>\n</blockquote>\n\
          <p>Yeah vanilla LLMs without reinforcement or other fancy finetuning methods\
          \ are pretty \"stupid\" \U0001F601</p>\n"
        raw: "> You can certainly nudge an LLM in the right direction, but they are\
          \ *fundamentally* incapable of arithmetics or real logical actions without\
          \ external help.\n> Don't mistake simple calculations being right as ability\
          \ to do maths.\n\nOf course you are right, that LLMs are not good at arithmetics,\
          \ and they are not built for that. We can never trust LLM output in arithmetic\
          \ problems (and other domains too \U0001F642). It'll give approximation\
          \ that looks legit, but for simple operations like sum of small numbers\
          \ etc that approximation should be (usually) correct.\n\n> > \"1+1=3\" doesn't\
          \ necessarily mean that the model was wrong. The '1+1=3' can mean many different\
          \ things, such as irony, a metaphor for synergy, and it can even be the\
          \ start of an equation like '1+1=3-1' which also is correct. \n> > \n> >\
          \ The issue is NOT that the model is incapable of such a simple operation!\
          \ It's because it doesn't understand what you actually want from it. If\
          \ you want to ensure the model knows what you mean, you either have to fine-tune\
          \ it or give an example by prefacing the equation, for example, '5+3=8 1+1=',\
          \ and now it's obvious that the expected answer is the sum of 1 and 1.\n\
          > > \n> > Several examples:\n> > \n> > Input: '5+3=8 1+1=', the model outputs\
          \ '2'\n> > Input: 'Sum of: 1+1=', the model outputs '2'\n> > Input: 'Sum\
          \ of: 62+16=', the model outputs '78'\n> \n> Hi Satoszi, \n> Thx for your\
          \ sharing! I haven't though from that side. I think that's quite interesting.\
          \ It's like LLM has many \"capabilities\" to answer this question but without\
          \ FT it doesn't know which one it should give.\n\nYeah vanilla LLMs without\
          \ reinforcement or other fancy finetuning methods are pretty \"stupid\"\
          \ \U0001F601"
        updatedAt: '2023-12-23T18:55:14.041Z'
      numEdits: 0
      reactions: []
    id: 65872d12e878be571b1a828f
    type: comment
  author: Satoszi
  content: "> You can certainly nudge an LLM in the right direction, but they are\
    \ *fundamentally* incapable of arithmetics or real logical actions without external\
    \ help.\n> Don't mistake simple calculations being right as ability to do maths.\n\
    \nOf course you are right, that LLMs are not good at arithmetics, and they are\
    \ not built for that. We can never trust LLM output in arithmetic problems (and\
    \ other domains too \U0001F642). It'll give approximation that looks legit, but\
    \ for simple operations like sum of small numbers etc that approximation should\
    \ be (usually) correct.\n\n> > \"1+1=3\" doesn't necessarily mean that the model\
    \ was wrong. The '1+1=3' can mean many different things, such as irony, a metaphor\
    \ for synergy, and it can even be the start of an equation like '1+1=3-1' which\
    \ also is correct. \n> > \n> > The issue is NOT that the model is incapable of\
    \ such a simple operation! It's because it doesn't understand what you actually\
    \ want from it. If you want to ensure the model knows what you mean, you either\
    \ have to fine-tune it or give an example by prefacing the equation, for example,\
    \ '5+3=8 1+1=', and now it's obvious that the expected answer is the sum of 1\
    \ and 1.\n> > \n> > Several examples:\n> > \n> > Input: '5+3=8 1+1=', the model\
    \ outputs '2'\n> > Input: 'Sum of: 1+1=', the model outputs '2'\n> > Input: 'Sum\
    \ of: 62+16=', the model outputs '78'\n> \n> Hi Satoszi, \n> Thx for your sharing!\
    \ I haven't though from that side. I think that's quite interesting. It's like\
    \ LLM has many \"capabilities\" to answer this question but without FT it doesn't\
    \ know which one it should give.\n\nYeah vanilla LLMs without reinforcement or\
    \ other fancy finetuning methods are pretty \"stupid\" \U0001F601"
  created_at: 2023-12-23 18:55:14+00:00
  edited: false
  hidden: false
  id: 65872d12e878be571b1a828f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: mistralai/Mixtral-8x7B-v0.1
repo_type: model
status: open
target_branch: null
title: Wrong solution for 1+1=
