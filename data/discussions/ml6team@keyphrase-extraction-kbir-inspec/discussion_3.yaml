!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Janni
conflicting_files: null
created_at: 2023-01-09 12:50:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c91a9549dd21df562c3d2b6682da698.svg
      fullname: Jan Biel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Janni
      type: user
    createdAt: '2023-01-09T12:50:46.000Z'
    data:
      edited: true
      editors:
      - Janni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c91a9549dd21df562c3d2b6682da698.svg
          fullname: Jan Biel
          isHf: false
          isPro: false
          name: Janni
          type: user
        html: '<p>Hey, im playing around with your model and trying to figure out
          if i can use the hidden_states for semantic search.</p>

          <p>Can you explain to me why for an empty input, the dimensionality of the
          hidden_state is torch.Size([25, 1, 2, 1024])?<br>As far as i can see the
          encoder Robertaencoder has 24*RobertaLayer, where is the 25 coming from?<br>Shouldnt
          the dimensionality be num_hidden_layers * 1 * tokens * hidden_size?</p>

          '
        raw: 'Hey, im playing around with your model and trying to figure out if i
          can use the hidden_states for semantic search.


          Can you explain to me why for an empty input, the dimensionality of the
          hidden_state is torch.Size([25, 1, 2, 1024])?

          As far as i can see the encoder Robertaencoder has 24*RobertaLayer, where
          is the 25 coming from?

          Shouldnt the dimensionality be num_hidden_layers * 1 * tokens * hidden_size?'
        updatedAt: '2023-01-09T21:02:57.974Z'
      numEdits: 1
      reactions: []
    id: 63bc0da6d8d676a2299dbb32
    type: comment
  author: Janni
  content: 'Hey, im playing around with your model and trying to figure out if i can
    use the hidden_states for semantic search.


    Can you explain to me why for an empty input, the dimensionality of the hidden_state
    is torch.Size([25, 1, 2, 1024])?

    As far as i can see the encoder Robertaencoder has 24*RobertaLayer, where is the
    25 coming from?

    Shouldnt the dimensionality be num_hidden_layers * 1 * tokens * hidden_size?'
  created_at: 2023-01-09 12:50:46+00:00
  edited: true
  hidden: false
  id: 63bc0da6d8d676a2299dbb32
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/0c91a9549dd21df562c3d2b6682da698.svg
      fullname: Jan Biel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Janni
      type: user
    createdAt: '2023-01-09T12:50:54.000Z'
    data:
      from: hidden_states
      to: hidden_states dimensionality
    id: 63bc0daeebc6b720d6058839
    type: title-change
  author: Janni
  created_at: 2023-01-09 12:50:54+00:00
  id: 63bc0daeebc6b720d6058839
  new_title: hidden_states dimensionality
  old_title: hidden_states
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640511710953-noauth.jpeg?w=200&h=200&f=face
      fullname: Thomas De Decker
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DeDeckerThomas
      type: user
    createdAt: '2023-01-09T17:18:29.000Z'
    data:
      edited: false
      editors:
      - DeDeckerThomas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640511710953-noauth.jpeg?w=200&h=200&f=face
          fullname: Thomas De Decker
          isHf: false
          isPro: false
          name: DeDeckerThomas
          type: user
        html: '<p>Hey,</p>

          <p>Thank you for using this model! Could you please provide a code snippet,
          so I can know what you are trying to do?</p>

          '
        raw: 'Hey,


          Thank you for using this model! Could you please provide a code snippet,
          so I can know what you are trying to do?'
        updatedAt: '2023-01-09T17:18:29.902Z'
      numEdits: 0
      reactions: []
    id: 63bc4c6518e2f66c5ea11b3d
    type: comment
  author: DeDeckerThomas
  content: 'Hey,


    Thank you for using this model! Could you please provide a code snippet, so I
    can know what you are trying to do?'
  created_at: 2023-01-09 17:18:29+00:00
  edited: false
  hidden: false
  id: 63bc4c6518e2f66c5ea11b3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0c91a9549dd21df562c3d2b6682da698.svg
      fullname: Jan Biel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Janni
      type: user
    createdAt: '2023-01-09T20:58:38.000Z'
    data:
      edited: false
      editors:
      - Janni
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0c91a9549dd21df562c3d2b6682da698.svg
          fullname: Jan Biel
          isHf: false
          isPro: false
          name: Janni
          type: user
        html: "<p>Sure changed config to \"output_hidden_states:true\"</p>\n<p>from\
          \ transformers import (<br>    TokenClassificationPipeline,<br>    AutoModelForTokenClassification,<br>\
          \    AutoTokenizer,<br>)</p>\n<h1 id=\"define-keyphrase-extraction-pipeline\"\
          >Define keyphrase extraction pipeline</h1>\n<p>class KeyphraseExtractionPipeline(TokenClassificationPipeline):<br>\
          \   def <strong>init</strong>(self, model, *args, **kwargs):<br>       \
          \ super().<strong>init</strong>(<br>            model=AutoModelForTokenClassification.from_pretrained(model_path),<br>\
          \            tokenizer=AutoTokenizer.from_pretrained(model),<br>       \
          \     *args,<br>            **kwargs<br>        )</p>\n<pre><code>def _forward(self,\
          \ model_inputs):\n    # Forward\n    special_tokens_mask = model_inputs.pop(\"\
          special_tokens_mask\")\n    offset_mapping = model_inputs.pop(\"offset_mapping\"\
          , None)\n    sentence = model_inputs.pop(\"sentence\")\n    outputs = self.model(**model_inputs)\n\
          \    logits = outputs[0]\n\n    # I am talking abouth these outputs, they\
          \ now contain [\"hidden_state\"]\n    embedding = get_embeddings(outputs[1])\n\
          \n    return {\n        \"logits\": logits,\n        \"special_tokens_mask\"\
          : special_tokens_mask,\n        \"offset_mapping\": offset_mapping,\n  \
          \      \"sentence\": sentence,\n        \"hidden_state\": outputs[1],\n\
          \        \"embedding\": embedding,\n        **model_inputs,\n    }\n\n\n\
          def postprocess(self, model_outputs):\n    results = super().postprocess(\n\
          \        model_outputs=model_outputs,\n        aggregation_strategy=AggregationStrategy.SIMPLE,\n\
          \    )\n    return {**model_outputs, **{\"keywords\": np.unique([result.get(\"\
          word\").strip() for result in results]).tolist()}}\n</code></pre>\n<p>model_path\
          \ = \"keyphrase-extraction-kbir-inspec\"<br>extractor = KeyphraseExtractionPipeline(model=model_path)</p>\n"
        raw: "Sure changed config to \"output_hidden_states:true\"\n\nfrom transformers\
          \ import (\n    TokenClassificationPipeline,\n    AutoModelForTokenClassification,\n\
          \    AutoTokenizer,\n)\n\n# Define keyphrase extraction pipeline\nclass\
          \ KeyphraseExtractionPipeline(TokenClassificationPipeline):\n   def __init__(self,\
          \ model, *args, **kwargs):\n        super().__init__(\n            model=AutoModelForTokenClassification.from_pretrained(model_path),\n\
          \            tokenizer=AutoTokenizer.from_pretrained(model),\n         \
          \   *args,\n            **kwargs\n        )\n        \n    def _forward(self,\
          \ model_inputs):\n        # Forward\n        special_tokens_mask = model_inputs.pop(\"\
          special_tokens_mask\")\n        offset_mapping = model_inputs.pop(\"offset_mapping\"\
          , None)\n        sentence = model_inputs.pop(\"sentence\")\n        outputs\
          \ = self.model(**model_inputs)\n        logits = outputs[0]\n\n        #\
          \ I am talking abouth these outputs, they now contain [\"hidden_state\"\
          ]\n        embedding = get_embeddings(outputs[1])\n\n        return {\n\
          \            \"logits\": logits,\n            \"special_tokens_mask\": special_tokens_mask,\n\
          \            \"offset_mapping\": offset_mapping,\n            \"sentence\"\
          : sentence,\n            \"hidden_state\": outputs[1],\n            \"embedding\"\
          : embedding,\n            **model_inputs,\n        }\n    \n    \n    def\
          \ postprocess(self, model_outputs):\n        results = super().postprocess(\n\
          \            model_outputs=model_outputs,\n            aggregation_strategy=AggregationStrategy.SIMPLE,\n\
          \        )\n        return {**model_outputs, **{\"keywords\": np.unique([result.get(\"\
          word\").strip() for result in results]).tolist()}}\n\nmodel_path = \"keyphrase-extraction-kbir-inspec\"\
          \nextractor = KeyphraseExtractionPipeline(model=model_path)"
        updatedAt: '2023-01-09T20:58:38.822Z'
      numEdits: 0
      reactions: []
    id: 63bc7ffe18e2f66c5ea574fd
    type: comment
  author: Janni
  content: "Sure changed config to \"output_hidden_states:true\"\n\nfrom transformers\
    \ import (\n    TokenClassificationPipeline,\n    AutoModelForTokenClassification,\n\
    \    AutoTokenizer,\n)\n\n# Define keyphrase extraction pipeline\nclass KeyphraseExtractionPipeline(TokenClassificationPipeline):\n\
    \   def __init__(self, model, *args, **kwargs):\n        super().__init__(\n \
    \           model=AutoModelForTokenClassification.from_pretrained(model_path),\n\
    \            tokenizer=AutoTokenizer.from_pretrained(model),\n            *args,\n\
    \            **kwargs\n        )\n        \n    def _forward(self, model_inputs):\n\
    \        # Forward\n        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\"\
    )\n        offset_mapping = model_inputs.pop(\"offset_mapping\", None)\n     \
    \   sentence = model_inputs.pop(\"sentence\")\n        outputs = self.model(**model_inputs)\n\
    \        logits = outputs[0]\n\n        # I am talking abouth these outputs, they\
    \ now contain [\"hidden_state\"]\n        embedding = get_embeddings(outputs[1])\n\
    \n        return {\n            \"logits\": logits,\n            \"special_tokens_mask\"\
    : special_tokens_mask,\n            \"offset_mapping\": offset_mapping,\n    \
    \        \"sentence\": sentence,\n            \"hidden_state\": outputs[1],\n\
    \            \"embedding\": embedding,\n            **model_inputs,\n        }\n\
    \    \n    \n    def postprocess(self, model_outputs):\n        results = super().postprocess(\n\
    \            model_outputs=model_outputs,\n            aggregation_strategy=AggregationStrategy.SIMPLE,\n\
    \        )\n        return {**model_outputs, **{\"keywords\": np.unique([result.get(\"\
    word\").strip() for result in results]).tolist()}}\n\nmodel_path = \"keyphrase-extraction-kbir-inspec\"\
    \nextractor = KeyphraseExtractionPipeline(model=model_path)"
  created_at: 2023-01-09 20:58:38+00:00
  edited: false
  hidden: false
  id: 63bc7ffe18e2f66c5ea574fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640511710953-noauth.jpeg?w=200&h=200&f=face
      fullname: Thomas De Decker
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DeDeckerThomas
      type: user
    createdAt: '2023-01-24T21:38:09.000Z'
    data:
      edited: false
      editors:
      - DeDeckerThomas
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640511710953-noauth.jpeg?w=200&h=200&f=face
          fullname: Thomas De Decker
          isHf: false
          isPro: false
          name: DeDeckerThomas
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Janni&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Janni\">@<span class=\"\
          underline\">Janni</span></a></span>\n\n\t</span></span> ,</p>\n<p>There\
          \ is a reason why there are 25 layers. The output of the embeddings is also\
          \ included. So the hidden layers exist out of the output of each layer +\
          \ the output of the embeddings.</p>\n<p>You can find more information in\
          \ this GitHub thread: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/1332#issuecomment-535374376\"\
          >https://github.com/huggingface/transformers/issues/1332</a>.<br>You can\
          \ also find this in <a href=\"https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/roberta#transformers.RobertaModel\"\
          >the HuggingFace Roberta documentation</a>.</p>\n<p>Hope it helps!</p>\n\
          <p>Kind regards,<br>Thomas De Decker</p>\n"
        raw: 'Hey @Janni ,


          There is a reason why there are 25 layers. The output of the embeddings
          is also included. So the hidden layers exist out of the output of each layer
          + the output of the embeddings.


          You can find more information in this GitHub thread: [https://github.com/huggingface/transformers/issues/1332](https://github.com/huggingface/transformers/issues/1332#issuecomment-535374376).

          You can also find this in [the HuggingFace Roberta documentation](https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/roberta#transformers.RobertaModel).


          Hope it helps!


          Kind regards,

          Thomas De Decker'
        updatedAt: '2023-01-24T21:38:09.698Z'
      numEdits: 0
      reactions: []
    id: 63d04fc12a72b1e24996c29c
    type: comment
  author: DeDeckerThomas
  content: 'Hey @Janni ,


    There is a reason why there are 25 layers. The output of the embeddings is also
    included. So the hidden layers exist out of the output of each layer + the output
    of the embeddings.


    You can find more information in this GitHub thread: [https://github.com/huggingface/transformers/issues/1332](https://github.com/huggingface/transformers/issues/1332#issuecomment-535374376).

    You can also find this in [the HuggingFace Roberta documentation](https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/roberta#transformers.RobertaModel).


    Hope it helps!


    Kind regards,

    Thomas De Decker'
  created_at: 2023-01-24 21:38:09+00:00
  edited: false
  hidden: false
  id: 63d04fc12a72b1e24996c29c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640511710953-noauth.jpeg?w=200&h=200&f=face
      fullname: Thomas De Decker
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DeDeckerThomas
      type: user
    createdAt: '2023-05-06T08:56:23.000Z'
    data:
      status: closed
    id: 64561637cd6567f52fb00cec
    type: status-change
  author: DeDeckerThomas
  created_at: 2023-05-06 07:56:23+00:00
  id: 64561637cd6567f52fb00cec
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: ml6team/keyphrase-extraction-kbir-inspec
repo_type: model
status: closed
target_branch: null
title: hidden_states dimensionality
