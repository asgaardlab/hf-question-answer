!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grandignatz
conflicting_files: null
created_at: 2024-01-05 19:45:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/4a42aAA0tNaNim1Tl-nSb.png?w=200&h=200&f=face
      fullname: "Andr\xE9 Wirth"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grandignatz
      type: user
    createdAt: '2024-01-05T19:45:21.000Z'
    data:
      edited: false
      editors:
      - grandignatz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7001608610153198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/4a42aAA0tNaNim1Tl-nSb.png?w=200&h=200&f=face
          fullname: "Andr\xE9 Wirth"
          isHf: false
          isPro: false
          name: grandignatz
          type: user
        html: '<p>The runpod template installation always gets stuck at:<br>text_generation_launcher:
          Download file: model-00006-of-00019.safetensors</p>

          <p>We tried multiple Pods (H100, A100, A6000) and everywhere it gets stuck
          at model part 6.</p>

          <p>Runpod Support was not able to help.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/K01Uvs7IN5YERW8ccG5ir.png"><img
          alt="Bildschirmfoto 2024-01-05 um 20.42.28.png" src="https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/K01Uvs7IN5YERW8ccG5ir.png"></a></p>

          '
        raw: "The runpod template installation always gets stuck at: \r\ntext_generation_launcher:\
          \ Download file: model-00006-of-00019.safetensors\r\n\r\nWe tried multiple\
          \ Pods (H100, A100, A6000) and everywhere it gets stuck at model part 6.\r\
          \n\r\nRunpod Support was not able to help.\r\n\r\n![Bildschirmfoto 2024-01-05\
          \ um 20.42.28.png](https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/K01Uvs7IN5YERW8ccG5ir.png)\r\
          \n"
        updatedAt: '2024-01-05T19:45:21.101Z'
      numEdits: 0
      reactions: []
    id: 65985c512fe7ca485f5d725a
    type: comment
  author: grandignatz
  content: "The runpod template installation always gets stuck at: \r\ntext_generation_launcher:\
    \ Download file: model-00006-of-00019.safetensors\r\n\r\nWe tried multiple Pods\
    \ (H100, A100, A6000) and everywhere it gets stuck at model part 6.\r\n\r\nRunpod\
    \ Support was not able to help.\r\n\r\n![Bildschirmfoto 2024-01-05 um 20.42.28.png](https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/K01Uvs7IN5YERW8ccG5ir.png)\r\
    \n"
  created_at: 2024-01-05 19:45:21+00:00
  edited: false
  hidden: false
  id: 65985c512fe7ca485f5d725a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-06T11:36:47.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9757419228553772
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Yeah, you can get it to work by reloading the pod a few times, this
          restarts the downloading from the last shard and eventually you''ll get
          them loaded.</p>

          <p>I''m working on pushing 8bit and 4bit models to hub that will reduce
          download size and speed and maybe side-step the issue. I''ve already updated
          the runpod template to download 8bit weights. Testing that now and will
          get it working on Monday.</p>

          '
        raw: 'Yeah, you can get it to work by reloading the pod a few times, this
          restarts the downloading from the last shard and eventually you''ll get
          them loaded.


          I''m working on pushing 8bit and 4bit models to hub that will reduce download
          size and speed and maybe side-step the issue. I''ve already updated the
          runpod template to download 8bit weights. Testing that now and will get
          it working on Monday.'
        updatedAt: '2024-01-06T11:36:47.714Z'
      numEdits: 0
      reactions: []
    id: 65993b4f8c5c66888670bdce
    type: comment
  author: RonanMcGovern
  content: 'Yeah, you can get it to work by reloading the pod a few times, this restarts
    the downloading from the last shard and eventually you''ll get them loaded.


    I''m working on pushing 8bit and 4bit models to hub that will reduce download
    size and speed and maybe side-step the issue. I''ve already updated the runpod
    template to download 8bit weights. Testing that now and will get it working on
    Monday.'
  created_at: 2024-01-06 11:36:47+00:00
  edited: false
  hidden: false
  id: 65993b4f8c5c66888670bdce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/4a42aAA0tNaNim1Tl-nSb.png?w=200&h=200&f=face
      fullname: "Andr\xE9 Wirth"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grandignatz
      type: user
    createdAt: '2024-01-06T12:51:43.000Z'
    data:
      edited: false
      editors:
      - grandignatz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9530720114707947
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/4a42aAA0tNaNim1Tl-nSb.png?w=200&h=200&f=face
          fullname: "Andr\xE9 Wirth"
          isHf: false
          isPro: false
          name: grandignatz
          type: user
        html: '<p>Ok, great thank you. I already tried restarting the different pods
          like 20 times yesterday but never got get past number 6. Will try again.</p>

          '
        raw: Ok, great thank you. I already tried restarting the different pods like
          20 times yesterday but never got get past number 6. Will try again.
        updatedAt: '2024-01-06T12:51:43.567Z'
      numEdits: 0
      reactions: []
    id: 65994cdf4d58d86c3e14a3b2
    type: comment
  author: grandignatz
  content: Ok, great thank you. I already tried restarting the different pods like
    20 times yesterday but never got get past number 6. Will try again.
  created_at: 2024-01-06 12:51:43+00:00
  edited: false
  hidden: false
  id: 65994cdf4d58d86c3e14a3b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/4a42aAA0tNaNim1Tl-nSb.png?w=200&h=200&f=face
      fullname: "Andr\xE9 Wirth"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grandignatz
      type: user
    createdAt: '2024-01-06T14:19:16.000Z'
    data:
      edited: false
      editors:
      - grandignatz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5641669034957886
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643f0063e2ea47d1700fac88/4a42aAA0tNaNim1Tl-nSb.png?w=200&h=200&f=face
          fullname: "Andr\xE9 Wirth"
          isHf: false
          isPro: false
          name: grandignatz
          type: user
        html: "<p>I tried it again and it seams like it now used the 8 bit branch\
          \ with only 5 parts. It completed the download but i only get a empty response\
          \ from the api:<br>{\"generated_text\":\"\"}</p>\n<p>Server log says:<br>generate{parameters=GenerateParameters\
          \ { best_of: None, temperature: None, repetition_penalty: None, top_k: None,\
          \ top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(200),\
          \ return_full_text: None, stop: [], truncate: None, watermark: false, details:\
          \ false, decoder_input_details: false, seed: None, top_n_tokens: None }\
          \ total_time=\"15.697707697s\" validation_time=\"465.588\xB5s\" queue_time=\"\
          86.82\xB5s\" inference_time=\"15.697155549s\" time_per_token=\"78.485777ms\"\
          \ seed=\"None\"}: text_generation_router::server: router/src/server.rs:289:\
          \ Success</p>\n<p>With generate_streame i get:<br>data:{\"token\":{\"id\"\
          :0,\"text\":\"\",\"logprob\":null,\"special\":true},\"generated_text\":null,\"\
          details\":null}</p>\n"
        raw: "I tried it again and it seams like it now used the 8 bit branch with\
          \ only 5 parts. It completed the download but i only get a empty response\
          \ from the api:\n{\"generated_text\":\"\"}\n\nServer log says:\ngenerate{parameters=GenerateParameters\
          \ { best_of: None, temperature: None, repetition_penalty: None, top_k: None,\
          \ top_p: None, typical_p: None, do_sample: false, max_new_tokens: Some(200),\
          \ return_full_text: None, stop: [], truncate: None, watermark: false, details:\
          \ false, decoder_input_details: false, seed: None, top_n_tokens: None }\
          \ total_time=\"15.697707697s\" validation_time=\"465.588\xB5s\" queue_time=\"\
          86.82\xB5s\" inference_time=\"15.697155549s\" time_per_token=\"78.485777ms\"\
          \ seed=\"None\"}: text_generation_router::server: router/src/server.rs:289:\
          \ Success\n\nWith generate_streame i get:\ndata:{\"token\":{\"id\":0,\"\
          text\":\"<unk>\",\"logprob\":null,\"special\":true},\"generated_text\":null,\"\
          details\":null}\n"
        updatedAt: '2024-01-06T14:19:16.551Z'
      numEdits: 0
      reactions: []
    id: 659961646d20ab21b013e4a1
    type: comment
  author: grandignatz
  content: "I tried it again and it seams like it now used the 8 bit branch with only\
    \ 5 parts. It completed the download but i only get a empty response from the\
    \ api:\n{\"generated_text\":\"\"}\n\nServer log says:\ngenerate{parameters=GenerateParameters\
    \ { best_of: None, temperature: None, repetition_penalty: None, top_k: None, top_p:\
    \ None, typical_p: None, do_sample: false, max_new_tokens: Some(200), return_full_text:\
    \ None, stop: [], truncate: None, watermark: false, details: false, decoder_input_details:\
    \ false, seed: None, top_n_tokens: None } total_time=\"15.697707697s\" validation_time=\"\
    465.588\xB5s\" queue_time=\"86.82\xB5s\" inference_time=\"15.697155549s\" time_per_token=\"\
    78.485777ms\" seed=\"None\"}: text_generation_router::server: router/src/server.rs:289:\
    \ Success\n\nWith generate_streame i get:\ndata:{\"token\":{\"id\":0,\"text\"\
    :\"<unk>\",\"logprob\":null,\"special\":true},\"generated_text\":null,\"details\"\
    :null}\n"
  created_at: 2024-01-06 14:19:16+00:00
  edited: false
  hidden: false
  id: 659961646d20ab21b013e4a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-08T12:44:58.000Z'
    data:
      from: Issue Runpod Deployment
      to: Runpod Deployment Troubleshooting
    id: 659bee4ab0c5357368b5f06c
    type: title-change
  author: RonanMcGovern
  created_at: 2024-01-08 12:44:58+00:00
  id: 659bee4ab0c5357368b5f06c
  new_title: Runpod Deployment Troubleshooting
  old_title: Issue Runpod Deployment
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-08T13:06:29.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9583063721656799
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Hi folks, some guidance here.</p>

          <h2 id="best-current-approach-use-the-main-branch-and-then---quantize-eetq">Best
          Current Approach (use the main branch and then --quantize eetq):</h2>

          <ul>

          <li>I''ve just set the pod to download the 16-bit weights from the main
          branch.</li>

          <li>There are often issues downloading the weights. Downloading gets stuck
          at various points and requires you to click on the three lines and then
          "Restart Pod" in a few points. Typically I need to re-start 3-4 times to
          get all of the weights downloaded. After download, it can take 10-15 mins
          for the shards to load onto the GPU (at least for an A6000).</li>

          <li>The Runpod template is the one on the main model card.</li>

          </ul>

          <h2 id="work-in-progress-a">Work in Progress #A</h2>

          <ul>

          <li>Ideally, rather than downloading the full 16-bit weights, we would download
          8-bit or 4-bit (nf4) weights.</li>

          <li>However, there is a bug stopping 8 and 4-bit weights being pushed to
          hub. I have opened issues (4bit, 8bit) and will write back here when I have
          more updates.</li>

          </ul>

          <h2 id="work-in-progress-b">Work in Progress #B</h2>

          <ul>

          <li>I don''t know the root cause of the weights getting stuck, but I see
          that issue with the raw Mixtral model as well. I have posted an <a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference/issues/1413">issue
          on that in TGI</a>.</li>

          </ul>

          '
        raw: 'Hi folks, some guidance here.


          ## Best Current Approach (use the main branch and then --quantize eetq):

          - I''ve just set the pod to download the 16-bit weights from the main branch.

          - There are often issues downloading the weights. Downloading gets stuck
          at various points and requires you to click on the three lines and then
          "Restart Pod" in a few points. Typically I need to re-start 3-4 times to
          get all of the weights downloaded. After download, it can take 10-15 mins
          for the shards to load onto the GPU (at least for an A6000).

          - The Runpod template is the one on the main model card.


          ## Work in Progress #A

          - Ideally, rather than downloading the full 16-bit weights, we would download
          8-bit or 4-bit (nf4) weights.

          - However, there is a bug stopping 8 and 4-bit weights being pushed to hub.
          I have opened issues (4bit, 8bit) and will write back here when I have more
          updates.


          ## Work in Progress #B

          - I don''t know the root cause of the weights getting stuck, but I see that
          issue with the raw Mixtral model as well. I have posted an [issue on that
          in TGI](https://github.com/huggingface/text-generation-inference/issues/1413).'
        updatedAt: '2024-01-08T13:15:34.522Z'
      numEdits: 1
      reactions: []
    id: 659bf35509ace14a949d533a
    type: comment
  author: RonanMcGovern
  content: 'Hi folks, some guidance here.


    ## Best Current Approach (use the main branch and then --quantize eetq):

    - I''ve just set the pod to download the 16-bit weights from the main branch.

    - There are often issues downloading the weights. Downloading gets stuck at various
    points and requires you to click on the three lines and then "Restart Pod" in
    a few points. Typically I need to re-start 3-4 times to get all of the weights
    downloaded. After download, it can take 10-15 mins for the shards to load onto
    the GPU (at least for an A6000).

    - The Runpod template is the one on the main model card.


    ## Work in Progress #A

    - Ideally, rather than downloading the full 16-bit weights, we would download
    8-bit or 4-bit (nf4) weights.

    - However, there is a bug stopping 8 and 4-bit weights being pushed to hub. I
    have opened issues (4bit, 8bit) and will write back here when I have more updates.


    ## Work in Progress #B

    - I don''t know the root cause of the weights getting stuck, but I see that issue
    with the raw Mixtral model as well. I have posted an [issue on that in TGI](https://github.com/huggingface/text-generation-inference/issues/1413).'
  created_at: 2024-01-08 13:06:29+00:00
  edited: true
  hidden: false
  id: 659bf35509ace14a949d533a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6407d2e24edf9f5c4fd26c01/gsQJj2EMG4H7eQ7qaL1Xb.png?w=200&h=200&f=face
      fullname: Reed Bender
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: reedbender
      type: user
    createdAt: '2024-01-10T12:29:39.000Z'
    data:
      edited: false
      editors:
      - reedbender
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9757635593414307
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6407d2e24edf9f5c4fd26c01/gsQJj2EMG4H7eQ7qaL1Xb.png?w=200&h=200&f=face
          fullname: Reed Bender
          isHf: false
          isPro: false
          name: reedbender
          type: user
        html: '<p>I have been able to get past the model weights being stuck, but
          once I get the model successfully deployed the generated_text is still empty.
          Is there a resolution for the response being empty after successful deployment?</p>

          '
        raw: I have been able to get past the model weights being stuck, but once
          I get the model successfully deployed the generated_text is still empty.
          Is there a resolution for the response being empty after successful deployment?
        updatedAt: '2024-01-10T12:29:39.712Z'
      numEdits: 0
      reactions: []
    id: 659e8db3ecad0e6acad785ea
    type: comment
  author: reedbender
  content: I have been able to get past the model weights being stuck, but once I
    get the model successfully deployed the generated_text is still empty. Is there
    a resolution for the response being empty after successful deployment?
  created_at: 2024-01-10 12:29:39+00:00
  edited: false
  hidden: false
  id: 659e8db3ecad0e6acad785ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-10T14:23:48.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8325412273406982
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Howdy Reed. I've just tested again and the template on the model\
          \ card is working. For example - using the ADVANCED inference repo - I'm\
          \ getting:</p>\n<pre><code>user: What clothes should I wear? I am in Dublin\n\
          \nfunction_call: {\n    \"name\": \"get_current_weather\",\n    \"arguments\"\
          : {\n        \"city\": \"Dublin\"\n    }\n}\n\nfunction_response: {\n  \
          \  \"temperature\": \"18 C\",\n    \"condition\": \"Partly Cloudy\"\n}\n\
          \nassistant:You should wear a sweater\n</code></pre>\n<p>This matches the\
          \ YouTube video about Mixtral. I also ran a test with no functions and it\
          \ ran fine (a speed test as per the youtube video).</p>\n<p>Are you using\
          \ apply_chat_template ? The prompt formatting is crucial.</p>\n<p>P.S. I'm\
          \ working on making an AWQ template now that should be quicker to download.</p>\n"
        raw: "Howdy Reed. I've just tested again and the template on the model card\
          \ is working. For example - using the ADVANCED inference repo - I'm getting:\n\
          ```\nuser: What clothes should I wear? I am in Dublin\n\nfunction_call:\
          \ {\n    \"name\": \"get_current_weather\",\n    \"arguments\": {\n    \
          \    \"city\": \"Dublin\"\n    }\n}\n\nfunction_response: {\n    \"temperature\"\
          : \"18 C\",\n    \"condition\": \"Partly Cloudy\"\n}\n\nassistant:You should\
          \ wear a sweater\n```\nThis matches the YouTube video about Mixtral. I also\
          \ ran a test with no functions and it ran fine (a speed test as per the\
          \ youtube video).\n\nAre you using apply_chat_template ? The prompt formatting\
          \ is crucial.\n\nP.S. I'm working on making an AWQ template now that should\
          \ be quicker to download."
        updatedAt: '2024-01-10T14:23:48.988Z'
      numEdits: 0
      reactions: []
    id: 659ea8746f296a1ceffc240b
    type: comment
  author: RonanMcGovern
  content: "Howdy Reed. I've just tested again and the template on the model card\
    \ is working. For example - using the ADVANCED inference repo - I'm getting:\n\
    ```\nuser: What clothes should I wear? I am in Dublin\n\nfunction_call: {\n  \
    \  \"name\": \"get_current_weather\",\n    \"arguments\": {\n        \"city\"\
    : \"Dublin\"\n    }\n}\n\nfunction_response: {\n    \"temperature\": \"18 C\"\
    ,\n    \"condition\": \"Partly Cloudy\"\n}\n\nassistant:You should wear a sweater\n\
    ```\nThis matches the YouTube video about Mixtral. I also ran a test with no functions\
    \ and it ran fine (a speed test as per the youtube video).\n\nAre you using apply_chat_template\
    \ ? The prompt formatting is crucial.\n\nP.S. I'm working on making an AWQ template\
    \ now that should be quicker to download."
  created_at: 2024-01-10 14:23:48+00:00
  edited: false
  hidden: false
  id: 659ea8746f296a1ceffc240b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-10T15:50:12.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8883664608001709
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Ok, the AWQ one-click runpod template is now on the model card.
          This is now the recommended way to run inference. The model is about 25
          GB (instead of ~100 GB) so it will be quicker to download.</p>

          <p>I''m therefore closing this issue.</p>

          <p>The downloading bug with TGI remains open on their github <a rel="nofollow"
          href="https://github.com/huggingface/text-generation-inference/issues/1186">here</a>.</p>

          <p>If you face new issues, just create a new issue - and please provide
          enough details that I can replicate.</p>

          '
        raw: 'Ok, the AWQ one-click runpod template is now on the model card. This
          is now the recommended way to run inference. The model is about 25 GB (instead
          of ~100 GB) so it will be quicker to download.


          I''m therefore closing this issue.


          The downloading bug with TGI remains open on their github [here](https://github.com/huggingface/text-generation-inference/issues/1186).


          If you face new issues, just create a new issue - and please provide enough
          details that I can replicate.'
        updatedAt: '2024-01-10T15:50:12.882Z'
      numEdits: 0
      reactions: []
    id: 659ebcb40009c13aee0221e9
    type: comment
  author: RonanMcGovern
  content: 'Ok, the AWQ one-click runpod template is now on the model card. This is
    now the recommended way to run inference. The model is about 25 GB (instead of
    ~100 GB) so it will be quicker to download.


    I''m therefore closing this issue.


    The downloading bug with TGI remains open on their github [here](https://github.com/huggingface/text-generation-inference/issues/1186).


    If you face new issues, just create a new issue - and please provide enough details
    that I can replicate.'
  created_at: 2024-01-10 15:50:12+00:00
  edited: false
  hidden: false
  id: 659ebcb40009c13aee0221e9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2024-01-10T17:53:40.000Z'
    data:
      status: closed
    id: 659ed9a47972fae4fd5173ef
    type: status-change
  author: RonanMcGovern
  created_at: 2024-01-10 17:53:40+00:00
  id: 659ed9a47972fae4fd5173ef
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6407d2e24edf9f5c4fd26c01/gsQJj2EMG4H7eQ7qaL1Xb.png?w=200&h=200&f=face
      fullname: Reed Bender
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: reedbender
      type: user
    createdAt: '2024-01-10T23:43:39.000Z'
    data:
      edited: false
      editors:
      - reedbender
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9783121347427368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6407d2e24edf9f5c4fd26c01/gsQJj2EMG4H7eQ7qaL1Xb.png?w=200&h=200&f=face
          fullname: Reed Bender
          isHf: false
          isPro: false
          name: reedbender
          type: user
        html: '<p>This is working for me now, thanks!</p>

          '
        raw: This is working for me now, thanks!
        updatedAt: '2024-01-10T23:43:39.453Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - RonanMcGovern
    id: 659f2bab2c4538d113a5e739
    type: comment
  author: reedbender
  content: This is working for me now, thanks!
  created_at: 2024-01-10 23:43:39+00:00
  edited: false
  hidden: false
  id: 659f2bab2c4538d113a5e739
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Trelis/Mixtral-8x7B-Instruct-v0.1-function-calling-v3
repo_type: model
status: closed
target_branch: null
title: Runpod Deployment Troubleshooting
