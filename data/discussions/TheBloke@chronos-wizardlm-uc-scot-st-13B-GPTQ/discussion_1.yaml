!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Mykee
conflicting_files: null
created_at: 2023-06-10 18:38:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
      fullname: Miklos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mykee
      type: user
    createdAt: '2023-06-10T19:38:54.000Z'
    data:
      edited: true
      editors:
      - Mykee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9064623117446899
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
          fullname: Miklos
          isHf: false
          isPro: false
          name: Mykee
          type: user
        html: '<p>I would use Text Webui with the model it loads, but as soon as I
          start a chat, I get this error:</p>

          <p>Traceback (most recent call last):<br>  File "J:\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 249, in generate_reply_HF<br>    output = shared.model.generate(**generate_params)[0]<br>  File
          "J:\oobabooga_windows\installer_files\env\lib\site-packages\auto_gptq\modeling_base.py",
          line 423, in generate<br>    return self.model.generate(**kwargs)<br>  File
          "J:\oobabooga_windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "J:\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1572, in generate<br>    return self.sample(<br>  File "J:\oobabooga_windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2619, in sample<br>    outputs = self(<br>  File "J:\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga_windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 688, in forward<br>    outputs = self.model(<br>  File "J:\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "J:\oobabooga_windows\text-generation-webui\repositories\GPTQ-for-LLaMa\llama_inference_offload.py",
          line 135, in forward<br>    if idx &lt;= (self.preload - 1):<br>  File "J:\oobabooga_windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1614, in <strong>getattr</strong><br>    raise AttributeError("''{}''
          object has no attribute ''{}''".format(<br>AttributeError: ''Offload_LlamaModel''
          object has no attribute ''preload''<br>Output generated in 0.02 seconds
          (0.00 tokens/s, 0 tokens, context 42, seed 1859993720)</p>

          <p>GGML model works fine but slow, so GPTQ version would be better. How
          can it be improved?</p>

          '
        raw: "I would use Text Webui with the model it loads, but as soon as I start\
          \ a chat, I get this error:\n\nTraceback (most recent call last):\n  File\
          \ \"J:\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 249, in generate_reply_HF\n    output = shared.model.generate(**generate_params)[0]\n\
          \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          auto_gptq\\modeling\\_base.py\", line 423, in generate\n    return self.model.generate(**kwargs)\n\
          \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\n    return\
          \ func(*args, **kwargs)\n  File \"J:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1572,\
          \ in generate\n    return self.sample(\n  File \"J:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2619, in sample\n    outputs = self(\n  File \"J:\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 688, in forward\n\
          \    outputs = self.model(\n  File \"J:\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"J:\\oobabooga_windows\\\
          text-generation-webui\\repositories\\GPTQ-for-LLaMa\\llama_inference_offload.py\"\
          , line 135, in forward\n    if idx <= (self.preload - 1):\n  File \"J:\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1614, in __getattr__\n    raise AttributeError(\"\
          '{}' object has no attribute '{}'\".format(\nAttributeError: 'Offload_LlamaModel'\
          \ object has no attribute 'preload'\nOutput generated in 0.02 seconds (0.00\
          \ tokens/s, 0 tokens, context 42, seed 1859993720)\n\nGGML model works fine\
          \ but slow, so GPTQ version would be better. How can it be improved?"
        updatedAt: '2023-06-11T19:54:29.959Z'
      numEdits: 2
      reactions: []
    id: 6484d14ebd642aa9df4c6d5c
    type: comment
  author: Mykee
  content: "I would use Text Webui with the model it loads, but as soon as I start\
    \ a chat, I get this error:\n\nTraceback (most recent call last):\n  File \"J:\\\
    oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\", line\
    \ 249, in generate_reply_HF\n    output = shared.model.generate(**generate_params)[0]\n\
    \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\\
    modeling\\_base.py\", line 423, in generate\n    return self.model.generate(**kwargs)\n\
    \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
    \ **kwargs)\n  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 1572, in generate\n    return self.sample(\n\
    \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    generation\\utils.py\", line 2619, in sample\n    outputs = self(\n  File \"J:\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
    \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 688, in forward\n    outputs = self.model(\n\
    \  File \"J:\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\torch\\\
    nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
    \ **kwargs)\n  File \"J:\\oobabooga_windows\\text-generation-webui\\repositories\\\
    GPTQ-for-LLaMa\\llama_inference_offload.py\", line 135, in forward\n    if idx\
    \ <= (self.preload - 1):\n  File \"J:\\oobabooga_windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1614, in __getattr__\n\
    \    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\nAttributeError:\
    \ 'Offload_LlamaModel' object has no attribute 'preload'\nOutput generated in\
    \ 0.02 seconds (0.00 tokens/s, 0 tokens, context 42, seed 1859993720)\n\nGGML\
    \ model works fine but slow, so GPTQ version would be better. How can it be improved?"
  created_at: 2023-06-10 18:38:54+00:00
  edited: true
  hidden: false
  id: 6484d14ebd642aa9df4c6d5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
      fullname: Miklos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mykee
      type: user
    createdAt: '2023-06-12T21:39:52.000Z'
    data:
      edited: false
      editors:
      - Mykee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47187530994415283
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
          fullname: Miklos
          isHf: false
          isPro: false
          name: Mykee
          type: user
        html: '<p>Ok, I''ve done it. I deleted the quantize_config.json file and it
          works fine with these parameters:</p>

          <p>  cpu_memory: 0<br>  auto_devices: false<br>  disk: false<br>  cpu: false<br>  bf16:
          false<br>  load_in_8bit: false<br>  trust_remote_code: false<br>  load_in_4bit:
          false<br>  compute_dtype: float16<br>  quant_type: nf4<br>  use_double_quant:
          false<br>  gptq_for_llama: false<br>  wbits: 4<br>  groupsize: 128<br>  model_type:
          llama<br>  pre_layer: 0<br>  triton: false<br>  desc_act: false<br>  threads:
          0<br>  n_batch: 512<br>  no_mmap: false<br>  mlock: false<br>  n_gpu_layers:
          0<br>  n_ctx: 2048<br>  llama_cpp_seed: 0.0<br>  gpu_memory_0: 0</p>

          '
        raw: "Ok, I've done it. I deleted the quantize_config.json file and it works\
          \ fine with these parameters:\n\n  cpu_memory: 0\n  auto_devices: false\n\
          \  disk: false\n  cpu: false\n  bf16: false\n  load_in_8bit: false\n  trust_remote_code:\
          \ false\n  load_in_4bit: false\n  compute_dtype: float16\n  quant_type:\
          \ nf4\n  use_double_quant: false\n  gptq_for_llama: false\n  wbits: 4\n\
          \  groupsize: 128\n  model_type: llama\n  pre_layer: 0\n  triton: false\n\
          \  desc_act: false\n  threads: 0\n  n_batch: 512\n  no_mmap: false\n  mlock:\
          \ false\n  n_gpu_layers: 0\n  n_ctx: 2048\n  llama_cpp_seed: 0.0\n  gpu_memory_0:\
          \ 0"
        updatedAt: '2023-06-12T21:39:52.397Z'
      numEdits: 0
      reactions: []
    id: 648790a8edc2358a252c1f30
    type: comment
  author: Mykee
  content: "Ok, I've done it. I deleted the quantize_config.json file and it works\
    \ fine with these parameters:\n\n  cpu_memory: 0\n  auto_devices: false\n  disk:\
    \ false\n  cpu: false\n  bf16: false\n  load_in_8bit: false\n  trust_remote_code:\
    \ false\n  load_in_4bit: false\n  compute_dtype: float16\n  quant_type: nf4\n\
    \  use_double_quant: false\n  gptq_for_llama: false\n  wbits: 4\n  groupsize:\
    \ 128\n  model_type: llama\n  pre_layer: 0\n  triton: false\n  desc_act: false\n\
    \  threads: 0\n  n_batch: 512\n  no_mmap: false\n  mlock: false\n  n_gpu_layers:\
    \ 0\n  n_ctx: 2048\n  llama_cpp_seed: 0.0\n  gpu_memory_0: 0"
  created_at: 2023-06-12 20:39:52+00:00
  edited: false
  hidden: false
  id: 648790a8edc2358a252c1f30
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/6b49b41b696f6bd5802594f772a83786.svg
      fullname: Miklos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mykee
      type: user
    createdAt: '2023-06-12T21:39:55.000Z'
    data:
      status: closed
    id: 648790ab5cbb2b89d8788e87
    type: status-change
  author: Mykee
  created_at: 2023-06-12 20:39:55+00:00
  id: 648790ab5cbb2b89d8788e87
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-12T22:36:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.982177734375
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>OK, that''s odd. No idea why deleting the quantize_config.json would
          help, as the params you''ve set there are exactly the same as was in the
          quantize_config.json.</p>

          <p>It might have been a text-generation-webui bug, ie some other param you
          had set was breaking it and either you changed that param at the same time,
          or else not having quantize_config.json caused it not to break on that invalid
          param.</p>

          <p>Glad it''s working now, but FYI quantize_config.json is a file you want
          and shouldn''t ever need to be deleted.</p>

          '
        raw: 'OK, that''s odd. No idea why deleting the quantize_config.json would
          help, as the params you''ve set there are exactly the same as was in the
          quantize_config.json.


          It might have been a text-generation-webui bug, ie some other param you
          had set was breaking it and either you changed that param at the same time,
          or else not having quantize_config.json caused it not to break on that invalid
          param.


          Glad it''s working now, but FYI quantize_config.json is a file you want
          and shouldn''t ever need to be deleted.'
        updatedAt: '2023-06-12T22:36:25.076Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Mykee
    id: 64879de985406d989f6d82e7
    type: comment
  author: TheBloke
  content: 'OK, that''s odd. No idea why deleting the quantize_config.json would help,
    as the params you''ve set there are exactly the same as was in the quantize_config.json.


    It might have been a text-generation-webui bug, ie some other param you had set
    was breaking it and either you changed that param at the same time, or else not
    having quantize_config.json caused it not to break on that invalid param.


    Glad it''s working now, but FYI quantize_config.json is a file you want and shouldn''t
    ever need to be deleted.'
  created_at: 2023-06-12 21:36:25+00:00
  edited: false
  hidden: false
  id: 64879de985406d989f6d82e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/chronos-wizardlm-uc-scot-st-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'AttributeError: ''Offload_LlamaModel'' object has no attribute ''preload'''
