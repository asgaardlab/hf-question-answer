!!python/object:huggingface_hub.community.DiscussionWithDetails
author: benjw
conflicting_files: null
created_at: 2023-05-08 14:05:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6036895ccce151a7eb86ce7e20baf828.svg
      fullname: Ben JW
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benjw
      type: user
    createdAt: '2023-05-08T15:05:29.000Z'
    data:
      edited: false
      editors:
      - benjw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6036895ccce151a7eb86ce7e20baf828.svg
          fullname: Ben JW
          isHf: false
          isPro: false
          name: benjw
          type: user
        html: '<p>When running the provided examples on the ''model card'' page, the
          generated output does follow the instructions, however, instead of providing
          just a single answer to a given query, the generated response will continue
          with generating more example cases and responses. Is that expected? How
          can I make sure that for a given query, I only get a single answer?<br>For
          instance, the example "Given a news article, classify its topic." has some
          example articles and classification results as part of the prompt (black
          color). The generated answer (blue color) for the last article provided
          without the answer in the prompt does include the classification result,
          but then goes on producing more example articles plus their classifications.</p>

          '
        raw: "When running the provided examples on the 'model card' page, the generated\
          \ output does follow the instructions, however, instead of providing just\
          \ a single answer to a given query, the generated response will continue\
          \ with generating more example cases and responses. Is that expected? How\
          \ can I make sure that for a given query, I only get a single answer?\r\n\
          For instance, the example \"Given a news article, classify its topic.\"\
          \ has some example articles and classification results as part of the prompt\
          \ (black color). The generated answer (blue color) for the last article\
          \ provided without the answer in the prompt does include the classification\
          \ result, but then goes on producing more example articles plus their classifications."
        updatedAt: '2023-05-08T15:05:29.207Z'
      numEdits: 0
      reactions: []
    id: 64590fb9b1202755ec5ebf16
    type: comment
  author: benjw
  content: "When running the provided examples on the 'model card' page, the generated\
    \ output does follow the instructions, however, instead of providing just a single\
    \ answer to a given query, the generated response will continue with generating\
    \ more example cases and responses. Is that expected? How can I make sure that\
    \ for a given query, I only get a single answer?\r\nFor instance, the example\
    \ \"Given a news article, classify its topic.\" has some example articles and\
    \ classification results as part of the prompt (black color). The generated answer\
    \ (blue color) for the last article provided without the answer in the prompt\
    \ does include the classification result, but then goes on producing more example\
    \ articles plus their classifications."
  created_at: 2023-05-08 14:05:29+00:00
  edited: false
  hidden: false
  id: 64590fb9b1202755ec5ebf16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6322ad266b1992383fa964ca/3zw6dbBYRT6lm8n6O-vkI.jpeg?w=200&h=200&f=face
      fullname: Charles Srisuwananukorn
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: csris
      type: user
    createdAt: '2023-05-08T15:58:41.000Z'
    data:
      edited: false
      editors:
      - csris
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6322ad266b1992383fa964ca/3zw6dbBYRT6lm8n6O-vkI.jpeg?w=200&h=200&f=face
          fullname: Charles Srisuwananukorn
          isHf: false
          isPro: false
          name: csris
          type: user
        html: '<p>Hi, Ben. You can use either a length limit or a stop sequence to
          control when the model stops generating. If you''re using Huggingface transformers,
          please see the <code>max_length</code> and <code>max_new_length</code> parameters
          and the <a href="https://huggingface.co/docs/transformers/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
          documentation.</p>

          '
        raw: Hi, Ben. You can use either a length limit or a stop sequence to control
          when the model stops generating. If you're using Huggingface transformers,
          please see the `max_length` and `max_new_length` parameters and the [StoppingCriteria](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.StoppingCriteria)
          documentation.
        updatedAt: '2023-05-08T15:58:41.120Z'
      numEdits: 0
      reactions: []
    id: 64591c31c5d0d57ba4220450
    type: comment
  author: csris
  content: Hi, Ben. You can use either a length limit or a stop sequence to control
    when the model stops generating. If you're using Huggingface transformers, please
    see the `max_length` and `max_new_length` parameters and the [StoppingCriteria](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.StoppingCriteria)
    documentation.
  created_at: 2023-05-08 14:58:41+00:00
  edited: false
  hidden: false
  id: 64591c31c5d0d57ba4220450
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/584a9db003775823e6b7f280645cbd6d.svg
      fullname: Pavel Larionov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pavelrst
      type: user
    createdAt: '2023-05-13T08:19:52.000Z'
    data:
      edited: true
      editors:
      - Pavelrst
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/584a9db003775823e6b7f280645cbd6d.svg
          fullname: Pavel Larionov
          isHf: false
          isPro: false
          name: Pavelrst
          type: user
        html: '<p>I have the same behavior with the "Q: The capital of France is?\nA:"</p>

          <p>The output is:</p>

          <p>Setting <code>pad_token_id</code> to <code>eos_token_id</code>:0 for
          open-end generation.<br>" Paris</p>

          <p>Question: What is the name of the river that runs through the capital
          of France?<br>A: Seine</p>

          <p>Question: What is the capital of France?<br>A: Paris</p>

          <p>Question: What is the capital of France"</p>

          <p>While  "max_length", "max_new_length" parameters and the StoppingCriteria
          can help with this,<br>I obviously don''t know what is the max_new_length
          and the stopping criteria I should use to cover all my cases.<br>Any suggestions
          how to make it work like in your example?</p>

          '
        raw: "I have the same behavior with the \"Q: The capital of France is?\\nA:\"\
          \n\nThe output is:\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end\
          \ generation.\n\" Paris\n\nQuestion: What is the name of the river that\
          \ runs through the capital of France?\nA: Seine\n\nQuestion: What is the\
          \ capital of France?\nA: Paris\n\nQuestion: What is the capital of France\"\
          \n\nWhile  \"max_length\", \"max_new_length\" parameters and the StoppingCriteria\
          \ can help with this, \nI obviously don't know what is the max_new_length\
          \ and the stopping criteria I should use to cover all my cases.\nAny suggestions\
          \ how to make it work like in your example?"
        updatedAt: '2023-05-13T08:29:06.108Z'
      numEdits: 1
      reactions: []
    id: 645f48284357049a57f274d8
    type: comment
  author: Pavelrst
  content: "I have the same behavior with the \"Q: The capital of France is?\\nA:\"\
    \n\nThe output is:\n\nSetting `pad_token_id` to `eos_token_id`:0 for open-end\
    \ generation.\n\" Paris\n\nQuestion: What is the name of the river that runs through\
    \ the capital of France?\nA: Seine\n\nQuestion: What is the capital of France?\n\
    A: Paris\n\nQuestion: What is the capital of France\"\n\nWhile  \"max_length\"\
    , \"max_new_length\" parameters and the StoppingCriteria can help with this, \n\
    I obviously don't know what is the max_new_length and the stopping criteria I\
    \ should use to cover all my cases.\nAny suggestions how to make it work like\
    \ in your example?"
  created_at: 2023-05-13 07:19:52+00:00
  edited: true
  hidden: false
  id: 645f48284357049a57f274d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6036895ccce151a7eb86ce7e20baf828.svg
      fullname: Ben JW
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: benjw
      type: user
    createdAt: '2023-05-14T14:19:42.000Z'
    data:
      edited: false
      editors:
      - benjw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6036895ccce151a7eb86ce7e20baf828.svg
          fullname: Ben JW
          isHf: false
          isPro: false
          name: benjw
          type: user
        html: '<p>Indeed, specifying a ''max_length'' would imply making assumptions
          about the generated answer in advance, but that would miss the point that
          the model itself should generate the correct answer (and stop after that).<br>If
          I already know the precise answer (and hence, its length), there''s no point
          querying the model. If I roughly limit the number of generated tokens to
          cover a variety of questions and answers, I would still have to figure out
          exactly where to split the actual answer from additionally generated bogus
          tokens -- just like now, when I don''t impose such limits.<br>Ideally, the
          model itself would yield a "stop/EOS  token" directly after producing the
          answer "Paris". Is there maybe another syntax/structure/prompt template
          we have to use (instead of "Q: ...?\nA:") to let the model know we are instructing
          it (i.e., the same structure that was used in the training data for instruction-tuning)?</p>

          '
        raw: "Indeed, specifying a 'max_length' would imply making assumptions about\
          \ the generated answer in advance, but that would miss the point that the\
          \ model itself should generate the correct answer (and stop after that).\
          \ \nIf I already know the precise answer (and hence, its length), there's\
          \ no point querying the model. If I roughly limit the number of generated\
          \ tokens to cover a variety of questions and answers, I would still have\
          \ to figure out exactly where to split the actual answer from additionally\
          \ generated bogus tokens -- just like now, when I don't impose such limits.\n\
          Ideally, the model itself would yield a \"stop/EOS  token\" directly after\
          \ producing the answer \"Paris\". Is there maybe another syntax/structure/prompt\
          \ template we have to use (instead of \"Q: ...?\\nA:\") to let the model\
          \ know we are instructing it (i.e., the same structure that was used in\
          \ the training data for instruction-tuning)?"
        updatedAt: '2023-05-14T14:19:42.265Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - niklas747
        - jweiss
        - tolgadimli
        - liujqian
    id: 6460edfe604bf89233337cdb
    type: comment
  author: benjw
  content: "Indeed, specifying a 'max_length' would imply making assumptions about\
    \ the generated answer in advance, but that would miss the point that the model\
    \ itself should generate the correct answer (and stop after that). \nIf I already\
    \ know the precise answer (and hence, its length), there's no point querying the\
    \ model. If I roughly limit the number of generated tokens to cover a variety\
    \ of questions and answers, I would still have to figure out exactly where to\
    \ split the actual answer from additionally generated bogus tokens -- just like\
    \ now, when I don't impose such limits.\nIdeally, the model itself would yield\
    \ a \"stop/EOS  token\" directly after producing the answer \"Paris\". Is there\
    \ maybe another syntax/structure/prompt template we have to use (instead of \"\
    Q: ...?\\nA:\") to let the model know we are instructing it (i.e., the same structure\
    \ that was used in the training data for instruction-tuning)?"
  created_at: 2023-05-14 13:19:42+00:00
  edited: false
  hidden: false
  id: 6460edfe604bf89233337cdb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: togethercomputer/RedPajama-INCITE-Instruct-3B-v1
repo_type: model
status: open
target_branch: null
title: Generated responses too long/repetitive/redundant?
