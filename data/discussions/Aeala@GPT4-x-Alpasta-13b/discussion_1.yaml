!!python/object:huggingface_hub.community.DiscussionWithDetails
author: teneriffa
conflicting_files: null
created_at: 2023-05-14 06:56:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
      fullname: teneriffa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: teneriffa
      type: user
    createdAt: '2023-05-14T07:56:27.000Z'
    data:
      edited: true
      editors:
      - teneriffa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/589e3f19a6b466a6b90a1ff222e82d2d.svg
          fullname: teneriffa
          isHf: false
          isPro: false
          name: teneriffa
          type: user
        html: "<p>I tried to load the quantized weights on oobabooga's text-generation-webui\
          \ but an error was occured. I downloaded the weights using download-model.py\
          \ comes with oobabooga's webui. How can I load this weights?</p>\n<p>I found\
          \ that pytorch_model-00001-of-0000?.bin files are not found. Should I download\
          \ those files?<br>If pytorch_model.bin.index.json needs to be edited, please\
          \ let me know how to do, because this file has 410 lines, I guess just substituting\
          \ with 4bit-128g.safetensors or 4bit.safetensors will not work.</p>\n<pre><code>INFO:Loading\
          \ Aeala_GPT4-x-Alpasta-13b...\nLoading checkpoint shards:   0%|        \
          \                                    | 0/3 [00:00&lt;?, ?it/s]\nTraceback\
          \ (most recent call last):\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 442, in load_state_dict\n    return torch.load(checkpoint_file, map_location=\"\
          cpu\")\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 862, in load\n    with _open_file_like(f, 'rb') as opened_file:\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 321, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 302, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'models/Aeala_GPT4-x-Alpasta-13b/pytorch_model-00001-of-00003.bin'\n\
          \nDuring handling of the above exception, another exception occurred:\n\n\
          Traceback (most recent call last):\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/text-generation-webui/server.py\"\
          , line 948, in &lt;module&gt;\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/text-generation-webui/modules/models.py\"\
          , line 85, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=trust_remote_code)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 471, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2795, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3109, in _load_pretrained_model\n    state_dict = load_state_dict(shard_file)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 445, in load_state_dict\n    with open(checkpoint_file) as f:\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'models/Aeala_GPT4-x-Alpasta-13b/pytorch_model-00001-of-00003.bin'\n\
          </code></pre>\n"
        raw: "I tried to load the quantized weights on oobabooga's text-generation-webui\
          \ but an error was occured. I downloaded the weights using download-model.py\
          \ comes with oobabooga's webui. How can I load this weights?\n\nI found\
          \ that pytorch_model-00001-of-0000?.bin files are not found. Should I download\
          \ those files?\nIf pytorch_model.bin.index.json needs to be edited, please\
          \ let me know how to do, because this file has 410 lines, I guess just substituting\
          \ with 4bit-128g.safetensors or 4bit.safetensors will not work.\n\n```\n\
          INFO:Loading Aeala_GPT4-x-Alpasta-13b...\nLoading checkpoint shards:   0%|\
          \                                            | 0/3 [00:00<?, ?it/s]\nTraceback\
          \ (most recent call last):\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 442, in load_state_dict\n    return torch.load(checkpoint_file, map_location=\"\
          cpu\")\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 862, in load\n    with _open_file_like(f, 'rb') as opened_file:\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 321, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
          , line 302, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'models/Aeala_GPT4-x-Alpasta-13b/pytorch_model-00001-of-00003.bin'\n\
          \nDuring handling of the above exception, another exception occurred:\n\n\
          Traceback (most recent call last):\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/text-generation-webui/server.py\"\
          , line 948, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/text-generation-webui/modules/models.py\"\
          , line 85, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"\
          {shared.args.model_dir}/{model_name}\"), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\
          \ if shared.args.bf16 else torch.float16, trust_remote_code=trust_remote_code)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 471, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2795, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 3109, in _load_pretrained_model\n    state_dict = load_state_dict(shard_file)\n\
          \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 445, in load_state_dict\n    with open(checkpoint_file) as f:\nFileNotFoundError:\
          \ [Errno 2] No such file or directory: 'models/Aeala_GPT4-x-Alpasta-13b/pytorch_model-00001-of-00003.bin'\n\
          ```"
        updatedAt: '2023-05-14T08:10:06.289Z'
      numEdits: 2
      reactions: []
    id: 6460942b933afb0106a6d80b
    type: comment
  author: teneriffa
  content: "I tried to load the quantized weights on oobabooga's text-generation-webui\
    \ but an error was occured. I downloaded the weights using download-model.py comes\
    \ with oobabooga's webui. How can I load this weights?\n\nI found that pytorch_model-00001-of-0000?.bin\
    \ files are not found. Should I download those files?\nIf pytorch_model.bin.index.json\
    \ needs to be edited, please let me know how to do, because this file has 410\
    \ lines, I guess just substituting with 4bit-128g.safetensors or 4bit.safetensors\
    \ will not work.\n\n```\nINFO:Loading Aeala_GPT4-x-Alpasta-13b...\nLoading checkpoint\
    \ shards:   0%|                                            | 0/3 [00:00<?, ?it/s]\n\
    Traceback (most recent call last):\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 442, in load_state_dict\n    return torch.load(checkpoint_file, map_location=\"\
    cpu\")\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
    , line 862, in load\n    with _open_file_like(f, 'rb') as opened_file:\n  File\
    \ \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
    , line 321, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n\
    \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/torch/serialization.py\"\
    , line 302, in __init__\n    super().__init__(open(name, mode))\nFileNotFoundError:\
    \ [Errno 2] No such file or directory: 'models/Aeala_GPT4-x-Alpasta-13b/pytorch_model-00001-of-00003.bin'\n\
    \nDuring handling of the above exception, another exception occurred:\n\nTraceback\
    \ (most recent call last):\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/text-generation-webui/server.py\"\
    , line 948, in <module>\n    shared.model, shared.tokenizer = load_model(shared.model_name)\n\
    \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/text-generation-webui/modules/models.py\"\
    , line 85, in load_model\n    model = LoaderClass.from_pretrained(Path(f\"{shared.args.model_dir}/{model_name}\"\
    ), low_cpu_mem_usage=True, torch_dtype=torch.bfloat16 if shared.args.bf16 else\
    \ torch.float16, trust_remote_code=trust_remote_code)\n  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 471, in from_pretrained\n    return model_class.from_pretrained(\n  File\
    \ \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 2795, in from_pretrained\n    ) = cls._load_pretrained_model(\n  File \"\
    /Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 3109, in _load_pretrained_model\n    state_dict = load_state_dict(shard_file)\n\
    \  File \"/Volumes/cuttingedges/large_lang_models/oobabooga_macos/installer_files/env/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 445, in load_state_dict\n    with open(checkpoint_file) as f:\nFileNotFoundError:\
    \ [Errno 2] No such file or directory: 'models/Aeala_GPT4-x-Alpasta-13b/pytorch_model-00001-of-00003.bin'\n\
    ```"
  created_at: 2023-05-14 06:56:27+00:00
  edited: true
  hidden: false
  id: 6460942b933afb0106a6d80b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BYs57LZ4sV-_IV1tRFD8W.png?w=200&h=200&f=face
      fullname: A'eala
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Aeala
      type: user
    createdAt: '2023-05-15T01:41:38.000Z'
    data:
      edited: false
      editors:
      - Aeala
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BYs57LZ4sV-_IV1tRFD8W.png?w=200&h=200&f=face
          fullname: A'eala
          isHf: false
          isPro: false
          name: Aeala
          type: user
        html: '<p>Hi! Thanks for using this ^~^ -- odd, I think that webui likes to
          have the models named the same as the folder (or if the folder name has
          something like -4bit-128g at the end? Did you try running with the <code>--wbits
          4</code> and such?). Try renaming the one you''d like to use to something
          like: GPT4-x-Alpasta-13b-4bit.safetensors (or 4bit-128g if you want to use
          that!) And it should load! If not, let me know and I''d be happy to help
          further ^~^ good luck!!</p>

          '
        raw: 'Hi! Thanks for using this ^\~^ -- odd, I think that webui likes to have
          the models named the same as the folder (or if the folder name has something
          like -4bit-128g at the end? Did you try running with the `--wbits 4` and
          such?). Try renaming the one you''d like to use to something like: GPT4-x-Alpasta-13b-4bit.safetensors
          (or 4bit-128g if you want to use that!) And it should load! If not, let
          me know and I''d be happy to help further ^\~^ good luck!!'
        updatedAt: '2023-05-15T01:41:38.106Z'
      numEdits: 0
      reactions: []
    id: 64618dd2bf985b8b4eba5d47
    type: comment
  author: Aeala
  content: 'Hi! Thanks for using this ^\~^ -- odd, I think that webui likes to have
    the models named the same as the folder (or if the folder name has something like
    -4bit-128g at the end? Did you try running with the `--wbits 4` and such?). Try
    renaming the one you''d like to use to something like: GPT4-x-Alpasta-13b-4bit.safetensors
    (or 4bit-128g if you want to use that!) And it should load! If not, let me know
    and I''d be happy to help further ^\~^ good luck!!'
  created_at: 2023-05-15 00:41:38+00:00
  edited: false
  hidden: false
  id: 64618dd2bf985b8b4eba5d47
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BYs57LZ4sV-_IV1tRFD8W.png?w=200&h=200&f=face
      fullname: A'eala
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Aeala
      type: user
    createdAt: '2023-05-15T01:42:53.000Z'
    data:
      edited: false
      editors:
      - Aeala
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BYs57LZ4sV-_IV1tRFD8W.png?w=200&h=200&f=face
          fullname: A'eala
          isHf: false
          isPro: false
          name: Aeala
          type: user
        html: '<p>See also: <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md</a></p>

          '
        raw: 'See also: https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md'
        updatedAt: '2023-05-15T01:42:53.624Z'
      numEdits: 0
      reactions: []
    id: 64618e1d3cc3259ce05d9d4a
    type: comment
  author: Aeala
  content: 'See also: https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md'
  created_at: 2023-05-15 00:42:53+00:00
  edited: false
  hidden: false
  id: 64618e1d3cc3259ce05d9d4a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Aeala/GPT4-x-Alpasta-13b
repo_type: model
status: open
target_branch: null
title: How to use the quantized weights?
