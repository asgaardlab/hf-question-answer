!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pszemraj
conflicting_files: null
created_at: 2022-06-20 12:30:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-20T13:30:08.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: '<p>hey, thanks for the great upload! my question is related to model
          inference. </p>

          <p>I was able to train using the trainer API on Colab, but in trying to
          run some inference in a notebook on non-dataset text, I get the following
          error even with <code>LongT5ForConditionalGeneration</code>:</p>

          <pre><code>TypeError: forward() got an unexpected keyword argument ''global_attention_mask''

          </code></pre>

          <p>(perhaps unrelated, but that same notebook works fine with <a href="https://huggingface.co/pszemraj/led-large-book-summary">this
          checkpoint for example</a>). I then turned to your example in the model
          card to see if I could replicate that and I am doing something wrong, but
          I can''t find where you define <code>global_attention_mask</code> in the
          example:</p>

          <pre><code>import torch

          from transformers import AutoTokenizer, LongT5ForConditionalGeneration


          tokenizer = AutoTokenizer.from_pretrained("Stancld/longt5-tglobal-large-16384-pubmed-3k_steps")


          input_ids = tokenizer(LONG_ARTICLE, return_tensors="pt").input_ids.to("cuda")


          model = LongT5ForConditionalGeneration.from_pretrained("Stancld/longt5-tglobal-large-16384-pubmed-3k_steps",
          return_dict_in_generate=True).to("cuda")


          sequences = model.generate(input_ids, global_attention_mask=global_attention_mask).sequences


          summary = tokenizer.batch_decode(sequences)

          </code></pre>

          <p>any help would be appreciated :)</p>

          '
        raw: "hey, thanks for the great upload! my question is related to model inference.\
          \ \r\n\r\nI was able to train using the trainer API on Colab, but in trying\
          \ to run some inference in a notebook on non-dataset text, I get the following\
          \ error even with `LongT5ForConditionalGeneration`:\r\n\r\n```\r\nTypeError:\
          \ forward() got an unexpected keyword argument 'global_attention_mask'\r\
          \n```\r\n\r\n(perhaps unrelated, but that same notebook works fine with\
          \ [this checkpoint for example](https://huggingface.co/pszemraj/led-large-book-summary)).\
          \ I then turned to your example in the model card to see if I could replicate\
          \ that and I am doing something wrong, but I can't find where you define\
          \ `global_attention_mask` in the example:\r\n\r\n```\r\nimport torch\r\n\
          from transformers import AutoTokenizer, LongT5ForConditionalGeneration\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\"\
          )\r\n\r\ninput_ids = tokenizer(LONG_ARTICLE, return_tensors=\"pt\").input_ids.to(\"\
          cuda\")\r\n\r\nmodel = LongT5ForConditionalGeneration.from_pretrained(\"\
          Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\", return_dict_in_generate=True).to(\"\
          cuda\")\r\n\r\nsequences = model.generate(input_ids, global_attention_mask=global_attention_mask).sequences\r\
          \n\r\nsummary = tokenizer.batch_decode(sequences)\r\n```\r\n\r\nany help\
          \ would be appreciated :)\r\n\r\n"
        updatedAt: '2022-06-20T13:30:08.682Z'
      numEdits: 0
      reactions: []
    id: 62b07660b7cad3cd82d96ce6
    type: comment
  author: pszemraj
  content: "hey, thanks for the great upload! my question is related to model inference.\
    \ \r\n\r\nI was able to train using the trainer API on Colab, but in trying to\
    \ run some inference in a notebook on non-dataset text, I get the following error\
    \ even with `LongT5ForConditionalGeneration`:\r\n\r\n```\r\nTypeError: forward()\
    \ got an unexpected keyword argument 'global_attention_mask'\r\n```\r\n\r\n(perhaps\
    \ unrelated, but that same notebook works fine with [this checkpoint for example](https://huggingface.co/pszemraj/led-large-book-summary)).\
    \ I then turned to your example in the model card to see if I could replicate\
    \ that and I am doing something wrong, but I can't find where you define `global_attention_mask`\
    \ in the example:\r\n\r\n```\r\nimport torch\r\nfrom transformers import AutoTokenizer,\
    \ LongT5ForConditionalGeneration\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\r\n\r\ninput_ids = tokenizer(LONG_ARTICLE,\
    \ return_tensors=\"pt\").input_ids.to(\"cuda\")\r\n\r\nmodel = LongT5ForConditionalGeneration.from_pretrained(\"\
    Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\", return_dict_in_generate=True).to(\"\
    cuda\")\r\n\r\nsequences = model.generate(input_ids, global_attention_mask=global_attention_mask).sequences\r\
    \n\r\nsummary = tokenizer.batch_decode(sequences)\r\n```\r\n\r\nany help would\
    \ be appreciated :)\r\n\r\n"
  created_at: 2022-06-20 12:30:08+00:00
  edited: false
  hidden: false
  id: 62b07660b7cad3cd82d96ce6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
      fullname: Daniel Stancl
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Stancld
      type: user
    createdAt: '2022-06-20T15:44:53.000Z'
    data:
      edited: false
      editors:
      - Stancld
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
          fullname: Daniel Stancl
          isHf: false
          isPro: false
          name: Stancld
          type: user
        html: '<p>Hi, thank you very much for pointing out this issue. It''s a mistake
          on my side. <code>LongT5</code> model accepts <code>attention_mask</code>,
          not <code>global_attentiona_mask</code>.. Sorry for the confusion, a I''ll
          fix that :]</p>

          '
        raw: Hi, thank you very much for pointing out this issue. It's a mistake on
          my side. `LongT5` model accepts `attention_mask`, not `global_attentiona_mask`..
          Sorry for the confusion, a I'll fix that :]
        updatedAt: '2022-06-20T15:44:53.915Z'
      numEdits: 0
      reactions: []
    id: 62b095f5a14cbd64386a6a71
    type: comment
  author: Stancld
  content: Hi, thank you very much for pointing out this issue. It's a mistake on
    my side. `LongT5` model accepts `attention_mask`, not `global_attentiona_mask`..
    Sorry for the confusion, a I'll fix that :]
  created_at: 2022-06-20 14:44:53+00:00
  edited: false
  hidden: false
  id: 62b095f5a14cbd64386a6a71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-22T14:04:16.000Z'
    data:
      edited: false
      editors:
      - pszemraj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
          fullname: Peter Szemraj
          isHf: false
          isPro: false
          name: pszemraj
          type: user
        html: "<p>Thanks! Works as expected now \U0001F44D</p>\n<p>I\u2019ll post\
          \ my checkpoints once I\u2019m happy with the performance but DAMN this\
          \ thing takes forever to train (even compared to LED at 16384) </p>\n"
        raw: "Thanks! Works as expected now \U0001F44D\n\nI\u2019ll post my checkpoints\
          \ once I\u2019m happy with the performance but DAMN this thing takes forever\
          \ to train (even compared to LED at 16384) "
        updatedAt: '2022-06-22T14:04:16.597Z'
      numEdits: 0
      reactions: []
    id: 62b3216051b07307bdfc02d4
    type: comment
  author: pszemraj
  content: "Thanks! Works as expected now \U0001F44D\n\nI\u2019ll post my checkpoints\
    \ once I\u2019m happy with the performance but DAMN this thing takes forever to\
    \ train (even compared to LED at 16384) "
  created_at: 2022-06-22 13:04:16+00:00
  edited: false
  hidden: false
  id: 62b3216051b07307bdfc02d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641390233716-60bccec062080d33f875cd0c.png?w=200&h=200&f=face
      fullname: Peter Szemraj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pszemraj
      type: user
    createdAt: '2022-06-22T14:04:16.000Z'
    data:
      status: closed
    id: 62b3216051b07307bdfc02d5
    type: status-change
  author: pszemraj
  created_at: 2022-06-22 13:04:16+00:00
  id: 62b3216051b07307bdfc02d5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Stancld/longt5-tglobal-large-16384-pubmed-3k_steps
repo_type: model
status: closed
target_branch: null
title: global attention mask for model inference
