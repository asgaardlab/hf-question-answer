!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kmfoda
conflicting_files: null
created_at: 2022-06-17 12:42:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-17T13:42:36.000Z'
    data:
      edited: true
      editors:
      - kmfoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: "<p>Thanks so much for they <span data-props=\"{&quot;user&quot;:&quot;Stancld&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Stancld\"\
          >@<span class=\"underline\">Stancld</span></a></span>\n\n\t</span></span>!\
          \ This is amazing. Are you able to share your training setup? </p>\n<p>I'm\
          \ trying to fine tune this on another dataset but I'm getting OOM errors\
          \ on just 1 GPU when max_source_length is set to 16384. I also run in to\
          \ the same problem when I use 4 GPUs and deepspeed model parallelisation.</p>\n"
        raw: "Thanks so much for they @Stancld! This is amazing. Are you able to share\
          \ your training setup? \n\nI'm trying to fine tune this on another dataset\
          \ but I'm getting OOM errors on just 1 GPU when max_source_length is set\
          \ to 16384. I also run in to the same problem when I use 4 GPUs and deepspeed\
          \ model parallelisation."
        updatedAt: '2022-06-17T13:43:18.042Z'
      numEdits: 1
      reactions: []
    id: 62ac84cc1bc1db521727a9c6
    type: comment
  author: kmfoda
  content: "Thanks so much for they @Stancld! This is amazing. Are you able to share\
    \ your training setup? \n\nI'm trying to fine tune this on another dataset but\
    \ I'm getting OOM errors on just 1 GPU when max_source_length is set to 16384.\
    \ I also run in to the same problem when I use 4 GPUs and deepspeed model parallelisation."
  created_at: 2022-06-17 12:42:36+00:00
  edited: true
  hidden: false
  id: 62ac84cc1bc1db521727a9c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
      fullname: Daniel Stancl
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Stancld
      type: user
    createdAt: '2022-06-17T21:27:00.000Z'
    data:
      edited: false
      editors:
      - Stancld
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
          fullname: Daniel Stancl
          isHf: false
          isPro: false
          name: Stancld
          type: user
        html: "<p>Heu <span data-props=\"{&quot;user&quot;:&quot;kmfoda&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/kmfoda\">@<span class=\"\
          underline\">kmfoda</span></a></span>\n\n\t</span></span>, I trained the\
          \ model using 2 A100 40GB GPUs, and was required to use gradient checkpointing\
          \ otherwise were getting OOM as well. Let me know if it helps. :]</p>\n"
        raw: Heu @kmfoda, I trained the model using 2 A100 40GB GPUs, and was required
          to use gradient checkpointing otherwise were getting OOM as well. Let me
          know if it helps. :]
        updatedAt: '2022-06-17T21:27:00.369Z'
      numEdits: 0
      reactions: []
    id: 62acf1a4717ee4c12b754598
    type: comment
  author: Stancld
  content: Heu @kmfoda, I trained the model using 2 A100 40GB GPUs, and was required
    to use gradient checkpointing otherwise were getting OOM as well. Let me know
    if it helps. :]
  created_at: 2022-06-17 20:27:00+00:00
  edited: false
  hidden: false
  id: 62acf1a4717ee4c12b754598
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
      fullname: Jorge Lopez Grisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jorgeutd
      type: user
    createdAt: '2022-06-17T22:53:16.000Z'
    data:
      edited: false
      editors:
      - Jorgeutd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
          fullname: Jorge Lopez Grisman
          isHf: false
          isPro: false
          name: Jorgeutd
          type: user
        html: '<p>Hey Daniel thank you for providing this model details. Is there
          any documentation out there on how to to train this model using the trainer
          API? I did it but the results do not look good. Also, on the tokenization
          step is it required to add the summarize prefix like: if model_checkpoint
          in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:<br>prefix = "summarize:
          "?</p>

          <p>Thank you.</p>

          '
        raw: 'Hey Daniel thank you for providing this model details. Is there any
          documentation out there on how to to train this model using the trainer
          API? I did it but the results do not look good. Also, on the tokenization
          step is it required to add the summarize prefix like: if model_checkpoint
          in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:

          prefix = "summarize: "?


          Thank you.'
        updatedAt: '2022-06-17T22:53:16.530Z'
      numEdits: 0
      reactions: []
    id: 62ad05dc6bc70f4a0ce2a0b1
    type: comment
  author: Jorgeutd
  content: 'Hey Daniel thank you for providing this model details. Is there any documentation
    out there on how to to train this model using the trainer API? I did it but the
    results do not look good. Also, on the tokenization step is it required to add
    the summarize prefix like: if model_checkpoint in ["t5-small", "t5-base", "t5-larg",
    "t5-3b", "t5-11b"]:

    prefix = "summarize: "?


    Thank you.'
  created_at: 2022-06-17 21:53:16+00:00
  edited: false
  hidden: false
  id: 62ad05dc6bc70f4a0ce2a0b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
      fullname: Daniel Stancl
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Stancld
      type: user
    createdAt: '2022-06-20T10:11:07.000Z'
    data:
      edited: false
      editors:
      - Stancld
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
          fullname: Daniel Stancl
          isHf: false
          isPro: false
          name: Stancld
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Jorgeutd&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Jorgeutd\"\
          >@<span class=\"underline\">Jorgeutd</span></a></span>\n\n\t</span></span>,\
          \ there's no prefix for <code>LongT5</code> model as it uses a different\
          \ pre-training technique from <code>T5</code>'s one.</p>\n<p>I used the\
          \ command below to run training using HF Seq2Seq Trainer :]</p>\n<pre><code\
          \ class=\"language-python\">run_summarization.py --model_name_or_path google/long-t5-tglobal-large\
          \  --do_train --do_eval --do_predict --dataset_name ccdv/pubmed-summarization\
          \ --max_source_length <span class=\"hljs-number\">16384</span> --max_target_length\
          \ <span class=\"hljs-number\">512</span> --per_device_train_batch_size <span\
          \ class=\"hljs-number\">1</span> --gradient_accumulation_steps <span class=\"\
          hljs-number\">64</span> --optim adafactor --learning_rate <span class=\"\
          hljs-number\">0.001</span> --lr_scheduler_type constant --num_train_epochs\
          \ <span class=\"hljs-number\">20</span> --gradient_checkpointing --bf16=<span\
          \ class=\"hljs-literal\">True</span> --per_device_eval_batch_size <span\
          \ class=\"hljs-number\">2</span> --predict_with_generate --generation_num_beams\
          \ <span class=\"hljs-number\">1</span> --generation_max_length <span class=\"\
          hljs-number\">512</span> --output_dir /tmp/longt5_pubmed --run_name LongT5-pubmed-16k-<span\
          \ class=\"hljs-number\">512</span>-bs_128 --report_to <span class=\"hljs-built_in\"\
          >all</span> --logging_steps <span class=\"hljs-number\">10</span> --eval_steps\
          \ <span class=\"hljs-number\">500</span> --evaluation_strategy steps --ddp_find_unused_parameters=<span\
          \ class=\"hljs-literal\">False</span> --no_cuda=<span class=\"hljs-literal\"\
          >False</span>\n</code></pre>\n"
        raw: 'Hey @Jorgeutd, there''s no prefix for `LongT5` model as it uses a different
          pre-training technique from `T5`''s one.


          I used the command below to run training using HF Seq2Seq Trainer :]


          ```python

          run_summarization.py --model_name_or_path google/long-t5-tglobal-large  --do_train
          --do_eval --do_predict --dataset_name ccdv/pubmed-summarization --max_source_length
          16384 --max_target_length 512 --per_device_train_batch_size 1 --gradient_accumulation_steps
          64 --optim adafactor --learning_rate 0.001 --lr_scheduler_type constant
          --num_train_epochs 20 --gradient_checkpointing --bf16=True --per_device_eval_batch_size
          2 --predict_with_generate --generation_num_beams 1 --generation_max_length
          512 --output_dir /tmp/longt5_pubmed --run_name LongT5-pubmed-16k-512-bs_128
          --report_to all --logging_steps 10 --eval_steps 500 --evaluation_strategy
          steps --ddp_find_unused_parameters=False --no_cuda=False

          ```'
        updatedAt: '2022-06-20T10:11:07.347Z'
      numEdits: 0
      reactions: []
    id: 62b047bb53d878042fab1c3e
    type: comment
  author: Stancld
  content: 'Hey @Jorgeutd, there''s no prefix for `LongT5` model as it uses a different
    pre-training technique from `T5`''s one.


    I used the command below to run training using HF Seq2Seq Trainer :]


    ```python

    run_summarization.py --model_name_or_path google/long-t5-tglobal-large  --do_train
    --do_eval --do_predict --dataset_name ccdv/pubmed-summarization --max_source_length
    16384 --max_target_length 512 --per_device_train_batch_size 1 --gradient_accumulation_steps
    64 --optim adafactor --learning_rate 0.001 --lr_scheduler_type constant --num_train_epochs
    20 --gradient_checkpointing --bf16=True --per_device_eval_batch_size 2 --predict_with_generate
    --generation_num_beams 1 --generation_max_length 512 --output_dir /tmp/longt5_pubmed
    --run_name LongT5-pubmed-16k-512-bs_128 --report_to all --logging_steps 10 --eval_steps
    500 --evaluation_strategy steps --ddp_find_unused_parameters=False --no_cuda=False

    ```'
  created_at: 2022-06-20 09:11:07+00:00
  edited: false
  hidden: false
  id: 62b047bb53d878042fab1c3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
      fullname: Jorge Lopez Grisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jorgeutd
      type: user
    createdAt: '2022-06-20T10:29:26.000Z'
    data:
      edited: false
      editors:
      - Jorgeutd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656704555518-60ca1290f6a945ddc0751dab.jpeg?w=200&h=200&f=face
          fullname: Jorge Lopez Grisman
          isHf: false
          isPro: false
          name: Jorgeutd
          type: user
        html: '<p>Thank you Daniel.</p>

          '
        raw: Thank you Daniel.
        updatedAt: '2022-06-20T10:29:26.295Z'
      numEdits: 0
      reactions: []
    id: 62b04c0653d878042fab8eaf
    type: comment
  author: Jorgeutd
  content: Thank you Daniel.
  created_at: 2022-06-20 09:29:26+00:00
  edited: false
  hidden: false
  id: 62b04c0653d878042fab8eaf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-23T07:14:56.000Z'
    data:
      edited: true
      editors:
      - kmfoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;Stancld&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Stancld\"\
          >@<span class=\"underline\">Stancld</span></a></span>\n\n\t</span></span>,\
          \ very helpful. Gradient checkpointing does work but I find it increases\
          \ training time 4x. Did you find this as well? Alternatively, did you ever\
          \ consider model partitioning using DeepSpeed for LongT5? I'm facing issues\
          \ doing so and wondering wether it's because of a limitation in DeepSpeed\
          \ working only with full attention models.</p>\n"
        raw: Thanks @Stancld, very helpful. Gradient checkpointing does work but I
          find it increases training time 4x. Did you find this as well? Alternatively,
          did you ever consider model partitioning using DeepSpeed for LongT5? I'm
          facing issues doing so and wondering wether it's because of a limitation
          in DeepSpeed working only with full attention models.
        updatedAt: '2022-06-23T10:21:57.407Z'
      numEdits: 1
      reactions: []
    id: 62b412f07f0f87073f4fbd28
    type: comment
  author: kmfoda
  content: Thanks @Stancld, very helpful. Gradient checkpointing does work but I find
    it increases training time 4x. Did you find this as well? Alternatively, did you
    ever consider model partitioning using DeepSpeed for LongT5? I'm facing issues
    doing so and wondering wether it's because of a limitation in DeepSpeed working
    only with full attention models.
  created_at: 2022-06-23 06:14:56+00:00
  edited: true
  hidden: false
  id: 62b412f07f0f87073f4fbd28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
      fullname: Daniel Stancl
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Stancld
      type: user
    createdAt: '2022-06-25T14:04:07.000Z'
    data:
      edited: false
      editors:
      - Stancld
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630393568321-6008b3370c3c11e544b73cba.jpeg?w=200&h=200&f=face
          fullname: Daniel Stancl
          isHf: false
          isPro: false
          name: Stancld
          type: user
        html: '<p>Yes, gradient checkpointing, unfortunately, slows down a training,
          however, I''m really surprised what the difference it is!<br>I haven''t
          tried training with DeepSpeed, but it definitely deserves a try. Nonetheless,
          I''m not any experienced with that :/</p>

          '
        raw: 'Yes, gradient checkpointing, unfortunately, slows down a training, however,
          I''m really surprised what the difference it is!

          I haven''t tried training with DeepSpeed, but it definitely deserves a try.
          Nonetheless, I''m not any experienced with that :/'
        updatedAt: '2022-06-25T14:04:07.264Z'
      numEdits: 0
      reactions: []
    id: 62b715d7d3296b4d6e84ba94
    type: comment
  author: Stancld
  content: 'Yes, gradient checkpointing, unfortunately, slows down a training, however,
    I''m really surprised what the difference it is!

    I haven''t tried training with DeepSpeed, but it definitely deserves a try. Nonetheless,
    I''m not any experienced with that :/'
  created_at: 2022-06-25 13:04:07+00:00
  edited: false
  hidden: false
  id: 62b715d7d3296b4d6e84ba94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-29T10:13:51.000Z'
    data:
      edited: false
      editors:
      - kmfoda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
          fullname: Karim Foda
          isHf: false
          isPro: false
          name: kmfoda
          type: user
        html: '<p>yeah it is surprising given that the original gradient_checkpointing
          paper says that speed should only slow down by 20%. I''ll debug this + try
          and get deepspeed working and let you know if I make any progress. Thanks
          for the help!</p>

          '
        raw: yeah it is surprising given that the original gradient_checkpointing
          paper says that speed should only slow down by 20%. I'll debug this + try
          and get deepspeed working and let you know if I make any progress. Thanks
          for the help!
        updatedAt: '2022-06-29T10:13:51.863Z'
      numEdits: 0
      reactions: []
    id: 62bc25dfb07d18d77fcca440
    type: comment
  author: kmfoda
  content: yeah it is surprising given that the original gradient_checkpointing paper
    says that speed should only slow down by 20%. I'll debug this + try and get deepspeed
    working and let you know if I make any progress. Thanks for the help!
  created_at: 2022-06-29 09:13:51+00:00
  edited: false
  hidden: false
  id: 62bc25dfb07d18d77fcca440
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1606374473622-5e9bf4984957053f60648a30.jpeg?w=200&h=200&f=face
      fullname: Karim Foda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kmfoda
      type: user
    createdAt: '2022-06-29T10:13:51.000Z'
    data:
      status: closed
    id: 62bc25dfb07d18d77fcca441
    type: status-change
  author: kmfoda
  created_at: 2022-06-29 09:13:51+00:00
  id: 62bc25dfb07d18d77fcca441
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a5f59a5ed7cdac65aecb5a1fb7c5ccfc.svg
      fullname: zhichao yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whaleloops
      type: user
    createdAt: '2022-07-12T16:55:16.000Z'
    data:
      edited: true
      editors:
      - whaleloops
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a5f59a5ed7cdac65aecb5a1fb7c5ccfc.svg
          fullname: zhichao yang
          isHf: false
          isPro: false
          name: whaleloops
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Stancld&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Stancld\">@<span class=\"\
          underline\">Stancld</span></a></span>\n\n\t</span></span>, When ddp, I encountered\
          \ the following error when I try to run your code in the middle of an epoch.\
          \ I used <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/v4.20.1/examples/pytorch/summarization/run_summarization.py\"\
          >run_summarization.py</a>. Is there anything particular about step 13? Any\
          \ suggestion for solving this error?</p>\n<p>1%|\u2588                 \
          \                                                                      \
          \                                                             | 13/1872\
          \ [16:51&lt;43:45:35, 84.74s/it]Traceback (most recent call last):<br> \
          \ File \"run_summarization.py\", line 737, in <br>    main()<br>  File \"\
          run_summarization.py\", line 656, in main<br>    train_result = trainer.train(resume_from_checkpoint=checkpoint)<br>\
          \  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 1409, in train<br>    return inner_training_loop(<br>  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 1649, in _inner_training_loop<br>    tr_loss_step = self.training_step(model,\
          \ inputs)<br>  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 2345, in training_step<br>    loss = self.compute_loss(model, inputs)<br>\
          \  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 2377, in compute_loss<br>    outputs = model(**inputs)<br>  File\
          \ \"/home//miniconda3/envs/t5long/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1110, in _call_impl<br>    return forward_call(*input, **kwargs)<br>\
          \  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\"\
          , line 947, in forward<br>    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():<br>RuntimeError:\
          \ Expected to have finished reduction in the prior iteration before starting\
          \ a new one. This error indicates that your module has parameters that were\
          \ not used in producing loss. You can enable unused parameter detection\
          \ by passing the keyword argument <code>find_unused_parameters=True</code>\
          \ to <code>torch.nn.parallel.DistributedDataParallel</code>, and by<br>making\
          \ sure all <code>forward</code> function outputs participate in calculating\
          \ loss.<br>If you already have done the above, then the distributed data\
          \ parallel module wasn't able to locate the output tensors in the return\
          \ value of your module's <code>forward</code> function. Please include the\
          \ loss function and the structure of the return value of <code>forward</code>\
          \ of your module when reporting this issue (e.g. list, dict, iterable).<br>Parameter\
          \ indices which did not receive grad for rank 0: 6</p>\n<p>Hardware:<br>6\
          \ Quadro RTX 6000 and 2 A100-40GB gpus, but I only used 2 A100-40GB gpus\
          \ for this task.</p>\n<p>Env:<br>transformers==4.20.1<br>torch==1.11.0+cu113</p>\n\
          <p>To Reproduce:<br>CUDA_VISIBLE_DEVICES=4,5 python -m torch.distributed.launch\
          \ --nproc_per_node 2 --master_port 56666 run_summarization.py <br>    --model_name_or_path\
          \ Stancld/longt5-tglobal-large-16384-pubmed-3k_steps  <br>    --do_train\
          \ --do_eval --do_predict <br>    --dataset_name ccdv/pubmed-summarization\
          \ <br>    --max_source_length 16384 --max_target_length 512 <br>    --per_device_train_batch_size\
          \ 1 --gradient_accumulation_steps 64 <br>    --optim adafactor --learning_rate\
          \ 0.001 --lr_scheduler_type constant --num_train_epochs 1 --gradient_checkpointing\
          \ <br>    --bf16=True --per_device_eval_batch_size 2 --predict_with_generate\
          \ --generation_num_beams 1 --generation_max_length 512 <br>    --output_dir\
          \ ./tmp/longt5_pubmed --run_name LongT5-pubmed-16k-512-bs_128 --report_to\
          \ all <br>    --logging_steps 100 --eval_steps 2000 --evaluation_strategy\
          \ steps --ddp_find_unused_parameters=False --no_cuda=False</p>\n<p>Here\
          \ is the failed <a rel=\"nofollow\" href=\"https://wandb.ai/whaleloops/pubmed_sum/runs/19h5mp66/overview?workspace=\"\
          >wandb</a>.</p>\n<p>I tried to run with 1 GPU, and it works for 50+ steps\
          \ without the error above.</p>\n"
        raw: "@Stancld, When ddp, I encountered the following error when I try to\
          \ run your code in the middle of an epoch. I used [run_summarization.py](https://github.com/huggingface/transformers/blob/v4.20.1/examples/pytorch/summarization/run_summarization.py).\
          \ Is there anything particular about step 13? Any suggestion for solving\
          \ this error?\n\n\n1%|\u2588                                           \
          \                                                                      \
          \                                   | 13/1872 [16:51<43:45:35, 84.74s/it]Traceback\
          \ (most recent call last):\n  File \"run_summarization.py\", line 737, in\
          \ <module>\n    main()\n  File \"run_summarization.py\", line 656, in main\n\
          \    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n \
          \ File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 1409, in train\n    return inner_training_loop(\n  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 1649, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
          \ inputs)\n  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 2345, in training_step\n    loss = self.compute_loss(model, inputs)\n\
          \  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
          , line 2377, in compute_loss\n    outputs = model(**inputs)\n  File \"/home//miniconda3/envs/t5long/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
          , line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\"\
          , line 947, in forward\n    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n\
          RuntimeError: Expected to have finished reduction in the prior iteration\
          \ before starting a new one. This error indicates that your module has parameters\
          \ that were not used in producing loss. You can enable unused parameter\
          \ detection by passing the keyword argument `find_unused_parameters=True`\
          \ to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all\
          \ `forward` function outputs participate in calculating loss.\nIf you already\
          \ have done the above, then the distributed data parallel module wasn't\
          \ able to locate the output tensors in the return value of your module's\
          \ `forward` function. Please include the loss function and the structure\
          \ of the return value of `forward` of your module when reporting this issue\
          \ (e.g. list, dict, iterable).\nParameter indices which did not receive\
          \ grad for rank 0: 6\n\nHardware:\n6 Quadro RTX 6000 and 2 A100-40GB gpus,\
          \ but I only used 2 A100-40GB gpus for this task.\n\nEnv:\ntransformers==4.20.1\n\
          torch==1.11.0+cu113\n\nTo Reproduce:\nCUDA_VISIBLE_DEVICES=4,5 python -m\
          \ torch.distributed.launch --nproc_per_node 2 --master_port 56666 run_summarization.py\
          \ \\\n    --model_name_or_path Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\
          \  \\\n    --do_train --do_eval --do_predict \\\n    --dataset_name ccdv/pubmed-summarization\
          \ \\\n    --max_source_length 16384 --max_target_length 512 \\\n    --per_device_train_batch_size\
          \ 1 --gradient_accumulation_steps 64 \\\n    --optim adafactor --learning_rate\
          \ 0.001 --lr_scheduler_type constant --num_train_epochs 1 --gradient_checkpointing\
          \ \\\n    --bf16=True --per_device_eval_batch_size 2 --predict_with_generate\
          \ --generation_num_beams 1 --generation_max_length 512 \\\n    --output_dir\
          \ ./tmp/longt5_pubmed --run_name LongT5-pubmed-16k-512-bs_128 --report_to\
          \ all \\\n    --logging_steps 100 --eval_steps 2000 --evaluation_strategy\
          \ steps --ddp_find_unused_parameters=False --no_cuda=False\n\nHere is the\
          \ failed [wandb](https://wandb.ai/whaleloops/pubmed_sum/runs/19h5mp66/overview?workspace=).\n\
          \nI tried to run with 1 GPU, and it works for 50+ steps without the error\
          \ above."
        updatedAt: '2022-07-12T17:02:33.718Z'
      numEdits: 2
      reactions: []
    id: 62cda774d17aee590596f2b8
    type: comment
  author: whaleloops
  content: "@Stancld, When ddp, I encountered the following error when I try to run\
    \ your code in the middle of an epoch. I used [run_summarization.py](https://github.com/huggingface/transformers/blob/v4.20.1/examples/pytorch/summarization/run_summarization.py).\
    \ Is there anything particular about step 13? Any suggestion for solving this\
    \ error?\n\n\n1%|\u2588                                                      \
    \                                                                            \
    \                  | 13/1872 [16:51<43:45:35, 84.74s/it]Traceback (most recent\
    \ call last):\n  File \"run_summarization.py\", line 737, in <module>\n    main()\n\
    \  File \"run_summarization.py\", line 656, in main\n    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n\
    \  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
    , line 1409, in train\n    return inner_training_loop(\n  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
    , line 1649, in _inner_training_loop\n    tr_loss_step = self.training_step(model,\
    \ inputs)\n  File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
    , line 2345, in training_step\n    loss = self.compute_loss(model, inputs)\n \
    \ File \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/transformers/trainer.py\"\
    , line 2377, in compute_loss\n    outputs = model(**inputs)\n  File \"/home//miniconda3/envs/t5long/lib/python3.8/site-packages/torch/nn/modules/module.py\"\
    , line 1110, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/home/miniconda3/envs/t5long/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\"\
    , line 947, in forward\n    if torch.is_grad_enabled() and self.reducer._rebuild_buckets():\n\
    RuntimeError: Expected to have finished reduction in the prior iteration before\
    \ starting a new one. This error indicates that your module has parameters that\
    \ were not used in producing loss. You can enable unused parameter detection by\
    \ passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`,\
    \ and by\nmaking sure all `forward` function outputs participate in calculating\
    \ loss.\nIf you already have done the above, then the distributed data parallel\
    \ module wasn't able to locate the output tensors in the return value of your\
    \ module's `forward` function. Please include the loss function and the structure\
    \ of the return value of `forward` of your module when reporting this issue (e.g.\
    \ list, dict, iterable).\nParameter indices which did not receive grad for rank\
    \ 0: 6\n\nHardware:\n6 Quadro RTX 6000 and 2 A100-40GB gpus, but I only used 2\
    \ A100-40GB gpus for this task.\n\nEnv:\ntransformers==4.20.1\ntorch==1.11.0+cu113\n\
    \nTo Reproduce:\nCUDA_VISIBLE_DEVICES=4,5 python -m torch.distributed.launch --nproc_per_node\
    \ 2 --master_port 56666 run_summarization.py \\\n    --model_name_or_path Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\
    \  \\\n    --do_train --do_eval --do_predict \\\n    --dataset_name ccdv/pubmed-summarization\
    \ \\\n    --max_source_length 16384 --max_target_length 512 \\\n    --per_device_train_batch_size\
    \ 1 --gradient_accumulation_steps 64 \\\n    --optim adafactor --learning_rate\
    \ 0.001 --lr_scheduler_type constant --num_train_epochs 1 --gradient_checkpointing\
    \ \\\n    --bf16=True --per_device_eval_batch_size 2 --predict_with_generate --generation_num_beams\
    \ 1 --generation_max_length 512 \\\n    --output_dir ./tmp/longt5_pubmed --run_name\
    \ LongT5-pubmed-16k-512-bs_128 --report_to all \\\n    --logging_steps 100 --eval_steps\
    \ 2000 --evaluation_strategy steps --ddp_find_unused_parameters=False --no_cuda=False\n\
    \nHere is the failed [wandb](https://wandb.ai/whaleloops/pubmed_sum/runs/19h5mp66/overview?workspace=).\n\
    \nI tried to run with 1 GPU, and it works for 50+ steps without the error above."
  created_at: 2022-07-12 15:55:16+00:00
  edited: true
  hidden: false
  id: 62cda774d17aee590596f2b8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/56af091aaff1b42dcfbae84a6ee1e7f7.svg
      fullname: Zhang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jianwen
      type: user
    createdAt: '2022-10-31T19:19:59.000Z'
    data:
      edited: false
      editors:
      - Jianwen
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/56af091aaff1b42dcfbae84a6ee1e7f7.svg
          fullname: Zhang
          isHf: false
          isPro: false
          name: Jianwen
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;whaleloops&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/whaleloops\">@<span class=\"\
          underline\">whaleloops</span></a></span>\n\n\t</span></span> : you can try\
          \ turn on \"--find_unused_parameters\", this works for me (but looks slowed\
          \ down a bit as well).</p>\n"
        raw: '@whaleloops : you can try turn on "--find_unused_parameters", this works
          for me (but looks slowed down a bit as well).'
        updatedAt: '2022-10-31T19:19:59.389Z'
      numEdits: 0
      reactions: []
    id: 63601fdfd9e4214b9a684579
    type: comment
  author: Jianwen
  content: '@whaleloops : you can try turn on "--find_unused_parameters", this works
    for me (but looks slowed down a bit as well).'
  created_at: 2022-10-31 18:19:59+00:00
  edited: false
  hidden: false
  id: 63601fdfd9e4214b9a684579
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Stancld/longt5-tglobal-large-16384-pubmed-3k_steps
repo_type: model
status: closed
target_branch: null
title: required setup for training
