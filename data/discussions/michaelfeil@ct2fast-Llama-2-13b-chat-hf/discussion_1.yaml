!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philschmid
conflicting_files: null
created_at: 2023-07-20 14:22:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-07-20T15:22:04.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9132755398750305
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: '<p>Super cool conversion. Whats the latency you see on which hardware?
          </p>

          '
        raw: 'Super cool conversion. Whats the latency you see on which hardware? '
        updatedAt: '2023-07-20T15:22:04.112Z'
      numEdits: 0
      reactions: []
    id: 64b9511cee257c3a4c011d45
    type: comment
  author: philschmid
  content: 'Super cool conversion. Whats the latency you see on which hardware? '
  created_at: 2023-07-20 14:22:04+00:00
  edited: false
  hidden: false
  id: 64b9511cee257c3a4c011d45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-07-20T21:22:29.000Z'
    data:
      edited: true
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6447466015815735
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: "<p>Most impressive thing first -  loading latency.  13B model into\
          \ VRAM - from first line of the script until forward takes 6.6s.</p>\n<p>Here\
          \ are some rough numbers on throughput, which are okay:</p>\n<p>Nvidia A6000<br>\
          \ input shape: (1, 65)   output shape: (1, 128)   Total time 3.440488675609231\
          \ seconds           avg tokens/s 37.204017239595814<br>input shape: (16,\
          \ 65) output shape: (16, 128) Total time 8.294885652139783 seconds     \
          \      avg tokens/s 246.89912385611842 (divide by 16)<br>memory: 13462 MiB\
          \ Vram (idle) -&gt; max 16GiB Vram (batch size 16, at 192 tokens)<br>Note:\
          \ Output does not contain the input tokens. Example for \u201Estatic batching\u201C\
          .<br>Note for the 7B variant:<br>input shape: (1, 65) output shape: (1,\
          \ 128) Total time     2.0009103175252676 seconds          avg tokens/s 63.97088309200725</p>\n"
        raw: "Most impressive thing first -  loading latency.  13B model into VRAM\
          \ - from first line of the script until forward takes 6.6s.\n\nHere are\
          \ some rough numbers on throughput, which are okay:\n\nNvidia A6000\n input\
          \ shape: (1, 65)   output shape: (1, 128)   Total time 3.440488675609231\
          \ seconds           avg tokens/s 37.204017239595814\ninput shape: (16, 65)\
          \ output shape: (16, 128) Total time 8.294885652139783 seconds         \
          \  avg tokens/s 246.89912385611842 (divide by 16)\nmemory: 13462 MiB Vram\
          \ (idle) -> max 16GiB Vram (batch size 16, at 192 tokens)\nNote: Output\
          \ does not contain the input tokens. Example for \u201Estatic batching\u201C\
          .\nNote for the 7B variant:\ninput shape: (1, 65) output shape: (1, 128)\
          \ Total time     2.0009103175252676 seconds          avg tokens/s 63.97088309200725"
        updatedAt: '2023-07-20T21:42:54.906Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - philschmid
        - jeffboudier
    id: 64b9a5955d5cc69c04647fa1
    type: comment
  author: michaelfeil
  content: "Most impressive thing first -  loading latency.  13B model into VRAM -\
    \ from first line of the script until forward takes 6.6s.\n\nHere are some rough\
    \ numbers on throughput, which are okay:\n\nNvidia A6000\n input shape: (1, 65)\
    \   output shape: (1, 128)   Total time 3.440488675609231 seconds           avg\
    \ tokens/s 37.204017239595814\ninput shape: (16, 65) output shape: (16, 128) Total\
    \ time 8.294885652139783 seconds           avg tokens/s 246.89912385611842 (divide\
    \ by 16)\nmemory: 13462 MiB Vram (idle) -> max 16GiB Vram (batch size 16, at 192\
    \ tokens)\nNote: Output does not contain the input tokens. Example for \u201E\
    static batching\u201C.\nNote for the 7B variant:\ninput shape: (1, 65) output\
    \ shape: (1, 128) Total time     2.0009103175252676 seconds          avg tokens/s\
    \ 63.97088309200725"
  created_at: 2023-07-20 20:22:29+00:00
  edited: true
  hidden: false
  id: 64b9a5955d5cc69c04647fa1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-07-21T17:22:20.000Z'
    data:
      edited: true
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6643550395965576
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;philschmid&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/philschmid\">@<span class=\"\
          underline\">philschmid</span></a></span>\n\n\t</span></span>  To give some\
          \ more context - ran the same benchmark (aka A6000, 13B, input shape: (1,\
          \ 65) output shape: (1, 128) ) on text-generation-interface 0.9.3.</p>\n\
          <pre><code>generate{parameters=GenerateParameters { best_of: Some(1), temperature:\
          \ Some(0.5),\n repetition_penalty: Some(1.03), top_k: Some(10), top_p: Some(0.95),\
          \ typical_p: Some(0.95), do_sample: true, max_new_tokens: 128,\n return_full_text:\
          \ Some(false), stop: [\"photographer\"], truncate: None, watermark: true,\n\
          \ details: true, decoder_input_details: true, seed: None } total_time=\"\
          5.575435704s\" validation_time=\"846.928\xB5s\"\n queue_time=\"151.59\xB5\
          s\" inference_time=\"5.574437698s\" time_per_token=\"43.550294ms\" seed=\"\
          Some(1584686632614596462)\"\n</code></pre>\n<p>Roughly it  takes 162% of\
          \ the time of ctranslate2 3.17.1 and also more ram.<br>bitsandbytes would\
          \ take less ram, but would also be slower.</p>\n<p>Disclaimer: results are\
          \ biased:</p>\n<ul>\n<li>strengths of  text-generation-interface are not\
          \ used. batch size=1,  gpus=1  (dynamic and continuous batching, ct2 can't\
          \ do tensor-parallel / multi-gpu inference)</li>\n<li>no statistical eval\
          \ (tried it 2-3 times)</li>\n</ul>\n"
        raw: "@philschmid  To give some more context - ran the same benchmark (aka\
          \ A6000, 13B, input shape: (1, 65) output shape: (1, 128) ) on text-generation-interface\
          \ 0.9.3.\n```\ngenerate{parameters=GenerateParameters { best_of: Some(1),\
          \ temperature: Some(0.5),\n repetition_penalty: Some(1.03), top_k: Some(10),\
          \ top_p: Some(0.95), typical_p: Some(0.95), do_sample: true, max_new_tokens:\
          \ 128,\n return_full_text: Some(false), stop: [\"photographer\"], truncate:\
          \ None, watermark: true,\n details: true, decoder_input_details: true, seed:\
          \ None } total_time=\"5.575435704s\" validation_time=\"846.928\xB5s\"\n\
          \ queue_time=\"151.59\xB5s\" inference_time=\"5.574437698s\" time_per_token=\"\
          43.550294ms\" seed=\"Some(1584686632614596462)\"\n```\nRoughly it  takes\
          \ 162% of the time of ctranslate2 3.17.1 and also more ram. \nbitsandbytes\
          \ would take less ram, but would also be slower.\n\nDisclaimer: results\
          \ are biased:\n- strengths of  text-generation-interface are not used. batch\
          \ size=1,  gpus=1  (dynamic and continuous batching, ct2 can't do tensor-parallel\
          \ / multi-gpu inference)\n- no statistical eval (tried it 2-3 times)"
        updatedAt: '2023-07-22T11:07:50.474Z'
      numEdits: 1
      reactions: []
    id: 64babecc6999b520ed1f6eb5
    type: comment
  author: michaelfeil
  content: "@philschmid  To give some more context - ran the same benchmark (aka A6000,\
    \ 13B, input shape: (1, 65) output shape: (1, 128) ) on text-generation-interface\
    \ 0.9.3.\n```\ngenerate{parameters=GenerateParameters { best_of: Some(1), temperature:\
    \ Some(0.5),\n repetition_penalty: Some(1.03), top_k: Some(10), top_p: Some(0.95),\
    \ typical_p: Some(0.95), do_sample: true, max_new_tokens: 128,\n return_full_text:\
    \ Some(false), stop: [\"photographer\"], truncate: None, watermark: true,\n details:\
    \ true, decoder_input_details: true, seed: None } total_time=\"5.575435704s\"\
    \ validation_time=\"846.928\xB5s\"\n queue_time=\"151.59\xB5s\" inference_time=\"\
    5.574437698s\" time_per_token=\"43.550294ms\" seed=\"Some(1584686632614596462)\"\
    \n```\nRoughly it  takes 162% of the time of ctranslate2 3.17.1 and also more\
    \ ram. \nbitsandbytes would take less ram, but would also be slower.\n\nDisclaimer:\
    \ results are biased:\n- strengths of  text-generation-interface are not used.\
    \ batch size=1,  gpus=1  (dynamic and continuous batching, ct2 can't do tensor-parallel\
    \ / multi-gpu inference)\n- no statistical eval (tried it 2-3 times)"
  created_at: 2023-07-21 16:22:20+00:00
  edited: true
  hidden: false
  id: 64babecc6999b520ed1f6eb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-07-22T08:20:14.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9852996468544006
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: '<p>Thank you for sharing! BTW. TGI also support GPTQ now. Maybe that''s
          also a good way to decrease memory needed. </p>

          '
        raw: "Thank you for sharing! BTW. TGI also support GPTQ now. Maybe that's\
          \ also a good way to decrease memory needed. \n"
        updatedAt: '2023-07-22T08:20:14.374Z'
      numEdits: 0
      reactions: []
    id: 64bb913e796f20daad439875
    type: comment
  author: philschmid
  content: "Thank you for sharing! BTW. TGI also support GPTQ now. Maybe that's also\
    \ a good way to decrease memory needed. \n"
  created_at: 2023-07-22 07:20:14+00:00
  edited: false
  hidden: false
  id: 64bb913e796f20daad439875
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: michaelfeil/ct2fast-Llama-2-13b-chat-hf
repo_type: model
status: open
target_branch: null
title: Latency
