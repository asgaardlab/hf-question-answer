!!python/object:huggingface_hub.community.DiscussionWithDetails
author: carson-together
conflicting_files: null
created_at: 2023-12-28 23:49:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c7f044b8685df8002113e3/kJGvyJiUmIb2ZVrxH8sHJ.jpeg?w=200&h=200&f=face
      fullname: Carson Lam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: carson-together
      type: user
    createdAt: '2023-12-28T23:49:58.000Z'
    data:
      edited: true
      editors:
      - carson-together
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8372361063957214
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64c7f044b8685df8002113e3/kJGvyJiUmIb2ZVrxH8sHJ.jpeg?w=200&h=200&f=face
          fullname: Carson Lam
          isHf: false
          isPro: false
          name: carson-together
          type: user
        html: "<p>I think there is a mismatch between the number of tokens in the\
          \ tokenizer vocab size and config.json</p>\n<p>When loading the tokenizer:<br><code>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"NousResearch/Nous-Hermes-2-Yi-34B\"\
          )</code><br>we encounter this warning:</p>\n<pre><code>Special tokens have\
          \ been added in the vocabulary, make sure the associated word embeddings\
          \ are fine-tuned or trained.\nYou are converting a LlamaTokenizer to a LlamaTokenizerFast,\
          \ but wrong indexes were founds when adding the `added_tokens` from the\
          \ `slow` tokenizer to the `fast`.  The following tokens had unexpected id\
          \ :\n    expected id: 64000, found: 1,  token: `&lt;|startoftext|&gt;`,\n\
          \    expected id: 64001, found: 2,  token: `&lt;|endoftext|&gt;`,\n. You\
          \ should try using `from_slow`.\nSpecial tokens have been added in the vocabulary,\
          \ make sure the associated word embeddings are fine-tuned or trained.\n\
          </code></pre>\n<p> The tokenizer vocab size is 64002<br><code>len(tokenizer)</code></p>\n\
          <pre><code>64002\n</code></pre>\n<p>but config.json reports the vocab size\
          \ is 64000</p>\n<pre><code> \"use_cache\": false,\n  \"vocab_size\": 64000\n\
          }\n</code></pre>\n"
        raw: "I think there is a mismatch between the number of tokens in the tokenizer\
          \ vocab size and config.json\n\nWhen loading the tokenizer:\n`tokenizer\
          \ = AutoTokenizer.from_pretrained(\"NousResearch/Nous-Hermes-2-Yi-34B\"\
          )`\nwe encounter this warning:\n```\nSpecial tokens have been added in the\
          \ vocabulary, make sure the associated word embeddings are fine-tuned or\
          \ trained.\nYou are converting a LlamaTokenizer to a LlamaTokenizerFast,\
          \ but wrong indexes were founds when adding the `added_tokens` from the\
          \ `slow` tokenizer to the `fast`.  The following tokens had unexpected id\
          \ :\n\texpected id: 64000, found: 1,  token: `<|startoftext|>`,\n\texpected\
          \ id: 64001, found: 2,  token: `<|endoftext|>`,\n. You should try using\
          \ `from_slow`.\nSpecial tokens have been added in the vocabulary, make sure\
          \ the associated word embeddings are fine-tuned or trained.\n```\n The tokenizer\
          \ vocab size is 64002\n`len(tokenizer)`\n```\n64002\n```\nbut config.json\
          \ reports the vocab size is 64000\n```\n \"use_cache\": false,\n  \"vocab_size\"\
          : 64000\n}\n```\n\n"
        updatedAt: '2023-12-28T23:51:34.692Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - pragaash
        - brucethemoose
    id: 658e09a61adf6d577e29d41b
    type: comment
  author: carson-together
  content: "I think there is a mismatch between the number of tokens in the tokenizer\
    \ vocab size and config.json\n\nWhen loading the tokenizer:\n`tokenizer = AutoTokenizer.from_pretrained(\"\
    NousResearch/Nous-Hermes-2-Yi-34B\")`\nwe encounter this warning:\n```\nSpecial\
    \ tokens have been added in the vocabulary, make sure the associated word embeddings\
    \ are fine-tuned or trained.\nYou are converting a LlamaTokenizer to a LlamaTokenizerFast,\
    \ but wrong indexes were founds when adding the `added_tokens` from the `slow`\
    \ tokenizer to the `fast`.  The following tokens had unexpected id :\n\texpected\
    \ id: 64000, found: 1,  token: `<|startoftext|>`,\n\texpected id: 64001, found:\
    \ 2,  token: `<|endoftext|>`,\n. You should try using `from_slow`.\nSpecial tokens\
    \ have been added in the vocabulary, make sure the associated word embeddings\
    \ are fine-tuned or trained.\n```\n The tokenizer vocab size is 64002\n`len(tokenizer)`\n\
    ```\n64002\n```\nbut config.json reports the vocab size is 64000\n```\n \"use_cache\"\
    : false,\n  \"vocab_size\": 64000\n}\n```\n\n"
  created_at: 2023-12-28 23:49:58+00:00
  edited: true
  hidden: false
  id: 658e09a61adf6d577e29d41b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-05T19:57:38.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.438010036945343
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah.</p>

          <p>Mergekit gives this warning:</p>

          <pre><code>WARNING:root:Token ''&lt;|startoftext|&gt;'' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B
          tokenizer but &gt;= vocab_size

          WARNING:root:Token ''&lt;|endoftext|&gt;'' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B
          tokenizer but &gt;= vocab_size

          </code></pre>

          '
        raw: 'Yeah.


          Mergekit gives this warning:


          ```

          WARNING:root:Token ''<|startoftext|>'' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B
          tokenizer but >= vocab_size

          WARNING:root:Token ''<|endoftext|>'' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B
          tokenizer but >= vocab_size

          ```'
        updatedAt: '2024-01-05T19:57:38.490Z'
      numEdits: 0
      reactions: []
    id: 65985f3217edd1f05307410a
    type: comment
  author: brucethemoose
  content: 'Yeah.


    Mergekit gives this warning:


    ```

    WARNING:root:Token ''<|startoftext|>'' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B
    tokenizer but >= vocab_size

    WARNING:root:Token ''<|endoftext|>'' present in /home/alpha/Models/Raw/NousResearch_Nous-Hermes-2-Yi-34B
    tokenizer but >= vocab_size

    ```'
  created_at: 2024-01-05 19:57:38+00:00
  edited: false
  hidden: false
  id: 65985f3217edd1f05307410a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-06T07:29:59.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9782666563987732
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Correcting the vocab size to 64002 doesn''t seem to work either.</p>

          '
        raw: 'Correcting the vocab size to 64002 doesn''t seem to work either.


          '
        updatedAt: '2024-01-06T07:29:59.723Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Sciumo
    id: 6599017722031755fdf05797
    type: comment
  author: brucethemoose
  content: 'Correcting the vocab size to 64002 doesn''t seem to work either.


    '
  created_at: 2024-01-06 07:29:59+00:00
  edited: false
  hidden: false
  id: 6599017722031755fdf05797
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: NousResearch/Nous-Hermes-2-Yi-34B
repo_type: model
status: open
target_branch: null
title: configs do not match tokenizer vocab size
