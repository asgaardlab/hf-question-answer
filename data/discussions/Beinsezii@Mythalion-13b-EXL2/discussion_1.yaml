!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mammour
conflicting_files: null
created_at: 2023-11-04 10:47:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7a12edd32b236ef45e0cc6525a4e389.svg
      fullname: Margaux Ammour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mammour
      type: user
    createdAt: '2023-11-04T11:47:36.000Z'
    data:
      edited: false
      editors:
      - mammour
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8736687302589417
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7a12edd32b236ef45e0cc6525a4e389.svg
          fullname: Margaux Ammour
          isHf: false
          isPro: false
          name: mammour
          type: user
        html: '<p>Hey, base perplexity in the measurements is over 2000 when it should
          be &lt; 10, kudos</p>

          '
        raw: Hey, base perplexity in the measurements is over 2000 when it should
          be < 10, kudos
        updatedAt: '2023-11-04T11:47:36.431Z'
      numEdits: 0
      reactions: []
    id: 65462f584d1931dc93d23ea3
    type: comment
  author: mammour
  content: Hey, base perplexity in the measurements is over 2000 when it should be
    < 10, kudos
  created_at: 2023-11-04 10:47:36+00:00
  edited: false
  hidden: false
  id: 65462f584d1931dc93d23ea3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b7a12edd32b236ef45e0cc6525a4e389.svg
      fullname: Margaux Ammour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mammour
      type: user
    createdAt: '2023-11-04T11:50:01.000Z'
    data:
      edited: false
      editors:
      - mammour
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9559980034828186
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b7a12edd32b236ef45e0cc6525a4e389.svg
          fullname: Margaux Ammour
          isHf: false
          isPro: false
          name: mammour
          type: user
        html: '<p>(all of your measurements seem to face this issue excepted for Tiefighter)</p>

          '
        raw: (all of your measurements seem to face this issue excepted for Tiefighter)
        updatedAt: '2023-11-04T11:50:01.494Z'
      numEdits: 0
      reactions: []
    id: 65462fe9565e3985e8720731
    type: comment
  author: mammour
  content: (all of your measurements seem to face this issue excepted for Tiefighter)
  created_at: 2023-11-04 10:50:01+00:00
  edited: false
  hidden: false
  id: 65462fe9565e3985e8720731
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/577bc85ee5d849291474012302052252.svg
      fullname: Raison D'etre
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Beinsezii
      type: user
    createdAt: '2023-11-04T23:05:29.000Z'
    data:
      edited: false
      editors:
      - Beinsezii
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9847921133041382
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/577bc85ee5d849291474012302052252.svg
          fullname: Raison D'etre
          isHf: false
          isPro: false
          name: Beinsezii
          type: user
        html: '<p>I use the exact same command for all of them. The first two were
          done with Exllama2 0.0.1 is the only difference, where tiefighter was done
          with 0.0.8</p>

          <p>Not entirely sure what "perplexity" means. The quantizations perform
          completely fine. I shared the Mythomax privately before uploading to HF
          and the responses were overwhelmingly positive.</p>

          '
        raw: 'I use the exact same command for all of them. The first two were done
          with Exllama2 0.0.1 is the only difference, where tiefighter was done with
          0.0.8


          Not entirely sure what "perplexity" means. The quantizations perform completely
          fine. I shared the Mythomax privately before uploading to HF and the responses
          were overwhelmingly positive.'
        updatedAt: '2023-11-04T23:05:29.744Z'
      numEdits: 0
      reactions: []
    id: 6546ce392119c8bdf26f968b
    type: comment
  author: Beinsezii
  content: 'I use the exact same command for all of them. The first two were done
    with Exllama2 0.0.1 is the only difference, where tiefighter was done with 0.0.8


    Not entirely sure what "perplexity" means. The quantizations perform completely
    fine. I shared the Mythomax privately before uploading to HF and the responses
    were overwhelmingly positive.'
  created_at: 2023-11-04 22:05:29+00:00
  edited: false
  hidden: false
  id: 6546ce392119c8bdf26f968b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/577bc85ee5d849291474012302052252.svg
      fullname: Raison D'etre
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Beinsezii
      type: user
    createdAt: '2023-11-05T00:41:51.000Z'
    data:
      edited: true
      editors:
      - Beinsezii
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9683926105499268
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/577bc85ee5d849291474012302052252.svg
          fullname: Raison D'etre
          isHf: false
          isPro: false
          name: Beinsezii
          type: user
        html: '<p>I ran test_inference.py on my 8bit mythomax quant and got an evaluation
          perplexity of 5.7459 which leads me to believe it''s just a err with how
          it''s represented in the .json from older exllama2 versions. The actual
          quants should still work as intended.</p>

          '
        raw: I ran test_inference.py on my 8bit mythomax quant and got an evaluation
          perplexity of 5.7459 which leads me to believe it's just a err with how
          it's represented in the .json from older exllama2 versions. The actual quants
          should still work as intended.
        updatedAt: '2023-11-05T00:47:48.797Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - mammour
    id: 6546e4cfcd0a56213928d54c
    type: comment
  author: Beinsezii
  content: I ran test_inference.py on my 8bit mythomax quant and got an evaluation
    perplexity of 5.7459 which leads me to believe it's just a err with how it's represented
    in the .json from older exllama2 versions. The actual quants should still work
    as intended.
  created_at: 2023-11-04 23:41:51+00:00
  edited: true
  hidden: false
  id: 6546e4cfcd0a56213928d54c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/b7a12edd32b236ef45e0cc6525a4e389.svg
      fullname: Margaux Ammour
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mammour
      type: user
    createdAt: '2023-11-07T15:07:14.000Z'
    data:
      status: closed
    id: 654a52a25cc8b44229fc0330
    type: status-change
  author: mammour
  created_at: 2023-11-07 15:07:14+00:00
  id: 654a52a25cc8b44229fc0330
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Beinsezii/Mythalion-13b-EXL2
repo_type: model
status: closed
target_branch: null
title: Perplexity issue
