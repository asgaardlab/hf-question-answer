!!python/object:huggingface_hub.community.DiscussionWithDetails
author: latitude
conflicting_files: null
created_at: 2022-06-06 15:11:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d56ff540a95d44b791149922d4b93dd.svg
      fullname: Latitude
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: latitude
      type: user
    createdAt: '2022-06-06T16:11:21.000Z'
    data:
      edited: false
      editors:
      - latitude
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d56ff540a95d44b791149922d4b93dd.svg
          fullname: Latitude
          isHf: false
          isPro: false
          name: latitude
          type: user
        html: '<p>Typically,  you would use this model together with the multi-QA_v1-mpnet-asymmetric-Q
          model, encoding the question with the Q model and the answers with the A
          model, and then doing a cosine similarity comparison. But the Inference
          API seems to be using the same model for both. Is there a way I can call
          the Inference API on a list of sentences and get back a list of vectors,
          and do the comparison myself?</p>

          '
        raw: Typically,  you would use this model together with the multi-QA_v1-mpnet-asymmetric-Q
          model, encoding the question with the Q model and the answers with the A
          model, and then doing a cosine similarity comparison. But the Inference
          API seems to be using the same model for both. Is there a way I can call
          the Inference API on a list of sentences and get back a list of vectors,
          and do the comparison myself?
        updatedAt: '2022-06-06T16:11:21.172Z'
      numEdits: 0
      reactions: []
    id: 629e27299122a27bff577645
    type: comment
  author: latitude
  content: Typically,  you would use this model together with the multi-QA_v1-mpnet-asymmetric-Q
    model, encoding the question with the Q model and the answers with the A model,
    and then doing a cosine similarity comparison. But the Inference API seems to
    be using the same model for both. Is there a way I can call the Inference API
    on a list of sentences and get back a list of vectors, and do the comparison myself?
  created_at: 2022-06-06 15:11:21+00:00
  edited: false
  hidden: false
  id: 629e27299122a27bff577645
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-A
repo_type: model
status: open
target_branch: null
title: Using the Q model and the A model with the Inference API?
