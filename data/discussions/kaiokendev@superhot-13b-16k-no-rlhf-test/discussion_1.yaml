!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flashvenom
conflicting_files: null
created_at: 2023-06-23 03:28:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T04:28:03.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9561259150505066
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>Seems like training data and params are the same, other than the
          different in config what is different with this model?</p>

          '
        raw: Seems like training data and params are the same, other than the different
          in config what is different with this model?
        updatedAt: '2023-06-23T04:28:03.659Z'
      numEdits: 0
      reactions: []
    id: 64951f53eb5dbb26b4d81200
    type: comment
  author: flashvenom
  content: Seems like training data and params are the same, other than the different
    in config what is different with this model?
  created_at: 2023-06-23 03:28:03+00:00
  edited: false
  hidden: false
  id: 64951f53eb5dbb26b4d81200
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-06-23T04:29:39.000Z'
    data:
      edited: false
      editors:
      - kaiokendev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9471721053123474
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>This model is trained on a scaling factor of 0.125 using the same
          technique that was used on the 8K model. This model should have max sequence
          length of 16384</p>

          '
        raw: This model is trained on a scaling factor of 0.125 using the same technique
          that was used on the 8K model. This model should have max sequence length
          of 16384
        updatedAt: '2023-06-23T04:29:39.047Z'
      numEdits: 0
      reactions: []
    id: 64951fb3fe1d53b7a3f17364
    type: comment
  author: kaiokendev
  content: This model is trained on a scaling factor of 0.125 using the same technique
    that was used on the 8K model. This model should have max sequence length of 16384
  created_at: 2023-06-23 03:29:39+00:00
  edited: false
  hidden: false
  id: 64951fb3fe1d53b7a3f17364
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T04:30:28.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8347339630126953
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>Ah, I wonder how the training on the 0.125 scaling factor affects
          performance at lower context lengths, ie. I wonder how the 8k model and
          16k model perform at lets say a 4k context length and 1:2 ratio</p>

          '
        raw: Ah, I wonder how the training on the 0.125 scaling factor affects performance
          at lower context lengths, ie. I wonder how the 8k model and 16k model perform
          at lets say a 4k context length and 1:2 ratio
        updatedAt: '2023-06-23T04:30:28.010Z'
      numEdits: 0
      reactions: []
    id: 64951fe4b70715a4bc29e3cc
    type: comment
  author: flashvenom
  content: Ah, I wonder how the training on the 0.125 scaling factor affects performance
    at lower context lengths, ie. I wonder how the 8k model and 16k model perform
    at lets say a 4k context length and 1:2 ratio
  created_at: 2023-06-23 03:30:28+00:00
  edited: false
  hidden: false
  id: 64951fe4b70715a4bc29e3cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-06-23T04:38:33.000Z'
    data:
      edited: true
      editors:
      - kaiokendev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9274632930755615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: '<p>A similar perplexity text should determine what performance difference
          exists, since the only difference between this and the 8K version is the
          scaling factor and max length, so it should be easy to compare the two.
          I will also try training one with just scaling of 0.5 (4096 max length)</p>

          '
        raw: A similar perplexity text should determine what performance difference
          exists, since the only difference between this and the 8K version is the
          scaling factor and max length, so it should be easy to compare the two.
          I will also try training one with just scaling of 0.5 (4096 max length)
        updatedAt: '2023-06-23T04:38:56.713Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flashvenom
    id: 649521c9fd5fd44072070d94
    type: comment
  author: kaiokendev
  content: A similar perplexity text should determine what performance difference
    exists, since the only difference between this and the 8K version is the scaling
    factor and max length, so it should be easy to compare the two. I will also try
    training one with just scaling of 0.5 (4096 max length)
  created_at: 2023-06-23 03:38:33+00:00
  edited: true
  hidden: false
  id: 649521c9fd5fd44072070d94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T05:58:32.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8489231467247009
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>sounds good, ill merge the adapter and see if I can get some numbers
          for you, im curious to see how it changes</p>

          '
        raw: sounds good, ill merge the adapter and see if I can get some numbers
          for you, im curious to see how it changes
        updatedAt: '2023-06-23T05:58:32.260Z'
      numEdits: 0
      reactions: []
    id: 6495348893e58d15bc91bc8a
    type: comment
  author: flashvenom
  content: sounds good, ill merge the adapter and see if I can get some numbers for
    you, im curious to see how it changes
  created_at: 2023-06-23 04:58:32+00:00
  edited: false
  hidden: false
  id: 6495348893e58d15bc91bc8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T17:34:59.000Z'
    data:
      edited: true
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9769085645675659
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>I have some results for you:</p>

          <p>16K merged into model has ppl of 7.3050, 8K merged into a model has ppl
          of 7.5387 (using 2k context)<br>16K is at 7.7976 and the 8K is at 7.7789
          - at 8k context, scaling at 4<br>16K is at 9.4433 and the 8K is at 11.3963
          - at 16K context and 8 scaling</p>

          <p>very interesting that the 16K model seems to be somewhat "better" at
          2K context, and also proof that the 16K training works with the lower ppl
          at higher scaling</p>

          '
        raw: 'I have some results for you:


          16K merged into model has ppl of 7.3050, 8K merged into a model has ppl
          of 7.5387 (using 2k context)

          16K is at 7.7976 and the 8K is at 7.7789 - at 8k context, scaling at 4

          16K is at 9.4433 and the 8K is at 11.3963 - at 16K context and 8 scaling


          very interesting that the 16K model seems to be somewhat "better" at 2K
          context, and also proof that the 16K training works with the lower ppl at
          higher scaling'
        updatedAt: '2023-06-23T17:42:16.870Z'
      numEdits: 1
      reactions: []
    id: 6495d7c35e85f2810600a040
    type: comment
  author: flashvenom
  content: 'I have some results for you:


    16K merged into model has ppl of 7.3050, 8K merged into a model has ppl of 7.5387
    (using 2k context)

    16K is at 7.7976 and the 8K is at 7.7789 - at 8k context, scaling at 4

    16K is at 9.4433 and the 8K is at 11.3963 - at 16K context and 8 scaling


    very interesting that the 16K model seems to be somewhat "better" at 2K context,
    and also proof that the 16K training works with the lower ppl at higher scaling'
  created_at: 2023-06-23 16:34:59+00:00
  edited: true
  hidden: false
  id: 6495d7c35e85f2810600a040
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
      fullname: FlashVenom
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flashvenom
      type: user
    createdAt: '2023-06-23T17:36:04.000Z'
    data:
      edited: false
      editors:
      - flashvenom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8945313692092896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/637c621facc078d5bec14073/MOKvlABZuesOL3rVmxalE.png?w=200&h=200&f=face
          fullname: FlashVenom
          isHf: false
          isPro: false
          name: flashvenom
          type: user
        html: '<p>This sounds silly, but can we train a 32K/64K model? I wonder if
          this will trend will continue for some reason -- we do need to test recall
          at larger context lengths too but with this pattern at 64K SuperHOT running
          at 8K context will probably be better than a 8K SuperHOT</p>

          '
        raw: This sounds silly, but can we train a 32K/64K model? I wonder if this
          will trend will continue for some reason -- we do need to test recall at
          larger context lengths too but with this pattern at 64K SuperHOT running
          at 8K context will probably be better than a 8K SuperHOT
        updatedAt: '2023-06-23T17:36:04.456Z'
      numEdits: 0
      reactions: []
    id: 6495d804767961ad096aca5a
    type: comment
  author: flashvenom
  content: This sounds silly, but can we train a 32K/64K model? I wonder if this will
    trend will continue for some reason -- we do need to test recall at larger context
    lengths too but with this pattern at 64K SuperHOT running at 8K context will probably
    be better than a 8K SuperHOT
  created_at: 2023-06-23 16:36:04+00:00
  edited: false
  hidden: false
  id: 6495d804767961ad096aca5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14366c6dcc941238852438e6ffa8bf4e.svg
      fullname: nxnhjrjtbjfzhrovwl@nthrl.com
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nxnhjrjtbjfzhrovwl
      type: user
    createdAt: '2023-06-23T21:51:09.000Z'
    data:
      edited: true
      editors:
      - nxnhjrjtbjfzhrovwl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9801415801048279
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14366c6dcc941238852438e6ffa8bf4e.svg
          fullname: nxnhjrjtbjfzhrovwl@nthrl.com
          isHf: false
          isPro: false
          name: nxnhjrjtbjfzhrovwl
          type: user
        html: '<blockquote>

          <p>I have some results for you:</p>

          <p>16K merged into model has ppl of 7.3050, 8K merged into a model has ppl
          of 7.5387 (using 2k context)<br>16K is at 7.7976 and the 8K is at 7.7789
          - at 8k context, scaling at 4<br>16K is at 9.4433 and the 8K is at 11.3963
          - at 16K context and 8 scaling</p>

          <p>very interesting that the 16K model seems to be somewhat "better" at
          2K context, and also proof that the 16K training works with the lower ppl
          at higher scaling</p>

          </blockquote>

          <p>this difference might just be because 16K was trained with lora rank
          4 and 8K was trained with lora rank 2</p>

          '
        raw: "> I have some results for you:\n> \n> 16K merged into model has ppl\
          \ of 7.3050, 8K merged into a model has ppl of 7.5387 (using 2k context)\n\
          > 16K is at 7.7976 and the 8K is at 7.7789 - at 8k context, scaling at 4\n\
          > 16K is at 9.4433 and the 8K is at 11.3963 - at 16K context and 8 scaling\n\
          > \n> very interesting that the 16K model seems to be somewhat \"better\"\
          \ at 2K context, and also proof that the 16K training works with the lower\
          \ ppl at higher scaling\n\nthis difference might just be because 16K was\
          \ trained with lora rank 4 and 8K was trained with lora rank 2"
        updatedAt: '2023-06-23T21:55:03.013Z'
      numEdits: 2
      reactions: []
    id: 649613cdf590e76c938833ce
    type: comment
  author: nxnhjrjtbjfzhrovwl
  content: "> I have some results for you:\n> \n> 16K merged into model has ppl of\
    \ 7.3050, 8K merged into a model has ppl of 7.5387 (using 2k context)\n> 16K is\
    \ at 7.7976 and the 8K is at 7.7789 - at 8k context, scaling at 4\n> 16K is at\
    \ 9.4433 and the 8K is at 11.3963 - at 16K context and 8 scaling\n> \n> very interesting\
    \ that the 16K model seems to be somewhat \"better\" at 2K context, and also proof\
    \ that the 16K training works with the lower ppl at higher scaling\n\nthis difference\
    \ might just be because 16K was trained with lora rank 4 and 8K was trained with\
    \ lora rank 2"
  created_at: 2023-06-23 20:51:09+00:00
  edited: true
  hidden: false
  id: 649613cdf590e76c938833ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-06-23T22:06:35.000Z'
    data:
      edited: true
      editors:
      - kaiokendev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.983558177947998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;flashvenom&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/flashvenom\">@<span class=\"\
          underline\">flashvenom</span></a></span>\n\n\t</span></span> I would expect\
          \ the ppl to be lower at 16K for the one trained with 16K since it learns\
          \ the proper dilated frequency. Still surprising it has lower ppl on the\
          \ short range as well</p>\n<blockquote>\n<p>can we train a 32K/64K model?</p>\n\
          </blockquote>\n<p>This model was a test just to see if my idea was correct\
          \ that even with 4K data, it should work even if you go to 16K. It seems\
          \ to work, so I would encourage others to try going even higher to find\
          \ the limit now that you don't need 16K data to train to 16K context. I\
          \ want to also investigate some other architectural changes we could make.</p>\n"
        raw: '@flashvenom I would expect the ppl to be lower at 16K for the one trained
          with 16K since it learns the proper dilated frequency. Still surprising
          it has lower ppl on the short range as well

          >can we train a 32K/64K model?


          This model was a test just to see if my idea was correct that even with
          4K data, it should work even if you go to 16K. It seems to work, so I would
          encourage others to try going even higher to find the limit now that you
          don''t need 16K data to train to 16K context. I want to also investigate
          some other architectural changes we could make.'
        updatedAt: '2023-06-23T22:07:01.178Z'
      numEdits: 1
      reactions: []
    id: 6496176bffedd4836d587a29
    type: comment
  author: kaiokendev
  content: '@flashvenom I would expect the ppl to be lower at 16K for the one trained
    with 16K since it learns the proper dilated frequency. Still surprising it has
    lower ppl on the short range as well

    >can we train a 32K/64K model?


    This model was a test just to see if my idea was correct that even with 4K data,
    it should work even if you go to 16K. It seems to work, so I would encourage others
    to try going even higher to find the limit now that you don''t need 16K data to
    train to 16K context. I want to also investigate some other architectural changes
    we could make.'
  created_at: 2023-06-23 21:06:35+00:00
  edited: true
  hidden: false
  id: 6496176bffedd4836d587a29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e42bdaa0cbfc34d7daf5eb318dfc7f8.svg
      fullname: Pierre-Louis Braun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alkeryn
      type: user
    createdAt: '2023-06-24T09:16:22.000Z'
    data:
      edited: false
      editors:
      - alkeryn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9741933941841125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e42bdaa0cbfc34d7daf5eb318dfc7f8.svg
          fullname: Pierre-Louis Braun
          isHf: false
          isPro: false
          name: alkeryn
          type: user
        html: '<p>training 32K/64K models might be worthwile for experiment sake,
          but not a lot of people have enough vram to do that.<br>this is a great
          breakthrough but we may want to look at new ways to increase context without
          increasing vram requirements as much, still, that''s amazing we could increase
          context that much.</p>

          '
        raw: 'training 32K/64K models might be worthwile for experiment sake, but
          not a lot of people have enough vram to do that.

          this is a great breakthrough but we may want to look at new ways to increase
          context without increasing vram requirements as much, still, that''s amazing
          we could increase context that much.'
        updatedAt: '2023-06-24T09:16:22.041Z'
      numEdits: 0
      reactions: []
    id: 6496b466b41e3c82ac306d19
    type: comment
  author: alkeryn
  content: 'training 32K/64K models might be worthwile for experiment sake, but not
    a lot of people have enough vram to do that.

    this is a great breakthrough but we may want to look at new ways to increase context
    without increasing vram requirements as much, still, that''s amazing we could
    increase context that much.'
  created_at: 2023-06-24 08:16:22+00:00
  edited: false
  hidden: false
  id: 6496b466b41e3c82ac306d19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-06-24T09:21:36.000Z'
    data:
      edited: true
      editors:
      - kaiokendev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416335821151733
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;alkeryn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/alkeryn\">@<span class=\"\
          underline\">alkeryn</span></a></span>\n\n\t</span></span> This method is\
          \ solely an augmentation to positional encodings. It only allows the context\
          \ of the pre-trained model to be increased without using much training data\
          \ or compute. The issue of quadratic attention is orthogonal to this method\
          \ (e.g. fast attention method will likely not have anything to do with position\
          \ encoding) Besides, mechanism such as xformers and flash attention also\
          \ exist, not to mention the recent vLLM, which can all work alongside since\
          \ the issue of attention and KV cache are solely outside the domain of position\
          \ information.</p>\n<p>Although I am currently working on a method to alleviate\
          \ the memory issue.</p>\n<p>EDIT: To clarify further, extending context\
          \ here is tackled as a position encoding problem. Using that context is\
          \ a separate issue.</p>\n"
        raw: '@alkeryn This method is solely an augmentation to positional encodings.
          It only allows the context of the pre-trained model to be increased without
          using much training data or compute. The issue of quadratic attention is
          orthogonal to this method (e.g. fast attention method will likely not have
          anything to do with position encoding) Besides, mechanism such as xformers
          and flash attention also exist, not to mention the recent vLLM, which can
          all work alongside since the issue of attention and KV cache are solely
          outside the domain of position information.


          Although I am currently working on a method to alleviate the memory issue.


          EDIT: To clarify further, extending context here is tackled as a position
          encoding problem. Using that context is a separate issue.'
        updatedAt: '2023-06-24T09:23:36.774Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - roffmonster
    id: 6496b5a0b41e3c82ac30899c
    type: comment
  author: kaiokendev
  content: '@alkeryn This method is solely an augmentation to positional encodings.
    It only allows the context of the pre-trained model to be increased without using
    much training data or compute. The issue of quadratic attention is orthogonal
    to this method (e.g. fast attention method will likely not have anything to do
    with position encoding) Besides, mechanism such as xformers and flash attention
    also exist, not to mention the recent vLLM, which can all work alongside since
    the issue of attention and KV cache are solely outside the domain of position
    information.


    Although I am currently working on a method to alleviate the memory issue.


    EDIT: To clarify further, extending context here is tackled as a position encoding
    problem. Using that context is a separate issue.'
  created_at: 2023-06-24 08:21:36+00:00
  edited: true
  hidden: false
  id: 6496b5a0b41e3c82ac30899c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: kaiokendev/superhot-13b-16k-no-rlhf-test
repo_type: model
status: open
target_branch: null
title: Difference between this and 8k version?
