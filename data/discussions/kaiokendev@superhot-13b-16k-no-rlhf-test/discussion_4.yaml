!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-07-05 06:11:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-07-05T07:11:23.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9077591896057129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>This technique sounds simply "too easy" to get such a good result.
          I bet LLM leaders like Anthropic/OpenAI has already discovered it, or not.  </p>

          <p>Just like how the Stable Diffusion community has discovered the "offset
          noise" technique that works on input noise distribution so that the model
          can extend to HDR images on which the model is not pretrained on.  The Stable
          Diffusion community has speculated that Midjounry v4 has already using this
          technique to achieve HDR-looking images. </p>

          <p>But, there is <a rel="nofollow" href="https://arxiv.org/pdf/2305.08891.pdf">paper</a>
          that points out that "offset noise" does not address the fundamental distribution
          truth of HDR images, it''s just a easy trick.</p>

          <p>I don''t think sampling RoPE actually extends the capacity of the model
          in terms of PPL</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/JV_mhfh6FVrdyk7Y10XAb.png"><img
          alt="msedge_wZ2oxm8gU4.png" src="https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/JV_mhfh6FVrdyk7Y10XAb.png"></a></p>

          <p>As the 8K model performs worse than the base model at 2048 tokens, and
          the 8k model simply reach about the same ppl at 8192 tokens as the base
          model would at 2048 tokens (which the 8k model is pretrained on)</p>

          <p>Nonetheless, some prompting techniques like CoT and ReAct that make the
          model deliberately spit out more tokens seem to increase the model''s N-Shot
          abilities across many tasks. I think we need Humman feedbacks to judge the
          actual benefits of RoPE sampling .</p>

          '
        raw: "This technique sounds simply \"too easy\" to get such a good result.\
          \ I bet LLM leaders like Anthropic/OpenAI has already discovered it, or\
          \ not.  \r\n\r\nJust like how the Stable Diffusion community has discovered\
          \ the \"offset noise\" technique that works on input noise distribution\
          \ so that the model can extend to HDR images on which the model is not pretrained\
          \ on.  The Stable Diffusion community has speculated that Midjounry v4 has\
          \ already using this technique to achieve HDR-looking images. \r\n\r\nBut,\
          \ there is [paper](https://arxiv.org/pdf/2305.08891.pdf) that points out\
          \ that \"offset noise\" does not address the fundamental distribution truth\
          \ of HDR images, it's just a easy trick.\r\n\r\nI don't think sampling RoPE\
          \ actually extends the capacity of the model in terms of PPL\r\n\r\n![msedge_wZ2oxm8gU4.png](https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/JV_mhfh6FVrdyk7Y10XAb.png)\r\
          \n\r\nAs the 8K model performs worse than the base model at 2048 tokens,\
          \ and the 8k model simply reach about the same ppl at 8192 tokens as the\
          \ base model would at 2048 tokens (which the 8k model is pretrained on)\r\
          \n\r\nNonetheless, some prompting techniques like CoT and ReAct that make\
          \ the model deliberately spit out more tokens seem to increase the model's\
          \ N-Shot abilities across many tasks. I think we need Humman feedbacks to\
          \ judge the actual benefits of RoPE sampling ."
        updatedAt: '2023-07-05T07:11:23.082Z'
      numEdits: 0
      reactions: []
    id: 64a5179b13645758bbc3f60d
    type: comment
  author: Yhyu13
  content: "This technique sounds simply \"too easy\" to get such a good result. I\
    \ bet LLM leaders like Anthropic/OpenAI has already discovered it, or not.  \r\
    \n\r\nJust like how the Stable Diffusion community has discovered the \"offset\
    \ noise\" technique that works on input noise distribution so that the model can\
    \ extend to HDR images on which the model is not pretrained on.  The Stable Diffusion\
    \ community has speculated that Midjounry v4 has already using this technique\
    \ to achieve HDR-looking images. \r\n\r\nBut, there is [paper](https://arxiv.org/pdf/2305.08891.pdf)\
    \ that points out that \"offset noise\" does not address the fundamental distribution\
    \ truth of HDR images, it's just a easy trick.\r\n\r\nI don't think sampling RoPE\
    \ actually extends the capacity of the model in terms of PPL\r\n\r\n![msedge_wZ2oxm8gU4.png](https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/JV_mhfh6FVrdyk7Y10XAb.png)\r\
    \n\r\nAs the 8K model performs worse than the base model at 2048 tokens, and the\
    \ 8k model simply reach about the same ppl at 8192 tokens as the base model would\
    \ at 2048 tokens (which the 8k model is pretrained on)\r\n\r\nNonetheless, some\
    \ prompting techniques like CoT and ReAct that make the model deliberately spit\
    \ out more tokens seem to increase the model's N-Shot abilities across many tasks.\
    \ I think we need Humman feedbacks to judge the actual benefits of RoPE sampling\
    \ ."
  created_at: 2023-07-05 06:11:23+00:00
  edited: false
  hidden: false
  id: 64a5179b13645758bbc3f60d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
      fullname: Kaio Ken
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: kaiokendev
      type: user
    createdAt: '2023-07-05T09:19:36.000Z'
    data:
      edited: true
      editors:
      - kaiokendev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9188430905342102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e476fce12c4d48bb1f0ac3e68ddc209.svg
          fullname: Kaio Ken
          isHf: false
          isPro: false
          name: kaiokendev
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> Hello, I think\
          \ your confusion is that you are relying on that chart to give you the answer,\
          \ I encourage you to finetune a model and run perplexity test on your own\
          \ model, rather than use the chart. This chart is a very specific case and\
          \ does not apply, it is only meant to show that the perplexity is decreasing\
          \ as sequence length increases.</p>\n<p>For example, refer to the next chart\
          \ on the page :<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/643c985f25681c3afab03789/uqu_kyQEojIYCA7EnD5MQ.png\"\
          ><img alt=\"ppl_mine.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/643c985f25681c3afab03789/uqu_kyQEojIYCA7EnD5MQ.png\"\
          ></a></p>\n<p>Here you can see the technique works and does not perform\
          \ any worse than the base model. For more in-depth benchmarking, refer to\
          \ Meta AI's paper: <a rel=\"nofollow\" href=\"https://arxiv.org/pdf/2306.15595.pdf\"\
          >https://arxiv.org/pdf/2306.15595.pdf</a><br>And as always you should perform\
          \ the test on your own to verify the results, not just consider any charts\
          \ or tables that others posted.</p>\n<blockquote>\n<p>As the 8K model performs\
          \ worse than the base model at 2048 tokens, and the 8k model simply reach\
          \ about the same ppl at 8192 tokens as the base model would at 2048 tokens\
          \ (which the 8k model is pretrained on)</p>\n</blockquote>\n<p>That is because\
          \ the scaling factor used there is for the purpose of extrapolation. To\
          \ get results on-par with the base model you need to use a factor of 0.5,\
          \ 0.25 is used because there is minimal perplexity loss when extrapolating,\
          \ but when the proper factor is used, there is no loss.</p>\n<p>The formula\
          \ is <code>pretrained length / cutoff length</code>, in this case it is\
          \ <code>2048 / 4096</code> as 4096 was the cutoff used for my models. If\
          \ you train with cutoff of 8192, then you would use <code>2048 / 8192</code>\
          \ and you will be able to use 8192 sequence length with no perplexity loss,\
          \ however you can go to 16384 with minimal perplexity loss. Does that make\
          \ sense?</p>\n"
        raw: '@Yhyu13 Hello, I think your confusion is that you are relying on that
          chart to give you the answer, I encourage you to finetune a model and run
          perplexity test on your own model, rather than use the chart. This chart
          is a very specific case and does not apply, it is only meant to show that
          the perplexity is decreasing as sequence length increases.


          For example, refer to the next chart on the page :

          ![ppl_mine.png](https://cdn-uploads.huggingface.co/production/uploads/643c985f25681c3afab03789/uqu_kyQEojIYCA7EnD5MQ.png)


          Here you can see the technique works and does not perform any worse than
          the base model. For more in-depth benchmarking, refer to Meta AI''s paper:
          https://arxiv.org/pdf/2306.15595.pdf

          And as always you should perform the test on your own to verify the results,
          not just consider any charts or tables that others posted.


          > As the 8K model performs worse than the base model at 2048 tokens, and
          the 8k model simply reach about the same ppl at 8192 tokens as the base
          model would at 2048 tokens (which the 8k model is pretrained on)


          That is because the scaling factor used there is for the purpose of extrapolation.
          To get results on-par with the base model you need to use a factor of 0.5,
          0.25 is used because there is minimal perplexity loss when extrapolating,
          but when the proper factor is used, there is no loss.


          The formula is `pretrained length / cutoff length`, in this case it is `2048
          / 4096` as 4096 was the cutoff used for my models. If you train with cutoff
          of 8192, then you would use `2048 / 8192` and you will be able to use 8192
          sequence length with no perplexity loss, however you can go to 16384 with
          minimal perplexity loss. Does that make sense?

          '
        updatedAt: '2023-07-05T09:23:21.267Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Yhyu13
        - roffmonster
    id: 64a535a88d6a9ca65a5b6a7e
    type: comment
  author: kaiokendev
  content: '@Yhyu13 Hello, I think your confusion is that you are relying on that
    chart to give you the answer, I encourage you to finetune a model and run perplexity
    test on your own model, rather than use the chart. This chart is a very specific
    case and does not apply, it is only meant to show that the perplexity is decreasing
    as sequence length increases.


    For example, refer to the next chart on the page :

    ![ppl_mine.png](https://cdn-uploads.huggingface.co/production/uploads/643c985f25681c3afab03789/uqu_kyQEojIYCA7EnD5MQ.png)


    Here you can see the technique works and does not perform any worse than the base
    model. For more in-depth benchmarking, refer to Meta AI''s paper: https://arxiv.org/pdf/2306.15595.pdf

    And as always you should perform the test on your own to verify the results, not
    just consider any charts or tables that others posted.


    > As the 8K model performs worse than the base model at 2048 tokens, and the 8k
    model simply reach about the same ppl at 8192 tokens as the base model would at
    2048 tokens (which the 8k model is pretrained on)


    That is because the scaling factor used there is for the purpose of extrapolation.
    To get results on-par with the base model you need to use a factor of 0.5, 0.25
    is used because there is minimal perplexity loss when extrapolating, but when
    the proper factor is used, there is no loss.


    The formula is `pretrained length / cutoff length`, in this case it is `2048 /
    4096` as 4096 was the cutoff used for my models. If you train with cutoff of 8192,
    then you would use `2048 / 8192` and you will be able to use 8192 sequence length
    with no perplexity loss, however you can go to 16384 with minimal perplexity loss.
    Does that make sense?

    '
  created_at: 2023-07-05 08:19:36+00:00
  edited: true
  hidden: false
  id: 64a535a88d6a9ca65a5b6a7e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: kaiokendev/superhot-13b-16k-no-rlhf-test
repo_type: model
status: open
target_branch: null
title: Possibility that Claude/ChatGPT uses similar techniques on adjusting RoPE sampling
  rate?
