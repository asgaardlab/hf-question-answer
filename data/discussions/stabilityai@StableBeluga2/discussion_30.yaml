!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sociopathic-hamster
conflicting_files: null
created_at: 2023-08-05 23:12:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
      fullname: Sam Rahimi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sociopathic-hamster
      type: user
    createdAt: '2023-08-06T00:12:21.000Z'
    data:
      edited: false
      editors:
      - sociopathic-hamster
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6052232384681702
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
          fullname: Sam Rahimi
          isHf: false
          isPro: false
          name: sociopathic-hamster
          type: user
        html: '<p>Will this run on 4 x 3090 (96GB total VRAM)? </p>

          '
        raw: 'Will this run on 4 x 3090 (96GB total VRAM)? '
        updatedAt: '2023-08-06T00:12:21.225Z'
      numEdits: 0
      reactions: []
    id: 64cee5659617774ce4d5ede3
    type: comment
  author: sociopathic-hamster
  content: 'Will this run on 4 x 3090 (96GB total VRAM)? '
  created_at: 2023-08-05 23:12:21+00:00
  edited: false
  hidden: false
  id: 64cee5659617774ce4d5ede3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
      fullname: Sam Rahimi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sociopathic-hamster
      type: user
    createdAt: '2023-08-06T00:14:07.000Z'
    data:
      edited: false
      editors:
      - sociopathic-hamster
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7261403203010559
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
          fullname: Sam Rahimi
          isHf: false
          isPro: false
          name: sociopathic-hamster
          type: user
        html: '<p>(and can it be ROPE scaled?)</p>

          '
        raw: (and can it be ROPE scaled?)
        updatedAt: '2023-08-06T00:14:07.502Z'
      numEdits: 0
      reactions: []
    id: 64cee5cf9c245c6ba71fadac
    type: comment
  author: sociopathic-hamster
  content: (and can it be ROPE scaled?)
  created_at: 2023-08-05 23:14:07+00:00
  edited: false
  hidden: false
  id: 64cee5cf9c245c6ba71fadac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-08-08T14:44:38.000Z'
    data:
      edited: true
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5542730093002319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: '<blockquote>

          <p>Will this run on 4 x 3090 (96GB total VRAM)?</p>

          </blockquote>

          <p>yes, it will run on 96gbs of vram</p>

          '
        raw: '> Will this run on 4 x 3090 (96GB total VRAM)?


          yes, it will run on 96gbs of vram

          '
        updatedAt: '2023-08-08T14:44:59.343Z'
      numEdits: 1
      reactions: []
    id: 64d254d6a3ff18cfd4fa1885
    type: comment
  author: breadlicker45
  content: '> Will this run on 4 x 3090 (96GB total VRAM)?


    yes, it will run on 96gbs of vram

    '
  created_at: 2023-08-08 13:44:38+00:00
  edited: true
  hidden: false
  id: 64d254d6a3ff18cfd4fa1885
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
      fullname: Sam Rahimi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sociopathic-hamster
      type: user
    createdAt: '2023-08-09T19:35:29.000Z'
    data:
      edited: false
      editors:
      - sociopathic-hamster
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.933961808681488
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
          fullname: Sam Rahimi
          isHf: false
          isPro: false
          name: sociopathic-hamster
          type: user
        html: '<p>Amazing, thanks! I wonder what the performance will be like... I
          mean, quantized versions of these SOTA 70b releases are an amazing advance
          and totally usable for many downstream applications, but subjectively the
          inference quality is noticably degraded. Compared to the original models,
          some of which subjectively "feel" equal or superior to ChatGPT (chat versions)
          or text-davinci-003 (llama 2 70b base model), at least for workloads that
          do not involve heavy code generation. </p>

          <p>If it really is possible to run Stable Beluga 2 and other SOTA finetunes
          of llama-2-70b on consumer-grade hardware like this, the path to a GPT-4
          killer becomes quite straightforward: </p>

          <p>Step 1: deploy a number of 70b models in parallel, some generalists like
          SB2, some other models finetuned for various workloads, and a bunch of RWKV
          finetunes for certain tasks (RWKV handles things like creative writing shockingly
          well and fits in 16gb of vram)<br>Step 2: put them behind a fast 13B model,
          fine-tuned for high performance classification tasks. A super-classifier
          if you will, which delegates requests to the most appropriate large model(s),
          assembles the result, and presents it to the user. Since any of the 70b
          generalist models currently trending are ALREADY on par with GPT 3.5 / ChatGPT,
          even a naive implementation of this architecture where no significant finetuning
          or training is done beyond prompt engineering, and that simply directs the
          request based on the gross strengths of the various models (e.g. code-writing
          vs mathematical reasoning vs multistep COT etc) - will be unarguably superior
          to GPT 3.5, and nipping at the heels of GPT4.<br>Step 2a: Add SOTA SDXL
          and various CV models to the system, making it fully multimodal like GPT-4
          supposedly is (but they censored it)<br>Step 3: finetune the 70b models
          so they know how and when to ask for help from their peers, creating a mesh
          topology that MAY not be all that different than from GPT-4''s current architecture
          (we know its made from multiple models, we don''t know what their individual
          characteristics are or how they work together). For bonus points, integrate
          a web scraper completely behind the scenes, that the models call upon when
          they need more context, just like a human would do when they google something...
          and then save the scraped data in a vector database that serves as long
          term memory shared by ALL the models, so that next time they don''t have
          to go to the web for answers (and periodically we can use the data stored
          in that retrieval system for finetuning the models themselves)</p>

          <p>At that point, presuming that the models are capable of admitting they
          don''t know the answer, or we create a specialist model for predicting whether
          an answer from another model is hallucinated, we now have a multi-tiered
          knowledge engine.</p>

          <p>do i know the answer?<br>does my colleague know the answer?<br>none of
          us know the answer, do we have anything in our knowledge base to guide us
          to the answer?<br>we don''t? scraper, please google that for me and return
          cleaned up text to provide me with some context<br>OK now we know the answer,
          please save this context to the knowledge base</p>

          <p>Steps 1 and 2 can be done by a normal (reasonably good) full-stack engineer
          who doesn''t mind spending $500 for cheap GPU compute on vast.ai to prove
          the concept and has familiarity with LLMs...<br>Step 3 should be doable
          in a few months by a team of 5 guys, including at least 1 senior ML engineer,
          server-wise you''re looking at an 8xA100 rig for inference, another for
          finetuning, and a bunch of smaller / cheaper instances for various ancillary
          models and classical computing. Probably $30k to surpass GPT-4 on reproducable
          benchmarks (not including salary for the 5 guys lol)</p>

          <p>BTW i fit into the "reasonably good full-stack engineer" category...
          I''m building cool apps that use LLMs, multimodal pipelines, etc... but
          I don''t have the academic background in neural network design, etc. So
          I might be missing something here :)</p>

          '
        raw: "Amazing, thanks! I wonder what the performance will be like... I mean,\
          \ quantized versions of these SOTA 70b releases are an amazing advance and\
          \ totally usable for many downstream applications, but subjectively the\
          \ inference quality is noticably degraded. Compared to the original models,\
          \ some of which subjectively \"feel\" equal or superior to ChatGPT (chat\
          \ versions) or text-davinci-003 (llama 2 70b base model), at least for workloads\
          \ that do not involve heavy code generation. \n\nIf it really is possible\
          \ to run Stable Beluga 2 and other SOTA finetunes of llama-2-70b on consumer-grade\
          \ hardware like this, the path to a GPT-4 killer becomes quite straightforward:\
          \ \n\nStep 1: deploy a number of 70b models in parallel, some generalists\
          \ like SB2, some other models finetuned for various workloads, and a bunch\
          \ of RWKV finetunes for certain tasks (RWKV handles things like creative\
          \ writing shockingly well and fits in 16gb of vram)\nStep 2: put them behind\
          \ a fast 13B model, fine-tuned for high performance classification tasks.\
          \ A super-classifier if you will, which delegates requests to the most appropriate\
          \ large model(s), assembles the result, and presents it to the user. Since\
          \ any of the 70b generalist models currently trending are ALREADY on par\
          \ with GPT 3.5 / ChatGPT, even a naive implementation of this architecture\
          \ where no significant finetuning or training is done beyond prompt engineering,\
          \ and that simply directs the request based on the gross strengths of the\
          \ various models (e.g. code-writing vs mathematical reasoning vs multistep\
          \ COT etc) - will be unarguably superior to GPT 3.5, and nipping at the\
          \ heels of GPT4.\nStep 2a: Add SOTA SDXL and various CV models to the system,\
          \ making it fully multimodal like GPT-4 supposedly is (but they censored\
          \ it)\nStep 3: finetune the 70b models so they know how and when to ask\
          \ for help from their peers, creating a mesh topology that MAY not be all\
          \ that different than from GPT-4's current architecture (we know its made\
          \ from multiple models, we don't know what their individual characteristics\
          \ are or how they work together). For bonus points, integrate a web scraper\
          \ completely behind the scenes, that the models call upon when they need\
          \ more context, just like a human would do when they google something...\
          \ and then save the scraped data in a vector database that serves as long\
          \ term memory shared by ALL the models, so that next time they don't have\
          \ to go to the web for answers (and periodically we can use the data stored\
          \ in that retrieval system for finetuning the models themselves)\n\nAt that\
          \ point, presuming that the models are capable of admitting they don't know\
          \ the answer, or we create a specialist model for predicting whether an\
          \ answer from another model is hallucinated, we now have a multi-tiered\
          \ knowledge engine.\n\ndo i know the answer? \ndoes my colleague know the\
          \ answer?\nnone of us know the answer, do we have anything in our knowledge\
          \ base to guide us to the answer?\nwe don't? scraper, please google that\
          \ for me and return cleaned up text to provide me with some context\nOK\
          \ now we know the answer, please save this context to the knowledge base\n\
          \nSteps 1 and 2 can be done by a normal (reasonably good) full-stack engineer\
          \ who doesn't mind spending $500 for cheap GPU compute on vast.ai to prove\
          \ the concept and has familiarity with LLMs... \nStep 3 should be doable\
          \ in a few months by a team of 5 guys, including at least 1 senior ML engineer,\
          \ server-wise you're looking at an 8xA100 rig for inference, another for\
          \ finetuning, and a bunch of smaller / cheaper instances for various ancillary\
          \ models and classical computing. Probably $30k to surpass GPT-4 on reproducable\
          \ benchmarks (not including salary for the 5 guys lol)\n\nBTW i fit into\
          \ the \"reasonably good full-stack engineer\" category... I'm building cool\
          \ apps that use LLMs, multimodal pipelines, etc... but I don't have the\
          \ academic background in neural network design, etc. So I might be missing\
          \ something here :)"
        updatedAt: '2023-08-09T19:35:29.618Z'
      numEdits: 0
      reactions: []
    id: 64d3ea810553a2522f21050f
    type: comment
  author: sociopathic-hamster
  content: "Amazing, thanks! I wonder what the performance will be like... I mean,\
    \ quantized versions of these SOTA 70b releases are an amazing advance and totally\
    \ usable for many downstream applications, but subjectively the inference quality\
    \ is noticably degraded. Compared to the original models, some of which subjectively\
    \ \"feel\" equal or superior to ChatGPT (chat versions) or text-davinci-003 (llama\
    \ 2 70b base model), at least for workloads that do not involve heavy code generation.\
    \ \n\nIf it really is possible to run Stable Beluga 2 and other SOTA finetunes\
    \ of llama-2-70b on consumer-grade hardware like this, the path to a GPT-4 killer\
    \ becomes quite straightforward: \n\nStep 1: deploy a number of 70b models in\
    \ parallel, some generalists like SB2, some other models finetuned for various\
    \ workloads, and a bunch of RWKV finetunes for certain tasks (RWKV handles things\
    \ like creative writing shockingly well and fits in 16gb of vram)\nStep 2: put\
    \ them behind a fast 13B model, fine-tuned for high performance classification\
    \ tasks. A super-classifier if you will, which delegates requests to the most\
    \ appropriate large model(s), assembles the result, and presents it to the user.\
    \ Since any of the 70b generalist models currently trending are ALREADY on par\
    \ with GPT 3.5 / ChatGPT, even a naive implementation of this architecture where\
    \ no significant finetuning or training is done beyond prompt engineering, and\
    \ that simply directs the request based on the gross strengths of the various\
    \ models (e.g. code-writing vs mathematical reasoning vs multistep COT etc) -\
    \ will be unarguably superior to GPT 3.5, and nipping at the heels of GPT4.\n\
    Step 2a: Add SOTA SDXL and various CV models to the system, making it fully multimodal\
    \ like GPT-4 supposedly is (but they censored it)\nStep 3: finetune the 70b models\
    \ so they know how and when to ask for help from their peers, creating a mesh\
    \ topology that MAY not be all that different than from GPT-4's current architecture\
    \ (we know its made from multiple models, we don't know what their individual\
    \ characteristics are or how they work together). For bonus points, integrate\
    \ a web scraper completely behind the scenes, that the models call upon when they\
    \ need more context, just like a human would do when they google something...\
    \ and then save the scraped data in a vector database that serves as long term\
    \ memory shared by ALL the models, so that next time they don't have to go to\
    \ the web for answers (and periodically we can use the data stored in that retrieval\
    \ system for finetuning the models themselves)\n\nAt that point, presuming that\
    \ the models are capable of admitting they don't know the answer, or we create\
    \ a specialist model for predicting whether an answer from another model is hallucinated,\
    \ we now have a multi-tiered knowledge engine.\n\ndo i know the answer? \ndoes\
    \ my colleague know the answer?\nnone of us know the answer, do we have anything\
    \ in our knowledge base to guide us to the answer?\nwe don't? scraper, please\
    \ google that for me and return cleaned up text to provide me with some context\n\
    OK now we know the answer, please save this context to the knowledge base\n\n\
    Steps 1 and 2 can be done by a normal (reasonably good) full-stack engineer who\
    \ doesn't mind spending $500 for cheap GPU compute on vast.ai to prove the concept\
    \ and has familiarity with LLMs... \nStep 3 should be doable in a few months by\
    \ a team of 5 guys, including at least 1 senior ML engineer, server-wise you're\
    \ looking at an 8xA100 rig for inference, another for finetuning, and a bunch\
    \ of smaller / cheaper instances for various ancillary models and classical computing.\
    \ Probably $30k to surpass GPT-4 on reproducable benchmarks (not including salary\
    \ for the 5 guys lol)\n\nBTW i fit into the \"reasonably good full-stack engineer\"\
    \ category... I'm building cool apps that use LLMs, multimodal pipelines, etc...\
    \ but I don't have the academic background in neural network design, etc. So I\
    \ might be missing something here :)"
  created_at: 2023-08-09 18:35:29+00:00
  edited: false
  hidden: false
  id: 64d3ea810553a2522f21050f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
      fullname: bread null
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: breadlicker45
      type: user
    createdAt: '2023-08-11T12:55:15.000Z'
    data:
      edited: true
      editors:
      - breadlicker45
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9308647513389587
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d86ca9ce3242be87bbfe541219ee2fac.svg
          fullname: bread null
          isHf: false
          isPro: false
          name: breadlicker45
          type: user
        html: '<blockquote>

          <p>Amazing, thanks! I wonder what the performance will be like... I mean,
          quantized versions of these SOTA 70b releases are an amazing advance and
          totally usable for many downstream applications, but subjectively the inference
          quality is noticably degraded. Compared to the original models, some of
          which subjectively "feel" equal or superior to ChatGPT (chat versions) or
          text-davinci-003 (llama 2 70b base model), at least for workloads that do
          not involve heavy code generation. </p>

          <p>If it really is possible to run Stable Beluga 2 and other SOTA finetunes
          of llama-2-70b on consumer-grade hardware like this, the path to a GPT-4
          killer becomes quite straightforward: </p>

          <p>Step 1: deploy a number of 70b models in parallel, some generalists like
          SB2, some other models finetuned for various workloads, and a bunch of RWKV
          finetunes for certain tasks (RWKV handles things like creative writing shockingly
          well and fits in 16gb of vram)<br>Step 2: put them behind a fast 13B model,
          fine-tuned for high performance classification tasks. A super-classifier
          if you will, which delegates requests to the most appropriate large model(s),
          assembles the result, and presents it to the user. Since any of the 70b
          generalist models currently trending are ALREADY on par with GPT 3.5 / ChatGPT,
          even a naive implementation of this architecture where no significant finetuning
          or training is done beyond prompt engineering, and that simply directs the
          request based on the gross strengths of the various models (e.g. code-writing
          vs mathematical reasoning vs multistep COT etc) - will be unarguably superior
          to GPT 3.5, and nipping at the heels of GPT4.<br>Step 2a: Add SOTA SDXL
          and various CV models to the system, making it fully multimodal like GPT-4
          supposedly is (but they censored it)<br>Step 3: finetune the 70b models
          so they know how and when to ask for help from their peers, creating a mesh
          topology that MAY not be all that different than from GPT-4''s current architecture
          (we know its made from multiple models, we don''t know what their individual
          characteristics are or how they work together). For bonus points, integrate
          a web scraper completely behind the scenes, that the models call upon when
          they need more context, just like a human would do when they google something...
          and then save the scraped data in a vector database that serves as long
          term memory shared by ALL the models, so that next time they don''t have
          to go to the web for answers (and periodically we can use the data stored
          in that retrieval system for finetuning the models themselves)</p>

          <p>At that point, presuming that the models are capable of admitting they
          don''t know the answer, or we create a specialist model for predicting whether
          an answer from another model is hallucinated, we now have a multi-tiered
          knowledge engine.</p>

          <p>do i know the answer?<br>does my colleague know the answer?<br>none of
          us know the answer, do we have anything in our knowledge base to guide us
          to the answer?<br>we don''t? scraper, please google that for me and return
          cleaned up text to provide me with some context<br>OK now we know the answer,
          please save this context to the knowledge base</p>

          <p>Steps 1 and 2 can be done by a normal (reasonably good) full-stack engineer
          who doesn''t mind spending $500 for cheap GPU compute on vast.ai to prove
          the concept and has familiarity with LLMs...<br>Step 3 should be doable
          in a few months by a team of 5 guys, including at least 1 senior ML engineer,
          server-wise you''re looking at an 8xA100 rig for inference, another for
          finetuning, and a bunch of smaller / cheaper instances for various ancillary
          models and classical computing. Probably $30k to surpass GPT-4 on reproducable
          benchmarks (not including salary for the 5 guys lol)</p>

          <p>BTW i fit into the "reasonably good full-stack engineer" category...
          I''m building cool apps that use LLMs, multimodal pipelines, etc... but
          I don''t have the academic background in neural network design, etc. So
          I might be missing something here :)</p>

          </blockquote>

          <p>Stable Beluga 2 will never beat chatgpt because it is being trained on
          chatgpt outputs</p>

          '
        raw: "> Amazing, thanks! I wonder what the performance will be like... I mean,\
          \ quantized versions of these SOTA 70b releases are an amazing advance and\
          \ totally usable for many downstream applications, but subjectively the\
          \ inference quality is noticably degraded. Compared to the original models,\
          \ some of which subjectively \"feel\" equal or superior to ChatGPT (chat\
          \ versions) or text-davinci-003 (llama 2 70b base model), at least for workloads\
          \ that do not involve heavy code generation. \n> \n> If it really is possible\
          \ to run Stable Beluga 2 and other SOTA finetunes of llama-2-70b on consumer-grade\
          \ hardware like this, the path to a GPT-4 killer becomes quite straightforward:\
          \ \n> \n> Step 1: deploy a number of 70b models in parallel, some generalists\
          \ like SB2, some other models finetuned for various workloads, and a bunch\
          \ of RWKV finetunes for certain tasks (RWKV handles things like creative\
          \ writing shockingly well and fits in 16gb of vram)\n> Step 2: put them\
          \ behind a fast 13B model, fine-tuned for high performance classification\
          \ tasks. A super-classifier if you will, which delegates requests to the\
          \ most appropriate large model(s), assembles the result, and presents it\
          \ to the user. Since any of the 70b generalist models currently trending\
          \ are ALREADY on par with GPT 3.5 / ChatGPT, even a naive implementation\
          \ of this architecture where no significant finetuning or training is done\
          \ beyond prompt engineering, and that simply directs the request based on\
          \ the gross strengths of the various models (e.g. code-writing vs mathematical\
          \ reasoning vs multistep COT etc) - will be unarguably superior to GPT 3.5,\
          \ and nipping at the heels of GPT4.\n> Step 2a: Add SOTA SDXL and various\
          \ CV models to the system, making it fully multimodal like GPT-4 supposedly\
          \ is (but they censored it)\n> Step 3: finetune the 70b models so they know\
          \ how and when to ask for help from their peers, creating a mesh topology\
          \ that MAY not be all that different than from GPT-4's current architecture\
          \ (we know its made from multiple models, we don't know what their individual\
          \ characteristics are or how they work together). For bonus points, integrate\
          \ a web scraper completely behind the scenes, that the models call upon\
          \ when they need more context, just like a human would do when they google\
          \ something... and then save the scraped data in a vector database that\
          \ serves as long term memory shared by ALL the models, so that next time\
          \ they don't have to go to the web for answers (and periodically we can\
          \ use the data stored in that retrieval system for finetuning the models\
          \ themselves)\n> \n> At that point, presuming that the models are capable\
          \ of admitting they don't know the answer, or we create a specialist model\
          \ for predicting whether an answer from another model is hallucinated, we\
          \ now have a multi-tiered knowledge engine.\n> \n> do i know the answer?\
          \ \n> does my colleague know the answer?\n> none of us know the answer,\
          \ do we have anything in our knowledge base to guide us to the answer?\n\
          > we don't? scraper, please google that for me and return cleaned up text\
          \ to provide me with some context\n> OK now we know the answer, please save\
          \ this context to the knowledge base\n> \n> Steps 1 and 2 can be done by\
          \ a normal (reasonably good) full-stack engineer who doesn't mind spending\
          \ $500 for cheap GPU compute on vast.ai to prove the concept and has familiarity\
          \ with LLMs... \n> Step 3 should be doable in a few months by a team of\
          \ 5 guys, including at least 1 senior ML engineer, server-wise you're looking\
          \ at an 8xA100 rig for inference, another for finetuning, and a bunch of\
          \ smaller / cheaper instances for various ancillary models and classical\
          \ computing. Probably $30k to surpass GPT-4 on reproducable benchmarks (not\
          \ including salary for the 5 guys lol)\n> \n> BTW i fit into the \"reasonably\
          \ good full-stack engineer\" category... I'm building cool apps that use\
          \ LLMs, multimodal pipelines, etc... but I don't have the academic background\
          \ in neural network design, etc. So I might be missing something here :)\n\
          \nStable Beluga 2 will never beat chatgpt because it is being trained on\
          \ chatgpt outputs"
        updatedAt: '2023-08-11T12:56:02.793Z'
      numEdits: 1
      reactions: []
    id: 64d62fb39fef656cfdfb2c63
    type: comment
  author: breadlicker45
  content: "> Amazing, thanks! I wonder what the performance will be like... I mean,\
    \ quantized versions of these SOTA 70b releases are an amazing advance and totally\
    \ usable for many downstream applications, but subjectively the inference quality\
    \ is noticably degraded. Compared to the original models, some of which subjectively\
    \ \"feel\" equal or superior to ChatGPT (chat versions) or text-davinci-003 (llama\
    \ 2 70b base model), at least for workloads that do not involve heavy code generation.\
    \ \n> \n> If it really is possible to run Stable Beluga 2 and other SOTA finetunes\
    \ of llama-2-70b on consumer-grade hardware like this, the path to a GPT-4 killer\
    \ becomes quite straightforward: \n> \n> Step 1: deploy a number of 70b models\
    \ in parallel, some generalists like SB2, some other models finetuned for various\
    \ workloads, and a bunch of RWKV finetunes for certain tasks (RWKV handles things\
    \ like creative writing shockingly well and fits in 16gb of vram)\n> Step 2: put\
    \ them behind a fast 13B model, fine-tuned for high performance classification\
    \ tasks. A super-classifier if you will, which delegates requests to the most\
    \ appropriate large model(s), assembles the result, and presents it to the user.\
    \ Since any of the 70b generalist models currently trending are ALREADY on par\
    \ with GPT 3.5 / ChatGPT, even a naive implementation of this architecture where\
    \ no significant finetuning or training is done beyond prompt engineering, and\
    \ that simply directs the request based on the gross strengths of the various\
    \ models (e.g. code-writing vs mathematical reasoning vs multistep COT etc) -\
    \ will be unarguably superior to GPT 3.5, and nipping at the heels of GPT4.\n\
    > Step 2a: Add SOTA SDXL and various CV models to the system, making it fully\
    \ multimodal like GPT-4 supposedly is (but they censored it)\n> Step 3: finetune\
    \ the 70b models so they know how and when to ask for help from their peers, creating\
    \ a mesh topology that MAY not be all that different than from GPT-4's current\
    \ architecture (we know its made from multiple models, we don't know what their\
    \ individual characteristics are or how they work together). For bonus points,\
    \ integrate a web scraper completely behind the scenes, that the models call upon\
    \ when they need more context, just like a human would do when they google something...\
    \ and then save the scraped data in a vector database that serves as long term\
    \ memory shared by ALL the models, so that next time they don't have to go to\
    \ the web for answers (and periodically we can use the data stored in that retrieval\
    \ system for finetuning the models themselves)\n> \n> At that point, presuming\
    \ that the models are capable of admitting they don't know the answer, or we create\
    \ a specialist model for predicting whether an answer from another model is hallucinated,\
    \ we now have a multi-tiered knowledge engine.\n> \n> do i know the answer? \n\
    > does my colleague know the answer?\n> none of us know the answer, do we have\
    \ anything in our knowledge base to guide us to the answer?\n> we don't? scraper,\
    \ please google that for me and return cleaned up text to provide me with some\
    \ context\n> OK now we know the answer, please save this context to the knowledge\
    \ base\n> \n> Steps 1 and 2 can be done by a normal (reasonably good) full-stack\
    \ engineer who doesn't mind spending $500 for cheap GPU compute on vast.ai to\
    \ prove the concept and has familiarity with LLMs... \n> Step 3 should be doable\
    \ in a few months by a team of 5 guys, including at least 1 senior ML engineer,\
    \ server-wise you're looking at an 8xA100 rig for inference, another for finetuning,\
    \ and a bunch of smaller / cheaper instances for various ancillary models and\
    \ classical computing. Probably $30k to surpass GPT-4 on reproducable benchmarks\
    \ (not including salary for the 5 guys lol)\n> \n> BTW i fit into the \"reasonably\
    \ good full-stack engineer\" category... I'm building cool apps that use LLMs,\
    \ multimodal pipelines, etc... but I don't have the academic background in neural\
    \ network design, etc. So I might be missing something here :)\n\nStable Beluga\
    \ 2 will never beat chatgpt because it is being trained on chatgpt outputs"
  created_at: 2023-08-11 11:55:15+00:00
  edited: true
  hidden: false
  id: 64d62fb39fef656cfdfb2c63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
      fullname: Sam Rahimi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sociopathic-hamster
      type: user
    createdAt: '2023-09-01T01:35:26.000Z'
    data:
      edited: false
      editors:
      - sociopathic-hamster
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761490821838379
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64ce902e228324a28bae2f5e/xV3FSR2txdJtSYiKbrLXI.png?w=200&h=200&f=face
          fullname: Sam Rahimi
          isHf: false
          isPro: false
          name: sociopathic-hamster
          type: user
        html: '<p>It''s being fine tuned on chatgpt outputs, true... but it was pretrained
          on far more than that. </p>

          <p>I do see your point. What I''m saying is that finetuning has become so
          easy and computationally cheap, it will be possible to achieve great results
          by using multiple fine tuned variants of this and other contemporary models,
          if the orchestration between them is done right </p>

          '
        raw: "It's being fine tuned on chatgpt outputs, true... but it was pretrained\
          \ on far more than that. \n\nI do see your point. What I'm saying is that\
          \ finetuning has become so easy and computationally cheap, it will be possible\
          \ to achieve great results by using multiple fine tuned variants of this\
          \ and other contemporary models, if the orchestration between them is done\
          \ right "
        updatedAt: '2023-09-01T01:35:26.883Z'
      numEdits: 0
      reactions: []
    id: 64f13fde57f46a4f63e2d15f
    type: comment
  author: sociopathic-hamster
  content: "It's being fine tuned on chatgpt outputs, true... but it was pretrained\
    \ on far more than that. \n\nI do see your point. What I'm saying is that finetuning\
    \ has become so easy and computationally cheap, it will be possible to achieve\
    \ great results by using multiple fine tuned variants of this and other contemporary\
    \ models, if the orchestration between them is done right "
  created_at: 2023-09-01 00:35:26+00:00
  edited: false
  hidden: false
  id: 64f13fde57f46a4f63e2d15f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597134725486-5e4d607b37cb5b49818287c0.jpeg?w=200&h=200&f=face
      fullname: Cahya Wirawan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cahya
      type: user
    createdAt: '2023-10-30T22:10:33.000Z'
    data:
      edited: false
      editors:
      - cahya
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6517536044120789
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1597134725486-5e4d607b37cb5b49818287c0.jpeg?w=200&h=200&f=face
          fullname: Cahya Wirawan
          isHf: false
          isPro: false
          name: cahya
          type: user
        html: '<p>do you mean MoE?</p>

          '
        raw: do you mean MoE?
        updatedAt: '2023-10-30T22:10:33.221Z'
      numEdits: 0
      reactions: []
    id: 654029d97466fdcc083c523d
    type: comment
  author: cahya
  content: do you mean MoE?
  created_at: 2023-10-30 21:10:33+00:00
  edited: false
  hidden: false
  id: 654029d97466fdcc083c523d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: stabilityai/StableBeluga2
repo_type: model
status: open
target_branch: null
title: Minimum VRAM / GPU specs?
