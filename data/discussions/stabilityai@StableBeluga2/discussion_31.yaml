!!python/object:huggingface_hub.community.DiscussionWithDetails
author: g1hahn
conflicting_files: null
created_at: 2023-08-18 17:48:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9736af1d9b30ce45c77592c95e8c42e7.svg
      fullname: Jiwon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: g1hahn
      type: user
    createdAt: '2023-08-18T18:48:38.000Z'
    data:
      edited: false
      editors:
      - g1hahn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47789904475212097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9736af1d9b30ce45c77592c95e8c42e7.svg
          fullname: Jiwon
          isHf: false
          isPro: false
          name: g1hahn
          type: user
        html: '<p>I was able to load the model in 4 bit so it fits in 35.6GB memory.<br>However
          when I do model.generate() it gives below error.  Does anyone have a clue?  Thanks!</p>

          <p>File /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:292,
          in LlamaDecoderLayer.forward(self, hidden_states, attention_mask, position_ids,
          past_key_value, output_attentions, use_cache)<br>    289 hidden_states =
          self.input_layernorm(hidden_states)<br>    291 # Self Attention<br>--&gt;
          292 hidden_states, self_attn_weights, present_key_value = self.self_attn(<br>    293     hidden_states=hidden_states,<br>    294     attention_mask=attention_mask,<br>    295     position_ids=position_ids,<br>    296     past_key_value=past_key_value,<br>    297     output_attentions=output_attentions,<br>    298     use_cache=use_cache,<br>    299
          )<br>    300 hidden_states = residual + hidden_states<br>    302 # Fully
          Connected</p>

          <p>File /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,
          in Module._call_impl(self, *args, **kwargs)<br>   1496 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1497 # this function,
          and just call forward.<br>   1498 if not (self._backward_hooks or self._backward_pre_hooks
          or self._forward_hooks or self._forward_pre_hooks<br>   1499         or
          _global_backward_pre_hooks or _global_backward_hooks<br>   1500         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1501     return
          forward_call(*args, **kwargs)<br>   1502 # Do not call functions when jit
          is used<br>   1503 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165,
          in add_hook_to_module..new_forward(*args, **kwargs)<br>    163         output
          = old_forward(*args, **kwargs)<br>    164 else:<br>--&gt; 165     output
          = old_forward(*args, **kwargs)<br>    166 return module._hf_hook.post_forward(module,
          output)</p>

          <p>File /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:195,
          in LlamaAttention.forward(self, hidden_states, attention_mask, position_ids,
          past_key_value, output_attentions, use_cache)<br>    192 bsz, q_len, _ =
          hidden_states.size()<br>    194 query_states = self.q_proj(hidden_states).view(bsz,
          q_len, self.num_heads, self.head_dim).transpose(1, 2)<br>--&gt; 195 key_states
          = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,
          2)<br>    196 value_states = self.v_proj(hidden_states).view(bsz, q_len,
          self.num_heads, self.head_dim).transpose(1, 2)<br>    198 kv_seq_len = key_states.shape[-2]</p>

          <p>RuntimeError: shape ''[1, 64, 64, 128]'' is invalid for input of size
          65536</p>

          '
        raw: "I was able to load the model in 4 bit so it fits in 35.6GB memory.\r\
          \nHowever when I do model.generate() it gives below error.  Does anyone\
          \ have a clue?  Thanks!\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    163\
          \         output = old_forward(*args, **kwargs)\r\n    164 else:\r\n-->\
          \ 165     output = old_forward(*args, **kwargs)\r\n    166 return module._hf_hook.post_forward(module,\
          \ output)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:292,\
          \ in LlamaDecoderLayer.forward(self, hidden_states, attention_mask, position_ids,\
          \ past_key_value, output_attentions, use_cache)\r\n    289 hidden_states\
          \ = self.input_layernorm(hidden_states)\r\n    291 # Self Attention\r\n\
          --> 292 hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\
          \n    293     hidden_states=hidden_states,\r\n    294     attention_mask=attention_mask,\r\
          \n    295     position_ids=position_ids,\r\n    296     past_key_value=past_key_value,\r\
          \n    297     output_attentions=output_attentions,\r\n    298     use_cache=use_cache,\r\
          \n    299 )\r\n    300 hidden_states = residual + hidden_states\r\n    302\
          \ # Fully Connected\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
          \ in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don't have\
          \ any hooks, we want to skip the rest of the logic in\r\n   1497 # this\
          \ function, and just call forward.\r\n   1498 if not (self._backward_hooks\
          \ or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\
          \n   1499         or _global_backward_pre_hooks or _global_backward_hooks\r\
          \n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1501     return forward_call(*args, **kwargs)\r\n   1502 # Do not call\
          \ functions when jit is used\r\n   1503 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165,\
          \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    163\
          \         output = old_forward(*args, **kwargs)\r\n    164 else:\r\n-->\
          \ 165     output = old_forward(*args, **kwargs)\r\n    166 return module._hf_hook.post_forward(module,\
          \ output)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:195,\
          \ in LlamaAttention.forward(self, hidden_states, attention_mask, position_ids,\
          \ past_key_value, output_attentions, use_cache)\r\n    192 bsz, q_len, _\
          \ = hidden_states.size()\r\n    194 query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n--> 195 key_states\
          \ = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,\
          \ 2)\r\n    196 value_states = self.v_proj(hidden_states).view(bsz, q_len,\
          \ self.num_heads, self.head_dim).transpose(1, 2)\r\n    198 kv_seq_len =\
          \ key_states.shape[-2]\r\n\r\nRuntimeError: shape '[1, 64, 64, 128]' is\
          \ invalid for input of size 65536"
        updatedAt: '2023-08-18T18:48:38.512Z'
      numEdits: 0
      reactions: []
    id: 64dfbd068e2084e1d7b52966
    type: comment
  author: g1hahn
  content: "I was able to load the model in 4 bit so it fits in 35.6GB memory.\r\n\
    However when I do model.generate() it gives below error.  Does anyone have a clue?\
    \  Thanks!\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    163     \
    \    output = old_forward(*args, **kwargs)\r\n    164 else:\r\n--> 165     output\
    \ = old_forward(*args, **kwargs)\r\n    166 return module._hf_hook.post_forward(module,\
    \ output)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:292,\
    \ in LlamaDecoderLayer.forward(self, hidden_states, attention_mask, position_ids,\
    \ past_key_value, output_attentions, use_cache)\r\n    289 hidden_states = self.input_layernorm(hidden_states)\r\
    \n    291 # Self Attention\r\n--> 292 hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n    293     hidden_states=hidden_states,\r\n    294   \
    \  attention_mask=attention_mask,\r\n    295     position_ids=position_ids,\r\n\
    \    296     past_key_value=past_key_value,\r\n    297     output_attentions=output_attentions,\r\
    \n    298     use_cache=use_cache,\r\n    299 )\r\n    300 hidden_states = residual\
    \ + hidden_states\r\n    302 # Fully Connected\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501,\
    \ in Module._call_impl(self, *args, **kwargs)\r\n   1496 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1497 # this function,\
    \ and just call forward.\r\n   1498 if not (self._backward_hooks or self._backward_pre_hooks\
    \ or self._forward_hooks or self._forward_pre_hooks\r\n   1499         or _global_backward_pre_hooks\
    \ or _global_backward_hooks\r\n   1500         or _global_forward_hooks or _global_forward_pre_hooks):\r\
    \n-> 1501     return forward_call(*args, **kwargs)\r\n   1502 # Do not call functions\
    \ when jit is used\r\n   1503 full_backward_hooks, non_full_backward_hooks = [],\
    \ []\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/accelerate/hooks.py:165,\
    \ in add_hook_to_module.<locals>.new_forward(*args, **kwargs)\r\n    163     \
    \    output = old_forward(*args, **kwargs)\r\n    164 else:\r\n--> 165     output\
    \ = old_forward(*args, **kwargs)\r\n    166 return module._hf_hook.post_forward(module,\
    \ output)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:195,\
    \ in LlamaAttention.forward(self, hidden_states, attention_mask, position_ids,\
    \ past_key_value, output_attentions, use_cache)\r\n    192 bsz, q_len, _ = hidden_states.size()\r\
    \n    194 query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads,\
    \ self.head_dim).transpose(1, 2)\r\n--> 195 key_states = self.k_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n    196 value_states\
    \ = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1,\
    \ 2)\r\n    198 kv_seq_len = key_states.shape[-2]\r\n\r\nRuntimeError: shape '[1,\
    \ 64, 64, 128]' is invalid for input of size 65536"
  created_at: 2023-08-18 17:48:38+00:00
  edited: false
  hidden: false
  id: 64dfbd068e2084e1d7b52966
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: stabilityai/StableBeluga2
repo_type: model
status: open
target_branch: null
title: Error after loading in 4bit
