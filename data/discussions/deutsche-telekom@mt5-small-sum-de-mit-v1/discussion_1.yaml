!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JingFan
conflicting_files: null
created_at: 2022-07-01 13:44:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acae1030bf660b3baa1837ee1566879f.svg
      fullname: Jing Fan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JingFan
      type: user
    createdAt: '2022-07-01T14:44:09.000Z'
    data:
      edited: false
      editors:
      - JingFan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acae1030bf660b3baa1837ee1566879f.svg
          fullname: Jing Fan
          isHf: false
          isPro: false
          name: JingFan
          type: user
        html: "<p>Hi there,</p>\n<p>my colleague <span data-props=\"{&quot;user&quot;:&quot;dennlinger&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dennlinger\"\
          >@<span class=\"underline\">dennlinger</span></a></span>\n\n\t</span></span>\
          \ and I are from the Institute of Computer Science at Heidelberg University,\
          \ currently investigating the performance of German abstractive summarizers.\
          \ We are very interested in your model and we have also evaluated your model\
          \ with the <a href=\"https://huggingface.co/datasets/mlsum\">MLSUM test\
          \ set</a> (all samples, no filtering). We found that our results are better\
          \ (see table below) than the <a href=\"https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1\"\
          >results you reported in the model card</a>. </p>\n<div class=\"max-w-full\
          \ overflow-auto\">\n\t<table>\n\t\t<thead><tr>\n<th>Parameters</th>\n<th>Rouge1-F1</th>\n\
          <th>Rouge2-F1</th>\n<th>RougeL-F1</th>\n</tr>\n\n\t\t</thead><tbody><tr>\n\
          <td>MLSUM (max_length=354, min_length=13, do_sample=false, truncation=True)</td>\n\
          <td>0.1882</td>\n<td>0.0553</td>\n<td>0.1448</td>\n</tr>\n</tbody>\n\t</table>\n\
          </div>\n<p>Length parameters were obtained based on statistics from the\
          \ MLSUM training set. We further do not explicitly prepend the <code>\"\
          summarize :\"</code> to any of the samples, FWIW.<br>Given the differing\
          \ results, we had some further questions about your model:</p>\n<ol>\n<li>For\
          \ the data preprocessing, only the records with no more than 94 summary\
          \ tokens were selected. Could we ask why you chose 94? Is this somehow related\
          \ to the SwissText dataset?</li>\n<li>As a follow-up, did you also perform\
          \ similar filtering on the validation and test set (potentially explaining\
          \ the different results we obtain)?</li>\n<li>There are no parameters specified\
          \ for the generation of summaries in the test set (aside from \"no beams\"\
          ). Does this imply that you simply performed greedy search for the summary\
          \ without specification of any further parameters (like minimum/maximum\
          \ length)?</li>\n<li>We randomly checked some source articles and the generated\
          \ summaries and have found that there is frequent repetition and semantic\
          \ incoherence in the generated summaries. The following figure is an example\
          \ of the first five articles in the MLSUM test set for your model's output.\
          \ Do you have any insights into why this might occur?</li>\n</ol>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1656686411036-62bf00c0731e4c7e1d965d52.png\"\
          ><img alt=\"mit.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1656686411036-62bf00c0731e4c7e1d965d52.png\"\
          ></a></p>\n<p>We would appreciate any comment and feedback!</p>\n<p>Best\
          \ wishes,</p>\n<p>Dennis and Jing</p>\n"
        raw: "Hi there,\r\n\r\nmy colleague @dennlinger and I are from the Institute\
          \ of Computer Science at Heidelberg University, currently investigating\
          \ the performance of German abstractive summarizers. We are very interested\
          \ in your model and we have also evaluated your model with the [MLSUM test\
          \ set](https://huggingface.co/datasets/mlsum) (all samples, no filtering).\
          \ We found that our results are better (see table below) than the [results\
          \ you reported in the model card](https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1).\
          \ \r\n\r\n| Parameters | Rouge1-F1 |Rouge2-F1 |RougeL-F1 |\r\n| ------ |\
          \ ------ |------ |------ |\r\n| MLSUM (max_length=354, min_length=13, do_sample=false,\
          \ truncation=True) | 0.1882 | 0.0553 | 0.1448|\r\n\r\nLength parameters\
          \ were obtained based on statistics from the MLSUM training set. We further\
          \ do not explicitly prepend the `\"summarize :\"` to any of the samples,\
          \ FWIW.\r\nGiven the differing results, we had some further questions about\
          \ your model:\r\n\r\n1. For the data preprocessing, only the records with\
          \ no more than 94 summary tokens were selected. Could we ask why you chose\
          \ 94? Is this somehow related to the SwissText dataset?\r\n2. As a follow-up,\
          \ did you also perform similar filtering on the validation and test set\
          \ (potentially explaining the different results we obtain)?\r\n3. There\
          \ are no parameters specified for the generation of summaries in the test\
          \ set (aside from \"no beams\"). Does this imply that you simply performed\
          \ greedy search for the summary without specification of any further parameters\
          \ (like minimum/maximum length)?\r\n4. We randomly checked some source articles\
          \ and the generated summaries and have found that there is frequent repetition\
          \ and semantic incoherence in the generated summaries. The following figure\
          \ is an example of the first five articles in the MLSUM test set for your\
          \ model's output. Do you have any insights into why this might occur?\r\n\
          \r\n\r\n![mit.png](https://cdn-uploads.huggingface.co/production/uploads/1656686411036-62bf00c0731e4c7e1d965d52.png)\r\
          \n\r\n\r\nWe would appreciate any comment and feedback!\r\n\r\nBest wishes,\r\
          \n\r\nDennis and Jing"
        updatedAt: '2022-07-01T14:44:09.183Z'
      numEdits: 0
      reactions: []
    id: 62bf083930a1888a761acd38
    type: comment
  author: JingFan
  content: "Hi there,\r\n\r\nmy colleague @dennlinger and I are from the Institute\
    \ of Computer Science at Heidelberg University, currently investigating the performance\
    \ of German abstractive summarizers. We are very interested in your model and\
    \ we have also evaluated your model with the [MLSUM test set](https://huggingface.co/datasets/mlsum)\
    \ (all samples, no filtering). We found that our results are better (see table\
    \ below) than the [results you reported in the model card](https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1).\
    \ \r\n\r\n| Parameters | Rouge1-F1 |Rouge2-F1 |RougeL-F1 |\r\n| ------ | ------\
    \ |------ |------ |\r\n| MLSUM (max_length=354, min_length=13, do_sample=false,\
    \ truncation=True) | 0.1882 | 0.0553 | 0.1448|\r\n\r\nLength parameters were obtained\
    \ based on statistics from the MLSUM training set. We further do not explicitly\
    \ prepend the `\"summarize :\"` to any of the samples, FWIW.\r\nGiven the differing\
    \ results, we had some further questions about your model:\r\n\r\n1. For the data\
    \ preprocessing, only the records with no more than 94 summary tokens were selected.\
    \ Could we ask why you chose 94? Is this somehow related to the SwissText dataset?\r\
    \n2. As a follow-up, did you also perform similar filtering on the validation\
    \ and test set (potentially explaining the different results we obtain)?\r\n3.\
    \ There are no parameters specified for the generation of summaries in the test\
    \ set (aside from \"no beams\"). Does this imply that you simply performed greedy\
    \ search for the summary without specification of any further parameters (like\
    \ minimum/maximum length)?\r\n4. We randomly checked some source articles and\
    \ the generated summaries and have found that there is frequent repetition and\
    \ semantic incoherence in the generated summaries. The following figure is an\
    \ example of the first five articles in the MLSUM test set for your model's output.\
    \ Do you have any insights into why this might occur?\r\n\r\n\r\n![mit.png](https://cdn-uploads.huggingface.co/production/uploads/1656686411036-62bf00c0731e4c7e1d965d52.png)\r\
    \n\r\n\r\nWe would appreciate any comment and feedback!\r\n\r\nBest wishes,\r\n\
    \r\nDennis and Jing"
  created_at: 2022-07-01 13:44:09+00:00
  edited: false
  hidden: false
  id: 62bf083930a1888a761acd38
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1595445065015-noauth.png?w=200&h=200&f=face
      fullname: Philip May
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: PhilipMay
      type: user
    createdAt: '2022-07-28T16:29:19.000Z'
    data:
      edited: true
      editors:
      - PhilipMay
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1595445065015-noauth.png?w=200&h=200&f=face
          fullname: Philip May
          isHf: false
          isPro: false
          name: PhilipMay
          type: user
        html: '<p>Hi,<br>sorry for the late reply.<br>Unfortunately, the computer
          on which I trained the models is not accessible right now. Therefore, I
          must unfortunately put you off a little longer.</p>

          <blockquote>

          <p>For the data preprocessing, only the records with no more than 94 summary
          tokens were selected. Could we ask why you chose 94? Is this somehow related
          to the SwissText dataset?</p>

          </blockquote>

          <p>I did some experiments to get a value for input and output seq. len.<br>I
          had to somehow balance them to avoid OOMs on the GPUs.</p>

          <p>These models can not be trained with FP16 due to a strange bug :-(</p>

          <p>94 is 96 - 2<br>-2 because of start and end token if I remember correctly.</p>

          <blockquote>

          <p>As a follow-up, did you also perform similar filtering on the validation
          and test set (potentially explaining the different results we obtain)?</p>

          </blockquote>

          <p>I have to check that on the code I can not access right now...</p>

          <blockquote>

          <p>There are no parameters specified for the generation of summaries in
          the test set (aside from "no beams"). Does this imply that you simply performed
          greedy search for the summary without specification of any further parameters
          (like minimum/maximum length)?</p>

          </blockquote>

          <p>For evaluation (and training) I did use this script:<br><a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py">https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py</a></p>

          <p>maybe you find your answers there?</p>

          <blockquote>

          <p>Do you have any insights into why this might occur?</p>

          </blockquote>

          <p>Thanks for sharing these insights.</p>

          <p>No, unfortunately not.<br>Do you have any idea how to avoid it?</p>

          <p>PS:<br>I plan to train more summarization models based on our new German
          T5 model:<br><a href="https://huggingface.co/GermanT5/t5-efficient-gc4-german-base-nl36">https://huggingface.co/GermanT5/t5-efficient-gc4-german-base-nl36</a></p>

          <p>If you have time and desire I would be happy to work with you on it.
          Maybe you have ideas which data sets can be taken and so?</p>

          <p>If you want I can set us - after my vacation times - a conference call?</p>

          '
        raw: 'Hi,

          sorry for the late reply.

          Unfortunately, the computer on which I trained the models is not accessible
          right now. Therefore, I must unfortunately put you off a little longer.


          > For the data preprocessing, only the records with no more than 94 summary
          tokens were selected. Could we ask why you chose 94? Is this somehow related
          to the SwissText dataset?


          I did some experiments to get a value for input and output seq. len.

          I had to somehow balance them to avoid OOMs on the GPUs.


          These models can not be trained with FP16 due to a strange bug :-(


          94 is 96 - 2

          -2 because of start and end token if I remember correctly.


          > As a follow-up, did you also perform similar filtering on the validation
          and test set (potentially explaining the different results we obtain)?


          I have to check that on the code I can not access right now...


          > There are no parameters specified for the generation of summaries in the
          test set (aside from "no beams"). Does this imply that you simply performed
          greedy search for the summary without specification of any further parameters
          (like minimum/maximum length)?


          For evaluation (and training) I did use this script:

          https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py


          maybe you find your answers there?


          > Do you have any insights into why this might occur?


          Thanks for sharing these insights.


          No, unfortunately not.

          Do you have any idea how to avoid it?


          PS:

          I plan to train more summarization models based on our new German T5 model:

          https://huggingface.co/GermanT5/t5-efficient-gc4-german-base-nl36


          If you have time and desire I would be happy to work with you on it. Maybe
          you have ideas which data sets can be taken and so?


          If you want I can set us - after my vacation times - a conference call?'
        updatedAt: '2022-07-28T16:38:51.292Z'
      numEdits: 2
      reactions: []
    id: 62e2b95f555a866437a8e2a6
    type: comment
  author: PhilipMay
  content: 'Hi,

    sorry for the late reply.

    Unfortunately, the computer on which I trained the models is not accessible right
    now. Therefore, I must unfortunately put you off a little longer.


    > For the data preprocessing, only the records with no more than 94 summary tokens
    were selected. Could we ask why you chose 94? Is this somehow related to the SwissText
    dataset?


    I did some experiments to get a value for input and output seq. len.

    I had to somehow balance them to avoid OOMs on the GPUs.


    These models can not be trained with FP16 due to a strange bug :-(


    94 is 96 - 2

    -2 because of start and end token if I remember correctly.


    > As a follow-up, did you also perform similar filtering on the validation and
    test set (potentially explaining the different results we obtain)?


    I have to check that on the code I can not access right now...


    > There are no parameters specified for the generation of summaries in the test
    set (aside from "no beams"). Does this imply that you simply performed greedy
    search for the summary without specification of any further parameters (like minimum/maximum
    length)?


    For evaluation (and training) I did use this script:

    https://github.com/huggingface/transformers/blob/main/examples/pytorch/summarization/run_summarization.py


    maybe you find your answers there?


    > Do you have any insights into why this might occur?


    Thanks for sharing these insights.


    No, unfortunately not.

    Do you have any idea how to avoid it?


    PS:

    I plan to train more summarization models based on our new German T5 model:

    https://huggingface.co/GermanT5/t5-efficient-gc4-german-base-nl36


    If you have time and desire I would be happy to work with you on it. Maybe you
    have ideas which data sets can be taken and so?


    If you want I can set us - after my vacation times - a conference call?'
  created_at: 2022-07-28 15:29:19+00:00
  edited: true
  hidden: false
  id: 62e2b95f555a866437a8e2a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585728817057-noauth.jpeg?w=200&h=200&f=face
      fullname: Dennis Aumiller
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dennlinger
      type: user
    createdAt: '2022-07-29T08:18:36.000Z'
    data:
      edited: false
      editors:
      - dennlinger
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1585728817057-noauth.jpeg?w=200&h=200&f=face
          fullname: Dennis Aumiller
          isHf: false
          isPro: false
          name: dennlinger
          type: user
        html: '<p>Hi Philip,<br>thanks so much for your response! Also very interesting
          to hear about the FP16 issue, I wasn''t aware of it, so good to know.</p>

          <p>As for the repetitions, we have since experimented with n-gram repetition
          penalties (either <a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size"><code>no_repeat_ngram_size</code></a>
          or less strictly <a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate.repetition_penalty"><code>repetition_penalty</code></a>,
          although the latter is less transparent to set as a parameter). On paper,
          the ROUGE scores did not move significantly, but at least this avoids issues
          such as the above quite well. I still think that we''d have to investigate
          a bit more to say for sure that this doesn''t cause other problems, but
          it seems to be a quick fix for very frequent repetitions.</p>

          <p>As for the length, one follow-up would be whether this then also applies
          to the test set, or did you use the (unfiltered) full test set for your
          scoring runs?</p>

          <p>And finally, great to hear that you are working on some newer models!
          I think Stefan is already aware of my dataset <a href="https://huggingface.co/datasets/dennlinger/klexikon">Klexikon</a>,
          however, texts are generally too long for the standard 512 input (and often
          also for the 512 output). I''m working on a weakly paragraph-aligned variant
          that might be suitable for your training purposes, although I expect that
          there is some content overlap in the source articles, given that we also
          use Wikipedia as an input text, similar to the Swisstext challenge data.
          However, data is quite high quality, as I''ve manually checked about 10%
          of the dataset, so let me know if you have any particular questions about
          it.</p>

          <p>Otherwise, we have used the German subset of the <code>wiki_lingua</code>
          dataset (careful, there are two different versions on the Huggingface Hub:
          the <a href="https://huggingface.co/datasets/wiki_lingua">original</a> and
          the re-crawled <a href="https://huggingface.co/datasets/GEM/wiki_lingua">GEM
          version</a>). However, the target "summaries" are often extremely short,
          and not very suitable for natural language summarization, I would say.</p>

          <p>Feel free to set up a call after you are back from vacation, I''m very
          keen to hear more!<br>Best,<br>Dennis</p>

          '
        raw: 'Hi Philip,

          thanks so much for your response! Also very interesting to hear about the
          FP16 issue, I wasn''t aware of it, so good to know.


          As for the repetitions, we have since experimented with n-gram repetition
          penalties (either [`no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size)
          or less strictly [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate.repetition_penalty),
          although the latter is less transparent to set as a parameter). On paper,
          the ROUGE scores did not move significantly, but at least this avoids issues
          such as the above quite well. I still think that we''d have to investigate
          a bit more to say for sure that this doesn''t cause other problems, but
          it seems to be a quick fix for very frequent repetitions.


          As for the length, one follow-up would be whether this then also applies
          to the test set, or did you use the (unfiltered) full test set for your
          scoring runs?


          And finally, great to hear that you are working on some newer models! I
          think Stefan is already aware of my dataset [Klexikon](https://huggingface.co/datasets/dennlinger/klexikon),
          however, texts are generally too long for the standard 512 input (and often
          also for the 512 output). I''m working on a weakly paragraph-aligned variant
          that might be suitable for your training purposes, although I expect that
          there is some content overlap in the source articles, given that we also
          use Wikipedia as an input text, similar to the Swisstext challenge data.
          However, data is quite high quality, as I''ve manually checked about 10%
          of the dataset, so let me know if you have any particular questions about
          it.


          Otherwise, we have used the German subset of the `wiki_lingua` dataset (careful,
          there are two different versions on the Huggingface Hub: the [original](https://huggingface.co/datasets/wiki_lingua)
          and the re-crawled [GEM version](https://huggingface.co/datasets/GEM/wiki_lingua)).
          However, the target "summaries" are often extremely short, and not very
          suitable for natural language summarization, I would say.


          Feel free to set up a call after you are back from vacation, I''m very keen
          to hear more!

          Best,

          Dennis'
        updatedAt: '2022-07-29T08:18:36.505Z'
      numEdits: 0
      reactions: []
    id: 62e397dcf3b208e2aecf8c4d
    type: comment
  author: dennlinger
  content: 'Hi Philip,

    thanks so much for your response! Also very interesting to hear about the FP16
    issue, I wasn''t aware of it, so good to know.


    As for the repetitions, we have since experimented with n-gram repetition penalties
    (either [`no_repeat_ngram_size`](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size)
    or less strictly [`repetition_penalty`](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate.repetition_penalty),
    although the latter is less transparent to set as a parameter). On paper, the
    ROUGE scores did not move significantly, but at least this avoids issues such
    as the above quite well. I still think that we''d have to investigate a bit more
    to say for sure that this doesn''t cause other problems, but it seems to be a
    quick fix for very frequent repetitions.


    As for the length, one follow-up would be whether this then also applies to the
    test set, or did you use the (unfiltered) full test set for your scoring runs?


    And finally, great to hear that you are working on some newer models! I think
    Stefan is already aware of my dataset [Klexikon](https://huggingface.co/datasets/dennlinger/klexikon),
    however, texts are generally too long for the standard 512 input (and often also
    for the 512 output). I''m working on a weakly paragraph-aligned variant that might
    be suitable for your training purposes, although I expect that there is some content
    overlap in the source articles, given that we also use Wikipedia as an input text,
    similar to the Swisstext challenge data. However, data is quite high quality,
    as I''ve manually checked about 10% of the dataset, so let me know if you have
    any particular questions about it.


    Otherwise, we have used the German subset of the `wiki_lingua` dataset (careful,
    there are two different versions on the Huggingface Hub: the [original](https://huggingface.co/datasets/wiki_lingua)
    and the re-crawled [GEM version](https://huggingface.co/datasets/GEM/wiki_lingua)).
    However, the target "summaries" are often extremely short, and not very suitable
    for natural language summarization, I would say.


    Feel free to set up a call after you are back from vacation, I''m very keen to
    hear more!

    Best,

    Dennis'
  created_at: 2022-07-29 07:18:36+00:00
  edited: false
  hidden: false
  id: 62e397dcf3b208e2aecf8c4d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: deutsche-telekom/mt5-small-sum-de-mit-v1
repo_type: model
status: open
target_branch: null
title: Better results than the reported results and some questions about the model
