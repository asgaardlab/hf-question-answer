!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheYuriLover
conflicting_files: null
created_at: 2023-08-01 02:31:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-01T03:31:46.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676772952079773
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Hey,</p>

          <p>As weird as it seems, those finetuned versions still feel like a QLora,
          I noticed this drop in quality quickly because when I try to make a story
          with only females, only the QLora versions of yours disregard that fact
          and add some male characters.</p>

          <p>Yet on those finetuned 2.0 and m2.0 models, this problem persists, it
          adds male characters and the outputs are shorter than the airoboros-l1-13b-1.4.</p>

          <p>I don''t know if that''s because the 1.4 dataset is superior to the 2.0
          and m2.0, but it''s a shame I still prefer the llama1 airoboros... </p>

          <p>My guess would be that the assumptions about GPT4 being dumber over time
          are true, you said you were using the june version of gpt4 to create the
          2.0 dataset, it seems like the march version of GPT4 gave better outputs
          and that''s why the 1.4 dataset seems to be better in my book.</p>

          <p>Edit: the 2.0 seems to be more consistent than m2.0 and sometimes it
          can happen the generation ends after a few sentences only</p>

          '
        raw: "Hey,\n\nAs weird as it seems, those finetuned versions still feel like\
          \ a QLora, I noticed this drop in quality quickly because when I try to\
          \ make a story with only females, only the QLora versions of yours disregard\
          \ that fact and add some male characters.\n\nYet on those finetuned 2.0\
          \ and m2.0 models, this problem persists, it adds male characters and the\
          \ outputs are shorter than the airoboros-l1-13b-1.4.\n\nI don't know if\
          \ that's because the 1.4 dataset is superior to the 2.0 and m2.0, but it's\
          \ a shame I still prefer the llama1 airoboros... \n\nMy guess would be that\
          \ the assumptions about GPT4 being dumber over time are true, you said you\
          \ were using the june version of gpt4 to create the 2.0 dataset, it seems\
          \ like the march version of GPT4 gave better outputs and that's why the\
          \ 1.4 dataset seems to be better in my book.\n\nEdit: the 2.0 seems to be\
          \ more consistent than m2.0 and sometimes it can happen the generation ends\
          \ after a few sentences only"
        updatedAt: '2023-08-01T04:07:06.247Z'
      numEdits: 2
      reactions: []
    id: 64c87ca2f3d2a59a4317ff1b
    type: comment
  author: TheYuriLover
  content: "Hey,\n\nAs weird as it seems, those finetuned versions still feel like\
    \ a QLora, I noticed this drop in quality quickly because when I try to make a\
    \ story with only females, only the QLora versions of yours disregard that fact\
    \ and add some male characters.\n\nYet on those finetuned 2.0 and m2.0 models,\
    \ this problem persists, it adds male characters and the outputs are shorter than\
    \ the airoboros-l1-13b-1.4.\n\nI don't know if that's because the 1.4 dataset\
    \ is superior to the 2.0 and m2.0, but it's a shame I still prefer the llama1\
    \ airoboros... \n\nMy guess would be that the assumptions about GPT4 being dumber\
    \ over time are true, you said you were using the june version of gpt4 to create\
    \ the 2.0 dataset, it seems like the march version of GPT4 gave better outputs\
    \ and that's why the 1.4 dataset seems to be better in my book.\n\nEdit: the 2.0\
    \ seems to be more consistent than m2.0 and sometimes it can happen the generation\
    \ ends after a few sentences only"
  created_at: 2023-08-01 02:31:46+00:00
  edited: true
  hidden: false
  id: 64c87ca2f3d2a59a4317ff1b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-08-01T07:13:20.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9681794047355652
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I assure you they were not fine-tuned with QLoRA - I used fastchat:<br><a
          rel="nofollow" href="https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503">https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503</a></p>

          <p>I might make a llama-1 version of this to see if it''s something in the
          llama-2 base model itself causing problems.</p>

          <p>When I was working on the new dataset, I had to be much more explicit
          with the prompts to get June gpt4 to respond the way I wanted it to (March
          version worked fine with much less detail), and even then it often ignored
          certain details or hallucinated extra criteria, so I suspect that''s where
          the problem is.</p>

          <p>When you say the 2.0 is more consistent, do you mean consistently better
          or worse?</p>

          '
        raw: 'I assure you they were not fine-tuned with QLoRA - I used fastchat:

          https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503


          I might make a llama-1 version of this to see if it''s something in the
          llama-2 base model itself causing problems.


          When I was working on the new dataset, I had to be much more explicit with
          the prompts to get June gpt4 to respond the way I wanted it to (March version
          worked fine with much less detail), and even then it often ignored certain
          details or hallucinated extra criteria, so I suspect that''s where the problem
          is.


          When you say the 2.0 is more consistent, do you mean consistently better
          or worse?'
        updatedAt: '2023-08-01T07:13:20.530Z'
      numEdits: 0
      reactions: []
    id: 64c8b090c547ed5243e0f240
    type: comment
  author: jondurbin
  content: 'I assure you they were not fine-tuned with QLoRA - I used fastchat:

    https://gist.github.com/jondurbin/7183e6edcc5cb57d5f544614d0ce0503


    I might make a llama-1 version of this to see if it''s something in the llama-2
    base model itself causing problems.


    When I was working on the new dataset, I had to be much more explicit with the
    prompts to get June gpt4 to respond the way I wanted it to (March version worked
    fine with much less detail), and even then it often ignored certain details or
    hallucinated extra criteria, so I suspect that''s where the problem is.


    When you say the 2.0 is more consistent, do you mean consistently better or worse?'
  created_at: 2023-08-01 06:13:20+00:00
  edited: false
  hidden: false
  id: 64c8b090c547ed5243e0f240
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-01T07:28:22.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9831517338752747
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>"I might make a llama-1 version of this to see if it''s something
          in the llama-2 base model itself causing problems."<br>I know I''m asking
          much, but I''d really love to get llama2-13b finetuned with the 1.4 Airoboros
          dataset instead, it worked so well on llama1 and I''m sure it will also
          do its magic for this one also.</p>

          <p>"When I was working on the new dataset, I had to be much more explicit
          with the prompts to get June gpt4 to respond the way I wanted it to (March
          version worked fine with much less detail), and even then it often ignored
          certain details or hallucinated extra criteria, so I suspect that''s where
          the problem is."<br>Yeah, same. I work as a datascientist and I''m often
          using gpt4 to make code, at the begining (mars -&gt; may) it was really
          easy to talk to it. Now it feels I have to explain like it''s a 5yo, it
          understands less and less what I''m asking for. I wouldn''t say it got dumber
          to the point it went back to legacy 3.5 level but it''s definitely not as
          smart as the march gpt4 counterpart, and that''s such a shame...<br>That
          probably explains the drop in quality we got on the finetunes, june gpt4
          doesn''t provide good enough outputs anymore, I wish we could go back to
          march gpt4, at least you still have the 1.4 dataset so there''s that I guess
          :p</p>

          <p>Edit: Looks like we''re not the only one noticing this ^^'' <a rel="nofollow"
          href="https://www.reddit.com/r/ChatGPT/comments/15ekje9/goodbye_chat_gpt_plus_subscription/">https://www.reddit.com/r/ChatGPT/comments/15ekje9/goodbye_chat_gpt_plus_subscription/</a></p>

          <p>"When you say the 2.0 is more consistent, do you mean consistently better
          or worse?"<br>For the better, I got less often serious halucinations on
          the 2.0 compared to the m2.0.</p>

          '
        raw: '"I might make a llama-1 version of this to see if it''s something in
          the llama-2 base model itself causing problems."

          I know I''m asking much, but I''d really love to get llama2-13b finetuned
          with the 1.4 Airoboros dataset instead, it worked so well on llama1 and
          I''m sure it will also do its magic for this one also.


          "When I was working on the new dataset, I had to be much more explicit with
          the prompts to get June gpt4 to respond the way I wanted it to (March version
          worked fine with much less detail), and even then it often ignored certain
          details or hallucinated extra criteria, so I suspect that''s where the problem
          is."

          Yeah, same. I work as a datascientist and I''m often using gpt4 to make
          code, at the begining (mars -> may) it was really easy to talk to it. Now
          it feels I have to explain like it''s a 5yo, it understands less and less
          what I''m asking for. I wouldn''t say it got dumber to the point it went
          back to legacy 3.5 level but it''s definitely not as smart as the march
          gpt4 counterpart, and that''s such a shame...

          That probably explains the drop in quality we got on the finetunes, june
          gpt4 doesn''t provide good enough outputs anymore, I wish we could go back
          to march gpt4, at least you still have the 1.4 dataset so there''s that
          I guess :p


          Edit: Looks like we''re not the only one noticing this ^^'' https://www.reddit.com/r/ChatGPT/comments/15ekje9/goodbye_chat_gpt_plus_subscription/


          "When you say the 2.0 is more consistent, do you mean consistently better
          or worse?"

          For the better, I got less often serious halucinations on the 2.0 compared
          to the m2.0.'
        updatedAt: '2023-08-01T07:33:29.061Z'
      numEdits: 3
      reactions: []
    id: 64c8b4167d0ea4e7f1272c28
    type: comment
  author: TheYuriLover
  content: '"I might make a llama-1 version of this to see if it''s something in the
    llama-2 base model itself causing problems."

    I know I''m asking much, but I''d really love to get llama2-13b finetuned with
    the 1.4 Airoboros dataset instead, it worked so well on llama1 and I''m sure it
    will also do its magic for this one also.


    "When I was working on the new dataset, I had to be much more explicit with the
    prompts to get June gpt4 to respond the way I wanted it to (March version worked
    fine with much less detail), and even then it often ignored certain details or
    hallucinated extra criteria, so I suspect that''s where the problem is."

    Yeah, same. I work as a datascientist and I''m often using gpt4 to make code,
    at the begining (mars -> may) it was really easy to talk to it. Now it feels I
    have to explain like it''s a 5yo, it understands less and less what I''m asking
    for. I wouldn''t say it got dumber to the point it went back to legacy 3.5 level
    but it''s definitely not as smart as the march gpt4 counterpart, and that''s such
    a shame...

    That probably explains the drop in quality we got on the finetunes, june gpt4
    doesn''t provide good enough outputs anymore, I wish we could go back to march
    gpt4, at least you still have the 1.4 dataset so there''s that I guess :p


    Edit: Looks like we''re not the only one noticing this ^^'' https://www.reddit.com/r/ChatGPT/comments/15ekje9/goodbye_chat_gpt_plus_subscription/


    "When you say the 2.0 is more consistent, do you mean consistently better or worse?"

    For the better, I got less often serious halucinations on the 2.0 compared to
    the m2.0.'
  created_at: 2023-08-01 06:28:22+00:00
  edited: true
  hidden: false
  id: 64c8b4167d0ea4e7f1272c28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-08-01T10:18:48.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9739453196525574
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<blockquote>

          <p>For the better, I got less often serious halucinations on the 2.0 compared
          to the m2.0.</p>

          </blockquote>

          <p>I''m not sure a llama-2 fine-tune of 1.4 would be what you want if this
          is the case, because m2.0 includes 1.4 so if it''s worse than 2.0 the problem
          is likely somewhere in the 1.4 dataset.</p>

          <p>It may also just be overfit, perhaps an earlier checkpoint model would
          do better.  Let me try a couple things .</p>

          '
        raw: '> For the better, I got less often serious halucinations on the 2.0
          compared to the m2.0.


          I''m not sure a llama-2 fine-tune of 1.4 would be what you want if this
          is the case, because m2.0 includes 1.4 so if it''s worse than 2.0 the problem
          is likely somewhere in the 1.4 dataset.


          It may also just be overfit, perhaps an earlier checkpoint model would do
          better.  Let me try a couple things .'
        updatedAt: '2023-08-01T10:18:48.446Z'
      numEdits: 0
      reactions: []
    id: 64c8dc084515835c4d95bcbf
    type: comment
  author: jondurbin
  content: '> For the better, I got less often serious halucinations on the 2.0 compared
    to the m2.0.


    I''m not sure a llama-2 fine-tune of 1.4 would be what you want if this is the
    case, because m2.0 includes 1.4 so if it''s worse than 2.0 the problem is likely
    somewhere in the 1.4 dataset.


    It may also just be overfit, perhaps an earlier checkpoint model would do better.  Let
    me try a couple things .'
  created_at: 2023-08-01 09:18:48+00:00
  edited: false
  hidden: false
  id: 64c8dc084515835c4d95bcbf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-01T17:09:04.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9800835847854614
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<blockquote>

          <p>I''m not sure a llama-2 fine-tune of 1.4 would be what you want if this
          is the case, because m2.0 includes 1.4 so if it''s worse than 2.0 the problem
          is likely somewhere in the 1.4 dataset.</p>

          </blockquote>

          <p>I think that it''s either because merging the 1.4 dataset with the 2.0
          dataset was a bad idea because the outputs are differents (june vs march
          gpt4) so the model has trouble to train with 2 different paradigm, or it''s
          because like you said according to the LIMA paper, you shouldn''t have a
          dataset too big and the merge is too big.<br>I don''t think there''s a problem
          with the 1.4 dataset, it made llama1-13b really interesting by itself, that''s
          why I wanted to see if it would do the same for llama2</p>

          '
        raw: '>I''m not sure a llama-2 fine-tune of 1.4 would be what you want if
          this is the case, because m2.0 includes 1.4 so if it''s worse than 2.0 the
          problem is likely somewhere in the 1.4 dataset.


          I think that it''s either because merging the 1.4 dataset with the 2.0 dataset
          was a bad idea because the outputs are differents (june vs march gpt4) so
          the model has trouble to train with 2 different paradigm, or it''s because
          like you said according to the LIMA paper, you shouldn''t have a dataset
          too big and the merge is too big.

          I don''t think there''s a problem with the 1.4 dataset, it made llama1-13b
          really interesting by itself, that''s why I wanted to see if it would do
          the same for llama2'
        updatedAt: '2023-08-01T17:09:24.151Z'
      numEdits: 1
      reactions: []
    id: 64c93c30166b73558339bd72
    type: comment
  author: TheYuriLover
  content: '>I''m not sure a llama-2 fine-tune of 1.4 would be what you want if this
    is the case, because m2.0 includes 1.4 so if it''s worse than 2.0 the problem
    is likely somewhere in the 1.4 dataset.


    I think that it''s either because merging the 1.4 dataset with the 2.0 dataset
    was a bad idea because the outputs are differents (june vs march gpt4) so the
    model has trouble to train with 2 different paradigm, or it''s because like you
    said according to the LIMA paper, you shouldn''t have a dataset too big and the
    merge is too big.

    I don''t think there''s a problem with the 1.4 dataset, it made llama1-13b really
    interesting by itself, that''s why I wanted to see if it would do the same for
    llama2'
  created_at: 2023-08-01 16:09:04+00:00
  edited: true
  hidden: false
  id: 64c93c30166b73558339bd72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-08-06T20:22:20.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8800371885299683
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>Congratulations, you''ve volunteered yourself to test this!</p>

          <p>Here are 7 models, tuned with varying datasets, some qlora, some full
          fine tunes, some llama-2, some not.</p>

          <ul>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-zane">https://huggingface.co/jondurbin/blind-test-13b-zane</a></li>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-vlad">https://huggingface.co/jondurbin/blind-test-13b-vlad</a></li>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-martha">https://huggingface.co/jondurbin/blind-test-13b-martha</a></li>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-jimmy">https://huggingface.co/jondurbin/blind-test-13b-jimmy</a></li>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-jasmine">https://huggingface.co/jondurbin/blind-test-13b-jasmine</a></li>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-janus">https://huggingface.co/jondurbin/blind-test-13b-janus</a></li>

          <li><a href="https://huggingface.co/jondurbin/blind-test-13b-francis">https://huggingface.co/jondurbin/blind-test-13b-francis</a></li>

          </ul>

          <p>One of these models is the one you seek - a full fine tune of llama-2-13b
          on the 1.4.1 dataset.  The rest are not.</p>

          <p>While is possible for you to identify some of them, as far as qlora vs
          FT, and whether they are llama-2 or llama-1, I''d ask that you just test
          them and not try to figure it out before hand.</p>

          <p>Run at least a few dozen prompts through each, then rank them on a scale
          of 0-10, and let me know.  I have a secret gist, with a git commit timestamp,
          that includes the mapping of what each of these models actually are.</p>

          '
        raw: 'Congratulations, you''ve volunteered yourself to test this!


          Here are 7 models, tuned with varying datasets, some qlora, some full fine
          tunes, some llama-2, some not.

          - https://huggingface.co/jondurbin/blind-test-13b-zane

          - https://huggingface.co/jondurbin/blind-test-13b-vlad

          - https://huggingface.co/jondurbin/blind-test-13b-martha

          - https://huggingface.co/jondurbin/blind-test-13b-jimmy

          - https://huggingface.co/jondurbin/blind-test-13b-jasmine

          - https://huggingface.co/jondurbin/blind-test-13b-janus

          - https://huggingface.co/jondurbin/blind-test-13b-francis


          One of these models is the one you seek - a full fine tune of llama-2-13b
          on the 1.4.1 dataset.  The rest are not.


          While is possible for you to identify some of them, as far as qlora vs FT,
          and whether they are llama-2 or llama-1, I''d ask that you just test them
          and not try to figure it out before hand.


          Run at least a few dozen prompts through each, then rank them on a scale
          of 0-10, and let me know.  I have a secret gist, with a git commit timestamp,
          that includes the mapping of what each of these models actually are.'
        updatedAt: '2023-08-06T20:22:20.521Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Hauserrodr
      - count: 1
        reaction: "\U0001F92F"
        users:
        - Nexesenex
    id: 64d000fc59503263d9f00845
    type: comment
  author: jondurbin
  content: 'Congratulations, you''ve volunteered yourself to test this!


    Here are 7 models, tuned with varying datasets, some qlora, some full fine tunes,
    some llama-2, some not.

    - https://huggingface.co/jondurbin/blind-test-13b-zane

    - https://huggingface.co/jondurbin/blind-test-13b-vlad

    - https://huggingface.co/jondurbin/blind-test-13b-martha

    - https://huggingface.co/jondurbin/blind-test-13b-jimmy

    - https://huggingface.co/jondurbin/blind-test-13b-jasmine

    - https://huggingface.co/jondurbin/blind-test-13b-janus

    - https://huggingface.co/jondurbin/blind-test-13b-francis


    One of these models is the one you seek - a full fine tune of llama-2-13b on the
    1.4.1 dataset.  The rest are not.


    While is possible for you to identify some of them, as far as qlora vs FT, and
    whether they are llama-2 or llama-1, I''d ask that you just test them and not
    try to figure it out before hand.


    Run at least a few dozen prompts through each, then rank them on a scale of 0-10,
    and let me know.  I have a secret gist, with a git commit timestamp, that includes
    the mapping of what each of these models actually are.'
  created_at: 2023-08-06 19:22:20+00:00
  edited: false
  hidden: false
  id: 64d000fc59503263d9f00845
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
      fullname: Henky!!
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Henk717
      type: user
    createdAt: '2023-08-06T20:38:06.000Z'
    data:
      edited: false
      editors:
      - Henk717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9786331057548523
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1640356718818-61c47e9c71a107e9d80e33e3.jpeg?w=200&h=200&f=face
          fullname: Henky!!
          isHf: false
          isPro: false
          name: Henk717
          type: user
        html: '<p>I shared the test with my community so they can also see which one
          they like best.<br>The models can be loaded on the free KoboldAI GPU colab
          using United so that is probably what they will use as a setup.</p>

          '
        raw: 'I shared the test with my community so they can also see which one they
          like best.

          The models can be loaded on the free KoboldAI GPU colab using United so
          that is probably what they will use as a setup.'
        updatedAt: '2023-08-06T20:38:06.534Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jondurbin
        - ausboss
    id: 64d004aeabc3308f0526ee0d
    type: comment
  author: Henk717
  content: 'I shared the test with my community so they can also see which one they
    like best.

    The models can be loaded on the free KoboldAI GPU colab using United so that is
    probably what they will use as a setup.'
  created_at: 2023-08-06 19:38:06+00:00
  edited: false
  hidden: false
  id: 64d004aeabc3308f0526ee0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-06T20:38:59.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-06T20:39:11.735Z'
      numEdits: 0
      reactions: []
    id: 64d004e3749587dbe02924b5
    type: comment
  author: TheYuriLover
  content: This comment has been hidden
  created_at: 2023-08-06 19:38:59+00:00
  edited: true
  hidden: true
  id: 64d004e3749587dbe02924b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-06T20:38:59.000Z'
    data:
      status: closed
    id: 64d004e3749587dbe02924b8
    type: status-change
  author: TheYuriLover
  created_at: 2023-08-06 19:38:59+00:00
  id: 64d004e3749587dbe02924b8
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-08-06T20:39:04.000Z'
    data:
      status: open
    id: 64d004e8bf39f9c8bebe0305
    type: status-change
  author: TheYuriLover
  created_at: 2023-08-06 19:39:04+00:00
  id: 64d004e8bf39f9c8bebe0305
  new_status: open
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/airoboros-l2-13b-gpt4-m2.0
repo_type: model
status: open
target_branch: null
title: The 2.0 and m2.0 versions still feel like a QLora
