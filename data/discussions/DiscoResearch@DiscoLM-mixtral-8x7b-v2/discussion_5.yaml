!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BigDeeper
conflicting_files: null
created_at: 2023-12-10 17:38:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
      fullname: Big Deeper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BigDeeper
      type: user
    createdAt: '2023-12-10T17:38:43.000Z'
    data:
      edited: false
      editors:
      - BigDeeper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5442032814025879
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8ace4395d175d9c1d3d11f191c3709b0.svg
          fullname: Big Deeper
          isHf: false
          isPro: false
          name: BigDeeper
          type: user
        html: '<p>Permuting layer 30<br>Permuting layer 31<br>model.embed_tokens.weight                        -&gt;
          token_embd.weight                        | F16    | [32002, 4096]<br>model.layers.0.input_layernorm.weight            -&gt;
          blk.0.attn_norm.weight                   | F16    | [4096]<br>Traceback
          (most recent call last):<br>  File "/home/developer/llama.cpp/convert.py",
          line 1228, in <br>    main()<br>  File "/home/developer/llama.cpp/convert.py",
          line 1215, in main<br>    model   = convert_model_names(model, params)<br>  File
          "/home/developer/llama.cpp/convert.py", line 1004, in convert_model_names<br>    raise
          Exception(f"Unexpected tensor name: {name}")<br>Exception: Unexpected tensor
          name: model.layers.0.mlp.experts.0.w1.weight</p>

          '
        raw: "Permuting layer 30\r\nPermuting layer 31\r\nmodel.embed_tokens.weight\
          \                        -> token_embd.weight                        | F16\
          \    | [32002, 4096]\r\nmodel.layers.0.input_layernorm.weight          \
          \  -> blk.0.attn_norm.weight                   | F16    | [4096]\r\nTraceback\
          \ (most recent call last):\r\n  File \"/home/developer/llama.cpp/convert.py\"\
          , line 1228, in <module>\r\n    main()\r\n  File \"/home/developer/llama.cpp/convert.py\"\
          , line 1215, in main\r\n    model   = convert_model_names(model, params)\r\
          \n  File \"/home/developer/llama.cpp/convert.py\", line 1004, in convert_model_names\r\
          \n    raise Exception(f\"Unexpected tensor name: {name}\")\r\nException:\
          \ Unexpected tensor name: model.layers.0.mlp.experts.0.w1.weight\r\n"
        updatedAt: '2023-12-10T17:38:43.261Z'
      numEdits: 0
      reactions: []
    id: 6575f7a3827f8fd666dadc1a
    type: comment
  author: BigDeeper
  content: "Permuting layer 30\r\nPermuting layer 31\r\nmodel.embed_tokens.weight\
    \                        -> token_embd.weight                        | F16   \
    \ | [32002, 4096]\r\nmodel.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight\
    \                   | F16    | [4096]\r\nTraceback (most recent call last):\r\n\
    \  File \"/home/developer/llama.cpp/convert.py\", line 1228, in <module>\r\n \
    \   main()\r\n  File \"/home/developer/llama.cpp/convert.py\", line 1215, in main\r\
    \n    model   = convert_model_names(model, params)\r\n  File \"/home/developer/llama.cpp/convert.py\"\
    , line 1004, in convert_model_names\r\n    raise Exception(f\"Unexpected tensor\
    \ name: {name}\")\r\nException: Unexpected tensor name: model.layers.0.mlp.experts.0.w1.weight\r\
    \n"
  created_at: 2023-12-10 17:38:43+00:00
  edited: false
  hidden: false
  id: 6575f7a3827f8fd666dadc1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-10T17:40:28.000Z'
    data:
      edited: false
      editors:
      - bjoernp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.984812319278717
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
          fullname: "Bj\xF6rn Pl\xFCster "
          isHf: false
          isPro: false
          name: bjoernp
          type: user
        html: '<p>Llama.cpp doesn''t have support for this architecture. They''ll
          probably wait until official arch is released before implementing </p>

          '
        raw: 'Llama.cpp doesn''t have support for this architecture. They''ll probably
          wait until official arch is released before implementing '
        updatedAt: '2023-12-10T17:40:28.520Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6575f80c8b44ef012b7440ed
    id: 6575f80c8b44ef012b7440eb
    type: comment
  author: bjoernp
  content: 'Llama.cpp doesn''t have support for this architecture. They''ll probably
    wait until official arch is released before implementing '
  created_at: 2023-12-10 17:40:28+00:00
  edited: false
  hidden: false
  id: 6575f80c8b44ef012b7440eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/4a0cff546914b0f094f4a33e376a2f16.svg
      fullname: "Bj\xF6rn Pl\xFCster "
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: bjoernp
      type: user
    createdAt: '2023-12-10T17:40:28.000Z'
    data:
      status: closed
    id: 6575f80c8b44ef012b7440ed
    type: status-change
  author: bjoernp
  created_at: 2023-12-10 17:40:28+00:00
  id: 6575f80c8b44ef012b7440ed
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: DiscoResearch/DiscoLM-mixtral-8x7b-v2
repo_type: model
status: closed
target_branch: null
title: Trying to quantize. Running into the issue below. Any suggestions?
