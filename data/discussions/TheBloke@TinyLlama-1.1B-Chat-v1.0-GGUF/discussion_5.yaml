!!python/object:huggingface_hub.community.DiscussionWithDetails
author: supercharge19
conflicting_files: null
created_at: 2024-01-20 05:56:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
      fullname: Jawad Mansoor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: supercharge19
      type: user
    createdAt: '2024-01-20T05:56:39.000Z'
    data:
      edited: false
      editors:
      - supercharge19
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8494609594345093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bbfe7f8c8e7ce8b50e48a4a2164a3c2e.svg
          fullname: Jawad Mansoor
          isHf: false
          isPro: false
          name: supercharge19
          type: user
        html: '<p>I have never been able to get a decent response out of any library
          other than kobold or llama cpp (not llama cpp python) and since I work with
          python a lot I tried ctransformers as well which is the worst to be used
          (in my experience). However, I am finding it very difficult to community
          with model using kobold or llama cpp (server) when used as drop in replacement
          for openai''s api.</p>

          <p>here is what i tried:</p>

          <p>1&gt; run server with command: ./server -m tinyllama.gguf<br>2&gt; (on
          different cmd/tab) run openai''s replacement: python api_like_OAI.py # must
          have flask installed<br>or just run kobold you''ll have an endpoint<br>3&gt;
          use following code:</p>

          <p>from langchain_openai import OpenAI<br>from langchain.prompts import
          PromptTemplate<br>from langchain.chains import LLMChain</p>

          <p>llm = OpenAI(openai_api_base="<a rel="nofollow" href="http://10.192.4.242:8081/v1&quot;">http://10.192.4.242:8081/v1"</a>,
          openai_api_key="somethig")<br>question = "How many planets are there in
          our solar system?"<br>template = """Question: {question}<br>Answer: Let''s
          think step by step."""<br>prompt = PromptTemplate(template=template, input_variables=["question"])<br>llm_chain
          = LLMChain(prompt=prompt, llm=llm)</p>

          <p>response = llm_chain.invoke(question, max_tokens=10)<br>print(response)</p>

          <p>If however i use any model with llama_cpp_python then I get very weird
          output, which i tried with different models (all quantized) with different
          prompts. nothing worked :''(</p>

          '
        raw: "I have never been able to get a decent response out of any library other\
          \ than kobold or llama cpp (not llama cpp python) and since I work with\
          \ python a lot I tried ctransformers as well which is the worst to be used\
          \ (in my experience). However, I am finding it very difficult to community\
          \ with model using kobold or llama cpp (server) when used as drop in replacement\
          \ for openai's api.\r\n\r\nhere is what i tried:\r\n\r\n1> run server with\
          \ command: ./server -m tinyllama.gguf \r\n2> (on different cmd/tab) run\
          \ openai's replacement: python api_like_OAI.py # must have flask installed\r\
          \nor just run kobold you'll have an endpoint\r\n3> use following code:\r\
          \n\r\nfrom langchain_openai import OpenAI\r\nfrom langchain.prompts import\
          \ PromptTemplate\r\nfrom langchain.chains import LLMChain\r\n\r\nllm = OpenAI(openai_api_base=\"\
          http://10.192.4.242:8081/v1\", openai_api_key=\"somethig\")\r\nquestion\
          \ = \"How many planets are there in our solar system?\"\r\ntemplate = \"\
          \"\"Question: {question}\r\nAnswer: Let's think step by step.\"\"\"\r\n\
          prompt = PromptTemplate(template=template, input_variables=[\"question\"\
          ])\r\nllm_chain = LLMChain(prompt=prompt, llm=llm)\r\n\r\nresponse = llm_chain.invoke(question,\
          \ max_tokens=10)\r\nprint(response)\r\n\r\nIf however i use any model with\
          \ llama_cpp_python then I get very weird output, which i tried with different\
          \ models (all quantized) with different prompts. nothing worked :'("
        updatedAt: '2024-01-20T05:56:39.950Z'
      numEdits: 0
      reactions: []
    id: 65ab6097ac588f2a1c42956b
    type: comment
  author: supercharge19
  content: "I have never been able to get a decent response out of any library other\
    \ than kobold or llama cpp (not llama cpp python) and since I work with python\
    \ a lot I tried ctransformers as well which is the worst to be used (in my experience).\
    \ However, I am finding it very difficult to community with model using kobold\
    \ or llama cpp (server) when used as drop in replacement for openai's api.\r\n\
    \r\nhere is what i tried:\r\n\r\n1> run server with command: ./server -m tinyllama.gguf\
    \ \r\n2> (on different cmd/tab) run openai's replacement: python api_like_OAI.py\
    \ # must have flask installed\r\nor just run kobold you'll have an endpoint\r\n\
    3> use following code:\r\n\r\nfrom langchain_openai import OpenAI\r\nfrom langchain.prompts\
    \ import PromptTemplate\r\nfrom langchain.chains import LLMChain\r\n\r\nllm =\
    \ OpenAI(openai_api_base=\"http://10.192.4.242:8081/v1\", openai_api_key=\"somethig\"\
    )\r\nquestion = \"How many planets are there in our solar system?\"\r\ntemplate\
    \ = \"\"\"Question: {question}\r\nAnswer: Let's think step by step.\"\"\"\r\n\
    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\r\n\
    llm_chain = LLMChain(prompt=prompt, llm=llm)\r\n\r\nresponse = llm_chain.invoke(question,\
    \ max_tokens=10)\r\nprint(response)\r\n\r\nIf however i use any model with llama_cpp_python\
    \ then I get very weird output, which i tried with different models (all quantized)\
    \ with different prompts. nothing worked :'("
  created_at: 2024-01-20 05:56:39+00:00
  edited: false
  hidden: false
  id: 65ab6097ac588f2a1c42956b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
repo_type: model
status: open
target_branch: null
title: Python bindings not working
