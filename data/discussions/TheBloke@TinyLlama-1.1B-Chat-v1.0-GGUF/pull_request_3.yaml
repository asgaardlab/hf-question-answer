!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jbochi
conflicting_files: []
created_at: 2024-01-10 19:37:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jbochi
      type: user
    createdAt: '2024-01-10T19:37:39.000Z'
    data:
      oid: 4d32359a583538e342125874298891deb7d4ebaa
      parents:
      - 52e7645ba7c309695bec7ac98f4f005b139cf465
      subject: Upload tinyllama-1.1b-chat-v1.0.Q4_1.gguf
    id: 659ef2030000000000000000
    type: commit
  author: jbochi
  created_at: 2024-01-10 19:37:39+00:00
  id: 659ef2030000000000000000
  oid: 4d32359a583538e342125874298891deb7d4ebaa
  summary: Upload tinyllama-1.1b-chat-v1.0.Q4_1.gguf
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
      fullname: J Bochi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jbochi
      type: user
    createdAt: '2024-01-10T19:37:40.000Z'
    data:
      edited: false
      editors:
      - jbochi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9240172505378723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1018b29e2da89f5920a58482e17a1948.svg
          fullname: J Bochi
          isHf: false
          isPro: false
          name: jbochi
          type: user
        html: '<p>I''ve been working on adding <a rel="nofollow" href="https://github.com/ml-explore/mlx/pull/350">GGUF
          support to MLX</a>, and Q4_1 seems like the format that''s the most aligned
          with MLX quantization. The quantization error is also a bit better than
          Q4_0 (tested with <a rel="nofollow" href="https://github.com/antirez/gguf-tools/pull/9">gguf-tools</a>)</p>

          '
        raw: I've been working on adding [GGUF support to MLX](https://github.com/ml-explore/mlx/pull/350),
          and Q4_1 seems like the format that's the most aligned with MLX quantization.
          The quantization error is also a bit better than Q4_0 (tested with [gguf-tools](https://github.com/antirez/gguf-tools/pull/9))
        updatedAt: '2024-01-10T19:37:40.238Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gardner
    id: 659ef20444a230e92ca6148d
    type: comment
  author: jbochi
  content: I've been working on adding [GGUF support to MLX](https://github.com/ml-explore/mlx/pull/350),
    and Q4_1 seems like the format that's the most aligned with MLX quantization.
    The quantization error is also a bit better than Q4_0 (tested with [gguf-tools](https://github.com/antirez/gguf-tools/pull/9))
  created_at: 2024-01-10 19:37:40+00:00
  edited: false
  hidden: false
  id: 659ef20444a230e92ca6148d
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 3
repo_id: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
repo_type: model
status: open
target_branch: refs/heads/main
title: Upload tinyllama-1.1b-chat-v1.0.Q4_1.gguf
