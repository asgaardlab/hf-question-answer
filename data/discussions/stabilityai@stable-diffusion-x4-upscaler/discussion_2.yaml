!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ironharvy
conflicting_files: null
created_at: 2022-12-07 04:33:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
      fullname: Iron Harvy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ironharvy
      type: user
    createdAt: '2022-12-07T04:33:37.000Z'
    data:
      edited: true
      editors:
      - ironharvy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
          fullname: Iron Harvy
          isHf: false
          isPro: false
          name: ironharvy
          type: user
        html: "<p>The example provided throws 'CUDA out of memory' error if image\
          \ for upscale is more then 128x128 (256x 256 for example). I'm running this\
          \ on 4090 with 24Gb of VRAM:<br>==code==<br>import requests<br>from PIL\
          \ import Image<br>from io import BytesIO<br>from diffusers import StableDiffusionUpscalePipeline<br>import\
          \ torch</p>\n<h1 id=\"load-model-and-scheduler\">load model and scheduler</h1>\n\
          <p>model_id = \"stabilityai/stable-diffusion-x4-upscaler\"<br>pipeline =\
          \ StableDiffusionUpscalePipeline.from_pretrained(model_id, revision=\"fp16\"\
          , torch_dtype=torch.float16)<br>pipeline = pipeline.to(\"cuda\")</p>\n<h1\
          \ id=\"lets-download-an--image\">let's download an  image</h1>\n<p>url =\
          \ \"<a href=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png&quot;\"\
          >https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\"\
          </a><br>response = requests.get(url)<br>low_res_img = Image.open(BytesIO(response.content)).convert(\"\
          RGB\")<br>low_res_img = low_res_img.resize((128, 128)) # &lt;==========================================================</p>\n\
          <p>prompt = \"a white cat\"</p>\n<p>upscaled_image = pipeline(prompt=prompt,\
          \ image=low_res_img).images[0]<br>upscaled_image.save(\"upsampled_cat.png\"\
          )<br>==end of code==</p>\n<p>$ python scripts/gradio/sr_cli.py /mnt/c/Users/User/Downloads/samples/00008.png\
          \ \"a professional photograph of an astronaut riding a horse\"<br>Fetching\
          \ 14 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 14/14 [00:00&lt;00:00, 55136.39it/s]<br>100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 75/75 [05:15&lt;00:00,\
          \  4.20s/it]<br>Traceback (most recent call last):<br>  File \"/home/lameradze/stablediffusion/scripts/gradio/sr_cli.py\"\
          , line 29, in <br>    upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]<br>\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context<br>    return func(*args, **kwargs)<br> \
          \ File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\"\
          , line 494, in <strong>call</strong><br>    image = self.decode_latents(latents.float())<br>\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\"\
          , line 258, in decode_latents<br>    image = self.vae.decode(latents).sample<br>\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
          , line 605, in decode<br>    decoded = self._decode(z).sample<br>  File\
          \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
          , line 577, in _decode<br>    dec = self.decoder(z)<br>  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
          , line 213, in forward<br>    sample = self.mid_block(sample)<br>  File\
          \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py\"\
          , line 312, in forward<br>    hidden_states = attn(hidden_states)<br>  File\
          \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl<br>    return forward_call(*input, **kwargs)<br>\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/attention.py\"\
          , line 350, in forward<br>    attention_scores = torch.baddbmm(<br>RuntimeError:\
          \ CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 23.99 GiB total\
          \ capacity; 18.49 GiB already allocated; 1.06 GiB free; 20.08 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n"
        raw: "The example provided throws 'CUDA out of memory' error if image for\
          \ upscale is more then 128x128 (256x 256 for example). I'm running this\
          \ on 4090 with 24Gb of VRAM:\n==code==\nimport requests\nfrom PIL import\
          \ Image\nfrom io import BytesIO\nfrom diffusers import StableDiffusionUpscalePipeline\n\
          import torch\n\n# load model and scheduler\nmodel_id = \"stabilityai/stable-diffusion-x4-upscaler\"\
          \npipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, revision=\"\
          fp16\", torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n\n\
          # let's download an  image\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\"\
          \nresponse = requests.get(url)\nlow_res_img = Image.open(BytesIO(response.content)).convert(\"\
          RGB\")\nlow_res_img = low_res_img.resize((128, 128)) # <==========================================================\n\
          \nprompt = \"a white cat\"\n\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n\
          upscaled_image.save(\"upsampled_cat.png\")\n==end of code==\n\n$ python\
          \ scripts/gradio/sr_cli.py /mnt/c/Users/User/Downloads/samples/00008.png\
          \ \"a professional photograph of an astronaut riding a horse\"\nFetching\
          \ 14 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588| 14/14 [00:00<00:00, 55136.39it/s]\n100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 75/75 [05:15<00:00,\
          \  4.20s/it]\nTraceback (most recent call last):\n  File \"/home/lameradze/stablediffusion/scripts/gradio/sr_cli.py\"\
          , line 29, in <module>\n    upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\"\
          , line 494, in __call__\n    image = self.decode_latents(latents.float())\n\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\"\
          , line 258, in decode_latents\n    image = self.vae.decode(latents).sample\n\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
          , line 605, in decode\n    decoded = self._decode(z).sample\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
          , line 577, in _decode\n    dec = self.decoder(z)\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
          , line 213, in forward\n    sample = self.mid_block(sample)\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py\"\
          , line 312, in forward\n    hidden_states = attn(hidden_states)\n  File\
          \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/attention.py\"\
          , line 350, in forward\n    attention_scores = torch.baddbmm(\nRuntimeError:\
          \ CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 23.99 GiB total\
          \ capacity; 18.49 GiB already allocated; 1.06 GiB free; 20.08 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2022-12-08T17:31:32.558Z'
      numEdits: 2
      reactions: []
    id: 639017a11c5a62372726ba46
    type: comment
  author: ironharvy
  content: "The example provided throws 'CUDA out of memory' error if image for upscale\
    \ is more then 128x128 (256x 256 for example). I'm running this on 4090 with 24Gb\
    \ of VRAM:\n==code==\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\n\
    from diffusers import StableDiffusionUpscalePipeline\nimport torch\n\n# load model\
    \ and scheduler\nmodel_id = \"stabilityai/stable-diffusion-x4-upscaler\"\npipeline\
    \ = StableDiffusionUpscalePipeline.from_pretrained(model_id, revision=\"fp16\"\
    , torch_dtype=torch.float16)\npipeline = pipeline.to(\"cuda\")\n\n# let's download\
    \ an  image\nurl = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-upscale/low_res_cat.png\"\
    \nresponse = requests.get(url)\nlow_res_img = Image.open(BytesIO(response.content)).convert(\"\
    RGB\")\nlow_res_img = low_res_img.resize((128, 128)) # <==========================================================\n\
    \nprompt = \"a white cat\"\n\nupscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n\
    upscaled_image.save(\"upsampled_cat.png\")\n==end of code==\n\n$ python scripts/gradio/sr_cli.py\
    \ /mnt/c/Users/User/Downloads/samples/00008.png \"a professional photograph of\
    \ an astronaut riding a horse\"\nFetching 14 files: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 14/14 [00:00<00:00, 55136.39it/s]\n100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 75/75 [05:15<00:00,  4.20s/it]\nTraceback (most recent\
    \ call last):\n  File \"/home/lameradze/stablediffusion/scripts/gradio/sr_cli.py\"\
    , line 29, in <module>\n    upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]\n\
    \  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\"\
    , line 494, in __call__\n    image = self.decode_latents(latents.float())\n  File\
    \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py\"\
    , line 258, in decode_latents\n    image = self.vae.decode(latents).sample\n \
    \ File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
    , line 605, in decode\n    decoded = self._decode(z).sample\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
    , line 577, in _decode\n    dec = self.decoder(z)\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/vae.py\"\
    , line 213, in forward\n    sample = self.mid_block(sample)\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py\"\
    , line 312, in forward\n    hidden_states = attn(hidden_states)\n  File \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1130, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/home/lameradze/miniconda3/envs/sd/lib/python3.9/site-packages/diffusers/models/attention.py\"\
    , line 350, in forward\n    attention_scores = torch.baddbmm(\nRuntimeError: CUDA\
    \ out of memory. Tried to allocate 16.00 GiB (GPU 0; 23.99 GiB total capacity;\
    \ 18.49 GiB already allocated; 1.06 GiB free; 20.08 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2022-12-07 04:33:37+00:00
  edited: true
  hidden: false
  id: 639017a11c5a62372726ba46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668596259536-noauth.png?w=200&h=200&f=face
      fullname: Nguyen Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nguyenmto
      type: user
    createdAt: '2022-12-07T13:57:13.000Z'
    data:
      edited: false
      editors:
      - nguyenmto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668596259536-noauth.png?w=200&h=200&f=face
          fullname: Nguyen Pham
          isHf: false
          isPro: false
          name: nguyenmto
          type: user
        html: '<p>I am getting the same issue,  I can''t upscale 256x256 image (supposed
          to be 1024x1024 after scale) in a g4dn.xlarge instance<br>Is there a hardware
          requirement for this model documented?</p>

          '
        raw: 'I am getting the same issue,  I can''t upscale 256x256 image (supposed
          to be 1024x1024 after scale) in a g4dn.xlarge instance

          Is there a hardware requirement for this model documented?'
        updatedAt: '2022-12-07T13:57:13.051Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - meteomer
    id: 63909bb9d90cd466b3c6d5d8
    type: comment
  author: nguyenmto
  content: 'I am getting the same issue,  I can''t upscale 256x256 image (supposed
    to be 1024x1024 after scale) in a g4dn.xlarge instance

    Is there a hardware requirement for this model documented?'
  created_at: 2022-12-07 13:57:13+00:00
  edited: false
  hidden: false
  id: 63909bb9d90cd466b3c6d5d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd38c9c0c687cde5f60811d8717a4163.svg
      fullname: Shin Shi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shinshi
      type: user
    createdAt: '2022-12-08T03:42:13.000Z'
    data:
      edited: false
      editors:
      - Shinshi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd38c9c0c687cde5f60811d8717a4163.svg
          fullname: Shin Shi
          isHf: false
          isPro: false
          name: Shinshi
          type: user
        html: '<p>pretty useless right now. nobody has 1TiB of VRAM.</p>

          '
        raw: pretty useless right now. nobody has 1TiB of VRAM.
        updatedAt: '2022-12-08T03:42:13.907Z'
      numEdits: 0
      reactions: []
    id: 63915d15b6b839bb614537fe
    type: comment
  author: Shinshi
  content: pretty useless right now. nobody has 1TiB of VRAM.
  created_at: 2022-12-08 03:42:13+00:00
  edited: false
  hidden: false
  id: 63915d15b6b839bb614537fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
      fullname: Iron Harvy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ironharvy
      type: user
    createdAt: '2022-12-08T17:30:30.000Z'
    data:
      edited: true
      editors:
      - ironharvy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
          fullname: Iron Harvy
          isHf: false
          isPro: false
          name: ironharvy
          type: user
        html: '<p>Updated the error message for the 256x256 error (the previous one
          was actually for 768x768)</p>

          '
        raw: Updated the error message for the 256x256 error (the previous one was
          actually for 768x768)
        updatedAt: '2022-12-08T17:30:38.950Z'
      numEdits: 1
      reactions: []
    id: 63921f367459f1b0cc7c0f78
    type: comment
  author: ironharvy
  content: Updated the error message for the 256x256 error (the previous one was actually
    for 768x768)
  created_at: 2022-12-08 17:30:30+00:00
  edited: true
  hidden: false
  id: 63921f367459f1b0cc7c0f78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d654051bea2f6f92de5fde560498e269.svg
      fullname: Michael Gobble
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mgobb
      type: user
    createdAt: '2022-12-11T05:38:44.000Z'
    data:
      edited: false
      editors:
      - mgobb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d654051bea2f6f92de5fde560498e269.svg
          fullname: Michael Gobble
          isHf: false
          isPro: false
          name: mgobb
          type: user
        html: '<p>Try installing xformers from <a rel="nofollow" href="https://github.com/facebookresearch/xformers">https://github.com/facebookresearch/xformers</a>.
          Once you confirm that works, then after you create the upscale pipeline
          and move it to the GPU, add the command pipeline.set_use_memory_efficient_attention_xformers(True).
          That should let you upscale a 512x512 image on a 24 gb GPU.</p>

          '
        raw: Try installing xformers from https://github.com/facebookresearch/xformers.
          Once you confirm that works, then after you create the upscale pipeline
          and move it to the GPU, add the command pipeline.set_use_memory_efficient_attention_xformers(True).
          That should let you upscale a 512x512 image on a 24 gb GPU.
        updatedAt: '2022-12-11T05:38:44.815Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - koutilya-pnvr
    id: 63956ce46dc8355020ba5158
    type: comment
  author: mgobb
  content: Try installing xformers from https://github.com/facebookresearch/xformers.
    Once you confirm that works, then after you create the upscale pipeline and move
    it to the GPU, add the command pipeline.set_use_memory_efficient_attention_xformers(True).
    That should let you upscale a 512x512 image on a 24 gb GPU.
  created_at: 2022-12-11 05:38:44+00:00
  edited: false
  hidden: false
  id: 63956ce46dc8355020ba5158
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668596259536-noauth.png?w=200&h=200&f=face
      fullname: Nguyen Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nguyenmto
      type: user
    createdAt: '2022-12-11T12:04:02.000Z'
    data:
      edited: false
      editors:
      - nguyenmto
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668596259536-noauth.png?w=200&h=200&f=face
          fullname: Nguyen Pham
          isHf: false
          isPro: false
          name: nguyenmto
          type: user
        html: '<blockquote>

          <p>Try installing xformers from <a rel="nofollow" href="https://github.com/facebookresearch/xformers">https://github.com/facebookresearch/xformers</a>.
          Once you confirm that works, then after you create the upscale pipeline
          and move it to the GPU, add the command pipeline.set_use_memory_efficient_attention_xformers(True).
          That should let you upscale a 512x512 image on a 24 gb GPU.</p>

          </blockquote>

          <p>Could you show the xformers output? This is mine</p>

          <pre><code>A matching Triton is not available, some optimizations will not
          be enabled.

          Error caught was: No module named ''triton''

          xFormers 0.0.15.dev+3ea7307.d20221209

          memory_efficient_attention.flshatt:      available - requires GPU with compute
          capability 7.5+

          memory_efficient_attention.cutlass:      available

          memory_efficient_attention.small_k:      available

          memory_efficient_attention.tritonflashatt: not built

          memory_efficient_attention.ftriton_bflsh: not built

          swiglu.fused.p.cpp:                      available

          is_triton_available:                     False

          is_functorch_available:                  False

          pytorch.version:                         1.12.1

          pytorch.cuda:                            available

          gpu.compute_capability:                  7.5

          gpu.name:                                Tesla T4

          </code></pre>

          <p>Besides, according to the docs, the correct syntax must be <code>pipeline.enable_xformers_memory_efficient_attention</code>,
          but it doesn''t help at all</p>

          '
        raw: '> Try installing xformers from https://github.com/facebookresearch/xformers.
          Once you confirm that works, then after you create the upscale pipeline
          and move it to the GPU, add the command pipeline.set_use_memory_efficient_attention_xformers(True).
          That should let you upscale a 512x512 image on a 24 gb GPU.


          Could you show the xformers output? This is mine

          ```

          A matching Triton is not available, some optimizations will not be enabled.

          Error caught was: No module named ''triton''

          xFormers 0.0.15.dev+3ea7307.d20221209

          memory_efficient_attention.flshatt:      available - requires GPU with compute
          capability 7.5+

          memory_efficient_attention.cutlass:      available

          memory_efficient_attention.small_k:      available

          memory_efficient_attention.tritonflashatt: not built

          memory_efficient_attention.ftriton_bflsh: not built

          swiglu.fused.p.cpp:                      available

          is_triton_available:                     False

          is_functorch_available:                  False

          pytorch.version:                         1.12.1

          pytorch.cuda:                            available

          gpu.compute_capability:                  7.5

          gpu.name:                                Tesla T4

          ```


          Besides, according to the docs, the correct syntax must be `pipeline.enable_xformers_memory_efficient_attention`,
          but it doesn''t help at all'
        updatedAt: '2022-12-11T12:04:02.074Z'
      numEdits: 0
      reactions: []
    id: 6395c732ab3455c2bd67e97f
    type: comment
  author: nguyenmto
  content: '> Try installing xformers from https://github.com/facebookresearch/xformers.
    Once you confirm that works, then after you create the upscale pipeline and move
    it to the GPU, add the command pipeline.set_use_memory_efficient_attention_xformers(True).
    That should let you upscale a 512x512 image on a 24 gb GPU.


    Could you show the xformers output? This is mine

    ```

    A matching Triton is not available, some optimizations will not be enabled.

    Error caught was: No module named ''triton''

    xFormers 0.0.15.dev+3ea7307.d20221209

    memory_efficient_attention.flshatt:      available - requires GPU with compute
    capability 7.5+

    memory_efficient_attention.cutlass:      available

    memory_efficient_attention.small_k:      available

    memory_efficient_attention.tritonflashatt: not built

    memory_efficient_attention.ftriton_bflsh: not built

    swiglu.fused.p.cpp:                      available

    is_triton_available:                     False

    is_functorch_available:                  False

    pytorch.version:                         1.12.1

    pytorch.cuda:                            available

    gpu.compute_capability:                  7.5

    gpu.name:                                Tesla T4

    ```


    Besides, according to the docs, the correct syntax must be `pipeline.enable_xformers_memory_efficient_attention`,
    but it doesn''t help at all'
  created_at: 2022-12-11 12:04:02+00:00
  edited: false
  hidden: false
  id: 6395c732ab3455c2bd67e97f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
      fullname: Iron Harvy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ironharvy
      type: user
    createdAt: '2022-12-13T04:13:04.000Z'
    data:
      edited: false
      editors:
      - ironharvy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
          fullname: Iron Harvy
          isHf: false
          isPro: false
          name: ironharvy
          type: user
        html: "<p>Looks like this did the trick, thank you <span data-props=\"{&quot;user&quot;:&quot;mgobb&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mgobb\"\
          >@<span class=\"underline\">mgobb</span></a></span>\n\n\t</span></span>\
          \ </p>\n<pre>$ python -m xformers.info\n</pre>\n<pre>A matching Triton is\
          \ not available, some optimizations will not be enabled.\nError caught was:\
          \ No module named 'triton'\nxFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:\
          \      available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
          \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
          \                      available\nis_triton_available:                 \
          \    False\nis_functorch_available:                  False\npytorch.version:\
          \                         1.12.1\npytorch.cuda:                        \
          \    available\ngpu.compute_capability:                  8.9\ngpu.name:\
          \                                NVIDIA GeForce RTX 4090\n</pre>\n<p>So\
          \ I installed 'triton'</p>\n<pre>A matching Triton is not available, some\
          \ optimizations will not be enabled.\nError caught was: module 'triton.language'\
          \ has no attribute 'constexpr'\nA matching Triton is not available, some\
          \ optimizations will not be enabled.\nError caught was: module 'triton.language'\
          \ has no attribute 'constexpr'\nxFormers 0.0.15.dev+4c06c79.d20221206\n\
          memory_efficient_attention.flshatt:      available - requires GPU with compute\
          \ capability 7.5+\nmemory_efficient_attention.cutlass:      available\n\
          memory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
          \                      available\nis_triton_available:                 \
          \    False\nis_functorch_available:                  False\npytorch.version:\
          \                         1.12.1\npytorch.cuda:                        \
          \    available\ngpu.compute_capability:                  8.9\ngpu.name:\
          \                                NVIDIA GeForce RTX 4090\n</pre>\n\n<p>after\
          \ checking online looks like the solution for the <code>triton</code> issue\
          \ is to install a newer version as written in here <a rel=\"nofollow\" href=\"\
          https://github.com/openai/triton/issues/625\">https://github.com/openai/triton/issues/625</a></p>\n\
          <pre>$ pip install triton==2.0.0.dev20221120\n...\n$ python -m xformers.info\n\
          xFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:\
          \      available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
          \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
          \                      available\nis_triton_available:                 \
          \    True\nis_functorch_available:                  False\npytorch.version:\
          \                         1.12.1\npytorch.cuda:                        \
          \    available\ngpu.compute_capability:                  8.9\ngpu.name:\
          \                                NVIDIA GeForce RTX 4090\n</pre>\n<p>I also\
          \ added pipeline.set_use_memory_efficient_attention_xformers(True) to the\
          \ script and tried running it with 512x512  and got 2048x2048 image</p>\n"
        raw: "Looks like this did the trick, thank you @mgobb \n<pre>\n$ python -m\
          \ xformers.info\n</pre>\n<pre>\nA matching Triton is not available, some\
          \ optimizations will not be enabled.\nError caught was: No module named\
          \ 'triton'\nxFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:\
          \      available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
          \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
          \                      available\nis_triton_available:                 \
          \    False\nis_functorch_available:                  False\npytorch.version:\
          \                         1.12.1\npytorch.cuda:                        \
          \    available\ngpu.compute_capability:                  8.9\ngpu.name:\
          \                                NVIDIA GeForce RTX 4090\n</pre>\nSo I installed\
          \ 'triton'\n<pre>\nA matching Triton is not available, some optimizations\
          \ will not be enabled.\nError caught was: module 'triton.language' has no\
          \ attribute 'constexpr'\nA matching Triton is not available, some optimizations\
          \ will not be enabled.\nError caught was: module 'triton.language' has no\
          \ attribute 'constexpr'\nxFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:\
          \      available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
          \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
          \                      available\nis_triton_available:                 \
          \    False\nis_functorch_available:                  False\npytorch.version:\
          \                         1.12.1\npytorch.cuda:                        \
          \    available\ngpu.compute_capability:                  8.9\ngpu.name:\
          \                                NVIDIA GeForce RTX 4090\n</pre>\n\nafter\
          \ checking online looks like the solution for the `triton` issue is to install\
          \ a newer version as written in here https://github.com/openai/triton/issues/625\n\
          \n<pre>\n$ pip install triton==2.0.0.dev20221120\n...\n$ python -m xformers.info\n\
          xFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:\
          \      available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
          \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
          \                      available\nis_triton_available:                 \
          \    True\nis_functorch_available:                  False\npytorch.version:\
          \                         1.12.1\npytorch.cuda:                        \
          \    available\ngpu.compute_capability:                  8.9\ngpu.name:\
          \                                NVIDIA GeForce RTX 4090\n</pre>\nI also\
          \ added pipeline.set_use_memory_efficient_attention_xformers(True) to the\
          \ script and tried running it with 512x512  and got 2048x2048 image\n\n"
        updatedAt: '2022-12-13T04:13:04.880Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - sitatech
        - jessekoska
        - Ternaus
        - ylingfeng
      relatedEventId: 6397fbd02595904d1dec6563
    id: 6397fbd02595904d1dec6562
    type: comment
  author: ironharvy
  content: "Looks like this did the trick, thank you @mgobb \n<pre>\n$ python -m xformers.info\n\
    </pre>\n<pre>\nA matching Triton is not available, some optimizations will not\
    \ be enabled.\nError caught was: No module named 'triton'\nxFormers 0.0.15.dev+4c06c79.d20221206\n\
    memory_efficient_attention.flshatt:      available - requires GPU with compute\
    \ capability 7.5+\nmemory_efficient_attention.cutlass:      available\nmemory_efficient_attention.small_k:\
    \      available\nswiglu.fused.p.cpp:                      available\nis_triton_available:\
    \                     False\nis_functorch_available:                  False\n\
    pytorch.version:                         1.12.1\npytorch.cuda:               \
    \             available\ngpu.compute_capability:                  8.9\ngpu.name:\
    \                                NVIDIA GeForce RTX 4090\n</pre>\nSo I installed\
    \ 'triton'\n<pre>\nA matching Triton is not available, some optimizations will\
    \ not be enabled.\nError caught was: module 'triton.language' has no attribute\
    \ 'constexpr'\nA matching Triton is not available, some optimizations will not\
    \ be enabled.\nError caught was: module 'triton.language' has no attribute 'constexpr'\n\
    xFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:   \
    \   available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
    \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
    \                      available\nis_triton_available:                     False\n\
    is_functorch_available:                  False\npytorch.version:             \
    \            1.12.1\npytorch.cuda:                            available\ngpu.compute_capability:\
    \                  8.9\ngpu.name:                                NVIDIA GeForce\
    \ RTX 4090\n</pre>\n\nafter checking online looks like the solution for the `triton`\
    \ issue is to install a newer version as written in here https://github.com/openai/triton/issues/625\n\
    \n<pre>\n$ pip install triton==2.0.0.dev20221120\n...\n$ python -m xformers.info\n\
    xFormers 0.0.15.dev+4c06c79.d20221206\nmemory_efficient_attention.flshatt:   \
    \   available - requires GPU with compute capability 7.5+\nmemory_efficient_attention.cutlass:\
    \      available\nmemory_efficient_attention.small_k:      available\nswiglu.fused.p.cpp:\
    \                      available\nis_triton_available:                     True\n\
    is_functorch_available:                  False\npytorch.version:             \
    \            1.12.1\npytorch.cuda:                            available\ngpu.compute_capability:\
    \                  8.9\ngpu.name:                                NVIDIA GeForce\
    \ RTX 4090\n</pre>\nI also added pipeline.set_use_memory_efficient_attention_xformers(True)\
    \ to the script and tried running it with 512x512  and got 2048x2048 image\n\n"
  created_at: 2022-12-13 04:13:04+00:00
  edited: false
  hidden: false
  id: 6397fbd02595904d1dec6562
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
      fullname: Iron Harvy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ironharvy
      type: user
    createdAt: '2022-12-13T04:13:04.000Z'
    data:
      status: closed
    id: 6397fbd02595904d1dec6563
    type: status-change
  author: ironharvy
  created_at: 2022-12-13 04:13:04+00:00
  id: 6397fbd02595904d1dec6563
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e2f45771115372fb09e3b058ad545044.svg
      fullname: Vidyasagar Sadhu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vsadhu
      type: user
    createdAt: '2022-12-29T01:32:20.000Z'
    data:
      edited: true
      editors:
      - vsadhu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e2f45771115372fb09e3b058ad545044.svg
          fullname: Vidyasagar Sadhu
          isHf: false
          isPro: false
          name: vsadhu
          type: user
        html: "<p>Thanks I installed triton, einops, pyre-extensions to resolve the\
          \ triton error.<br><code>pip install triton==2.0.0.dev20221120</code><br><code>pip\
          \ install pyre-extensions==0.0.23</code><br><code>pip install einops</code></p>\n\
          <p>I am getting the same output as <span data-props=\"{&quot;user&quot;:&quot;ironharvy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ironharvy\"\
          >@<span class=\"underline\">ironharvy</span></a></span>\n\n\t</span></span>\
          \ 's above. </p>\n<pre><code>xFormers 0.0.15.dev395+git.7e05e2c\nmemory_efficient_attention.cutlassF:\
          \               available\nmemory_efficient_attention.cutlassB:        \
          \       available\nmemory_efficient_attention.flshattF:               available\n\
          memory_efficient_attention.flshattB:               available\nmemory_efficient_attention.smallkF:\
          \                available\nmemory_efficient_attention.smallkB:        \
          \        available\nmemory_efficient_attention.tritonflashattF:        available\n\
          memory_efficient_attention.tritonflashattB:        available\nswiglu.fused.p.cpp:\
          \                                available\nis_triton_available:       \
          \                        True\nis_functorch_available:                 \
          \           False\npytorch.version:                                   1.13.0\n\
          pytorch.cuda:                                      available\ngpu.compute_capability:\
          \                            7.5\ngpu.name:                            \
          \              NVIDIA GeForce RTX 2080 Ti\n</code></pre>\n<p>However, even\
          \ in my case, is_functorch_available is set to False. Is it possible to\
          \ resolve this as well?</p>\n"
        raw: "Thanks I installed triton, einops, pyre-extensions to resolve the triton\
          \ error. \n`pip install triton==2.0.0.dev20221120`\n`pip install pyre-extensions==0.0.23`\n\
          `pip install einops`\n\nI am getting the same output as @ironharvy 's above.\
          \ \n```\nxFormers 0.0.15.dev395+git.7e05e2c\nmemory_efficient_attention.cutlassF:\
          \               available\nmemory_efficient_attention.cutlassB:        \
          \       available\nmemory_efficient_attention.flshattF:               available\n\
          memory_efficient_attention.flshattB:               available\nmemory_efficient_attention.smallkF:\
          \                available\nmemory_efficient_attention.smallkB:        \
          \        available\nmemory_efficient_attention.tritonflashattF:        available\n\
          memory_efficient_attention.tritonflashattB:        available\nswiglu.fused.p.cpp:\
          \                                available\nis_triton_available:       \
          \                        True\nis_functorch_available:                 \
          \           False\npytorch.version:                                   1.13.0\n\
          pytorch.cuda:                                      available\ngpu.compute_capability:\
          \                            7.5\ngpu.name:                            \
          \              NVIDIA GeForce RTX 2080 Ti\n```\n\nHowever, even in my case,\
          \ is_functorch_available is set to False. Is it possible to resolve this\
          \ as well?"
        updatedAt: '2023-01-02T04:30:00.047Z'
      numEdits: 3
      reactions: []
    id: 63acee24d86d6389819d6dd5
    type: comment
  author: vsadhu
  content: "Thanks I installed triton, einops, pyre-extensions to resolve the triton\
    \ error. \n`pip install triton==2.0.0.dev20221120`\n`pip install pyre-extensions==0.0.23`\n\
    `pip install einops`\n\nI am getting the same output as @ironharvy 's above. \n\
    ```\nxFormers 0.0.15.dev395+git.7e05e2c\nmemory_efficient_attention.cutlassF:\
    \               available\nmemory_efficient_attention.cutlassB:              \
    \ available\nmemory_efficient_attention.flshattF:               available\nmemory_efficient_attention.flshattB:\
    \               available\nmemory_efficient_attention.smallkF:               \
    \ available\nmemory_efficient_attention.smallkB:                available\nmemory_efficient_attention.tritonflashattF:\
    \        available\nmemory_efficient_attention.tritonflashattB:        available\n\
    swiglu.fused.p.cpp:                                available\nis_triton_available:\
    \                               True\nis_functorch_available:                \
    \            False\npytorch.version:                                   1.13.0\n\
    pytorch.cuda:                                      available\ngpu.compute_capability:\
    \                            7.5\ngpu.name:                                  \
    \        NVIDIA GeForce RTX 2080 Ti\n```\n\nHowever, even in my case, is_functorch_available\
    \ is set to False. Is it possible to resolve this as well?"
  created_at: 2022-12-29 01:32:20+00:00
  edited: true
  hidden: false
  id: 63acee24d86d6389819d6dd5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d70028bb671a4fba8617fb687188d20e.svg
      fullname: dip deb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dipdeb
      type: user
    createdAt: '2023-01-03T13:49:41.000Z'
    data:
      edited: false
      editors:
      - dipdeb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d70028bb671a4fba8617fb687188d20e.svg
          fullname: dip deb
          isHf: false
          isPro: false
          name: dipdeb
          type: user
        html: '<p>Unable to install triton</p>

          <p>C:\Users\User&gt;pip install triton==2.0.0.dev20221120<br>Looking in
          indexes: <a rel="nofollow" href="https://pypi.org/simple">https://pypi.org/simple</a>,
          <a rel="nofollow" href="https://pypi.ngc.nvidia.com">https://pypi.ngc.nvidia.com</a><br>ERROR:
          Could not find a version that satisfies the requirement triton==2.0.0.dev20221120
          (from versions: none)<br>ERROR: No matching distribution found for triton==2.0.0.dev20221120</p>

          '
        raw: 'Unable to install triton


          C:\Users\User>pip install triton==2.0.0.dev20221120

          Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com

          ERROR: Could not find a version that satisfies the requirement triton==2.0.0.dev20221120
          (from versions: none)

          ERROR: No matching distribution found for triton==2.0.0.dev20221120'
        updatedAt: '2023-01-03T13:49:41.838Z'
      numEdits: 0
      reactions: []
    id: 63b432758c5f44c16904e92e
    type: comment
  author: dipdeb
  content: 'Unable to install triton


    C:\Users\User>pip install triton==2.0.0.dev20221120

    Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com

    ERROR: Could not find a version that satisfies the requirement triton==2.0.0.dev20221120
    (from versions: none)

    ERROR: No matching distribution found for triton==2.0.0.dev20221120'
  created_at: 2023-01-03 13:49:41+00:00
  edited: false
  hidden: false
  id: 63b432758c5f44c16904e92e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
      fullname: Iron Harvy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ironharvy
      type: user
    createdAt: '2023-01-12T00:11:14.000Z'
    data:
      edited: false
      editors:
      - ironharvy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/41472abd8137979312c7a12e82f3ef43.svg
          fullname: Iron Harvy
          isHf: false
          isPro: false
          name: ironharvy
          type: user
        html: '<p>Which python version are you using?</p>

          <blockquote>

          <p>python -V<br>Python 3.9.15</p>

          </blockquote>

          <blockquote>

          <p>pip --version<br>pip 22.3.1</p>

          </blockquote>

          '
        raw: 'Which python version are you using?


          > python -V

          Python 3.9.15


          > pip --version

          pip 22.3.1'
        updatedAt: '2023-01-12T00:11:14.393Z'
      numEdits: 0
      reactions: []
    id: 63bf502282f7306d0750db9a
    type: comment
  author: ironharvy
  content: 'Which python version are you using?


    > python -V

    Python 3.9.15


    > pip --version

    pip 22.3.1'
  created_at: 2023-01-12 00:11:14+00:00
  edited: false
  hidden: false
  id: 63bf502282f7306d0750db9a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d70028bb671a4fba8617fb687188d20e.svg
      fullname: dip deb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dipdeb
      type: user
    createdAt: '2023-01-14T09:02:59.000Z'
    data:
      edited: false
      editors:
      - dipdeb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d70028bb671a4fba8617fb687188d20e.svg
          fullname: dip deb
          isHf: false
          isPro: false
          name: dipdeb
          type: user
        html: '<p>Python 3.10.8<br>pip 22.2.2</p>

          '
        raw: 'Python 3.10.8

          pip 22.2.2'
        updatedAt: '2023-01-14T09:02:59.945Z'
      numEdits: 0
      reactions: []
    id: 63c26fc3aa5c9b8417054015
    type: comment
  author: dipdeb
  content: 'Python 3.10.8

    pip 22.2.2'
  created_at: 2023-01-14 09:02:59+00:00
  edited: false
  hidden: false
  id: 63c26fc3aa5c9b8417054015
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: stabilityai/stable-diffusion-x4-upscaler
repo_type: model
status: closed
target_branch: null
title: CUDA out of memory
