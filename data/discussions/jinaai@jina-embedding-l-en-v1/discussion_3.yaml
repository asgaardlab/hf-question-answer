!!python/object:huggingface_hub.community.DiscussionWithDetails
author: michaelfeil
conflicting_files: null
created_at: 2023-10-25 09:29:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-10-25T10:29:44.000Z'
    data:
      edited: false
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9330859184265137
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: '<p>When converting using CTranslate2,  this BERT models seems to break,
          thanks to a "GAMMA" parameter, that seems to differ from the other Bert
          implementations. From my side, this is super hard to troubleshoot. Any comment
          on that?</p>

          '
        raw: When converting using CTranslate2,  this BERT models seems to break,
          thanks to a "GAMMA" parameter, that seems to differ from the other Bert
          implementations. From my side, this is super hard to troubleshoot. Any comment
          on that?
        updatedAt: '2023-10-25T10:29:44.705Z'
      numEdits: 0
      reactions: []
    id: 6538ee1857a5ee1d303db9dd
    type: comment
  author: michaelfeil
  content: When converting using CTranslate2,  this BERT models seems to break, thanks
    to a "GAMMA" parameter, that seems to differ from the other Bert implementations.
    From my side, this is super hard to troubleshoot. Any comment on that?
  created_at: 2023-10-25 09:29:44+00:00
  edited: false
  hidden: false
  id: 6538ee1857a5ee1d303db9dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476ff2699a5ce743ccea3fc/zmFmF8tXXDaAGcl8RYiRr.jpeg?w=200&h=200&f=face
      fullname: "Michael G\xFCnther"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: michael-guenther
      type: user
    createdAt: '2023-10-26T11:57:42.000Z'
    data:
      edited: false
      editors:
      - michael-guenther
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8611369729042053
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6476ff2699a5ce743ccea3fc/zmFmF8tXXDaAGcl8RYiRr.jpeg?w=200&h=200&f=face
          fullname: "Michael G\xFCnther"
          isHf: false
          isPro: false
          name: michael-guenther
          type: user
        html: '<p>I never used CTranslate2, however, this model is based on T5  and
          not based on BERT. Besides, it is important, when loding this model with
          the transformers library, you need to explicitly use the <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L1872">T5Encoder
          class</a>. Our new models, e.g., <a href="https://huggingface.co/jinaai/jina-embeddings-v2-base-en">jina-embeddings-v2-base-en</a>
          use a backbone model which is more similar to BERT but also modified to
          support larger input text values. So here you have to make sure that it
          is using the our specific implementation (when using trasformer models this
          is done by setting <code>trust_remote_code=True</code> in the <code>from_pretrained</code>
          function. Otherwise it will fall back to a BERT implementation (and raise
          a warning) and the model will just produce random embeddings. I hope this
          helps you.</p>

          '
        raw: I never used CTranslate2, however, this model is based on T5  and not
          based on BERT. Besides, it is important, when loding this model with the
          transformers library, you need to explicitly use the [T5Encoder class](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L1872).
          Our new models, e.g., [jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en)
          use a backbone model which is more similar to BERT but also modified to
          support larger input text values. So here you have to make sure that it
          is using the our specific implementation (when using trasformer models this
          is done by setting `trust_remote_code=True` in the `from_pretrained` function.
          Otherwise it will fall back to a BERT implementation (and raise a warning)
          and the model will just produce random embeddings. I hope this helps you.
        updatedAt: '2023-10-26T11:57:42.351Z'
      numEdits: 0
      reactions: []
    id: 653a543648518d0b3a9e484f
    type: comment
  author: michael-guenther
  content: I never used CTranslate2, however, this model is based on T5  and not based
    on BERT. Besides, it is important, when loding this model with the transformers
    library, you need to explicitly use the [T5Encoder class](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L1872).
    Our new models, e.g., [jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en)
    use a backbone model which is more similar to BERT but also modified to support
    larger input text values. So here you have to make sure that it is using the our
    specific implementation (when using trasformer models this is done by setting
    `trust_remote_code=True` in the `from_pretrained` function. Otherwise it will
    fall back to a BERT implementation (and raise a warning) and the model will just
    produce random embeddings. I hope this helps you.
  created_at: 2023-10-26 10:57:42+00:00
  edited: false
  hidden: false
  id: 653a543648518d0b3a9e484f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: jinaai/jina-embedding-l-en-v1
repo_type: model
status: open
target_branch: null
title: Gamma parameter breaks conversion to CTranslate2
