!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ghthaker1955
conflicting_files: null
created_at: 2023-12-09 14:33:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/27f91ea801fd851b66e0b2d41fa4ba5e.svg
      fullname: Gautam Thaker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghthaker1955
      type: user
    createdAt: '2023-12-09T14:33:07.000Z'
    data:
      edited: true
      editors:
      - ghthaker1955
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9695611000061035
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/27f91ea801fd851b66e0b2d41fa4ba5e.svg
          fullname: Gautam Thaker
          isHf: false
          isPro: false
          name: ghthaker1955
          type: user
        html: '<p>Hi:</p>

          <p>I would like to run this model. I have been testing llama-2 on replicate.com
          as  a paid service (not expensive), but it seems this model may be easiest
          to run on "inference endpoint"?  I also have a 64 Gb linux desktop (with
          a RTX 3000 Mobile/Max-Q GPU) - would be happy to run on that if cloud options
          too difficult. </p>

          <p>Anyone has instructions on how to run this model in any of these manners?
          I have read <a rel="nofollow" href="https://github.com/epfLLM/meditron/blob/main/deployment/README.md">https://github.com/epfLLM/meditron/blob/main/deployment/README.md</a>
          and it gives directions about how to use this model with API calls from
          a client. However, I don''t know if I need to run this in the cloud or  if
          a 64 Gb Ubuntu desktop is sufficient. (I don''t have  a mac.)</p>

          <p>I need to access this model by making API calls, that is my usecase.</p>

          '
        raw: "Hi:\n\nI would like to run this model. I have been testing llama-2 on\
          \ replicate.com as  a paid service (not expensive), but it seems this model\
          \ may be easiest to run on \"inference endpoint\"?  I also have a 64 Gb\
          \ linux desktop (with a RTX 3000 Mobile/Max-Q GPU) - would be happy to run\
          \ on that if cloud options too difficult. \n\nAnyone has instructions on\
          \ how to run this model in any of these manners? I have read https://github.com/epfLLM/meditron/blob/main/deployment/README.md\
          \ and it gives directions about how to use this model with API calls from\
          \ a client. However, I don't know if I need to run this in the cloud or\
          \  if a 64 Gb Ubuntu desktop is sufficient. (I don't have  a mac.)\n\nI\
          \ need to access this model by making API calls, that is my usecase.\n"
        updatedAt: '2023-12-09T16:42:02.178Z'
      numEdits: 2
      reactions: []
    id: 65747aa3556a38db58946970
    type: comment
  author: ghthaker1955
  content: "Hi:\n\nI would like to run this model. I have been testing llama-2 on\
    \ replicate.com as  a paid service (not expensive), but it seems this model may\
    \ be easiest to run on \"inference endpoint\"?  I also have a 64 Gb linux desktop\
    \ (with a RTX 3000 Mobile/Max-Q GPU) - would be happy to run on that if cloud\
    \ options too difficult. \n\nAnyone has instructions on how to run this model\
    \ in any of these manners? I have read https://github.com/epfLLM/meditron/blob/main/deployment/README.md\
    \ and it gives directions about how to use this model with API calls from a client.\
    \ However, I don't know if I need to run this in the cloud or  if a 64 Gb Ubuntu\
    \ desktop is sufficient. (I don't have  a mac.)\n\nI need to access this model\
    \ by making API calls, that is my usecase.\n"
  created_at: 2023-12-09 14:33:07+00:00
  edited: true
  hidden: false
  id: 65747aa3556a38db58946970
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zywnbildRdLRgqAUXfbkl.jpeg?w=200&h=200&f=face
      fullname: Grzegorz Wierzowiecki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gwpl
      type: user
    createdAt: '2023-12-18T14:38:06.000Z'
    data:
      edited: false
      editors:
      - gwpl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8967306613922119
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/zywnbildRdLRgqAUXfbkl.jpeg?w=200&h=200&f=face
          fullname: Grzegorz Wierzowiecki
          isHf: false
          isPro: false
          name: gwpl
          type: user
        html: '<p><a rel="nofollow" href="https://ollama.ai/">https://ollama.ai/</a>
          makes it easy to run models. Maybe instructions how to run it with <a rel="nofollow"
          href="https://ollama.ai/">https://ollama.ai/</a> would be helpful ?</p>

          '
        raw: https://ollama.ai/ makes it easy to run models. Maybe instructions how
          to run it with https://ollama.ai/ would be helpful ?
        updatedAt: '2023-12-18T14:38:06.593Z'
      numEdits: 0
      reactions: []
    id: 6580594e365f7676d24dee52
    type: comment
  author: gwpl
  content: https://ollama.ai/ makes it easy to run models. Maybe instructions how
    to run it with https://ollama.ai/ would be helpful ?
  created_at: 2023-12-18 14:38:06+00:00
  edited: false
  hidden: false
  id: 6580594e365f7676d24dee52
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: epfl-llm/meditron-70b
repo_type: model
status: open
target_branch: null
title: How to run/access this model using API calls on either "inference endpoints",
  "replicate" or own 64Gb Linux desktop?
