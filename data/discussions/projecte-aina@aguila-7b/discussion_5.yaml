!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hythyt
conflicting_files: null
created_at: 2023-10-28 20:28:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5faad56418afcbbf44f6a99b504739df.svg
      fullname: hyt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hythyt
      type: user
    createdAt: '2023-10-28T21:28:52.000Z'
    data:
      edited: false
      editors:
      - hythyt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5424242615699768
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5faad56418afcbbf44f6a99b504739df.svg
          fullname: hyt
          isHf: false
          isPro: false
          name: hythyt
          type: user
        html: "<p>Hello, three days ago I was able to load the model on a Google Colab\
          \ with a T4 GPU (free account). Now, when loading the shards, it overloads\
          \ the RAM and doesn't manage to load it onto the GPU.<br>Thanks in advance.</p>\n\
          <p>This is the code that I-m using:</p>\n<p>import torch<br>from transformers\
          \ import pipeline, AutoTokenizer, AutoModelForCausalLM</p>\n<p>input_text\
          \ = \"El mercat del barri \xE9s fant\xE0stic, hi pots trobar\"</p>\n<p>device\
          \ = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"<br>model_id  =\
          \ \"projecte-aina/aguila-7b\"<br>tokenizer = AutoTokenizer.from_pretrained(model_id)<br>print(device)<br>generator\
          \ = pipeline(<br>    \"text-generation\",<br>    model=model_id,<br>   \
          \ tokenizer=tokenizer,<br>    device_map=device,<br>    trust_remote_code=True,<br>\
          \    torch_dtype=torch.bfloat16,<br>)<br>generation = generator(<br>   \
          \ input_text,<br>    do_sample=True,<br>    top_k=10,<br>    max_new_tokens=10,<br>\
          \    eos_token_id=tokenizer.eos_token_id,<br>)<br>print(f\"Result: {generation[0]['generated_text']}\"\
          )</p>\n"
        raw: "Hello, three days ago I was able to load the model on a Google Colab\
          \ with a T4 GPU (free account). Now, when loading the shards, it overloads\
          \ the RAM and doesn't manage to load it onto the GPU.\r\nThanks in advance.\r\
          \n\r\nThis is the code that I-m using:\r\n\r\nimport torch\r\nfrom transformers\
          \ import pipeline, AutoTokenizer, AutoModelForCausalLM\r\n\r\ninput_text\
          \ = \"El mercat del barri \xE9s fant\xE0stic, hi pots trobar\"\r\n\r\ndevice\
          \ = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\nmodel_id  =\
          \ \"projecte-aina/aguila-7b\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\
          \nprint(device)\r\ngenerator = pipeline(\r\n    \"text-generation\",\r\n\
          \    model=model_id,\r\n    tokenizer=tokenizer,\r\n    device_map=device,\r\
          \n    trust_remote_code=True,\r\n    torch_dtype=torch.bfloat16,\r\n)\r\n\
          generation = generator(\r\n    input_text,\r\n    do_sample=True,\r\n  \
          \  top_k=10,\r\n    max_new_tokens=10,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
          \n)\r\nprint(f\"Result: {generation[0]['generated_text']}\")"
        updatedAt: '2023-10-28T21:28:52.917Z'
      numEdits: 0
      reactions: []
    id: 653d7d14aa1f487614cd4e30
    type: comment
  author: hythyt
  content: "Hello, three days ago I was able to load the model on a Google Colab with\
    \ a T4 GPU (free account). Now, when loading the shards, it overloads the RAM\
    \ and doesn't manage to load it onto the GPU.\r\nThanks in advance.\r\n\r\nThis\
    \ is the code that I-m using:\r\n\r\nimport torch\r\nfrom transformers import\
    \ pipeline, AutoTokenizer, AutoModelForCausalLM\r\n\r\ninput_text = \"El mercat\
    \ del barri \xE9s fant\xE0stic, hi pots trobar\"\r\n\r\ndevice = \"cuda:0\" if\
    \ torch.cuda.is_available() else \"cpu\"\r\nmodel_id  = \"projecte-aina/aguila-7b\"\
    \r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\nprint(device)\r\ngenerator\
    \ = pipeline(\r\n    \"text-generation\",\r\n    model=model_id,\r\n    tokenizer=tokenizer,\r\
    \n    device_map=device,\r\n    trust_remote_code=True,\r\n    torch_dtype=torch.bfloat16,\r\
    \n)\r\ngeneration = generator(\r\n    input_text,\r\n    do_sample=True,\r\n \
    \   top_k=10,\r\n    max_new_tokens=10,\r\n    eos_token_id=tokenizer.eos_token_id,\r\
    \n)\r\nprint(f\"Result: {generation[0]['generated_text']}\")"
  created_at: 2023-10-28 20:28:52+00:00
  edited: false
  hidden: false
  id: 653d7d14aa1f487614cd4e30
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: projecte-aina/aguila-7b
repo_type: model
status: open
target_branch: null
title: RAM crash when loading shards
