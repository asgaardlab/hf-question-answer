!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fikavec
conflicting_files: null
created_at: 2023-04-16 14:26:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
      fullname: fikavec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fikavec
      type: user
    createdAt: '2023-04-16T15:26:28.000Z'
    data:
      edited: false
      editors:
      - fikavec
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
          fullname: fikavec
          isHf: false
          isPro: false
          name: fikavec
          type: user
        html: '<p>I don''t have a GPU, but "llama_13b_ru_turbo_alpaca_lora_llamacpp"
          I really liked both the speed and the answers on CPU. I would like to look
          at Saiga, can you tell me how to megre this model with LLaMA 13B and then
          convert it to ggml format and quantize to 4bits without GPU? Thank you very
          much for the models!</p>

          '
        raw: I don't have a GPU, but "llama_13b_ru_turbo_alpaca_lora_llamacpp" I really
          liked both the speed and the answers on CPU. I would like to look at Saiga,
          can you tell me how to megre this model with LLaMA 13B and then convert
          it to ggml format and quantize to 4bits without GPU? Thank you very much
          for the models!
        updatedAt: '2023-04-16T15:26:28.357Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - fikavec
    id: 643c13a4b409fef15e08d786
    type: comment
  author: fikavec
  content: I don't have a GPU, but "llama_13b_ru_turbo_alpaca_lora_llamacpp" I really
    liked both the speed and the answers on CPU. I would like to look at Saiga, can
    you tell me how to megre this model with LLaMA 13B and then convert it to ggml
    format and quantize to 4bits without GPU? Thank you very much for the models!
  created_at: 2023-04-16 14:26:28+00:00
  edited: false
  hidden: false
  id: 643c13a4b409fef15e08d786
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-04-16T23:55:20.000Z'
    data:
      edited: false
      editors:
      - IlyaGusev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: '<p>Hi!<br>For previous models, the procedure was this:</p>

          <ol>

          <li>Converting to a native format with <a rel="nofollow" href="https://github.com/tloen/alpaca-lora/blob/main/export_state_dict_checkpoint.py">https://github.com/tloen/alpaca-lora/blob/main/export_state_dict_checkpoint.py</a></li>

          <li>Converting to ggml fp16 with convert.py from <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></li>

          <li>Quantization with "quantize" binary from <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></li>

          </ol>

          <p>However, I had some strange problems with the latest version of a "llama_13b_ru_turbo_alpaca_lora"
          model, and I''m not sure it will work as expected for this model.<br>I do
          have plans to test it myself, but probably only after releasing versions
          with OpenAssistant data integration.</p>

          '
        raw: 'Hi!

          For previous models, the procedure was this:

          1. Converting to a native format with https://github.com/tloen/alpaca-lora/blob/main/export_state_dict_checkpoint.py

          2. Converting to ggml fp16 with convert.py from https://github.com/ggerganov/llama.cpp

          3. Quantization with "quantize" binary from https://github.com/ggerganov/llama.cpp


          However, I had some strange problems with the latest version of a "llama_13b_ru_turbo_alpaca_lora"
          model, and I''m not sure it will work as expected for this model.

          I do have plans to test it myself, but probably only after releasing versions
          with OpenAssistant data integration.'
        updatedAt: '2023-04-16T23:55:20.190Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - fikavec
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fikavec
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ColCh
    id: 643c8ae82168686700421b3e
    type: comment
  author: IlyaGusev
  content: 'Hi!

    For previous models, the procedure was this:

    1. Converting to a native format with https://github.com/tloen/alpaca-lora/blob/main/export_state_dict_checkpoint.py

    2. Converting to ggml fp16 with convert.py from https://github.com/ggerganov/llama.cpp

    3. Quantization with "quantize" binary from https://github.com/ggerganov/llama.cpp


    However, I had some strange problems with the latest version of a "llama_13b_ru_turbo_alpaca_lora"
    model, and I''m not sure it will work as expected for this model.

    I do have plans to test it myself, but probably only after releasing versions
    with OpenAssistant data integration.'
  created_at: 2023-04-16 22:55:20+00:00
  edited: false
  hidden: false
  id: 643c8ae82168686700421b3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
      fullname: fikavec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fikavec
      type: user
    createdAt: '2023-04-17T19:37:54.000Z'
    data:
      edited: false
      editors:
      - fikavec
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6cba9a99b3cf3aa76d73bbe23de03750.svg
          fullname: fikavec
          isHf: false
          isPro: false
          name: fikavec
          type: user
        html: "<p>Thanks, I was able to merge and convert \"saiga_13b_lora\" to fp16\
          \ (25 Gb resulting .bin model) and fp32 (55 Gb resulting .bin model).<br>When\
          \ testing, it turned out that if I ask my questions directly, without prompt,\
          \ the results are unstable and much worse than using prompt.<br><code>&lt;start&gt;system\\\
          n\u0422\u044B \u2014 \u0421\u0430\u0439\u0433\u0430, \u0440\u0443\u0441\u0441\
          \u043A\u043E\u044F\u0437\u044B\u0447\u043D\u044B\u0439 \u0430\u0432\u0442\
          \u043E\u043C\u0430\u0442\u0438\u0447\u0435\u0441\u043A\u0438\u0439 \u0430\
          \u0441\u0441\u0438\u0441\u0442\u0435\u043D\u0442. \u0422\u044B \u0440\u0430\
          \u0437\u0433\u043E\u0432\u0430\u0440\u0438\u0432\u0430\u0435\u0448\u044C\
          \ \u0441 \u043B\u044E\u0434\u044C\u043C\u0438 \u0438 \u043F\u043E\u043C\u043E\
          \u0433\u0430\u0435\u0448\u044C \u0438\u043C. &lt;end&gt;\\n&lt;start&gt;user\\\
          nMY QUESTION HERE&lt;end&gt;\\n&lt;start&gt;</code><br>However, if I use\
          \ the above recommended prompt fromat (with - MY QUESTION HERE), the model\
          \ does not stop after answering my question and continues up to n_predict\
          \ (\"max_new_tokens\": 1024 from generation_config.json) to generate random\
          \ tasks and dialogs in the format: </p>\n<pre><code>&lt;start&gt;user\\\
          nRANDOM HERE&lt;end&gt;\\n\n&lt;start&gt;bot\\nRANDOM HERE&lt;end&gt;\\\
          n\n...\nto 1024 tokens\n</code></pre>\n<p>Is there any way to deal with\
          \ this so that the model completes generation after answering my question\
          \ and does not continue to generate a random (fictional by model) continuation\
          \ of the dialogue?<br>The model itself seemed more knowledgeable than \"\
          llama_13b_ru_turbo_alpaca_lora_llamacpp\", but the latter works very well,\
          \ recommended on the resource <a rel=\"nofollow\" href=\"https://github.com/tloen/alpaca-lora\"\
          >https://github.com/tloen/alpaca-lora</a> (it's strange that only the 7b\
          \ version, not the 13b),  and there is no problem with excessive generation,\
          \ although max_new_tokens is not enough to generate code or long answers,\
          \ at least the answers break off in mid-sentence.</p>\n"
        raw: "Thanks, I was able to merge and convert \"saiga_13b_lora\" to fp16 (25\
          \ Gb resulting .bin model) and fp32 (55 Gb resulting .bin model). \nWhen\
          \ testing, it turned out that if I ask my questions directly, without prompt,\
          \ the results are unstable and much worse than using prompt.\n`\n<start>system\\\
          n\u0422\u044B \u2014 \u0421\u0430\u0439\u0433\u0430, \u0440\u0443\u0441\u0441\
          \u043A\u043E\u044F\u0437\u044B\u0447\u043D\u044B\u0439 \u0430\u0432\u0442\
          \u043E\u043C\u0430\u0442\u0438\u0447\u0435\u0441\u043A\u0438\u0439 \u0430\
          \u0441\u0441\u0438\u0441\u0442\u0435\u043D\u0442. \u0422\u044B \u0440\u0430\
          \u0437\u0433\u043E\u0432\u0430\u0440\u0438\u0432\u0430\u0435\u0448\u044C\
          \ \u0441 \u043B\u044E\u0434\u044C\u043C\u0438 \u0438 \u043F\u043E\u043C\u043E\
          \u0433\u0430\u0435\u0448\u044C \u0438\u043C. <end>\\n<start>user\\nMY QUESTION\
          \ HERE<end>\\n<start>\n`\nHowever, if I use the above recommended prompt\
          \ fromat (with - MY QUESTION HERE), the model does not stop after answering\
          \ my question and continues up to n_predict (\"max_new_tokens\": 1024 from\
          \ generation_config.json) to generate random tasks and dialogs in the format:\
          \ \n```\n<start>user\\nRANDOM HERE<end>\\n\n<start>bot\\nRANDOM HERE<end>\\\
          n\n...\nto 1024 tokens\n```\nIs there any way to deal with this so that\
          \ the model completes generation after answering my question and does not\
          \ continue to generate a random (fictional by model) continuation of the\
          \ dialogue?\nThe model itself seemed more knowledgeable than \"llama_13b_ru_turbo_alpaca_lora_llamacpp\"\
          , but the latter works very well, recommended on the resource https://github.com/tloen/alpaca-lora\
          \ (it's strange that only the 7b version, not the 13b),  and there is no\
          \ problem with excessive generation, although max_new_tokens is not enough\
          \ to generate code or long answers, at least the answers break off in mid-sentence."
        updatedAt: '2023-04-17T19:37:54.168Z'
      numEdits: 0
      reactions: []
    id: 643da012aa3c5728ed651cc4
    type: comment
  author: fikavec
  content: "Thanks, I was able to merge and convert \"saiga_13b_lora\" to fp16 (25\
    \ Gb resulting .bin model) and fp32 (55 Gb resulting .bin model). \nWhen testing,\
    \ it turned out that if I ask my questions directly, without prompt, the results\
    \ are unstable and much worse than using prompt.\n`\n<start>system\\n\u0422\u044B\
    \ \u2014 \u0421\u0430\u0439\u0433\u0430, \u0440\u0443\u0441\u0441\u043A\u043E\u044F\
    \u0437\u044B\u0447\u043D\u044B\u0439 \u0430\u0432\u0442\u043E\u043C\u0430\u0442\
    \u0438\u0447\u0435\u0441\u043A\u0438\u0439 \u0430\u0441\u0441\u0438\u0441\u0442\
    \u0435\u043D\u0442. \u0422\u044B \u0440\u0430\u0437\u0433\u043E\u0432\u0430\u0440\
    \u0438\u0432\u0430\u0435\u0448\u044C \u0441 \u043B\u044E\u0434\u044C\u043C\u0438\
    \ \u0438 \u043F\u043E\u043C\u043E\u0433\u0430\u0435\u0448\u044C \u0438\u043C.\
    \ <end>\\n<start>user\\nMY QUESTION HERE<end>\\n<start>\n`\nHowever, if I use\
    \ the above recommended prompt fromat (with - MY QUESTION HERE), the model does\
    \ not stop after answering my question and continues up to n_predict (\"max_new_tokens\"\
    : 1024 from generation_config.json) to generate random tasks and dialogs in the\
    \ format: \n```\n<start>user\\nRANDOM HERE<end>\\n\n<start>bot\\nRANDOM HERE<end>\\\
    n\n...\nto 1024 tokens\n```\nIs there any way to deal with this so that the model\
    \ completes generation after answering my question and does not continue to generate\
    \ a random (fictional by model) continuation of the dialogue?\nThe model itself\
    \ seemed more knowledgeable than \"llama_13b_ru_turbo_alpaca_lora_llamacpp\",\
    \ but the latter works very well, recommended on the resource https://github.com/tloen/alpaca-lora\
    \ (it's strange that only the 7b version, not the 13b),  and there is no problem\
    \ with excessive generation, although max_new_tokens is not enough to generate\
    \ code or long answers, at least the answers break off in mid-sentence."
  created_at: 2023-04-17 18:37:54+00:00
  edited: false
  hidden: false
  id: 643da012aa3c5728ed651cc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-04-17T21:24:50.000Z'
    data:
      edited: false
      editors:
      - IlyaGusev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: '<blockquote>

          <p>Is there any way to deal with this so that the model completes generation
          after answering my question and does not continue to generate a random (fictional
          by model) continuation of the dialogue?</p>

          </blockquote>

          <p>The current version requires an EOS token different from the default
          one. You should refer to <a rel="nofollow" href="https://colab.research.google.com/drive/1WuoSlKMdGB-D_OQrHOFta13Ph1--Eq7L">Colab</a>,
          there is an example of how to use it. I''ll fix it in the next version.</p>

          '
        raw: '>Is there any way to deal with this so that the model completes generation
          after answering my question and does not continue to generate a random (fictional
          by model) continuation of the dialogue?


          The current version requires an EOS token different from the default one.
          You should refer to [Colab](https://colab.research.google.com/drive/1WuoSlKMdGB-D_OQrHOFta13Ph1--Eq7L),
          there is an example of how to use it. I''ll fix it in the next version.'
        updatedAt: '2023-04-17T21:24:50.660Z'
      numEdits: 0
      reactions: []
    id: 643db92227c06bb2bf7cfcb6
    type: comment
  author: IlyaGusev
  content: '>Is there any way to deal with this so that the model completes generation
    after answering my question and does not continue to generate a random (fictional
    by model) continuation of the dialogue?


    The current version requires an EOS token different from the default one. You
    should refer to [Colab](https://colab.research.google.com/drive/1WuoSlKMdGB-D_OQrHOFta13Ph1--Eq7L),
    there is an example of how to use it. I''ll fix it in the next version.'
  created_at: 2023-04-17 20:24:50+00:00
  edited: false
  hidden: false
  id: 643db92227c06bb2bf7cfcb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-05-15T22:32:38.000Z'
    data:
      edited: false
      editors:
      - IlyaGusev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: '<p><a href="https://huggingface.co/IlyaGusev/saiga_13b_lora_llamacpp">https://huggingface.co/IlyaGusev/saiga_13b_lora_llamacpp</a></p>

          '
        raw: https://huggingface.co/IlyaGusev/saiga_13b_lora_llamacpp
        updatedAt: '2023-05-15T22:32:38.751Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - scepter
      relatedEventId: 6462b306cce92c7d882fc3e5
    id: 6462b306cce92c7d882fc3e4
    type: comment
  author: IlyaGusev
  content: https://huggingface.co/IlyaGusev/saiga_13b_lora_llamacpp
  created_at: 2023-05-15 21:32:38+00:00
  edited: false
  hidden: false
  id: 6462b306cce92c7d882fc3e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-05-15T22:32:38.000Z'
    data:
      status: closed
    id: 6462b306cce92c7d882fc3e5
    type: status-change
  author: IlyaGusev
  created_at: 2023-05-15 21:32:38+00:00
  id: 6462b306cce92c7d882fc3e5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: IlyaGusev/saiga_13b_lora
repo_type: model
status: closed
target_branch: null
title: Any plans to release Llama.cpp compatible Saiga version?
