!!python/object:huggingface_hub.community.DiscussionWithDetails
author: loretoparisi
conflicting_files: null
created_at: 2022-09-07 09:40:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2022-09-07T10:40:16.000Z'
    data:
      edited: false
      editors:
      - loretoparisi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: '<p>How to run inference with ONNX runtime?</p>

          '
        raw: How to run inference with ONNX runtime?
        updatedAt: '2022-09-07T10:40:16.832Z'
      numEdits: 0
      reactions: []
    id: 631875103b76a42d80b62396
    type: comment
  author: loretoparisi
  content: How to run inference with ONNX runtime?
  created_at: 2022-09-07 09:40:16+00:00
  edited: false
  hidden: false
  id: 631875103b76a42d80b62396
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4bd341cbfbf5756993efa214ae491742.svg
      fullname: Sergei Belousov
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bes-dev
      type: user
    createdAt: '2022-09-07T12:36:12.000Z'
    data:
      edited: false
      editors:
      - bes-dev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4bd341cbfbf5756993efa214ae491742.svg
          fullname: Sergei Belousov
          isHf: false
          isPro: false
          name: bes-dev
          type: user
        html: '<p>you can try our experimental code here: <a rel="nofollow" href="https://github.com/bes-dev/stable_diffusion.openvino/tree/sb/onnxruntime">https://github.com/bes-dev/stable_diffusion.openvino/tree/sb/onnxruntime</a><br>But
          we don''t recommend to use it</p>

          '
        raw: 'you can try our experimental code here: https://github.com/bes-dev/stable_diffusion.openvino/tree/sb/onnxruntime

          But we don''t recommend to use it'
        updatedAt: '2022-09-07T12:36:12.614Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - loretoparisi
    id: 6318903c4754015938f146f8
    type: comment
  author: bes-dev
  content: 'you can try our experimental code here: https://github.com/bes-dev/stable_diffusion.openvino/tree/sb/onnxruntime

    But we don''t recommend to use it'
  created_at: 2022-09-07 11:36:12+00:00
  edited: false
  hidden: false
  id: 6318903c4754015938f146f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84097d289e36187e24eb088e5976a52d.svg
      fullname: Fernando
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ferqui
      type: user
    createdAt: '2022-09-10T10:57:38.000Z'
    data:
      edited: false
      editors:
      - ferqui
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84097d289e36187e24eb088e5976a52d.svg
          fullname: Fernando
          isHf: false
          isPro: false
          name: ferqui
          type: user
        html: "<p>Hi, I am trying to convert the ONNX model to a Tensorflow model,\
          \ to try to use it in tensorflow lite. But I get the following error with\
          \ both vae_encoder and unet:</p>\n<pre><code>File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/backend_tf_module.py\"\
          , line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n\
          \    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/backend.py\"\
          , line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node,\
          \ tensor_dict=tensor_dict, strict=strict)\n    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/handler.py\"\
          , line 59, in handle  *\n        return ver_handle(node, **kwargs)\n   \
          \ File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/backend/conv.py\"\
          , line 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"\
          ])\n    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/backend/conv_mixin.py\"\
          , line 30, in conv  *\n        x_rank = len(x.get_shape())\n\n    ValueError:\
          \ Cannot take the length of shape with unknown rank.\n</code></pre>\n<p>From\
          \ what I have been seeing debugging the code in the case of the encoder,\
          \ the input \"input.240\" of the convolution \"Conv_465\" is not an array.<br><code>Tensor(\"\
          onnx_tf_prefix_Mul_464:0\", dtype=float32)</code></p>\n<p>I attach the code\
          \ to reproduce it:</p>\n<pre><code>import onnx\nfrom onnx_tf.backend import\
          \ prepare\n\nonnx_model = onnx.load(\"stable-diffusion-v1-4-onnx/vae_encoder.onnx\"\
          )  # load onnx model\nprint(onnx.helper.printable_graph(onnx_model.graph))\n\
          \ntf_rep = prepare(onnx_model)  # prepare tf representation\ntf_rep.export_graph(\"\
          models/vae_encoder\")  # export the model\n</code></pre>\n<p>Do you have\
          \ any idea why this might be or how it could be solved? Thanks!</p>\n"
        raw: "Hi, I am trying to convert the ONNX model to a Tensorflow model, to\
          \ try to use it in tensorflow lite. But I get the following error with both\
          \ vae_encoder and unet:\n\n```\nFile \"/usr/local/lib/python3.7/dist-packages/onnx_tf/backend_tf_module.py\"\
          , line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n\
          \    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/backend.py\"\
          , line 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node,\
          \ tensor_dict=tensor_dict, strict=strict)\n    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/handler.py\"\
          , line 59, in handle  *\n        return ver_handle(node, **kwargs)\n   \
          \ File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/backend/conv.py\"\
          , line 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"\
          ])\n    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/backend/conv_mixin.py\"\
          , line 30, in conv  *\n        x_rank = len(x.get_shape())\n\n    ValueError:\
          \ Cannot take the length of shape with unknown rank.\n```\nFrom what I have\
          \ been seeing debugging the code in the case of the encoder, the input \"\
          input.240\" of the convolution \"Conv_465\" is not an array.\n```Tensor(\"\
          onnx_tf_prefix_Mul_464:0\", dtype=float32)```\n\nI attach the code to reproduce\
          \ it:\n```\nimport onnx\nfrom onnx_tf.backend import prepare\n\nonnx_model\
          \ = onnx.load(\"stable-diffusion-v1-4-onnx/vae_encoder.onnx\")  # load onnx\
          \ model\nprint(onnx.helper.printable_graph(onnx_model.graph))\n\ntf_rep\
          \ = prepare(onnx_model)  # prepare tf representation\ntf_rep.export_graph(\"\
          models/vae_encoder\")  # export the model\n```\nDo you have any idea why\
          \ this might be or how it could be solved? Thanks!"
        updatedAt: '2022-09-10T10:57:38.702Z'
      numEdits: 0
      reactions: []
    id: 631c6da244503b72277c5d85
    type: comment
  author: ferqui
  content: "Hi, I am trying to convert the ONNX model to a Tensorflow model, to try\
    \ to use it in tensorflow lite. But I get the following error with both vae_encoder\
    \ and unet:\n\n```\nFile \"/usr/local/lib/python3.7/dist-packages/onnx_tf/backend_tf_module.py\"\
    , line 99, in __call__  *\n        output_ops = self.backend._onnx_node_to_tensorflow_op(onnx_node,\n\
    \    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/backend.py\", line\
    \ 347, in _onnx_node_to_tensorflow_op  *\n        return handler.handle(node,\
    \ tensor_dict=tensor_dict, strict=strict)\n    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/handler.py\"\
    , line 59, in handle  *\n        return ver_handle(node, **kwargs)\n    File \"\
    /usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/backend/conv.py\", line\
    \ 15, in version_11  *\n        return cls.conv(node, kwargs[\"tensor_dict\"])\n\
    \    File \"/usr/local/lib/python3.7/dist-packages/onnx_tf/handlers/backend/conv_mixin.py\"\
    , line 30, in conv  *\n        x_rank = len(x.get_shape())\n\n    ValueError:\
    \ Cannot take the length of shape with unknown rank.\n```\nFrom what I have been\
    \ seeing debugging the code in the case of the encoder, the input \"input.240\"\
    \ of the convolution \"Conv_465\" is not an array.\n```Tensor(\"onnx_tf_prefix_Mul_464:0\"\
    , dtype=float32)```\n\nI attach the code to reproduce it:\n```\nimport onnx\n\
    from onnx_tf.backend import prepare\n\nonnx_model = onnx.load(\"stable-diffusion-v1-4-onnx/vae_encoder.onnx\"\
    )  # load onnx model\nprint(onnx.helper.printable_graph(onnx_model.graph))\n\n\
    tf_rep = prepare(onnx_model)  # prepare tf representation\ntf_rep.export_graph(\"\
    models/vae_encoder\")  # export the model\n```\nDo you have any idea why this\
    \ might be or how it could be solved? Thanks!"
  created_at: 2022-09-10 09:57:38+00:00
  edited: false
  hidden: false
  id: 631c6da244503b72277c5d85
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bes-dev/stable-diffusion-v1-4-onnx
repo_type: model
status: open
target_branch: null
title: Inference
