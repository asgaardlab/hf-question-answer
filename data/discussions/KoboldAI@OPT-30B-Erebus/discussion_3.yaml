!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheFairyMan
conflicting_files: null
created_at: 2023-05-11 00:41:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-05-11T01:41:27.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<p>Sorry for the request, but I do not know anybody with a good enough
          computer to ask this for, since this model by default requires around 64GB
          of VRAM and even if you put the remaining requirements on the CPU, it runs
          super slow on in comparison and may generate less than half a token per
          second.<br>Even a 4090 will have trouble running it as states in the other
          discussion, thus why I ask if somebody could help and make it or if there
          is already one available that we didn''t find.</p>

          '
        raw: "Sorry for the request, but I do not know anybody with a good enough\
          \ computer to ask this for, since this model by default requires around\
          \ 64GB of VRAM and even if you put the remaining requirements on the CPU,\
          \ it runs super slow on in comparison and may generate less than half a\
          \ token per second.\r\nEven a 4090 will have trouble running it as states\
          \ in the other discussion, thus why I ask if somebody could help and make\
          \ it or if there is already one available that we didn't find."
        updatedAt: '2023-05-11T01:41:27.371Z'
      numEdits: 0
      reactions: []
    id: 645c47c77848314a460cfff8
    type: comment
  author: TheFairyMan
  content: "Sorry for the request, but I do not know anybody with a good enough computer\
    \ to ask this for, since this model by default requires around 64GB of VRAM and\
    \ even if you put the remaining requirements on the CPU, it runs super slow on\
    \ in comparison and may generate less than half a token per second.\r\nEven a\
    \ 4090 will have trouble running it as states in the other discussion, thus why\
    \ I ask if somebody could help and make it or if there is already one available\
    \ that we didn't find."
  created_at: 2023-05-11 00:41:27+00:00
  edited: false
  hidden: false
  id: 645c47c77848314a460cfff8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: KoboldAI/OPT-30B-Erebus
repo_type: model
status: open
target_branch: null
title: (Request) Anybody able to make a 4bit quantized version
