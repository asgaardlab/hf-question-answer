!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Kizna
conflicting_files: null
created_at: 2023-01-30 18:14:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df8b9f9df56641118ee89a71184fc2bf.svg
      fullname: Kizna towork
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kizna
      type: user
    createdAt: '2023-01-30T18:14:52.000Z'
    data:
      edited: false
      editors:
      - Kizna
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df8b9f9df56641118ee89a71184fc2bf.svg
          fullname: Kizna towork
          isHf: false
          isPro: false
          name: Kizna
          type: user
        html: '<p>I''m looking to put together a rig at home that can handle 30b parameters
          models and below, I know this would mean a pretty penny in terms of hardware.
          So I wanted to ask somewhere that I might actually get a straight answer.
          </p>

          <p>Would used k, p, and m series Tesla GPU''s be suitable for such? And
          how much VRAM would i be looking at to run a 30b model?</p>

          '
        raw: "I'm looking to put together a rig at home that can handle 30b parameters\
          \ models and below, I know this would mean a pretty penny in terms of hardware.\
          \ So I wanted to ask somewhere that I might actually get a straight answer.\
          \ \r\n\r\nWould used k, p, and m series Tesla GPU's be suitable for such?\
          \ And how much VRAM would i be looking at to run a 30b model?"
        updatedAt: '2023-01-30T18:14:52.142Z'
      numEdits: 0
      reactions: []
    id: 63d8091c635499f2163f9532
    type: comment
  author: Kizna
  content: "I'm looking to put together a rig at home that can handle 30b parameters\
    \ models and below, I know this would mean a pretty penny in terms of hardware.\
    \ So I wanted to ask somewhere that I might actually get a straight answer. \r\
    \n\r\nWould used k, p, and m series Tesla GPU's be suitable for such? And how\
    \ much VRAM would i be looking at to run a 30b model?"
  created_at: 2023-01-30 18:14:52+00:00
  edited: false
  hidden: false
  id: 63d8091c635499f2163f9532
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649241748232-61c47ffe6f3fbe5eef28f408.png?w=200&h=200&f=face
      fullname: Julius ter Pelkwijk
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mrseeker87
      type: user
    createdAt: '2023-01-30T18:29:43.000Z'
    data:
      edited: true
      editors:
      - mrseeker87
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1649241748232-61c47ffe6f3fbe5eef28f408.png?w=200&h=200&f=face
          fullname: Julius ter Pelkwijk
          isHf: false
          isPro: false
          name: mrseeker87
          type: user
        html: '<p>Looking at least 60Gb of VRAM in 16-bit mode. I ran it on the Horde
          with 2x A6000 and that was good enough...</p>

          '
        raw: Looking at least 60Gb of VRAM in 16-bit mode. I ran it on the Horde with
          2x A6000 and that was good enough...
        updatedAt: '2023-01-30T18:29:57.119Z'
      numEdits: 1
      reactions: []
    id: 63d80c975ccb2bccedb925e5
    type: comment
  author: mrseeker87
  content: Looking at least 60Gb of VRAM in 16-bit mode. I ran it on the Horde with
    2x A6000 and that was good enough...
  created_at: 2023-01-30 18:29:43+00:00
  edited: true
  hidden: false
  id: 63d80c975ccb2bccedb925e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df8b9f9df56641118ee89a71184fc2bf.svg
      fullname: Kizna towork
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kizna
      type: user
    createdAt: '2023-01-30T18:32:59.000Z'
    data:
      edited: false
      editors:
      - Kizna
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df8b9f9df56641118ee89a71184fc2bf.svg
          fullname: Kizna towork
          isHf: false
          isPro: false
          name: Kizna
          type: user
        html: '<blockquote>

          <p>Looking at least 60Gb of VRAM in 16-bit mode. I ran it on the Horde with
          2x A6000 and that was good enough...</p>

          </blockquote>

          <p>That''s a actually not all that much, if earlier series Tesla cards can
          handle it that is. Trying to dip my feet without going huge first.</p>

          '
        raw: '> Looking at least 60Gb of VRAM in 16-bit mode. I ran it on the Horde
          with 2x A6000 and that was good enough...


          That''s a actually not all that much, if earlier series Tesla cards can
          handle it that is. Trying to dip my feet without going huge first.'
        updatedAt: '2023-01-30T18:32:59.165Z'
      numEdits: 0
      reactions: []
    id: 63d80d5b9df3238d894be983
    type: comment
  author: Kizna
  content: '> Looking at least 60Gb of VRAM in 16-bit mode. I ran it on the Horde
    with 2x A6000 and that was good enough...


    That''s a actually not all that much, if earlier series Tesla cards can handle
    it that is. Trying to dip my feet without going huge first.'
  created_at: 2023-01-30 18:32:59+00:00
  edited: false
  hidden: false
  id: 63d80d5b9df3238d894be983
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
      fullname: Kira Slith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSlith
      type: user
    createdAt: '2023-02-14T11:37:28.000Z'
    data:
      edited: true
      editors:
      - KSlith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
          fullname: Kira Slith
          isHf: false
          isPro: false
          name: KSlith
          type: user
        html: '<blockquote>

          <p>That''s a actually not all that much, if earlier series Tesla cards can
          handle it that is. Trying to dip my feet without going huge first.</p>

          </blockquote>

          <p>If you wanna jump in and get your feet wet with THIS model right NOW
          with whatever 6gb+ VRAM 10 series+ GPU you already have, don''t mind some
          JANK, don''t mind killing a SATA SSD, and don''t mind using text-generation-webui
          to manage your models, you can always abuse Windows'' pagefile system or
          Linux''s SWAP partitions to inflate your RAM pool, and then shunt it all
          over there using the <code>--auto-devices</code> and <code>--gpu-memory</code>
          options together.<br>Personally, I''m running it with just an old 120gb
          Samsung 860 Evo I got for $25 (80gb pagefile with the rest of the drive
          set aside for Samsung''s automatic overprovisioning) and my dinky mobile
          1660ti 6gb. I''m barely pulling it off without running VMEM out while chatting
          with the bot, it''ll eventually kill the SSD, and each response takes up
          to an hour if the history pool gets too big or complex, but it works.</p>

          '
        raw: '> That''s a actually not all that much, if earlier series Tesla cards
          can handle it that is. Trying to dip my feet without going huge first.


          If you wanna jump in and get your feet wet with THIS model right NOW with
          whatever 6gb+ VRAM 10 series+ GPU you already have, don''t mind some JANK,
          don''t mind killing a SATA SSD, and don''t mind using text-generation-webui
          to manage your models, you can always abuse Windows'' pagefile system or
          Linux''s SWAP partitions to inflate your RAM pool, and then shunt it all
          over there using the ```--auto-devices``` and ```--gpu-memory``` options
          together.

          Personally, I''m running it with just an old 120gb Samsung 860 Evo I got
          for $25 (80gb pagefile with the rest of the drive set aside for Samsung''s
          automatic overprovisioning) and my dinky mobile 1660ti 6gb. I''m barely
          pulling it off without running VMEM out while chatting with the bot, it''ll
          eventually kill the SSD, and each response takes up to an hour if the history
          pool gets too big or complex, but it works.'
        updatedAt: '2023-02-14T20:52:28.318Z'
      numEdits: 1
      reactions: []
    id: 63eb7278e9c64d757d455974
    type: comment
  author: KSlith
  content: '> That''s a actually not all that much, if earlier series Tesla cards
    can handle it that is. Trying to dip my feet without going huge first.


    If you wanna jump in and get your feet wet with THIS model right NOW with whatever
    6gb+ VRAM 10 series+ GPU you already have, don''t mind some JANK, don''t mind
    killing a SATA SSD, and don''t mind using text-generation-webui to manage your
    models, you can always abuse Windows'' pagefile system or Linux''s SWAP partitions
    to inflate your RAM pool, and then shunt it all over there using the ```--auto-devices```
    and ```--gpu-memory``` options together.

    Personally, I''m running it with just an old 120gb Samsung 860 Evo I got for $25
    (80gb pagefile with the rest of the drive set aside for Samsung''s automatic overprovisioning)
    and my dinky mobile 1660ti 6gb. I''m barely pulling it off without running VMEM
    out while chatting with the bot, it''ll eventually kill the SSD, and each response
    takes up to an hour if the history pool gets too big or complex, but it works.'
  created_at: 2023-02-14 11:37:28+00:00
  edited: true
  hidden: false
  id: 63eb7278e9c64d757d455974
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-02-19T04:04:59.000Z'
    data:
      edited: true
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KSlith&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/KSlith\">@<span class=\"\
          underline\">KSlith</span></a></span>\n\n\t</span></span>  Can this run on\
          \ a 3080ti with 12GB of vram and 64GB of ram without adding the pagefile\
          \ since I don't want to fry my SSDs?<br>Also, sorry for hijacking the thread...</p>\n"
        raw: '@KSlith  Can this run on a 3080ti with 12GB of vram and 64GB of ram
          without adding the pagefile since I don''t want to fry my SSDs?

          Also, sorry for hijacking the thread...'
        updatedAt: '2023-02-19T04:05:17.087Z'
      numEdits: 1
      reactions: []
    id: 63f19febcbadc519b85e6931
    type: comment
  author: yehiaserag
  content: '@KSlith  Can this run on a 3080ti with 12GB of vram and 64GB of ram without
    adding the pagefile since I don''t want to fry my SSDs?

    Also, sorry for hijacking the thread...'
  created_at: 2023-02-19 04:04:59+00:00
  edited: true
  hidden: false
  id: 63f19febcbadc519b85e6931
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
      fullname: Kira Slith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KSlith
      type: user
    createdAt: '2023-02-19T04:54:24.000Z'
    data:
      edited: false
      editors:
      - KSlith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676370430241-noauth.png?w=200&h=200&f=face
          fullname: Kira Slith
          isHf: false
          isPro: false
          name: KSlith
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;KSlith&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KSlith\"\
          >@<span class=\"underline\">KSlith</span></a></span>\n\n\t</span></span>\
          \  Can this run on a 3080ti with 12GB of vram and 64GB of ram without adding\
          \ the pagefile since I don't want to fry my SSDs?<br>Also, sorry for hijacking\
          \ the thread...</p>\n</blockquote>\n<p>It might, but only barely. Remember\
          \ to cap your GPU RAM usage to ~9gb so you have enough space for the model\
          \ to spread it's legs. In the 3080's case, you're going to bottleneck on\
          \ your System RAM speed.</p>\n"
        raw: '> @KSlith  Can this run on a 3080ti with 12GB of vram and 64GB of ram
          without adding the pagefile since I don''t want to fry my SSDs?

          > Also, sorry for hijacking the thread...


          It might, but only barely. Remember to cap your GPU RAM usage to ~9gb so
          you have enough space for the model to spread it''s legs. In the 3080''s
          case, you''re going to bottleneck on your System RAM speed.'
        updatedAt: '2023-02-19T04:54:24.091Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - FgRegistr
    id: 63f1ab8036d0cf11413028fe
    type: comment
  author: KSlith
  content: '> @KSlith  Can this run on a 3080ti with 12GB of vram and 64GB of ram
    without adding the pagefile since I don''t want to fry my SSDs?

    > Also, sorry for hijacking the thread...


    It might, but only barely. Remember to cap your GPU RAM usage to ~9gb so you have
    enough space for the model to spread it''s legs. In the 3080''s case, you''re
    going to bottleneck on your System RAM speed.'
  created_at: 2023-02-19 04:54:24+00:00
  edited: false
  hidden: false
  id: 63f1ab8036d0cf11413028fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08c323d03420ede923bed832bff267b9.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sylveon
      type: user
    createdAt: '2023-03-16T19:58:51.000Z'
    data:
      edited: false
      editors:
      - Sylveon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08c323d03420ede923bed832bff267b9.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: Sylveon
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KSlith&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/KSlith\">@<span class=\"\
          underline\">KSlith</span></a></span>\n\n\t</span></span> Is there a way\
          \ to combine the system ram with the vram (any tutorial), I continue to\
          \ receive cuda out of memory errors when loading any model over 6b. buying\
          \ more ram is much more affordable than any ampere card.  Also, any information\
          \ about ML models that can help me make my own would be appreciated, I am\
          \ very new to ML models</p>\n"
        raw: '@KSlith Is there a way to combine the system ram with the vram (any
          tutorial), I continue to receive cuda out of memory errors when loading
          any model over 6b. buying more ram is much more affordable than any ampere
          card.  Also, any information about ML models that can help me make my own
          would be appreciated, I am very new to ML models'
        updatedAt: '2023-03-16T19:58:51.351Z'
      numEdits: 0
      reactions: []
    id: 641374fba919becde372ce88
    type: comment
  author: Sylveon
  content: '@KSlith Is there a way to combine the system ram with the vram (any tutorial),
    I continue to receive cuda out of memory errors when loading any model over 6b.
    buying more ram is much more affordable than any ampere card.  Also, any information
    about ML models that can help me make my own would be appreciated, I am very new
    to ML models'
  created_at: 2023-03-16 18:58:51+00:00
  edited: false
  hidden: false
  id: 641374fba919becde372ce88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-03-18T05:36:04.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Sylveon&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Sylveon\">@<span class=\"\
          underline\">Sylveon</span></a></span>\n\n\t</span></span> if you are using\
          \ text generation webui then it's pretty simple<br>Check this: <a rel=\"\
          nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide\"\
          >https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide</a></p>\n"
        raw: "@Sylveon if you are using text generation webui then it's pretty simple\
          \ \nCheck this: https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide"
        updatedAt: '2023-03-18T05:36:04.461Z'
      numEdits: 0
      reactions: []
    id: 64154dc45576a3b812222248
    type: comment
  author: yehiaserag
  content: "@Sylveon if you are using text generation webui then it's pretty simple\
    \ \nCheck this: https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide"
  created_at: 2023-03-18 04:36:04+00:00
  edited: false
  hidden: false
  id: 64154dc45576a3b812222248
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08c323d03420ede923bed832bff267b9.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sylveon
      type: user
    createdAt: '2023-04-03T19:50:42.000Z'
    data:
      edited: false
      editors:
      - Sylveon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08c323d03420ede923bed832bff267b9.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: Sylveon
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KSlith&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/KSlith\">@<span class=\"\
          underline\">KSlith</span></a></span>\n\n\t</span></span> thanks for the\
          \ support, I would love to run the 30B model (currently 1080ti and 32gv\
          \ ram) by maybe buying more system ram, but would it really be worth it?\
          \ running a 30B model over a 13B model. I know there are metrics for testing\
          \ how well a text gen model is capable. but I have not found any examples\
          \ comparing many models input/output and really analyzing the differences.\
          \ Also do you know of a good tutorial for how to create my own model? something\
          \ small just for fun.</p>\n"
        raw: '@KSlith thanks for the support, I would love to run the 30B model (currently
          1080ti and 32gv ram) by maybe buying more system ram, but would it really
          be worth it? running a 30B model over a 13B model. I know there are metrics
          for testing how well a text gen model is capable. but I have not found any
          examples comparing many models input/output and really analyzing the differences.
          Also do you know of a good tutorial for how to create my own model? something
          small just for fun.'
        updatedAt: '2023-04-03T19:50:42.996Z'
      numEdits: 0
      reactions: []
    id: 642b2e12bf3d1ac926dac5dc
    type: comment
  author: Sylveon
  content: '@KSlith thanks for the support, I would love to run the 30B model (currently
    1080ti and 32gv ram) by maybe buying more system ram, but would it really be worth
    it? running a 30B model over a 13B model. I know there are metrics for testing
    how well a text gen model is capable. but I have not found any examples comparing
    many models input/output and really analyzing the differences. Also do you know
    of a good tutorial for how to create my own model? something small just for fun.'
  created_at: 2023-04-03 18:50:42+00:00
  edited: false
  hidden: false
  id: 642b2e12bf3d1ac926dac5dc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66e487d187c9285cc75c2fefea2aa1d1.svg
      fullname: Paul Solo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaulSolo
      type: user
    createdAt: '2023-04-04T13:31:39.000Z'
    data:
      edited: false
      editors:
      - PaulSolo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66e487d187c9285cc75c2fefea2aa1d1.svg
          fullname: Paul Solo
          isHf: false
          isPro: false
          name: PaulSolo
          type: user
        html: '<p>What about CPU ONLY?<br>Lets say 44 cores\88 threads and 128GB RAM?</p>

          '
        raw: 'What about CPU ONLY?

          Lets say 44 cores\88 threads and 128GB RAM?'
        updatedAt: '2023-04-04T13:31:39.802Z'
      numEdits: 0
      reactions: []
    id: 642c26bbcd5f4f52d4a525f9
    type: comment
  author: PaulSolo
  content: 'What about CPU ONLY?

    Lets say 44 cores\88 threads and 128GB RAM?'
  created_at: 2023-04-04 12:31:39+00:00
  edited: false
  hidden: false
  id: 642c26bbcd5f4f52d4a525f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08c323d03420ede923bed832bff267b9.svg
      fullname: Serena
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sylveon
      type: user
    createdAt: '2023-04-24T20:58:36.000Z'
    data:
      edited: false
      editors:
      - Sylveon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08c323d03420ede923bed832bff267b9.svg
          fullname: Serena
          isHf: false
          isPro: false
          name: Sylveon
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;PaulSolo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/PaulSolo\">@<span class=\"\
          underline\">PaulSolo</span></a></span>\n\n\t</span></span> that is a lot\
          \ of ram. Have you tried it?</p>\n"
        raw: '@PaulSolo that is a lot of ram. Have you tried it?'
        updatedAt: '2023-04-24T20:58:36.696Z'
      numEdits: 0
      reactions: []
    id: 6446ed7cf9dc06bea2a99fc0
    type: comment
  author: Sylveon
  content: '@PaulSolo that is a lot of ram. Have you tried it?'
  created_at: 2023-04-24 19:58:36+00:00
  edited: false
  hidden: false
  id: 6446ed7cf9dc06bea2a99fc0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: KoboldAI/OPT-30B-Erebus
repo_type: model
status: open
target_branch: null
title: Hardware Question
