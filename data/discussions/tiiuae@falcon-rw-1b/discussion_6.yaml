!!python/object:huggingface_hub.community.DiscussionWithDetails
author: marcovirgolin
conflicting_files: null
created_at: 2023-07-17 10:19:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/882b0e28af7aa1b03dc881c90c3fc65e.svg
      fullname: Marco Virgolin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marcovirgolin
      type: user
    createdAt: '2023-07-17T11:19:36.000Z'
    data:
      edited: false
      editors:
      - marcovirgolin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8292639851570129
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/882b0e28af7aa1b03dc881c90c3fc65e.svg
          fullname: Marco Virgolin
          isHf: false
          isPro: false
          name: marcovirgolin
          type: user
        html: '<p>Hi there.</p>

          <p>I finetuned the falcon model prior to the changes to the configuration
          and modelling files (from *_RW.py to *_falcon.py).<br>Now, when I try to
          load my local model, I get:<br><code>Could not locate the configuration_RW.py
          inside tiiuae/falcon-rw-1b.</code></p>

          <p>I tried passing the <code>revision</code> field, pointing to the commit
          hash that preceeds the changes, with:<br><code>model = AutoModelForCausalLM.from_pretrained(         local_path,
          trust_remote_code=True, revision="7fb349a0a7b09213458a45a2861342c7f2d2d3fc")
          </code><br>But I still get that error. I see, via debugging, that <code>revision==None</code>
          as it goes through the transformers library code.</p>

          <p>Can you help me with this? I''d be great if backward compatibility is
          not lost.</p>

          '
        raw: "Hi there.\r\n\r\nI finetuned the falcon model prior to the changes to\
          \ the configuration and modelling files (from *_RW.py to *_falcon.py).\r\
          \nNow, when I try to load my local model, I get: \r\n`Could not locate the\
          \ configuration_RW.py inside tiiuae/falcon-rw-1b.`\r\n\r\nI tried passing\
          \ the `revision` field, pointing to the commit hash that preceeds the changes,\
          \ with:\r\n`model = AutoModelForCausalLM.from_pretrained(\r\n        local_path,\
          \ trust_remote_code=True, revision=\"7fb349a0a7b09213458a45a2861342c7f2d2d3fc\"\
          )\r\n`\r\nBut I still get that error. I see, via debugging, that `revision==None`\
          \ as it goes through the transformers library code.\r\n\r\nCan you help\
          \ me with this? I'd be great if backward compatibility is not lost."
        updatedAt: '2023-07-17T11:19:36.738Z'
      numEdits: 0
      reactions: []
    id: 64b523c8bcfd8542d647ceac
    type: comment
  author: marcovirgolin
  content: "Hi there.\r\n\r\nI finetuned the falcon model prior to the changes to\
    \ the configuration and modelling files (from *_RW.py to *_falcon.py).\r\nNow,\
    \ when I try to load my local model, I get: \r\n`Could not locate the configuration_RW.py\
    \ inside tiiuae/falcon-rw-1b.`\r\n\r\nI tried passing the `revision` field, pointing\
    \ to the commit hash that preceeds the changes, with:\r\n`model = AutoModelForCausalLM.from_pretrained(\r\
    \n        local_path, trust_remote_code=True, revision=\"7fb349a0a7b09213458a45a2861342c7f2d2d3fc\"\
    )\r\n`\r\nBut I still get that error. I see, via debugging, that `revision==None`\
    \ as it goes through the transformers library code.\r\n\r\nCan you help me with\
    \ this? I'd be great if backward compatibility is not lost."
  created_at: 2023-07-17 10:19:36+00:00
  edited: false
  hidden: false
  id: 64b523c8bcfd8542d647ceac
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: tiiuae/falcon-rw-1b
repo_type: model
status: open
target_branch: null
title: Cannot load finetuned model due to changes in *_RW.py files
