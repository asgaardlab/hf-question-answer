!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aledane
conflicting_files: null
created_at: 2024-01-15 09:43:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T09:43:33.000Z'
    data:
      edited: true
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9882894158363342
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<p>Hi,</p>

          <p>I would like to understand what is the configuration used in the HuggingChat,
          since it gives to me some results that are very different from what I am
          getting and I''d like to obtain the same.</p>

          <p>Thank you.</p>

          '
        raw: 'Hi,


          I would like to understand what is the configuration used in the HuggingChat,
          since it gives to me some results that are very different from what I am
          getting and I''d like to obtain the same.


          Thank you.'
        updatedAt: '2024-01-15T09:46:19.405Z'
      numEdits: 1
      reactions: []
    id: 65a4fe4589fcb4ac13def095
    type: comment
  author: aledane
  content: 'Hi,


    I would like to understand what is the configuration used in the HuggingChat,
    since it gives to me some results that are very different from what I am getting
    and I''d like to obtain the same.


    Thank you.'
  created_at: 2024-01-15 09:43:33+00:00
  edited: true
  hidden: false
  id: 65a4fe4589fcb4ac13def095
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T09:46:32.000Z'
    data:
      from: What is the version of the chat?
      to: What is the version of the HuggingChat?
    id: 65a4fef8d0e350dbc9a6e84d
    type: title-change
  author: aledane
  created_at: 2024-01-15 09:46:32+00:00
  id: 65a4fef8d0e350dbc9a6e84d
  new_title: What is the version of the HuggingChat?
  old_title: What is the version of the chat?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T14:50:47.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9737204313278198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>Could u be more clear? How different are they?</p>

          '
        raw: Could u be more clear? How different are they?
        updatedAt: '2024-01-15T14:50:47.609Z'
      numEdits: 0
      reactions: []
    id: 65a54647895d1eca731b9312
    type: comment
  author: GreyForever
  content: Could u be more clear? How different are they?
  created_at: 2024-01-15 14:50:47+00:00
  edited: false
  hidden: false
  id: 65a54647895d1eca731b9312
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T14:54:03.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.894946813583374
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<p>Yes exactly. I want to understand if, with respect to the standard
          model we find in this page, for the HuggingChat they are using some set
          of parameters or if everything is set as default, or if there is an intermediate
          step (like a fine tuning). Thanks.</p>

          '
        raw: Yes exactly. I want to understand if, with respect to the standard model
          we find in this page, for the HuggingChat they are using some set of parameters
          or if everything is set as default, or if there is an intermediate step
          (like a fine tuning). Thanks.
        updatedAt: '2024-01-15T14:54:03.478Z'
      numEdits: 0
      reactions: []
    id: 65a5470b534e60db99317abb
    type: comment
  author: aledane
  content: Yes exactly. I want to understand if, with respect to the standard model
    we find in this page, for the HuggingChat they are using some set of parameters
    or if everything is set as default, or if there is an intermediate step (like
    a fine tuning). Thanks.
  created_at: 2024-01-15 14:54:03+00:00
  edited: false
  hidden: false
  id: 65a5470b534e60db99317abb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T15:06:07.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9654061198234558
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>Sorry, I might not have been clear, I meant to ask what exactly
          differs from the answers the model in huggingchat provided compared to this
          one? Cause it shouldnt be a fine tuned version or anything, they should
          be the same I believe.</p>

          '
        raw: Sorry, I might not have been clear, I meant to ask what exactly differs
          from the answers the model in huggingchat provided compared to this one?
          Cause it shouldnt be a fine tuned version or anything, they should be the
          same I believe.
        updatedAt: '2024-01-15T15:06:07.427Z'
      numEdits: 0
      reactions: []
    id: 65a549df680cb2eb9442074c
    type: comment
  author: GreyForever
  content: Sorry, I might not have been clear, I meant to ask what exactly differs
    from the answers the model in huggingchat provided compared to this one? Cause
    it shouldnt be a fine tuned version or anything, they should be the same I believe.
  created_at: 2024-01-15 15:06:07+00:00
  edited: false
  hidden: false
  id: 65a549df680cb2eb9442074c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T16:39:24.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9561373591423035
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<p>Ok, I try to explain my use case at high level.</p>

          <p>I am trying to use this model as binary classifier. So, I am using a
          prompt in such a way that the LLM takes a text and classifies it in a class
          or the other one.<br>I used the model with the original setting of parameters
          (by using the HuggingFace pipeline) within the "text-generation" task.<br>However,
          I get different answers by the HuggingChat with respect to my model deployed
          on AWS Sagemaker (I tried on some texts and the classification is often
          different).<br>For this reason, I would like to understand if the model
          deployed in the HuggingChat has some particular configration or other that
          differs from that one you obtain by using the "pipeline" mode of HuggingFace.</p>

          <p>I hope to have been a bit more clear.</p>

          '
        raw: "Ok, I try to explain my use case at high level.\n\nI am trying to use\
          \ this model as binary classifier. So, I am using a prompt in such a way\
          \ that the LLM takes a text and classifies it in a class or the other one.\n\
          I used the model with the original setting of parameters (by using the HuggingFace\
          \ pipeline) within the \"text-generation\" task. \nHowever, I get different\
          \ answers by the HuggingChat with respect to my model deployed on AWS Sagemaker\
          \ (I tried on some texts and the classification is often different).\nFor\
          \ this reason, I would like to understand if the model deployed in the HuggingChat\
          \ has some particular configration or other that differs from that one you\
          \ obtain by using the \"pipeline\" mode of HuggingFace.\n\nI hope to have\
          \ been a bit more clear."
        updatedAt: '2024-01-15T16:39:24.391Z'
      numEdits: 0
      reactions: []
    id: 65a55fbc974f8f811f7c131f
    type: comment
  author: aledane
  content: "Ok, I try to explain my use case at high level.\n\nI am trying to use\
    \ this model as binary classifier. So, I am using a prompt in such a way that\
    \ the LLM takes a text and classifies it in a class or the other one.\nI used\
    \ the model with the original setting of parameters (by using the HuggingFace\
    \ pipeline) within the \"text-generation\" task. \nHowever, I get different answers\
    \ by the HuggingChat with respect to my model deployed on AWS Sagemaker (I tried\
    \ on some texts and the classification is often different).\nFor this reason,\
    \ I would like to understand if the model deployed in the HuggingChat has some\
    \ particular configration or other that differs from that one you obtain by using\
    \ the \"pipeline\" mode of HuggingFace.\n\nI hope to have been a bit more clear."
  created_at: 2024-01-15 16:39:24+00:00
  edited: false
  hidden: false
  id: 65a55fbc974f8f811f7c131f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T17:12:34.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822033047676086
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>I see, it should be the same for as far as I know it. If you want
          deterministic results however (the same result every time you give it the
          same input) you must set the temperature to 1, and just to be sure, are
          you using the prompt format provided? It would be really helpful if I could
          have some code to see what''s going on.</p>

          '
        raw: I see, it should be the same for as far as I know it. If you want deterministic
          results however (the same result every time you give it the same input)
          you must set the temperature to 1, and just to be sure, are you using the
          prompt format provided? It would be really helpful if I could have some
          code to see what's going on.
        updatedAt: '2024-01-15T17:12:34.427Z'
      numEdits: 0
      reactions: []
    id: 65a567829f0a34c173c42e71
    type: comment
  author: GreyForever
  content: I see, it should be the same for as far as I know it. If you want deterministic
    results however (the same result every time you give it the same input) you must
    set the temperature to 1, and just to be sure, are you using the prompt format
    provided? It would be really helpful if I could have some code to see what's going
    on.
  created_at: 2024-01-15 17:12:34+00:00
  edited: false
  hidden: false
  id: 65a567829f0a34c173c42e71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T17:21:15.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6758260130882263
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>For information, you can access the source code of certain spaces
          by yourself on hugging face. For example: <a href="https://huggingface.co/spaces/huggingchat/chat-ui-template">https://huggingface.co/spaces/huggingchat/chat-ui-template</a>
          has a lot of information about the parameters used in <a href="https://huggingface.co/spaces/huggingchat/chat-ui-template/tree/main/defaults">https://huggingface.co/spaces/huggingchat/chat-ui-template/tree/main/defaults</a>
          .</p>

          '
        raw: 'For information, you can access the source code of certain spaces by
          yourself on hugging face. For example: https://huggingface.co/spaces/huggingchat/chat-ui-template
          has a lot of information about the parameters used in https://huggingface.co/spaces/huggingchat/chat-ui-template/tree/main/defaults
          .'
        updatedAt: '2024-01-15T17:21:15.035Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aledane
    id: 65a5698b8df9302d15c23c20
    type: comment
  author: GreyForever
  content: 'For information, you can access the source code of certain spaces by yourself
    on hugging face. For example: https://huggingface.co/spaces/huggingchat/chat-ui-template
    has a lot of information about the parameters used in https://huggingface.co/spaces/huggingchat/chat-ui-template/tree/main/defaults
    .'
  created_at: 2024-01-15 17:21:15+00:00
  edited: false
  hidden: false
  id: 65a5698b8df9302d15c23c20
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T17:24:41.000Z'
    data:
      edited: true
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7552505731582642
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: "<p>I cannot share a link unfortunately since I am working as consultant.<br>Anyway,\
          \ the configuration setting for now is just the following (I noticed that\
          \ I need a max_new_tokens set otherwise the answer was too short).</p>\n\
          <pre><code>                            \"torch_dtype\":torch.bfloat16,\n\
          \                            \"return_full_text\" : True,\n            \
          \                \"device_map\" : \"auto\",\n                          \
          \  \"max_new_tokens\" : 64\n</code></pre>\n<p>and for the prompt format,\
          \ I am using this:</p>\n<pre><code>                \"model_type\" : \"hugging-face\"\
          ,\n                \"prompt_format\" : \"&lt;s&gt;[INST] {instructions}\\\
          nInput:\\n{input}\\n[/INST]\"\n</code></pre>\n<p>where instructions is the\
          \ prompt where I ask to classify in two classes.<br>Then I have some Python\
          \ classes implemented to call the model, but basically I am doing this:</p>\n\
          <p>self.hf_pipeline_llm = HuggingFacePipeline(pipeline=self.pipeline_llm)<br>self.pipeline_llm\
          \  = pipeline(task=self.model_task,<br>                                \
          \          model=self.model_name,<br>                                  \
          \        **self.model_params)</p>\n"
        raw: "I cannot share a link unfortunately since I am working as consultant.\n\
          Anyway, the configuration setting for now is just the following (I noticed\
          \ that I need a max_new_tokens set otherwise the answer was too short).\n\
          \n                                \"torch_dtype\":torch.bfloat16,\n    \
          \                            \"return_full_text\" : True,\n            \
          \                    \"device_map\" : \"auto\",\n                      \
          \          \"max_new_tokens\" : 64\n\nand for the prompt format, I am using\
          \ this:\n\n                    \"model_type\" : \"hugging-face\",\n    \
          \                \"prompt_format\" : \"<s>[INST] {instructions}\\nInput:\\\
          n{input}\\n[/INST]\"\n\n\n\nwhere instructions is the prompt where I ask\
          \ to classify in two classes.\nThen I have some Python classes implemented\
          \ to call the model, but basically I am doing this:\n\nself.hf_pipeline_llm\
          \ = HuggingFacePipeline(pipeline=self.pipeline_llm) \nself.pipeline_llm\
          \  = pipeline(task=self.model_task, \n                                 \
          \         model=self.model_name, \n                                    \
          \      **self.model_params)"
        updatedAt: '2024-01-15T17:27:52.412Z'
      numEdits: 4
      reactions: []
    id: 65a56a59af89bf3f1377df9f
    type: comment
  author: aledane
  content: "I cannot share a link unfortunately since I am working as consultant.\n\
    Anyway, the configuration setting for now is just the following (I noticed that\
    \ I need a max_new_tokens set otherwise the answer was too short).\n\n       \
    \                         \"torch_dtype\":torch.bfloat16,\n                  \
    \              \"return_full_text\" : True,\n                                \"\
    device_map\" : \"auto\",\n                                \"max_new_tokens\" :\
    \ 64\n\nand for the prompt format, I am using this:\n\n                    \"\
    model_type\" : \"hugging-face\",\n                    \"prompt_format\" : \"<s>[INST]\
    \ {instructions}\\nInput:\\n{input}\\n[/INST]\"\n\n\n\nwhere instructions is the\
    \ prompt where I ask to classify in two classes.\nThen I have some Python classes\
    \ implemented to call the model, but basically I am doing this:\n\nself.hf_pipeline_llm\
    \ = HuggingFacePipeline(pipeline=self.pipeline_llm) \nself.pipeline_llm  = pipeline(task=self.model_task,\
    \ \n                                          model=self.model_name, \n      \
    \                                    **self.model_params)"
  created_at: 2024-01-15 17:24:41+00:00
  edited: true
  hidden: false
  id: 65a56a59af89bf3f1377df9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T17:27:39.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5541125535964966
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: "<p>I see, I just found the parameters  you asked for by the way:<br>MODELS=<code>[\
          \     {       \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",       \"\
          displayName\": \"mistralai/Mistral-7B-Instruct-v0.1\",       \"description\"\
          : \"Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms\
          \ Llama2 13B in benchmarks.\",       \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
          ,       \"preprompt\": \"\",       \"chatPromptTemplate\" : \"&lt;s&gt;{{#each\
          \ messages}}{{#ifUser}}[INST] {{#if <span data-props=\"{&quot;user&quot;:&quot;first&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/first\"\
          >@<span class=\"underline\">first</span></a></span>\n\n\t</span></span>}}{{#if\
          \ <span data-props=\"{&quot;user&quot;:&quot;root&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/root\">@<span class=\"\
          underline\">root</span></a></span>\n\n\t</span></span>.preprompt}}{{@root.preprompt}}\\\
          n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}&lt;/s&gt;{{/ifAssistant}}{{/each}}\"\
          ,       \"parameters\": {         \"temperature\": 0.1,         \"top_p\"\
          : 0.95,         \"repetition_penalty\": 1.2,         \"top_k\": 50,    \
          \     \"truncate\": 3072,         \"max_new_tokens\": 1024,         \"stop\"\
          : [\"&lt;/s&gt;\"]       },       \"promptExamples\": [         {      \
          \     \"title\": \"Write an email from bullet list\",           \"prompt\"\
          : \"As a restaurant owner, write a professional email to the supplier to\
          \ get these products every week: \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread\
          \ (x12)\"         }, {           \"title\": \"Code a snake game\",     \
          \      \"prompt\": \"Code a basic snake game in python, give explanations\
          \ for each step.\"         }, {           \"title\": \"Assist in a task\"\
          ,           \"prompt\": \"How do I make a delicious lemon cheesecake?\"\
          \         }       ]     } ]</code></p>\n<p>You can find it here: <a href=\"\
          https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\">https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env</a></p>\n\
          <p>Maybe this will help you out !</p>\n"
        raw: "I see, I just found the parameters  you asked for by the way:\nMODELS=`[\n\
          \    {\n      \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n      \"\
          displayName\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n      \"description\"\
          : \"Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms\
          \ Llama2 13B in benchmarks.\",\n      \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
          ,\n      \"preprompt\": \"\",\n      \"chatPromptTemplate\" : \"<s>{{#each\
          \ messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\\
          n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}\"\
          ,\n      \"parameters\": {\n        \"temperature\": 0.1,\n        \"top_p\"\
          : 0.95,\n        \"repetition_penalty\": 1.2,\n        \"top_k\": 50,\n\
          \        \"truncate\": 3072,\n        \"max_new_tokens\": 1024,\n      \
          \  \"stop\": [\"</s>\"]\n      },\n      \"promptExamples\": [\n       \
          \ {\n          \"title\": \"Write an email from bullet list\",\n       \
          \   \"prompt\": \"As a restaurant owner, write a professional email to the\
          \ supplier to get these products every week: \\n\\n- Wine (x10)\\n- Eggs\
          \ (x24)\\n- Bread (x12)\"\n        }, {\n          \"title\": \"Code a snake\
          \ game\",\n          \"prompt\": \"Code a basic snake game in python, give\
          \ explanations for each step.\"\n        }, {\n          \"title\": \"Assist\
          \ in a task\",\n          \"prompt\": \"How do I make a delicious lemon\
          \ cheesecake?\"\n        }\n      ]\n    }\n]`\n\nYou can find it here:\
          \ https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\n\nMaybe\
          \ this will help you out !"
        updatedAt: '2024-01-15T17:27:39.988Z'
      numEdits: 0
      reactions: []
    id: 65a56b0b4d251e93564508f1
    type: comment
  author: GreyForever
  content: "I see, I just found the parameters  you asked for by the way:\nMODELS=`[\n\
    \    {\n      \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n      \"displayName\"\
    : \"mistralai/Mistral-7B-Instruct-v0.1\",\n      \"description\": \"Mistral 7B\
    \ is a new Apache 2.0 model, released by Mistral AI that outperforms Llama2 13B\
    \ in benchmarks.\",\n      \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
    ,\n      \"preprompt\": \"\",\n      \"chatPromptTemplate\" : \"<s>{{#each messages}}{{#ifUser}}[INST]\
    \ {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\n{{/if}}{{/if}}{{content}}\
    \ [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}\"\
    ,\n      \"parameters\": {\n        \"temperature\": 0.1,\n        \"top_p\":\
    \ 0.95,\n        \"repetition_penalty\": 1.2,\n        \"top_k\": 50,\n      \
    \  \"truncate\": 3072,\n        \"max_new_tokens\": 1024,\n        \"stop\": [\"\
    </s>\"]\n      },\n      \"promptExamples\": [\n        {\n          \"title\"\
    : \"Write an email from bullet list\",\n          \"prompt\": \"As a restaurant\
    \ owner, write a professional email to the supplier to get these products every\
    \ week: \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread (x12)\"\n        }, {\n  \
    \        \"title\": \"Code a snake game\",\n          \"prompt\": \"Code a basic\
    \ snake game in python, give explanations for each step.\"\n        }, {\n   \
    \       \"title\": \"Assist in a task\",\n          \"prompt\": \"How do I make\
    \ a delicious lemon cheesecake?\"\n        }\n      ]\n    }\n]`\n\nYou can find\
    \ it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\n\n\
    Maybe this will help you out !"
  created_at: 2024-01-15 17:27:39+00:00
  edited: false
  hidden: false
  id: 65a56b0b4d251e93564508f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T17:36:40.000Z'
    data:
      edited: true
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8743791580200195
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>Also, I noticed your prompt format is wrong.</p>

          <pre><code>The example provided in the READ_ME is:

          text = "&lt;s&gt;[INST] What is your favourite condiment? [/INST]"

          "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds
          just the right amount of zesty flavour to whatever I''m cooking up in the
          kitchen!&lt;/s&gt; "

          "[INST] Do you have mayonnaise recipes? [/INST]"



          As it suggests, the &lt;s&gt;&lt;/s&gt; section is only required for the
          past discussions, the instruction you are making now does not require, to
          be simple, you can do this if you do not care about the past exchanges with
          the bot:

          "[INST] {instructions}\nInput:\n{input}\n[/INST]"


          But I recommend you do this instead for example:

          "&lt;s&gt;[INST] I will give you the description of people, and I want you
          to respond with a json. For example: "A 18 years old woman"[/INST]{"age":"woman","genre":"woman"}&lt;/s&gt;[INST]{new
          sentence} [/INST] "

          </code></pre>

          '
        raw: 'Also, I noticed your prompt format is wrong.

          ```

          The example provided in the READ_ME is:

          text = "<s>[INST] What is your favourite condiment? [/INST]"

          "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds
          just the right amount of zesty flavour to whatever I''m cooking up in the
          kitchen!</s> "

          "[INST] Do you have mayonnaise recipes? [/INST]"



          As it suggests, the <s></s> section is only required for the past discussions,
          the instruction you are making now does not require, to be simple, you can
          do this if you do not care about the past exchanges with the bot:

          "[INST] {instructions}\nInput:\n{input}\n[/INST]"


          But I recommend you do this instead for example:

          "<s>[INST] I will give you the description of people, and I want you to
          respond with a json. For example: "A 18 years old woman"[/INST]{"age":"woman","genre":"woman"}</s>[INST]{new
          sentence} [/INST] "

          ```

          '
        updatedAt: '2024-01-15T17:41:28.449Z'
      numEdits: 4
      reactions: []
    id: 65a56d28b7897304be8451f8
    type: comment
  author: GreyForever
  content: 'Also, I noticed your prompt format is wrong.

    ```

    The example provided in the READ_ME is:

    text = "<s>[INST] What is your favourite condiment? [/INST]"

    "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds just
    the right amount of zesty flavour to whatever I''m cooking up in the kitchen!</s>
    "

    "[INST] Do you have mayonnaise recipes? [/INST]"



    As it suggests, the <s></s> section is only required for the past discussions,
    the instruction you are making now does not require, to be simple, you can do
    this if you do not care about the past exchanges with the bot:

    "[INST] {instructions}\nInput:\n{input}\n[/INST]"


    But I recommend you do this instead for example:

    "<s>[INST] I will give you the description of people, and I want you to respond
    with a json. For example: "A 18 years old woman"[/INST]{"age":"woman","genre":"woman"}</s>[INST]{new
    sentence} [/INST] "

    ```

    '
  created_at: 2024-01-15 17:36:40+00:00
  edited: true
  hidden: false
  id: 65a56d28b7897304be8451f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T17:45:24.000Z'
    data:
      edited: true
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6390959620475769
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: "<p>Thank you very much for both comments!<br>Few questions:</p>\n<blockquote>\n\
          <p>I see, I just found the parameters  you asked for by the way:<br>MODELS=<code>[\
          \     {       \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",       \"\
          displayName\": \"mistralai/Mistral-7B-Instruct-v0.1\",       \"description\"\
          : \"Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms\
          \ Llama2 13B in benchmarks.\",       \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
          ,       \"preprompt\": \"\",       \"chatPromptTemplate\" : \"&lt;s&gt;{{#each\
          \ messages}}{{#ifUser}}[INST] {{#if <span data-props=\"{&quot;user&quot;:&quot;first&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/first\"\
          >@<span class=\"underline\">first</span></a></span>\n\n\t</span></span>}}{{#if\
          \ <span data-props=\"{&quot;user&quot;:&quot;root&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/root\">@<span class=\"\
          underline\">root</span></a></span>\n\n\t</span></span>.preprompt}}{{@root.preprompt}}\\\
          n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}&lt;/s&gt;{{/ifAssistant}}{{/each}}\"\
          ,       \"parameters\": {         \"temperature\": 0.1,         \"top_p\"\
          : 0.95,         \"repetition_penalty\": 1.2,         \"top_k\": 50,    \
          \     \"truncate\": 3072,         \"max_new_tokens\": 1024,         \"stop\"\
          : [\"&lt;/s&gt;\"]       },       \"promptExamples\": [         {      \
          \     \"title\": \"Write an email from bullet list\",           \"prompt\"\
          : \"As a restaurant owner, write a professional email to the supplier to\
          \ get these products every week: \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread\
          \ (x12)\"         }, {           \"title\": \"Code a snake game\",     \
          \      \"prompt\": \"Code a basic snake game in python, give explanations\
          \ for each step.\"         }, {           \"title\": \"Assist in a task\"\
          ,           \"prompt\": \"How do I make a delicious lemon cheesecake?\"\
          \         }       ]     } ]</code></p>\n<p>You can find it here: <a href=\"\
          https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\">https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env</a></p>\n\
          <p>Maybe this will help you out !</p>\n</blockquote>\n<p>By chance do you\
          \ have also that one for the v02? Because this is the v01 but I do not manage\
          \ to find the right repo</p>\n"
        raw: "Thank you very much for both comments!\nFew questions:\n> I see, I just\
          \ found the parameters  you asked for by the way:\n> MODELS=`[\n>     {\n\
          >       \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n>       \"displayName\"\
          : \"mistralai/Mistral-7B-Instruct-v0.1\",\n>       \"description\": \"Mistral\
          \ 7B is a new Apache 2.0 model, released by Mistral AI that outperforms\
          \ Llama2 13B in benchmarks.\",\n>       \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
          ,\n>       \"preprompt\": \"\",\n>       \"chatPromptTemplate\" : \"<s>{{#each\
          \ messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\\
          n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}\"\
          ,\n>       \"parameters\": {\n>         \"temperature\": 0.1,\n>       \
          \  \"top_p\": 0.95,\n>         \"repetition_penalty\": 1.2,\n>         \"\
          top_k\": 50,\n>         \"truncate\": 3072,\n>         \"max_new_tokens\"\
          : 1024,\n>         \"stop\": [\"</s>\"]\n>       },\n>       \"promptExamples\"\
          : [\n>         {\n>           \"title\": \"Write an email from bullet list\"\
          ,\n>           \"prompt\": \"As a restaurant owner, write a professional\
          \ email to the supplier to get these products every week: \\n\\n- Wine (x10)\\\
          n- Eggs (x24)\\n- Bread (x12)\"\n>         }, {\n>           \"title\":\
          \ \"Code a snake game\",\n>           \"prompt\": \"Code a basic snake game\
          \ in python, give explanations for each step.\"\n>         }, {\n>     \
          \      \"title\": \"Assist in a task\",\n>           \"prompt\": \"How do\
          \ I make a delicious lemon cheesecake?\"\n>         }\n>       ]\n>    \
          \ }\n> ]`\n> \n> You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\n\
          > \n> Maybe this will help you out !\n\n\nBy chance do you have also that\
          \ one for the v02? Because this is the v01 but I do not manage to find the\
          \ right repo"
        updatedAt: '2024-01-15T17:48:08.890Z'
      numEdits: 2
      reactions: []
    id: 65a56f34482c457f35b74112
    type: comment
  author: aledane
  content: "Thank you very much for both comments!\nFew questions:\n> I see, I just\
    \ found the parameters  you asked for by the way:\n> MODELS=`[\n>     {\n>   \
    \    \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n>       \"displayName\"\
    : \"mistralai/Mistral-7B-Instruct-v0.1\",\n>       \"description\": \"Mistral\
    \ 7B is a new Apache 2.0 model, released by Mistral AI that outperforms Llama2\
    \ 13B in benchmarks.\",\n>       \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
    ,\n>       \"preprompt\": \"\",\n>       \"chatPromptTemplate\" : \"<s>{{#each\
    \ messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\\
    n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}\"\
    ,\n>       \"parameters\": {\n>         \"temperature\": 0.1,\n>         \"top_p\"\
    : 0.95,\n>         \"repetition_penalty\": 1.2,\n>         \"top_k\": 50,\n> \
    \        \"truncate\": 3072,\n>         \"max_new_tokens\": 1024,\n>         \"\
    stop\": [\"</s>\"]\n>       },\n>       \"promptExamples\": [\n>         {\n>\
    \           \"title\": \"Write an email from bullet list\",\n>           \"prompt\"\
    : \"As a restaurant owner, write a professional email to the supplier to get these\
    \ products every week: \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread (x12)\"\n>\
    \         }, {\n>           \"title\": \"Code a snake game\",\n>           \"\
    prompt\": \"Code a basic snake game in python, give explanations for each step.\"\
    \n>         }, {\n>           \"title\": \"Assist in a task\",\n>           \"\
    prompt\": \"How do I make a delicious lemon cheesecake?\"\n>         }\n>    \
    \   ]\n>     }\n> ]`\n> \n> You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\n\
    > \n> Maybe this will help you out !\n\n\nBy chance do you have also that one\
    \ for the v02? Because this is the v01 but I do not manage to find the right repo"
  created_at: 2024-01-15 17:45:24+00:00
  edited: true
  hidden: false
  id: 65a56f34482c457f35b74112
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-15T17:47:27.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8909520506858826
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<blockquote>

          <p>Also, I noticed your prompt format is wrong.</p>

          <pre><code>The example provided in the READ_ME is:

          text = "&lt;s&gt;[INST] What is your favourite condiment? [/INST]"

          "Well, I''m quite partial to a good squeeze of fresh lemon juice. It adds
          just the right amount of zesty flavour to whatever I''m cooking up in the
          kitchen!&lt;/s&gt; "

          "[INST] Do you have mayonnaise recipes? [/INST]"



          As it suggests, the &lt;s&gt;&lt;/s&gt; section is only required for the
          past discussions, the instruction you are making now does not require, to
          be simple, you can do this if you do not care about the past exchanges with
          the bot:

          "[INST] {instructions}\nInput:\n{input}\n[/INST]"


          But I recommend you do this instead for example:

          "&lt;s&gt;[INST] I will give you the description of people, and I want you
          to respond with a json. For example: "A 18 years old woman"[/INST]{"age":"woman","genre":"woman"}&lt;/s&gt;[INST]{new
          sentence} [/INST] "

          </code></pre>

          </blockquote>

          <p>Concerning this, in your last prompt, do you suggest to use the example
          as a few shot and put in the "new sentence" part the text I want to classify?</p>

          '
        raw: "> Also, I noticed your prompt format is wrong.\n> ```\n> The example\
          \ provided in the READ_ME is:\n> text = \"<s>[INST] What is your favourite\
          \ condiment? [/INST]\"\n> \"Well, I'm quite partial to a good squeeze of\
          \ fresh lemon juice. It adds just the right amount of zesty flavour to whatever\
          \ I'm cooking up in the kitchen!</s> \"\n> \"[INST] Do you have mayonnaise\
          \ recipes? [/INST]\"\n> \n> \n> As it suggests, the <s></s> section is only\
          \ required for the past discussions, the instruction you are making now\
          \ does not require, to be simple, you can do this if you do not care about\
          \ the past exchanges with the bot:\n> \"[INST] {instructions}\\nInput:\\\
          n{input}\\n[/INST]\"\n> \n> But I recommend you do this instead for example:\n\
          > \"<s>[INST] I will give you the description of people, and I want you\
          \ to respond with a json. For example: \"A 18 years old woman\"[/INST]{\"\
          age\":\"woman\",\"genre\":\"woman\"}</s>[INST]{new sentence} [/INST] \"\n\
          > ```\n\nConcerning this, in your last prompt, do you suggest to use the\
          \ example as a few shot and put in the \"new sentence\" part the text I\
          \ want to classify?"
        updatedAt: '2024-01-15T17:47:27.448Z'
      numEdits: 0
      reactions: []
    id: 65a56faf215aabac48b9c20b
    type: comment
  author: aledane
  content: "> Also, I noticed your prompt format is wrong.\n> ```\n> The example provided\
    \ in the READ_ME is:\n> text = \"<s>[INST] What is your favourite condiment? [/INST]\"\
    \n> \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds\
    \ just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\
    \ \"\n> \"[INST] Do you have mayonnaise recipes? [/INST]\"\n> \n> \n> As it suggests,\
    \ the <s></s> section is only required for the past discussions, the instruction\
    \ you are making now does not require, to be simple, you can do this if you do\
    \ not care about the past exchanges with the bot:\n> \"[INST] {instructions}\\\
    nInput:\\n{input}\\n[/INST]\"\n> \n> But I recommend you do this instead for example:\n\
    > \"<s>[INST] I will give you the description of people, and I want you to respond\
    \ with a json. For example: \"A 18 years old woman\"[/INST]{\"age\":\"woman\"\
    ,\"genre\":\"woman\"}</s>[INST]{new sentence} [/INST] \"\n> ```\n\nConcerning\
    \ this, in your last prompt, do you suggest to use the example as a few shot and\
    \ put in the \"new sentence\" part the text I want to classify?"
  created_at: 2024-01-15 17:47:27+00:00
  edited: false
  hidden: false
  id: 65a56faf215aabac48b9c20b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T18:10:12.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9438421130180359
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: '<p>Exactly, it gives it more context of what it should do. So you put
          the instruction, then an example and the bot''s response yourself, so it
          knows what to do next time. Give it a shot.</p>

          '
        raw: Exactly, it gives it more context of what it should do. So you put the
          instruction, then an example and the bot's response yourself, so it knows
          what to do next time. Give it a shot.
        updatedAt: '2024-01-15T18:10:12.539Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aledane
    id: 65a575049acab19980d58d0c
    type: comment
  author: GreyForever
  content: Exactly, it gives it more context of what it should do. So you put the
    instruction, then an example and the bot's response yourself, so it knows what
    to do next time. Give it a shot.
  created_at: 2024-01-15 18:10:12+00:00
  edited: false
  hidden: false
  id: 65a575049acab19980d58d0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
      fullname: Forever
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GreyForever
      type: user
    createdAt: '2024-01-15T18:19:40.000Z'
    data:
      edited: false
      editors:
      - GreyForever
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6040335893630981
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64161701107962562e9b1006/HM8Je8C8ie1aCpdse4SUY.png?w=200&h=200&f=face
          fullname: Forever
          isHf: false
          isPro: false
          name: GreyForever
          type: user
        html: "<blockquote>\n<p>Thank you very much for both comments!<br>Few questions:</p>\n\
          <blockquote>\n<p>I see, I just found the parameters  you asked for by the\
          \ way:<br>MODELS=<code>[     {       \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\"\
          ,       \"displayName\": \"mistralai/Mistral-7B-Instruct-v0.1\",       \"\
          description\": \"Mistral 7B is a new Apache 2.0 model, released by Mistral\
          \ AI that outperforms Llama2 13B in benchmarks.\",       \"websiteUrl\"\
          : \"https://mistral.ai/news/announcing-mistral-7b/\",       \"preprompt\"\
          : \"\",       \"chatPromptTemplate\" : \"&lt;s&gt;{{#each messages}}{{#ifUser}}[INST]\
          \ {{#if <span data-props=\"{&quot;user&quot;:&quot;first&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/first\">@<span class=\"\
          underline\">first</span></a></span>\n\n\t</span></span>}}{{#if <span data-props=\"\
          {&quot;user&quot;:&quot;root&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/root\">@<span class=\"underline\">root</span></a></span>\n\
          \n\t</span></span>.preprompt}}{{@root.preprompt}}\\n{{/if}}{{/if}}{{content}}\
          \ [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}&lt;/s&gt;{{/ifAssistant}}{{/each}}\"\
          ,       \"parameters\": {         \"temperature\": 0.1,         \"top_p\"\
          : 0.95,         \"repetition_penalty\": 1.2,         \"top_k\": 50,    \
          \     \"truncate\": 3072,         \"max_new_tokens\": 1024,         \"stop\"\
          : [\"&lt;/s&gt;\"]       },       \"promptExamples\": [         {      \
          \     \"title\": \"Write an email from bullet list\",           \"prompt\"\
          : \"As a restaurant owner, write a professional email to the supplier to\
          \ get these products every week: \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread\
          \ (x12)\"         }, {           \"title\": \"Code a snake game\",     \
          \      \"prompt\": \"Code a basic snake game in python, give explanations\
          \ for each step.\"         }, {           \"title\": \"Assist in a task\"\
          ,           \"prompt\": \"How do I make a delicious lemon cheesecake?\"\
          \         }       ]     } ]</code></p>\n<p>You can find it here: <a href=\"\
          https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\">https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env</a></p>\n\
          <p>Maybe this will help you out !</p>\n</blockquote>\n<p>By chance do you\
          \ have also that one for the v02? Because this is the v01 but I do not manage\
          \ to find the right repo</p>\n</blockquote>\n<p>Here you go : <a href=\"\
          https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env.template\"\
          >https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env.template</a>\
          \ </p>\n<p>There is the parameters for all of them.</p>\n"
        raw: "> Thank you very much for both comments!\n> Few questions:\n> > I see,\
          \ I just found the parameters  you asked for by the way:\n> > MODELS=`[\n\
          > >     {\n> >       \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n\
          > >       \"displayName\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n> >\
          \       \"description\": \"Mistral 7B is a new Apache 2.0 model, released\
          \ by Mistral AI that outperforms Llama2 13B in benchmarks.\",\n> >     \
          \  \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\",\n\
          > >       \"preprompt\": \"\",\n> >       \"chatPromptTemplate\" : \"<s>{{#each\
          \ messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\\
          n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}\"\
          ,\n> >       \"parameters\": {\n> >         \"temperature\": 0.1,\n> > \
          \        \"top_p\": 0.95,\n> >         \"repetition_penalty\": 1.2,\n> >\
          \         \"top_k\": 50,\n> >         \"truncate\": 3072,\n> >         \"\
          max_new_tokens\": 1024,\n> >         \"stop\": [\"</s>\"]\n> >       },\n\
          > >       \"promptExamples\": [\n> >         {\n> >           \"title\"\
          : \"Write an email from bullet list\",\n> >           \"prompt\": \"As a\
          \ restaurant owner, write a professional email to the supplier to get these\
          \ products every week: \\n\\n- Wine (x10)\\n- Eggs (x24)\\n- Bread (x12)\"\
          \n> >         }, {\n> >           \"title\": \"Code a snake game\",\n> >\
          \           \"prompt\": \"Code a basic snake game in python, give explanations\
          \ for each step.\"\n> >         }, {\n> >           \"title\": \"Assist\
          \ in a task\",\n> >           \"prompt\": \"How do I make a delicious lemon\
          \ cheesecake?\"\n> >         }\n> >       ]\n> >     }\n> > ]`\n> > \n>\
          \ > You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\n\
          > > \n> > Maybe this will help you out !\n> \n> \n> By chance do you have\
          \ also that one for the v02? Because this is the v01 but I do not manage\
          \ to find the right repo\n\nHere you go : https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env.template\
          \ \n\nThere is the parameters for all of them."
        updatedAt: '2024-01-15T18:19:40.457Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aledane
    id: 65a5773c000ded69cb559316
    type: comment
  author: GreyForever
  content: "> Thank you very much for both comments!\n> Few questions:\n> > I see,\
    \ I just found the parameters  you asked for by the way:\n> > MODELS=`[\n> > \
    \    {\n> >       \"name\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n> >     \
    \  \"displayName\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n> >       \"description\"\
    : \"Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms\
    \ Llama2 13B in benchmarks.\",\n> >       \"websiteUrl\": \"https://mistral.ai/news/announcing-mistral-7b/\"\
    ,\n> >       \"preprompt\": \"\",\n> >       \"chatPromptTemplate\" : \"<s>{{#each\
    \ messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\\
    n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}\"\
    ,\n> >       \"parameters\": {\n> >         \"temperature\": 0.1,\n> >       \
    \  \"top_p\": 0.95,\n> >         \"repetition_penalty\": 1.2,\n> >         \"\
    top_k\": 50,\n> >         \"truncate\": 3072,\n> >         \"max_new_tokens\"\
    : 1024,\n> >         \"stop\": [\"</s>\"]\n> >       },\n> >       \"promptExamples\"\
    : [\n> >         {\n> >           \"title\": \"Write an email from bullet list\"\
    ,\n> >           \"prompt\": \"As a restaurant owner, write a professional email\
    \ to the supplier to get these products every week: \\n\\n- Wine (x10)\\n- Eggs\
    \ (x24)\\n- Bread (x12)\"\n> >         }, {\n> >           \"title\": \"Code a\
    \ snake game\",\n> >           \"prompt\": \"Code a basic snake game in python,\
    \ give explanations for each step.\"\n> >         }, {\n> >           \"title\"\
    : \"Assist in a task\",\n> >           \"prompt\": \"How do I make a delicious\
    \ lemon cheesecake?\"\n> >         }\n> >       ]\n> >     }\n> > ]`\n> > \n>\
    \ > You can find it here: https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env\n\
    > > \n> > Maybe this will help you out !\n> \n> \n> By chance do you have also\
    \ that one for the v02? Because this is the v01 but I do not manage to find the\
    \ right repo\n\nHere you go : https://huggingface.co/spaces/huggingchat/chat-ui/blob/main/.env.template\
    \ \n\nThere is the parameters for all of them."
  created_at: 2024-01-15 18:19:40+00:00
  edited: false
  hidden: false
  id: 65a5773c000ded69cb559316
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
      fullname: Alessandro Danesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aledane
      type: user
    createdAt: '2024-01-16T08:18:41.000Z'
    data:
      edited: false
      editors:
      - aledane
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9456402063369751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd593abed16bff380191c3d88670874f.svg
          fullname: Alessandro Danesi
          isHf: false
          isPro: false
          name: aledane
          type: user
        html: '<p>Thanks a lot for everything!</p>

          '
        raw: Thanks a lot for everything!
        updatedAt: '2024-01-16T08:18:41.355Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - GreyForever
        - Wauplin
    id: 65a63be10a2ac7c42e7ccb55
    type: comment
  author: aledane
  content: Thanks a lot for everything!
  created_at: 2024-01-16 08:18:41+00:00
  edited: false
  hidden: false
  id: 65a63be10a2ac7c42e7ccb55
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 39
repo_id: mistralai/Mistral-7B-Instruct-v0.2
repo_type: model
status: open
target_branch: null
title: What is the version of the HuggingChat?
