!!python/object:huggingface_hub.community.DiscussionWithDetails
author: navidmadani
conflicting_files: null
created_at: 2023-12-21 13:26:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5037116d5c1097769cf4a5fa982a106d.svg
      fullname: Navid Madani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navidmadani
      type: user
    createdAt: '2023-12-21T13:26:42.000Z'
    data:
      edited: false
      editors:
      - navidmadani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7630935907363892
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5037116d5c1097769cf4a5fa982a106d.svg
          fullname: Navid Madani
          isHf: false
          isPro: false
          name: navidmadani
          type: user
        html: '<p>Hey all, I run into this error when I try to build a prompt using
          the <code>tokenizer.apply_chat_template(messages, return_tensors="pt")</code>.
          The problem is with the system message inside the messages there. When I
          remove the system message I won''t get the <code>jinja2.exceptions.TemplateError:
          Conversation roles must alternate user/assistant/user/assistant/...</code>
          error anymore..</p>

          <p>that is because according to the tokenizer config <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json">here</a>,
          the chat template does not support system messages. So I''m wondering how
          the model was trained for supporting system messages and how should we use
          it?</p>

          '
        raw: "Hey all, I run into this error when I try to build a prompt using the\
          \ `tokenizer.apply_chat_template(messages, return_tensors=\"pt\")`. The\
          \ problem is with the system message inside the messages there. When I remove\
          \ the system message I won't get the `jinja2.exceptions.TemplateError: Conversation\
          \ roles must alternate user/assistant/user/assistant/...` error anymore..\r\
          \n\r\nthat is because according to the tokenizer config [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json),\
          \ the chat template does not support system messages. So I'm wondering how\
          \ the model was trained for supporting system messages and how should we\
          \ use it?"
        updatedAt: '2023-12-21T13:26:42.397Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - team-cm
        - ChvckN0rri5
        - jermyn-DT
    id: 65843d123925d68d7c69c49c
    type: comment
  author: navidmadani
  content: "Hey all, I run into this error when I try to build a prompt using the\
    \ `tokenizer.apply_chat_template(messages, return_tensors=\"pt\")`. The problem\
    \ is with the system message inside the messages there. When I remove the system\
    \ message I won't get the `jinja2.exceptions.TemplateError: Conversation roles\
    \ must alternate user/assistant/user/assistant/...` error anymore..\r\n\r\nthat\
    \ is because according to the tokenizer config [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json),\
    \ the chat template does not support system messages. So I'm wondering how the\
    \ model was trained for supporting system messages and how should we use it?"
  created_at: 2023-12-21 13:26:42+00:00
  edited: false
  hidden: false
  id: 65843d123925d68d7c69c49c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645abe4d333fb18357861454/BQJNtm4pIo2vGyqdnOYK-.jpeg?w=200&h=200&f=face
      fullname: Chvck N0rri5
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChvckN0rri5
      type: user
    createdAt: '2024-01-02T23:12:01.000Z'
    data:
      edited: true
      editors:
      - ChvckN0rri5
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6905000805854797
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/645abe4d333fb18357861454/BQJNtm4pIo2vGyqdnOYK-.jpeg?w=200&h=200&f=face
          fullname: Chvck N0rri5
          isHf: false
          isPro: false
          name: ChvckN0rri5
          type: user
        html: "<h1 id=\"hope-this-is-helpful\">Hope this is helpful</h1>\n<h2 id=\"\
          how-we-do-it\">How we do it</h2>\n<p>We use <a rel=\"nofollow\" href=\"\
          https://docs.mistral.ai/self-deployment/vllm/\">vLLM to provide an openai\
          \ compatible chat endpoint</a> that can use system messages. Together with\
          \ <a rel=\"nofollow\" href=\"https://python.langchain.com/docs/integrations/llms/vllm#openai-compatible-server\"\
          >langchain</a> we are able to accomplish this. </p>\n<h2 id=\"alternate-solutions\"\
          >Alternate solutions</h2>\n<p>For TS Junkies <a rel=\"nofollow\" href=\"\
          https://js.langchain.com/docs/integrations/chat/mistral\">here is a solution</a>.<br>If\
          \ you don't have the hardware to run the model, here is a <a rel=\"nofollow\"\
          \ href=\"https://python.langchain.com/docs/integrations/chat/mistralai\"\
          >Mistral API solution using langchain in Python</a> (Mistral API is in <a\
          \ rel=\"nofollow\" href=\"https://console.mistral.ai\">beta</a>)</p>\n<p>Here\
          \ is a post with a <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/41#65284b827b99d91baa1037bb\"\
          >python solution</a> that may be more relevant to your error message. This\
          \ one uses the user, assistant, user framework that the error mentions.\
          \ </p>\n<h2 id=\"example\">Example</h2>\n<pre><code class=\"language-python\"\
          >inference_server_url = <span class=\"hljs-string\">\"http://localhost:8000/v1\"\
          </span>\n\nchat = ChatOpenAI(\n    model=<span class=\"hljs-string\">\"\
          mistralai/Mistral-7B-Instruct-v0.2\"</span>,\n    openai_api_key=<span class=\"\
          hljs-string\">\"EMPTY\"</span>,\n    openai_api_base=inference_server_url,\n\
          \    max_tokens=<span class=\"hljs-number\">250</span>,\n    temperature=<span\
          \ class=\"hljs-number\">0</span>,\n)\n\nmessages = [\n    SystemMessage(\n\
          \        content=<span class=\"hljs-string\">\"You are a helpful assistant\
          \ that translates English to Italian.\"</span>\n    ),\n    HumanMessage(\n\
          \        content=<span class=\"hljs-string\">\"Translate the following sentence\
          \ from English to Italian: I love programming.\"</span>\n    ),\n]\nchat(messages)\n\
          </code></pre>\n<pre><code class=\"language-python\">AIMessage(content=<span\
          \ class=\"hljs-string\">' Io amo programmare'</span>, additional_kwargs={},\
          \ example=<span class=\"hljs-literal\">False</span>)\n</code></pre>\n<p>If\
          \ this doesn't help, then it's at least a handy reference. </p>\n"
        raw: "# Hope this is helpful\n\n## How we do it\nWe use [vLLM to provide an\
          \ openai compatible chat endpoint](https://docs.mistral.ai/self-deployment/vllm/)\
          \ that can use system messages. Together with [langchain](https://python.langchain.com/docs/integrations/llms/vllm#openai-compatible-server)\
          \ we are able to accomplish this. \n\n## Alternate solutions\nFor TS Junkies\
          \ [here is a solution](https://js.langchain.com/docs/integrations/chat/mistral).\n\
          If you don't have the hardware to run the model, here is a [Mistral API\
          \ solution using langchain in Python](https://python.langchain.com/docs/integrations/chat/mistralai)\
          \ (Mistral API is in [beta](https://console.mistral.ai))\n\nHere is a post\
          \ with a [python solution](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/41#65284b827b99d91baa1037bb)\
          \ that may be more relevant to your error message. This one uses the user,\
          \ assistant, user framework that the error mentions. \n\n\n## Example\n\
          ```python\ninference_server_url = \"http://localhost:8000/v1\"\n\nchat =\
          \ ChatOpenAI(\n    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n    openai_api_key=\"\
          EMPTY\",\n    openai_api_base=inference_server_url,\n    max_tokens=250,\n\
          \    temperature=0,\n)\n\nmessages = [\n    SystemMessage(\n        content=\"\
          You are a helpful assistant that translates English to Italian.\"\n    ),\n\
          \    HumanMessage(\n        content=\"Translate the following sentence from\
          \ English to Italian: I love programming.\"\n    ),\n]\nchat(messages)\n\
          ```\n```python\nAIMessage(content=' Io amo programmare', additional_kwargs={},\
          \ example=False)\n```\n\nIf this doesn't help, then it's at least a handy\
          \ reference. "
        updatedAt: '2024-01-02T23:22:33.942Z'
      numEdits: 3
      reactions: []
    id: 659498412a0a886ef0d8c98b
    type: comment
  author: ChvckN0rri5
  content: "# Hope this is helpful\n\n## How we do it\nWe use [vLLM to provide an\
    \ openai compatible chat endpoint](https://docs.mistral.ai/self-deployment/vllm/)\
    \ that can use system messages. Together with [langchain](https://python.langchain.com/docs/integrations/llms/vllm#openai-compatible-server)\
    \ we are able to accomplish this. \n\n## Alternate solutions\nFor TS Junkies [here\
    \ is a solution](https://js.langchain.com/docs/integrations/chat/mistral).\nIf\
    \ you don't have the hardware to run the model, here is a [Mistral API solution\
    \ using langchain in Python](https://python.langchain.com/docs/integrations/chat/mistralai)\
    \ (Mistral API is in [beta](https://console.mistral.ai))\n\nHere is a post with\
    \ a [python solution](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/discussions/41#65284b827b99d91baa1037bb)\
    \ that may be more relevant to your error message. This one uses the user, assistant,\
    \ user framework that the error mentions. \n\n\n## Example\n```python\ninference_server_url\
    \ = \"http://localhost:8000/v1\"\n\nchat = ChatOpenAI(\n    model=\"mistralai/Mistral-7B-Instruct-v0.2\"\
    ,\n    openai_api_key=\"EMPTY\",\n    openai_api_base=inference_server_url,\n\
    \    max_tokens=250,\n    temperature=0,\n)\n\nmessages = [\n    SystemMessage(\n\
    \        content=\"You are a helpful assistant that translates English to Italian.\"\
    \n    ),\n    HumanMessage(\n        content=\"Translate the following sentence\
    \ from English to Italian: I love programming.\"\n    ),\n]\nchat(messages)\n\
    ```\n```python\nAIMessage(content=' Io amo programmare', additional_kwargs={},\
    \ example=False)\n```\n\nIf this doesn't help, then it's at least a handy reference. "
  created_at: 2024-01-02 23:12:01+00:00
  edited: true
  hidden: false
  id: 659498412a0a886ef0d8c98b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: mistralai/Mistral-7B-Instruct-v0.2
repo_type: model
status: open
target_branch: null
title: system prompt template
