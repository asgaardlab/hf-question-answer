!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xcjthu
conflicting_files: null
created_at: 2024-01-15 02:58:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f499a37019359a3c488ba6cc11751fc.svg
      fullname: Chaojun XIAO
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xcjthu
      type: user
    createdAt: '2024-01-15T02:58:14.000Z'
    data:
      edited: false
      editors:
      - xcjthu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8992947340011597
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f499a37019359a3c488ba6cc11751fc.svg
          fullname: Chaojun XIAO
          isHf: false
          isPro: false
          name: xcjthu
          type: user
        html: '<p>According to the <code>config.json</code> file, the base model <code>Mistral-7B-v0.1</code>
          and its corresponding instruction tuning version, <code>Mistral-7B-Instruct-v0.1</code>,
          have a maximum length of 32k. However, the report for <code>Mistral-7B</code>
          indicates that these models are trained within an 8k context window. So,
          what is the maximum length these models can handle? </p>

          <p>Additionally, the <code>config.json</code> file reveals that the RoPE
          base for <code>Mistral-7B-Instruct-v0.2</code> has changed from 10000.0
          to 1000000.0. Does this mean that the model was fine-tuned after the NTW-Aware
          positional encoding transformation?</p>

          '
        raw: "According to the `config.json` file, the base model `Mistral-7B-v0.1`\
          \ and its corresponding instruction tuning version, `Mistral-7B-Instruct-v0.1`,\
          \ have a maximum length of 32k. However, the report for `Mistral-7B` indicates\
          \ that these models are trained within an 8k context window. So, what is\
          \ the maximum length these models can handle? \r\n\r\nAdditionally, the\
          \ `config.json` file reveals that the RoPE base for `Mistral-7B-Instruct-v0.2`\
          \ has changed from 10000.0 to 1000000.0. Does this mean that the model was\
          \ fine-tuned after the NTW-Aware positional encoding transformation?"
        updatedAt: '2024-01-15T02:58:14.350Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - almugabo
    id: 65a49f46e82b0b8490ea320c
    type: comment
  author: xcjthu
  content: "According to the `config.json` file, the base model `Mistral-7B-v0.1`\
    \ and its corresponding instruction tuning version, `Mistral-7B-Instruct-v0.1`,\
    \ have a maximum length of 32k. However, the report for `Mistral-7B` indicates\
    \ that these models are trained within an 8k context window. So, what is the maximum\
    \ length these models can handle? \r\n\r\nAdditionally, the `config.json` file\
    \ reveals that the RoPE base for `Mistral-7B-Instruct-v0.2` has changed from 10000.0\
    \ to 1000000.0. Does this mean that the model was fine-tuned after the NTW-Aware\
    \ positional encoding transformation?"
  created_at: 2024-01-15 02:58:14+00:00
  edited: false
  hidden: false
  id: 65a49f46e82b0b8490ea320c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 37
repo_id: mistralai/Mistral-7B-Instruct-v0.2
repo_type: model
status: open
target_branch: null
title: What is the maximum length of Mistral-7B-Instruct-v0.2?
