!!python/object:huggingface_hub.community.DiscussionWithDetails
author: GokhanAI
conflicting_files: null
created_at: 2024-01-18 05:36:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd6e6be435b348a866b413725eb31515.svg
      fullname: Ersoz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GokhanAI
      type: user
    createdAt: '2024-01-18T05:36:35.000Z'
    data:
      edited: true
      editors:
      - GokhanAI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9134495258331299
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd6e6be435b348a866b413725eb31515.svg
          fullname: Ersoz
          isHf: false
          isPro: false
          name: GokhanAI
          type: user
        html: '<p>Hello,<br>My question is, I am applying a model SFT to chat data.
          I do this with the LLama-7b&amp;13b-Chat-hf and Mistral-Instruction_V_2.0
          models. </p>

          <p>But my question is, should we make this type of chat format SFT or should
          we apply DPO or RHLF directly?</p>

          <p>I apply LORA (for SFT).</p>

          <p>I observed that when I perform SFT with Lllama, the model loses its structure
          slightly, but it can also answer other questions other than data.</p>

          <p>But when I do this on Mistral, it loses its entire structure and cannot
          answer any questions other than data.</p>

          <p>What''s the problem with this? What path should we follow? Isn''t SFT
          suitable?<br>Do we need to do DPO or RHLF directly? Topics I really don''t
          understand. Where am I doing wrong?</p>

          <p>Also;<br>I am doing these operations by going over ready-made coding
          such as SFTrainer and DeepSpeedChat, but what should be the difference?
          How can I continue without destroying the structure?</p>

          <p>Caution: You can pay attention to BOS and EOS.</p>

          <p>The format is the same as this example;</p>

          <p>Llama Example:<br> -- &gt; <s>[INST] hello [/INST] how can i help you
          </s><s>[INST] .... [/INST] .... </s></p><s>

          <p>Mistral Example:<br>--&gt;  <s> [INST] hello [/INST] how can i help you
          </s>[INST] .... [/INST] .... </p>

          </s>'
        raw: "Hello,\nMy question is, I am applying a model SFT to chat data. I do\
          \ this with the LLama-7b&13b-Chat-hf and Mistral-Instruction_V_2.0 models.\
          \ \n\nBut my question is, should we make this type of chat format SFT or\
          \ should we apply DPO or RHLF directly?\n\nI apply LORA (for SFT).\n \n\
          I observed that when I perform SFT with Lllama, the model loses its structure\
          \ slightly, but it can also answer other questions other than data.\n\n\
          But when I do this on Mistral, it loses its entire structure and cannot\
          \ answer any questions other than data.\n\nWhat's the problem with this?\
          \ What path should we follow? Isn't SFT suitable?\nDo we need to do DPO\
          \ or RHLF directly? Topics I really don't understand. Where am I doing wrong?\n\
          \nAlso;\nI am doing these operations by going over ready-made coding such\
          \ as SFTrainer and DeepSpeedChat, but what should be the difference? How\
          \ can I continue without destroying the structure?\n\n\nCaution: You can\
          \ pay attention to BOS and EOS.\n\nThe format is the same as this example;\n\
          \nLlama Example:\n -- > <s>[INST] hello [/INST] how can i help you </s><s>[INST]\
          \ .... [/INST] .... \n\nMistral Example:\n-->  <s> [INST] hello [/INST]\
          \ how can i help you </s>[INST] .... [/INST] .... \n\n"
        updatedAt: '2024-01-18T05:37:44.869Z'
      numEdits: 2
      reactions: []
    id: 65a8b8e310342794b7044bd2
    type: comment
  author: GokhanAI
  content: "Hello,\nMy question is, I am applying a model SFT to chat data. I do this\
    \ with the LLama-7b&13b-Chat-hf and Mistral-Instruction_V_2.0 models. \n\nBut\
    \ my question is, should we make this type of chat format SFT or should we apply\
    \ DPO or RHLF directly?\n\nI apply LORA (for SFT).\n \nI observed that when I\
    \ perform SFT with Lllama, the model loses its structure slightly, but it can\
    \ also answer other questions other than data.\n\nBut when I do this on Mistral,\
    \ it loses its entire structure and cannot answer any questions other than data.\n\
    \nWhat's the problem with this? What path should we follow? Isn't SFT suitable?\n\
    Do we need to do DPO or RHLF directly? Topics I really don't understand. Where\
    \ am I doing wrong?\n\nAlso;\nI am doing these operations by going over ready-made\
    \ coding such as SFTrainer and DeepSpeedChat, but what should be the difference?\
    \ How can I continue without destroying the structure?\n\n\nCaution: You can pay\
    \ attention to BOS and EOS.\n\nThe format is the same as this example;\n\nLlama\
    \ Example:\n -- > <s>[INST] hello [/INST] how can i help you </s><s>[INST] ....\
    \ [/INST] .... \n\nMistral Example:\n-->  <s> [INST] hello [/INST] how can i help\
    \ you </s>[INST] .... [/INST] .... \n\n"
  created_at: 2024-01-18 05:36:35+00:00
  edited: true
  hidden: false
  id: 65a8b8e310342794b7044bd2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: mistralai/Mistral-7B-Instruct-v0.2
repo_type: model
status: open
target_branch: null
title: SFT Results So Bad
