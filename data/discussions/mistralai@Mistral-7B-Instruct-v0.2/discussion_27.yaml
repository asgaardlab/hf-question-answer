!!python/object:huggingface_hub.community.DiscussionWithDetails
author: roboboot
conflicting_files: null
created_at: 2023-12-20 16:22:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
      fullname: Roberto Battistoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roboboot
      type: user
    createdAt: '2023-12-20T16:22:55.000Z'
    data:
      edited: false
      editors:
      - roboboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6759809851646423
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
          fullname: Roberto Battistoni
          isHf: false
          isPro: false
          name: roboboot
          type: user
        html: '<p>I''m on Windows 10 Enterprise (64 bit - Intel(R) Xeon(R) Gold 6238R
          CPU @ 2.20GHz) and got the Mistral-7B-Instruct-v0.2. </p>

          <p>The running code is the following:</p>

          <p>**<br>from transformers import AutoModelForCausalLM, AutoTokenizer<br>device
          = "cuda" if torch.cuda.is_available() else "cpu"<br>model = AutoModelForCausalLM.from_pretrained("../Mistral-7B-Instruct-v0.2")<br>tokenizer
          = AutoTokenizer.from_pretrained("../Mistral-7B-Instruct-v0.2")<br>**</p>

          <p>Every time I run the code I get the error:</p>

          <p>Process finished with exit code -1073741819 (0xC0000005)</p>

          <p>Do you have any suggestions?</p>

          '
        raw: "I'm on Windows 10 Enterprise (64 bit - Intel(R) Xeon(R) Gold 6238R CPU\
          \ @ 2.20GHz) and got the Mistral-7B-Instruct-v0.2. \r\n\r\nThe running code\
          \ is the following:\r\n\r\n**\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\ndevice = \"cuda\" if torch.cuda.is_available() else \"\
          cpu\"\r\nmodel = AutoModelForCausalLM.from_pretrained(\"../Mistral-7B-Instruct-v0.2\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(\"../Mistral-7B-Instruct-v0.2\"\
          )\r\n**\r\n\r\nEvery time I run the code I get the error:\r\n\r\nProcess\
          \ finished with exit code -1073741819 (0xC0000005)\r\n\r\nDo you have any\
          \ suggestions?"
        updatedAt: '2023-12-20T16:22:55.229Z'
      numEdits: 0
      reactions: []
    id: 658314dfa38dbd48cec39634
    type: comment
  author: roboboot
  content: "I'm on Windows 10 Enterprise (64 bit - Intel(R) Xeon(R) Gold 6238R CPU\
    \ @ 2.20GHz) and got the Mistral-7B-Instruct-v0.2. \r\n\r\nThe running code is\
    \ the following:\r\n\r\n**\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    ../Mistral-7B-Instruct-v0.2\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    ../Mistral-7B-Instruct-v0.2\")\r\n**\r\n\r\nEvery time I run the code I get the\
    \ error:\r\n\r\nProcess finished with exit code -1073741819 (0xC0000005)\r\n\r\
    \nDo you have any suggestions?"
  created_at: 2023-12-20 16:22:55+00:00
  edited: false
  hidden: false
  id: 658314dfa38dbd48cec39634
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6587254a1a576242efec51fe/ueXomG9Q1RFABvXPjP4ep.png?w=200&h=200&f=face
      fullname: Delta Lux
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DeltaLux
      type: user
    createdAt: '2024-01-01T21:43:59.000Z'
    data:
      edited: false
      editors:
      - DeltaLux
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7223729491233826
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6587254a1a576242efec51fe/ueXomG9Q1RFABvXPjP4ep.png?w=200&h=200&f=face
          fullname: Delta Lux
          isHf: false
          isPro: false
          name: DeltaLux
          type: user
        html: '<p>Maybe try this:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM,
          AutoTokenizer

          model_id = <span class="hljs-string">"../Mistral-7B-Instruct-v0.2"</span>

          model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span
          class="hljs-string">"cpu"</span>)

          tokenizer = AutoTokenizer.from_pretrained(model_id)


          text = <span class="hljs-string">"&lt;s&gt;[INST] What is your favourite
          condiment? [/INST]"</span>

          inputs = tokenizer(text, return_tensors=<span class="hljs-string">"pt"</span>)


          outputs = model.generate(**inputs, max_new_tokens=<span class="hljs-number">20</span>)

          <span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span
          class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))

          </code></pre>

          <p>I got the same error and what fixed it was specifying the property <code>device_map="cpu"</code>
          when calling <code>from_pretrained</code>. I also get the same error when
          I run out of RAM while loading the model as on my machine it consumes about
          29 GB of memory. If this code doesn''t fix it, check the RAM consumption
          while it tries to load the model to see if it is that.</p>

          '
        raw: 'Maybe try this:


          ```python

          from transformers import AutoModelForCausalLM, AutoTokenizer

          model_id = "../Mistral-7B-Instruct-v0.2"

          model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cpu")

          tokenizer = AutoTokenizer.from_pretrained(model_id)


          text = "<s>[INST] What is your favourite condiment? [/INST]"

          inputs = tokenizer(text, return_tensors="pt")


          outputs = model.generate(**inputs, max_new_tokens=20)

          print(tokenizer.decode(outputs[0], skip_special_tokens=True))

          ```

          I got the same error and what fixed it was specifying the property `device_map="cpu"`
          when calling `from_pretrained`. I also get the same error when I run out
          of RAM while loading the model as on my machine it consumes about 29 GB
          of memory. If this code doesn''t fix it, check the RAM consumption while
          it tries to load the model to see if it is that.'
        updatedAt: '2024-01-01T21:43:59.253Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - roboboot
    id: 6593321fac02633c0dfc7054
    type: comment
  author: DeltaLux
  content: 'Maybe try this:


    ```python

    from transformers import AutoModelForCausalLM, AutoTokenizer

    model_id = "../Mistral-7B-Instruct-v0.2"

    model = AutoModelForCausalLM.from_pretrained(model_id, device_map="cpu")

    tokenizer = AutoTokenizer.from_pretrained(model_id)


    text = "<s>[INST] What is your favourite condiment? [/INST]"

    inputs = tokenizer(text, return_tensors="pt")


    outputs = model.generate(**inputs, max_new_tokens=20)

    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

    ```

    I got the same error and what fixed it was specifying the property `device_map="cpu"`
    when calling `from_pretrained`. I also get the same error when I run out of RAM
    while loading the model as on my machine it consumes about 29 GB of memory. If
    this code doesn''t fix it, check the RAM consumption while it tries to load the
    model to see if it is that.'
  created_at: 2024-01-01 21:43:59+00:00
  edited: false
  hidden: false
  id: 6593321fac02633c0dfc7054
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
      fullname: Roberto Battistoni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: roboboot
      type: user
    createdAt: '2024-01-02T08:10:40.000Z'
    data:
      edited: false
      editors:
      - roboboot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9440702795982361
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/884b079b78c2cb13527d5b70c8aa198d.svg
          fullname: Roberto Battistoni
          isHf: false
          isPro: false
          name: roboboot
          type: user
        html: '<p>Yes, true, it''s a memory problem. I have 16 GB of RAM and it''s
          not enough on a Windows machine.</p>

          <p>So the model will be entirely loaded in memory....</p>

          <p>thx</p>

          '
        raw: 'Yes, true, it''s a memory problem. I have 16 GB of RAM and it''s not
          enough on a Windows machine.


          So the model will be entirely loaded in memory....


          thx'
        updatedAt: '2024-01-02T08:10:40.596Z'
      numEdits: 0
      reactions: []
    id: 6593c5003533188375bf3280
    type: comment
  author: roboboot
  content: 'Yes, true, it''s a memory problem. I have 16 GB of RAM and it''s not enough
    on a Windows machine.


    So the model will be entirely loaded in memory....


    thx'
  created_at: 2024-01-02 08:10:40+00:00
  edited: false
  hidden: false
  id: 6593c5003533188375bf3280
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: mistralai/Mistral-7B-Instruct-v0.2
repo_type: model
status: open
target_branch: null
title: Process finished with exit code -1073741819 (0xC0000005) on Windows 10 Enterprise
  and Python 3.11
