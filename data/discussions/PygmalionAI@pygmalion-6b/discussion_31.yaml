!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Turbina99
conflicting_files: null
created_at: 2023-05-05 04:26:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Turbina99
      type: user
    createdAt: '2023-05-05T05:26:40.000Z'
    data:
      edited: false
      editors:
      - Turbina99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
          fullname: Max
          isHf: false
          isPro: false
          name: Turbina99
          type: user
        html: '<p>How  much ram it needs? my 24GB was not enought<br>looks like it
          is being  run on CPU</p>

          '
        raw: "How  much ram it needs? my 24GB was not enought\r\nlooks like it is\
          \ being  run on CPU"
        updatedAt: '2023-05-05T05:26:40.823Z'
      numEdits: 0
      reactions: []
    id: 64549390363bb3aaf9cbb9f0
    type: comment
  author: Turbina99
  content: "How  much ram it needs? my 24GB was not enought\r\nlooks like it is being\
    \  run on CPU"
  created_at: 2023-05-05 04:26:40+00:00
  edited: false
  hidden: false
  id: 64549390363bb3aaf9cbb9f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-05T07:25:13.000Z'
    data:
      edited: true
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>24GB should be more than enough for a 6B model... I run the Pygmalion
          7B model in full BF16 precision on my 16GB 4080. If it''s running on the
          CPU then it''s more likely that you haven''t installed one of the required
          libraries or something. I would suggest using Oobabooga installed via their
          installation scripts, here is a link to the Windows version: <a rel="nofollow"
          href="https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_windows.zip">https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_windows.zip</a></p>

          <p>The benefit of using their setup script is that it will install everything
          you need for your hardware. Also, if you tried using the GPU and the memory
          was not enough, it would likely just die and not work at all, I don''t think
          it would magically switch to CPU mode without you telling it to, so it sounds
          more like something''s not set up for it to use the GPU...</p>

          '
        raw: '24GB should be more than enough for a 6B model... I run the Pygmalion
          7B model in full BF16 precision on my 16GB 4080. If it''s running on the
          CPU then it''s more likely that you haven''t installed one of the required
          libraries or something. I would suggest using Oobabooga installed via their
          installation scripts, here is a link to the Windows version: https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_windows.zip


          The benefit of using their setup script is that it will install everything
          you need for your hardware. Also, if you tried using the GPU and the memory
          was not enough, it would likely just die and not work at all, I don''t think
          it would magically switch to CPU mode without you telling it to, so it sounds
          more like something''s not set up for it to use the GPU...'
        updatedAt: '2023-05-05T07:29:30.733Z'
      numEdits: 1
      reactions: []
    id: 6454af59d55525a4fedd14e1
    type: comment
  author: RazielAU
  content: '24GB should be more than enough for a 6B model... I run the Pygmalion
    7B model in full BF16 precision on my 16GB 4080. If it''s running on the CPU then
    it''s more likely that you haven''t installed one of the required libraries or
    something. I would suggest using Oobabooga installed via their installation scripts,
    here is a link to the Windows version: https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga_windows.zip


    The benefit of using their setup script is that it will install everything you
    need for your hardware. Also, if you tried using the GPU and the memory was not
    enough, it would likely just die and not work at all, I don''t think it would
    magically switch to CPU mode without you telling it to, so it sounds more like
    something''s not set up for it to use the GPU...'
  created_at: 2023-05-05 06:25:13+00:00
  edited: true
  hidden: false
  id: 6454af59d55525a4fedd14e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Turbina99
      type: user
    createdAt: '2023-05-05T07:35:51.000Z'
    data:
      edited: false
      editors:
      - Turbina99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
          fullname: Max
          isHf: false
          isPro: false
          name: Turbina99
          type: user
        html: '<p>well  I  tired to run it in pycharm using:</p>

          <p>from transformers import pipeline</p>

          <p>text_generation = pipeline("text-generation", model="PygmalionAI/pygmalion-6b")<br>generated_text
          = text_generation("Hello, how are you?")<br>print(generated_text[0][''generated_text''])</p>

          '
        raw: 'well  I  tired to run it in pycharm using:


          from transformers import pipeline


          text_generation = pipeline("text-generation", model="PygmalionAI/pygmalion-6b")

          generated_text = text_generation("Hello, how are you?")

          print(generated_text[0][''generated_text''])'
        updatedAt: '2023-05-05T07:35:51.629Z'
      numEdits: 0
      reactions: []
    id: 6454b1d7f61f10d69dbefd25
    type: comment
  author: Turbina99
  content: 'well  I  tired to run it in pycharm using:


    from transformers import pipeline


    text_generation = pipeline("text-generation", model="PygmalionAI/pygmalion-6b")

    generated_text = text_generation("Hello, how are you?")

    print(generated_text[0][''generated_text''])'
  created_at: 2023-05-05 06:35:51+00:00
  edited: false
  hidden: false
  id: 6454b1d7f61f10d69dbefd25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Turbina99
      type: user
    createdAt: '2023-05-05T07:39:44.000Z'
    data:
      edited: false
      editors:
      - Turbina99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
          fullname: Max
          isHf: false
          isPro: false
          name: Turbina99
          type: user
        html: '<p>on option with GPU, also does not work:<br>from transformers import
          pipeline</p>

          <p>text_generation = pipeline("text-generation",<br>                            model="PygmalionAI/pygmalion-6b",<br>                            device=0)  #
          specify the GPU device number</p>

          <p>generated_text = text_generation("Hello, how are you?")<br>print(generated_text[0][''generated_text''])</p>

          <p>and the RAM memroy 24GB is full</p>

          '
        raw: "on option with GPU, also does not work:\nfrom transformers import pipeline\n\
          \ntext_generation = pipeline(\"text-generation\",\n                    \
          \        model=\"PygmalionAI/pygmalion-6b\",\n                         \
          \   device=0)  # specify the GPU device number\n\ngenerated_text = text_generation(\"\
          Hello, how are you?\")\nprint(generated_text[0]['generated_text'])\n\nand\
          \ the RAM memroy 24GB is full"
        updatedAt: '2023-05-05T07:39:44.091Z'
      numEdits: 0
      reactions: []
    id: 6454b2c0fe2f48cb4b5d9e85
    type: comment
  author: Turbina99
  content: "on option with GPU, also does not work:\nfrom transformers import pipeline\n\
    \ntext_generation = pipeline(\"text-generation\",\n                          \
    \  model=\"PygmalionAI/pygmalion-6b\",\n                            device=0)\
    \  # specify the GPU device number\n\ngenerated_text = text_generation(\"Hello,\
    \ how are you?\")\nprint(generated_text[0]['generated_text'])\n\nand the RAM memroy\
    \ 24GB is full"
  created_at: 2023-05-05 06:39:44+00:00
  edited: false
  hidden: false
  id: 6454b2c0fe2f48cb4b5d9e85
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
      fullname: Francois Grobbelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RazielAU
      type: user
    createdAt: '2023-05-05T08:12:15.000Z'
    data:
      edited: true
      editors:
      - RazielAU
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/612aa89fa431d67a04818e1ac312bcec.svg
          fullname: Francois Grobbelaar
          isHf: false
          isPro: false
          name: RazielAU
          type: user
        html: '<p>Oh, you''re trying to do this in code? I''ll pass this over to someone
          else to support, my suggestion is to start with oobabooga as it has both
          example code and installs all the libraries you need. As I said earlier,
          GPU support requires a whole bunch of extra libraries, it''s not going to
          work if you don''t have them installed. If your hardware doesn''t support
          16bit, then you might have to load it in 8 bit mode. Again, check ooba for
          code/requirement examples. Also, do remember that oobabooga provides a Kobold
          compatible API and a new streaming text API, so you can connect to it via
          API and use it that way as well.</p>

          '
        raw: Oh, you're trying to do this in code? I'll pass this over to someone
          else to support, my suggestion is to start with oobabooga as it has both
          example code and installs all the libraries you need. As I said earlier,
          GPU support requires a whole bunch of extra libraries, it's not going to
          work if you don't have them installed. If your hardware doesn't support
          16bit, then you might have to load it in 8 bit mode. Again, check ooba for
          code/requirement examples. Also, do remember that oobabooga provides a Kobold
          compatible API and a new streaming text API, so you can connect to it via
          API and use it that way as well.
        updatedAt: '2023-05-05T08:13:43.639Z'
      numEdits: 1
      reactions: []
    id: 6454ba5ff61f10d69dbfe730
    type: comment
  author: RazielAU
  content: Oh, you're trying to do this in code? I'll pass this over to someone else
    to support, my suggestion is to start with oobabooga as it has both example code
    and installs all the libraries you need. As I said earlier, GPU support requires
    a whole bunch of extra libraries, it's not going to work if you don't have them
    installed. If your hardware doesn't support 16bit, then you might have to load
    it in 8 bit mode. Again, check ooba for code/requirement examples. Also, do remember
    that oobabooga provides a Kobold compatible API and a new streaming text API,
    so you can connect to it via API and use it that way as well.
  created_at: 2023-05-05 07:12:15+00:00
  edited: true
  hidden: false
  id: 6454ba5ff61f10d69dbfe730
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
      fullname: Max
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Turbina99
      type: user
    createdAt: '2023-05-05T17:43:58.000Z'
    data:
      edited: false
      editors:
      - Turbina99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cf867f5fa3782eec6efb66a423502ed.svg
          fullname: Max
          isHf: false
          isPro: false
          name: Turbina99
          type: user
        html: '<p>Yes i tried to load it in  pycharm, becase i wanted to attach some
          logic</p>

          '
        raw: Yes i tried to load it in  pycharm, becase i wanted to attach some logic
        updatedAt: '2023-05-05T17:43:58.417Z'
      numEdits: 0
      reactions: []
    id: 6455405efe2f48cb4b6b9ea5
    type: comment
  author: Turbina99
  content: Yes i tried to load it in  pycharm, becase i wanted to attach some logic
  created_at: 2023-05-05 16:43:58+00:00
  edited: false
  hidden: false
  id: 6455405efe2f48cb4b6b9ea5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5315b0d4924b551bc26dd89d6b055039.svg
      fullname: Alec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlecKarfonta
      type: user
    createdAt: '2023-05-11T18:02:07.000Z'
    data:
      edited: false
      editors:
      - AlecKarfonta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5315b0d4924b551bc26dd89d6b055039.svg
          fullname: Alec
          isHf: false
          isPro: false
          name: AlecKarfonta
          type: user
        html: '<p>I experienced the same thing. It takes 26-27gb so just barely too
          big for the gpu. The precision would help but I''m not sure where to set
          that.</p>

          '
        raw: I experienced the same thing. It takes 26-27gb so just barely too big
          for the gpu. The precision would help but I'm not sure where to set that.
        updatedAt: '2023-05-11T18:02:07.545Z'
      numEdits: 0
      reactions: []
    id: 645d2d9ff1e3b219cb0ca520
    type: comment
  author: AlecKarfonta
  content: I experienced the same thing. It takes 26-27gb so just barely too big for
    the gpu. The precision would help but I'm not sure where to set that.
  created_at: 2023-05-11 17:02:07+00:00
  edited: false
  hidden: false
  id: 645d2d9ff1e3b219cb0ca520
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5315b0d4924b551bc26dd89d6b055039.svg
      fullname: Alec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlecKarfonta
      type: user
    createdAt: '2023-05-13T16:17:05.000Z'
    data:
      edited: false
      editors:
      - AlecKarfonta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5315b0d4924b551bc26dd89d6b055039.svg
          fullname: Alec
          isHf: false
          isPro: false
          name: AlecKarfonta
          type: user
        html: '<p>Found you can set the precision by calling model.half()<br>Need
          to also call that at the end of your input tensor.</p>

          <p>Something to note your cpu might not support all the fp16 operations
          so if you use this is will likely only run on the gpu now. So just make
          sure to call model.cuda()</p>

          '
        raw: 'Found you can set the precision by calling model.half()

          Need to also call that at the end of your input tensor.


          Something to note your cpu might not support all the fp16 operations so
          if you use this is will likely only run on the gpu now. So just make sure
          to call model.cuda()'
        updatedAt: '2023-05-13T16:17:05.991Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Cloud789
    id: 645fb801a93c1779eb0dbb14
    type: comment
  author: AlecKarfonta
  content: 'Found you can set the precision by calling model.half()

    Need to also call that at the end of your input tensor.


    Something to note your cpu might not support all the fp16 operations so if you
    use this is will likely only run on the gpu now. So just make sure to call model.cuda()'
  created_at: 2023-05-13 15:17:05+00:00
  edited: false
  hidden: false
  id: 645fb801a93c1779eb0dbb14
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/26c6304ba44e271cf8dd0b6707fbc73a.svg
      fullname: Robby Knospe
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kirisuma
      type: user
    createdAt: '2023-08-18T12:16:43.000Z'
    data:
      edited: false
      editors:
      - Kirisuma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9844120144844055
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/26c6304ba44e271cf8dd0b6707fbc73a.svg
          fullname: Robby Knospe
          isHf: false
          isPro: false
          name: Kirisuma
          type: user
        html: '<p>its running fine on my 8gb 3060ti. Sure response time is somewhat
          between 15 to 25 seconds. But I can life with that. </p>

          '
        raw: 'its running fine on my 8gb 3060ti. Sure response time is somewhat between
          15 to 25 seconds. But I can life with that. '
        updatedAt: '2023-08-18T12:16:43.692Z'
      numEdits: 0
      reactions: []
    id: 64df612b75f5da105e96562f
    type: comment
  author: Kirisuma
  content: 'its running fine on my 8gb 3060ti. Sure response time is somewhat between
    15 to 25 seconds. But I can life with that. '
  created_at: 2023-08-18 11:16:43+00:00
  edited: false
  hidden: false
  id: 64df612b75f5da105e96562f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 31
repo_id: PygmalionAI/pygmalion-6b
repo_type: model
status: open
target_branch: null
title: 'How  much ram it needs? '
