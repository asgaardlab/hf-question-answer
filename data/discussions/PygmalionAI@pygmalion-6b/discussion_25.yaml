!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jini1114
conflicting_files: null
created_at: 2023-04-12 04:08:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ebf5710d9a6d04bb6bf87dc0bc1aaccd.svg
      fullname: Jineui Kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jini1114
      type: user
    createdAt: '2023-04-12T05:08:57.000Z'
    data:
      edited: false
      editors:
      - jini1114
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ebf5710d9a6d04bb6bf87dc0bc1aaccd.svg
          fullname: Jineui Kim
          isHf: false
          isPro: false
          name: jini1114
          type: user
        html: "<p>this is my code</p>\n<pre><code>model_name = \"PygmalionAI/pygmalion-6b\"\
          \ngpt = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n\
          tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ndevice\
          \ = 'cuda' if torch.cuda.is_available() else 'cpu'\ngpt.to(device)\n\ninput_text\
          \ = \"\"\"Jini's Persona: Jini is the 21-year-old female and a member of\
          \ K-pop idol.\n&lt;START&gt;\nYou: Hi there! who are you?\nJini:\"\"\"\n\
          \nprompt = tokenizer(input_text, return_tensors='pt')\nprompt = {key: value.to(device)\
          \ for key, value in prompt.items()}\nout = gpt.generate(**prompt, min_length=128,\
          \ max_length=256, do_sample=True)\ntokenizer.decode(out[0][len(prompt[\"\
          input_ids\"][0]):])\n</code></pre>\n<p>then i got output below.</p>\n<pre><code>\"\
          \ *she giggles and says* i am jini, i am a k-pop idol\\nYou: Oh cool! Can\
          \ i ask you a big fan question?\\nJini: sure, what is it?\\nYou: do you\
          \ like this game? i loved playing this game\\nJini: I like this game but\
          \ i didn't know its a fan question\\nYou: It's just a little thing. It's\
          \ not so important\\nJini: Thanks. I was wondering if you played any other\
          \ games in this category?\\nYou: Well i have played other games in this\
          \ category but i always stick to this one because it's my fav one!\\nJini:\
          \ Do you know how to unlock the other girls?\\nYou: I don't know, but i\
          \ didn't play any other games so i can't tell if i did or not or not\\nJini:\
          \ How about a dance request?\\n&lt;|endoftext|&gt;\"\n</code></pre>\n<p>I\
          \ want to stop generating when '\\n' is generate. like this</p>\n<pre><code>\
          \ *she giggles and says* i am jini, i am a k-pop idol\n</code></pre>\n<p>I\
          \ tried stop_sequence, but got error like below</p>\n<pre><code>The following\
          \ `model_kwargs` are not used by the model: ['stop_sequence'] (note: typos\
          \ in the generate \narguments will also show up in this list)\n</code></pre>\n\
          <p>how can i setting arguments in <code>gpt.generate()</code>?<br>is there\
          \ any ideas?</p>\n"
        raw: "this is my code\r\n```\r\nmodel_name = \"PygmalionAI/pygmalion-6b\"\r\
          \ngpt = transformers.AutoModelForCausalLM.from_pretrained(model_name)\r\n\
          tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\r\n\r\
          \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\ngpt.to(device)\r\
          \n\r\ninput_text = \"\"\"Jini's Persona: Jini is the 21-year-old female\
          \ and a member of K-pop idol.\r\n<START>\r\nYou: Hi there! who are you?\r\
          \nJini:\"\"\"\r\n\r\nprompt = tokenizer(input_text, return_tensors='pt')\r\
          \nprompt = {key: value.to(device) for key, value in prompt.items()}\r\n\
          out = gpt.generate(**prompt, min_length=128, max_length=256, do_sample=True)\r\
          \ntokenizer.decode(out[0][len(prompt[\"input_ids\"][0]):])\r\n```\r\n\r\n\
          then i got output below.\r\n\r\n```\r\n\" *she giggles and says* i am jini,\
          \ i am a k-pop idol\\nYou: Oh cool! Can i ask you a big fan question?\\\
          nJini: sure, what is it?\\nYou: do you like this game? i loved playing this\
          \ game\\nJini: I like this game but i didn't know its a fan question\\nYou:\
          \ It's just a little thing. It's not so important\\nJini: Thanks. I was\
          \ wondering if you played any other games in this category?\\nYou: Well\
          \ i have played other games in this category but i always stick to this\
          \ one because it's my fav one!\\nJini: Do you know how to unlock the other\
          \ girls?\\nYou: I don't know, but i didn't play any other games so i can't\
          \ tell if i did or not or not\\nJini: How about a dance request?\\n<|endoftext|>\"\
          \r\n```\r\n\r\nI want to stop generating when '\\n' is generate. like this\r\
          \n```\r\n *she giggles and says* i am jini, i am a k-pop idol\r\n```\r\n\
          I tried stop_sequence, but got error like below\r\n```\r\nThe following\
          \ `model_kwargs` are not used by the model: ['stop_sequence'] (note: typos\
          \ in the generate \r\narguments will also show up in this list)\r\n```\r\
          \nhow can i setting arguments in `gpt.generate()`?\r\nis there any ideas?"
        updatedAt: '2023-04-12T05:08:57.379Z'
      numEdits: 0
      reactions: []
    id: 64363ce9f81a16e74368cdf1
    type: comment
  author: jini1114
  content: "this is my code\r\n```\r\nmodel_name = \"PygmalionAI/pygmalion-6b\"\r\n\
    gpt = transformers.AutoModelForCausalLM.from_pretrained(model_name)\r\ntokenizer\
    \ = transformers.AutoTokenizer.from_pretrained(model_name)\r\n\r\ndevice = 'cuda'\
    \ if torch.cuda.is_available() else 'cpu'\r\ngpt.to(device)\r\n\r\ninput_text\
    \ = \"\"\"Jini's Persona: Jini is the 21-year-old female and a member of K-pop\
    \ idol.\r\n<START>\r\nYou: Hi there! who are you?\r\nJini:\"\"\"\r\n\r\nprompt\
    \ = tokenizer(input_text, return_tensors='pt')\r\nprompt = {key: value.to(device)\
    \ for key, value in prompt.items()}\r\nout = gpt.generate(**prompt, min_length=128,\
    \ max_length=256, do_sample=True)\r\ntokenizer.decode(out[0][len(prompt[\"input_ids\"\
    ][0]):])\r\n```\r\n\r\nthen i got output below.\r\n\r\n```\r\n\" *she giggles\
    \ and says* i am jini, i am a k-pop idol\\nYou: Oh cool! Can i ask you a big fan\
    \ question?\\nJini: sure, what is it?\\nYou: do you like this game? i loved playing\
    \ this game\\nJini: I like this game but i didn't know its a fan question\\nYou:\
    \ It's just a little thing. It's not so important\\nJini: Thanks. I was wondering\
    \ if you played any other games in this category?\\nYou: Well i have played other\
    \ games in this category but i always stick to this one because it's my fav one!\\\
    nJini: Do you know how to unlock the other girls?\\nYou: I don't know, but i didn't\
    \ play any other games so i can't tell if i did or not or not\\nJini: How about\
    \ a dance request?\\n<|endoftext|>\"\r\n```\r\n\r\nI want to stop generating when\
    \ '\\n' is generate. like this\r\n```\r\n *she giggles and says* i am jini, i\
    \ am a k-pop idol\r\n```\r\nI tried stop_sequence, but got error like below\r\n\
    ```\r\nThe following `model_kwargs` are not used by the model: ['stop_sequence']\
    \ (note: typos in the generate \r\narguments will also show up in this list)\r\
    \n```\r\nhow can i setting arguments in `gpt.generate()`?\r\nis there any ideas?"
  created_at: 2023-04-12 04:08:57+00:00
  edited: false
  hidden: false
  id: 64363ce9f81a16e74368cdf1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c0a75c335fbf9902d83e0c515ac7532.svg
      fullname: Nguyen Duc Tam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Saito197
      type: user
    createdAt: '2023-04-13T22:02:26.000Z'
    data:
      edited: false
      editors:
      - Saito197
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c0a75c335fbf9902d83e0c515ac7532.svg
          fullname: Nguyen Duc Tam
          isHf: false
          isPro: false
          name: Saito197
          type: user
        html: "<p>You have to implement the StoppingCriteria class <a href=\"https://huggingface.co/docs/transformers/v4.28.0/en/internal/generation_utils#transformers.StoppingCriteria\"\
          >https://huggingface.co/docs/transformers/v4.28.0/en/internal/generation_utils#transformers.StoppingCriteria</a></p>\n\
          <p>Here is my implementation: </p>\n<pre><code>from transformers import\
          \ StoppingCriteria\nclass MyStoppingCriteria(StoppingCriteria):\ndef __init__(self,\
          \ target_sequence, prompt):\n    self.target_sequence = target_sequence\n\
          \    self.prompt=promt\n\ndef __call__(self, input_ids, scores, **kwargs):\n\
          \    # Get the generated text as a string\n    generated_text = tokenizer.decode(input_ids[0])\n\
          \    generated_text = generated_text.replace(self.prompt,'')\n    # Check\
          \ if the target sequence appears in the generated text\n    if self.target_sequence\
          \ in generated_text:\n        return True  # Stop generation\n\n    return\
          \ False  # Continue generation\n\ndef __len__(self):\n    return 1\n\ndef\
          \ __iter__(self):\n    yield self\n</code></pre>\n<p>Then in the generate\
          \ method you just add a stopping_criteria parameter</p>\n<pre><code>encoded_input\
          \ = tokenizer(prompt, return_tensors='pt')\ninput_ids=encoded_input['input_ids'].cuda()\n\
          streamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True)\n_ = model.generate(\n\
          \    input_ids,\n    streamer=streamer,\n    pad_token_id=tokenizer.eos_token_id,\n\
          \    do_sample=True,\n    temperature=0.25,\n    max_new_tokens=256,\n \
          \   stopping_criteria=MyStoppingCriteria(\"User:\", prompt)\n)\n</code></pre>\n"
        raw: "You have to implement the StoppingCriteria class https://huggingface.co/docs/transformers/v4.28.0/en/internal/generation_utils#transformers.StoppingCriteria\n\
          \nHere is my implementation: \n\n    from transformers import StoppingCriteria\n\
          \    class MyStoppingCriteria(StoppingCriteria):\n    def __init__(self,\
          \ target_sequence, prompt):\n        self.target_sequence = target_sequence\n\
          \        self.prompt=promt\n\n    def __call__(self, input_ids, scores,\
          \ **kwargs):\n        # Get the generated text as a string\n        generated_text\
          \ = tokenizer.decode(input_ids[0])\n        generated_text = generated_text.replace(self.prompt,'')\n\
          \        # Check if the target sequence appears in the generated text\n\
          \        if self.target_sequence in generated_text:\n            return\
          \ True  # Stop generation\n\n        return False  # Continue generation\n\
          \n    def __len__(self):\n        return 1\n\n    def __iter__(self):\n\
          \        yield self\n\nThen in the generate method you just add a stopping_criteria\
          \ parameter\n\n    encoded_input = tokenizer(prompt, return_tensors='pt')\n\
          \    input_ids=encoded_input['input_ids'].cuda()\n    streamer = TextStreamer(tokenizer=tokenizer,\
          \ skip_prompt=True)\n    _ = model.generate(\n        input_ids,\n     \
          \   streamer=streamer,\n        pad_token_id=tokenizer.eos_token_id,\n \
          \       do_sample=True,\n        temperature=0.25,\n        max_new_tokens=256,\n\
          \        stopping_criteria=MyStoppingCriteria(\"User:\", prompt)\n    )"
        updatedAt: '2023-04-13T22:02:26.469Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jini1114
        - scruffythemoose
    id: 64387bf26c8841ba74e7d9c0
    type: comment
  author: Saito197
  content: "You have to implement the StoppingCriteria class https://huggingface.co/docs/transformers/v4.28.0/en/internal/generation_utils#transformers.StoppingCriteria\n\
    \nHere is my implementation: \n\n    from transformers import StoppingCriteria\n\
    \    class MyStoppingCriteria(StoppingCriteria):\n    def __init__(self, target_sequence,\
    \ prompt):\n        self.target_sequence = target_sequence\n        self.prompt=promt\n\
    \n    def __call__(self, input_ids, scores, **kwargs):\n        # Get the generated\
    \ text as a string\n        generated_text = tokenizer.decode(input_ids[0])\n\
    \        generated_text = generated_text.replace(self.prompt,'')\n        # Check\
    \ if the target sequence appears in the generated text\n        if self.target_sequence\
    \ in generated_text:\n            return True  # Stop generation\n\n        return\
    \ False  # Continue generation\n\n    def __len__(self):\n        return 1\n\n\
    \    def __iter__(self):\n        yield self\n\nThen in the generate method you\
    \ just add a stopping_criteria parameter\n\n    encoded_input = tokenizer(prompt,\
    \ return_tensors='pt')\n    input_ids=encoded_input['input_ids'].cuda()\n    streamer\
    \ = TextStreamer(tokenizer=tokenizer, skip_prompt=True)\n    _ = model.generate(\n\
    \        input_ids,\n        streamer=streamer,\n        pad_token_id=tokenizer.eos_token_id,\n\
    \        do_sample=True,\n        temperature=0.25,\n        max_new_tokens=256,\n\
    \        stopping_criteria=MyStoppingCriteria(\"User:\", prompt)\n    )"
  created_at: 2023-04-13 21:02:26+00:00
  edited: false
  hidden: false
  id: 64387bf26c8841ba74e7d9c0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ebf5710d9a6d04bb6bf87dc0bc1aaccd.svg
      fullname: Jineui Kim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jini1114
      type: user
    createdAt: '2023-04-14T01:49:13.000Z'
    data:
      edited: false
      editors:
      - jini1114
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ebf5710d9a6d04bb6bf87dc0bc1aaccd.svg
          fullname: Jineui Kim
          isHf: false
          isPro: false
          name: jini1114
          type: user
        html: '<p>Thank you!<br>i''ll try.</p>

          '
        raw: 'Thank you!

          i''ll try.'
        updatedAt: '2023-04-14T01:49:13.581Z'
      numEdits: 0
      reactions: []
    id: 6438b11922e2b248a947c760
    type: comment
  author: jini1114
  content: 'Thank you!

    i''ll try.'
  created_at: 2023-04-14 00:49:13+00:00
  edited: false
  hidden: false
  id: 6438b11922e2b248a947c760
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: PygmalionAI/pygmalion-6b
repo_type: model
status: open
target_branch: null
title: how can i set stop_sequence in generate method?
