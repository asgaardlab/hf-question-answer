!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Uilo
conflicting_files: null
created_at: 2023-02-28 00:26:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
      fullname: Tren
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Uilo
      type: user
    createdAt: '2023-02-28T00:26:59.000Z'
    data:
      edited: false
      editors:
      - Uilo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
          fullname: Tren
          isHf: false
          isPro: false
          name: Uilo
          type: user
        html: "<p>I'm new to this, just trying to get started using the model on SageMaker,\
          \ using the new Deploy to SageMaker function/script.</p>\n<p>After copying\
          \ across, and starting an inference endpoint (using the supplied code) I\
          \ get the following error when trying to run:</p>\n<blockquote>\n<p>ModelError:\
          \ An error occurred (ModelError) when calling the InvokeEndpoint operation:\
          \ Received client error (400) from primary with message \"{<br>  \"code\"\
          : 400,<br>  \"type\": \"InternalServerException\",<br>  \"message\": \"\
          Could not load model /.sagemaker/mms/models/PygmalionAI__pygmalion-6b with\
          \ any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.gptj.modeling_gptj.GPTJForCausalLM\\\
          u0027\\u003e).\"<br>}</p>\n</blockquote>\n<p>This is the input, as supplied\
          \ in the deploy script:</p>\n<pre><code>predictor.predict({\n'inputs': {\n\
          \    \"past_user_inputs\": [\"Which movie is the best ?\"],\n    \"generated_responses\"\
          : [\"It's Die Hard for sure.\"],\n    \"text\": \"Can you explain why ?\"\
          \n}\n})\n</code></pre>\n<p>Cloudwatch logs:</p>\n<blockquote>\n<p>[INFO\
          \ ] W-PygmalionAI__pygmalion-6b-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
          \ - mms.service.PredictionException: Could not load model /.sagemaker/mms/models/PygmalionAI__pygmalion-6b\
          \ with any of the following classes: (&lt;class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'&gt;,\
          \ &lt;class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'&gt;,\
          \ &lt;class 'transformers.models.gptj.modeling_gptj.GPTJForCausalLM'&gt;).\
          \ : 400</p>\n</blockquote>\n"
        raw: "I'm new to this, just trying to get started using the model on SageMaker,\
          \ using the new Deploy to SageMaker function/script.\r\n\r\nAfter copying\
          \ across, and starting an inference endpoint (using the supplied code) I\
          \ get the following error when trying to run:\r\n\r\n> ModelError: An error\
          \ occurred (ModelError) when calling the InvokeEndpoint operation: Received\
          \ client error (400) from primary with message \"{\r\n  \"code\": 400,\r\
          \n  \"type\": \"InternalServerException\",\r\n  \"message\": \"Could not\
          \ load model /.sagemaker/mms/models/PygmalionAI__pygmalion-6b with any of\
          \ the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.gptj.modeling_gptj.GPTJForCausalLM\\\
          u0027\\u003e).\"\r\n}\r\n\r\nThis is the input, as supplied in the deploy\
          \ script:\r\n\r\n    predictor.predict({\r\n\t'inputs': {\r\n\t\t\"past_user_inputs\"\
          : [\"Which movie is the best ?\"],\r\n\t\t\"generated_responses\": [\"It's\
          \ Die Hard for sure.\"],\r\n\t\t\"text\": \"Can you explain why ?\"\r\n\t\
          }\r\n    })\r\n\r\nCloudwatch logs:\r\n\r\n> [INFO ] W-PygmalionAI__pygmalion-6b-2-stdout\
          \ com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException:\
          \ Could not load model /.sagemaker/mms/models/PygmalionAI__pygmalion-6b\
          \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,\
          \ <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,\
          \ <class 'transformers.models.gptj.modeling_gptj.GPTJForCausalLM'>). : 400\r\
          \n\r\n"
        updatedAt: '2023-02-28T00:26:59.567Z'
      numEdits: 0
      reactions: []
    id: 63fd4a533c880680af452e00
    type: comment
  author: Uilo
  content: "I'm new to this, just trying to get started using the model on SageMaker,\
    \ using the new Deploy to SageMaker function/script.\r\n\r\nAfter copying across,\
    \ and starting an inference endpoint (using the supplied code) I get the following\
    \ error when trying to run:\r\n\r\n> ModelError: An error occurred (ModelError)\
    \ when calling the InvokeEndpoint operation: Received client error (400) from\
    \ primary with message \"{\r\n  \"code\": 400,\r\n  \"type\": \"InternalServerException\"\
    ,\r\n  \"message\": \"Could not load model /.sagemaker/mms/models/PygmalionAI__pygmalion-6b\
    \ with any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM\\\
    u0027\\u003e, \\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
    u0027\\u003e, \\u003cclass \\u0027transformers.models.gptj.modeling_gptj.GPTJForCausalLM\\\
    u0027\\u003e).\"\r\n}\r\n\r\nThis is the input, as supplied in the deploy script:\r\
    \n\r\n    predictor.predict({\r\n\t'inputs': {\r\n\t\t\"past_user_inputs\": [\"\
    Which movie is the best ?\"],\r\n\t\t\"generated_responses\": [\"It's Die Hard\
    \ for sure.\"],\r\n\t\t\"text\": \"Can you explain why ?\"\r\n\t}\r\n    })\r\n\
    \r\nCloudwatch logs:\r\n\r\n> [INFO ] W-PygmalionAI__pygmalion-6b-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle\
    \ - mms.service.PredictionException: Could not load model /.sagemaker/mms/models/PygmalionAI__pygmalion-6b\
    \ with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSeq2SeqLM'>,\
    \ <class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class\
    \ 'transformers.models.gptj.modeling_gptj.GPTJForCausalLM'>). : 400\r\n\r\n"
  created_at: 2023-02-28 00:26:59+00:00
  edited: false
  hidden: false
  id: 63fd4a533c880680af452e00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
      fullname: Tren
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Uilo
      type: user
    createdAt: '2023-02-28T04:31:39.000Z'
    data:
      edited: false
      editors:
      - Uilo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
          fullname: Tren
          isHf: false
          isPro: false
          name: Uilo
          type: user
        html: '<p>Seems I have to supply the model via S3 in a .tar.gz format.</p>

          <p>After doing that I get an InternalServiceException:  gpt_neox</p>

          <p>According to EleutherAI this is related to an incompatible Transformer
          version. The SageMaker boilerplate code specifies 4.17. Eleuther says to
          use 4.25... but SageMaker doesn''t support anything past 4.17.... oh dear...</p>

          <p>So is there  a workaround by using a requirements.txt file along with
          the model somehow? Any hints on how to do that, and what versions do we
          need to be using?</p>

          '
        raw: 'Seems I have to supply the model via S3 in a .tar.gz format.


          After doing that I get an InternalServiceException:  gpt_neox


          According to EleutherAI this is related to an incompatible Transformer version.
          The SageMaker boilerplate code specifies 4.17. Eleuther says to use 4.25...
          but SageMaker doesn''t support anything past 4.17.... oh dear...


          So is there  a workaround by using a requirements.txt file along with the
          model somehow? Any hints on how to do that, and what versions do we need
          to be using?'
        updatedAt: '2023-02-28T04:31:39.406Z'
      numEdits: 0
      reactions: []
    id: 63fd83ab9ac81ec88b09d162
    type: comment
  author: Uilo
  content: 'Seems I have to supply the model via S3 in a .tar.gz format.


    After doing that I get an InternalServiceException:  gpt_neox


    According to EleutherAI this is related to an incompatible Transformer version.
    The SageMaker boilerplate code specifies 4.17. Eleuther says to use 4.25... but
    SageMaker doesn''t support anything past 4.17.... oh dear...


    So is there  a workaround by using a requirements.txt file along with the model
    somehow? Any hints on how to do that, and what versions do we need to be using?'
  created_at: 2023-02-28 04:31:39+00:00
  edited: false
  hidden: false
  id: 63fd83ab9ac81ec88b09d162
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-03-01T14:56:32.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<p>Unfortunately I can''t really help you with this - I don''t use
          SageMaker so I have no idea how it works.</p>

          '
        raw: Unfortunately I can't really help you with this - I don't use SageMaker
          so I have no idea how it works.
        updatedAt: '2023-03-01T14:56:32.034Z'
      numEdits: 0
      reactions: []
    id: 63ff67a02e30e7993ee27e46
    type: comment
  author: 11b
  content: Unfortunately I can't really help you with this - I don't use SageMaker
    so I have no idea how it works.
  created_at: 2023-03-01 14:56:32+00:00
  edited: false
  hidden: false
  id: 63ff67a02e30e7993ee27e46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
      fullname: Tren
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Uilo
      type: user
    createdAt: '2023-03-02T01:35:45.000Z'
    data:
      edited: false
      editors:
      - Uilo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
          fullname: Tren
          isHf: false
          isPro: false
          name: Uilo
          type: user
        html: '<p>I managed to get the 1.3B version going, just trying to get the
          6B going now.</p>

          <p>Requirements text:</p>

          <pre><code>transformers==4.24.0

          torch==1.13.1

          </code></pre>

          <p>This is the error I''m getting now when trying to launch 6b:</p>

          <blockquote>

          <p>[WARN ] W-9000-model com.amazonaws.ml.mms.wlm.BatchAggregator - Load
          model failed: model, error: Worker died.</p>

          </blockquote>

          '
        raw: "I managed to get the 1.3B version going, just trying to get the 6B going\
          \ now.\n\nRequirements text:\n\n    transformers==4.24.0\n    torch==1.13.1\n\
          \n\nThis is the error I'm getting now when trying to launch 6b:\n\n> [WARN\
          \ ] W-9000-model com.amazonaws.ml.mms.wlm.BatchAggregator - Load model failed:\
          \ model, error: Worker died."
        updatedAt: '2023-03-02T01:35:45.338Z'
      numEdits: 0
      reactions: []
    id: 63fffd71ea1782e758d0189a
    type: comment
  author: Uilo
  content: "I managed to get the 1.3B version going, just trying to get the 6B going\
    \ now.\n\nRequirements text:\n\n    transformers==4.24.0\n    torch==1.13.1\n\n\
    \nThis is the error I'm getting now when trying to launch 6b:\n\n> [WARN ] W-9000-model\
    \ com.amazonaws.ml.mms.wlm.BatchAggregator - Load model failed: model, error:\
    \ Worker died."
  created_at: 2023-03-02 01:35:45+00:00
  edited: false
  hidden: false
  id: 63fffd71ea1782e758d0189a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-03-17T20:45:00.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<p>Again, I can''t really help with this since I''ve never used SageMaker.
          Your best bet is probably contacting whatever support channel SageMaker
          offers to their customers.</p>

          '
        raw: Again, I can't really help with this since I've never used SageMaker.
          Your best bet is probably contacting whatever support channel SageMaker
          offers to their customers.
        updatedAt: '2023-03-17T20:45:00.272Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6414d14cb6e553f665ec9cdd
    id: 6414d14cb6e553f665ec9cdc
    type: comment
  author: 11b
  content: Again, I can't really help with this since I've never used SageMaker. Your
    best bet is probably contacting whatever support channel SageMaker offers to their
    customers.
  created_at: 2023-03-17 19:45:00+00:00
  edited: false
  hidden: false
  id: 6414d14cb6e553f665ec9cdc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-03-17T20:45:00.000Z'
    data:
      status: closed
    id: 6414d14cb6e553f665ec9cdd
    type: status-change
  author: 11b
  created_at: 2023-03-17 19:45:00+00:00
  id: 6414d14cb6e553f665ec9cdd
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a8062a5858f939ba72e2a031993063e.svg
      fullname: Vikram Cothur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fire-Hound
      type: user
    createdAt: '2023-04-29T14:58:52.000Z'
    data:
      edited: false
      editors:
      - Fire-Hound
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a8062a5858f939ba72e2a031993063e.svg
          fullname: Vikram Cothur
          isHf: false
          isPro: false
          name: Fire-Hound
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Uilo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Uilo\">@<span class=\"\
          underline\">Uilo</span></a></span>\n\n\t</span></span> I have the same issue,\
          \ did you solve it?</p>\n"
        raw: '@Uilo I have the same issue, did you solve it?'
        updatedAt: '2023-04-29T14:58:52.356Z'
      numEdits: 0
      reactions: []
    id: 644d30ac0dc952d2459f3f4b
    type: comment
  author: Fire-Hound
  content: '@Uilo I have the same issue, did you solve it?'
  created_at: 2023-04-29 13:58:52+00:00
  edited: false
  hidden: false
  id: 644d30ac0dc952d2459f3f4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
      fullname: Tren
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Uilo
      type: user
    createdAt: '2023-05-01T03:01:57.000Z'
    data:
      edited: false
      editors:
      - Uilo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/336c64a5d992c231070d39d8c76303bf.svg
          fullname: Tren
          isHf: false
          isPro: false
          name: Uilo
          type: user
        html: '<p>Not really... kind of with the smaller models.</p>

          <p>Have a look at this for some more info though:<br><a rel="nofollow" href="https://reddit.com/r/PygmalionAI/comments/11dmqly/running_pyg_on_aws_sagemaker/">https://reddit.com/r/PygmalionAI/comments/11dmqly/running_pyg_on_aws_sagemaker/</a></p>

          '
        raw: 'Not really... kind of with the smaller models.


          Have a look at this for some more info though:

          https://reddit.com/r/PygmalionAI/comments/11dmqly/running_pyg_on_aws_sagemaker/'
        updatedAt: '2023-05-01T03:01:57.824Z'
      numEdits: 0
      reactions: []
    id: 644f2ba5cf72e60a5b892b00
    type: comment
  author: Uilo
  content: 'Not really... kind of with the smaller models.


    Have a look at this for some more info though:

    https://reddit.com/r/PygmalionAI/comments/11dmqly/running_pyg_on_aws_sagemaker/'
  created_at: 2023-05-01 02:01:57+00:00
  edited: false
  hidden: false
  id: 644f2ba5cf72e60a5b892b00
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: PygmalionAI/pygmalion-6b
repo_type: model
status: closed
target_branch: null
title: Error running on SageMaker
