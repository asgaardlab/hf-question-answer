!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Conanak99
conflicting_files: null
created_at: 2023-02-22 05:22:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de0cdb393b4fcc100d471591994a5e0b.svg
      fullname: Harry Pham
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Conanak99
      type: user
    createdAt: '2023-02-22T05:22:24.000Z'
    data:
      edited: true
      editors:
      - Conanak99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de0cdb393b4fcc100d471591994a5e0b.svg
          fullname: Harry Pham
          isHf: false
          isPro: false
          name: Conanak99
          type: user
        html: '<p>Recently, with the help of Flexgen, we can offload OPT model with
          limited GPU memory <a rel="nofollow" href="https://github.com/FMInference/FlexGen">https://github.com/FMInference/FlexGen</a></p>

          <p>Some experiments show that we can offload OPT-6.7B and OPT-13B with just
          ~2GB VRAM.</p>

          <p>Unfortunately, Pygmalion is based on GPT model. Is there any plan for
          training another Pygmalion model based on OPT? This will help people with
          low GPU run the model locally, and we can also run bigger Pygmalion model
          on Colab with 16GB limit.</p>

          '
        raw: 'Recently, with the help of Flexgen, we can offload OPT model with limited
          GPU memory https://github.com/FMInference/FlexGen


          Some experiments show that we can offload OPT-6.7B and OPT-13B with just
          ~2GB VRAM.


          Unfortunately, Pygmalion is based on GPT model. Is there any plan for training
          another Pygmalion model based on OPT? This will help people with low GPU
          run the model locally, and we can also run bigger Pygmalion model on Colab
          with 16GB limit.'
        updatedAt: '2023-02-22T05:23:49.377Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - yolajo6991
        - JohnnySky
        - Crataco
    id: 63f5a690fcf95ecac2b499b3
    type: comment
  author: Conanak99
  content: 'Recently, with the help of Flexgen, we can offload OPT model with limited
    GPU memory https://github.com/FMInference/FlexGen


    Some experiments show that we can offload OPT-6.7B and OPT-13B with just ~2GB
    VRAM.


    Unfortunately, Pygmalion is based on GPT model. Is there any plan for training
    another Pygmalion model based on OPT? This will help people with low GPU run the
    model locally, and we can also run bigger Pygmalion model on Colab with 16GB limit.'
  created_at: 2023-02-22 05:22:24+00:00
  edited: true
  hidden: false
  id: 63f5a690fcf95ecac2b499b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-02-23T18:51:39.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<p>Not at the moment, since I don''t like how restrictive OPT''s license
          is.</p>

          <p>However, I <em>am</em> keeping an eye on the project since the developers
          plan to support other model architectures (at least according to their roadmap).
          If I get the compute resources to train bigger models, they will likely
          be based on NeoX, so feel free leave a thumbs up for NeoX support on the
          FlexGen repo: <a rel="nofollow" href="https://github.com/FMInference/FlexGen/issues/9">https://github.com/FMInference/FlexGen/issues/9</a></p>

          '
        raw: 'Not at the moment, since I don''t like how restrictive OPT''s license
          is.


          However, I _am_ keeping an eye on the project since the developers plan
          to support other model architectures (at least according to their roadmap).
          If I get the compute resources to train bigger models, they will likely
          be based on NeoX, so feel free leave a thumbs up for NeoX support on the
          FlexGen repo: https://github.com/FMInference/FlexGen/issues/9'
        updatedAt: '2023-02-23T18:51:39.598Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\u2764\uFE0F"
        users:
        - Conanak99
        - Crataco
        - werelax
        - birdup
        - RealBrandon
        - akashgoel-id
    id: 63f7b5bbae70dee480301e46
    type: comment
  author: 11b
  content: 'Not at the moment, since I don''t like how restrictive OPT''s license
    is.


    However, I _am_ keeping an eye on the project since the developers plan to support
    other model architectures (at least according to their roadmap). If I get the
    compute resources to train bigger models, they will likely be based on NeoX, so
    feel free leave a thumbs up for NeoX support on the FlexGen repo: https://github.com/FMInference/FlexGen/issues/9'
  created_at: 2023-02-23 18:51:39+00:00
  edited: false
  hidden: false
  id: 63f7b5bbae70dee480301e46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6fba0723e67a3b8f59a8899e23243330.svg
      fullname: a a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirrorcult
      type: user
    createdAt: '2023-03-09T09:48:52.000Z'
    data:
      edited: false
      editors:
      - mirrorcult
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6fba0723e67a3b8f59a8899e23243330.svg
          fullname: a a
          isHf: false
          isPro: false
          name: mirrorcult
          type: user
        html: '<p>possibility of a model based on LLaMA 7B/13B? though I imagine the
          same restrictiveness applies there</p>

          '
        raw: possibility of a model based on LLaMA 7B/13B? though I imagine the same
          restrictiveness applies there
        updatedAt: '2023-03-09T09:48:52.422Z'
      numEdits: 0
      reactions: []
    id: 6409ab84907a62fc935ed270
    type: comment
  author: mirrorcult
  content: possibility of a model based on LLaMA 7B/13B? though I imagine the same
    restrictiveness applies there
  created_at: 2023-03-09 09:48:52+00:00
  edited: false
  hidden: false
  id: 6409ab84907a62fc935ed270
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6fba0723e67a3b8f59a8899e23243330.svg
      fullname: a a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirrorcult
      type: user
    createdAt: '2023-03-09T09:50:23.000Z'
    data:
      edited: false
      editors:
      - mirrorcult
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6fba0723e67a3b8f59a8899e23243330.svg
          fullname: a a
          isHf: false
          isPro: false
          name: mirrorcult
          type: user
        html: '<p>but would ofc be much more cost efficient to fine tune than neoX-20B
          (and much more creative)</p>

          '
        raw: but would ofc be much more cost efficient to fine tune than neoX-20B
          (and much more creative)
        updatedAt: '2023-03-09T09:50:23.756Z'
      numEdits: 0
      reactions: []
    id: 6409abdf4e96c83e02733c78
    type: comment
  author: mirrorcult
  content: but would ofc be much more cost efficient to fine tune than neoX-20B (and
    much more creative)
  created_at: 2023-03-09 09:50:23+00:00
  edited: false
  hidden: false
  id: 6409abdf4e96c83e02733c78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
      fullname: '0x000011b'
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: 11b
      type: user
    createdAt: '2023-03-10T16:51:50.000Z'
    data:
      edited: false
      editors:
      - 11b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9214712eea954e552c204296941d9da7.svg
          fullname: '0x000011b'
          isHf: false
          isPro: false
          name: 11b
          type: user
        html: '<blockquote>

          <p>possibility of a model based on LLaMA 7B/13B? though I imagine the same
          restrictiveness applies there</p>

          </blockquote>

          <p>Yep, the CEO of HuggingFace himself has asked people to not upload any
          LLaMA models until further notice. The claims about the model''s performance
          are very exciting though, so if Meta allows distribution of fine-tunes I
          do plan on trying it.</p>

          '
        raw: '> possibility of a model based on LLaMA 7B/13B? though I imagine the
          same restrictiveness applies there


          Yep, the CEO of HuggingFace himself has asked people to not upload any LLaMA
          models until further notice. The claims about the model''s performance are
          very exciting though, so if Meta allows distribution of fine-tunes I do
          plan on trying it.'
        updatedAt: '2023-03-10T16:51:50.348Z'
      numEdits: 0
      reactions: []
    id: 640b60260da599711a9ef480
    type: comment
  author: 11b
  content: '> possibility of a model based on LLaMA 7B/13B? though I imagine the same
    restrictiveness applies there


    Yep, the CEO of HuggingFace himself has asked people to not upload any LLaMA models
    until further notice. The claims about the model''s performance are very exciting
    though, so if Meta allows distribution of fine-tunes I do plan on trying it.'
  created_at: 2023-03-10 16:51:50+00:00
  edited: false
  hidden: false
  id: 640b60260da599711a9ef480
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: PygmalionAI/pygmalion-6b
repo_type: model
status: open
target_branch: null
title: Is there any plan for a Pygmalion model based on OPT
