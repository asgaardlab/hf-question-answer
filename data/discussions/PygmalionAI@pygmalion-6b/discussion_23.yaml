!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Joseph717171
conflicting_files: null
created_at: 2023-04-03 10:23:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea4398745974d781ae9dc0e95b12cabe.svg
      fullname: Joseph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joseph717171
      type: user
    createdAt: '2023-04-03T11:23:37.000Z'
    data:
      edited: true
      editors:
      - Joseph717171
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea4398745974d781ae9dc0e95b12cabe.svg
          fullname: Joseph
          isHf: false
          isPro: false
          name: Joseph717171
          type: user
        html: '<p>RWKV is an RNN with transformer-level LLM performance. It can be
          directly trained like a GPT (parallelizable). So it''s combining the best
          of RNN and transformers - great performance, fast inference, saves VRAM,
          fast training, "infinite" ctx_len, and free sentence embedding. And, it''s
          100% attention-free (You only need the hidden state at position t to compute
          the state at position t+1 - you can use the "GPT" mode to quickly compute
          the hidden state for the "RNN" mode.).<br> It looks promising.<br>Check
          it out:<br><a rel="nofollow" href="https://github.com/BlinkDL/RWKV-LM">https://github.com/BlinkDL/RWKV-LM</a></p>

          <p><a href="https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B">https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B</a></p>

          <p><a href="https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio">https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio</a></p>

          <p>Discord: <a rel="nofollow" href="https://discord.gg/bDSBUMeFpc">https://discord.gg/bDSBUMeFpc</a></p>

          <p>Sepp Hochreiter, a pioneer in Deep Learning who is known for vanishing
          gradient and LSTM,  had this to say about Raven (RKWV): </p>

          <blockquote>

          <p>Github github.com/BlinkDL/RWKV-LM: RNN with transformer-level performance,
          without using attention. Similar to Apple''s Attention Free Transformer.
          All trained models open-source. Inference is very fast (even on CPUs) and
          might work on cell phones.<br><a rel="nofollow" href="https://twitter.com/hochreitersepp/status/1524270961314484227?s=46&amp;t=KC7cX_tVezEZLb2ntKap9g">https://twitter.com/hochreitersepp/status/1524270961314484227?s=46&amp;t=KC7cX_tVezEZLb2ntKap9g</a></p>

          </blockquote>

          <p>User feedback from Raven (RKWV) GitHub page:</p>

          <blockquote>

          <p>I''ve so far toyed around the character-based model on our relatively
          small pre-training dataset (around 10GB of text), and the results are extremely
          good - similar ppl to models taking much, much longer to train.</p>

          </blockquote>

          <blockquote>

          <p>dear god rwkv is fast. i switched to another tab after starting training
          it from scratch &amp; when i returned it was emitting plausible english
          &amp; maori words, i left to go microwave some coffee &amp; when i came
          back it was producing fully grammatically correct sentences.</p>

          </blockquote>

          <p><a rel="nofollow" href="https://github.com/BlinkDL/RWKV-LM">https://github.com/BlinkDL/RWKV-LM</a></p>

          '
        raw: "RWKV is an RNN with transformer-level LLM performance. It can be directly\
          \ trained like a GPT (parallelizable). So it's combining the best of RNN\
          \ and transformers - great performance, fast inference, saves VRAM, fast\
          \ training, \"infinite\" ctx_len, and free sentence embedding. And, it's\
          \ 100% attention-free (You only need the hidden state at position t to compute\
          \ the state at position t+1 - you can use the \"GPT\" mode to quickly compute\
          \ the hidden state for the \"RNN\" mode.).\n It looks promising. \nCheck\
          \ it out:\nhttps://github.com/BlinkDL/RWKV-LM\n\nhttps://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B\n\
          \nhttps://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\n\nDiscord: https://discord.gg/bDSBUMeFpc\n\
          \nSepp Hochreiter, a pioneer in Deep Learning who is known for vanishing\
          \ gradient and LSTM,  had this to say about Raven (RKWV): \n> Github github.com/BlinkDL/RWKV-LM:\
          \ RNN with transformer-level performance, without using attention. Similar\
          \ to Apple's Attention Free Transformer. All trained models open-source.\
          \ Inference is very fast (even on CPUs) and might work on cell phones. \n\
          https://twitter.com/hochreitersepp/status/1524270961314484227?s=46&t=KC7cX_tVezEZLb2ntKap9g\n\
          \nUser feedback from Raven (RKWV) GitHub page:\n> I've so far toyed around\
          \ the character-based model on our relatively small pre-training dataset\
          \ (around 10GB of text), and the results are extremely good - similar ppl\
          \ to models taking much, much longer to train.\n\n> dear god rwkv is fast.\
          \ i switched to another tab after starting training it from scratch & when\
          \ i returned it was emitting plausible english & maori words, i left to\
          \ go microwave some coffee & when i came back it was producing fully grammatically\
          \ correct sentences.\n\nhttps://github.com/BlinkDL/RWKV-LM"
        updatedAt: '2023-04-03T12:33:48.232Z'
      numEdits: 6
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - coremic
        - KOTOB
      - count: 1
        reaction: "\U0001F91D"
        users:
        - coremic
    id: 642ab73992094a1b74dce62f
    type: comment
  author: Joseph717171
  content: "RWKV is an RNN with transformer-level LLM performance. It can be directly\
    \ trained like a GPT (parallelizable). So it's combining the best of RNN and transformers\
    \ - great performance, fast inference, saves VRAM, fast training, \"infinite\"\
    \ ctx_len, and free sentence embedding. And, it's 100% attention-free (You only\
    \ need the hidden state at position t to compute the state at position t+1 - you\
    \ can use the \"GPT\" mode to quickly compute the hidden state for the \"RNN\"\
    \ mode.).\n It looks promising. \nCheck it out:\nhttps://github.com/BlinkDL/RWKV-LM\n\
    \nhttps://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B\n\nhttps://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\n\
    \nDiscord: https://discord.gg/bDSBUMeFpc\n\nSepp Hochreiter, a pioneer in Deep\
    \ Learning who is known for vanishing gradient and LSTM,  had this to say about\
    \ Raven (RKWV): \n> Github github.com/BlinkDL/RWKV-LM: RNN with transformer-level\
    \ performance, without using attention. Similar to Apple's Attention Free Transformer.\
    \ All trained models open-source. Inference is very fast (even on CPUs) and might\
    \ work on cell phones. \nhttps://twitter.com/hochreitersepp/status/1524270961314484227?s=46&t=KC7cX_tVezEZLb2ntKap9g\n\
    \nUser feedback from Raven (RKWV) GitHub page:\n> I've so far toyed around the\
    \ character-based model on our relatively small pre-training dataset (around 10GB\
    \ of text), and the results are extremely good - similar ppl to models taking\
    \ much, much longer to train.\n\n> dear god rwkv is fast. i switched to another\
    \ tab after starting training it from scratch & when i returned it was emitting\
    \ plausible english & maori words, i left to go microwave some coffee & when i\
    \ came back it was producing fully grammatically correct sentences.\n\nhttps://github.com/BlinkDL/RWKV-LM"
  created_at: 2023-04-03 10:23:37+00:00
  edited: true
  hidden: false
  id: 642ab73992094a1b74dce62f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ea4398745974d781ae9dc0e95b12cabe.svg
      fullname: Joseph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joseph717171
      type: user
    createdAt: '2023-04-03T11:27:09.000Z'
    data:
      status: closed
    id: 642ab80d422abb9016fed0de
    type: status-change
  author: Joseph717171
  created_at: 2023-04-03 10:27:09+00:00
  id: 642ab80d422abb9016fed0de
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ea4398745974d781ae9dc0e95b12cabe.svg
      fullname: Joseph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joseph717171
      type: user
    createdAt: '2023-04-03T11:27:18.000Z'
    data:
      status: open
    id: 642ab81657a01eec542148a5
    type: status-change
  author: Joseph717171
  created_at: 2023-04-03 10:27:18+00:00
  id: 642ab81657a01eec542148a5
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bc18d01a047da654176c19ef8851de0a.svg
      fullname: Ryan Matovu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: coremic
      type: user
    createdAt: '2023-04-30T15:31:59.000Z'
    data:
      edited: false
      editors:
      - coremic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bc18d01a047da654176c19ef8851de0a.svg
          fullname: Ryan Matovu
          isHf: false
          isPro: false
          name: coremic
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Joseph717171&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Joseph717171\"\
          >@<span class=\"underline\">Joseph717171</span></a></span>\n\n\t</span></span>\
          \ could you contact us at aicomp#7175 to discuss this approach further as\
          \ we are pursuing it currently.</p>\n"
        raw: Hey @Joseph717171 could you contact us at aicomp#7175 to discuss this
          approach further as we are pursuing it currently.
        updatedAt: '2023-04-30T15:31:59.392Z'
      numEdits: 0
      reactions: []
    id: 644e89efddf20748b0595eb2
    type: comment
  author: coremic
  content: Hey @Joseph717171 could you contact us at aicomp#7175 to discuss this approach
    further as we are pursuing it currently.
  created_at: 2023-04-30 14:31:59+00:00
  edited: false
  hidden: false
  id: 644e89efddf20748b0595eb2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: PygmalionAI/pygmalion-6b
repo_type: model
status: open
target_branch: null
title: Raven (RKWV) as a potential LLM for Pygmalion to use.
