!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-04-28 04:25:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-04-28T05:25:00.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Thanks for making 3bit 128G quantized model for the community! </p>

          <p>The 30B 3bit 128G model seems to meet a sweet spot that outperform 13B
          fp16 model:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/iF9f5bjIL4_9Dhhb-MRoi.png"><img
          alt="msedge_wKQ6yw1t6Z.png" src="https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/iF9f5bjIL4_9Dhhb-MRoi.png"></a><br>Which
          motivated me to just about convert the model by myself.</p>

          <p>I was very pleasent with locally deploying 13B fp16 models on a dual
          3090 server. Now I am going to try two instances of 30B 3bit model</p>

          '
        raw: "Thanks for making 3bit 128G quantized model for the community! \r\n\r\
          \nThe 30B 3bit 128G model seems to meet a sweet spot that outperform 13B\
          \ fp16 model: \r\n![msedge_wKQ6yw1t6Z.png](https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/iF9f5bjIL4_9Dhhb-MRoi.png)\r\
          \nWhich motivated me to just about convert the model by myself.\r\n\r\n\
          I was very pleasent with locally deploying 13B fp16 models on a dual 3090\
          \ server. Now I am going to try two instances of 30B 3bit model"
        updatedAt: '2023-04-28T05:25:00.380Z'
      numEdits: 0
      reactions: []
    id: 644b58ac958b7796980c1718
    type: comment
  author: Yhyu13
  content: "Thanks for making 3bit 128G quantized model for the community! \r\n\r\n\
    The 30B 3bit 128G model seems to meet a sweet spot that outperform 13B fp16 model:\
    \ \r\n![msedge_wKQ6yw1t6Z.png](https://cdn-uploads.huggingface.co/production/uploads/64438bcb1bc692d87b237c04/iF9f5bjIL4_9Dhhb-MRoi.png)\r\
    \nWhich motivated me to just about convert the model by myself.\r\n\r\nI was very\
    \ pleasent with locally deploying 13B fp16 models on a dual 3090 server. Now I\
    \ am going to try two instances of 30B 3bit model"
  created_at: 2023-04-28 04:25:00+00:00
  edited: false
  hidden: false
  id: 644b58ac958b7796980c1718
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
      fullname: Bow Wow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: tsumeone
      type: user
    createdAt: '2023-04-28T05:27:37.000Z'
    data:
      edited: true
      editors:
      - tsumeone
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b37d0ca991cec7656ef9519c2032bdf5.svg
          fullname: Bow Wow
          isHf: false
          isPro: false
          name: tsumeone
          type: user
        html: '<p>Glad you are finding use for it!  This one scores Wikitext2 at 5.22.  I
          tried publishing model card with my eval results, but HF is having problems.</p>

          <p>Here are the results I got (better to just compare to the results on
          my other two supercot quants, since then it''s apples to apples)</p>

          <pre><code>WikiText2: 5.22 (12% worse than 4bit non-groupsize)

          PTB: 19.63 (11% worse than 4bit non-groupsize)

          C4: 6.93 (7% worse than 4bit non-groupsize)

          </code></pre>

          '
        raw: "Glad you are finding use for it!  This one scores Wikitext2 at 5.22.\
          \  I tried publishing model card with my eval results, but HF is having\
          \ problems.\n\nHere are the results I got (better to just compare to the\
          \ results on my other two supercot quants, since then it's apples to apples)\n\
          \n    WikiText2: 5.22 (12% worse than 4bit non-groupsize)\n    PTB: 19.63\
          \ (11% worse than 4bit non-groupsize)\n    C4: 6.93 (7% worse than 4bit\
          \ non-groupsize)"
        updatedAt: '2023-04-28T05:38:45.610Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 644b5949af97dfd24c1bb73b
    type: comment
  author: tsumeone
  content: "Glad you are finding use for it!  This one scores Wikitext2 at 5.22. \
    \ I tried publishing model card with my eval results, but HF is having problems.\n\
    \nHere are the results I got (better to just compare to the results on my other\
    \ two supercot quants, since then it's apples to apples)\n\n    WikiText2: 5.22\
    \ (12% worse than 4bit non-groupsize)\n    PTB: 19.63 (11% worse than 4bit non-groupsize)\n\
    \    C4: 6.93 (7% worse than 4bit non-groupsize)"
  created_at: 2023-04-28 04:27:37+00:00
  edited: true
  hidden: false
  id: 644b5949af97dfd24c1bb73b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
      fullname: tp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Onix22
      type: user
    createdAt: '2023-05-03T20:01:04.000Z'
    data:
      edited: false
      editors:
      - Onix22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/04297acee24206d3972d73a2bc960ee8.svg
          fullname: tp
          isHf: false
          isPro: false
          name: Onix22
          type: user
        html: '<p>It is hard to tell the difference but looks like those results are
          reasonably accurate .  3bit version writes a bit shorter responses  but
          at least now it  fits more text into memory before crashing.</p>

          <p>Still hard to decide if it is better to use 4bit model which is a little
          bit better or smaller model which has more context space.</p>

          '
        raw: 'It is hard to tell the difference but looks like those results are reasonably
          accurate .  3bit version writes a bit shorter responses  but at least now
          it  fits more text into memory before crashing.


          Still hard to decide if it is better to use 4bit model which is a little
          bit better or smaller model which has more context space.'
        updatedAt: '2023-05-03T20:01:04.133Z'
      numEdits: 0
      reactions: []
    id: 6452bd808fe6558e3282f968
    type: comment
  author: Onix22
  content: 'It is hard to tell the difference but looks like those results are reasonably
    accurate .  3bit version writes a bit shorter responses  but at least now it  fits
    more text into memory before crashing.


    Still hard to decide if it is better to use 4bit model which is a little bit better
    or smaller model which has more context space.'
  created_at: 2023-05-03 19:01:04+00:00
  edited: false
  hidden: false
  id: 6452bd808fe6558e3282f968
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: tsumeone/llama-30b-supercot-3bit-128g-cuda
repo_type: model
status: open
target_branch: null
title: '30B 3bit seems pretty sweet by the official evaluation '
