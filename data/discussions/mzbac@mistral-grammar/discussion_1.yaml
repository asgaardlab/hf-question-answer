!!python/object:huggingface_hub.community.DiscussionWithDetails
author: antderosa
conflicting_files: null
created_at: 2023-10-23 00:30:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11ee6e9f933c0317ae2ca77e48a6e114.svg
      fullname: Anthony DeRosa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: antderosa
      type: user
    createdAt: '2023-10-23T01:30:17.000Z'
    data:
      edited: false
      editors:
      - antderosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6436468362808228
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11ee6e9f933c0317ae2ca77e48a6e114.svg
          fullname: Anthony DeRosa
          isHf: false
          isPro: false
          name: antderosa
          type: user
        html: '<p>After running the command: </p>

          <p>./server -m models/"$MODEL_FILE" -c 8192</p>

          <p>I get the following error:</p>

          <p>{"timestamp":1698024323,"level":"INFO","function":"main","line":1324,"message":"build
          info","build":1408,"commit":"22c69a2"}<br>{"timestamp":1698024323,"level":"INFO","function":"main","line":1330,"message":"system
          info","n_threads":8,"n_threads_batch":-1,"total_threads":10,"system_info":"AVX
          = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =
          0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS
          = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | "}<br>gguf_init_from_file: invalid
          magic characters .<br>error loading model: llama_model_loader: failed to
          load model from models/</p>

          <p>llama_load_model_from_file: failed to load model<br>llama_init_from_gpt_params:
          error: failed to load model ''models/''<br>{"timestamp":1698024323,"level":"ERROR","function":"loadModel","line":267,"message":"unable
          to load model","model":"models/"}</p>

          '
        raw: "After running the command: \r\n\r\n./server -m models/\"$MODEL_FILE\"\
          \ -c 8192\r\n\r\nI get the following error:\r\n\r\n{\"timestamp\":1698024323,\"\
          level\":\"INFO\",\"function\":\"main\",\"line\":1324,\"message\":\"build\
          \ info\",\"build\":1408,\"commit\":\"22c69a2\"}\r\n{\"timestamp\":1698024323,\"\
          level\":\"INFO\",\"function\":\"main\",\"line\":1330,\"message\":\"system\
          \ info\",\"n_threads\":8,\"n_threads_batch\":-1,\"total_threads\":10,\"\
          system_info\":\"AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \"}\r\ngguf_init_from_file:\
          \ invalid magic characters .\r\nerror loading model: llama_model_loader:\
          \ failed to load model from models/\r\n\r\nllama_load_model_from_file: failed\
          \ to load model\r\nllama_init_from_gpt_params: error: failed to load model\
          \ 'models/'\r\n{\"timestamp\":1698024323,\"level\":\"ERROR\",\"function\"\
          :\"loadModel\",\"line\":267,\"message\":\"unable to load model\",\"model\"\
          :\"models/\"}\r\n"
        updatedAt: '2023-10-23T01:30:17.249Z'
      numEdits: 0
      reactions: []
    id: 6535cca9fca2c10e4318b0ae
    type: comment
  author: antderosa
  content: "After running the command: \r\n\r\n./server -m models/\"$MODEL_FILE\"\
    \ -c 8192\r\n\r\nI get the following error:\r\n\r\n{\"timestamp\":1698024323,\"\
    level\":\"INFO\",\"function\":\"main\",\"line\":1324,\"message\":\"build info\"\
    ,\"build\":1408,\"commit\":\"22c69a2\"}\r\n{\"timestamp\":1698024323,\"level\"\
    :\"INFO\",\"function\":\"main\",\"line\":1330,\"message\":\"system info\",\"n_threads\"\
    :8,\"n_threads_batch\":-1,\"total_threads\":10,\"system_info\":\"AVX = 0 | AVX2\
    \ = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 |\
    \ ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 |\
    \ SSSE3 = 0 | VSX = 0 | \"}\r\ngguf_init_from_file: invalid magic characters .\r\
    \nerror loading model: llama_model_loader: failed to load model from models/\r\
    \n\r\nllama_load_model_from_file: failed to load model\r\nllama_init_from_gpt_params:\
    \ error: failed to load model 'models/'\r\n{\"timestamp\":1698024323,\"level\"\
    :\"ERROR\",\"function\":\"loadModel\",\"line\":267,\"message\":\"unable to load\
    \ model\",\"model\":\"models/\"}\r\n"
  created_at: 2023-10-23 00:30:17+00:00
  edited: false
  hidden: false
  id: 6535cca9fca2c10e4318b0ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/513e0d2c15cfbb1542cb268eb2c8d68b.svg
      fullname: 'null'
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mzbac
      type: user
    createdAt: '2023-10-23T01:37:30.000Z'
    data:
      edited: false
      editors:
      - mzbac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5796577334403992
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/513e0d2c15cfbb1542cb268eb2c8d68b.svg
          fullname: 'null'
          isHf: false
          isPro: false
          name: mzbac
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;antderosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/antderosa\">@<span class=\"\
          underline\">antderosa</span></a></span>\n\n\t</span></span><br>If you want\
          \ to use the fine-tuned model, you have to get a downloadable link for the\
          \ model in HF and update it accordingly in the shell script. For example:</p>\n\
          <pre><code>MODEL_URL=\"https://huggingface.co/mzbac/mistral-grammar/resolve/main/Mistral-7B-grammar-checker-v1.1.Q5_K_M.gguf\"\
          \nMODEL_FILE=\"Mistral-7B-grammar-checker-v1.1.Q5_K_M.gguf\"\n</code></pre>\n"
        raw: "@antderosa \nIf you want to use the fine-tuned model, you have to get\
          \ a downloadable link for the model in HF and update it accordingly in the\
          \ shell script. For example:\n```\nMODEL_URL=\"https://huggingface.co/mzbac/mistral-grammar/resolve/main/Mistral-7B-grammar-checker-v1.1.Q5_K_M.gguf\"\
          \nMODEL_FILE=\"Mistral-7B-grammar-checker-v1.1.Q5_K_M.gguf\"\n```\n"
        updatedAt: '2023-10-23T01:37:30.825Z'
      numEdits: 0
      reactions: []
    id: 6535ce5a0917932040c0ba80
    type: comment
  author: mzbac
  content: "@antderosa \nIf you want to use the fine-tuned model, you have to get\
    \ a downloadable link for the model in HF and update it accordingly in the shell\
    \ script. For example:\n```\nMODEL_URL=\"https://huggingface.co/mzbac/mistral-grammar/resolve/main/Mistral-7B-grammar-checker-v1.1.Q5_K_M.gguf\"\
    \nMODEL_FILE=\"Mistral-7B-grammar-checker-v1.1.Q5_K_M.gguf\"\n```\n"
  created_at: 2023-10-23 00:37:30+00:00
  edited: false
  hidden: false
  id: 6535ce5a0917932040c0ba80
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mzbac/mistral-grammar
repo_type: model
status: open
target_branch: null
title: Error when trying to start server
