!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Dailyfocus
conflicting_files: null
created_at: 2023-07-29 01:39:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a25def7fa12afdd140f409a5c072514e.svg
      fullname: addsion dayley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dailyfocus
      type: user
    createdAt: '2023-07-29T02:39:54.000Z'
    data:
      edited: false
      editors:
      - Dailyfocus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8527066707611084
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a25def7fa12afdd140f409a5c072514e.svg
          fullname: addsion dayley
          isHf: false
          isPro: false
          name: Dailyfocus
          type: user
        html: "<p>As a preface, I'm relatively new to this and have had some success\
          \ with running the 7b and 13b GGML models and even the 33b vicuna GGML using\
          \ the text-generation-webui. However, I've encountered a challenge when\
          \ attempting to run the 70b models.</p>\n<p>I've downloaded the StableBeluga2-GGML\
          \ model files for use with text-generation-webui on my Intel Mac, but whenever\
          \ I attempt to run the server, I encounter an error. The model fails to\
          \ load due to a shape mismatch for the 'layers.0.attention.wk.weight' tensor.</p>\n\
          <p>I downloaded and unzipped the following files in the model folder for\
          \ the StableBeluga2-GGML model:</p>\n<ul>\n<li>stablebeluga2.ggmlv3.q5_1.zip\
          \ and stablebeluga2.ggmlv3.q5_1.z01 (q5_1)</li>\n<li>stablebeluga2.ggmlv3.q6_K.zip\
          \ and stablebeluga2.ggmlv3.q6_K.z01 (q6_K)</li>\n<li>stablebeluga2.ggmlv3.q8_0.zip\
          \ and stablebeluga2.ggmlv3.q8_0.z01 (q8_0)</li>\n</ul>\n<p>During the process,\
          \ I noticed a reference to adding a flag for CPU-only support, but I'm uncertain\
          \ whether this is applicable to my case or how to implement it while using\
          \ the text-generation-webui.</p>\n<p>The error message that I received after\
          \ running the server.py script was as follows:</p>\n<ul>\n<li>\"Error loading\
          \ model: llama.cpp: tensor 'layers.0.attention.wk.weight' has wrong shape;\
          \ expected 8192 x 8192, got 8192 x 1024. Llama_load_model_from_file: failed\
          \ to load model.\"<br>The traceback further indicated an AssertionError\
          \ at \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/llama_cpp/llama.py\"\
          , line 313, in init.</li>\n</ul>\n<p>Any suggestions on how to resolve this\
          \ shape mismatch error so that I can properly load and run the StableBeluga2-GGML\
          \ model with text-generation-webui would be greatly appreciated, or if there\
          \ are any good resources for learning since I have been unsuccessful with\
          \ any of the 70b models and don\u2019t know where it is going wrong. Thank\
          \ you in advance for your help! :)</p>\n"
        raw: "As a preface, I'm relatively new to this and have had some success with\
          \ running the 7b and 13b GGML models and even the 33b vicuna GGML using\
          \ the text-generation-webui. However, I've encountered a challenge when\
          \ attempting to run the 70b models.\r\n\r\nI've downloaded the StableBeluga2-GGML\
          \ model files for use with text-generation-webui on my Intel Mac, but whenever\
          \ I attempt to run the server, I encounter an error. The model fails to\
          \ load due to a shape mismatch for the 'layers.0.attention.wk.weight' tensor.\r\
          \n\r\nI downloaded and unzipped the following files in the model folder\
          \ for the StableBeluga2-GGML model:\r\n* stablebeluga2.ggmlv3.q5_1.zip and\
          \ stablebeluga2.ggmlv3.q5_1.z01 (q5_1)\r\n* stablebeluga2.ggmlv3.q6_K.zip\
          \ and stablebeluga2.ggmlv3.q6_K.z01 (q6_K)\r\n* stablebeluga2.ggmlv3.q8_0.zip\
          \ and stablebeluga2.ggmlv3.q8_0.z01 (q8_0)\r\n\r\nDuring the process, I\
          \ noticed a reference to adding a flag for CPU-only support, but I'm uncertain\
          \ whether this is applicable to my case or how to implement it while using\
          \ the text-generation-webui.\r\n\r\nThe error message that I received after\
          \ running the server.py script was as follows:\r\n\r\n* \"Error loading\
          \ model: llama.cpp: tensor 'layers.0.attention.wk.weight' has wrong shape;\
          \ expected 8192 x 8192, got 8192 x 1024. Llama_load_model_from_file: failed\
          \ to load model.\"\r\nThe traceback further indicated an AssertionError\
          \ at \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/llama_cpp/llama.py\"\
          , line 313, in init.\r\n\r\nAny suggestions on how to resolve this shape\
          \ mismatch error so that I can properly load and run the StableBeluga2-GGML\
          \ model with text-generation-webui would be greatly appreciated, or if there\
          \ are any good resources for learning since I have been unsuccessful with\
          \ any of the 70b models and don\u2019t know where it is going wrong. Thank\
          \ you in advance for your help! :)"
        updatedAt: '2023-07-29T02:39:54.781Z'
      numEdits: 0
      reactions: []
    id: 64c47bfaaedc433778f5ccc1
    type: comment
  author: Dailyfocus
  content: "As a preface, I'm relatively new to this and have had some success with\
    \ running the 7b and 13b GGML models and even the 33b vicuna GGML using the text-generation-webui.\
    \ However, I've encountered a challenge when attempting to run the 70b models.\r\
    \n\r\nI've downloaded the StableBeluga2-GGML model files for use with text-generation-webui\
    \ on my Intel Mac, but whenever I attempt to run the server, I encounter an error.\
    \ The model fails to load due to a shape mismatch for the 'layers.0.attention.wk.weight'\
    \ tensor.\r\n\r\nI downloaded and unzipped the following files in the model folder\
    \ for the StableBeluga2-GGML model:\r\n* stablebeluga2.ggmlv3.q5_1.zip and stablebeluga2.ggmlv3.q5_1.z01\
    \ (q5_1)\r\n* stablebeluga2.ggmlv3.q6_K.zip and stablebeluga2.ggmlv3.q6_K.z01\
    \ (q6_K)\r\n* stablebeluga2.ggmlv3.q8_0.zip and stablebeluga2.ggmlv3.q8_0.z01\
    \ (q8_0)\r\n\r\nDuring the process, I noticed a reference to adding a flag for\
    \ CPU-only support, but I'm uncertain whether this is applicable to my case or\
    \ how to implement it while using the text-generation-webui.\r\n\r\nThe error\
    \ message that I received after running the server.py script was as follows:\r\
    \n\r\n* \"Error loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
    \ has wrong shape; expected 8192 x 8192, got 8192 x 1024. Llama_load_model_from_file:\
    \ failed to load model.\"\r\nThe traceback further indicated an AssertionError\
    \ at \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/llama_cpp/llama.py\"\
    , line 313, in init.\r\n\r\nAny suggestions on how to resolve this shape mismatch\
    \ error so that I can properly load and run the StableBeluga2-GGML model with\
    \ text-generation-webui would be greatly appreciated, or if there are any good\
    \ resources for learning since I have been unsuccessful with any of the 70b models\
    \ and don\u2019t know where it is going wrong. Thank you in advance for your help!\
    \ :)"
  created_at: 2023-07-29 01:39:54+00:00
  edited: false
  hidden: false
  id: 64c47bfaaedc433778f5ccc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4434e5847426583821ee7f0e1ce506b9.svg
      fullname: Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: John-Smith-007
      type: user
    createdAt: '2023-07-29T03:50:18.000Z'
    data:
      edited: true
      editors:
      - John-Smith-007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8247082233428955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4434e5847426583821ee7f0e1ce506b9.svg
          fullname: Smith
          isHf: false
          isPro: false
          name: John-Smith-007
          type: user
        html: '<p>You need to pass the argument <code>-gqa 8</code> as it''s not detected
          automatically yet.</p>

          <p><code>  -gqa N, --gqa N       grouped-query attention factor (TEMP!!!
          use 8 for LLaMAv2 70B) (default: 1)</code></p>

          <p><a rel="nofollow" href="https://github.com/facebookresearch/llama/issues/407">https://github.com/facebookresearch/llama/issues/407</a></p>

          '
        raw: 'You need to pass the argument `-gqa 8` as it''s not detected automatically
          yet.


          ```  -gqa N, --gqa N       grouped-query attention factor (TEMP!!! use 8
          for LLaMAv2 70B) (default: 1)```


          https://github.com/facebookresearch/llama/issues/407'
        updatedAt: '2023-07-29T03:51:36.008Z'
      numEdits: 1
      reactions: []
    id: 64c48c7a4c9bebfa6aa18bd9
    type: comment
  author: John-Smith-007
  content: 'You need to pass the argument `-gqa 8` as it''s not detected automatically
    yet.


    ```  -gqa N, --gqa N       grouped-query attention factor (TEMP!!! use 8 for LLaMAv2
    70B) (default: 1)```


    https://github.com/facebookresearch/llama/issues/407'
  created_at: 2023-07-29 02:50:18+00:00
  edited: true
  hidden: false
  id: 64c48c7a4c9bebfa6aa18bd9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a25def7fa12afdd140f409a5c072514e.svg
      fullname: addsion dayley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dailyfocus
      type: user
    createdAt: '2023-07-29T05:04:00.000Z'
    data:
      edited: false
      editors:
      - Dailyfocus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9810716509819031
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a25def7fa12afdd140f409a5c072514e.svg
          fullname: addsion dayley
          isHf: false
          isPro: false
          name: Dailyfocus
          type: user
        html: "<p>Thank you, I got it to work when I changed that! also I thought\
          \ you needed all of the files to run, so i had close to 700gb\u2019s downloaded\
          \ to try it when I only needed 29gb\u2019s.</p>\n"
        raw: "Thank you, I got it to work when I changed that! also I thought you\
          \ needed all of the files to run, so i had close to 700gb\u2019s downloaded\
          \ to try it when I only needed 29gb\u2019s."
        updatedAt: '2023-07-29T05:04:00.101Z'
      numEdits: 0
      reactions: []
    id: 64c49dc02581696666cbc960
    type: comment
  author: Dailyfocus
  content: "Thank you, I got it to work when I changed that! also I thought you needed\
    \ all of the files to run, so i had close to 700gb\u2019s downloaded to try it\
    \ when I only needed 29gb\u2019s."
  created_at: 2023-07-29 04:04:00+00:00
  edited: false
  hidden: false
  id: 64c49dc02581696666cbc960
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a25def7fa12afdd140f409a5c072514e.svg
      fullname: addsion dayley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dailyfocus
      type: user
    createdAt: '2023-07-29T05:04:04.000Z'
    data:
      status: closed
    id: 64c49dc4e77ea9f28128f820
    type: status-change
  author: Dailyfocus
  created_at: 2023-07-29 04:04:04+00:00
  id: 64c49dc4e77ea9f28128f820
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/StableBeluga2-70B-GGML
repo_type: model
status: closed
target_branch: null
title: Difficulty Running StableBeluga2-GGML 70b Models with text-generation-webui
  on an Intel Mac
