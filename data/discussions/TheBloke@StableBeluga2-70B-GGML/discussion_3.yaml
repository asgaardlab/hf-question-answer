!!python/object:huggingface_hub.community.DiscussionWithDetails
author: danielus
conflicting_files: null
created_at: 2023-07-29 06:31:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9764fe4daecf98ade3a3e22765722f78.svg
      fullname: Danieluss
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danielus
      type: user
    createdAt: '2023-07-29T07:31:41.000Z'
    data:
      edited: false
      editors:
      - danielus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9267798662185669
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9764fe4daecf98ade3a3e22765722f78.svg
          fullname: Danieluss
          isHf: false
          isPro: false
          name: danielus
          type: user
        html: '<p>Is there any way to work out how much ''performance'' the quantised
          versions lose compared to the original? So that you can get an idea of which
          quantisation level to choose and maximise the ratio of resources used/generation
          performance<br>In the github repository of llama.cpp I only found an old
          post of the different accuracies for the quantisation levels, but I assume
          it has become obsolete given the speed at which this world is advancing!</p>

          '
        raw: "Is there any way to work out how much 'performance' the quantised versions\
          \ lose compared to the original? So that you can get an idea of which quantisation\
          \ level to choose and maximise the ratio of resources used/generation performance\r\
          \nIn the github repository of llama.cpp I only found an old post of the\
          \ different accuracies for the quantisation levels, but I assume it has\
          \ become obsolete given the speed at which this world is advancing!"
        updatedAt: '2023-07-29T07:31:41.346Z'
      numEdits: 0
      reactions: []
    id: 64c4c05dbf19548901973b5e
    type: comment
  author: danielus
  content: "Is there any way to work out how much 'performance' the quantised versions\
    \ lose compared to the original? So that you can get an idea of which quantisation\
    \ level to choose and maximise the ratio of resources used/generation performance\r\
    \nIn the github repository of llama.cpp I only found an old post of the different\
    \ accuracies for the quantisation levels, but I assume it has become obsolete\
    \ given the speed at which this world is advancing!"
  created_at: 2023-07-29 06:31:41+00:00
  edited: false
  hidden: false
  id: 64c4c05dbf19548901973b5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
      fullname: Jeff Wadsworth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffwadsworth
      type: user
    createdAt: '2023-08-23T00:41:38.000Z'
    data:
      edited: false
      editors:
      - jeffwadsworth
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989089846611023
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0fa6d53891ffd28118296b5cf51cb70d.svg
          fullname: Jeff Wadsworth
          isHf: false
          isPro: false
          name: jeffwadsworth
          type: user
        html: '<p>Trust me, the 8 bit is worth it.  At least in regards to reasoning.</p>

          '
        raw: Trust me, the 8 bit is worth it.  At least in regards to reasoning.
        updatedAt: '2023-08-23T00:41:38.473Z'
      numEdits: 0
      reactions: []
    id: 64e555c21ba234c1e5b5e7eb
    type: comment
  author: jeffwadsworth
  content: Trust me, the 8 bit is worth it.  At least in regards to reasoning.
  created_at: 2023-08-22 23:41:38+00:00
  edited: false
  hidden: false
  id: 64e555c21ba234c1e5b5e7eb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/StableBeluga2-70B-GGML
repo_type: model
status: open
target_branch: null
title: Performance of quantified models
