!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rjmehta
conflicting_files: null
created_at: 2023-12-09 19:25:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-12-09T19:25:08.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9229480028152466
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>I see this is picking a lot of attention. Is this a new Mistral
          architecture release or a new model tuned on a different dataset?</p>

          '
        raw: I see this is picking a lot of attention. Is this a new Mistral architecture
          release or a new model tuned on a different dataset?
        updatedAt: '2023-12-09T19:25:08.831Z'
      numEdits: 0
      reactions: []
    id: 6574bf14d40e6ed3261e372f
    type: comment
  author: rjmehta
  content: I see this is picking a lot of attention. Is this a new Mistral architecture
    release or a new model tuned on a different dataset?
  created_at: 2023-12-09 19:25:08+00:00
  edited: false
  hidden: false
  id: 6574bf14d40e6ed3261e372f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-12-09T20:03:30.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9645055532455444
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You mean Mixtral in general, or v2 specifically?</p>

          <p>v2 is a fine tune by DiscoResearch</p>

          <p>Mixtral is a 7B x 8 MOE model stealth released by MistralAI on Friday
          via BitTorrent. It will be officially uploaded by them soon, I assume.  In
          the meantime they provided just the weights with no code, which only works
          with one specific inference framework and not with general HF Transformers.
          But DiscoResearch did the work to get it working with Hugging Face before
          that, and I quantised it in my other Mixtral repo.</p>

          <p>Then Disco have done a fine tuning of it, which is this v2 model here.</p>

          '
        raw: 'You mean Mixtral in general, or v2 specifically?


          v2 is a fine tune by DiscoResearch


          Mixtral is a 7B x 8 MOE model stealth released by MistralAI on Friday via
          BitTorrent. It will be officially uploaded by them soon, I assume.  In the
          meantime they provided just the weights with no code, which only works with
          one specific inference framework and not with general HF Transformers. But
          DiscoResearch did the work to get it working with Hugging Face before that,
          and I quantised it in my other Mixtral repo.


          Then Disco have done a fine tuning of it, which is this v2 model here.'
        updatedAt: '2023-12-09T20:03:30.855Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - eberan
        - RafauUu
        - JoeySalmons
        - mclassHF2023
        - mirek190
        - jlzhou
      - count: 2
        reaction: "\U0001F92F"
        users:
        - hwpoison89
        - Yhyu13
      - count: 1
        reaction: "\U0001F91D"
        users:
        - hwpoison89
    id: 6574c8121345577b70b91521
    type: comment
  author: TheBloke
  content: 'You mean Mixtral in general, or v2 specifically?


    v2 is a fine tune by DiscoResearch


    Mixtral is a 7B x 8 MOE model stealth released by MistralAI on Friday via BitTorrent.
    It will be officially uploaded by them soon, I assume.  In the meantime they provided
    just the weights with no code, which only works with one specific inference framework
    and not with general HF Transformers. But DiscoResearch did the work to get it
    working with Hugging Face before that, and I quantised it in my other Mixtral
    repo.


    Then Disco have done a fine tuning of it, which is this v2 model here.'
  created_at: 2023-12-09 20:03:30+00:00
  edited: false
  hidden: false
  id: 6574c8121345577b70b91521
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa47fb2fc7039660ee89593cd6777f41.svg
      fullname: Mike Kasprzak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PoVRAZOR
      type: user
    createdAt: '2023-12-09T20:08:18.000Z'
    data:
      edited: false
      editors:
      - PoVRAZOR
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8835919499397278
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa47fb2fc7039660ee89593cd6777f41.svg
          fullname: Mike Kasprzak
          isHf: false
          isPro: false
          name: PoVRAZOR
          type: user
        html: "<p>Much appreciated for all you do <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>.\
          \ </p>\n"
        raw: 'Much appreciated for all you do @TheBloke. '
        updatedAt: '2023-12-09T20:08:18.302Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mclassHF2023
    id: 6574c932addd9129ff5edcce
    type: comment
  author: PoVRAZOR
  content: 'Much appreciated for all you do @TheBloke. '
  created_at: 2023-12-09 20:08:18+00:00
  edited: false
  hidden: false
  id: 6574c932addd9129ff5edcce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/DiscoLM-mixtral-8x7b-v2-GPTQ
repo_type: model
status: open
target_branch: null
title: What model is this?
