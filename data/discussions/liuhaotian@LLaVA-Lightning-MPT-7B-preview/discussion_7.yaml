!!python/object:huggingface_hub.community.DiscussionWithDetails
author: crystal-technologies
conflicting_files: null
created_at: 2023-12-15 13:42:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646ead21ee36629c15bb6487/csohtL1aHhnXdEwrk5lJw.png?w=200&h=200&f=face
      fullname: Vatsal Dutt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: crystal-technologies
      type: user
    createdAt: '2023-12-15T13:42:15.000Z'
    data:
      edited: false
      editors:
      - crystal-technologies
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9638175964355469
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/646ead21ee36629c15bb6487/csohtL1aHhnXdEwrk5lJw.png?w=200&h=200&f=face
          fullname: Vatsal Dutt
          isHf: false
          isPro: false
          name: crystal-technologies
          type: user
        html: '<p>Hello! I am extremely impressed by the work that you have done.<br>I
          am interested in training my own LLaVa-MPT model using a more higher-end
          MPT-30B  base model, but I cannot seem to find any instructions on how to
          do so. I have come across various resources that explain the training process
          using LLaMA models, but not specifically for the MPT. If you could provide
          any guidance or resources, it would be greatly appreciated.</p>

          <p>Thank you very much!</p>

          '
        raw: "Hello! I am extremely impressed by the work that you have done.\r\n\
          I am interested in training my own LLaVa-MPT model using a more higher-end\
          \ MPT-30B  base model, but I cannot seem to find any instructions on how\
          \ to do so. I have come across various resources that explain the training\
          \ process using LLaMA models, but not specifically for the MPT. If you could\
          \ provide any guidance or resources, it would be greatly appreciated.\r\n\
          \r\nThank you very much!"
        updatedAt: '2023-12-15T13:42:15.742Z'
      numEdits: 0
      reactions: []
    id: 657c57b7b12a3c8df32a4d18
    type: comment
  author: crystal-technologies
  content: "Hello! I am extremely impressed by the work that you have done.\r\nI am\
    \ interested in training my own LLaVa-MPT model using a more higher-end MPT-30B\
    \  base model, but I cannot seem to find any instructions on how to do so. I have\
    \ come across various resources that explain the training process using LLaMA\
    \ models, but not specifically for the MPT. If you could provide any guidance\
    \ or resources, it would be greatly appreciated.\r\n\r\nThank you very much!"
  created_at: 2023-12-15 13:42:15+00:00
  edited: false
  hidden: false
  id: 657c57b7b12a3c8df32a4d18
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: liuhaotian/LLaVA-Lightning-MPT-7B-preview
repo_type: model
status: open
target_branch: null
title: Training Instructions
