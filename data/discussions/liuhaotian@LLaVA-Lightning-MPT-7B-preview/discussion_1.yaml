!!python/object:huggingface_hub.community.DiscussionWithDetails
author: thefaheem
conflicting_files: null
created_at: 2023-05-08 13:25:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-08T14:25:10.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>How to Use This Delta Weight Applied MPT Weights of LLaVa with the\
          \ eval/run_llava.py script. It'll be Very Helpful.</p>\n<p>Thanks in Advance\U0001F60A\
          \ <span data-props=\"{&quot;user&quot;:&quot;liuhaotian&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/liuhaotian\">@<span class=\"\
          underline\">liuhaotian</span></a></span>\n\n\t</span></span> </p>\n"
        raw: "How to Use This Delta Weight Applied MPT Weights of LLaVa with the eval/run_llava.py\
          \ script. It'll be Very Helpful.\r\n\r\nThanks in Advance\U0001F60A @liuhaotian "
        updatedAt: '2023-05-08T14:25:10.833Z'
      numEdits: 0
      reactions: []
    id: 64590646990172cd1d72268c
    type: comment
  author: thefaheem
  content: "How to Use This Delta Weight Applied MPT Weights of LLaVa with the eval/run_llava.py\
    \ script. It'll be Very Helpful.\r\n\r\nThanks in Advance\U0001F60A @liuhaotian "
  created_at: 2023-05-08 13:25:10+00:00
  edited: false
  hidden: false
  id: 64590646990172cd1d72268c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg?w=200&h=200&f=face
      fullname: Haotian Liu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: liuhaotian
      type: user
    createdAt: '2023-05-08T16:09:06.000Z'
    data:
      edited: false
      editors:
      - liuhaotian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674248167280-63898b61ec1f539adc0f4da2.jpeg?w=200&h=200&f=face
          fullname: Haotian Liu
          isHf: false
          isPro: false
          name: liuhaotian
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;thefaheem&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/thefaheem\"\
          >@<span class=\"underline\">thefaheem</span></a></span>\n\n\t</span></span>,\
          \ you can simply pull the latest code base and set <code>--model-name</code>\
          \ to <code>liuhaotian/LLaVA-Lightning-MPT-7B-preview</code>.</p>\n<p>It\
          \ will download and load the model automatically.</p>\n"
        raw: 'Hi @thefaheem, you can simply pull the latest code base and set `--model-name`
          to `liuhaotian/LLaVA-Lightning-MPT-7B-preview`.


          It will download and load the model automatically.'
        updatedAt: '2023-05-08T16:09:06.355Z'
      numEdits: 0
      reactions: []
    id: 64591ea2c5d0d57ba4223159
    type: comment
  author: liuhaotian
  content: 'Hi @thefaheem, you can simply pull the latest code base and set `--model-name`
    to `liuhaotian/LLaVA-Lightning-MPT-7B-preview`.


    It will download and load the model automatically.'
  created_at: 2023-05-08 15:09:06+00:00
  edited: false
  hidden: false
  id: 64591ea2c5d0d57ba4223159
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-08T16:31:39.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>This is What I Asked You in Your Twitter DM. Anyways Thanks For\
          \ Effort and Support. </p>\n<p>Your Work inspires Me amd it's Really Amazing\
          \ <span data-props=\"{&quot;user&quot;:&quot;liuhaotian&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/liuhaotian\">@<span class=\"\
          underline\">liuhaotian</span></a></span>\n\n\t</span></span></p>\n"
        raw: "This is What I Asked You in Your Twitter DM. Anyways Thanks For Effort\
          \ and Support. \n\nYour Work inspires Me amd it's Really Amazing @liuhaotian"
        updatedAt: '2023-05-08T16:31:39.886Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645923eb39e6aea69cc0d30e
    id: 645923eb39e6aea69cc0d30d
    type: comment
  author: thefaheem
  content: "This is What I Asked You in Your Twitter DM. Anyways Thanks For Effort\
    \ and Support. \n\nYour Work inspires Me amd it's Really Amazing @liuhaotian"
  created_at: 2023-05-08 15:31:39+00:00
  edited: false
  hidden: false
  id: 645923eb39e6aea69cc0d30d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-08T16:31:39.000Z'
    data:
      status: closed
    id: 645923eb39e6aea69cc0d30e
    type: status-change
  author: thefaheem
  created_at: 2023-05-08 15:31:39+00:00
  id: 645923eb39e6aea69cc0d30e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-09T06:59:15.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p>This is What I Got when I initialise the model with device map with\
          \ \"auto\"</p>\n<p>/content/llava/model/mpt/attention.py:148: UserWarning:\
          \ Using <code>attn_impl: torch</code>. If your model does not use <code>alibi</code>\
          \ or <code>prefix_lm</code> we recommend using <code>attn_impl: flash</code>\
          \ otherwise we recommend using <code>attn_impl: triton</code>.<br>  warnings.warn('Using\
          \ <code>attn_impl: torch</code>. If your model does not use <code>alibi</code>\
          \ or ' + '<code>prefix_lm</code> we recommend using <code>attn_impl: flash</code>\
          \ otherwise ' + 'we recommend using <code>attn_impl: triton</code>.')<br>Some\
          \ weights of the model checkpoint at openai/clip-vit-large-patch14 were\
          \ not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm2.weight',\
          \ 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight',\
          \ 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'text_projection.weight', 'text_model.encoder.layers.9.layer_norm1.bias',\
          \ 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight',\
          \ 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight',\
          \ 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight',\
          \ 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias',\
          \ 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight',\
          \ 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight',\
          \ 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.weight',\
          \ 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias',\
          \ 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight',\
          \ 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias',\
          \ 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.weight',\
          \ 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias',\
          \ 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight',\
          \ 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.layer_norm1.bias',\
          \ 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight',\
          \ 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias',\
          \ 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias',\
          \ 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.0.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.5.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight',\
          \ 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias',\
          \ 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight',\
          \ 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight',\
          \ 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight',\
          \ 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight',\
          \ 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias',\
          \ 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight',\
          \ 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias',\
          \ 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias',\
          \ 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias',\
          \ 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias',\
          \ 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias',\
          \ 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias',\
          \ 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight',\
          \ 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.bias',\
          \ 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.weight',\
          \ 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias',\
          \ 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias',\
          \ 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias',\
          \ 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight',\
          \ 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias',\
          \ 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight']</p>\n\
          <ul>\n<li>This IS expected if you are initializing CLIPVisionModel from\
          \ the checkpoint of a model trained on another task or with another architecture\
          \ (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).</li>\n<li>This IS NOT expected if you are initializing CLIPVisionModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).<br>\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most\
          \ recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\
          <br>\u2502 in &lt;cell line: 3&gt;:3                                   \
          \                                           \u2502<br>\u2502           \
          \                                                                      \
          \                 \u2502<br>\u2502 /content/llava/llava_eval.py:39 in llava_generate\
          \                                                \u2502<br>\u2502      \
          \                                                                      \
          \                      \u2502<br>\u2502    36 \u2502   tokenizer = AutoTokenizer.from_pretrained(model_name)\
          \                                  \u2502<br>\u2502    37 \u2502       \
          \                                                                      \
          \             \u2502<br>\u2502    38 \u2502   if \"mpt\" in model_name.lower():\
          \                                                        \u2502<br>\u2502\
          \ \u2771  39 \u2502   \u2502   model = LlavaMPTForCausalLM.from_pretrained(model_name,\
          \ low_cpu_mem_usage=True,    \u2502<br>\u2502    40 \u2502   else:     \
          \                                                                      \
          \       \u2502<br>\u2502    41 \u2502   \u2502   model = LlavaLlamaForCausalLM.from_pretrained(model_name,\
          \ low_cpu_mem_usage=True   \u2502<br>\u2502    42 \u2502   image_processor\
          \ = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, t \
          \  \u2502<br>\u2502                                                    \
          \                                              \u2502<br>\u2502 /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2693\
          \ in from_pretrained   \u2502<br>\u2502                                \
          \                                                                  \u2502\
          <br>\u2502   2690 \u2502   \u2502   \u2502   )                         \
          \                                                    \u2502<br>\u2502  \
          \ 2691 \u2502   \u2502   \u2502                                        \
          \                                         \u2502<br>\u2502   2692 \u2502\
          \   \u2502   \u2502   if model._no_split_modules is None:              \
          \                             \u2502<br>\u2502 \u2771 2693 \u2502   \u2502\
          \   \u2502   \u2502   raise ValueError(f\"{model.<strong>class</strong>.<strong>name</strong>}\
          \ does not support <code>device_m  \u2502 \u2502   2694 \u2502   \u2502\
          \   \u2502   no_split_modules = model._no_split_modules                \
          \                    \u2502 \u2502   2695 \u2502   \u2502   \u2502   if\
          \ device_map not in [\"auto\", \"balanced\", \"balanced_low_0\", \"sequential\"\
          ]:    \u2502 \u2502   2696 \u2502   \u2502   \u2502   \u2502   raise ValueError(\
          \                                                         \u2502 \u2570\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u256F ValueError: LlavaMPTForCausalLM does not support </code>device_map='auto'`\
          \ yet.</li>\n</ul>\n<p>is there any way to solve this and load the model\
          \ with device map as auto to utilise GPU, CPU and Disk.</p>\n"
        raw: "This is What I Got when I initialise the model with device map with\
          \ \"auto\"\n\n/content/llava/model/mpt/attention.py:148: UserWarning: Using\
          \ `attn_impl: torch`. If your model does not use `alibi` or `prefix_lm`\
          \ we recommend using `attn_impl: flash` otherwise we recommend using `attn_impl:\
          \ triton`.\n  warnings.warn('Using `attn_impl: torch`. If your model does\
          \ not use `alibi` or ' + '`prefix_lm` we recommend using `attn_impl: flash`\
          \ otherwise ' + 'we recommend using `attn_impl: triton`.')\nSome weights\
          \ of the model checkpoint at openai/clip-vit-large-patch14 were not used\
          \ when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm2.weight',\
          \ 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight',\
          \ 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias',\
          \ 'text_projection.weight', 'text_model.encoder.layers.9.layer_norm1.bias',\
          \ 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight',\
          \ 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight',\
          \ 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight',\
          \ 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias',\
          \ 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight',\
          \ 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight',\
          \ 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.weight',\
          \ 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm2.bias',\
          \ 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight',\
          \ 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias',\
          \ 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.weight',\
          \ 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias',\
          \ 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight',\
          \ 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.layer_norm1.bias',\
          \ 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight',\
          \ 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.bias',\
          \ 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias',\
          \ 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.0.mlp.fc1.bias', 'logit_scale', 'text_model.encoder.layers.5.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight',\
          \ 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias',\
          \ 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight',\
          \ 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight',\
          \ 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.weight',\
          \ 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight',\
          \ 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm1.bias',\
          \ 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight',\
          \ 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias',\
          \ 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight',\
          \ 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias',\
          \ 'visual_projection.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias',\
          \ 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias',\
          \ 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias',\
          \ 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight',\
          \ 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias',\
          \ 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias',\
          \ 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight',\
          \ 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight',\
          \ 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias',\
          \ 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.bias',\
          \ 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.weight',\
          \ 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight',\
          \ 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight',\
          \ 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.4.layer_norm1.bias',\
          \ 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias',\
          \ 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias',\
          \ 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight',\
          \ 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias',\
          \ 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias',\
          \ 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias',\
          \ 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight']\n\
          - This IS expected if you are initializing CLIPVisionModel from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\n- This IS NOT expected if you are initializing CLIPVisionModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \ in <cell line: 3>:3                                                  \
          \                            \u2502\n\u2502                            \
          \                                                                      \u2502\
          \n\u2502 /content/llava/llava_eval.py:39 in llava_generate             \
          \                                   \u2502\n\u2502                     \
          \                                                                      \
          \       \u2502\n\u2502    36 \u2502   tokenizer = AutoTokenizer.from_pretrained(model_name)\
          \                                  \u2502\n\u2502    37 \u2502         \
          \                                                                      \
          \           \u2502\n\u2502    38 \u2502   if \"mpt\" in model_name.lower():\
          \                                                        \u2502\n\u2502\
          \ \u2771  39 \u2502   \u2502   model = LlavaMPTForCausalLM.from_pretrained(model_name,\
          \ low_cpu_mem_usage=True,    \u2502\n\u2502    40 \u2502   else:       \
          \                                                                      \
          \     \u2502\n\u2502    41 \u2502   \u2502   model = LlavaLlamaForCausalLM.from_pretrained(model_name,\
          \ low_cpu_mem_usage=True   \u2502\n\u2502    42 \u2502   image_processor\
          \ = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, t \
          \  \u2502\n\u2502                                                      \
          \                                            \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2693\
          \ in from_pretrained   \u2502\n\u2502                                  \
          \                                                                \u2502\n\
          \u2502   2690 \u2502   \u2502   \u2502   )                             \
          \                                                \u2502\n\u2502   2691 \u2502\
          \   \u2502   \u2502                                                    \
          \                             \u2502\n\u2502   2692 \u2502   \u2502   \u2502\
          \   if model._no_split_modules is None:                                \
          \           \u2502\n\u2502 \u2771 2693 \u2502   \u2502   \u2502   \u2502\
          \   raise ValueError(f\"{model.__class__.__name__} does not support `device_m\
          \  \u2502\n\u2502   2694 \u2502   \u2502   \u2502   no_split_modules = model._no_split_modules\
          \                                    \u2502\n\u2502   2695 \u2502   \u2502\
          \   \u2502   if device_map not in [\"auto\", \"balanced\", \"balanced_low_0\"\
          , \"sequential\"]:    \u2502\n\u2502   2696 \u2502   \u2502   \u2502   \u2502\
          \   raise ValueError(                                                  \
          \       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u256F\nValueError: LlavaMPTForCausalLM does\
          \ not support `device_map='auto'` yet.\n\nis there any way to solve this\
          \ and load the model with device map as auto to utilise GPU, CPU and Disk."
        updatedAt: '2023-05-09T06:59:15.877Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6459ef43bdf5b1fa58e84235
    id: 6459ef43bdf5b1fa58e84234
    type: comment
  author: thefaheem
  content: "This is What I Got when I initialise the model with device map with \"\
    auto\"\n\n/content/llava/model/mpt/attention.py:148: UserWarning: Using `attn_impl:\
    \ torch`. If your model does not use `alibi` or `prefix_lm` we recommend using\
    \ `attn_impl: flash` otherwise we recommend using `attn_impl: triton`.\n  warnings.warn('Using\
    \ `attn_impl: torch`. If your model does not use `alibi` or ' + '`prefix_lm` we\
    \ recommend using `attn_impl: flash` otherwise ' + 'we recommend using `attn_impl:\
    \ triton`.')\nSome weights of the model checkpoint at openai/clip-vit-large-patch14\
    \ were not used when initializing CLIPVisionModel: ['text_model.encoder.layers.0.layer_norm2.weight',\
    \ 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.bias',\
    \ 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight',\
    \ 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight',\
    \ 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias',\
    \ 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias',\
    \ 'text_projection.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.bias',\
    \ 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight',\
    \ 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight',\
    \ 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight',\
    \ 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight',\
    \ 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight',\
    \ 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight',\
    \ 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight',\
    \ 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias',\
    \ 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias',\
    \ 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight',\
    \ 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight',\
    \ 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight',\
    \ 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight',\
    \ 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight',\
    \ 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias',\
    \ 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias',\
    \ 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight',\
    \ 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight',\
    \ 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias',\
    \ 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias',\
    \ 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.final_layer_norm.weight',\
    \ 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias',\
    \ 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm1.weight',\
    \ 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias',\
    \ 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight',\
    \ 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.weight',\
    \ 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias',\
    \ 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.bias',\
    \ 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias',\
    \ 'logit_scale', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight',\
    \ 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias',\
    \ 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias',\
    \ 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias',\
    \ 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias',\
    \ 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias',\
    \ 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight',\
    \ 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight',\
    \ 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.bias',\
    \ 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias',\
    \ 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.embeddings.position_ids',\
    \ 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias',\
    \ 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight',\
    \ 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias',\
    \ 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'visual_projection.weight',\
    \ 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias',\
    \ 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias',\
    \ 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight',\
    \ 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias',\
    \ 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias',\
    \ 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias',\
    \ 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.embeddings.position_embedding.weight',\
    \ 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight',\
    \ 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight',\
    \ 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.bias',\
    \ 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.bias',\
    \ 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias',\
    \ 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight',\
    \ 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight',\
    \ 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias',\
    \ 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias',\
    \ 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight',\
    \ 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight',\
    \ 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight',\
    \ 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias',\
    \ 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias',\
    \ 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm2.weight',\
    \ 'text_model.encoder.layers.2.self_attn.out_proj.weight']\n- This IS expected\
    \ if you are initializing CLIPVisionModel from the checkpoint of a model trained\
    \ on another task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing\
    \ CLIPVisionModel from the checkpoint of a model that you expect to be exactly\
    \ identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
    \ model).\n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256E\n\u2502 in <cell line: 3>:3             \
    \                                                                 \u2502\n\u2502\
    \                                                                            \
    \                      \u2502\n\u2502 /content/llava/llava_eval.py:39 in llava_generate\
    \                                                \u2502\n\u2502              \
    \                                                                            \
    \        \u2502\n\u2502    36 \u2502   tokenizer = AutoTokenizer.from_pretrained(model_name)\
    \                                  \u2502\n\u2502    37 \u2502               \
    \                                                                           \u2502\
    \n\u2502    38 \u2502   if \"mpt\" in model_name.lower():                    \
    \                                    \u2502\n\u2502 \u2771  39 \u2502   \u2502\
    \   model = LlavaMPTForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True,\
    \    \u2502\n\u2502    40 \u2502   else:                                     \
    \                                             \u2502\n\u2502    41 \u2502   \u2502\
    \   model = LlavaLlamaForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True\
    \   \u2502\n\u2502    42 \u2502   image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower,\
    \ t   \u2502\n\u2502                                                         \
    \                                         \u2502\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2693\
    \ in from_pretrained   \u2502\n\u2502                                        \
    \                                                          \u2502\n\u2502   2690\
    \ \u2502   \u2502   \u2502   )                                               \
    \                              \u2502\n\u2502   2691 \u2502   \u2502   \u2502\
    \                                                                            \
    \     \u2502\n\u2502   2692 \u2502   \u2502   \u2502   if model._no_split_modules\
    \ is None:                                           \u2502\n\u2502 \u2771 2693\
    \ \u2502   \u2502   \u2502   \u2502   raise ValueError(f\"{model.__class__.__name__}\
    \ does not support `device_m  \u2502\n\u2502   2694 \u2502   \u2502   \u2502 \
    \  no_split_modules = model._no_split_modules                                \
    \    \u2502\n\u2502   2695 \u2502   \u2502   \u2502   if device_map not in [\"\
    auto\", \"balanced\", \"balanced_low_0\", \"sequential\"]:    \u2502\n\u2502 \
    \  2696 \u2502   \u2502   \u2502   \u2502   raise ValueError(                \
    \                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u256F\nValueError: LlavaMPTForCausalLM does not support `device_map='auto'`\
    \ yet.\n\nis there any way to solve this and load the model with device map as\
    \ auto to utilise GPU, CPU and Disk."
  created_at: 2023-05-09 05:59:15+00:00
  edited: false
  hidden: false
  id: 6459ef43bdf5b1fa58e84234
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-09T06:59:15.000Z'
    data:
      status: open
    id: 6459ef43bdf5b1fa58e84235
    type: status-change
  author: thefaheem
  created_at: 2023-05-09 05:59:15+00:00
  id: 6459ef43bdf5b1fa58e84235
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
      fullname: Mohammed Faheem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thefaheem
      type: user
    createdAt: '2023-05-09T06:59:54.000Z'
    data:
      edited: false
      editors:
      - thefaheem
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1679399646223-noauth.png?w=200&h=200&f=face
          fullname: Mohammed Faheem
          isHf: false
          isPro: false
          name: thefaheem
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;liuhaotian&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/liuhaotian\">@<span class=\"\
          underline\">liuhaotian</span></a></span>\n\n\t</span></span> look down here!</p>\n"
        raw: '@liuhaotian look down here!'
        updatedAt: '2023-05-09T06:59:54.990Z'
      numEdits: 0
      reactions: []
    id: 6459ef6a2829ab9e9247b555
    type: comment
  author: thefaheem
  content: '@liuhaotian look down here!'
  created_at: 2023-05-09 05:59:54+00:00
  edited: false
  hidden: false
  id: 6459ef6a2829ab9e9247b555
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: liuhaotian/LLaVA-Lightning-MPT-7B-preview
repo_type: model
status: open
target_branch: null
title: Using with the Script
