!!python/object:huggingface_hub.community.DiscussionWithDetails
author: RecViking
conflicting_files: null
created_at: 2023-05-15 13:06:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
      fullname: garrett galloway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: RecViking
      type: user
    createdAt: '2023-05-15T14:06:50.000Z'
    data:
      edited: true
      editors:
      - RecViking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
          fullname: garrett galloway
          isHf: false
          isPro: true
          name: RecViking
          type: user
        html: '<p>What would cause this model to end up CPU bound while running inference?
          This is loaded to GPU but seems to be stuck doing some portion of the inference
          on CPU. I have the same issue whether loaded as AutoModelForCausalLM.from_pretrained
          and pipeline. Inference is <em>SUPER</em> slow and it won''t load up my
          GPU much more then 30% on usage.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png"><img
          alt="Screenshot 2023-05-15 at 8.04.30 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png"></a></p>

          <p>I''ve snipped the relevant code (minus includes) if I''m doing anything
          wrong when loading these.</p>

          <p>pipe workflow:<br>path = self.settings[''model_string'']<br>pipe = pipeline("text-generation",
          model=path, torch_dtype=torch.bfloat16, device=0)<br>self.pipe = pipe<br>return
          self.pipe(inputs, **parameters)</p>

          <p>AutoModel workflow:<br>path = self.settings[''model_string'']<br>self.tokenizer
          = AutoTokenizer.from_pretrained(path)<br>model = AutoModelForCausalLM.from_pretrained(path,
          return_dict=True, load_in_8bit=True, device_map=self.device, torch_dtype=torch.float16)<br>self.model
          = model<br>inputs = self.tokenizer(inputs, return_tensors="pt").to("cuda")<br>outputs
          = self.model.generate(**inputs, **parameters)<br>return self.tokenizer.decode(outputs[0],
          skip_special_tokens=False)</p>

          '
        raw: 'What would cause this model to end up CPU bound while running inference?
          This is loaded to GPU but seems to be stuck doing some portion of the inference
          on CPU. I have the same issue whether loaded as AutoModelForCausalLM.from_pretrained
          and pipeline. Inference is *SUPER* slow and it won''t load up my GPU much
          more then 30% on usage.

          ![Screenshot 2023-05-15 at 8.04.30 AM.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png)



          I''ve snipped the relevant code (minus includes) if I''m doing anything
          wrong when loading these.


          pipe workflow:

          path = self.settings[''model_string'']

          pipe = pipeline("text-generation", model=path, torch_dtype=torch.bfloat16,
          device=0)

          self.pipe = pipe

          return self.pipe(inputs, **parameters)


          AutoModel workflow:

          path = self.settings[''model_string'']

          self.tokenizer = AutoTokenizer.from_pretrained(path)

          model = AutoModelForCausalLM.from_pretrained(path, return_dict=True, load_in_8bit=True,
          device_map=self.device, torch_dtype=torch.float16)

          self.model = model

          inputs = self.tokenizer(inputs, return_tensors="pt").to("cuda")

          outputs = self.model.generate(**inputs, **parameters)

          return self.tokenizer.decode(outputs[0], skip_special_tokens=False)'
        updatedAt: '2023-05-15T14:24:41.220Z'
      numEdits: 1
      reactions: []
    id: 64623c7ad9b49021880ed35d
    type: comment
  author: RecViking
  content: 'What would cause this model to end up CPU bound while running inference?
    This is loaded to GPU but seems to be stuck doing some portion of the inference
    on CPU. I have the same issue whether loaded as AutoModelForCausalLM.from_pretrained
    and pipeline. Inference is *SUPER* slow and it won''t load up my GPU much more
    then 30% on usage.

    ![Screenshot 2023-05-15 at 8.04.30 AM.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png)



    I''ve snipped the relevant code (minus includes) if I''m doing anything wrong
    when loading these.


    pipe workflow:

    path = self.settings[''model_string'']

    pipe = pipeline("text-generation", model=path, torch_dtype=torch.bfloat16, device=0)

    self.pipe = pipe

    return self.pipe(inputs, **parameters)


    AutoModel workflow:

    path = self.settings[''model_string'']

    self.tokenizer = AutoTokenizer.from_pretrained(path)

    model = AutoModelForCausalLM.from_pretrained(path, return_dict=True, load_in_8bit=True,
    device_map=self.device, torch_dtype=torch.float16)

    self.model = model

    inputs = self.tokenizer(inputs, return_tensors="pt").to("cuda")

    outputs = self.model.generate(**inputs, **parameters)

    return self.tokenizer.decode(outputs[0], skip_special_tokens=False)'
  created_at: 2023-05-15 13:06:50+00:00
  edited: true
  hidden: false
  id: 64623c7ad9b49021880ed35d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62457ca9e1ecfd09533c0f21/22zWCnNpw_7CQZHSH4ryw.jpeg?w=200&h=200&f=face
      fullname: Sayf Eddine HAMMEMI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: piratos
      type: user
    createdAt: '2023-05-28T15:31:32.000Z'
    data:
      edited: true
      editors:
      - piratos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62457ca9e1ecfd09533c0f21/22zWCnNpw_7CQZHSH4ryw.jpeg?w=200&h=200&f=face
          fullname: Sayf Eddine HAMMEMI
          isHf: false
          isPro: false
          name: piratos
          type: user
        html: '<p>I have the same issue on other LLMs too, I suspect this is coming
          from bitsandbytes lib used when loading in 8 bits<br><a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues/388">https://github.com/TimDettmers/bitsandbytes/issues/388</a></p>

          '
        raw: 'I have the same issue on other LLMs too, I suspect this is coming from
          bitsandbytes lib used when loading in 8 bits

          https://github.com/TimDettmers/bitsandbytes/issues/388'
        updatedAt: '2023-05-28T15:31:54.116Z'
      numEdits: 1
      reactions: []
    id: 647373d42a74fb43ccdecd4a
    type: comment
  author: piratos
  content: 'I have the same issue on other LLMs too, I suspect this is coming from
    bitsandbytes lib used when loading in 8 bits

    https://github.com/TimDettmers/bitsandbytes/issues/388'
  created_at: 2023-05-28 14:31:32+00:00
  edited: true
  hidden: false
  id: 647373d42a74fb43ccdecd4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
      fullname: garrett galloway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: RecViking
      type: user
    createdAt: '2023-05-28T15:55:40.000Z'
    data:
      edited: true
      editors:
      - RecViking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
          fullname: garrett galloway
          isHf: false
          isPro: true
          name: RecViking
          type: user
        html: '<p>It runs CPU bound regardless of the mode you run it in. (here''s
          fp16)<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/JIy7FznL4N2qLeoiyCG_I.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/JIy7FznL4N2qLeoiyCG_I.png"></a><br>tokenizer
          = AutoTokenizer.from_pretrained("HuggingFaceH4/starchat-alpha")#path)<br>model
          = AutoModelForCausalLM.from_pretrained(<br>    "HuggingFaceH4/starchat-alpha",<br>    return_dict=True,<br>    #load_in_8bit=True,<br>    device_map="auto",#{"":2},<br>    torch_dtype=torch.float16,<br>    trust_remote_code=True,<br>    local_files_only=True,<br>)<br>model.resize_token_embeddings(len(tokenizer))<br>#model
          = PeftModel.from_pretrained(model, path)</p>

          <p>Still running badly (here''s default, whatever it uses)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/Bj9XBxfUVqkaIfDikYFCx.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/Bj9XBxfUVqkaIfDikYFCx.png"></a></p>

          <p>tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/starchat-alpha")#path)<br>model
          = AutoModelForCausalLM.from_pretrained(<br>    "HuggingFaceH4/starchat-alpha",<br>    return_dict=True,<br>    #load_in_8bit=True,<br>    device_map="auto",#{"":2},<br>    #torch_dtype=torch.float16,<br>    trust_remote_code=True,<br>    local_files_only=True,<br>)<br>model.resize_token_embeddings(len(tokenizer))</p>

          '
        raw: "It runs CPU bound regardless of the mode you run it in. (here's fp16)\n\
          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/JIy7FznL4N2qLeoiyCG_I.png)\n\
          tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/starchat-alpha\"\
          )#path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/starchat-alpha\"\
          ,\n    return_dict=True,\n    #load_in_8bit=True,\n    device_map=\"auto\"\
          ,#{\"\":2},\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n\
          \    local_files_only=True,\n)\nmodel.resize_token_embeddings(len(tokenizer))\n\
          #model = PeftModel.from_pretrained(model, path)\n\nStill running badly (here's\
          \ default, whatever it uses)\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/Bj9XBxfUVqkaIfDikYFCx.png)\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/starchat-alpha\"\
          )#path)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/starchat-alpha\"\
          ,\n    return_dict=True,\n    #load_in_8bit=True,\n    device_map=\"auto\"\
          ,#{\"\":2},\n    #torch_dtype=torch.float16,\n    trust_remote_code=True,\n\
          \    local_files_only=True,\n)\nmodel.resize_token_embeddings(len(tokenizer))"
        updatedAt: '2023-05-28T15:56:20.589Z'
      numEdits: 1
      reactions: []
    id: 6473797c6cff2f867205d389
    type: comment
  author: RecViking
  content: "It runs CPU bound regardless of the mode you run it in. (here's fp16)\n\
    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/JIy7FznL4N2qLeoiyCG_I.png)\n\
    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/starchat-alpha\")#path)\n\
    model = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/starchat-alpha\"\
    ,\n    return_dict=True,\n    #load_in_8bit=True,\n    device_map=\"auto\",#{\"\
    \":2},\n    torch_dtype=torch.float16,\n    trust_remote_code=True,\n    local_files_only=True,\n\
    )\nmodel.resize_token_embeddings(len(tokenizer))\n#model = PeftModel.from_pretrained(model,\
    \ path)\n\nStill running badly (here's default, whatever it uses)\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/Bj9XBxfUVqkaIfDikYFCx.png)\n\
    \ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/starchat-alpha\")#path)\n\
    model = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/starchat-alpha\"\
    ,\n    return_dict=True,\n    #load_in_8bit=True,\n    device_map=\"auto\",#{\"\
    \":2},\n    #torch_dtype=torch.float16,\n    trust_remote_code=True,\n    local_files_only=True,\n\
    )\nmodel.resize_token_embeddings(len(tokenizer))"
  created_at: 2023-05-28 14:55:40+00:00
  edited: true
  hidden: false
  id: 6473797c6cff2f867205d389
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: CPU bound when loaded on GPU?
