!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vermanic
conflicting_files: null
created_at: 2023-09-07 16:49:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
      fullname: Nikhil Verma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vermanic
      type: user
    createdAt: '2023-09-07T17:49:01.000Z'
    data:
      edited: true
      editors:
      - vermanic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6436129808425903
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aed90af7a731cc77677f3603f75bfac9.svg
          fullname: Nikhil Verma
          isHf: false
          isPro: false
          name: vermanic
          type: user
        html: "<p>So the output of my model ends abruptly and I ideally want it to\
          \ complete the paragraph/sentences/code which it was it between of.<br>Although\
          \ I have provided max_new_tokens = 300 and also in prompt I give to limit\
          \ by 300 words.</p>\n<p>The response is always big and ends abruptly. Any\
          \ way I can ask for a complete output within desired number of output tokens?\
          \ </p>\n<p>Code:</p>\n<pre><code>checkpoint = \"HuggingFaceH4/starchat-alpha\"\
          \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" # \"cuda:X\"\
          \ for GPU usage or \"cpu\" for CPU usage\nclass StarCoderModel:\n  def __init__(self):\n\
          \    print(\"Running in \" + device)\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          \    # make sure `--gpus all` is provided in docker run command if gpu is\
          \ required\n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ device_map='auto')\n\n  def infer(self, input_text, token_count):\n  \
          \  print(input_text)\n    print(token_count)\n    inputs = self.tokenizer.encode(input_text,\
          \ return_tensors=\"pt\").to(device)\n    print(len(self.tokenizer.tokenize(input_text)))\n\
          \    outputs = self.model.generate(inputs,  max_new_tokens=token_count,\
          \ pad_token_id=self.tokenizer.eos_token_id)\n    return self.tokenizer.decode(outputs[0])[len(input_text):]\n\
          </code></pre>\n<p>Sample:</p>\n<pre><code>private DataType FuntionName(String\
          \ someId) {\n    // TODO: Replace with implementation that utilizes someId\
          \ to obtain information\n    return DataType.Value;\n}\n\n\nThe comment:\n\
          \n- If someId is present in the code, use the getAPI from Client with someId\
          \ as a parameter to obtain some information.\n- If the\n</code></pre>\n"
        raw: "So the output of my model ends abruptly and I ideally want it to complete\
          \ the paragraph/sentences/code which it was it between of.\nAlthough I have\
          \ provided max_new_tokens = 300 and also in prompt I give to limit by 300\
          \ words.\n\nThe response is always big and ends abruptly. Any way I can\
          \ ask for a complete output within desired number of output tokens? \n\n\
          Code:\n```\ncheckpoint = \"HuggingFaceH4/starchat-alpha\"\ndevice = \"cuda\"\
          \ if torch.cuda.is_available() else \"cpu\" # \"cuda:X\" for GPU usage or\
          \ \"cpu\" for CPU usage\nclass StarCoderModel:\n  def __init__(self):\n\
          \    print(\"Running in \" + device)\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
          \    # make sure `--gpus all` is provided in docker run command if gpu is\
          \ required\n    self.model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ device_map='auto')\n\n  def infer(self, input_text, token_count):\n  \
          \  print(input_text)\n    print(token_count)\n    inputs = self.tokenizer.encode(input_text,\
          \ return_tensors=\"pt\").to(device)\n    print(len(self.tokenizer.tokenize(input_text)))\n\
          \    outputs = self.model.generate(inputs,  max_new_tokens=token_count,\
          \ pad_token_id=self.tokenizer.eos_token_id)\n    return self.tokenizer.decode(outputs[0])[len(input_text):]\n\
          ```\n\n\nSample:\n\n```\nprivate DataType FuntionName(String someId) {\n\
          \    // TODO: Replace with implementation that utilizes someId to obtain\
          \ information\n    return DataType.Value;\n}\n\n\nThe comment:\n\n- If someId\
          \ is present in the code, use the getAPI from Client with someId as a parameter\
          \ to obtain some information.\n- If the\n\n```"
        updatedAt: '2023-09-07T17:52:08.028Z'
      numEdits: 3
      reactions: []
    id: 64fa0d0d31a82e0d40fdc2bc
    type: comment
  author: vermanic
  content: "So the output of my model ends abruptly and I ideally want it to complete\
    \ the paragraph/sentences/code which it was it between of.\nAlthough I have provided\
    \ max_new_tokens = 300 and also in prompt I give to limit by 300 words.\n\nThe\
    \ response is always big and ends abruptly. Any way I can ask for a complete output\
    \ within desired number of output tokens? \n\nCode:\n```\ncheckpoint = \"HuggingFaceH4/starchat-alpha\"\
    \ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\" # \"cuda:X\" for\
    \ GPU usage or \"cpu\" for CPU usage\nclass StarCoderModel:\n  def __init__(self):\n\
    \    print(\"Running in \" + device)\n    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\
    \    # make sure `--gpus all` is provided in docker run command if gpu is required\n\
    \    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')\n\
    \n  def infer(self, input_text, token_count):\n    print(input_text)\n    print(token_count)\n\
    \    inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\
    \    print(len(self.tokenizer.tokenize(input_text)))\n    outputs = self.model.generate(inputs,\
    \  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)\n   \
    \ return self.tokenizer.decode(outputs[0])[len(input_text):]\n```\n\n\nSample:\n\
    \n```\nprivate DataType FuntionName(String someId) {\n    // TODO: Replace with\
    \ implementation that utilizes someId to obtain information\n    return DataType.Value;\n\
    }\n\n\nThe comment:\n\n- If someId is present in the code, use the getAPI from\
    \ Client with someId as a parameter to obtain some information.\n- If the\n\n\
    ```"
  created_at: 2023-09-07 16:49:01+00:00
  edited: true
  hidden: false
  id: 64fa0d0d31a82e0d40fdc2bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: Incomplete Output even with max_new_tokens
