!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vivekam101
conflicting_files: null
created_at: 2023-05-27 17:16:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ea5f0f2dd81f4ec3fc27f25fa906075.svg
      fullname: vivek tv
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vivekam101
      type: user
    createdAt: '2023-05-27T18:16:57.000Z'
    data:
      edited: false
      editors:
      - vivekam101
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ea5f0f2dd81f4ec3fc27f25fa906075.svg
          fullname: vivek tv
          isHf: false
          isPro: false
          name: vivekam101
          type: user
        html: "<p>I followed above changes. But still Generated text are getting truncated<br>prompt:\
          \ How can I write a Python function to generate the nth Fibonacci number?</p>\n\
          <p>response: Here is a simple Python function to generate the nth Fibonacci\
          \ number:</p>\n<p>```python<br>def fib(n):<br>if n &lt;= 1</p>\n<p>prompt:\
          \ can you explain me the algorithm of merge sort ?</p>\n<p>response: Sure,\
          \ I\u2019d be happy to explain the algorithm of merge sort.<br>Merge sort\
          \ is a divide-and-conquer algorithm that works by</p>\n<p>Code Snippet<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(MODEL_DIR)<br>model = AutoModelForCausalLM.from_pretrained(MODEL_DIR,\
          \ device=\"auto\", torch_dtype=torch.bfloat16)<br>text = \"&lt;|system|&gt;\\\
          n&lt;|end|&gt;\\n&lt;|user|&gt;\" + text + \"&lt;|end|&gt;\\n&lt;|assistant|&gt;\"\
          <br>inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)<br>outputs\
          \ = model.generate(inputs, do_sample=True, max_new_tokens=64)<br>response\
          \ = tokenizer.decode(outputs[0])<br>--<br>any idea why its happening ? help\
          \ will be appreciated</p>\n"
        raw: "I followed above changes. But still Generated text are getting truncated\r\
          \nprompt: How can I write a Python function to generate the nth Fibonacci\
          \ number?\r\n\r\nresponse: Here is a simple Python function to generate\
          \ the nth Fibonacci number:\r\n\r\n```python\r\ndef fib(n):\r\nif n <= 1\r\
          \n\r\nprompt: can you explain me the algorithm of merge sort ?\r\n\r\nresponse:\
          \ Sure, I\u2019d be happy to explain the algorithm of merge sort.\r\nMerge\
          \ sort is a divide-and-conquer algorithm that works by\r\n\r\nCode Snippet\r\
          \ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\r\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR,\
          \ device=\"auto\", torch_dtype=torch.bfloat16)\r\ntext = \"<|system|>\\\
          n<|end|>\\n<|user|>\" + text + \"<|end|>\\n<|assistant|>\"\r\ninputs = tokenizer.encode(text,\
          \ return_tensors=\"pt\").to(device)\r\noutputs = model.generate(inputs,\
          \ do_sample=True, max_new_tokens=64)\r\nresponse = tokenizer.decode(outputs[0])\r\
          \n--\r\nany idea why its happening ? help will be appreciated"
        updatedAt: '2023-05-27T18:16:57.819Z'
      numEdits: 0
      reactions: []
    id: 6472491922016353ae3b32f4
    type: comment
  author: vivekam101
  content: "I followed above changes. But still Generated text are getting truncated\r\
    \nprompt: How can I write a Python function to generate the nth Fibonacci number?\r\
    \n\r\nresponse: Here is a simple Python function to generate the nth Fibonacci\
    \ number:\r\n\r\n```python\r\ndef fib(n):\r\nif n <= 1\r\n\r\nprompt: can you\
    \ explain me the algorithm of merge sort ?\r\n\r\nresponse: Sure, I\u2019d be\
    \ happy to explain the algorithm of merge sort.\r\nMerge sort is a divide-and-conquer\
    \ algorithm that works by\r\n\r\nCode Snippet\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(MODEL_DIR, device=\"auto\", torch_dtype=torch.bfloat16)\r\
    \ntext = \"<|system|>\\n<|end|>\\n<|user|>\" + text + \"<|end|>\\n<|assistant|>\"\
    \r\ninputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\r\noutputs\
    \ = model.generate(inputs, do_sample=True, max_new_tokens=64)\r\nresponse = tokenizer.decode(outputs[0])\r\
    \n--\r\nany idea why its happening ? help will be appreciated"
  created_at: 2023-05-27 17:16:57+00:00
  edited: false
  hidden: false
  id: 6472491922016353ae3b32f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c3cb5f0f16a801197928edd2530c03f0.svg
      fullname: LI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jnkr36
      type: user
    createdAt: '2023-06-09T04:44:11.000Z'
    data:
      edited: true
      editors:
      - jnkr36
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4343510568141937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c3cb5f0f16a801197928edd2530c03f0.svg
          fullname: LI
          isHf: false
          isPro: false
          name: jnkr36
          type: user
        html: '<p>please try this below:</p>

          <p>import torch<br>from transformers import pipeline</p>

          <p>pipe = pipeline("text-generation", model=your model path, torch_dtype=torch.bfloat16,
          device_map="auto")</p>

          <p>text = "How can I write a Python function to generate the nth Fibonacci
          number?"<br>prompt_template = "&lt;|system|&gt;\n&lt;|end|&gt;\n&lt;|user|&gt;\n{query}&lt;|end|&gt;\n&lt;|assistant|&gt;"<br>prompt
          = prompt_template.format(query=text)<br>outputs = pipe(prompt, max_new_tokens=256,
          stop_sequence=''&lt;|end|&gt;'', do_sample=True, temperature=0.2, top_k=50,
          top_p=0.95, eos_token_id=49155)<br>print(outputs)<br>print(outputs[0][''generated_text''])<br>generated
          = outputs[0][''generated_text''].split(''&lt;|assistant|&gt;'')[-1]<br>print(generated)</p>

          '
        raw: 'please try this below:


          import torch

          from transformers import pipeline


          pipe = pipeline("text-generation", model=your model path, torch_dtype=torch.bfloat16,
          device_map="auto")


          text = "How can I write a Python function to generate the nth Fibonacci
          number?"

          prompt_template = "<|system|>\n<|end|>\n<|user|>\n{query}<|end|>\n<|assistant|>"

          prompt = prompt_template.format(query=text)

          outputs = pipe(prompt, max_new_tokens=256, stop_sequence=''<|end|>'', do_sample=True,
          temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)

          print(outputs)

          print(outputs[0][''generated_text''])

          generated = outputs[0][''generated_text''].split(''<|assistant|>'')[-1]

          print(generated)'
        updatedAt: '2023-06-09T04:44:31.628Z'
      numEdits: 1
      reactions: []
    id: 6482ae1b3c609184f6dfffa8
    type: comment
  author: jnkr36
  content: 'please try this below:


    import torch

    from transformers import pipeline


    pipe = pipeline("text-generation", model=your model path, torch_dtype=torch.bfloat16,
    device_map="auto")


    text = "How can I write a Python function to generate the nth Fibonacci number?"

    prompt_template = "<|system|>\n<|end|>\n<|user|>\n{query}<|end|>\n<|assistant|>"

    prompt = prompt_template.format(query=text)

    outputs = pipe(prompt, max_new_tokens=256, stop_sequence=''<|end|>'', do_sample=True,
    temperature=0.2, top_k=50, top_p=0.95, eos_token_id=49155)

    print(outputs)

    print(outputs[0][''generated_text''])

    generated = outputs[0][''generated_text''].split(''<|assistant|>'')[-1]

    print(generated)'
  created_at: 2023-06-09 03:44:11+00:00
  edited: true
  hidden: false
  id: 6482ae1b3c609184f6dfffa8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: Text Truncation even with increase in max_new_tokens
