!!python/object:huggingface_hub.community.DiscussionWithDetails
author: merlinarer
conflicting_files: null
created_at: 2023-05-13 08:16:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
      fullname: merlinchan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: merlinarer
      type: user
    createdAt: '2023-05-13T09:16:00.000Z'
    data:
      edited: true
      editors:
      - merlinarer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0640fbbf09e7f5bc19d4f483e64e7678.svg
          fullname: merlinchan
          isHf: false
          isPro: false
          name: merlinarer
          type: user
        html: "<p>Thanks for the nice work! I want to deploy the chat model in my\
          \ GPUs with your palyground, while I fail to process the stream properly.\
          \ Can you share the server code that process the prompt and return stream\
          \ ?<br>I use the following code:</p>\n<pre><code>    output = \"\"\n   \
          \ stream = pipe(prompt)\n    for idx, response in enumerate(stream):\n \
          \       output += response['generated_text'].replace(prompt, '')\n     \
          \   if idx == 0:\n            history.append(\" \" + output)\n        else:\n\
          \            history[-1] = output\n        chat = [(history[i].strip(),\
          \ history[i + 1].strip()) for i in range(0, len(history) - 1, 2)]\n    \
          \    yield chat, history, user_message, \"\"\n</code></pre>\n<p>while it\
          \ can only respose in first time and got nothing after that. I check it\
          \ and find that, everytime  the pipe just generate a <code>/n</code> after\
          \ prompt and that is why user got nothing .</p>\n"
        raw: "Thanks for the nice work! I want to deploy the chat model in my GPUs\
          \ with your palyground, while I fail to process the stream properly. Can\
          \ you share the server code that process the prompt and return stream ?\n\
          I use the following code:\n```\n    output = \"\"\n    stream = pipe(prompt)\n\
          \    for idx, response in enumerate(stream):\n        output += response['generated_text'].replace(prompt,\
          \ '')\n        if idx == 0:\n            history.append(\" \" + output)\n\
          \        else:\n            history[-1] = output\n        chat = [(history[i].strip(),\
          \ history[i + 1].strip()) for i in range(0, len(history) - 1, 2)]\n    \
          \    yield chat, history, user_message, \"\"\n```\nwhile it can only respose\
          \ in first time and got nothing after that. I check it and find that, everytime\
          \  the pipe just generate a `/n` after prompt and that is why user got nothing\
          \ ."
        updatedAt: '2023-05-13T09:54:00.858Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - TreeisnotTree
        - insight431
    id: 645f5550a1f1dedc46d45910
    type: comment
  author: merlinarer
  content: "Thanks for the nice work! I want to deploy the chat model in my GPUs with\
    \ your palyground, while I fail to process the stream properly. Can you share\
    \ the server code that process the prompt and return stream ?\nI use the following\
    \ code:\n```\n    output = \"\"\n    stream = pipe(prompt)\n    for idx, response\
    \ in enumerate(stream):\n        output += response['generated_text'].replace(prompt,\
    \ '')\n        if idx == 0:\n            history.append(\" \" + output)\n    \
    \    else:\n            history[-1] = output\n        chat = [(history[i].strip(),\
    \ history[i + 1].strip()) for i in range(0, len(history) - 1, 2)]\n        yield\
    \ chat, history, user_message, \"\"\n```\nwhile it can only respose in first time\
    \ and got nothing after that. I check it and find that, everytime  the pipe just\
    \ generate a `/n` after prompt and that is why user got nothing ."
  created_at: 2023-05-13 08:16:00+00:00
  edited: true
  hidden: false
  id: 645f5550a1f1dedc46d45910
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: Can you share the server code for local deploy?
