!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ravra
conflicting_files: null
created_at: 2023-06-01 10:36:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d300a67dc77c52dd595d97090213763.svg
      fullname: RR
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ravra
      type: user
    createdAt: '2023-06-01T11:36:48.000Z'
    data:
      edited: true
      editors:
      - ravra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d300a67dc77c52dd595d97090213763.svg
          fullname: RR
          isHf: false
          isPro: false
          name: ravra
          type: user
        html: "<p>Hi,</p>\n<p>I tried to deploy Model on Vertex AI, but it is giving\
          \ error.</p>\n<p>I am getting error like: ERROR 2023-05-31T07:38:18.376283832Z\
          \ [resource.labels.taskName: workerpool0-0] File \"main.py\", line 8, in<br>{<br>\"\
          insertId\": \"1hjwmhtfm1rso4\",<br>\"jsonPayload\": {<br>\"message\": \"\
          \ File \"main.py\", line 8, in \\n\",<br>\"attrs\": {<br>\"tag\": \"workerpool0-0\"\
          <br>},<br>\"levelname\": \"ERROR\"<br>},<br>\"resource\": {<br>\"type\"\
          : \"ml_job\",<br>\"labels\": {<br>\"job_id\": \"3056473000426602496\",<br>\"\
          task_name\": \"workerpool0-0\",<br>\"project_id\": \"api-appexecutable-com\"\
          <br>}<br>},<br>\"timestamp\": \"2023-05-31T07:38:18.376283832Z\",<br>\"\
          severity\": \"ERROR\",<br>\"labels\": {<br>\"ml.googleapis.com/tpu_worker_id\"\
          : \"\",<br>\"compute.googleapis.com/resource_name\": \"cmle-training-18349684525105594269\"\
          ,<br>\"ml.googleapis.com/trial_type\": \"\",<br>\"ml.googleapis.com/job_id/log_area\"\
          : \"root\",<br>\"ml.googleapis.com/trial_id\": \"\",<br>\"compute.googleapis.com/resource_id\"\
          : \"1504603656255391738\",<br>\"compute.googleapis.com/zone\": \"us-west1-b\"\
          <br>},<br>\"logName\": \"projects/api-appexecutable-com/logs/workerpool0-0\"\
          ,<br>\"receiveTimestamp\": \"2023-05-31T07:38:52.295831776Z\"<br>}</p>\n\
          <h1 id=\"-----mainpy-is\">---- main.py is:</h1>\n<p>from flask import Flask,\
          \ request, jsonify<br>import torch<br>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM</p>\n<p>app = Flask(name)</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(\"\
          HuggingFaceH4/starchat-alpha\")<br>model = AutoModelForCausalLM.from_pretrained(\"\
          HuggingFaceH4/starchat-alpha\",<br>load_in_8bit=True,<br>device_map='auto',<br>torch_dtype=torch.float16)</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;app&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/app\">@<span class=\"\
          underline\">app</span></a></span>\n\n\t</span></span>.route('/generate',\
          \ methods=['POST'])<br>def generate():<br>data = request.json<br>input_prompt\
          \ = data['prompt']<br>system_prompt = \"\\nBelow is a conversation between\
          \ a human user and a helpful AI coding assistant.\\n\"<br>user_prompt =\
          \ f\"\\n{input_prompt}\\n\"<br>assistant_prompt = \"\"<br>full_prompt =\
          \ system_prompt + user_prompt + assistant_prompt<br>inputs = tokenizer.encode(full_prompt,\
          \ return_tensors=\"pt\").to('cuda')<br>outputs = model.generate(inputs,<br>eos_token_id\
          \ = 0,<br>pad_token_id = 0,<br>max_length=256,<br>early_stopping=True)<br>output\
          \ = tokenizer.decode(outputs[0])<br>output = output[len(full_prompt):]<br>if\
          \ \"\" in output:<br>cutoff = output.find(\"\")<br>output = output[:cutoff]<br>return\
          \ jsonify({'response': output})</p>\n<p>if name == 'main':<br>app.run(host='0.0.0.0',\
          \ port=5000)</p>\n<h1 id=\"-----requirementstxt-having\">---- requirements.txt\
          \ having</h1>\n<p>flask==2.0.1<br>torch==1.10.0<br>transformers==4.8.2</p>\n\
          <h1 id=\"-----dockerfile-having\">---- Dockerfile having</h1>\n<p>FROM python:3.8-slim-buster<br>WORKDIR\
          \ /app<br>COPY . .<br>COPY requirements.txt ./<br>RUN pip install --no-cache-dir\
          \ -r requirements.txt<br>CMD [\"python\", \"main.py\"]</p>\n<p>Can anyone\
          \ guide me, where I am doing mistake</p>\n"
        raw: 'Hi,


          I tried to deploy Model on Vertex AI, but it is giving error.


          I am getting error like: ERROR 2023-05-31T07:38:18.376283832Z [resource.labels.taskName:
          workerpool0-0] File "main.py", line 8, in

          {

          "insertId": "1hjwmhtfm1rso4",

          "jsonPayload": {

          "message": " File "main.py", line 8, in \n",

          "attrs": {

          "tag": "workerpool0-0"

          },

          "levelname": "ERROR"

          },

          "resource": {

          "type": "ml_job",

          "labels": {

          "job_id": "3056473000426602496",

          "task_name": "workerpool0-0",

          "project_id": "api-appexecutable-com"

          }

          },

          "timestamp": "2023-05-31T07:38:18.376283832Z",

          "severity": "ERROR",

          "labels": {

          "ml.googleapis.com/tpu_worker_id": "",

          "compute.googleapis.com/resource_name": "cmle-training-18349684525105594269",

          "ml.googleapis.com/trial_type": "",

          "ml.googleapis.com/job_id/log_area": "root",

          "ml.googleapis.com/trial_id": "",

          "compute.googleapis.com/resource_id": "1504603656255391738",

          "compute.googleapis.com/zone": "us-west1-b"

          },

          "logName": "projects/api-appexecutable-com/logs/workerpool0-0",

          "receiveTimestamp": "2023-05-31T07:38:52.295831776Z"

          }


          # ---- main.py is:


          from flask import Flask, request, jsonify

          import torch

          from transformers import AutoTokenizer, AutoModelForCausalLM


          app = Flask(name)


          tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/starchat-alpha")

          model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/starchat-alpha",

          load_in_8bit=True,

          device_map=''auto'',

          torch_dtype=torch.float16)


          @app.route(''/generate'', methods=[''POST''])

          def generate():

          data = request.json

          input_prompt = data[''prompt'']

          system_prompt = "\nBelow is a conversation between a human user and a helpful
          AI coding assistant.\n"

          user_prompt = f"\n{input_prompt}\n"

          assistant_prompt = ""

          full_prompt = system_prompt + user_prompt + assistant_prompt

          inputs = tokenizer.encode(full_prompt, return_tensors="pt").to(''cuda'')

          outputs = model.generate(inputs,

          eos_token_id = 0,

          pad_token_id = 0,

          max_length=256,

          early_stopping=True)

          output = tokenizer.decode(outputs[0])

          output = output[len(full_prompt):]

          if "" in output:

          cutoff = output.find("")

          output = output[:cutoff]

          return jsonify({''response'': output})


          if name == ''main'':

          app.run(host=''0.0.0.0'', port=5000)



          # ---- requirements.txt having


          flask==2.0.1

          torch==1.10.0

          transformers==4.8.2


          # ---- Dockerfile having


          FROM python:3.8-slim-buster

          WORKDIR /app

          COPY . .

          COPY requirements.txt ./

          RUN pip install --no-cache-dir -r requirements.txt

          CMD ["python", "main.py"]



          Can anyone guide me, where I am doing mistake'
        updatedAt: '2023-06-01T11:38:32.043Z'
      numEdits: 2
      reactions: []
    id: 647882d075a9a0cfabd06e4f
    type: comment
  author: ravra
  content: 'Hi,


    I tried to deploy Model on Vertex AI, but it is giving error.


    I am getting error like: ERROR 2023-05-31T07:38:18.376283832Z [resource.labels.taskName:
    workerpool0-0] File "main.py", line 8, in

    {

    "insertId": "1hjwmhtfm1rso4",

    "jsonPayload": {

    "message": " File "main.py", line 8, in \n",

    "attrs": {

    "tag": "workerpool0-0"

    },

    "levelname": "ERROR"

    },

    "resource": {

    "type": "ml_job",

    "labels": {

    "job_id": "3056473000426602496",

    "task_name": "workerpool0-0",

    "project_id": "api-appexecutable-com"

    }

    },

    "timestamp": "2023-05-31T07:38:18.376283832Z",

    "severity": "ERROR",

    "labels": {

    "ml.googleapis.com/tpu_worker_id": "",

    "compute.googleapis.com/resource_name": "cmle-training-18349684525105594269",

    "ml.googleapis.com/trial_type": "",

    "ml.googleapis.com/job_id/log_area": "root",

    "ml.googleapis.com/trial_id": "",

    "compute.googleapis.com/resource_id": "1504603656255391738",

    "compute.googleapis.com/zone": "us-west1-b"

    },

    "logName": "projects/api-appexecutable-com/logs/workerpool0-0",

    "receiveTimestamp": "2023-05-31T07:38:52.295831776Z"

    }


    # ---- main.py is:


    from flask import Flask, request, jsonify

    import torch

    from transformers import AutoTokenizer, AutoModelForCausalLM


    app = Flask(name)


    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/starchat-alpha")

    model = AutoModelForCausalLM.from_pretrained("HuggingFaceH4/starchat-alpha",

    load_in_8bit=True,

    device_map=''auto'',

    torch_dtype=torch.float16)


    @app.route(''/generate'', methods=[''POST''])

    def generate():

    data = request.json

    input_prompt = data[''prompt'']

    system_prompt = "\nBelow is a conversation between a human user and a helpful
    AI coding assistant.\n"

    user_prompt = f"\n{input_prompt}\n"

    assistant_prompt = ""

    full_prompt = system_prompt + user_prompt + assistant_prompt

    inputs = tokenizer.encode(full_prompt, return_tensors="pt").to(''cuda'')

    outputs = model.generate(inputs,

    eos_token_id = 0,

    pad_token_id = 0,

    max_length=256,

    early_stopping=True)

    output = tokenizer.decode(outputs[0])

    output = output[len(full_prompt):]

    if "" in output:

    cutoff = output.find("")

    output = output[:cutoff]

    return jsonify({''response'': output})


    if name == ''main'':

    app.run(host=''0.0.0.0'', port=5000)



    # ---- requirements.txt having


    flask==2.0.1

    torch==1.10.0

    transformers==4.8.2


    # ---- Dockerfile having


    FROM python:3.8-slim-buster

    WORKDIR /app

    COPY . .

    COPY requirements.txt ./

    RUN pip install --no-cache-dir -r requirements.txt

    CMD ["python", "main.py"]



    Can anyone guide me, where I am doing mistake'
  created_at: 2023-06-01 10:36:48+00:00
  edited: true
  hidden: false
  id: 647882d075a9a0cfabd06e4f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: How to deploy this on Vertex AI?
