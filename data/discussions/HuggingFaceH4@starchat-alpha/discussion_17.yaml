!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tonyaw
conflicting_files: null
created_at: 2023-06-07 13:46:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed8d9a55a19d8c5a138ba918d7c2450e.svg
      fullname: Tony W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonyaw
      type: user
    createdAt: '2023-06-07T14:46:39.000Z'
    data:
      edited: false
      editors:
      - tonyaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4581805467605591
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed8d9a55a19d8c5a138ba918d7c2450e.svg
          fullname: Tony W
          isHf: false
          isPro: false
          name: tonyaw
          type: user
        html: '<p>I want to use PEFT+LoRA to fine-tune starchat-alpha.<br>I assume
          "target_modules" shall be set to "starcoder" according to following code:<br>"utils/other.py"<br>TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING
          = {<br>"t5": ["q", "v"],<br>"mt5": ["q", "v"],<br>"bart": ["q_proj", "v_proj"],<br>"gpt2":
          ["c_attn"],<br>"bloom": ["query_key_value"],<br>"blip-2": ["q", "v", "q_proj",
          "v_proj"],<br>"opt": ["q_proj", "v_proj"],<br>"gptj": ["q_proj", "v_proj"],<br>"gpt_neox":
          ["query_key_value"],<br>"gpt_neo": ["q_proj", "v_proj"],<br>"bert": ["query",
          "value"],<br>"roberta": ["query", "value"],<br>"xlm-roberta": ["query",
          "value"],<br>"electra": ["query", "value"],<br>"deberta-v2": ["query_proj",
          "value_proj"],<br>"deberta": ["in_proj"],<br>"layoutlm": ["query", "value"],<br>"llama":
          ["q_proj", "v_proj"],<br>"chatglm": ["query_key_value"],<br>"starcoder":
          ["c_attn"]</p>

          <p>Then I got the following error:<br>Unhandled Exception<br>Traceback (most
          recent call last):<br>File "./starcoder_train.py", line 410, in<br>main()<br>File
          "./starcoder_train.py", line 329, in main<br>model = get_peft_model(model,
          peft_config)<br>File "/usr/local/lib/python3.8/dist-packages/peft/mapping.py",
          line 120, in get_peft_model<br>return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model,
          peft_config)<br>File "/usr/local/lib/python3.8/dist-packages/peft/peft_model.py",
          line 691, in init<br>super().init(model, peft_config, adapter_name)<br>File
          "/usr/local/lib/python3.8/dist-packages/peft/peft_model.py", line 100, in
          init<br>self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](<br>File
          "/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py", line 174,
          in init<br>self.add_adapter(adapter_name, self.peft_config[adapter_name])<br>File
          "/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py", line 181,
          in add_adapter<br>self._find_and_replace(adapter_name)<br>File "/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py",
          line 309, in _find_and_replace<br>raise ValueError(<br>ValueError: Target
          modules starcoder not found in the base model. Please check the target modules
          and try again.</p>

          <p>Looks like it is caused by "weight_map" defined in pytorch_model.bin.index.json.
          All of them are started with transformer, and doesn''t contain "starcoder":<br>"weight_map":
          {<br>"lm_head.weight": "pytorch_model-00004-of-00004.bin",<br>"transformer.h.0.attn.c_attn.bias":
          "pytorch_model-00001-of-00004.bin",<br>"transformer.h.0.attn.c_attn.weight":
          "pytorch_model-00001-of-00004.bin",<br>"transformer.h.0.attn.c_proj.bias":
          "pytorch_model-00001-of-00004.bin",<br>"transformer.h.0.attn.c_proj.weight":
          "pytorch_model-00001-of-00004.bin",<br>"transformer.h.0.ln_1.bias": "pytorch_model-00001-of-00004.bin",<br>"transformer.h.0.ln_1.weight":
          "pytorch_model-00001-of-00004.bin",</p>

          <p>Could you please help to check if my usage is right or is it a bug?</p>

          '
        raw: "I want to use PEFT+LoRA to fine-tune starchat-alpha.\r\nI assume \"\
          target_modules\" shall be set to \"starcoder\" according to following code:\r\
          \n\"utils/other.py\"\r\nTRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\
          \ = {\r\n\"t5\": [\"q\", \"v\"],\r\n\"mt5\": [\"q\", \"v\"],\r\n\"bart\"\
          : [\"q_proj\", \"v_proj\"],\r\n\"gpt2\": [\"c_attn\"],\r\n\"bloom\": [\"\
          query_key_value\"],\r\n\"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"\
          ],\r\n\"opt\": [\"q_proj\", \"v_proj\"],\r\n\"gptj\": [\"q_proj\", \"v_proj\"\
          ],\r\n\"gpt_neox\": [\"query_key_value\"],\r\n\"gpt_neo\": [\"q_proj\",\
          \ \"v_proj\"],\r\n\"bert\": [\"query\", \"value\"],\r\n\"roberta\": [\"\
          query\", \"value\"],\r\n\"xlm-roberta\": [\"query\", \"value\"],\r\n\"electra\"\
          : [\"query\", \"value\"],\r\n\"deberta-v2\": [\"query_proj\", \"value_proj\"\
          ],\r\n\"deberta\": [\"in_proj\"],\r\n\"layoutlm\": [\"query\", \"value\"\
          ],\r\n\"llama\": [\"q_proj\", \"v_proj\"],\r\n\"chatglm\": [\"query_key_value\"\
          ],\r\n\"starcoder\": [\"c_attn\"]\r\n\r\nThen I got the following error:\r\
          \nUnhandled Exception\r\nTraceback (most recent call last):\r\nFile \"./starcoder_train.py\"\
          , line 410, in\r\nmain()\r\nFile \"./starcoder_train.py\", line 329, in\
          \ main\r\nmodel = get_peft_model(model, peft_config)\r\nFile \"/usr/local/lib/python3.8/dist-packages/peft/mapping.py\"\
          , line 120, in get_peft_model\r\nreturn MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model,\
          \ peft_config)\r\nFile \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\"\
          , line 691, in init\r\nsuper().init(model, peft_config, adapter_name)\r\n\
          File \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\", line\
          \ 100, in init\r\nself.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\r\
          \nFile \"/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py\", line\
          \ 174, in init\r\nself.add_adapter(adapter_name, self.peft_config[adapter_name])\r\
          \nFile \"/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py\", line\
          \ 181, in add_adapter\r\nself._find_and_replace(adapter_name)\r\nFile \"\
          /usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py\", line 309,\
          \ in _find_and_replace\r\nraise ValueError(\r\nValueError: Target modules\
          \ starcoder not found in the base model. Please check the target modules\
          \ and try again.\r\n\r\nLooks like it is caused by \"weight_map\" defined\
          \ in pytorch_model.bin.index.json. All of them are started with transformer,\
          \ and doesn't contain \"starcoder\":\r\n\"weight_map\": {\r\n\"lm_head.weight\"\
          : \"pytorch_model-00004-of-00004.bin\",\r\n\"transformer.h.0.attn.c_attn.bias\"\
          : \"pytorch_model-00001-of-00004.bin\",\r\n\"transformer.h.0.attn.c_attn.weight\"\
          : \"pytorch_model-00001-of-00004.bin\",\r\n\"transformer.h.0.attn.c_proj.bias\"\
          : \"pytorch_model-00001-of-00004.bin\",\r\n\"transformer.h.0.attn.c_proj.weight\"\
          : \"pytorch_model-00001-of-00004.bin\",\r\n\"transformer.h.0.ln_1.bias\"\
          : \"pytorch_model-00001-of-00004.bin\",\r\n\"transformer.h.0.ln_1.weight\"\
          : \"pytorch_model-00001-of-00004.bin\",\r\n\r\nCould you please help to\
          \ check if my usage is right or is it a bug?"
        updatedAt: '2023-06-07T14:46:39.701Z'
      numEdits: 0
      reactions: []
    id: 6480984f856901b0edb9bd56
    type: comment
  author: tonyaw
  content: "I want to use PEFT+LoRA to fine-tune starchat-alpha.\r\nI assume \"target_modules\"\
    \ shall be set to \"starcoder\" according to following code:\r\n\"utils/other.py\"\
    \r\nTRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING = {\r\n\"t5\": [\"q\",\
    \ \"v\"],\r\n\"mt5\": [\"q\", \"v\"],\r\n\"bart\": [\"q_proj\", \"v_proj\"],\r\
    \n\"gpt2\": [\"c_attn\"],\r\n\"bloom\": [\"query_key_value\"],\r\n\"blip-2\":\
    \ [\"q\", \"v\", \"q_proj\", \"v_proj\"],\r\n\"opt\": [\"q_proj\", \"v_proj\"\
    ],\r\n\"gptj\": [\"q_proj\", \"v_proj\"],\r\n\"gpt_neox\": [\"query_key_value\"\
    ],\r\n\"gpt_neo\": [\"q_proj\", \"v_proj\"],\r\n\"bert\": [\"query\", \"value\"\
    ],\r\n\"roberta\": [\"query\", \"value\"],\r\n\"xlm-roberta\": [\"query\", \"\
    value\"],\r\n\"electra\": [\"query\", \"value\"],\r\n\"deberta-v2\": [\"query_proj\"\
    , \"value_proj\"],\r\n\"deberta\": [\"in_proj\"],\r\n\"layoutlm\": [\"query\"\
    , \"value\"],\r\n\"llama\": [\"q_proj\", \"v_proj\"],\r\n\"chatglm\": [\"query_key_value\"\
    ],\r\n\"starcoder\": [\"c_attn\"]\r\n\r\nThen I got the following error:\r\nUnhandled\
    \ Exception\r\nTraceback (most recent call last):\r\nFile \"./starcoder_train.py\"\
    , line 410, in\r\nmain()\r\nFile \"./starcoder_train.py\", line 329, in main\r\
    \nmodel = get_peft_model(model, peft_config)\r\nFile \"/usr/local/lib/python3.8/dist-packages/peft/mapping.py\"\
    , line 120, in get_peft_model\r\nreturn MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model,\
    \ peft_config)\r\nFile \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\"\
    , line 691, in init\r\nsuper().init(model, peft_config, adapter_name)\r\nFile\
    \ \"/usr/local/lib/python3.8/dist-packages/peft/peft_model.py\", line 100, in\
    \ init\r\nself.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\r\
    \nFile \"/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py\", line 174,\
    \ in init\r\nself.add_adapter(adapter_name, self.peft_config[adapter_name])\r\n\
    File \"/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py\", line 181,\
    \ in add_adapter\r\nself._find_and_replace(adapter_name)\r\nFile \"/usr/local/lib/python3.8/dist-packages/peft/tuners/lora.py\"\
    , line 309, in _find_and_replace\r\nraise ValueError(\r\nValueError: Target modules\
    \ starcoder not found in the base model. Please check the target modules and try\
    \ again.\r\n\r\nLooks like it is caused by \"weight_map\" defined in pytorch_model.bin.index.json.\
    \ All of them are started with transformer, and doesn't contain \"starcoder\"\
    :\r\n\"weight_map\": {\r\n\"lm_head.weight\": \"pytorch_model-00004-of-00004.bin\"\
    ,\r\n\"transformer.h.0.attn.c_attn.bias\": \"pytorch_model-00001-of-00004.bin\"\
    ,\r\n\"transformer.h.0.attn.c_attn.weight\": \"pytorch_model-00001-of-00004.bin\"\
    ,\r\n\"transformer.h.0.attn.c_proj.bias\": \"pytorch_model-00001-of-00004.bin\"\
    ,\r\n\"transformer.h.0.attn.c_proj.weight\": \"pytorch_model-00001-of-00004.bin\"\
    ,\r\n\"transformer.h.0.ln_1.bias\": \"pytorch_model-00001-of-00004.bin\",\r\n\"\
    transformer.h.0.ln_1.weight\": \"pytorch_model-00001-of-00004.bin\",\r\n\r\nCould\
    \ you please help to check if my usage is right or is it a bug?"
  created_at: 2023-06-07 13:46:39+00:00
  edited: false
  hidden: false
  id: 6480984f856901b0edb9bd56
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ed8d9a55a19d8c5a138ba918d7c2450e.svg
      fullname: Tony W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonyaw
      type: user
    createdAt: '2023-06-08T08:56:46.000Z'
    data:
      edited: false
      editors:
      - tonyaw
      hidden: false
      identifiedLanguage:
        language: th
        probability: 0.056722622364759445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ed8d9a55a19d8c5a138ba918d7c2450e.svg
          fullname: Tony W
          isHf: false
          isPro: false
          name: tonyaw
          type: user
        html: '<p>target_modules=[''c_attn'']</p>

          '
        raw: target_modules=['c_attn']
        updatedAt: '2023-06-08T08:56:46.651Z'
      numEdits: 0
      reactions: []
      relatedEventId: 648197ce722e358ebf8495e6
    id: 648197ce722e358ebf8495e5
    type: comment
  author: tonyaw
  content: target_modules=['c_attn']
  created_at: 2023-06-08 07:56:46+00:00
  edited: false
  hidden: false
  id: 648197ce722e358ebf8495e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ed8d9a55a19d8c5a138ba918d7c2450e.svg
      fullname: Tony W
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tonyaw
      type: user
    createdAt: '2023-06-08T08:56:46.000Z'
    data:
      status: closed
    id: 648197ce722e358ebf8495e6
    type: status-change
  author: tonyaw
  created_at: 2023-06-08 07:56:46+00:00
  id: 648197ce722e358ebf8495e6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: closed
target_branch: null
title: How to use PEFT+LoRA to fine-tune starchat-alpha
