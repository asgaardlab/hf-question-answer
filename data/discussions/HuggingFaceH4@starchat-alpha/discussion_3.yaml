!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jloganolson
conflicting_files: null
created_at: 2023-05-10 19:38:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e905ea4fedc4e2d5a18ddcec2924f7c5.svg
      fullname: Logan Olson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jloganolson
      type: user
    createdAt: '2023-05-10T20:38:28.000Z'
    data:
      edited: false
      editors:
      - jloganolson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e905ea4fedc4e2d5a18ddcec2924f7c5.svg
          fullname: Logan Olson
          isHf: false
          isPro: false
          name: jloganolson
          type: user
        html: '<p>Maybe I just haven''t run this large of a model before but I''m
          blown away by the difference in speed for this 16b model versus a 12b parameter
          model like pythia. Are there any speed-up tips? Things I''m doing so far:</p>

          <ul>

          <li>On pytorch 2.0</li>

          <li>Confirmed it''s running on CUDA (device=0 and  VRAM is soaked) </li>

          <li>torch_dtype=torch.bfloat16 </li>

          <li>even tried loading as 8 bit to no noticeable speed-up (but I guess that
          last one is more to alleviate memory than speed things up).</li>

          </ul>

          <p>Any other ideas?</p>

          '
        raw: "Maybe I just haven't run this large of a model before but I'm blown\
          \ away by the difference in speed for this 16b model versus a 12b parameter\
          \ model like pythia. Are there any speed-up tips? Things I'm doing so far:\r\
          \n* On pytorch 2.0\r\n* Confirmed it's running on CUDA (device=0 and  VRAM\
          \ is soaked) \r\n* torch_dtype=torch.bfloat16 \r\n* even tried loading as\
          \ 8 bit to no noticeable speed-up (but I guess that last one is more to\
          \ alleviate memory than speed things up). \r\n\r\nAny other ideas?"
        updatedAt: '2023-05-10T20:38:28.010Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MichaelFried
    id: 645c00c44093d5c140e3a599
    type: comment
  author: jloganolson
  content: "Maybe I just haven't run this large of a model before but I'm blown away\
    \ by the difference in speed for this 16b model versus a 12b parameter model like\
    \ pythia. Are there any speed-up tips? Things I'm doing so far:\r\n* On pytorch\
    \ 2.0\r\n* Confirmed it's running on CUDA (device=0 and  VRAM is soaked) \r\n\
    * torch_dtype=torch.bfloat16 \r\n* even tried loading as 8 bit to no noticeable\
    \ speed-up (but I guess that last one is more to alleviate memory than speed things\
    \ up). \r\n\r\nAny other ideas?"
  created_at: 2023-05-10 19:38:28+00:00
  edited: false
  hidden: false
  id: 645c00c44093d5c140e3a599
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d1048c04fb3cd2f5cf0a9993758b1d5.svg
      fullname: Michael Fried
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MichaelFried
      type: user
    createdAt: '2023-05-11T17:37:51.000Z'
    data:
      edited: true
      editors:
      - MichaelFried
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d1048c04fb3cd2f5cf0a9993758b1d5.svg
          fullname: Michael Fried
          isHf: false
          isPro: false
          name: MichaelFried
          type: user
        html: '<p>Did pytorch 2.0 gave you noticable improvement?</p>

          '
        raw: Did pytorch 2.0 gave you noticable improvement?
        updatedAt: '2023-05-11T17:38:54.543Z'
      numEdits: 2
      reactions: []
    id: 645d27ef4435a8ae3fc712cc
    type: comment
  author: MichaelFried
  content: Did pytorch 2.0 gave you noticable improvement?
  created_at: 2023-05-11 16:37:51+00:00
  edited: true
  hidden: false
  id: 645d27ef4435a8ae3fc712cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e905ea4fedc4e2d5a18ddcec2924f7c5.svg
      fullname: Logan Olson
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jloganolson
      type: user
    createdAt: '2023-05-12T03:52:23.000Z'
    data:
      edited: false
      editors:
      - jloganolson
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e905ea4fedc4e2d5a18ddcec2924f7c5.svg
          fullname: Logan Olson
          isHf: false
          isPro: false
          name: jloganolson
          type: user
        html: '<p>I actually didn''t try Pytorch 1.x so I don''t know! The one last
          thing I was going to try was deepspeed inference (per this <a rel="nofollow"
          href="https://www.deepspeed.ai/tutorials/inference-tutorial/">https://www.deepspeed.ai/tutorials/inference-tutorial/</a>)
          but I don''t know how much improvement I''m going to see on a single GPU
          machine.</p>

          '
        raw: I actually didn't try Pytorch 1.x so I don't know! The one last thing
          I was going to try was deepspeed inference (per this https://www.deepspeed.ai/tutorials/inference-tutorial/)
          but I don't know how much improvement I'm going to see on a single GPU machine.
        updatedAt: '2023-05-12T03:52:23.025Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MichaelFried
    id: 645db7f74435a8ae3fcbd960
    type: comment
  author: jloganolson
  content: I actually didn't try Pytorch 1.x so I don't know! The one last thing I
    was going to try was deepspeed inference (per this https://www.deepspeed.ai/tutorials/inference-tutorial/)
    but I don't know how much improvement I'm going to see on a single GPU machine.
  created_at: 2023-05-12 02:52:23+00:00
  edited: false
  hidden: false
  id: 645db7f74435a8ae3fcbd960
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
      fullname: Arjun Guha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: arjunguha
      type: user
    createdAt: '2023-05-13T19:01:10.000Z'
    data:
      edited: false
      editors:
      - arjunguha
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1664332914111-62d8315bad693a1a962864b3.png?w=200&h=200&f=face
          fullname: Arjun Guha
          isHf: false
          isPro: false
          name: arjunguha
          type: user
        html: '<p>Try editing the config.json file to say <code>use_cache: true</code>.
          That will help.</p>

          '
        raw: 'Try editing the config.json file to say `use_cache: true`. That will
          help.'
        updatedAt: '2023-05-13T19:01:10.272Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - MichaelFried
    id: 645fde7625a4075bcf9d2e15
    type: comment
  author: arjunguha
  content: 'Try editing the config.json file to say `use_cache: true`. That will help.'
  created_at: 2023-05-13 18:01:10+00:00
  edited: false
  hidden: false
  id: 645fde7625a4075bcf9d2e15
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png?w=200&h=200&f=face
      fullname: Leandro von Werra
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lvwerra
      type: user
    createdAt: '2023-05-13T19:06:25.000Z'
    data:
      edited: false
      editors:
      - lvwerra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5e48005437cb5b49818287a5/4uCXGGui-9QifAT4qelxU.png?w=200&h=200&f=face
          fullname: Leandro von Werra
          isHf: true
          isPro: false
          name: lvwerra
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;lewtun&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lewtun\">@<span class=\"\
          underline\">lewtun</span></a></span>\n\n\t</span></span></p>\n"
        raw: cc @lewtun
        updatedAt: '2023-05-13T19:06:25.202Z'
      numEdits: 0
      reactions: []
    id: 645fdfb14357049a57f6c669
    type: comment
  author: lvwerra
  content: cc @lewtun
  created_at: 2023-05-13 18:06:25+00:00
  edited: false
  hidden: false
  id: 645fdfb14357049a57f6c669
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d1048c04fb3cd2f5cf0a9993758b1d5.svg
      fullname: Michael Fried
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MichaelFried
      type: user
    createdAt: '2023-05-13T21:16:57.000Z'
    data:
      edited: false
      editors:
      - MichaelFried
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d1048c04fb3cd2f5cf0a9993758b1d5.svg
          fullname: Michael Fried
          isHf: false
          isPro: false
          name: MichaelFried
          type: user
        html: '<blockquote>

          <p>Try editing the config.json file to say <code>use_cache: true</code>.
          That will help.</p>

          </blockquote>

          <p>thanks.<br>I''ve noticed 40% faster inference by using <code>use_cache:
          true</code></p>

          '
        raw: '> Try editing the config.json file to say `use_cache: true`. That will
          help.


          thanks.

          I''ve noticed 40% faster inference by using `use_cache: true`'
        updatedAt: '2023-05-13T21:16:57.754Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - NERDDISCO
    id: 645ffe49a93c1779eb0f9449
    type: comment
  author: MichaelFried
  content: '> Try editing the config.json file to say `use_cache: true`. That will
    help.


    thanks.

    I''ve noticed 40% faster inference by using `use_cache: true`'
  created_at: 2023-05-13 20:16:57+00:00
  edited: false
  hidden: false
  id: 645ffe49a93c1779eb0f9449
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
      fullname: garrett galloway
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: RecViking
      type: user
    createdAt: '2023-05-15T12:12:26.000Z'
    data:
      edited: true
      editors:
      - RecViking
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/dip_4_H7cB_YjkVoyNUhY.png?w=200&h=200&f=face
          fullname: garrett galloway
          isHf: false
          isPro: true
          name: RecViking
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png"><img
          alt="Screenshot 2023-05-15 at 8.04.30 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png"></a></p>

          <p>Edit: I''m peeling this out into it''s own thread.<br><a href="https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6">https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6</a></p>

          '
        raw: '![Screenshot 2023-05-15 at 8.04.30 AM.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png)


          Edit: I''m peeling this out into it''s own thread.

          https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6'
        updatedAt: '2023-05-15T14:08:22.000Z'
      numEdits: 2
      reactions: []
    id: 646221aa7e7d4c356d8a89ed
    type: comment
  author: RecViking
  content: '![Screenshot 2023-05-15 at 8.04.30 AM.png](https://cdn-uploads.huggingface.co/production/uploads/63a77a9e8729ce5b56441f5f/L8STgoXVtTPITYcLb4TU3.png)


    Edit: I''m peeling this out into it''s own thread.

    https://huggingface.co/HuggingFaceH4/starchat-alpha/discussions/6'
  created_at: 2023-05-15 11:12:26+00:00
  edited: true
  hidden: false
  id: 646221aa7e7d4c356d8a89ed
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: General tips around inference speed?
