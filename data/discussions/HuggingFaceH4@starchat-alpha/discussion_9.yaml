!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adityasharma2695
conflicting_files: null
created_at: 2023-05-19 05:03:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9c496c49e200c02e797a7e837b8de8b.svg
      fullname: Aditya Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityasharma2695
      type: user
    createdAt: '2023-05-19T06:03:17.000Z'
    data:
      edited: true
      editors:
      - adityasharma2695
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9c496c49e200c02e797a7e837b8de8b.svg
          fullname: Aditya Sharma
          isHf: false
          isPro: false
          name: adityasharma2695
          type: user
        html: '<p>Hi </p>

          <p>I want to know if anyone has tried the inference endpoint for this model,
          currently I am not using a paid account so I want to know if this model
          can be accessed via APIs using the inference endpoints.</p>

          <p>Thanks!<br>Aditya</p>

          '
        raw: "Hi \n\nI want to know if anyone has tried the inference endpoint for\
          \ this model, currently I am not using a paid account so I want to know\
          \ if this model can be accessed via APIs using the inference endpoints.\n\
          \nThanks!\nAditya"
        updatedAt: '2023-05-19T06:08:02.436Z'
      numEdits: 1
      reactions: []
    id: 6467112576bb704aa13f47fa
    type: comment
  author: adityasharma2695
  content: "Hi \n\nI want to know if anyone has tried the inference endpoint for this\
    \ model, currently I am not using a paid account so I want to know if this model\
    \ can be accessed via APIs using the inference endpoints.\n\nThanks!\nAditya"
  created_at: 2023-05-19 05:03:17+00:00
  edited: true
  hidden: false
  id: 6467112576bb704aa13f47fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7Q4MjNeGHa9GaMiSNeTdE.jpeg?w=200&h=200&f=face
      fullname: Tim Pietrusky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NERDDISCO
      type: user
    createdAt: '2023-05-21T11:19:00.000Z'
    data:
      edited: false
      editors:
      - NERDDISCO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7Q4MjNeGHa9GaMiSNeTdE.jpeg?w=200&h=200&f=face
          fullname: Tim Pietrusky
          isHf: false
          isPro: false
          name: NERDDISCO
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;adityasharma2695&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/adityasharma2695\"\
          >@<span class=\"underline\">adityasharma2695</span></a></span>\n\n\t</span></span>\
          \ I also would love to access the model via the free Inference API. I just\
          \ got a pro-account, but it doesn't change anything, the model is still\
          \ not available and you get this message if you want to access it via the\
          \ free Inference API:</p>\n<pre><code>The model HuggingFaceH4/starchat-alpha\
          \ is too large to be loaded automatically (31GB &gt; 10GB). For commercial\
          \ use please use PRO spaces (https://huggingface.co/spaces) or Inference\
          \ Endpoints (https://huggingface.co/inference-endpoints)\n</code></pre>\n\
          <p>As mentioned in <a href=\"https://huggingface.co/spaces/HuggingFaceH4/starchat-playground/discussions/3#6469fc4e96cfe72aef76aaeb\"\
          >https://huggingface.co/spaces/HuggingFaceH4/starchat-playground/discussions/3#6469fc4e96cfe72aef76aaeb</a>,\
          \ I think there is no possibility right now to use the model for free via\
          \ the Inference API. But I might be wrong \U0001F648</p>\n"
        raw: "@adityasharma2695 I also would love to access the model via the free\
          \ Inference API. I just got a pro-account, but it doesn't change anything,\
          \ the model is still not available and you get this message if you want\
          \ to access it via the free Inference API:\n\n```\nThe model HuggingFaceH4/starchat-alpha\
          \ is too large to be loaded automatically (31GB > 10GB). For commercial\
          \ use please use PRO spaces (https://huggingface.co/spaces) or Inference\
          \ Endpoints (https://huggingface.co/inference-endpoints)\n```\n\nAs mentioned\
          \ in https://huggingface.co/spaces/HuggingFaceH4/starchat-playground/discussions/3#6469fc4e96cfe72aef76aaeb,\
          \ I think there is no possibility right now to use the model for free via\
          \ the Inference API. But I might be wrong \U0001F648"
        updatedAt: '2023-05-21T11:19:00.614Z'
      numEdits: 0
      reactions: []
    id: 6469fe24f51b56a285bba8eb
    type: comment
  author: NERDDISCO
  content: "@adityasharma2695 I also would love to access the model via the free Inference\
    \ API. I just got a pro-account, but it doesn't change anything, the model is\
    \ still not available and you get this message if you want to access it via the\
    \ free Inference API:\n\n```\nThe model HuggingFaceH4/starchat-alpha is too large\
    \ to be loaded automatically (31GB > 10GB). For commercial use please use PRO\
    \ spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)\n\
    ```\n\nAs mentioned in https://huggingface.co/spaces/HuggingFaceH4/starchat-playground/discussions/3#6469fc4e96cfe72aef76aaeb,\
    \ I think there is no possibility right now to use the model for free via the\
    \ Inference API. But I might be wrong \U0001F648"
  created_at: 2023-05-21 10:19:00+00:00
  edited: false
  hidden: false
  id: 6469fe24f51b56a285bba8eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b9c496c49e200c02e797a7e837b8de8b.svg
      fullname: Aditya Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityasharma2695
      type: user
    createdAt: '2023-05-26T08:35:06.000Z'
    data:
      edited: false
      editors:
      - adityasharma2695
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b9c496c49e200c02e797a7e837b8de8b.svg
          fullname: Aditya Sharma
          isHf: false
          isPro: false
          name: adityasharma2695
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;NERDDISCO&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/NERDDISCO\">@<span class=\"\
          underline\">NERDDISCO</span></a></span>\n\n\t</span></span><br>Thanks for\
          \ the reply and it's great that you got a paid account as well.<br>Does\
          \ it offer to deploy the model to use on interface via API? just asking\
          \ if you observed anything like that.</p>\n"
        raw: "@NERDDISCO \nThanks for the reply and it's great that you got a paid\
          \ account as well. \nDoes it offer to deploy the model to use on interface\
          \ via API? just asking if you observed anything like that."
        updatedAt: '2023-05-26T08:35:06.187Z'
      numEdits: 0
      reactions: []
    id: 64706f3a1f0e7ee7fb16d8c9
    type: comment
  author: adityasharma2695
  content: "@NERDDISCO \nThanks for the reply and it's great that you got a paid account\
    \ as well. \nDoes it offer to deploy the model to use on interface via API? just\
    \ asking if you observed anything like that."
  created_at: 2023-05-26 07:35:06+00:00
  edited: false
  hidden: false
  id: 64706f3a1f0e7ee7fb16d8c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7Q4MjNeGHa9GaMiSNeTdE.jpeg?w=200&h=200&f=face
      fullname: Tim Pietrusky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NERDDISCO
      type: user
    createdAt: '2023-05-26T21:00:00.000Z'
    data:
      edited: false
      editors:
      - NERDDISCO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7Q4MjNeGHa9GaMiSNeTdE.jpeg?w=200&h=200&f=face
          fullname: Tim Pietrusky
          isHf: false
          isPro: false
          name: NERDDISCO
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;adityasharma2695&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/adityasharma2695\"\
          >@<span class=\"underline\">adityasharma2695</span></a></span>\n\n\t</span></span>\
          \ yes, you can deploy your own Inference Endpoint, then you can use it via\
          \ API. But that has nothing to do with the pro-account. You could also do\
          \ this without a pro-account. </p>\n<p>What I'm trying to do right now is\
          \ to use quantisation to have a version of the model, that runs on slow\
          \ hardware. I will ping you once I get this working.</p>\n"
        raw: "@adityasharma2695 yes, you can deploy your own Inference Endpoint, then\
          \ you can use it via API. But that has nothing to do with the pro-account.\
          \ You could also do this without a pro-account. \n\nWhat I'm trying to do\
          \ right now is to use quantisation to have a version of the model, that\
          \ runs on slow hardware. I will ping you once I get this working."
        updatedAt: '2023-05-26T21:00:00.551Z'
      numEdits: 0
      reactions: []
    id: 64711dd01c2bfd5b7bf9ed71
    type: comment
  author: NERDDISCO
  content: "@adityasharma2695 yes, you can deploy your own Inference Endpoint, then\
    \ you can use it via API. But that has nothing to do with the pro-account. You\
    \ could also do this without a pro-account. \n\nWhat I'm trying to do right now\
    \ is to use quantisation to have a version of the model, that runs on slow hardware.\
    \ I will ping you once I get this working."
  created_at: 2023-05-26 20:00:00+00:00
  edited: false
  hidden: false
  id: 64711dd01c2bfd5b7bf9ed71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2d300a67dc77c52dd595d97090213763.svg
      fullname: RR
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ravra
      type: user
    createdAt: '2023-05-31T09:43:22.000Z'
    data:
      edited: false
      editors:
      - ravra
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2d300a67dc77c52dd595d97090213763.svg
          fullname: RR
          isHf: false
          isPro: false
          name: ravra
          type: user
        html: "<p>Can anyone guide me, how to deploy this on Vertex AI.</p>\n<p>I\
          \ am getting error like: ERROR 2023-05-31T07:38:18.376283832Z [resource.labels.taskName:\
          \ workerpool0-0] File \"main.py\", line 8, in <br>  {<br>    \"insertId\"\
          : \"1hjwmhtfm1rso4\",<br>    \"jsonPayload\": {<br>      \"message\": \"\
          \  File \"main.py\", line 8, in \\n\",<br>      \"attrs\": {<br>       \
          \ \"tag\": \"workerpool0-0\"<br>      },<br>      \"levelname\": \"ERROR\"\
          <br>    },<br>    \"resource\": {<br>      \"type\": \"ml_job\",<br>   \
          \   \"labels\": {<br>        \"job_id\": \"3056473000426602496\",<br>  \
          \      \"task_name\": \"workerpool0-0\",<br>        \"project_id\": \"api-appexecutable-com\"\
          <br>      }<br>    },<br>    \"timestamp\": \"2023-05-31T07:38:18.376283832Z\"\
          ,<br>    \"severity\": \"ERROR\",<br>    \"labels\": {<br>      \"ml.googleapis.com/tpu_worker_id\"\
          : \"\",<br>      \"compute.googleapis.com/resource_name\": \"cmle-training-18349684525105594269\"\
          ,<br>      \"ml.googleapis.com/trial_type\": \"\",<br>      \"ml.googleapis.com/job_id/log_area\"\
          : \"root\",<br>      \"ml.googleapis.com/trial_id\": \"\",<br>      \"compute.googleapis.com/resource_id\"\
          : \"1504603656255391738\",<br>      \"compute.googleapis.com/zone\": \"\
          us-west1-b\"<br>    },<br>    \"logName\": \"projects/api-appexecutable-com/logs/workerpool0-0\"\
          ,<br>    \"receiveTimestamp\": \"2023-05-31T07:38:52.295831776Z\"<br>  }</p>\n\
          <p>My main.py is:</p>\n<p>from flask import Flask, request, jsonify<br>import\
          \ torch<br>from transformers import AutoTokenizer, AutoModelForCausalLM</p>\n\
          <p>app = Flask(<strong>name</strong>)</p>\n<p>tokenizer = AutoTokenizer.from_pretrained(\"\
          HuggingFaceH4/starchat-alpha\")<br>model = AutoModelForCausalLM.from_pretrained(\"\
          HuggingFaceH4/starchat-alpha\",<br>                                    \
          \          load_in_8bit=True,<br>                                      \
          \        device_map='auto',<br>                                        \
          \      torch_dtype=torch.float16)</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;app&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/app\"\
          >@<span class=\"underline\">app</span></a></span>\n\n\t</span></span>.route('/generate',\
          \ methods=['POST'])<br>def generate():<br>    data = request.json<br>  \
          \  input_prompt = data['prompt']<br>    system_prompt = \"\\nBelow is a\
          \ conversation between a human user and a helpful AI coding assistant.\\\
          n\"<br>    user_prompt = f\"\\n{input_prompt}\\n\"<br>    assistant_prompt\
          \ = \"\"<br>    full_prompt = system_prompt + user_prompt + assistant_prompt<br>\
          \    inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to('cuda')<br>\
          \    outputs = model.generate(inputs,<br>                             eos_token_id\
          \ = 0,<br>                             pad_token_id = 0,<br>           \
          \                  max_length=256,<br>                             early_stopping=True)<br>\
          \    output = tokenizer.decode(outputs[0])<br>    output = output[len(full_prompt):]<br>\
          \    if \"\" in output:<br>        cutoff = output.find(\"\")<br>      \
          \  output = output[:cutoff]<br>    return jsonify({'response': output})</p>\n\
          <p>if <strong>name</strong> == '<strong>main</strong>':<br>    app.run(host='0.0.0.0',\
          \ port=5000)</p>\n"
        raw: "Can anyone guide me, how to deploy this on Vertex AI.\n\nI am getting\
          \ error like: ERROR 2023-05-31T07:38:18.376283832Z [resource.labels.taskName:\
          \ workerpool0-0] File \"main.py\", line 8, in <module>\n  {\n    \"insertId\"\
          : \"1hjwmhtfm1rso4\",\n    \"jsonPayload\": {\n      \"message\": \"  File\
          \ \\\"main.py\\\", line 8, in <module>\\n\",\n      \"attrs\": {\n     \
          \   \"tag\": \"workerpool0-0\"\n      },\n      \"levelname\": \"ERROR\"\
          \n    },\n    \"resource\": {\n      \"type\": \"ml_job\",\n      \"labels\"\
          : {\n        \"job_id\": \"3056473000426602496\",\n        \"task_name\"\
          : \"workerpool0-0\",\n        \"project_id\": \"api-appexecutable-com\"\n\
          \      }\n    },\n    \"timestamp\": \"2023-05-31T07:38:18.376283832Z\"\
          ,\n    \"severity\": \"ERROR\",\n    \"labels\": {\n      \"ml.googleapis.com/tpu_worker_id\"\
          : \"\",\n      \"compute.googleapis.com/resource_name\": \"cmle-training-18349684525105594269\"\
          ,\n      \"ml.googleapis.com/trial_type\": \"\",\n      \"ml.googleapis.com/job_id/log_area\"\
          : \"root\",\n      \"ml.googleapis.com/trial_id\": \"\",\n      \"compute.googleapis.com/resource_id\"\
          : \"1504603656255391738\",\n      \"compute.googleapis.com/zone\": \"us-west1-b\"\
          \n    },\n    \"logName\": \"projects/api-appexecutable-com/logs/workerpool0-0\"\
          ,\n    \"receiveTimestamp\": \"2023-05-31T07:38:52.295831776Z\"\n  }\n\n\
          \nMy main.py is:\n\nfrom flask import Flask, request, jsonify\nimport torch\n\
          from transformers import AutoTokenizer, AutoModelForCausalLM\n\napp = Flask(__name__)\n\
          \ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/starchat-alpha\"\
          )\nmodel = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/starchat-alpha\"\
          ,\n                                              load_in_8bit=True,\n  \
          \                                            device_map='auto',\n      \
          \                                        torch_dtype=torch.float16)\n\n\
          @app.route('/generate', methods=['POST'])\ndef generate():\n    data = request.json\n\
          \    input_prompt = data['prompt']\n    system_prompt = \"\\nBelow is a\
          \ conversation between a human user and a helpful AI coding assistant.\\\
          n\"\n    user_prompt = f\"\\n{input_prompt}\\n\"\n    assistant_prompt =\
          \ \"\"\n    full_prompt = system_prompt + user_prompt + assistant_prompt\n\
          \    inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to('cuda')\n\
          \    outputs = model.generate(inputs,\n                             eos_token_id\
          \ = 0,\n                             pad_token_id = 0,\n               \
          \              max_length=256,\n                             early_stopping=True)\n\
          \    output = tokenizer.decode(outputs[0])\n    output = output[len(full_prompt):]\n\
          \    if \"\" in output:\n        cutoff = output.find(\"\")\n        output\
          \ = output[:cutoff]\n    return jsonify({'response': output})\n\nif __name__\
          \ == '__main__':\n    app.run(host='0.0.0.0', port=5000)"
        updatedAt: '2023-05-31T09:43:22.848Z'
      numEdits: 0
      reactions: []
    id: 647716ba40c99df876018b1a
    type: comment
  author: ravra
  content: "Can anyone guide me, how to deploy this on Vertex AI.\n\nI am getting\
    \ error like: ERROR 2023-05-31T07:38:18.376283832Z [resource.labels.taskName:\
    \ workerpool0-0] File \"main.py\", line 8, in <module>\n  {\n    \"insertId\"\
    : \"1hjwmhtfm1rso4\",\n    \"jsonPayload\": {\n      \"message\": \"  File \\\"\
    main.py\\\", line 8, in <module>\\n\",\n      \"attrs\": {\n        \"tag\": \"\
    workerpool0-0\"\n      },\n      \"levelname\": \"ERROR\"\n    },\n    \"resource\"\
    : {\n      \"type\": \"ml_job\",\n      \"labels\": {\n        \"job_id\": \"\
    3056473000426602496\",\n        \"task_name\": \"workerpool0-0\",\n        \"\
    project_id\": \"api-appexecutable-com\"\n      }\n    },\n    \"timestamp\": \"\
    2023-05-31T07:38:18.376283832Z\",\n    \"severity\": \"ERROR\",\n    \"labels\"\
    : {\n      \"ml.googleapis.com/tpu_worker_id\": \"\",\n      \"compute.googleapis.com/resource_name\"\
    : \"cmle-training-18349684525105594269\",\n      \"ml.googleapis.com/trial_type\"\
    : \"\",\n      \"ml.googleapis.com/job_id/log_area\": \"root\",\n      \"ml.googleapis.com/trial_id\"\
    : \"\",\n      \"compute.googleapis.com/resource_id\": \"1504603656255391738\"\
    ,\n      \"compute.googleapis.com/zone\": \"us-west1-b\"\n    },\n    \"logName\"\
    : \"projects/api-appexecutable-com/logs/workerpool0-0\",\n    \"receiveTimestamp\"\
    : \"2023-05-31T07:38:52.295831776Z\"\n  }\n\n\nMy main.py is:\n\nfrom flask import\
    \ Flask, request, jsonify\nimport torch\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\n\napp = Flask(__name__)\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
    HuggingFaceH4/starchat-alpha\")\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    HuggingFaceH4/starchat-alpha\",\n                                            \
    \  load_in_8bit=True,\n                                              device_map='auto',\n\
    \                                              torch_dtype=torch.float16)\n\n\
    @app.route('/generate', methods=['POST'])\ndef generate():\n    data = request.json\n\
    \    input_prompt = data['prompt']\n    system_prompt = \"\\nBelow is a conversation\
    \ between a human user and a helpful AI coding assistant.\\n\"\n    user_prompt\
    \ = f\"\\n{input_prompt}\\n\"\n    assistant_prompt = \"\"\n    full_prompt =\
    \ system_prompt + user_prompt + assistant_prompt\n    inputs = tokenizer.encode(full_prompt,\
    \ return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(inputs,\n  \
    \                           eos_token_id = 0,\n                             pad_token_id\
    \ = 0,\n                             max_length=256,\n                       \
    \      early_stopping=True)\n    output = tokenizer.decode(outputs[0])\n    output\
    \ = output[len(full_prompt):]\n    if \"\" in output:\n        cutoff = output.find(\"\
    \")\n        output = output[:cutoff]\n    return jsonify({'response': output})\n\
    \nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)"
  created_at: 2023-05-31 08:43:22+00:00
  edited: false
  hidden: false
  id: 647716ba40c99df876018b1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7Q4MjNeGHa9GaMiSNeTdE.jpeg?w=200&h=200&f=face
      fullname: Tim Pietrusky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NERDDISCO
      type: user
    createdAt: '2023-05-31T12:22:47.000Z'
    data:
      edited: false
      editors:
      - NERDDISCO
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/7Q4MjNeGHa9GaMiSNeTdE.jpeg?w=200&h=200&f=face
          fullname: Tim Pietrusky
          isHf: false
          isPro: false
          name: NERDDISCO
          type: user
        html: "<p>@ravineshraj I think you should open a new discussion, as we talked\
          \ in here about using the hosting possibilities of Hugging Face, not Vertex\
          \ AI \U0001F64F</p>\n"
        raw: "@ravineshraj I think you should open a new discussion, as we talked\
          \ in here about using the hosting possibilities of Hugging Face, not Vertex\
          \ AI \U0001F64F"
        updatedAt: '2023-05-31T12:22:47.442Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ravra
    id: 64773c1740c99df87604f158
    type: comment
  author: NERDDISCO
  content: "@ravineshraj I think you should open a new discussion, as we talked in\
    \ here about using the hosting possibilities of Hugging Face, not Vertex AI \U0001F64F"
  created_at: 2023-05-31 11:22:47+00:00
  edited: false
  hidden: false
  id: 64773c1740c99df87604f158
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: Has anybody tried the inference endpoints of this model
