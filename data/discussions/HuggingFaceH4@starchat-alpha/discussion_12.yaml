!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaZy3138
conflicting_files: null
created_at: 2023-05-29 04:55:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bfd933bfce71bb8847326e02aeb94aa.svg
      fullname: L Z
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaZy3138
      type: user
    createdAt: '2023-05-29T05:55:54.000Z'
    data:
      edited: false
      editors:
      - LaZy3138
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bfd933bfce71bb8847326e02aeb94aa.svg
          fullname: L Z
          isHf: false
          isPro: false
          name: LaZy3138
          type: user
        html: '<p>code:</p>

          <p>inputs = str.format("&lt;|system|&gt;\n&lt;|end|&gt;\n&lt;|user|&gt;%s&lt;|end|&gt;\n&lt;|assistant|&gt;"
          % (question))<br>logger.info(f"input is :\n{inputs}.")<br>pipe = pipeline("text-generation",
          model="/root_fs/home/tonyaw/machine_learning/nlp/huggingface.co/starchat-alpha")<br>outputs
          = pipe(inputs, max_new_tokens=8000)<br>logger.info(f"output=\n{outputs}")</p>

          <hr>

          <p>error:</p>

          <p>Unhandled Exception<br>Traceback (most recent call last):<br>&nbsp; File
          "./llm_test.py", line 126, in <br>&nbsp;&nbsp;&nbsp; starchat_test()<br>&nbsp;
          File "./llm_test.py", line 117, in starchat_test<br>&nbsp;&nbsp;&nbsp; outputs
          = pipe(inputs, max_new_tokens=8000)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_generation.py",
          line 209, in <strong>call</strong><br>&nbsp;&nbsp;&nbsp; return super().<strong>call</strong>(text_inputs,
          **kwargs)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py",
          line 1109, in <strong>call</strong><br>&nbsp;&nbsp;&nbsp; return self.run_single(inputs,
          preprocess_params, forward_params, postprocess_params)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py",
          line 1116, in run_single<br>&nbsp;&nbsp;&nbsp; model_outputs = self.forward(model_inputs,
          **forward_params)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py",
          line 1015, in forward<br>&nbsp;&nbsp;&nbsp; model_outputs = self._forward(model_inputs,
          **forward_params)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_generation.py",
          line 251, in _forward<br>&nbsp;&nbsp;&nbsp; generated_sequence = self.model.generate(input_ids=input_ids,
          attention_mask=attention_mask, **generate_kwargs)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py",
          line 115, in decorate_context<br>&nbsp;&nbsp;&nbsp; return func(*args, **kwargs)<br>&nbsp;
          File "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py",
          line 1437, in generate<br>&nbsp;&nbsp;&nbsp; return self.greedy_search(<br>&nbsp;
          File "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py",
          line 2248, in greedy_search<br>&nbsp;&nbsp;&nbsp; outputs = self(<br>&nbsp;
          File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>&nbsp;&nbsp;&nbsp; return forward_call(*args,
          **kwargs)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
          line 808, in forward<br>&nbsp;&nbsp;&nbsp; transformer_outputs = self.transformer(<br>&nbsp;
          File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py",
          line 1501, in _call_impl<br>&nbsp;&nbsp;&nbsp; return forward_call(*args,
          **kwargs)<br>&nbsp; File "/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
          line 605, in forward<br>&nbsp;&nbsp;&nbsp; self_attention_mask = self_attention_mask
          * attention_mask.view(batch_size, 1, -1).to(<br>RuntimeError: The size of
          tensor a (8192) must match the size of tensor b (8193) at non-singleton
          dimension 2</p>

          '
        raw: "code:\r\n\r\ninputs = str.format(\"<|system|>\\n<|end|>\\n<|user|>%s<|end|>\\\
          n<|assistant|>\" % (question))\r\nlogger.info(f\"input is :\\n{inputs}.\"\
          )\r\npipe = pipeline(\"text-generation\", model=\"/root_fs/home/tonyaw/machine_learning/nlp/huggingface.co/starchat-alpha\"\
          )\r\noutputs = pipe(inputs, max_new_tokens=8000) \r\nlogger.info(f\"output=\\\
          n{outputs}\")\r\n-------------------------\r\nerror:\r\n\r\nUnhandled Exception\r\
          \nTraceback (most recent call last):\r\n\_ File \"./llm_test.py\", line\
          \ 126, in <module>\r\n\_\_\_ starchat_test()\r\n\_ File \"./llm_test.py\"\
          , line 117, in starchat_test\r\n\_\_\_ outputs = pipe(inputs, max_new_tokens=8000)\r\
          \n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_generation.py\"\
          , line 209, in __call__\r\n\_\_\_ return super().__call__(text_inputs, **kwargs)\r\
          \n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
          , line 1109, in __call__\r\n\_\_\_ return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
          , line 1116, in run_single\r\n\_\_\_ model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
          , line 1015, in forward\r\n\_\_\_ model_outputs = self._forward(model_inputs,\
          \ **forward_params)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_generation.py\"\
          , line 251, in _forward\r\n\_\_\_ generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n\_\_\_ return func(*args, **kwargs)\r\
          \n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\"\
          , line 1437, in generate\r\n\_\_\_ return self.greedy_search(\r\n\_ File\
          \ \"/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\"\
          , line 2248, in greedy_search\r\n\_\_\_ outputs = self(\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n\_\_\_ return forward_call(*args, **kwargs)\r\
          \n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 808, in forward\r\n\_\_\_ transformer_outputs = self.transformer(\r\
          \n\_ File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"\
          , line 1501, in _call_impl\r\n\_\_\_ return forward_call(*args, **kwargs)\r\
          \n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
          , line 605, in forward\r\n\_\_\_ self_attention_mask = self_attention_mask\
          \ * attention_mask.view(batch_size, 1, -1).to(\r\nRuntimeError: The size\
          \ of tensor a (8192) must match the size of tensor b (8193) at non-singleton\
          \ dimension 2"
        updatedAt: '2023-05-29T05:55:54.756Z'
      numEdits: 0
      reactions: []
    id: 64743e6a71f07ae738d61403
    type: comment
  author: LaZy3138
  content: "code:\r\n\r\ninputs = str.format(\"<|system|>\\n<|end|>\\n<|user|>%s<|end|>\\\
    n<|assistant|>\" % (question))\r\nlogger.info(f\"input is :\\n{inputs}.\")\r\n\
    pipe = pipeline(\"text-generation\", model=\"/root_fs/home/tonyaw/machine_learning/nlp/huggingface.co/starchat-alpha\"\
    )\r\noutputs = pipe(inputs, max_new_tokens=8000) \r\nlogger.info(f\"output=\\\
    n{outputs}\")\r\n-------------------------\r\nerror:\r\n\r\nUnhandled Exception\r\
    \nTraceback (most recent call last):\r\n\_ File \"./llm_test.py\", line 126, in\
    \ <module>\r\n\_\_\_ starchat_test()\r\n\_ File \"./llm_test.py\", line 117, in\
    \ starchat_test\r\n\_\_\_ outputs = pipe(inputs, max_new_tokens=8000)\r\n\_ File\
    \ \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_generation.py\"\
    , line 209, in __call__\r\n\_\_\_ return super().__call__(text_inputs, **kwargs)\r\
    \n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
    , line 1109, in __call__\r\n\_\_\_ return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
    , line 1116, in run_single\r\n\_\_\_ model_outputs = self.forward(model_inputs,\
    \ **forward_params)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/base.py\"\
    , line 1015, in forward\r\n\_\_\_ model_outputs = self._forward(model_inputs,\
    \ **forward_params)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/pipelines/text_generation.py\"\
    , line 251, in _forward\r\n\_\_\_ generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=attention_mask, **generate_kwargs)\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n\_\_\_ return func(*args, **kwargs)\r\n\_ File\
    \ \"/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\"\
    , line 1437, in generate\r\n\_\_\_ return self.greedy_search(\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py\"\
    , line 2248, in greedy_search\r\n\_\_\_ outputs = self(\r\n\_ File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\"\
    , line 1501, in _call_impl\r\n\_\_\_ return forward_call(*args, **kwargs)\r\n\_\
    \ File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
    , line 808, in forward\r\n\_\_\_ transformer_outputs = self.transformer(\r\n\_\
    \ File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\",\
    \ line 1501, in _call_impl\r\n\_\_\_ return forward_call(*args, **kwargs)\r\n\_\
    \ File \"/usr/local/lib/python3.8/dist-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py\"\
    , line 605, in forward\r\n\_\_\_ self_attention_mask = self_attention_mask * attention_mask.view(batch_size,\
    \ 1, -1).to(\r\nRuntimeError: The size of tensor a (8192) must match the size\
    \ of tensor b (8193) at non-singleton dimension 2"
  created_at: 2023-05-29 04:55:54+00:00
  edited: false
  hidden: false
  id: 64743e6a71f07ae738d61403
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: HuggingFaceH4/starchat-alpha
repo_type: model
status: open
target_branch: null
title: max new tokens error despite setting lower size, The size of tensor a (8192)
  must match the size of tensor b (8193) at non-singleton dimension 2
