!!python/object:huggingface_hub.community.DiscussionWithDetails
author: matheusdias
conflicting_files: null
created_at: 2023-02-24 20:38:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e01ccf5ff3ac2c878ea20d235c206d8.svg
      fullname: Matheus Dias
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matheusdias
      type: user
    createdAt: '2023-02-24T20:38:47.000Z'
    data:
      edited: false
      editors:
      - matheusdias
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e01ccf5ff3ac2c878ea20d235c206d8.svg
          fullname: Matheus Dias
          isHf: false
          isPro: false
          name: matheusdias
          type: user
        html: '<p>Hi! I am unable to use BLIP 2.0 through the transformers library
          - I think the library needs to be updates still to have the model. Is there
          anyway to get around this?</p>

          '
        raw: Hi! I am unable to use BLIP 2.0 through the transformers library - I
          think the library needs to be updates still to have the model. Is there
          anyway to get around this?
        updatedAt: '2023-02-24T20:38:47.077Z'
      numEdits: 0
      reactions: []
    id: 63f92057ff971cb331577e37
    type: comment
  author: matheusdias
  content: Hi! I am unable to use BLIP 2.0 through the transformers library - I think
    the library needs to be updates still to have the model. Is there anyway to get
    around this?
  created_at: 2023-02-24 20:38:47+00:00
  edited: false
  hidden: false
  id: 63f92057ff971cb331577e37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-24T20:56:34.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Yes as the model is brand new you need to install Transformers from
          source for the moment:</p>

          <pre><code>pip install git+https://github.com/huggingface/transformers.git

          </code></pre>

          '
        raw: 'Yes as the model is brand new you need to install Transformers from
          source for the moment:

          ```

          pip install git+https://github.com/huggingface/transformers.git

          ```'
        updatedAt: '2023-02-24T20:56:34.479Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - matheusdias
    id: 63f92482ce1f61ce15132fe4
    type: comment
  author: nielsr
  content: 'Yes as the model is brand new you need to install Transformers from source
    for the moment:

    ```

    pip install git+https://github.com/huggingface/transformers.git

    ```'
  created_at: 2023-02-24 20:56:34+00:00
  edited: false
  hidden: false
  id: 63f92482ce1f61ce15132fe4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c6a0bbb952f3503fa3f76c0c85b175b.svg
      fullname: Tim-Cedric
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: timcedric
      type: user
    createdAt: '2023-02-26T11:33:57.000Z'
    data:
      edited: false
      editors:
      - timcedric
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c6a0bbb952f3503fa3f76c0c85b175b.svg
          fullname: Tim-Cedric
          isHf: false
          isPro: false
          name: timcedric
          type: user
        html: '<p>I need it through the library unfortunately, how long does it usually
          take to get it into the library as well?</p>

          '
        raw: I need it through the library unfortunately, how long does it usually
          take to get it into the library as well?
        updatedAt: '2023-02-26T11:33:57.779Z'
      numEdits: 0
      reactions: []
    id: 63fb43a5e7c66d1eab101ef0
    type: comment
  author: timcedric
  content: I need it through the library unfortunately, how long does it usually take
    to get it into the library as well?
  created_at: 2023-02-26 11:33:57+00:00
  edited: false
  hidden: false
  id: 63fb43a5e7c66d1eab101ef0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-02-27T12:43:48.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>A new Transformers version will be released in March (typically
          a release is done every month).</p>

          '
        raw: A new Transformers version will be released in March (typically a release
          is done every month).
        updatedAt: '2023-02-27T12:43:48.081Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - timcedric
    id: 63fca584abd98a1e107e1ac6
    type: comment
  author: nielsr
  content: A new Transformers version will be released in March (typically a release
    is done every month).
  created_at: 2023-02-27 12:43:48+00:00
  edited: false
  hidden: false
  id: 63fca584abd98a1e107e1ac6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e01ccf5ff3ac2c878ea20d235c206d8.svg
      fullname: Matheus Dias
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: matheusdias
      type: user
    createdAt: '2023-02-28T05:52:15.000Z'
    data:
      edited: true
      editors:
      - matheusdias
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e01ccf5ff3ac2c878ea20d235c206d8.svg
          fullname: Matheus Dias
          isHf: false
          isPro: false
          name: matheusdias
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;nielsr&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/nielsr\">@<span class=\"\
          underline\">nielsr</span></a></span>\n\n\t</span></span> I am having a hard\
          \ time downloading the model to colab because its so large and consumes\
          \ all my RAM. Is this expected (and I should a cluster with more RAM) or\
          \ is there some way around that? I had no problems with the other caption\
          \ generation models.</p>\n"
        raw: '@nielsr I am having a hard time downloading the model to colab because
          its so large and consumes all my RAM. Is this expected (and I should a cluster
          with more RAM) or is there some way around that? I had no problems with
          the other caption generation models.'
        updatedAt: '2023-02-28T06:43:38.876Z'
      numEdits: 1
      reactions: []
    id: 63fd968f9ac81ec88b0b6d9d
    type: comment
  author: matheusdias
  content: '@nielsr I am having a hard time downloading the model to colab because
    its so large and consumes all my RAM. Is this expected (and I should a cluster
    with more RAM) or is there some way around that? I had no problems with the other
    caption generation models.'
  created_at: 2023-02-28 05:52:15+00:00
  edited: true
  hidden: false
  id: 63fd968f9ac81ec88b0b6d9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ce6567edadfd86017017904a7e07765.svg
      fullname: Niu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neumann21
      type: user
    createdAt: '2023-04-11T03:28:38.000Z'
    data:
      edited: false
      editors:
      - Neumann21
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ce6567edadfd86017017904a7e07765.svg
          fullname: Niu
          isHf: false
          isPro: false
          name: Neumann21
          type: user
        html: "<p>I met an error when I run the model on GPU In full precision:<br>(base)\
          \ wangson: CUDA_VISIBLE_DEVICES=0 python test.blip.py<br>Loading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06&lt;00:00,  3.25s/it]<br>/home/anaconda/lib/python3.10/site-packages/transformers/generation/utils.py:1288:\
          \ UserWarning: Using <code>max_length</code>'s default (20) to control the\
          \ generation length. This behaviour is deprecated and will be removed from\
          \ the config in v5 of Transformers -- we recommend using <code>max_new_tokens</code>\
          \ to control the maximum length of the generation.</p>\n<p>How did this\
          \ happen? Is this due to some packages' version diff?</p>\n"
        raw: "I met an error when I run the model on GPU In full precision:\n(base)\
          \ wangson: CUDA_VISIBLE_DEVICES=0 python test.blip.py \nLoading checkpoint\
          \ shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00,  3.25s/it]\n\
          /home/anaconda/lib/python3.10/site-packages/transformers/generation/utils.py:1288:\
          \ UserWarning: Using `max_length`'s default (20) to control the generation\
          \ length. This behaviour is deprecated and will be removed from the config\
          \ in v5 of Transformers -- we recommend using `max_new_tokens` to control\
          \ the maximum length of the generation.\n\nHow did this happen? Is this\
          \ due to some packages' version diff?"
        updatedAt: '2023-04-11T03:28:38.435Z'
      numEdits: 0
      reactions: []
    id: 6434d3e6938d07505bbbbcda
    type: comment
  author: Neumann21
  content: "I met an error when I run the model on GPU In full precision:\n(base)\
    \ wangson: CUDA_VISIBLE_DEVICES=0 python test.blip.py \nLoading checkpoint shards:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 2/2 [00:06<00:00,  3.25s/it]\n/home/anaconda/lib/python3.10/site-packages/transformers/generation/utils.py:1288:\
    \ UserWarning: Using `max_length`'s default (20) to control the generation length.\
    \ This behaviour is deprecated and will be removed from the config in v5 of Transformers\
    \ -- we recommend using `max_new_tokens` to control the maximum length of the\
    \ generation.\n\nHow did this happen? Is this due to some packages' version diff?"
  created_at: 2023-04-11 02:28:38+00:00
  edited: false
  hidden: false
  id: 6434d3e6938d07505bbbbcda
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Salesforce/blip2-opt-2.7b
repo_type: model
status: open
target_branch: null
title: How to use BLIP 2.0
