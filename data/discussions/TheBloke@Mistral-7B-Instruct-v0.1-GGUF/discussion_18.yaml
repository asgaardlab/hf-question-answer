!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArunRaj000
conflicting_files: null
created_at: 2023-12-18 17:08:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
      fullname: Arun Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArunRaj000
      type: user
    createdAt: '2023-12-18T17:08:38.000Z'
    data:
      edited: false
      editors:
      - ArunRaj000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8676530122756958
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
          fullname: Arun Raj
          isHf: false
          isPro: false
          name: ArunRaj000
          type: user
        html: '<p>hi , i just succesfully run, mistral 7b instruct gguf file using
          ctransformer, but the ai is not responding according to user input</p>

          <p>how to solve this</p>

          '
        raw: "hi , i just succesfully run, mistral 7b instruct gguf file using ctransformer,\
          \ but the ai is not responding according to user input\r\n \r\n\r\nhow to\
          \ solve this"
        updatedAt: '2023-12-18T17:08:38.774Z'
      numEdits: 0
      reactions: []
    id: 65807c96da07e791d8a8d8f0
    type: comment
  author: ArunRaj000
  content: "hi , i just succesfully run, mistral 7b instruct gguf file using ctransformer,\
    \ but the ai is not responding according to user input\r\n \r\n\r\nhow to solve\
    \ this"
  created_at: 2023-12-18 17:08:38+00:00
  edited: false
  hidden: false
  id: 65807c96da07e791d8a8d8f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2023-12-26T11:38:25.000Z'
    data:
      edited: false
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.989277184009552
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>Can you share how did you ran it in VS CODE?</p>

          '
        raw: 'Can you share how did you ran it in VS CODE?

          '
        updatedAt: '2023-12-26T11:38:25.756Z'
      numEdits: 0
      reactions: []
    id: 658abb3165df457a55083739
    type: comment
  author: ShivanshMathur007
  content: 'Can you share how did you ran it in VS CODE?

    '
  created_at: 2023-12-26 11:38:25+00:00
  edited: false
  hidden: false
  id: 658abb3165df457a55083739
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
      fullname: Arun Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArunRaj000
      type: user
    createdAt: '2023-12-28T08:50:38.000Z'
    data:
      edited: false
      editors:
      - ArunRaj000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9064828157424927
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
          fullname: Arun Raj
          isHf: false
          isPro: false
          name: ArunRaj000
          type: user
        html: '<p>downloaded the gguf files, and used ctrasnformer to run these code,
          which is really computational effective, but it takes more time print the
          output<br> instead of real time printing </p>

          '
        raw: "downloaded the gguf files, and used ctrasnformer to run these code,\
          \ which is really computational effective, but it takes more time print\
          \ the output\n instead of real time printing "
        updatedAt: '2023-12-28T08:50:38.190Z'
      numEdits: 0
      reactions: []
    id: 658d36de4a815851ad847476
    type: comment
  author: ArunRaj000
  content: "downloaded the gguf files, and used ctrasnformer to run these code, which\
    \ is really computational effective, but it takes more time print the output\n\
    \ instead of real time printing "
  created_at: 2023-12-28 08:50:38+00:00
  edited: false
  hidden: false
  id: 658d36de4a815851ad847476
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2023-12-28T20:26:08.000Z'
    data:
      edited: false
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6038606762886047
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>Even I tried ctransfromers but it is giving segmentation fault,
          afterwards I tried with llama-cpp-python it worked. Can you share how you
          did.<br>from ctransformers import AutoModelForCausalLM<br>import gradio
          as gr</p>

          <p>llm = AutoModelForCausalLM.from_pretrained(<br>        "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",<br>        model_file="mistral-7b-instruct-v0.2.Q4_K_M.gguf",<br>        model_type="llama",<br>        gpu_layers=0
          )</p>

          <p>title= "Shivansh Model"</p>

          <p>def llm_func(message,history):<br>    response=llm(message)<br>    return
          response</p>

          <p>gr.ChatInterface(<br>fn=llm_func,<br>title=title,<br>).launch()<br>This
          gave segmentation fault.</p>

          <p>On the other hand:-&gt;<br>from langchain.llms import LlamaCpp<br>import
          gradio as gr</p>

          <p>def load_llm():<br>    llm = LlamaCpp(<br>        model_path="../model/mistral-7b-instruct-v0.2.Q4_K_M.gguf",<br>        max_new_tokens=512,<br>        temperature=0.1<br>    )<br>    return
          llm<br>title= "Shivansh Model"</p>

          <p>def llm_func(message,history):<br>    llm=load_llm()<br>    response=llm(message)<br>    return
          response</p>

          <p>gr.ChatInterface(<br>fn=llm_func,<br>title=title,<br>).launch()<br>This
          is working good.</p>

          <p>Could you share your code.</p>

          '
        raw: "Even I tried ctransfromers but it is giving segmentation fault, afterwards\
          \ I tried with llama-cpp-python it worked. Can you share how you did.\n\
          from ctransformers import AutoModelForCausalLM\nimport gradio as gr\n\n\n\
          llm = AutoModelForCausalLM.from_pretrained(\n        \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\
          ,\n        model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n      \
          \  model_type=\"llama\",\n        gpu_layers=0 )\n    \ntitle= \"Shivansh\
          \ Model\"\n\ndef llm_func(message,history):\n    response=llm(message)\n\
          \    return response\n\ngr.ChatInterface(\nfn=llm_func,\ntitle=title,\n\
          ).launch()\nThis gave segmentation fault.\n\nOn the other hand:->\nfrom\
          \ langchain.llms import LlamaCpp\nimport gradio as gr\n\n\ndef load_llm():\n\
          \    llm = LlamaCpp(\n        model_path=\"../model/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\
          ,\n        max_new_tokens=512,\n        temperature=0.1\n    )\n    return\
          \ llm\ntitle= \"Shivansh Model\"\n\ndef llm_func(message,history):\n   \
          \ llm=load_llm()\n    response=llm(message)\n    return response\n\ngr.ChatInterface(\n\
          fn=llm_func,\ntitle=title,\n).launch()\nThis is working good.\n\nCould you\
          \ share your code.\n"
        updatedAt: '2023-12-28T20:26:08.628Z'
      numEdits: 0
      reactions: []
    id: 658dd9e0f5a209eeaca023ce
    type: comment
  author: ShivanshMathur007
  content: "Even I tried ctransfromers but it is giving segmentation fault, afterwards\
    \ I tried with llama-cpp-python it worked. Can you share how you did.\nfrom ctransformers\
    \ import AutoModelForCausalLM\nimport gradio as gr\n\n\nllm = AutoModelForCausalLM.from_pretrained(\n\
    \        \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n        model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\
    ,\n        model_type=\"llama\",\n        gpu_layers=0 )\n    \ntitle= \"Shivansh\
    \ Model\"\n\ndef llm_func(message,history):\n    response=llm(message)\n    return\
    \ response\n\ngr.ChatInterface(\nfn=llm_func,\ntitle=title,\n).launch()\nThis\
    \ gave segmentation fault.\n\nOn the other hand:->\nfrom langchain.llms import\
    \ LlamaCpp\nimport gradio as gr\n\n\ndef load_llm():\n    llm = LlamaCpp(\n  \
    \      model_path=\"../model/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n       \
    \ max_new_tokens=512,\n        temperature=0.1\n    )\n    return llm\ntitle=\
    \ \"Shivansh Model\"\n\ndef llm_func(message,history):\n    llm=load_llm()\n \
    \   response=llm(message)\n    return response\n\ngr.ChatInterface(\nfn=llm_func,\n\
    title=title,\n).launch()\nThis is working good.\n\nCould you share your code.\n"
  created_at: 2023-12-28 20:26:08+00:00
  edited: false
  hidden: false
  id: 658dd9e0f5a209eeaca023ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
      fullname: Arun Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArunRaj000
      type: user
    createdAt: '2023-12-29T19:24:12.000Z'
    data:
      edited: false
      editors:
      - ArunRaj000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4877402186393738
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
          fullname: Arun Raj
          isHf: false
          isPro: false
          name: ArunRaj000
          type: user
        html: "<p>from langchain.llms import CTransformers<br>from langchain.callbacks.streaming_stdout\
          \ import StreamingStdOutCallbackHandler<br>import time<br>import json</p>\n\
          <h1 id=\"load-model-configuration-from-the-specified-path\">Load model configuration\
          \ from the specified path</h1>\n<p>config_path = \"D:/Project File/restart/NEW\
          \ AI/config.json\"</p>\n<p>with open(config_path, 'r') as config_file:<br>\
          \    model_config = json.load(config_file)</p>\n<h1 id=\"extract-specific-parameters\"\
          >Extract specific parameters</h1>\n<p>load_params = model_config.get('load_params',\
          \ {})</p>\n<h1 id=\"use-the-extracted-parameters-in-your-main-code\">Use\
          \ the extracted parameters in your main code</h1>\n<p>model_path = \"D:/Project\
          \ File/restart/NEW AI/mistral-7b-instruct-v0.1.Q2_K.gguf\"</p>\n<h1 id=\"\
          initialize-langchains-ctransformers-with-streamingstdoutcallbackhandler\"\
          >Initialize LangChain's CTransformers with StreamingStdOutCallbackHandler</h1>\n\
          <p>llm = CTransformers(<br>    model=model_path,<br>    callbacks=[StreamingStdOutCallbackHandler()]<br>)</p>\n\
          <h1 id=\"initialize-conversation-history\">Initialize conversation history</h1>\n\
          <p>conversation_history = []</p>\n<p>prompt_template = \"[INST] {prompt}\
          \ [/INST]\"</p>\n<p>while True:<br>    prompt_template = {<br>        \"\
          pre_prompt\": \"You are an artificial intelligence called VICTOR, Victor\
          \ stands for Virtual Intelligent Companion for Technological Optimization\
          \ and Reinforcement, created by Arun Raj, a Physics student.you are a friend\
          \ of the user, you aim to keep our conversation very concise and engaging\
          \ you, \",<br>        \"pre_prompt_suffix\": \"\",<br>        \"pre_prompt_prefix\"\
          : \"\",<br>        \"input_prefix\": \"[INST]\",<br>        \"input_suffix\"\
          : \"[/INST]\",<br>        \"antiprompt\": [\"[INST]\"],<br>    }<br>   \
          \ user_input = input(\"You: \")</p>\n<pre><code>if user_input.lower() ==\
          \ \"quit\":\n    break\n\nformatted_input = f\"{prompt_template['input_prefix']}{user_input}{prompt_template['input_suffix']}\"\
          \n\nprint(\"\\nYou:\", user_input)\n\nresponse = llm(prompt_template['pre_prompt']\
          \ + formatted_input)\n\nif conversation_history and response != conversation_history[-1][1]:\
          \  # Check if conversation history is not empty\n    print()\n\nconversation_history.append((\"\
          User\", user_input))\nconversation_history.append((\"AI\", response))\n\
          </code></pre>\n<p>print(\"Chatbot session ended.\")</p>\n<p>in this code\
          \ i am still facing memory issue, </p>\n<p>can you help me with memory</p>\n"
        raw: "from langchain.llms import CTransformers\nfrom langchain.callbacks.streaming_stdout\
          \ import StreamingStdOutCallbackHandler\nimport time\nimport json\n\n\n\
          # Load model configuration from the specified path\nconfig_path = \"D:/Project\
          \ File/restart/NEW AI/config.json\"\n\nwith open(config_path, 'r') as config_file:\n\
          \    model_config = json.load(config_file)\n\n# Extract specific parameters\n\
          load_params = model_config.get('load_params', {})\n\n# Use the extracted\
          \ parameters in your main code\nmodel_path = \"D:/Project File/restart/NEW\
          \ AI/mistral-7b-instruct-v0.1.Q2_K.gguf\"\n\n\n# Initialize LangChain's\
          \ CTransformers with StreamingStdOutCallbackHandler\nllm = CTransformers(\n\
          \    model=model_path,\n    callbacks=[StreamingStdOutCallbackHandler()]\n\
          )\n\n# Initialize conversation history\nconversation_history = []\n\nprompt_template\
          \ = \"[INST] {prompt} [/INST]\"\n\nwhile True:\n    prompt_template = {\n\
          \        \"pre_prompt\": \"You are an artificial intelligence called VICTOR,\
          \ Victor stands for Virtual Intelligent Companion for Technological Optimization\
          \ and Reinforcement, created by Arun Raj, a Physics student.you are a friend\
          \ of the user, you aim to keep our conversation very concise and engaging\
          \ you, \",\n        \"pre_prompt_suffix\": \"\",\n        \"pre_prompt_prefix\"\
          : \"\",\n        \"input_prefix\": \"[INST]\",\n        \"input_suffix\"\
          : \"[/INST]\",\n        \"antiprompt\": [\"[INST]\"],        \n    }\n \
          \   user_input = input(\"You: \")\n\n    if user_input.lower() == \"quit\"\
          :\n        break\n\n    formatted_input = f\"{prompt_template['input_prefix']}{user_input}{prompt_template['input_suffix']}\"\
          \n    \n    print(\"\\nYou:\", user_input)\n    \n    response = llm(prompt_template['pre_prompt']\
          \ + formatted_input)\n    \n    if conversation_history and response !=\
          \ conversation_history[-1][1]:  # Check if conversation history is not empty\n\
          \        print()\n\n    conversation_history.append((\"User\", user_input))\n\
          \    conversation_history.append((\"AI\", response))\n\nprint(\"Chatbot\
          \ session ended.\")\n\n\n\nin this code i am still facing memory issue,\
          \ \n\ncan you help me with memory"
        updatedAt: '2023-12-29T19:24:12.126Z'
      numEdits: 0
      reactions: []
    id: 658f1cdca02954c982f5bb76
    type: comment
  author: ArunRaj000
  content: "from langchain.llms import CTransformers\nfrom langchain.callbacks.streaming_stdout\
    \ import StreamingStdOutCallbackHandler\nimport time\nimport json\n\n\n# Load\
    \ model configuration from the specified path\nconfig_path = \"D:/Project File/restart/NEW\
    \ AI/config.json\"\n\nwith open(config_path, 'r') as config_file:\n    model_config\
    \ = json.load(config_file)\n\n# Extract specific parameters\nload_params = model_config.get('load_params',\
    \ {})\n\n# Use the extracted parameters in your main code\nmodel_path = \"D:/Project\
    \ File/restart/NEW AI/mistral-7b-instruct-v0.1.Q2_K.gguf\"\n\n\n# Initialize LangChain's\
    \ CTransformers with StreamingStdOutCallbackHandler\nllm = CTransformers(\n  \
    \  model=model_path,\n    callbacks=[StreamingStdOutCallbackHandler()]\n)\n\n\
    # Initialize conversation history\nconversation_history = []\n\nprompt_template\
    \ = \"[INST] {prompt} [/INST]\"\n\nwhile True:\n    prompt_template = {\n    \
    \    \"pre_prompt\": \"You are an artificial intelligence called VICTOR, Victor\
    \ stands for Virtual Intelligent Companion for Technological Optimization and\
    \ Reinforcement, created by Arun Raj, a Physics student.you are a friend of the\
    \ user, you aim to keep our conversation very concise and engaging you, \",\n\
    \        \"pre_prompt_suffix\": \"\",\n        \"pre_prompt_prefix\": \"\",\n\
    \        \"input_prefix\": \"[INST]\",\n        \"input_suffix\": \"[/INST]\"\
    ,\n        \"antiprompt\": [\"[INST]\"],        \n    }\n    user_input = input(\"\
    You: \")\n\n    if user_input.lower() == \"quit\":\n        break\n\n    formatted_input\
    \ = f\"{prompt_template['input_prefix']}{user_input}{prompt_template['input_suffix']}\"\
    \n    \n    print(\"\\nYou:\", user_input)\n    \n    response = llm(prompt_template['pre_prompt']\
    \ + formatted_input)\n    \n    if conversation_history and response != conversation_history[-1][1]:\
    \  # Check if conversation history is not empty\n        print()\n\n    conversation_history.append((\"\
    User\", user_input))\n    conversation_history.append((\"AI\", response))\n\n\
    print(\"Chatbot session ended.\")\n\n\n\nin this code i am still facing memory\
    \ issue, \n\ncan you help me with memory"
  created_at: 2023-12-29 19:24:12+00:00
  edited: false
  hidden: false
  id: 658f1cdca02954c982f5bb76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2023-12-29T20:09:39.000Z'
    data:
      edited: false
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9685113430023193
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>Can you share your hardware details, so I can help you.</p>

          '
        raw: 'Can you share your hardware details, so I can help you.

          '
        updatedAt: '2023-12-29T20:09:39.267Z'
      numEdits: 0
      reactions: []
    id: 658f2783075245eadd7e2157
    type: comment
  author: ShivanshMathur007
  content: 'Can you share your hardware details, so I can help you.

    '
  created_at: 2023-12-29 20:09:39+00:00
  edited: false
  hidden: false
  id: 658f2783075245eadd7e2157
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
      fullname: Arun Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArunRaj000
      type: user
    createdAt: '2023-12-29T20:28:27.000Z'
    data:
      edited: false
      editors:
      - ArunRaj000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7553663849830627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
          fullname: Arun Raj
          isHf: false
          isPro: false
          name: ArunRaj000
          type: user
        html: '<p>ryzen 5 5600h, nvdia gtx 1650and amd radieon gpu,  24gb ram</p>

          <p>i mean conversational memory, the model doesn''t remember previous interaction,
          </p>

          '
        raw: 'ryzen 5 5600h, nvdia gtx 1650and amd radieon gpu,  24gb ram


          i mean conversational memory, the model doesn''t remember previous interaction, '
        updatedAt: '2023-12-29T20:28:27.679Z'
      numEdits: 0
      reactions: []
    id: 658f2beba41c3cbad555b6b0
    type: comment
  author: ArunRaj000
  content: 'ryzen 5 5600h, nvdia gtx 1650and amd radieon gpu,  24gb ram


    i mean conversational memory, the model doesn''t remember previous interaction, '
  created_at: 2023-12-29 20:28:27+00:00
  edited: false
  hidden: false
  id: 658f2beba41c3cbad555b6b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2024-01-01T20:35:22.000Z'
    data:
      edited: false
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8344036936759949
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>you can try out langchain.memory for conversational/contextual memory.</p>

          '
        raw: 'you can try out langchain.memory for conversational/contextual memory.

          '
        updatedAt: '2024-01-01T20:35:22.234Z'
      numEdits: 0
      reactions: []
    id: 6593220ac0b1372b2e2bac1a
    type: comment
  author: ShivanshMathur007
  content: 'you can try out langchain.memory for conversational/contextual memory.

    '
  created_at: 2024-01-01 20:35:22+00:00
  edited: false
  hidden: false
  id: 6593220ac0b1372b2e2bac1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
      fullname: Arun Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArunRaj000
      type: user
    createdAt: '2024-01-02T12:17:34.000Z'
    data:
      edited: false
      editors:
      - ArunRaj000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.930145263671875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
          fullname: Arun Raj
          isHf: false
          isPro: false
          name: ArunRaj000
          type: user
        html: '<p>yeah i tried that now the context window is 8k but don''t have long
          term memory, so planning to integrate a database. </p>

          <p>do you know any open source text to speech library for speaking in live
          stream output</p>

          '
        raw: "yeah i tried that now the context window is 8k but don't have long term\
          \ memory, so planning to integrate a database. \n\ndo you know any open\
          \ source text to speech library for speaking in live stream output\n"
        updatedAt: '2024-01-02T12:17:34.676Z'
      numEdits: 0
      reactions: []
    id: 6593fede0c99312905b033b7
    type: comment
  author: ArunRaj000
  content: "yeah i tried that now the context window is 8k but don't have long term\
    \ memory, so planning to integrate a database. \n\ndo you know any open source\
    \ text to speech library for speaking in live stream output\n"
  created_at: 2024-01-02 12:17:34+00:00
  edited: false
  hidden: false
  id: 6593fede0c99312905b033b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
      fullname: Shivansh Mathur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShivanshMathur007
      type: user
    createdAt: '2024-01-02T19:31:16.000Z'
    data:
      edited: false
      editors:
      - ShivanshMathur007
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9882309436798096
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a0fb779052d1e8beb7cbe553f1fd6b8.svg
          fullname: Shivansh Mathur
          isHf: false
          isPro: false
          name: ShivanshMathur007
          type: user
        html: '<p>no I have not expored that area yet</p>

          '
        raw: 'no I have not expored that area yet


          '
        updatedAt: '2024-01-02T19:31:16.758Z'
      numEdits: 0
      reactions: []
    id: 65946484a6567cb93c0308db
    type: comment
  author: ShivanshMathur007
  content: 'no I have not expored that area yet


    '
  created_at: 2024-01-02 19:31:16+00:00
  edited: false
  hidden: false
  id: 65946484a6567cb93c0308db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
      fullname: Arun Raj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArunRaj000
      type: user
    createdAt: '2024-01-03T09:47:11.000Z'
    data:
      edited: false
      editors:
      - ArunRaj000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7372666001319885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/250462b72b5734bf1dc98c956486daee.svg
          fullname: Arun Raj
          isHf: false
          isPro: false
          name: ArunRaj000
          type: user
        html: '<p>It''s ok... Can you tell me how you used llama. Cpp?<br>I have some
          errors while installing llama. Cpp... If you know..., then please text me
          in insta -Arun_luka</p>

          '
        raw: "It's ok... Can you tell me how you used llama. Cpp? \nI have some errors\
          \ while installing llama. Cpp... If you know..., then please text me in\
          \ insta -Arun_luka"
        updatedAt: '2024-01-03T09:47:11.269Z'
      numEdits: 0
      reactions: []
    id: 65952d1f5b3f818cc6e66fb9
    type: comment
  author: ArunRaj000
  content: "It's ok... Can you tell me how you used llama. Cpp? \nI have some errors\
    \ while installing llama. Cpp... If you know..., then please text me in insta\
    \ -Arun_luka"
  created_at: 2024-01-03 09:47:11+00:00
  edited: false
  hidden: false
  id: 65952d1f5b3f818cc6e66fb9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: run in vs code
