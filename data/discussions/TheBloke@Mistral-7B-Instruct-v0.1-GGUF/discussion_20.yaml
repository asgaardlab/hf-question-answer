!!python/object:huggingface_hub.community.DiscussionWithDetails
author: icemaro
conflicting_files: null
created_at: 2024-01-24 09:30:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1bb5641f6c9f41bdb204f6aa2b7de630.svg
      fullname: Omar EL HACHIMI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: icemaro
      type: user
    createdAt: '2024-01-24T09:30:34.000Z'
    data:
      edited: false
      editors:
      - icemaro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6184836626052856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1bb5641f6c9f41bdb204f6aa2b7de630.svg
          fullname: Omar EL HACHIMI
          isHf: false
          isPro: false
          name: icemaro
          type: user
        html: '<p>I''m encountering issues while trying to run an Ollama container
          behind a proxy. Here are the steps I''ve taken and the issues I''ve faced:</p>

          <ol>

          <li><p><strong>Creating an Image with Certificate</strong>:</p>

          <pre><code>cat Dockerfile

          FROM ollama/ollama

          COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt

          RUN update-ca-certificates

          </code></pre>

          </li>

          <li><p><strong>Starting a Container Using This Image with Proxy Variables
          Injected</strong>:</p>

          <pre><code>docker run -d \

          -e HTTPS_PROXY=http://x.x.x.x:3128 \

          -e HTTP_PROXY=http://x.x.x.x:3128 \

          -e http_proxy=http://x.x.x.x:3128 \

          -e https_proxy=http://x.x.x.x:3128 \

          -p 11434:11434 ollama-with-ca

          </code></pre>

          </li>

          <li><p><strong>Inside the Container</strong>:</p>

          <ul>

          <li>Ran <code>apt-get update</code> to confirm internet access and proper
          proxy functionality.</li>

          <li>Executed <code>ollama pull mistral</code> and <code>ollama run mistral:instruct</code>,
          but consistently encountered the error: "Error: something went wrong, please
          see the Ollama server logs for details."</li>

          <li>Container logs (<code>docker logs 8405972b3d6b</code>) showed no errors,
          only the following information:<pre><code>Couldn''t find ''/root/.ollama/id_ed25519''.
          Generating new private key.

          Your new public key is: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDppYjymfVcdtDNT/umLfrzlIx1QquQ/gTuSI7SAV194

          2024/01/24 08:40:55 images.go:808: total blobs: 0

          2024/01/24 08:40:55 images.go:815: total unused blobs removed: 0

          2024/01/24 08:40:55 routes.go:930: Listening on [::]:11434 (version 0.1.20)

          2024/01/24 08:40:56 shim_ext_server.go:142: Dynamic LLM variants [cuda]

          2024/01/24 08:40:56 gpu.go:88: Detecting GPU type

          2024/01/24 08:40:56 gpu.go:203: Searching for GPU management library libnvidia-ml.so

          2024/01/24 08:40:56 gpu.go:248: Discovered GPU libraries: []

          2024/01/24 08:40:56 gpu.go:203: Searching for GPU management library librocm_smi64.so

          2024/01/24 08:40:56 gpu.go:248: Discovered GPU libraries: []

          2024/01/24 08:40:56 routes.go:953: no GPU detected

          </code></pre>

          </li>

          </ul>

          </li>

          <li><p><strong>Using Wget to Download the Model</strong>:</p>

          <ul>

          <li><p>Successfully downloaded "mistral-7b-instruct-v0.1.Q5_K_M.gguf" via
          <code>wget</code>.</p>

          </li>

          <li><p>Created a simple ModelFile:</p>

          <pre><code>FROM /home/mistral-7b-instruct-v0.1.Q5_K_M.gguf

          </code></pre>

          </li>

          <li><p>Executed <code>ollama create mistralModel -f Modelfile</code>, resulting
          in the same error: "Error: something went wrong, please see the Ollama server
          logs for details."</p>

          </li>

          <li><p>The logs from <code>docker logs 8405972b3d6b</code> again showed
          no error:<br>```<br>Couldn''t find ''/root/.ollama/id_ed25519''. Generating
          new private key.<br>Your new public key is:</p>

          <p>ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDppYjymfVcdtDNT/umLfrzlIx1QquQ/gTuSI7SAV194</p>

          <p>2024/01/24 08:40:55 images.go:808: total blobs: 0<br>2024/01/24 08:40:55
          images.go:815: total unused blobs removed: 0<br>2024/01/24 08:40:55 routes.go:930:
          Listening on [::]:11434 (version 0.1.20)<br>2024/01/24 08:40:56 shim_ext_server.go:142:
          Dynamic LLM variants [cuda]<br>2024/01/24 08:40:56 gpu.go:88: Detecting
          GPU type</p>

          </li>

          </ul>

          </li>

          </ol>

          <p>When Making a http request on the ollama server in my Navigator i get
          an "Ollama running"</p>

          <p>i also found that even the "ollama list"<br>gives the same error " Error:
          something went wrong, please see the ollama server logs for details " ans
          still no logs.</p>

          <p>i did not find any logs in the files where Ollama saves logs , the only
          logs are the docker logs , and they contain nothing</p>

          '
        raw: "I'm encountering issues while trying to run an Ollama container behind\
          \ a proxy. Here are the steps I've taken and the issues I've faced:\r\n\r\
          \n1. **Creating an Image with Certificate**:\r\n   ```\r\n   cat Dockerfile\r\
          \n   FROM ollama/ollama\r\n   COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\r\
          \n   RUN update-ca-certificates\r\n   ```\r\n\r\n2. **Starting a Container\
          \ Using This Image with Proxy Variables Injected**:\r\n   ```\r\n   docker\
          \ run -d \\\r\n   -e HTTPS_PROXY=http://x.x.x.x:3128 \\\r\n   -e HTTP_PROXY=http://x.x.x.x:3128\
          \ \\\r\n   -e http_proxy=http://x.x.x.x:3128 \\\r\n   -e https_proxy=http://x.x.x.x:3128\
          \ \\\r\n   -p 11434:11434 ollama-with-ca\r\n   ```\r\n\r\n3. **Inside the\
          \ Container**:\r\n   - Ran `apt-get update` to confirm internet access and\
          \ proper proxy functionality.\r\n   - Executed `ollama pull mistral` and\
          \ `ollama run mistral:instruct`, but consistently encountered the error:\
          \ \"Error: something went wrong, please see the Ollama server logs for details.\"\
          \r\n   - Container logs (`docker logs 8405972b3d6b`) showed no errors, only\
          \ the following information:\r\n     ```\r\n     Couldn't find '/root/.ollama/id_ed25519'.\
          \ Generating new private key.\r\n     Your new public key is: ssh-ed25519\
          \ AAAAC3NzaC1lZDI1NTE5AAAAIDppYjymfVcdtDNT/umLfrzlIx1QquQ/gTuSI7SAV194\r\
          \n     2024/01/24 08:40:55 images.go:808: total blobs: 0\r\n     2024/01/24\
          \ 08:40:55 images.go:815: total unused blobs removed: 0\r\n     2024/01/24\
          \ 08:40:55 routes.go:930: Listening on [::]:11434 (version 0.1.20)\r\n \
          \    2024/01/24 08:40:56 shim_ext_server.go:142: Dynamic LLM variants [cuda]\r\
          \n     2024/01/24 08:40:56 gpu.go:88: Detecting GPU type\r\n     2024/01/24\
          \ 08:40:56 gpu.go:203: Searching for GPU management library libnvidia-ml.so\r\
          \n     2024/01/24 08:40:56 gpu.go:248: Discovered GPU libraries: []\r\n\
          \     2024/01/24 08:40:56 gpu.go:203: Searching for GPU management library\
          \ librocm_smi64.so\r\n     2024/01/24 08:40:56 gpu.go:248: Discovered GPU\
          \ libraries: []\r\n     2024/01/24 08:40:56 routes.go:953: no GPU detected\r\
          \n     ```\r\n\r\n4. **Using Wget to Download the Model**:\r\n   - Successfully\
          \ downloaded \"mistral-7b-instruct-v0.1.Q5_K_M.gguf\" via `wget`.\r\n  \
          \ - Created a simple ModelFile:\r\n     ```\r\n     FROM /home/mistral-7b-instruct-v0.1.Q5_K_M.gguf\r\
          \n     ```\r\n   - Executed `ollama create mistralModel -f Modelfile`, resulting\
          \ in the same error: \"Error: something went wrong, please see the Ollama\
          \ server logs for details.\"\r\n   - The logs from `docker logs 8405972b3d6b`\
          \ again showed no error:\r\n     ```\r\n     Couldn't find '/root/.ollama/id_ed25519'.\
          \ Generating new private key.\r\n     Your new public key is:\r\n\r\n  \
          \   ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDppYjymfVcdtDNT/umLfrzlIx1QquQ/gTuSI7SAV194\r\
          \n\r\n     2024/01/24 08:40:55 images.go:808: total blobs: 0\r\n     2024/01/24\
          \ 08:40:55 images.go:815: total unused blobs removed: 0\r\n     2024/01/24\
          \ 08:40:55 routes.go:930: Listening on [::]:11434 (version 0.1.20)\r\n \
          \    2024/01/24 08:40:56 shim_ext_server.go:142: Dynamic LLM variants [cuda]\r\
          \n     2024/01/24 08:40:56 gpu.go:88: Detecting GPU type\r\n     \r\n\r\n\
          When Making a http request on the ollama server in my Navigator i get an\
          \ \"Ollama running\"\r\n\r\ni also found that even the \"ollama list\"\r\
          \ngives the same error \" Error: something went wrong, please see the ollama\
          \ server logs for details \" ans still no logs.\r\n\r\ni did not find any\
          \ logs in the files where Ollama saves logs , the only logs are the docker\
          \ logs , and they contain nothing"
        updatedAt: '2024-01-24T09:30:34.600Z'
      numEdits: 0
      reactions: []
    id: 65b0d8ba262e6f34ef0f08dc
    type: comment
  author: icemaro
  content: "I'm encountering issues while trying to run an Ollama container behind\
    \ a proxy. Here are the steps I've taken and the issues I've faced:\r\n\r\n1.\
    \ **Creating an Image with Certificate**:\r\n   ```\r\n   cat Dockerfile\r\n \
    \  FROM ollama/ollama\r\n   COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\r\
    \n   RUN update-ca-certificates\r\n   ```\r\n\r\n2. **Starting a Container Using\
    \ This Image with Proxy Variables Injected**:\r\n   ```\r\n   docker run -d \\\
    \r\n   -e HTTPS_PROXY=http://x.x.x.x:3128 \\\r\n   -e HTTP_PROXY=http://x.x.x.x:3128\
    \ \\\r\n   -e http_proxy=http://x.x.x.x:3128 \\\r\n   -e https_proxy=http://x.x.x.x:3128\
    \ \\\r\n   -p 11434:11434 ollama-with-ca\r\n   ```\r\n\r\n3. **Inside the Container**:\r\
    \n   - Ran `apt-get update` to confirm internet access and proper proxy functionality.\r\
    \n   - Executed `ollama pull mistral` and `ollama run mistral:instruct`, but consistently\
    \ encountered the error: \"Error: something went wrong, please see the Ollama\
    \ server logs for details.\"\r\n   - Container logs (`docker logs 8405972b3d6b`)\
    \ showed no errors, only the following information:\r\n     ```\r\n     Couldn't\
    \ find '/root/.ollama/id_ed25519'. Generating new private key.\r\n     Your new\
    \ public key is: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDppYjymfVcdtDNT/umLfrzlIx1QquQ/gTuSI7SAV194\r\
    \n     2024/01/24 08:40:55 images.go:808: total blobs: 0\r\n     2024/01/24 08:40:55\
    \ images.go:815: total unused blobs removed: 0\r\n     2024/01/24 08:40:55 routes.go:930:\
    \ Listening on [::]:11434 (version 0.1.20)\r\n     2024/01/24 08:40:56 shim_ext_server.go:142:\
    \ Dynamic LLM variants [cuda]\r\n     2024/01/24 08:40:56 gpu.go:88: Detecting\
    \ GPU type\r\n     2024/01/24 08:40:56 gpu.go:203: Searching for GPU management\
    \ library libnvidia-ml.so\r\n     2024/01/24 08:40:56 gpu.go:248: Discovered GPU\
    \ libraries: []\r\n     2024/01/24 08:40:56 gpu.go:203: Searching for GPU management\
    \ library librocm_smi64.so\r\n     2024/01/24 08:40:56 gpu.go:248: Discovered\
    \ GPU libraries: []\r\n     2024/01/24 08:40:56 routes.go:953: no GPU detected\r\
    \n     ```\r\n\r\n4. **Using Wget to Download the Model**:\r\n   - Successfully\
    \ downloaded \"mistral-7b-instruct-v0.1.Q5_K_M.gguf\" via `wget`.\r\n   - Created\
    \ a simple ModelFile:\r\n     ```\r\n     FROM /home/mistral-7b-instruct-v0.1.Q5_K_M.gguf\r\
    \n     ```\r\n   - Executed `ollama create mistralModel -f Modelfile`, resulting\
    \ in the same error: \"Error: something went wrong, please see the Ollama server\
    \ logs for details.\"\r\n   - The logs from `docker logs 8405972b3d6b` again showed\
    \ no error:\r\n     ```\r\n     Couldn't find '/root/.ollama/id_ed25519'. Generating\
    \ new private key.\r\n     Your new public key is:\r\n\r\n     ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDppYjymfVcdtDNT/umLfrzlIx1QquQ/gTuSI7SAV194\r\
    \n\r\n     2024/01/24 08:40:55 images.go:808: total blobs: 0\r\n     2024/01/24\
    \ 08:40:55 images.go:815: total unused blobs removed: 0\r\n     2024/01/24 08:40:55\
    \ routes.go:930: Listening on [::]:11434 (version 0.1.20)\r\n     2024/01/24 08:40:56\
    \ shim_ext_server.go:142: Dynamic LLM variants [cuda]\r\n     2024/01/24 08:40:56\
    \ gpu.go:88: Detecting GPU type\r\n     \r\n\r\nWhen Making a http request on\
    \ the ollama server in my Navigator i get an \"Ollama running\"\r\n\r\ni also\
    \ found that even the \"ollama list\"\r\ngives the same error \" Error: something\
    \ went wrong, please see the ollama server logs for details \" ans still no logs.\r\
    \n\r\ni did not find any logs in the files where Ollama saves logs , the only\
    \ logs are the docker logs , and they contain nothing"
  created_at: 2024-01-24 09:30:34+00:00
  edited: false
  hidden: false
  id: 65b0d8ba262e6f34ef0f08dc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Issues Running Ollama Container Behind Proxy - No Error Logs Found
