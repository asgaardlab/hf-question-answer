!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philgrey
conflicting_files: null
created_at: 2023-11-23 08:48:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-11-23T08:48:11.000Z'
    data:
      edited: false
      editors:
      - philgrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8122323751449585
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: '<p>I''ve followed Deployment guide for this model but, I''ve got following
          error</p>

          <p>UnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-11-22-21-44-54-401:
          Failed. Reason: The primary container for production variant AllTraffic
          did not pass the ping health check. Please check CloudWatch logs for this
          endpoint..</p>

          '
        raw: "I've followed Deployment guide for this model but, I've got following\
          \ error\r\n\r\nUnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-11-22-21-44-54-401:\
          \ Failed. Reason: The primary container for production variant AllTraffic\
          \ did not pass the ping health check. Please check CloudWatch logs for this\
          \ endpoint.."
        updatedAt: '2023-11-23T08:48:11.864Z'
      numEdits: 0
      reactions: []
    id: 655f11cbb11e49dd1fe8fed1
    type: comment
  author: philgrey
  content: "I've followed Deployment guide for this model but, I've got following\
    \ error\r\n\r\nUnexpectedStatusException: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-11-22-21-44-54-401:\
    \ Failed. Reason: The primary container for production variant AllTraffic did\
    \ not pass the ping health check. Please check CloudWatch logs for this endpoint.."
  created_at: 2023-11-23 08:48:11+00:00
  edited: false
  hidden: false
  id: 655f11cbb11e49dd1fe8fed1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-23T09:43:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9299234747886658
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This is a GGUF model, it''s not supported on SageMaker.</p>

          <p>Please see the GPTQ or AWQ model instead - both are supported by Text
          Generation Inference, which should be supported on SageMaker.</p>

          '
        raw: 'This is a GGUF model, it''s not supported on SageMaker.


          Please see the GPTQ or AWQ model instead - both are supported by Text Generation
          Inference, which should be supported on SageMaker.'
        updatedAt: '2023-11-23T09:43:19.945Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - philgrey
    id: 655f1eb76821269b27fd8ce2
    type: comment
  author: TheBloke
  content: 'This is a GGUF model, it''s not supported on SageMaker.


    Please see the GPTQ or AWQ model instead - both are supported by Text Generation
    Inference, which should be supported on SageMaker.'
  created_at: 2023-11-23 09:43:19+00:00
  edited: false
  hidden: false
  id: 655f1eb76821269b27fd8ce2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-11-23T09:46:22.000Z'
    data:
      edited: true
      editors:
      - philgrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6994774341583252
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: '<p>Thanks for your quick response.<br>I wanna ask one more.<br>Here,
          what is the meaning of HUGGING_FACE_HUB_TOKEN?<br>If it is token for my
          account, I think it has no meaning.<br>Can you explain about this?<br>Thanks
          very much</p>

          <p>config = {<br>  ''HF_MODEL_ID'': "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
          # model_id from hf.co/models<br>  ''SM_NUM_GPUS'': json.dumps(number_of_gpu),
          # Number of GPU used per replica<br>  ''MAX_INPUT_LENGTH'': json.dumps(2048),  #
          Max length of input text<br>  ''MAX_TOTAL_TOKENS'': json.dumps(4096),  #
          Max length of the generation (including input text)<br>  ''MAX_BATCH_TOTAL_TOKENS'':
          json.dumps(4096),  # Limits the number of tokens that can be processed in
          parallel during the generation<br>  ''HUGGING_FACE_HUB_TOKEN'': ""<br>}</p>

          <h1 id="create-huggingfacemodel-with-the-image-uri">create HuggingFaceModel
          with the image uri</h1>

          <p>llm_model = HuggingFaceModel(<br>  role=role,<br>  image_uri=llm_image,<br>  env=config<br>)</p>

          '
        raw: "Thanks for your quick response.\nI wanna ask one more.\nHere, what is\
          \ the meaning of HUGGING_FACE_HUB_TOKEN?\nIf it is token for my account,\
          \ I think it has no meaning.\nCan you explain about this?\nThanks very much\n\
          \nconfig = {\n  'HF_MODEL_ID': \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          , # model_id from hf.co/models\n  'SM_NUM_GPUS': json.dumps(number_of_gpu),\
          \ # Number of GPU used per replica\n  'MAX_INPUT_LENGTH': json.dumps(2048),\
          \  # Max length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(4096), \
          \ # Max length of the generation (including input text)\n  'MAX_BATCH_TOTAL_TOKENS':\
          \ json.dumps(4096),  # Limits the number of tokens that can be processed\
          \ in parallel during the generation\n  'HUGGING_FACE_HUB_TOKEN': \"\"\n\
          }\n\n# create HuggingFaceModel with the image uri\nllm_model = HuggingFaceModel(\n\
          \  role=role,\n  image_uri=llm_image,\n  env=config\n)"
        updatedAt: '2023-11-23T09:46:49.740Z'
      numEdits: 1
      reactions: []
    id: 655f1f6eb11e49dd1feac8dd
    type: comment
  author: philgrey
  content: "Thanks for your quick response.\nI wanna ask one more.\nHere, what is\
    \ the meaning of HUGGING_FACE_HUB_TOKEN?\nIf it is token for my account, I think\
    \ it has no meaning.\nCan you explain about this?\nThanks very much\n\nconfig\
    \ = {\n  'HF_MODEL_ID': \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", # model_id\
    \ from hf.co/models\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU\
    \ used per replica\n  'MAX_INPUT_LENGTH': json.dumps(2048),  # Max length of input\
    \ text\n  'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation\
    \ (including input text)\n  'MAX_BATCH_TOTAL_TOKENS': json.dumps(4096),  # Limits\
    \ the number of tokens that can be processed in parallel during the generation\n\
    \  'HUGGING_FACE_HUB_TOKEN': \"\"\n}\n\n# create HuggingFaceModel with the image\
    \ uri\nllm_model = HuggingFaceModel(\n  role=role,\n  image_uri=llm_image,\n \
    \ env=config\n)"
  created_at: 2023-11-23 09:46:22+00:00
  edited: true
  hidden: false
  id: 655f1f6eb11e49dd1feac8dd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 15
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Can't deploy to sagemaker
