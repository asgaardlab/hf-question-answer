!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rambocoder
conflicting_files: null
created_at: 2023-09-28 01:24:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
      fullname: Alex K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rambocoder
      type: user
    createdAt: '2023-09-28T02:24:15.000Z'
    data:
      edited: false
      editors:
      - rambocoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.777621865272522
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b01d8f1efc4c4cc9c3df2ae9df4b7dc.svg
          fullname: Alex K
          isHf: false
          isPro: false
          name: rambocoder
          type: user
        html: '<p>I am impressed. Works with latest llama.cpp without any issues.</p>

          '
        raw: I am impressed. Works with latest llama.cpp without any issues.
        updatedAt: '2023-09-28T02:24:15.630Z'
      numEdits: 0
      reactions: []
    id: 6514e3cf54c3d8f611237a10
    type: comment
  author: rambocoder
  content: I am impressed. Works with latest llama.cpp without any issues.
  created_at: 2023-09-28 01:24:15+00:00
  edited: false
  hidden: false
  id: 6514e3cf54c3d8f611237a10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a9fab8bab1855ff03279a9/r0paWL_ruSsIMbYKNTvNS.png?w=200&h=200&f=face
      fullname: Tanvir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TanvirOnHF
      type: user
    createdAt: '2023-09-28T02:38:56.000Z'
    data:
      edited: false
      editors:
      - TanvirOnHF
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5855751633644104
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a9fab8bab1855ff03279a9/r0paWL_ruSsIMbYKNTvNS.png?w=200&h=200&f=face
          fullname: Tanvir
          isHf: false
          isPro: false
          name: TanvirOnHF
          type: user
        html: '<p>Indeed!</p>

          '
        raw: Indeed!
        updatedAt: '2023-09-28T02:38:56.314Z'
      numEdits: 0
      reactions: []
    id: 6514e7404a40f1435969be01
    type: comment
  author: TanvirOnHF
  content: Indeed!
  created_at: 2023-09-28 01:38:56+00:00
  edited: false
  hidden: false
  id: 6514e7404a40f1435969be01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647433130da364bd0d014179/d_Xhm637GpIcvKNM1nluR.png?w=200&h=200&f=face
      fullname: Sethu Iyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sethuiyer
      type: user
    createdAt: '2023-09-28T04:59:20.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647433130da364bd0d014179/d_Xhm637GpIcvKNM1nluR.png?w=200&h=200&f=face
          fullname: Sethu Iyer
          isHf: false
          isPro: false
          name: sethuiyer
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-28T05:02:41.188Z'
      numEdits: 0
      reactions: []
    id: 651508284b4f53b8268b3c35
    type: comment
  author: sethuiyer
  content: This comment has been hidden
  created_at: 2023-09-28 03:59:20+00:00
  edited: true
  hidden: true
  id: 651508284b4f53b8268b3c35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-09-29T05:59:54.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9765442609786987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: '<p>How did you ran, can you please share the code?</p>

          '
        raw: 'How did you ran, can you please share the code?

          '
        updatedAt: '2023-09-29T05:59:54.210Z'
      numEdits: 0
      reactions: []
    id: 651667da7a07d71e9c74fe51
    type: comment
  author: ianuvrat
  content: 'How did you ran, can you please share the code?

    '
  created_at: 2023-09-29 04:59:54+00:00
  edited: false
  hidden: false
  id: 651667da7a07d71e9c74fe51
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZdU0rWpmK17L7-ECZI3-H.png?w=200&h=200&f=face
      fullname: Dan Simmonds
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BingoBird
      type: user
    createdAt: '2023-09-29T10:04:48.000Z'
    data:
      edited: false
      editors:
      - BingoBird
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32984957098960876
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ZdU0rWpmK17L7-ECZI3-H.png?w=200&h=200&f=face
          fullname: Dan Simmonds
          isHf: false
          isPro: false
          name: BingoBird
          type: user
        html: '<p>:What is a good invocation and paramaters for mistral 7b?</p>

          <p>I''m testing with --temp 0.6 --mirostat 2 --mirostat-ent 6 --mirostat-lr
          0.2 -n 2048 -c 2048 -n -1 --repeat-last-n 1600 --repeat-penalty 1.2 </p>

          '
        raw: ':What is a good invocation and paramaters for mistral 7b?


          I''m testing with --temp 0.6 --mirostat 2 --mirostat-ent 6 --mirostat-lr
          0.2 -n 2048 -c 2048 -n -1 --repeat-last-n 1600 --repeat-penalty 1.2 '
        updatedAt: '2023-09-29T10:04:48.020Z'
      numEdits: 0
      reactions: []
    id: 6516a140ac3f1b1d27ec3d11
    type: comment
  author: BingoBird
  content: ':What is a good invocation and paramaters for mistral 7b?


    I''m testing with --temp 0.6 --mirostat 2 --mirostat-ent 6 --mirostat-lr 0.2 -n
    2048 -c 2048 -n -1 --repeat-last-n 1600 --repeat-penalty 1.2 '
  created_at: 2023-09-29 09:04:48+00:00
  edited: false
  hidden: false
  id: 6516a140ac3f1b1d27ec3d11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6516ed02ee8c603b70074a38/-B7uJCmSKR4WXCPe3MdEG.png?w=200&h=200&f=face
      fullname: TKay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TK-Master
      type: user
    createdAt: '2023-09-29T16:23:07.000Z'
    data:
      edited: false
      editors:
      - TK-Master
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.575187087059021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6516ed02ee8c603b70074a38/-B7uJCmSKR4WXCPe3MdEG.png?w=200&h=200&f=face
          fullname: TKay
          isHf: false
          isPro: false
          name: TK-Master
          type: user
        html: "<p>This has to be indeed the best 7b model I have tried.. for those\
          \ who can't get it to run in text-generation-ui (I sure couldn't, it's broken\
          \ af) here's some code and detailed instructions for a simple llama-cpp-python\
          \ chatbot using this model.</p>\n<p>First, I recommend a clean python installation\
          \ with pip etc,  you can use a virtual environment for this (I'm using miniconda\
          \ with python version 3.10).<br>Then I installed llama-cpp-python with cuda\
          \ support using the following commands (in windows cmd).</p>\n<p><code>set\
          \ CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" &amp;&amp; pip install llama-cpp-python<br>set\
          \ CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" &amp;&amp; set FORCE_CMAKE=1 &amp;&amp;\
          \ set CUDAFLAGS=\"-arch=all -lcublas\"<br>python -m pip install <a rel=\"\
          nofollow\" href=\"https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/basic/llama_cpp_python-0.2.7+cu118-cp310-cp310-win_amd64.whl\"\
          >https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/basic/llama_cpp_python-0.2.7+cu118-cp310-cp310-win_amd64.whl</a>\
          \ --no-cache-dir</code></p>\n<p>(Note: this works for me using cuda 11.8,\
          \ no avx. For other versions you might wanna replace the link with another\
          \ from <a rel=\"nofollow\" href=\"https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels\"\
          >https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels</a>)</p>\n<p>And\
          \ that's it.. now you can run the following python script to ask the model\
          \ questions.</p>\n<p><code>python simpleStreamChat.py</code></p>\n<pre><code>import\
          \ json\nimport argparse\nfrom llama_cpp import Llama\n\nparser = argparse.ArgumentParser()\n\
          parser.add_argument(\"-m\", \"--model\", type=str, default=\"../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
          )\nparser.add_argument(\"-pt\", \"--prompt\", type=str, default=\"&lt;s&gt;[INST]{prompt}[/INST]\"\
          )\nargs = parser.parse_args()\n\nprompt_template = args.prompt\n\nprint(\"\
          Loading model \" + args.model)\nllm = Llama(model_path=args.model, n_gpu_layers=35,\
          \ n_ctx=4096, temp=0.7, repeat_penalty=1.1, verbose=False)\n\nstream = \"\
          \"#llm(\"Question: What are the names of the planets in the solar system?\
          \ Answer: \", max_tokens=48,stop=[\"Q:\", \"\\n\"],stream=True)\n\n# Function\
          \ - Print response output in chunks (stream)\ndef printresponse(response):\n\
          \    completion_text = ''\n    # iterate through the stream of events and\
          \ print it\n    print(f\"Bot:\", end=\"\", flush=True)\n    for event in\
          \ response:\n        event_text = event['choices'][0]['text']\n        completion_text\
          \ += event_text\n        print(f\"{event_text}\", end=\"\", flush=True)\n\
          \n    print(\"\",flush=True)\n    # remember context\n    #context.append({\"\
          role\": \"assistant\", \"content\" : completion_text})\n    return completion_text\n\
          \n#printresponse(stream)\n\nwhile True:\n    try:\n        u_input = input(\"\
          -&gt; \")\n        \n        prompt = prompt_template.format(prompt=u_input)\n\
          \        stream = llm(prompt, max_tokens=512, stream=True)\n        response\
          \ = printresponse(stream)\n        print()\n\n    except KeyboardInterrupt:\n\
          \        print(\"\\n..(Response interrupted).\")#continue\n    print()\n\
          </code></pre>\n<p>Note: set verbose=True to see token generation times etc.\
          \ n_gpu_layers=how many layers on gpu, n_ctx=context size</p>\n<p>Uncomment\
          \ <code>stream = \"\"#llm(\"Question: What are the names of the planets\
          \ in the solar system? Answer: \", max_tokens=48,stop=[\"Q:\", \"\\n\"],stream=True)</code><br>and\
          \ <code>#printresponse(stream)</code> if you want.</p>\n<p>You're welcome!</p>\n"
        raw: "This has to be indeed the best 7b model I have tried.. for those who\
          \ can't get it to run in text-generation-ui (I sure couldn't, it's broken\
          \ af) here's some code and detailed instructions for a simple llama-cpp-python\
          \ chatbot using this model.\n\nFirst, I recommend a clean python installation\
          \ with pip etc,  you can use a virtual environment for this (I'm using miniconda\
          \ with python version 3.10).\nThen I installed llama-cpp-python with cuda\
          \ support using the following commands (in windows cmd).\n\n<code>set CMAKE_ARGS=\"\
          -DLLAMA_CUBLAS=on\" && pip install llama-cpp-python\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\
          \ && set FORCE_CMAKE=1 && set CUDAFLAGS=\"-arch=all -lcublas\"\npython -m\
          \ pip install https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/basic/llama_cpp_python-0.2.7+cu118-cp310-cp310-win_amd64.whl\
          \ --no-cache-dir</code>\n\n(Note: this works for me using cuda 11.8, no\
          \ avx. For other versions you might wanna replace the link with another\
          \ from https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels)\n\nAnd\
          \ that's it.. now you can run the following python script to ask the model\
          \ questions.\n\n<code>python simpleStreamChat.py</code>\n\n```\nimport json\n\
          import argparse\nfrom llama_cpp import Llama\n\nparser = argparse.ArgumentParser()\n\
          parser.add_argument(\"-m\", \"--model\", type=str, default=\"../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
          )\nparser.add_argument(\"-pt\", \"--prompt\", type=str, default=\"<s>[INST]{prompt}[/INST]\"\
          )\nargs = parser.parse_args()\n\nprompt_template = args.prompt\n\nprint(\"\
          Loading model \" + args.model)\nllm = Llama(model_path=args.model, n_gpu_layers=35,\
          \ n_ctx=4096, temp=0.7, repeat_penalty=1.1, verbose=False)\n\nstream = \"\
          \"#llm(\"Question: What are the names of the planets in the solar system?\
          \ Answer: \", max_tokens=48,stop=[\"Q:\", \"\\n\"],stream=True)\n\n# Function\
          \ - Print response output in chunks (stream)\ndef printresponse(response):\n\
          \    completion_text = ''\n    # iterate through the stream of events and\
          \ print it\n    print(f\"Bot:\", end=\"\", flush=True)\n    for event in\
          \ response:\n        event_text = event['choices'][0]['text']\n        completion_text\
          \ += event_text\n        print(f\"{event_text}\", end=\"\", flush=True)\n\
          \n    print(\"\",flush=True)\n    # remember context\n    #context.append({\"\
          role\": \"assistant\", \"content\" : completion_text})\n    return completion_text\n\
          \n#printresponse(stream)\n\nwhile True:\n    try:\n        u_input = input(\"\
          -> \")\n\t\t\n        prompt = prompt_template.format(prompt=u_input)\n\
          \        stream = llm(prompt, max_tokens=512, stream=True)\n        response\
          \ = printresponse(stream)\n        print()\n\n    except KeyboardInterrupt:\n\
          \        print(\"\\n..(Response interrupted).\")#continue\n    print()\n\
          ```\n\nNote: set verbose=True to see token generation times etc. n_gpu_layers=how\
          \ many layers on gpu, n_ctx=context size\n\nUncomment ```stream = \"\"#llm(\"\
          Question: What are the names of the planets in the solar system? Answer:\
          \ \", max_tokens=48,stop=[\"Q:\", \"\\n\"],stream=True)```\nand ```#printresponse(stream)```\
          \ if you want.\n\nYou're welcome!"
        updatedAt: '2023-09-29T16:23:07.487Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TanvirOnHF
    id: 6516f9eb186bc3b699363d53
    type: comment
  author: TK-Master
  content: "This has to be indeed the best 7b model I have tried.. for those who can't\
    \ get it to run in text-generation-ui (I sure couldn't, it's broken af) here's\
    \ some code and detailed instructions for a simple llama-cpp-python chatbot using\
    \ this model.\n\nFirst, I recommend a clean python installation with pip etc,\
    \  you can use a virtual environment for this (I'm using miniconda with python\
    \ version 3.10).\nThen I installed llama-cpp-python with cuda support using the\
    \ following commands (in windows cmd).\n\n<code>set CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"\
    \ && pip install llama-cpp-python\nset CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" && set\
    \ FORCE_CMAKE=1 && set CUDAFLAGS=\"-arch=all -lcublas\"\npython -m pip install\
    \ https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/basic/llama_cpp_python-0.2.7+cu118-cp310-cp310-win_amd64.whl\
    \ --no-cache-dir</code>\n\n(Note: this works for me using cuda 11.8, no avx. For\
    \ other versions you might wanna replace the link with another from https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels)\n\
    \nAnd that's it.. now you can run the following python script to ask the model\
    \ questions.\n\n<code>python simpleStreamChat.py</code>\n\n```\nimport json\n\
    import argparse\nfrom llama_cpp import Llama\n\nparser = argparse.ArgumentParser()\n\
    parser.add_argument(\"-m\", \"--model\", type=str, default=\"../models/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
    )\nparser.add_argument(\"-pt\", \"--prompt\", type=str, default=\"<s>[INST]{prompt}[/INST]\"\
    )\nargs = parser.parse_args()\n\nprompt_template = args.prompt\n\nprint(\"Loading\
    \ model \" + args.model)\nllm = Llama(model_path=args.model, n_gpu_layers=35,\
    \ n_ctx=4096, temp=0.7, repeat_penalty=1.1, verbose=False)\n\nstream = \"\"#llm(\"\
    Question: What are the names of the planets in the solar system? Answer: \", max_tokens=48,stop=[\"\
    Q:\", \"\\n\"],stream=True)\n\n# Function - Print response output in chunks (stream)\n\
    def printresponse(response):\n    completion_text = ''\n    # iterate through\
    \ the stream of events and print it\n    print(f\"Bot:\", end=\"\", flush=True)\n\
    \    for event in response:\n        event_text = event['choices'][0]['text']\n\
    \        completion_text += event_text\n        print(f\"{event_text}\", end=\"\
    \", flush=True)\n\n    print(\"\",flush=True)\n    # remember context\n    #context.append({\"\
    role\": \"assistant\", \"content\" : completion_text})\n    return completion_text\n\
    \n#printresponse(stream)\n\nwhile True:\n    try:\n        u_input = input(\"\
    -> \")\n\t\t\n        prompt = prompt_template.format(prompt=u_input)\n      \
    \  stream = llm(prompt, max_tokens=512, stream=True)\n        response = printresponse(stream)\n\
    \        print()\n\n    except KeyboardInterrupt:\n        print(\"\\n..(Response\
    \ interrupted).\")#continue\n    print()\n```\n\nNote: set verbose=True to see\
    \ token generation times etc. n_gpu_layers=how many layers on gpu, n_ctx=context\
    \ size\n\nUncomment ```stream = \"\"#llm(\"Question: What are the names of the\
    \ planets in the solar system? Answer: \", max_tokens=48,stop=[\"Q:\", \"\\n\"\
    ],stream=True)```\nand ```#printresponse(stream)``` if you want.\n\nYou're welcome!"
  created_at: 2023-09-29 15:23:07+00:00
  edited: false
  hidden: false
  id: 6516f9eb186bc3b699363d53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
      fullname: Ziggy Stardust
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nurb432
      type: user
    createdAt: '2023-09-29T16:45:09.000Z'
    data:
      edited: false
      editors:
      - Nurb432
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9435169696807861
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7e4979f04fda14b73a43c398ce7da27.svg
          fullname: Ziggy Stardust
          isHf: false
          isPro: false
          name: Nurb432
          type: user
        html: '<p>odd. working fine for me and i have not updated anything in a couple
          of weeks. ( ooba )</p>

          '
        raw: odd. working fine for me and i have not updated anything in a couple
          of weeks. ( ooba )
        updatedAt: '2023-09-29T16:45:09.280Z'
      numEdits: 0
      reactions: []
    id: 6516ff155ae78fd448a73d7c
    type: comment
  author: Nurb432
  content: odd. working fine for me and i have not updated anything in a couple of
    weeks. ( ooba )
  created_at: 2023-09-29 15:45:09+00:00
  edited: false
  hidden: false
  id: 6516ff155ae78fd448a73d7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6392925180a33abfebeb822e/RD2zafxM6BvCpmnZsoLGe.jpeg?w=200&h=200&f=face
      fullname: Anup Ghatage
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aghatage
      type: user
    createdAt: '2023-09-29T23:18:50.000Z'
    data:
      edited: false
      editors:
      - aghatage
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8259628415107727
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6392925180a33abfebeb822e/RD2zafxM6BvCpmnZsoLGe.jpeg?w=200&h=200&f=face
          fullname: Anup Ghatage
          isHf: false
          isPro: false
          name: aghatage
          type: user
        html: '<p>Anyone able to use it with constricting grammar in llama.cpp ?</p>

          '
        raw: Anyone able to use it with constricting grammar in llama.cpp ?
        updatedAt: '2023-09-29T23:18:50.997Z'
      numEdits: 0
      reactions: []
    id: 65175b5a70746a75c1fe838e
    type: comment
  author: aghatage
  content: Anyone able to use it with constricting grammar in llama.cpp ?
  created_at: 2023-09-29 22:18:50+00:00
  edited: false
  hidden: false
  id: 65175b5a70746a75c1fe838e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71ac1bbf7a5f589b22bafff1ee6f4eb9.svg
      fullname: UserB_tm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atstim731
      type: user
    createdAt: '2023-10-01T05:32:03.000Z'
    data:
      edited: false
      editors:
      - atstim731
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8682715892791748
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71ac1bbf7a5f589b22bafff1ee6f4eb9.svg
          fullname: UserB_tm
          isHf: false
          isPro: false
          name: atstim731
          type: user
        html: '<p>Hands down the best 7b model, holy cow.<br>For starters, I have
          a custom character, but the settings I''m using in tgwui are:<br>Instruction
          Template: Mistral (no modifications)<br>Generation Preset: Divine Intellect<br>Model
          Loader: LlamaCpp<br>The model is smart, retains context after several turns,  great
          inference, picks up on nuance.<br>Mistral just hit''s different than Llama,
          no judgement on Meta.<br>If you''ve watched Frazier, Mistral is like a very
          smart Roz, and Llama is Maris.  </p>

          '
        raw: "Hands down the best 7b model, holy cow. \nFor starters, I have a custom\
          \ character, but the settings I'm using in tgwui are:\nInstruction Template:\
          \ Mistral (no modifications)\nGeneration Preset: Divine Intellect\nModel\
          \ Loader: LlamaCpp\nThe model is smart, retains context after several turns,\
          \  great inference, picks up on nuance. \nMistral just hit's different than\
          \ Llama, no judgement on Meta. \nIf you've watched Frazier, Mistral is like\
          \ a very smart Roz, and Llama is Maris.  \n"
        updatedAt: '2023-10-01T05:32:03.882Z'
      numEdits: 0
      reactions: []
    id: 6519045329af405887ceafb4
    type: comment
  author: atstim731
  content: "Hands down the best 7b model, holy cow. \nFor starters, I have a custom\
    \ character, but the settings I'm using in tgwui are:\nInstruction Template: Mistral\
    \ (no modifications)\nGeneration Preset: Divine Intellect\nModel Loader: LlamaCpp\n\
    The model is smart, retains context after several turns,  great inference, picks\
    \ up on nuance. \nMistral just hit's different than Llama, no judgement on Meta.\
    \ \nIf you've watched Frazier, Mistral is like a very smart Roz, and Llama is\
    \ Maris.  \n"
  created_at: 2023-10-01 04:32:03+00:00
  edited: false
  hidden: false
  id: 6519045329af405887ceafb4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d71b0b5e00c01a6f06a84beac7fe0a2c.svg
      fullname: Eric DUMOULIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edumoulin
      type: user
    createdAt: '2023-10-02T16:38:56.000Z'
    data:
      edited: true
      editors:
      - edumoulin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9301049709320068
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d71b0b5e00c01a6f06a84beac7fe0a2c.svg
          fullname: Eric DUMOULIN
          isHf: false
          isPro: false
          name: edumoulin
          type: user
        html: '<p>I was able to run this model in Q5_K_M.gguf through oobabooga on
          a M2 MacBookPro with 16GB : it runs very smoothly with 1 layer in GPU units,
          quite faster than 7B Llama2 or Vigogne with same quantization. However,
          it seems sometimes to struggle with long conversations (the answers get
          lesser accurate and you need to reload the model).<br>Tested on bash and
          SQL code, the results where relevant in most cases.</p>

          '
        raw: 'I was able to run this model in Q5_K_M.gguf through oobabooga on a M2
          MacBookPro with 16GB : it runs very smoothly with 1 layer in GPU units,
          quite faster than 7B Llama2 or Vigogne with same quantization. However,
          it seems sometimes to struggle with long conversations (the answers get
          lesser accurate and you need to reload the model).

          Tested on bash and SQL code, the results where relevant in most cases.'
        updatedAt: '2023-10-02T16:45:53.197Z'
      numEdits: 1
      reactions: []
    id: 651af220dae56722e34c76f3
    type: comment
  author: edumoulin
  content: 'I was able to run this model in Q5_K_M.gguf through oobabooga on a M2
    MacBookPro with 16GB : it runs very smoothly with 1 layer in GPU units, quite
    faster than 7B Llama2 or Vigogne with same quantization. However, it seems sometimes
    to struggle with long conversations (the answers get lesser accurate and you need
    to reload the model).

    Tested on bash and SQL code, the results where relevant in most cases.'
  created_at: 2023-10-02 15:38:56+00:00
  edited: true
  hidden: false
  id: 651af220dae56722e34c76f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66cc500136f5e4a4b1c6f24246c1a1a1.svg
      fullname: Vince
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Akalilol
      type: user
    createdAt: '2023-10-03T21:09:08.000Z'
    data:
      edited: false
      editors:
      - Akalilol
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.52583247423172
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66cc500136f5e4a4b1c6f24246c1a1a1.svg
          fullname: Vince
          isHf: false
          isPro: false
          name: Akalilol
          type: user
        html: '<p>Is it uncensored ?</p>

          '
        raw: Is it uncensored ?
        updatedAt: '2023-10-03T21:09:08.585Z'
      numEdits: 0
      reactions: []
    id: 651c82f42dec9d6101be6a80
    type: comment
  author: Akalilol
  content: Is it uncensored ?
  created_at: 2023-10-03 20:09:08+00:00
  edited: false
  hidden: false
  id: 651c82f42dec9d6101be6a80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
      fullname: Anuvrat Shukla
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ianuvrat
      type: user
    createdAt: '2023-10-04T03:54:19.000Z'
    data:
      edited: false
      editors:
      - ianuvrat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9675329923629761
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae6006ff15a7e6e63e042b2987d20a5d.svg
          fullname: Anuvrat Shukla
          isHf: false
          isPro: false
          name: ianuvrat
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;edumoulin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/edumoulin\">@<span class=\"\
          underline\">edumoulin</span></a></span>\n\n\t</span></span> , you tested\
          \ in ql code as in? I want to understand how can we use this model to query\
          \ database/csv or panda datafame. I tried with langchain but no luck. Would\
          \ you be so kind to point out the tools/code how to achieve this?  I feel\
          \ that it would be useful to a lot of people. Thank you very much in advance.</p>\n"
        raw: '@edumoulin , you tested in ql code as in? I want to understand how can
          we use this model to query database/csv or panda datafame. I tried with
          langchain but no luck. Would you be so kind to point out the tools/code
          how to achieve this?  I feel that it would be useful to a lot of people.
          Thank you very much in advance.'
        updatedAt: '2023-10-04T03:54:19.127Z'
      numEdits: 0
      reactions: []
    id: 651ce1eb28950a07703566c5
    type: comment
  author: ianuvrat
  content: '@edumoulin , you tested in ql code as in? I want to understand how can
    we use this model to query database/csv or panda datafame. I tried with langchain
    but no luck. Would you be so kind to point out the tools/code how to achieve this?  I
    feel that it would be useful to a lot of people. Thank you very much in advance.'
  created_at: 2023-10-04 02:54:19+00:00
  edited: false
  hidden: false
  id: 651ce1eb28950a07703566c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d71b0b5e00c01a6f06a84beac7fe0a2c.svg
      fullname: Eric DUMOULIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edumoulin
      type: user
    createdAt: '2023-10-04T07:13:45.000Z'
    data:
      edited: false
      editors:
      - edumoulin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9334158301353455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d71b0b5e00c01a6f06a84beac7fe0a2c.svg
          fullname: Eric DUMOULIN
          isHf: false
          isPro: false
          name: edumoulin
          type: user
        html: "<blockquote>\n<p>Is it uncensored ?</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;Akalilol&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Akalilol\">@<span class=\"underline\">Akalilol</span></a></span>\n\
          \n\t</span></span>, this is what they claim on their website, \"It does\
          \ not have any moderation mechanism. We\u2019re looking forward to engaging\
          \ with the community on ways to make the model finely respect guardrails,\
          \ allowing for deployment in environments requiring moderated outputs\"\
          .<br>But I did not try to ask for questionable topics... \U0001F642</p>\n"
        raw: "> Is it uncensored ?\n\n@Akalilol, this is what they claim on their\
          \ website, \"It does not have any moderation mechanism. We\u2019re looking\
          \ forward to engaging with the community on ways to make the model finely\
          \ respect guardrails, allowing for deployment in environments requiring\
          \ moderated outputs\". \nBut I did not try to ask for questionable topics...\
          \ \U0001F642"
        updatedAt: '2023-10-04T07:13:45.517Z'
      numEdits: 0
      reactions: []
    id: 651d10a91114653f30b1d253
    type: comment
  author: edumoulin
  content: "> Is it uncensored ?\n\n@Akalilol, this is what they claim on their website,\
    \ \"It does not have any moderation mechanism. We\u2019re looking forward to engaging\
    \ with the community on ways to make the model finely respect guardrails, allowing\
    \ for deployment in environments requiring moderated outputs\". \nBut I did not\
    \ try to ask for questionable topics... \U0001F642"
  created_at: 2023-10-04 06:13:45+00:00
  edited: false
  hidden: false
  id: 651d10a91114653f30b1d253
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d71b0b5e00c01a6f06a84beac7fe0a2c.svg
      fullname: Eric DUMOULIN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: edumoulin
      type: user
    createdAt: '2023-10-04T07:22:39.000Z'
    data:
      edited: false
      editors:
      - edumoulin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9476670026779175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d71b0b5e00c01a6f06a84beac7fe0a2c.svg
          fullname: Eric DUMOULIN
          isHf: false
          isPro: false
          name: edumoulin
          type: user
        html: "<blockquote>\n<p> you tested in ql code as in? I want to understand\
          \ how can we use this model to query database/csv or panda datafame. </p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;ianuvrat&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ianuvrat\"\
          >@<span class=\"underline\">ianuvrat</span></a></span>\n\n\t</span></span>,\
          \ actually I was only able to test its capabilities in chat mode for writing\
          \ bash scripts and SQL statements, only using natural language. For now,\
          \ it seems not to work in instruct mode, nor does it accept training at\
          \ this time, at least using oobabooga's functions (<a rel=\"nofollow\" href=\"\
          https://github.com/oobabooga/text-generation-webui\">https://github.com/oobabooga/text-generation-webui</a>).\
          \ </p>\n<p>I guess this will change over time as we are currently in 0.1\
          \ version. I'm also curious to go deeper in database exploration</p>\n"
        raw: ">  you tested in ql code as in? I want to understand how can we use\
          \ this model to query database/csv or panda datafame. \n\n@ianuvrat, actually\
          \ I was only able to test its capabilities in chat mode for writing bash\
          \ scripts and SQL statements, only using natural language. For now, it seems\
          \ not to work in instruct mode, nor does it accept training at this time,\
          \ at least using oobabooga's functions (https://github.com/oobabooga/text-generation-webui).\
          \ \n\nI guess this will change over time as we are currently in 0.1 version.\
          \ I'm also curious to go deeper in database exploration"
        updatedAt: '2023-10-04T07:22:39.191Z'
      numEdits: 0
      reactions: []
    id: 651d12bf6d56d105ee903b8a
    type: comment
  author: edumoulin
  content: ">  you tested in ql code as in? I want to understand how can we use this\
    \ model to query database/csv or panda datafame. \n\n@ianuvrat, actually I was\
    \ only able to test its capabilities in chat mode for writing bash scripts and\
    \ SQL statements, only using natural language. For now, it seems not to work in\
    \ instruct mode, nor does it accept training at this time, at least using oobabooga's\
    \ functions (https://github.com/oobabooga/text-generation-webui). \n\nI guess\
    \ this will change over time as we are currently in 0.1 version. I'm also curious\
    \ to go deeper in database exploration"
  created_at: 2023-10-04 06:22:39+00:00
  edited: false
  hidden: false
  id: 651d12bf6d56d105ee903b8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d479aad0ffa2cee9619972d5ecb0f4a1.svg
      fullname: Green
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Colderthanice
      type: user
    createdAt: '2023-10-13T22:31:49.000Z'
    data:
      edited: false
      editors:
      - Colderthanice
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8795061111450195
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d479aad0ffa2cee9619972d5ecb0f4a1.svg
          fullname: Green
          isHf: false
          isPro: false
          name: Colderthanice
          type: user
        html: '<p>Mistral is so good, exceeds expectation.</p>

          '
        raw: Mistral is so good, exceeds expectation.
        updatedAt: '2023-10-13T22:31:49.097Z'
      numEdits: 0
      reactions: []
    id: 6529c555caddc9e608d6e44a
    type: comment
  author: Colderthanice
  content: Mistral is so good, exceeds expectation.
  created_at: 2023-10-13 21:31:49+00:00
  edited: false
  hidden: false
  id: 6529c555caddc9e608d6e44a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6420d6ba69a2c29338817858/aqX0H2R0qQgk2G467PKAG.jpeg?w=200&h=200&f=face
      fullname: Abhi Patel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abhirajeshbhai
      type: user
    createdAt: '2023-10-14T17:42:19.000Z'
    data:
      edited: false
      editors:
      - abhirajeshbhai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9437720775604248
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6420d6ba69a2c29338817858/aqX0H2R0qQgk2G467PKAG.jpeg?w=200&h=200&f=face
          fullname: Abhi Patel
          isHf: false
          isPro: false
          name: abhirajeshbhai
          type: user
        html: '<p>I have a question, I am trying to generate a poem, but it only generates
          half poems.. How do i make it to generate full poems?? </p>

          '
        raw: 'I have a question, I am trying to generate a poem, but it only generates
          half poems.. How do i make it to generate full poems?? '
        updatedAt: '2023-10-14T17:42:19.405Z'
      numEdits: 0
      reactions: []
    id: 652ad2fb90e317f2437aaa53
    type: comment
  author: abhirajeshbhai
  content: 'I have a question, I am trying to generate a poem, but it only generates
    half poems.. How do i make it to generate full poems?? '
  created_at: 2023-10-14 16:42:19+00:00
  edited: false
  hidden: false
  id: 652ad2fb90e317f2437aaa53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/217f2e89fd5899e1b68ccfe51202e95a.svg
      fullname: Vinay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: whoknowsmeinhf
      type: user
    createdAt: '2023-10-17T10:59:01.000Z'
    data:
      edited: false
      editors:
      - whoknowsmeinhf
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9382248520851135
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/217f2e89fd5899e1b68ccfe51202e95a.svg
          fullname: Vinay
          isHf: false
          isPro: false
          name: whoknowsmeinhf
          type: user
        html: '<p>It''s responses are good. Not sure why it''s performing poorly for
          CoVe (Chain of verification) to minimize hallucinations </p>

          <ul>

          <li><a rel="nofollow" href="https://github.com/jagilley/fact-checker">https://github.com/jagilley/fact-checker</a></li>

          </ul>

          <p>Any suggestions ?</p>

          '
        raw: "It's responses are good. Not sure why it's performing poorly for CoVe\
          \ (Chain of verification) to minimize hallucinations \n\n- https://github.com/jagilley/fact-checker\n\
          \nAny suggestions ?"
        updatedAt: '2023-10-17T10:59:01.641Z'
      numEdits: 0
      reactions: []
    id: 652e68f57b0079ff0345e1ee
    type: comment
  author: whoknowsmeinhf
  content: "It's responses are good. Not sure why it's performing poorly for CoVe\
    \ (Chain of verification) to minimize hallucinations \n\n- https://github.com/jagilley/fact-checker\n\
    \nAny suggestions ?"
  created_at: 2023-10-17 09:59:01+00:00
  edited: false
  hidden: false
  id: 652e68f57b0079ff0345e1ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: This model is amazingly good
