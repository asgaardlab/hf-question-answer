!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philgrey
conflicting_files: null
created_at: 2023-11-13 02:28:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
      fullname: John Grey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philgrey
      type: user
    createdAt: '2023-11-13T02:28:44.000Z'
    data:
      edited: false
      editors:
      - philgrey
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6358187198638916
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ixKl5PVwauxFVuRMwv-ON.jpeg?w=200&h=200&f=face
          fullname: John Grey
          isHf: false
          isPro: false
          name: philgrey
          type: user
        html: '<p>llm = LlamaCpp(<br>    temperature=0,<br>    model_path = "mistral-7b-instruct-v0.1.Q4_K_M.gguf",<br>    max_tokens=32,<br>    stop=["Q:",
          "\n"],<br>)</p>

          <p>My script is like above. I''ve downloaded model using following script</p>

          <p>downloader = AutoModelForCausalLM.from_pretrained("TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
          model_file="mistral-7b-instruct-v0.1.Q4_K_M.gguf", model_type="llama")</p>

          <p>I''ve saved gguf model in the same folder with my script but, getting
          following error</p>

          <p>Exception has occurred: ValidationError<br>1 validation error for LlamaCpp<br><strong>root</strong><br>  Could
          not load Llama model from path: mistral-7b-instruct-v0.1.Q4_K_M.gguf. Received
          error Model path does not exist: mistral-7b-instruct-v0.1.Q4_K_M.gguf (type=value_error)<br>  File
          "E:\work\Daily\11_3\dragon\ConvM\llama.py", line 22, in <br>    llm = LlamaCpp(<br>pydantic.v1.error_wrappers.ValidationError:
          1 validation error for LlamaCpp<br><strong>root</strong><br>  Could not
          load Llama model from path: mistral-7b-instruct-v0.1.Q4_K_M.gguf. Received
          error Model path does not exist: mistral-7b-instruct-v0.1.Q4_K_M.gguf (type=value_error)</p>

          '
        raw: "llm = LlamaCpp(\r\n    temperature=0,\r\n    model_path = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
          ,\r\n    max_tokens=32,\r\n    stop=[\"Q:\", \"\\n\"],\r\n)\r\n\r\nMy script\
          \ is like above. I've downloaded model using following script\r\n\r\ndownloader\
          \ = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          , model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"llama\"\
          )\r\n\r\nI've saved gguf model in the same folder with my script but, getting\
          \ following error\r\n\r\nException has occurred: ValidationError\r\n1 validation\
          \ error for LlamaCpp\r\n__root__\r\n  Could not load Llama model from path:\
          \ mistral-7b-instruct-v0.1.Q4_K_M.gguf. Received error Model path does not\
          \ exist: mistral-7b-instruct-v0.1.Q4_K_M.gguf (type=value_error)\r\n  File\
          \ \"E:\\work\\Daily\\11_3\\dragon\\ConvM\\llama.py\", line 22, in <module>\r\
          \n    llm = LlamaCpp(\r\npydantic.v1.error_wrappers.ValidationError: 1 validation\
          \ error for LlamaCpp\r\n__root__\r\n  Could not load Llama model from path:\
          \ mistral-7b-instruct-v0.1.Q4_K_M.gguf. Received error Model path does not\
          \ exist: mistral-7b-instruct-v0.1.Q4_K_M.gguf (type=value_error)"
        updatedAt: '2023-11-13T02:28:45.000Z'
      numEdits: 0
      reactions: []
    id: 655189dc35d81e51531b82d2
    type: comment
  author: philgrey
  content: "llm = LlamaCpp(\r\n    temperature=0,\r\n    model_path = \"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
    ,\r\n    max_tokens=32,\r\n    stop=[\"Q:\", \"\\n\"],\r\n)\r\n\r\nMy script is\
    \ like above. I've downloaded model using following script\r\n\r\ndownloader =\
    \ AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
    , model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"llama\")\r\
    \n\r\nI've saved gguf model in the same folder with my script but, getting following\
    \ error\r\n\r\nException has occurred: ValidationError\r\n1 validation error for\
    \ LlamaCpp\r\n__root__\r\n  Could not load Llama model from path: mistral-7b-instruct-v0.1.Q4_K_M.gguf.\
    \ Received error Model path does not exist: mistral-7b-instruct-v0.1.Q4_K_M.gguf\
    \ (type=value_error)\r\n  File \"E:\\work\\Daily\\11_3\\dragon\\ConvM\\llama.py\"\
    , line 22, in <module>\r\n    llm = LlamaCpp(\r\npydantic.v1.error_wrappers.ValidationError:\
    \ 1 validation error for LlamaCpp\r\n__root__\r\n  Could not load Llama model\
    \ from path: mistral-7b-instruct-v0.1.Q4_K_M.gguf. Received error Model path does\
    \ not exist: mistral-7b-instruct-v0.1.Q4_K_M.gguf (type=value_error)"
  created_at: 2023-11-13 02:28:44+00:00
  edited: false
  hidden: false
  id: 655189dc35d81e51531b82d2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: Can't use downloaded model
