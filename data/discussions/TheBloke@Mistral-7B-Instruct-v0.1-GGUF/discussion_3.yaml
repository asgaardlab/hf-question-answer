!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Harm
conflicting_files: null
created_at: 2023-10-03 10:53:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
      fullname: Harm Buisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Harm
      type: user
    createdAt: '2023-10-03T11:53:12.000Z'
    data:
      edited: true
      editors:
      - Harm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8018880486488342
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
          fullname: Harm Buisman
          isHf: false
          isPro: false
          name: Harm
          type: user
        html: '<p>The Mistral-7B-Instruct-v0.1-GGUF card mentions "The model will
          work at sequence lengths of 4096, or lower."</p>

          <p>But when I import the model it only seems to support max context length
          of 512.<br>model._llm.context_length --&gt; 512</p>

          <p>When I run a larger prompt I get:<br>WARNING:ctransformers:Number of
          tokens (850) exceeded maximum context length (512).</p>

          <p>How can I utilize the longer context length for the Mistral-7B-Instruct-v0.1-GGUF
          model?</p>

          '
        raw: 'The Mistral-7B-Instruct-v0.1-GGUF card mentions "The model will work
          at sequence lengths of 4096, or lower."


          But when I import the model it only seems to support max context length
          of 512.

          model._llm.context_length --> 512


          When I run a larger prompt I get:

          WARNING:ctransformers:Number of tokens (850) exceeded maximum context length
          (512).


          How can I utilize the longer context length for the Mistral-7B-Instruct-v0.1-GGUF
          model?'
        updatedAt: '2023-10-03T12:33:52.071Z'
      numEdits: 3
      reactions: []
    id: 651c00a8f25d16c5af5f0fca
    type: comment
  author: Harm
  content: 'The Mistral-7B-Instruct-v0.1-GGUF card mentions "The model will work at
    sequence lengths of 4096, or lower."


    But when I import the model it only seems to support max context length of 512.

    model._llm.context_length --> 512


    When I run a larger prompt I get:

    WARNING:ctransformers:Number of tokens (850) exceeded maximum context length (512).


    How can I utilize the longer context length for the Mistral-7B-Instruct-v0.1-GGUF
    model?'
  created_at: 2023-10-03 10:53:12+00:00
  edited: true
  hidden: false
  id: 651c00a8f25d16c5af5f0fca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-10-03T12:01:27.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8890640735626221
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>You have to manually set it up. It\u2019s normally set up for all\
          \ models ans 512. Also, it should support around 8k context lenght(slightly\
          \ lower).</p>\n"
        raw: "You have to manually set it up. It\u2019s normally set up for all models\
          \ ans 512. Also, it should support around 8k context lenght(slightly lower)."
        updatedAt: '2023-10-03T12:01:27.791Z'
      numEdits: 0
      reactions: []
    id: 651c0297ca84a54d9cf8dcdf
    type: comment
  author: YaTharThShaRma999
  content: "You have to manually set it up. It\u2019s normally set up for all models\
    \ ans 512. Also, it should support around 8k context lenght(slightly lower)."
  created_at: 2023-10-03 11:01:27+00:00
  edited: false
  hidden: false
  id: 651c0297ca84a54d9cf8dcdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
      fullname: Harm Buisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Harm
      type: user
    createdAt: '2023-10-03T12:03:50.000Z'
    data:
      edited: false
      editors:
      - Harm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9027373194694519
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
          fullname: Harm Buisman
          isHf: false
          isPro: false
          name: Harm
          type: user
        html: "<blockquote>\n<p>You have to manually set it up. It\u2019s normally\
          \ set up for all models ans 512. Also, it should support around 8k context\
          \ lenght(slightly lower).</p>\n</blockquote>\n<p>Ok, do you have any suggestions\
          \ or pointers how to do so?</p>\n"
        raw: "> You have to manually set it up. It\u2019s normally set up for all\
          \ models ans 512. Also, it should support around 8k context lenght(slightly\
          \ lower).\n\nOk, do you have any suggestions or pointers how to do so?"
        updatedAt: '2023-10-03T12:03:50.921Z'
      numEdits: 0
      reactions: []
    id: 651c03266794263227fdb89d
    type: comment
  author: Harm
  content: "> You have to manually set it up. It\u2019s normally set up for all models\
    \ ans 512. Also, it should support around 8k context lenght(slightly lower).\n\
    \nOk, do you have any suggestions or pointers how to do so?"
  created_at: 2023-10-03 11:03:50+00:00
  edited: false
  hidden: false
  id: 651c03266794263227fdb89d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13160c36f4ee06ea2c8aa50a66114c35.svg
      fullname: Asfand Saleem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asfandsaleem
      type: user
    createdAt: '2023-10-03T12:10:59.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/13160c36f4ee06ea2c8aa50a66114c35.svg
          fullname: Asfand Saleem
          isHf: false
          isPro: false
          name: asfandsaleem
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-03T12:39:33.974Z'
      numEdits: 0
      reactions: []
    id: 651c04d39571da71a4db706e
    type: comment
  author: asfandsaleem
  content: This comment has been hidden
  created_at: 2023-10-03 11:10:59+00:00
  edited: true
  hidden: true
  id: 651c04d39571da71a4db706e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/820472235f43fdd590bb0a7ce1603dd6.svg
      fullname: Marek Kerka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: marekk
      type: user
    createdAt: '2023-10-03T12:15:05.000Z'
    data:
      edited: false
      editors:
      - marekk
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5583492517471313
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/820472235f43fdd590bb0a7ce1603dd6.svg
          fullname: Marek Kerka
          isHf: false
          isPro: false
          name: marekk
          type: user
        html: '<p>You can use </p>

          <pre><code>pip install llama-cpp-python

          wget https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q5_K_M.gguf

          </code></pre>

          <p>And after this for example:</p>

          <pre><code>from llama_cpp import Llama

          llm = Llama(model_path="wizardlm-13b-v1.2.Q5_K_M.gguf", n_ctx=4096, n_gpu_layers=-1)

          print(llm(prompt, max_tokens=1024, temperature=0))

          </code></pre>

          <p>Just change name of model and path.</p>

          '
        raw: "You can use \n```\npip install llama-cpp-python\nwget https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q5_K_M.gguf\n\
          ```\nAnd after this for example:\n```\nfrom llama_cpp import Llama\nllm\
          \ = Llama(model_path=\"wizardlm-13b-v1.2.Q5_K_M.gguf\", n_ctx=4096, n_gpu_layers=-1)\n\
          print(llm(prompt, max_tokens=1024, temperature=0))\n```\nJust change name\
          \ of model and path."
        updatedAt: '2023-10-03T12:15:05.689Z'
      numEdits: 0
      reactions: []
    id: 651c05c93265a1bb83b128e5
    type: comment
  author: marekk
  content: "You can use \n```\npip install llama-cpp-python\nwget https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GGUF/resolve/main/wizardlm-13b-v1.2.Q5_K_M.gguf\n\
    ```\nAnd after this for example:\n```\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"\
    wizardlm-13b-v1.2.Q5_K_M.gguf\", n_ctx=4096, n_gpu_layers=-1)\nprint(llm(prompt,\
    \ max_tokens=1024, temperature=0))\n```\nJust change name of model and path."
  created_at: 2023-10-03 11:15:05+00:00
  edited: false
  hidden: false
  id: 651c05c93265a1bb83b128e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
      fullname: Harm Buisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Harm
      type: user
    createdAt: '2023-10-03T12:42:32.000Z'
    data:
      edited: true
      editors:
      - Harm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7427700757980347
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
          fullname: Harm Buisman
          isHf: false
          isPro: false
          name: Harm
          type: user
        html: '<p>Everyone thanks for the suggestions. Was just pointed to the context_length
          parameter from ctransformers. Context length is upgraded to 4096 by:<br>from
          ctransformers import AutoModelForCausalLM</p>

          <p>model = AutoModelForCausalLM.from_pretrained(<br>    "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",<br>    model_file="mistral-7b-instruct-v0.1.Q4_K_M.gguf",<br>    model_type="mistral",<br>    gpu_layers=50,<br>    hf=True,<br>    context_length=4096)</p>

          '
        raw: "Everyone thanks for the suggestions. Was just pointed to the context_length\
          \ parameter from ctransformers. Context length is upgraded to 4096 by:\n\
          from ctransformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
          ,\n    model_type=\"mistral\",\n    gpu_layers=50,\n    hf=True,\n    context_length=4096)\n"
        updatedAt: '2023-10-03T12:42:57.411Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Vincent6m
      relatedEventId: 651c0c388bf8bf53260efe63
    id: 651c0c388bf8bf53260efe61
    type: comment
  author: Harm
  content: "Everyone thanks for the suggestions. Was just pointed to the context_length\
    \ parameter from ctransformers. Context length is upgraded to 4096 by:\nfrom ctransformers\
    \ import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n    model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\
    ,\n    model_type=\"mistral\",\n    gpu_layers=50,\n    hf=True,\n    context_length=4096)\n"
  created_at: 2023-10-03 11:42:32+00:00
  edited: true
  hidden: false
  id: 651c0c388bf8bf53260efe61
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d4f2327c8a02c325382d211c6f7fd874.svg
      fullname: Harm Buisman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Harm
      type: user
    createdAt: '2023-10-03T12:42:32.000Z'
    data:
      status: closed
    id: 651c0c388bf8bf53260efe63
    type: status-change
  author: Harm
  created_at: 2023-10-03 11:42:32+00:00
  id: 651c0c388bf8bf53260efe63
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: closed
target_branch: null
title: How to get up to 4096 context length?
