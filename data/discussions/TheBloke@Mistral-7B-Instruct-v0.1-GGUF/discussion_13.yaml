!!python/object:huggingface_hub.community.DiscussionWithDetails
author: shivammehta
conflicting_files: null
created_at: 2023-11-17 11:46:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2e441d2ae599b51824fb9db8d2a89fff.svg
      fullname: mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shivammehta
      type: user
    createdAt: '2023-11-17T11:46:59.000Z'
    data:
      edited: true
      editors:
      - shivammehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.96849524974823
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2e441d2ae599b51824fb9db8d2a89fff.svg
          fullname: mehta
          isHf: false
          isPro: false
          name: shivammehta
          type: user
        html: '<p>When experimenting with this model, I''ve observed occasional discrepancies
          in its output. Sometimes it provides the correct response, and sometimes
          times it doesn''t, even when presented with the same or similar questions.
          I have two inquiries: Why does this occur, and how can we address this issue?<br>Like
          Output Arrives at the correct Answer. (Occasionally does not arrive at the
          correct answer, the behavior is not 100% predictable.</p>

          <p>Code -<br>from huggingface_hub import hf_hub_download<br>from langchain.llms
          import LlamaCpp</p>

          <p>MODEL_ID = "TheBloke/Mistral-7B-Instruct-v0.1-GGUF""</p>

          <p>CONTEXT_WINDOW_SIZE = 4096<br>MAX_NEW_TOKENS = 1024</p>

          <p>model_path = hf_hub_download(<br>            repo_id=MODEL_ID,<br>            filename=MODEL_BASENAME,<br>            resume_download=True,<br>            cache_dir="./models",<br>        )<br>kwargs
          = {<br>            "model_path": model_path,<br>            "n_ctx": CONTEXT_WINDOW_SIZE,<br>            "max_tokens":
          MAX_NEW_TOKENS,<br>            "n_gpu_layers":4<br>        }<br>llm = LlamaCpp(<br>    model_path=model_path,<br>    temperature=0.1,<br>    n_ctx=4096,<br>    max_tokens=1024,<br>    n_batch=100,<br>    top_p=1,<br>    verbose=True,<br>    n_gpu_layers=100)
          </p>

          <p>agent = create_csv_agent(llm, [''./Data/Employees.csv'',''./Data/Verticals.csv''],
          verbose=True)<br>response = agent.run("Which vertical name has the most
          number of resignations")<br>print(response)</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/QbGzalg9eRCB9WwyKTmYD.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/QbGzalg9eRCB9WwyKTmYD.png"></a></p>

          <p>QUERY:</p>

          <ol>

          <li>What are the steps / measures to be taken to ensure that there is consistency
          in the final answer?</li>

          <li>We have observed on various runs the paths to arriving at the final
          answer keep changing. Assuming the initial path that is chosen is a wrong
          path, how to ensure that the reasoning of the LLM takes corrective measures
          to finally arrive at the correct answer? (We have seen sometimes these corrective
          actions are infact taken..)</li>

          </ol>

          '
        raw: "When experimenting with this model, I've observed occasional discrepancies\
          \ in its output. Sometimes it provides the correct response, and sometimes\
          \ times it doesn't, even when presented with the same or similar questions.\
          \ I have two inquiries: Why does this occur, and how can we address this\
          \ issue?\nLike Output Arrives at the correct Answer. (Occasionally does\
          \ not arrive at the correct answer, the behavior is not 100% predictable.\n\
          \nCode -\nfrom huggingface_hub import hf_hub_download\nfrom langchain.llms\
          \ import LlamaCpp\n\nMODEL_ID = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          \"\n\nCONTEXT_WINDOW_SIZE = 4096\nMAX_NEW_TOKENS = 1024\n\nmodel_path =\
          \ hf_hub_download(\n            repo_id=MODEL_ID,\n            filename=MODEL_BASENAME,\n\
          \            resume_download=True,\n            cache_dir=\"./models\",\n\
          \        )\nkwargs = {\n            \"model_path\": model_path,\n      \
          \      \"n_ctx\": CONTEXT_WINDOW_SIZE,\n            \"max_tokens\": MAX_NEW_TOKENS,\n\
          \            \"n_gpu_layers\":4\n        }\nllm = LlamaCpp(\n    model_path=model_path,\n\
          \    temperature=0.1,\n    n_ctx=4096,\n    max_tokens=1024,\n    n_batch=100,\n\
          \    top_p=1,\n    verbose=True,\n    n_gpu_layers=100) \n\nagent = create_csv_agent(llm,\
          \ ['./Data/Employees.csv','./Data/Verticals.csv'], verbose=True)\nresponse\
          \ = agent.run(\"Which vertical name has the most number of resignations\"\
          )\nprint(response)\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/QbGzalg9eRCB9WwyKTmYD.png)\n\
          \nQUERY:\n1. What are the steps / measures to be taken to ensure that there\
          \ is consistency in the final answer?\n2. We have observed on various runs\
          \ the paths to arriving at the final answer keep changing. Assuming the\
          \ initial path that is chosen is a wrong path, how to ensure that the reasoning\
          \ of the LLM takes corrective measures to finally arrive at the correct\
          \ answer? (We have seen sometimes these corrective actions are infact taken..)"
        updatedAt: '2023-11-17T12:06:31.402Z'
      numEdits: 2
      reactions: []
    id: 655752b3997e6f9682e83b4d
    type: comment
  author: shivammehta
  content: "When experimenting with this model, I've observed occasional discrepancies\
    \ in its output. Sometimes it provides the correct response, and sometimes times\
    \ it doesn't, even when presented with the same or similar questions. I have two\
    \ inquiries: Why does this occur, and how can we address this issue?\nLike Output\
    \ Arrives at the correct Answer. (Occasionally does not arrive at the correct\
    \ answer, the behavior is not 100% predictable.\n\nCode -\nfrom huggingface_hub\
    \ import hf_hub_download\nfrom langchain.llms import LlamaCpp\n\nMODEL_ID = \"\
    TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\"\n\nCONTEXT_WINDOW_SIZE = 4096\nMAX_NEW_TOKENS\
    \ = 1024\n\nmodel_path = hf_hub_download(\n            repo_id=MODEL_ID,\n   \
    \         filename=MODEL_BASENAME,\n            resume_download=True,\n      \
    \      cache_dir=\"./models\",\n        )\nkwargs = {\n            \"model_path\"\
    : model_path,\n            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n            \"max_tokens\"\
    : MAX_NEW_TOKENS,\n            \"n_gpu_layers\":4\n        }\nllm = LlamaCpp(\n\
    \    model_path=model_path,\n    temperature=0.1,\n    n_ctx=4096,\n    max_tokens=1024,\n\
    \    n_batch=100,\n    top_p=1,\n    verbose=True,\n    n_gpu_layers=100) \n\n\
    agent = create_csv_agent(llm, ['./Data/Employees.csv','./Data/Verticals.csv'],\
    \ verbose=True)\nresponse = agent.run(\"Which vertical name has the most number\
    \ of resignations\")\nprint(response)\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6453a499fc2b5f69e8fb0fde/QbGzalg9eRCB9WwyKTmYD.png)\n\
    \nQUERY:\n1. What are the steps / measures to be taken to ensure that there is\
    \ consistency in the final answer?\n2. We have observed on various runs the paths\
    \ to arriving at the final answer keep changing. Assuming the initial path that\
    \ is chosen is a wrong path, how to ensure that the reasoning of the LLM takes\
    \ corrective measures to finally arrive at the correct answer? (We have seen sometimes\
    \ these corrective actions are infact taken..)"
  created_at: 2023-11-17 11:46:59+00:00
  edited: true
  hidden: false
  id: 655752b3997e6f9682e83b4d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: 'Addressing Inconsistencies in Model Outputs: Understanding and Solutions'
