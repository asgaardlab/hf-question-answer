!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lazyDataScientist
conflicting_files: null
created_at: 2023-10-04 14:35:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
      fullname: Cedrick Hesketh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lazyDataScientist
      type: user
    createdAt: '2023-10-04T15:35:30.000Z'
    data:
      edited: false
      editors:
      - lazyDataScientist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47423699498176575
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a39ec8f27255b6b571101/7J_BcRm7ua0WZNIGwEzlo.png?w=200&h=200&f=face
          fullname: Cedrick Hesketh
          isHf: false
          isPro: false
          name: lazyDataScientist
          type: user
        html: "<p>After running this code I am receiving this error. Note: i am running\
          \ this in a CoLab environment.</p>\n<pre><code>from ctransformers import\
          \ AutoModelForCausalLM\n\n# Set gpu_layers to the number of layers to offload\
          \ to GPU. Set to 0 if no GPU acceleration is available on your system.\n\
          llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          , model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\", model_type=\"mistral\"\
          , gpu_layers=50)\n</code></pre>\n<pre><code>---------------------------------------------------------------------------\n\
          OSError                                   Traceback (most recent call last)\n\
          &lt;ipython-input-7-a38c39cda463&gt; in &lt;cell line: 4&gt;()\n      2\
          \ \n      3 # Set gpu_layers to the number of layers to offload to GPU.\
          \ Set to 0 if no GPU acceleration is available on your system.\n----&gt;\
          \ 4 llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          , model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\", model_type=\"mistral\"\
          , gpu_layers=50)\n\n3 frames\n/usr/local/lib/python3.10/dist-packages/ctransformers/hub.py\
          \ in from_pretrained(cls, model_path_or_repo_id, model_type, model_file,\
          \ config, lib, local_files_only, revision, hf, **kwargs)\n    173      \
          \       )\n    174 \n--&gt; 175         llm = LLM(\n    176            \
          \ model_path=model_path,\n    177             model_type=model_type,\n\n\
          /usr/local/lib/python3.10/dist-packages/ctransformers/llm.py in __init__(self,\
          \ model_path, model_type, config, lib)\n    244             model_type =\
          \ \"gguf\"\n    245 \n--&gt; 246         self._lib = load_library(lib, gpu=config.gpu_layers\
          \ &gt; 0)\n    247         self._llm = self._lib.ctransformers_llm_create(\n\
          \    248             model_path.encode(),\n\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\
          \ in load_library(path, gpu)\n    124     if \"cuda\" in path:\n    125\
          \         load_cuda()\n--&gt; 126     lib = CDLL(path)\n    127 \n    128\
          \     lib.ctransformers_llm_create.argtypes = [\n\n/usr/lib/python3.10/ctypes/__init__.py\
          \ in __init__(self, name, mode, handle, use_errno, use_last_error, winmode)\n\
          \    372 \n    373         if handle is None:\n--&gt; 374             self._handle\
          \ = _dlopen(self._name, mode)\n    375         else:\n    376          \
          \   self._handle = handle\n\nOSError: libcudart.so.12: cannot open shared\
          \ object file: No such file or directory\n</code></pre>\n"
        raw: "After running this code I am receiving this error. Note: i am running\
          \ this in a CoLab environment.\r\n```\r\nfrom ctransformers import AutoModelForCausalLM\r\
          \n\r\n# Set gpu_layers to the number of layers to offload to GPU. Set to\
          \ 0 if no GPU acceleration is available on your system.\r\nllm = AutoModelForCausalLM.from_pretrained(\"\
          TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\"\
          , model_type=\"mistral\", gpu_layers=50)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\
          \nOSError                                   Traceback (most recent call\
          \ last)\r\n<ipython-input-7-a38c39cda463> in <cell line: 4>()\r\n      2\
          \ \r\n      3 # Set gpu_layers to the number of layers to offload to GPU.\
          \ Set to 0 if no GPU acceleration is available on your system.\r\n---->\
          \ 4 llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\
          , model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\", model_type=\"mistral\"\
          , gpu_layers=50)\r\n\r\n3 frames\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/hub.py\
          \ in from_pretrained(cls, model_path_or_repo_id, model_type, model_file,\
          \ config, lib, local_files_only, revision, hf, **kwargs)\r\n    173    \
          \         )\r\n    174 \r\n--> 175         llm = LLM(\r\n    176       \
          \      model_path=model_path,\r\n    177             model_type=model_type,\r\
          \n\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py in __init__(self,\
          \ model_path, model_type, config, lib)\r\n    244             model_type\
          \ = \"gguf\"\r\n    245 \r\n--> 246         self._lib = load_library(lib,\
          \ gpu=config.gpu_layers > 0)\r\n    247         self._llm = self._lib.ctransformers_llm_create(\r\
          \n    248             model_path.encode(),\r\n\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\
          \ in load_library(path, gpu)\r\n    124     if \"cuda\" in path:\r\n   \
          \ 125         load_cuda()\r\n--> 126     lib = CDLL(path)\r\n    127 \r\n\
          \    128     lib.ctransformers_llm_create.argtypes = [\r\n\r\n/usr/lib/python3.10/ctypes/__init__.py\
          \ in __init__(self, name, mode, handle, use_errno, use_last_error, winmode)\r\
          \n    372 \r\n    373         if handle is None:\r\n--> 374            \
          \ self._handle = _dlopen(self._name, mode)\r\n    375         else:\r\n\
          \    376             self._handle = handle\r\n\r\nOSError: libcudart.so.12:\
          \ cannot open shared object file: No such file or directory\r\n```"
        updatedAt: '2023-10-04T15:35:30.855Z'
      numEdits: 0
      reactions: []
    id: 651d8642bd6eae2bc3ffdff3
    type: comment
  author: lazyDataScientist
  content: "After running this code I am receiving this error. Note: i am running\
    \ this in a CoLab environment.\r\n```\r\nfrom ctransformers import AutoModelForCausalLM\r\
    \n\r\n# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if\
    \ no GPU acceleration is available on your system.\r\nllm = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\"\
    , model_type=\"mistral\", gpu_layers=50)\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\
    \nOSError                                   Traceback (most recent call last)\r\
    \n<ipython-input-7-a38c39cda463> in <cell line: 4>()\r\n      2 \r\n      3 #\
    \ Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU\
    \ acceleration is available on your system.\r\n----> 4 llm = AutoModelForCausalLM.from_pretrained(\"\
    TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q2_K.gguf\"\
    , model_type=\"mistral\", gpu_layers=50)\r\n\r\n3 frames\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/hub.py\
    \ in from_pretrained(cls, model_path_or_repo_id, model_type, model_file, config,\
    \ lib, local_files_only, revision, hf, **kwargs)\r\n    173             )\r\n\
    \    174 \r\n--> 175         llm = LLM(\r\n    176             model_path=model_path,\r\
    \n    177             model_type=model_type,\r\n\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\
    \ in __init__(self, model_path, model_type, config, lib)\r\n    244          \
    \   model_type = \"gguf\"\r\n    245 \r\n--> 246         self._lib = load_library(lib,\
    \ gpu=config.gpu_layers > 0)\r\n    247         self._llm = self._lib.ctransformers_llm_create(\r\
    \n    248             model_path.encode(),\r\n\r\n/usr/local/lib/python3.10/dist-packages/ctransformers/llm.py\
    \ in load_library(path, gpu)\r\n    124     if \"cuda\" in path:\r\n    125  \
    \       load_cuda()\r\n--> 126     lib = CDLL(path)\r\n    127 \r\n    128   \
    \  lib.ctransformers_llm_create.argtypes = [\r\n\r\n/usr/lib/python3.10/ctypes/__init__.py\
    \ in __init__(self, name, mode, handle, use_errno, use_last_error, winmode)\r\n\
    \    372 \r\n    373         if handle is None:\r\n--> 374             self._handle\
    \ = _dlopen(self._name, mode)\r\n    375         else:\r\n    376            \
    \ self._handle = handle\r\n\r\nOSError: libcudart.so.12: cannot open shared object\
    \ file: No such file or directory\r\n```"
  created_at: 2023-10-04 14:35:30+00:00
  edited: false
  hidden: false
  id: 651d8642bd6eae2bc3ffdff3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lB_IN3agrUh49Ta5OcMg4.png?w=200&h=200&f=face
      fullname: "Marco Louren\xE7o"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: esuriddick
      type: user
    createdAt: '2023-10-24T08:35:47.000Z'
    data:
      edited: false
      editors:
      - esuriddick
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8630456924438477
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lB_IN3agrUh49Ta5OcMg4.png?w=200&h=200&f=face
          fullname: "Marco Louren\xE7o"
          isHf: false
          isPro: false
          name: esuriddick
          type: user
        html: '<p>I''m getting a different error while running ctransformers in my
          laptop: "Model type ''mistral'' is not supported."</p>

          <p>I''ve used falcon 7b and wizardlm 7b in the past with the exact same
          setup. Not sure whether ctransformers is actually compatible with this model
          or not :s</p>

          '
        raw: 'I''m getting a different error while running ctransformers in my laptop:
          "Model type ''mistral'' is not supported."


          I''ve used falcon 7b and wizardlm 7b in the past with the exact same setup.
          Not sure whether ctransformers is actually compatible with this model or
          not :s'
        updatedAt: '2023-10-24T08:35:47.328Z'
      numEdits: 0
      reactions: []
    id: 653781e35b08fc7fd59d0170
    type: comment
  author: esuriddick
  content: 'I''m getting a different error while running ctransformers in my laptop:
    "Model type ''mistral'' is not supported."


    I''ve used falcon 7b and wizardlm 7b in the past with the exact same setup. Not
    sure whether ctransformers is actually compatible with this model or not :s'
  created_at: 2023-10-24 07:35:47+00:00
  edited: false
  hidden: false
  id: 653781e35b08fc7fd59d0170
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
      fullname: Boddu Surya Venkat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MLconArtist
      type: user
    createdAt: '2023-10-26T09:20:49.000Z'
    data:
      edited: false
      editors:
      - MLconArtist
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7230457067489624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/64cabba5300d22ba20b05b32c09dee6c.svg
          fullname: Boddu Surya Venkat
          isHf: false
          isPro: false
          name: MLconArtist
          type: user
        html: '<p>I have a similar issue.</p>

          <pre><code>RuntimeError: Failed to create LLM ''mistral'' from ''Models\mistral-7b-v0.1.Q4_K_M.gguf''

          </code></pre>

          '
        raw: "I have a similar issue.\n\n    RuntimeError: Failed to create LLM 'mistral'\
          \ from 'Models\\mistral-7b-v0.1.Q4_K_M.gguf'"
        updatedAt: '2023-10-26T09:20:49.765Z'
      numEdits: 0
      reactions: []
    id: 653a2f71863b9327f60dd855
    type: comment
  author: MLconArtist
  content: "I have a similar issue.\n\n    RuntimeError: Failed to create LLM 'mistral'\
    \ from 'Models\\mistral-7b-v0.1.Q4_K_M.gguf'"
  created_at: 2023-10-26 08:20:49+00:00
  edited: false
  hidden: false
  id: 653a2f71863b9327f60dd855
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lB_IN3agrUh49Ta5OcMg4.png?w=200&h=200&f=face
      fullname: "Marco Louren\xE7o"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: esuriddick
      type: user
    createdAt: '2023-12-12T10:26:50.000Z'
    data:
      edited: false
      editors:
      - esuriddick
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9478628635406494
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/lB_IN3agrUh49Ta5OcMg4.png?w=200&h=200&f=face
          fullname: "Marco Louren\xE7o"
          isHf: false
          isPro: false
          name: esuriddick
          type: user
        html: '<p>After updating CTransformers, the issue was solved on my side (apologies
          for delay on reply).</p>

          '
        raw: After updating CTransformers, the issue was solved on my side (apologies
          for delay on reply).
        updatedAt: '2023-12-12T10:26:50.819Z'
      numEdits: 0
      reactions: []
    id: 6578356aa87010c9f882601f
    type: comment
  author: esuriddick
  content: After updating CTransformers, the issue was solved on my side (apologies
    for delay on reply).
  created_at: 2023-12-12 10:26:50+00:00
  edited: false
  hidden: false
  id: 6578356aa87010c9f882601f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Mistral-7B-Instruct-v0.1-GGUF
repo_type: model
status: open
target_branch: null
title: 'ctransformers: OSError No such file or directory issue'
