!!python/object:huggingface_hub.community.DiscussionWithDetails
author: streatycat
conflicting_files: null
created_at: 2023-11-24 04:38:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b11d8ef6763e93b8f07275db1d3437e.svg
      fullname: streatycat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: streatycat
      type: user
    createdAt: '2023-11-24T04:38:09.000Z'
    data:
      edited: false
      editors:
      - streatycat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5720685720443726
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b11d8ef6763e93b8f07275db1d3437e.svg
          fullname: streatycat
          isHf: false
          isPro: false
          name: streatycat
          type: user
        html: "<p>[I found that llama.cpp already supports mpt, I downloaded <a href=\"\
          https://huggingface.co/maddes8cht/mosaicml-mpt-30b-chat-gguf/resolve/main/mosaicml-mpt-30b-chat-Q8_0.gguf\"\
          >gguf</a> from here, and it did load it with llama.cpp, but its return result\
          \ looks bad.</p>\n<p>I start the server as follow:</p>\n<pre><code>git clone\
          \ https://github.com/abetlen/llama-cpp-python.git\ncd llama-cpp-python\n\
          docker build -t llama-cpp-python-cuda docker/cuda_simple/\ndocker run --gpus\
          \ all --rm -it -p 8000:8000 -v ${/path/to/models}:/models -e MODEL=/models/${model-filename}\
          \ llama-cpp-python-cuda python3 -m llama_cpp.server --n_gpu_layers ${X}\
          \ --n_ctx ${L}\n</code></pre>\n<p>And I post the request as follow:</p>\n\
          <pre><code>URL: http://localhost:8000/v1/chat/completions\nBODY:\n\n{\n\"\
          messages\": [\n{\"role\": \"user\", \"content\": \"What is 5+7?\"}],\n\"\
          max_tokens\": 8000\n}\n\nRESPONCE:\n\n{\n    \"id\": \"chatcmpl-bfe9eaf4-2ab4-419d-ab8b-313add0706f9\"\
          ,\n    \"object\": \"chat.completion\",\n    \"created\": 1700798859,\n\
          \    \"model\": \"/models/mosaicml-mpt-30b-chat-Q8_0.gguf\",\n    \"choices\"\
          : [\n        {\n            \"index\": 0,\n            \"message\": {\n\
          \                \"content\": \"&gt;&lt;/s&gt;\\n&lt;s&gt;&amp;nbsp;&lt;/s&gt;&lt;br/&gt;\
          \ &lt;/body&gt; &lt;/html&gt; \\n\\nNotice the use of JavaScript to handle\
          \ user input and submit it via AJAX. ...\\n\\n\",\n                \"role\"\
          : \"assistant\"\n            },\n            \"finish_reason\": \"stop\"\
          \n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 16,\n\
          \        \"completion_tokens\": 2258,\n        \"total_tokens\": 2274\n\
          \    }\n}\n</code></pre>\n<p>But, it works well on the demo page:<br><a\
          \ rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/assets/34021271/3c1bf567-d3c3-4625-9212-c4817ced5b92\"\
          ><img alt=\"1700799966235\" src=\"https://github.com/ggerganov/llama.cpp/assets/34021271/3c1bf567-d3c3-4625-9212-c4817ced5b92\"\
          ></a></p>\n<p>I have report a discussion on <a rel=\"nofollow\" href=\"\
          https://github.com/abetlen/llama-cpp-python/discussions/939\">github</a>,\
          \ Welcome to participate in the discussion.</p>\n"
        raw: "[I found that llama.cpp already supports mpt, I downloaded [gguf](https://huggingface.co/maddes8cht/mosaicml-mpt-30b-chat-gguf/resolve/main/mosaicml-mpt-30b-chat-Q8_0.gguf)\
          \ from here, and it did load it with llama.cpp, but its return result looks\
          \ bad.\r\n\r\nI start the server as follow:\r\n```\r\ngit clone https://github.com/abetlen/llama-cpp-python.git\r\
          \ncd llama-cpp-python\r\ndocker build -t llama-cpp-python-cuda docker/cuda_simple/\r\
          \ndocker run --gpus all --rm -it -p 8000:8000 -v ${/path/to/models}:/models\
          \ -e MODEL=/models/${model-filename} llama-cpp-python-cuda python3 -m llama_cpp.server\
          \ --n_gpu_layers ${X} --n_ctx ${L}\r\n```\r\n\r\nAnd I post the request\
          \ as follow:\r\n\r\n```\r\nURL: http://localhost:8000/v1/chat/completions\r\
          \nBODY:\r\n\r\n{\r\n\"messages\": [\r\n{\"role\": \"user\", \"content\"\
          : \"What is 5+7?\"}],\r\n\"max_tokens\": 8000\r\n}\r\n\r\nRESPONCE:\r\n\r\
          \n{\r\n    \"id\": \"chatcmpl-bfe9eaf4-2ab4-419d-ab8b-313add0706f9\",\r\n\
          \    \"object\": \"chat.completion\",\r\n    \"created\": 1700798859,\r\n\
          \    \"model\": \"/models/mosaicml-mpt-30b-chat-Q8_0.gguf\",\r\n    \"choices\"\
          : [\r\n        {\r\n            \"index\": 0,\r\n            \"message\"\
          : {\r\n                \"content\": \"></s>\\n<s>&nbsp;</s><br/> </body>\
          \ </html> \\n\\nNotice the use of JavaScript to handle user input and submit\
          \ it via AJAX. ...\\n\\n\",\r\n                \"role\": \"assistant\"\r\
          \n            },\r\n            \"finish_reason\": \"stop\"\r\n        }\r\
          \n    ],\r\n    \"usage\": {\r\n        \"prompt_tokens\": 16,\r\n     \
          \   \"completion_tokens\": 2258,\r\n        \"total_tokens\": 2274\r\n \
          \   }\r\n}\r\n```\r\n\r\nBut, it works well on the demo page:\r\n![1700799966235](https://github.com/ggerganov/llama.cpp/assets/34021271/3c1bf567-d3c3-4625-9212-c4817ced5b92)\r\
          \n\r\nI have report a discussion on [github](https://github.com/abetlen/llama-cpp-python/discussions/939),\
          \ Welcome to participate in the discussion."
        updatedAt: '2023-11-24T04:38:09.915Z'
      numEdits: 0
      reactions: []
    id: 656028b15b627f441ad9121f
    type: comment
  author: streatycat
  content: "[I found that llama.cpp already supports mpt, I downloaded [gguf](https://huggingface.co/maddes8cht/mosaicml-mpt-30b-chat-gguf/resolve/main/mosaicml-mpt-30b-chat-Q8_0.gguf)\
    \ from here, and it did load it with llama.cpp, but its return result looks bad.\r\
    \n\r\nI start the server as follow:\r\n```\r\ngit clone https://github.com/abetlen/llama-cpp-python.git\r\
    \ncd llama-cpp-python\r\ndocker build -t llama-cpp-python-cuda docker/cuda_simple/\r\
    \ndocker run --gpus all --rm -it -p 8000:8000 -v ${/path/to/models}:/models -e\
    \ MODEL=/models/${model-filename} llama-cpp-python-cuda python3 -m llama_cpp.server\
    \ --n_gpu_layers ${X} --n_ctx ${L}\r\n```\r\n\r\nAnd I post the request as follow:\r\
    \n\r\n```\r\nURL: http://localhost:8000/v1/chat/completions\r\nBODY:\r\n\r\n{\r\
    \n\"messages\": [\r\n{\"role\": \"user\", \"content\": \"What is 5+7?\"}],\r\n\
    \"max_tokens\": 8000\r\n}\r\n\r\nRESPONCE:\r\n\r\n{\r\n    \"id\": \"chatcmpl-bfe9eaf4-2ab4-419d-ab8b-313add0706f9\"\
    ,\r\n    \"object\": \"chat.completion\",\r\n    \"created\": 1700798859,\r\n\
    \    \"model\": \"/models/mosaicml-mpt-30b-chat-Q8_0.gguf\",\r\n    \"choices\"\
    : [\r\n        {\r\n            \"index\": 0,\r\n            \"message\": {\r\n\
    \                \"content\": \"></s>\\n<s>&nbsp;</s><br/> </body> </html> \\\
    n\\nNotice the use of JavaScript to handle user input and submit it via AJAX.\
    \ ...\\n\\n\",\r\n                \"role\": \"assistant\"\r\n            },\r\n\
    \            \"finish_reason\": \"stop\"\r\n        }\r\n    ],\r\n    \"usage\"\
    : {\r\n        \"prompt_tokens\": 16,\r\n        \"completion_tokens\": 2258,\r\
    \n        \"total_tokens\": 2274\r\n    }\r\n}\r\n```\r\n\r\nBut, it works well\
    \ on the demo page:\r\n![1700799966235](https://github.com/ggerganov/llama.cpp/assets/34021271/3c1bf567-d3c3-4625-9212-c4817ced5b92)\r\
    \n\r\nI have report a discussion on [github](https://github.com/abetlen/llama-cpp-python/discussions/939),\
    \ Welcome to participate in the discussion."
  created_at: 2023-11-24 04:38:09+00:00
  edited: false
  hidden: false
  id: 656028b15b627f441ad9121f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: maddes8cht/mosaicml-mpt-30b-chat-gguf
repo_type: model
status: open
target_branch: null
title: How to deploy it with llama.cpp?
