!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CR2022
conflicting_files: null
created_at: 2023-11-11 17:43:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-11T17:43:23.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9263001680374146
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>Admin</p>

          <p>What is your max context length in tokens?<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>The maximum token limit per response is currently 4096. Please feel free
          to ask any questions or topics I can help you with :)<br>Admin</p>

          <p>In Oobabooga your context length is set to 32768<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>Thank you for the information, please let me know if there''s anything
          else that I may assist. You have some strange things in my character creation.<br>Admin</p>

          <p>I think the problem is related to that you tell me that your max content
          length in tokens is 4096 but your max tokens length is set to 32768 tokens
          they use technologies like rope scaling or rope frequency does that mean
          anything to you?<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>The maximum token limit per response is actually 4096, but you can ask
          me for help on any questions or subjects!<br>Admin</p>

          <p>Do you know what ctx value is?<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>The current maximum token count is 2048 but your context length is set
          to 3276<br>Admin</p>

          <p>No it is set to 32768 tokens that is also your ctx value.<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>I apologize for any confusion, please let me know if you have questions
          or topics where I may help you with :)<br>Admin</p>

          <p>The problem is I am trying to have a diagnose conversion with you but
          you do not understand me and that is what I mean I am trying to figure out
          together with you what the problem is may I show to you terminal output
          debug information of your language model?<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>It seems like there''s an error in my system. Please let me know if you
          have questions or subjects where I can assist you! :)<br>Admin</p>

          <p>May I show your diagnostic information of your language model file?<br>Openchat_3.5-16k.Q5_K_M</p>

          <p>174 tokens is the maximum length of my response limit per message</p>

          <p>Any idea how to solve it? Or is it just that these models using rope
          scaling become dumber the more you talk to them? I estimate that they start
          to become dumber and repetitive when they exceed the 4096 tokens they seem
          to believe they are created with. They are not at all using the whole 32K
          context at least not in a clean install of the latest Oobabooga text generation
          webui with the default assistant template and the default model settings
          provided by the gguf.</p>

          '
        raw: "Admin\r\n\r\nWhat is your max context length in tokens?\r\nOpenchat_3.5-16k.Q5_K_M\r\
          \n\r\nThe maximum token limit per response is currently 4096. Please feel\
          \ free to ask any questions or topics I can help you with :)\r\nAdmin\r\n\
          \r\nIn Oobabooga your context length is set to 32768\r\nOpenchat_3.5-16k.Q5_K_M\r\
          \n\r\nThank you for the information, please let me know if there's anything\
          \ else that I may assist. You have some strange things in my character creation.\r\
          \nAdmin\r\n\r\nI think the problem is related to that you tell me that your\
          \ max content length in tokens is 4096 but your max tokens length is set\
          \ to 32768 tokens they use technologies like rope scaling or rope frequency\
          \ does that mean anything to you?\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\nThe\
          \ maximum token limit per response is actually 4096, but you can ask me\
          \ for help on any questions or subjects!\r\nAdmin\r\n\r\nDo you know what\
          \ ctx value is?\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\nThe current maximum token\
          \ count is 2048 but your context length is set to 3276\r\nAdmin\r\n\r\n\
          No it is set to 32768 tokens that is also your ctx value.\r\nOpenchat_3.5-16k.Q5_K_M\r\
          \n\r\nI apologize for any confusion, please let me know if you have questions\
          \ or topics where I may help you with :)\r\nAdmin\r\n\r\nThe problem is\
          \ I am trying to have a diagnose conversion with you but you do not understand\
          \ me and that is what I mean I am trying to figure out together with you\
          \ what the problem is may I show to you terminal output debug information\
          \ of your language model?\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\nIt seems like\
          \ there's an error in my system. Please let me know if you have questions\
          \ or subjects where I can assist you! :)\r\nAdmin\r\n\r\nMay I show your\
          \ diagnostic information of your language model file?\r\nOpenchat_3.5-16k.Q5_K_M\r\
          \n\r\n174 tokens is the maximum length of my response limit per message\r\
          \n\r\nAny idea how to solve it? Or is it just that these models using rope\
          \ scaling become dumber the more you talk to them? I estimate that they\
          \ start to become dumber and repetitive when they exceed the 4096 tokens\
          \ they seem to believe they are created with. They are not at all using\
          \ the whole 32K context at least not in a clean install of the latest Oobabooga\
          \ text generation webui with the default assistant template and the default\
          \ model settings provided by the gguf."
        updatedAt: '2023-11-11T17:43:23.980Z'
      numEdits: 0
      reactions: []
    id: 654fbd3b259ab6029606cb78
    type: comment
  author: CR2022
  content: "Admin\r\n\r\nWhat is your max context length in tokens?\r\nOpenchat_3.5-16k.Q5_K_M\r\
    \n\r\nThe maximum token limit per response is currently 4096. Please feel free\
    \ to ask any questions or topics I can help you with :)\r\nAdmin\r\n\r\nIn Oobabooga\
    \ your context length is set to 32768\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\nThank\
    \ you for the information, please let me know if there's anything else that I\
    \ may assist. You have some strange things in my character creation.\r\nAdmin\r\
    \n\r\nI think the problem is related to that you tell me that your max content\
    \ length in tokens is 4096 but your max tokens length is set to 32768 tokens they\
    \ use technologies like rope scaling or rope frequency does that mean anything\
    \ to you?\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\nThe maximum token limit per response\
    \ is actually 4096, but you can ask me for help on any questions or subjects!\r\
    \nAdmin\r\n\r\nDo you know what ctx value is?\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\
    \nThe current maximum token count is 2048 but your context length is set to 3276\r\
    \nAdmin\r\n\r\nNo it is set to 32768 tokens that is also your ctx value.\r\nOpenchat_3.5-16k.Q5_K_M\r\
    \n\r\nI apologize for any confusion, please let me know if you have questions\
    \ or topics where I may help you with :)\r\nAdmin\r\n\r\nThe problem is I am trying\
    \ to have a diagnose conversion with you but you do not understand me and that\
    \ is what I mean I am trying to figure out together with you what the problem\
    \ is may I show to you terminal output debug information of your language model?\r\
    \nOpenchat_3.5-16k.Q5_K_M\r\n\r\nIt seems like there's an error in my system.\
    \ Please let me know if you have questions or subjects where I can assist you!\
    \ :)\r\nAdmin\r\n\r\nMay I show your diagnostic information of your language model\
    \ file?\r\nOpenchat_3.5-16k.Q5_K_M\r\n\r\n174 tokens is the maximum length of\
    \ my response limit per message\r\n\r\nAny idea how to solve it? Or is it just\
    \ that these models using rope scaling become dumber the more you talk to them?\
    \ I estimate that they start to become dumber and repetitive when they exceed\
    \ the 4096 tokens they seem to believe they are created with. They are not at\
    \ all using the whole 32K context at least not in a clean install of the latest\
    \ Oobabooga text generation webui with the default assistant template and the\
    \ default model settings provided by the gguf."
  created_at: 2023-11-11 17:43:23+00:00
  edited: false
  hidden: false
  id: 654fbd3b259ab6029606cb78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-11T17:44:54.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3539767563343048
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>llm_load_print_meta: format           = unknown<br>llm_load_print_meta:
          arch             = llama<br>llm_load_print_meta: vocab type       = SPM<br>llm_load_print_meta:
          n_vocab          = 32002<br>llm_load_print_meta: n_merges         = 0<br>llm_load_print_meta:
          n_ctx_train      = 32768<br>llm_load_print_meta: n_embd           = 4096<br>llm_load_print_meta:
          n_head           = 32<br>llm_load_print_meta: n_head_kv        = 8<br>llm_load_print_meta:
          n_layer          = 32<br>llm_load_print_meta: n_rot            = 128<br>llm_load_print_meta:
          n_gqa            = 4<br>llm_load_print_meta: f_norm_eps       = 0.0e+00<br>llm_load_print_meta:
          f_norm_rms_eps   = 1.0e-05<br>llm_load_print_meta: n_ff             = 14336<br>llm_load_print_meta:
          freq_base_train  = 1000000.0<br>llm_load_print_meta: freq_scale_train =
          1<br>llm_load_print_meta: model type       = 7B<br>llm_load_print_meta:
          model ftype      = mostly Q5_K - Medium<br>llm_load_print_meta: model params     =
          7.24 B<br>llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW)<br>llm_load_print_meta:
          general.name   = nurtureai_openchat_3.5-16k<br>llm_load_print_meta: BOS
          token = 1 ''<s>''<br>llm_load_print_meta: EOS token = 32000 ''&lt;|end_of_turn|&gt;''<br>llm_load_print_meta:
          UNK token = 0 ''''<br>llm_load_print_meta: LF token  = 13 ''&lt;0x0A&gt;''<br>llm_load_tensors:
          ggml ctx size =    0.09 MB<br>llm_load_tensors: using CUDA for GPU acceleration<br>llm_load_tensors:
          mem required  = 4893.10 MB<br>llm_load_tensors: offloading 0 repeating layers
          to GPU<br>llm_load_tensors: offloaded 0/35 layers to GPU<br>llm_load_tensors:
          VRAM used: 0.00 MB<br>...................................................................................................<br>llama_new_context_with_model:
          n_ctx      = 32768<br>llama_new_context_with_model: freq_base  = 1000000.0<br>llama_new_context_with_model:
          freq_scale = 1<br>llama_new_context_with_model: kv self size  = 4096.00
          MB<br>llama_new_context_with_model: compute buffer total size = 2141.88
          MB<br>llama_new_context_with_model: VRAM scratch buffer: 2136.00 MB<br>llama_new_context_with_model:
          total VRAM used: 2136.00 MB (model: 0.00 MB, context: 2136.00 MB)<br>AVX
          = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA =
          1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS
          = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |<br>2023-11-11 15:51:08 INFO:Loaded
          the model in 6.74 seconds.<br>Output generated in 16.39 seconds (3.42 tokens/s,
          56 tokens, context 72, seed 1017857071)<br>Llama.generate: prefix-match
          hit<br>Output generated in 18.82 seconds (3.77 tokens/s, 71 tokens, context
          195, seed 249506680)<br>Llama.generate: prefix-match hit</s></p><s>

          </s>'
        raw: 'llm_load_print_meta: format           = unknown

          llm_load_print_meta: arch             = llama

          llm_load_print_meta: vocab type       = SPM

          llm_load_print_meta: n_vocab          = 32002

          llm_load_print_meta: n_merges         = 0

          llm_load_print_meta: n_ctx_train      = 32768

          llm_load_print_meta: n_embd           = 4096

          llm_load_print_meta: n_head           = 32

          llm_load_print_meta: n_head_kv        = 8

          llm_load_print_meta: n_layer          = 32

          llm_load_print_meta: n_rot            = 128

          llm_load_print_meta: n_gqa            = 4

          llm_load_print_meta: f_norm_eps       = 0.0e+00

          llm_load_print_meta: f_norm_rms_eps   = 1.0e-05

          llm_load_print_meta: n_ff             = 14336

          llm_load_print_meta: freq_base_train  = 1000000.0

          llm_load_print_meta: freq_scale_train = 1

          llm_load_print_meta: model type       = 7B

          llm_load_print_meta: model ftype      = mostly Q5_K - Medium

          llm_load_print_meta: model params     = 7.24 B

          llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW)

          llm_load_print_meta: general.name   = nurtureai_openchat_3.5-16k

          llm_load_print_meta: BOS token = 1 ''<s>''

          llm_load_print_meta: EOS token = 32000 ''<|end_of_turn|>''

          llm_load_print_meta: UNK token = 0 ''<unk>''

          llm_load_print_meta: LF token  = 13 ''<0x0A>''

          llm_load_tensors: ggml ctx size =    0.09 MB

          llm_load_tensors: using CUDA for GPU acceleration

          llm_load_tensors: mem required  = 4893.10 MB

          llm_load_tensors: offloading 0 repeating layers to GPU

          llm_load_tensors: offloaded 0/35 layers to GPU

          llm_load_tensors: VRAM used: 0.00 MB

          ...................................................................................................

          llama_new_context_with_model: n_ctx      = 32768

          llama_new_context_with_model: freq_base  = 1000000.0

          llama_new_context_with_model: freq_scale = 1

          llama_new_context_with_model: kv self size  = 4096.00 MB

          llama_new_context_with_model: compute buffer total size = 2141.88 MB

          llama_new_context_with_model: VRAM scratch buffer: 2136.00 MB

          llama_new_context_with_model: total VRAM used: 2136.00 MB (model: 0.00 MB,
          context: 2136.00 MB)

          AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA
          = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 |
          BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 |

          2023-11-11 15:51:08 INFO:Loaded the model in 6.74 seconds.

          Output generated in 16.39 seconds (3.42 tokens/s, 56 tokens, context 72,
          seed 1017857071)

          Llama.generate: prefix-match hit

          Output generated in 18.82 seconds (3.77 tokens/s, 71 tokens, context 195,
          seed 249506680)

          Llama.generate: prefix-match hit'
        updatedAt: '2023-11-11T17:44:54.012Z'
      numEdits: 0
      reactions: []
    id: 654fbd965491c57ee9aef795
    type: comment
  author: CR2022
  content: 'llm_load_print_meta: format           = unknown

    llm_load_print_meta: arch             = llama

    llm_load_print_meta: vocab type       = SPM

    llm_load_print_meta: n_vocab          = 32002

    llm_load_print_meta: n_merges         = 0

    llm_load_print_meta: n_ctx_train      = 32768

    llm_load_print_meta: n_embd           = 4096

    llm_load_print_meta: n_head           = 32

    llm_load_print_meta: n_head_kv        = 8

    llm_load_print_meta: n_layer          = 32

    llm_load_print_meta: n_rot            = 128

    llm_load_print_meta: n_gqa            = 4

    llm_load_print_meta: f_norm_eps       = 0.0e+00

    llm_load_print_meta: f_norm_rms_eps   = 1.0e-05

    llm_load_print_meta: n_ff             = 14336

    llm_load_print_meta: freq_base_train  = 1000000.0

    llm_load_print_meta: freq_scale_train = 1

    llm_load_print_meta: model type       = 7B

    llm_load_print_meta: model ftype      = mostly Q5_K - Medium

    llm_load_print_meta: model params     = 7.24 B

    llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW)

    llm_load_print_meta: general.name   = nurtureai_openchat_3.5-16k

    llm_load_print_meta: BOS token = 1 ''<s>''

    llm_load_print_meta: EOS token = 32000 ''<|end_of_turn|>''

    llm_load_print_meta: UNK token = 0 ''<unk>''

    llm_load_print_meta: LF token  = 13 ''<0x0A>''

    llm_load_tensors: ggml ctx size =    0.09 MB

    llm_load_tensors: using CUDA for GPU acceleration

    llm_load_tensors: mem required  = 4893.10 MB

    llm_load_tensors: offloading 0 repeating layers to GPU

    llm_load_tensors: offloaded 0/35 layers to GPU

    llm_load_tensors: VRAM used: 0.00 MB

    ...................................................................................................

    llama_new_context_with_model: n_ctx      = 32768

    llama_new_context_with_model: freq_base  = 1000000.0

    llama_new_context_with_model: freq_scale = 1

    llama_new_context_with_model: kv self size  = 4096.00 MB

    llama_new_context_with_model: compute buffer total size = 2141.88 MB

    llama_new_context_with_model: VRAM scratch buffer: 2136.00 MB

    llama_new_context_with_model: total VRAM used: 2136.00 MB (model: 0.00 MB, context:
    2136.00 MB)

    AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1
    | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 |
    SSE3 = 1 | SSSE3 = 0 | VSX = 0 |

    2023-11-11 15:51:08 INFO:Loaded the model in 6.74 seconds.

    Output generated in 16.39 seconds (3.42 tokens/s, 56 tokens, context 72, seed
    1017857071)

    Llama.generate: prefix-match hit

    Output generated in 18.82 seconds (3.77 tokens/s, 71 tokens, context 195, seed
    249506680)

    Llama.generate: prefix-match hit'
  created_at: 2023-11-11 17:44:54+00:00
  edited: false
  hidden: false
  id: 654fbd965491c57ee9aef795
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-11T22:16:57.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9835425019264221
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>The problem is that you are expecting the model to know its context\
          \ and where it\u2019s loaded. The model has 0% idea about where it\u2019\
          s loaded and pretty much 0% idea about its context as well.</p>\n<p>This\
          \ stuff is usually told in the system prompt. Even ChatGPT or gpt4 would\
          \ react the same unless you told it in the system prompt.</p>\n"
        raw: "The problem is that you are expecting the model to know its context\
          \ and where it\u2019s loaded. The model has 0% idea about where it\u2019\
          s loaded and pretty much 0% idea about its context as well.\n\nThis stuff\
          \ is usually told in the system prompt. Even ChatGPT or gpt4 would react\
          \ the same unless you told it in the system prompt."
        updatedAt: '2023-11-11T22:16:57.126Z'
      numEdits: 0
      reactions: []
    id: 654ffd591b3d3092a1ccb8b4
    type: comment
  author: YaTharThShaRma999
  content: "The problem is that you are expecting the model to know its context and\
    \ where it\u2019s loaded. The model has 0% idea about where it\u2019s loaded and\
    \ pretty much 0% idea about its context as well.\n\nThis stuff is usually told\
    \ in the system prompt. Even ChatGPT or gpt4 would react the same unless you told\
    \ it in the system prompt."
  created_at: 2023-11-11 22:16:57+00:00
  edited: false
  hidden: false
  id: 654ffd591b3d3092a1ccb8b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-12T00:48:46.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560725092887878
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>This was just an example the whole model fails to reason about pretty
          much anything if it exceeds 4096 tokens. It behaves normally in the beginning
          but then it goes dumb and crazy.</p>

          '
        raw: This was just an example the whole model fails to reason about pretty
          much anything if it exceeds 4096 tokens. It behaves normally in the beginning
          but then it goes dumb and crazy.
        updatedAt: '2023-11-12T00:48:46.608Z'
      numEdits: 0
      reactions: []
    id: 655020ee0774258908a5cdfa
    type: comment
  author: CR2022
  content: This was just an example the whole model fails to reason about pretty much
    anything if it exceeds 4096 tokens. It behaves normally in the beginning but then
    it goes dumb and crazy.
  created_at: 2023-11-12 00:48:46+00:00
  edited: false
  hidden: false
  id: 655020ee0774258908a5cdfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-11-12T00:52:48.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9200900793075562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>Basically you have a 4096 tokens model that is using rope to extend
          context to supposedly 32K tokens but it goes crazy or dumb when it exceeds
          around 4096 tokens.</p>

          '
        raw: Basically you have a 4096 tokens model that is using rope to extend context
          to supposedly 32K tokens but it goes crazy or dumb when it exceeds around
          4096 tokens.
        updatedAt: '2023-11-12T00:52:48.183Z'
      numEdits: 0
      reactions: []
    id: 655021e0259ab60296145d53
    type: comment
  author: CR2022
  content: Basically you have a 4096 tokens model that is using rope to extend context
    to supposedly 32K tokens but it goes crazy or dumb when it exceeds around 4096
    tokens.
  created_at: 2023-11-12 00:52:48+00:00
  edited: false
  hidden: false
  id: 655021e0259ab60296145d53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2024-01-09T17:33:17.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9616101980209351
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CR2022&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CR2022\">@<span class=\"\
          underline\">CR2022</span></a></span>\n\n\t</span></span> I think you dont\
          \ understand how language models work.<br>First put, any model even gpt4\
          \ or chatgpt has no idea about where its loaded or how much context it has\
          \ unless its trained on many questions and answers on it. </p>\n<p>You can\
          \ kind of solve it by putting it on the system prompt like you are an ai\
          \ loaded in text generation web ui and you have 32k context. But it needs\
          \ to even know about text generation web ui which is kind of new and most\
          \ models dont know about it.</p>\n<p>Also im not sure if u are using the\
          \ correct prompt format</p>\n<p>Its not that its dumb but rather you arent\
          \ asking it correctly.</p>\n"
        raw: "@CR2022 I think you dont understand how language models work.\nFirst\
          \ put, any model even gpt4 or chatgpt has no idea about where its loaded\
          \ or how much context it has unless its trained on many questions and answers\
          \ on it. \n\nYou can kind of solve it by putting it on the system prompt\
          \ like you are an ai loaded in text generation web ui and you have 32k context.\
          \ But it needs to even know about text generation web ui which is kind of\
          \ new and most models dont know about it.\n\nAlso im not sure if u are using\
          \ the correct prompt format\n\nIts not that its dumb but rather you arent\
          \ asking it correctly."
        updatedAt: '2024-01-09T17:33:17.899Z'
      numEdits: 0
      reactions: []
    id: 659d835d50c1bbee5be5295a
    type: comment
  author: YaTharThShaRma999
  content: "@CR2022 I think you dont understand how language models work.\nFirst put,\
    \ any model even gpt4 or chatgpt has no idea about where its loaded or how much\
    \ context it has unless its trained on many questions and answers on it. \n\n\
    You can kind of solve it by putting it on the system prompt like you are an ai\
    \ loaded in text generation web ui and you have 32k context. But it needs to even\
    \ know about text generation web ui which is kind of new and most models dont\
    \ know about it.\n\nAlso im not sure if u are using the correct prompt format\n\
    \nIts not that its dumb but rather you arent asking it correctly."
  created_at: 2024-01-09 17:33:17+00:00
  edited: false
  hidden: false
  id: 659d835d50c1bbee5be5295a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: TheBloke/openchat_3.5-GGUF
repo_type: model
status: open
target_branch: null
title: These models that use rope behave very weird (dumb) with default settings of
  32768 in latest Oobabooga text generation webui.
