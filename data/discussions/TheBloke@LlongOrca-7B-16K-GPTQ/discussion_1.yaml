!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mancub
conflicting_files: null
created_at: 2023-08-13 02:09:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-08-13T03:09:15.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9616075158119202
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>Hey\
          \ hope all is well, I've been gone for a bit and I see you've been churning\
          \ out an insane amount of models every day - yer nuts. :)</p>\n<p>Anyway,\
          \ wondering why is there a clear lack of 33B models coming out (in general)?</p>\n\
          <p>All I see now is either 70B which is unusable for most enthusiasts with\
          \ 24GB VRAM and requires at least 48GB, or 7B/13B which are lower-end. The\
          \ 33B seemed like a sweet spot that would fill the 24GB nicely and offer\
          \ best of all worlds.</p>\n<p>Sorry to post this into an unrelated model,\
          \ but there's no contact point on HF to email you directly.</p>\n<p>Thanks!</p>\n"
        raw: "@TheBloke \n\nHey hope all is well, I've been gone for a bit and I see\
          \ you've been churning out an insane amount of models every day - yer nuts.\
          \ :)\n\nAnyway, wondering why is there a clear lack of 33B models coming\
          \ out (in general)?\n\nAll I see now is either 70B which is unusable for\
          \ most enthusiasts with 24GB VRAM and requires at least 48GB, or 7B/13B\
          \ which are lower-end. The 33B seemed like a sweet spot that would fill\
          \ the 24GB nicely and offer best of all worlds.\n\nSorry to post this into\
          \ an unrelated model, but there's no contact point on HF to email you directly.\n\
          \nThanks!"
        updatedAt: '2023-08-13T03:09:32.756Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - actionpace
    id: 64d8495b6db135cfc8e2cf34
    type: comment
  author: mancub
  content: "@TheBloke \n\nHey hope all is well, I've been gone for a bit and I see\
    \ you've been churning out an insane amount of models every day - yer nuts. :)\n\
    \nAnyway, wondering why is there a clear lack of 33B models coming out (in general)?\n\
    \nAll I see now is either 70B which is unusable for most enthusiasts with 24GB\
    \ VRAM and requires at least 48GB, or 7B/13B which are lower-end. The 33B seemed\
    \ like a sweet spot that would fill the 24GB nicely and offer best of all worlds.\n\
    \nSorry to post this into an unrelated model, but there's no contact point on\
    \ HF to email you directly.\n\nThanks!"
  created_at: 2023-08-13 02:09:15+00:00
  edited: true
  hidden: false
  id: 64d8495b6db135cfc8e2cf34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-13T09:48:54.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8618460893630981
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>ggml versions of 70b works great with 24GB cards  :D </p>

          '
        raw: 'ggml versions of 70b works great with 24GB cards  :D '
        updatedAt: '2023-08-13T09:48:54.378Z'
      numEdits: 0
      reactions: []
    id: 64d8a706c2eedf9af85d9592
    type: comment
  author: mirek190
  content: 'ggml versions of 70b works great with 24GB cards  :D '
  created_at: 2023-08-13 08:48:54+00:00
  edited: false
  hidden: false
  id: 64d8a706c2eedf9af85d9592
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-08-14T00:46:19.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.946807324886322
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/mirek190\">@<span class=\"\
          underline\">mirek190</span></a></span>\n\n\t</span></span> </p>\n<p>Which\
          \ model are you talking about because they are all over 24GB in size?</p>\n\
          <p>Unless you have multiple 3090s or you do not load all of the layers,\
          \ I don't see how would you be content with a 70B model at 5-6 t/s considering\
          \ smaller models produce 50+ t/s</p>\n"
        raw: "@mirek190 \n\nWhich model are you talking about because they are all\
          \ over 24GB in size?\n\nUnless you have multiple 3090s or you do not load\
          \ all of the layers, I don't see how would you be content with a 70B model\
          \ at 5-6 t/s considering smaller models produce 50+ t/s"
        updatedAt: '2023-08-14T00:46:19.855Z'
      numEdits: 0
      reactions: []
    id: 64d9795babf475a808019335
    type: comment
  author: mancub
  content: "@mirek190 \n\nWhich model are you talking about because they are all over\
    \ 24GB in size?\n\nUnless you have multiple 3090s or you do not load all of the\
    \ layers, I don't see how would you be content with a 70B model at 5-6 t/s considering\
    \ smaller models produce 50+ t/s"
  created_at: 2023-08-13 23:46:19+00:00
  edited: false
  hidden: false
  id: 64d9795babf475a808019335
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/950776037909f20547028242591f5879.svg
      fullname: David King
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: actionpace
      type: user
    createdAt: '2023-08-14T00:51:52.000Z'
    data:
      edited: true
      editors:
      - actionpace
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9903008937835693
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/950776037909f20547028242591f5879.svg
          fullname: David King
          isHf: false
          isPro: false
          name: actionpace
          type: user
        html: '<p>I was thinking about that too. Does it mean you can put some layers
          on GPU and rely on the rest on multi core CPU? I would like the ability
          to experiment more on 30B+ models as well</p>

          '
        raw: I was thinking about that too. Does it mean you can put some layers on
          GPU and rely on the rest on multi core CPU? I would like the ability to
          experiment more on 30B+ models as well
        updatedAt: '2023-08-14T00:53:41.882Z'
      numEdits: 2
      reactions: []
    id: 64d97aa8887f55fb6e6896cf
    type: comment
  author: actionpace
  content: I was thinking about that too. Does it mean you can put some layers on
    GPU and rely on the rest on multi core CPU? I would like the ability to experiment
    more on 30B+ models as well
  created_at: 2023-08-13 23:51:52+00:00
  edited: true
  hidden: false
  id: 64d97aa8887f55fb6e6896cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-08-14T07:22:54.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.926256537437439
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;mirek190&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mirek190\"\
          >@<span class=\"underline\">mirek190</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Which model are you talking about because they are all over 24GB\
          \ in size?</p>\n<p>Unless you have multiple 3090s or you do not load all\
          \ of the layers, I don't see how would you be content with a 70B model at\
          \ 5-6 t/s considering smaller models produce 50+ t/s</p>\n</blockquote>\n\
          <p>Of course ggml format I meant .<br>For instance q4km (mostly 15 bit weights)\
          \  and 40 layers on GPU I have 2.5 t/s.</p>\n<p>Something lower like 33B\
          \ q4km ggml I have 30 t/s  as all layers fit on rtx 3090.</p>\n"
        raw: "> @mirek190 \n> \n> Which model are you talking about because they are\
          \ all over 24GB in size?\n> \n> Unless you have multiple 3090s or you do\
          \ not load all of the layers, I don't see how would you be content with\
          \ a 70B model at 5-6 t/s considering smaller models produce 50+ t/s\n\n\
          Of course ggml format I meant .\nFor instance q4km (mostly 15 bit weights)\
          \  and 40 layers on GPU I have 2.5 t/s.\n\nSomething lower like 33B q4km\
          \ ggml I have 30 t/s  as all layers fit on rtx 3090."
        updatedAt: '2023-08-14T07:25:25.377Z'
      numEdits: 2
      reactions: []
    id: 64d9d64ec79ca7ce7705b63b
    type: comment
  author: mirek190
  content: "> @mirek190 \n> \n> Which model are you talking about because they are\
    \ all over 24GB in size?\n> \n> Unless you have multiple 3090s or you do not load\
    \ all of the layers, I don't see how would you be content with a 70B model at\
    \ 5-6 t/s considering smaller models produce 50+ t/s\n\nOf course ggml format\
    \ I meant .\nFor instance q4km (mostly 15 bit weights)  and 40 layers on GPU I\
    \ have 2.5 t/s.\n\nSomething lower like 33B q4km ggml I have 30 t/s  as all layers\
    \ fit on rtx 3090."
  created_at: 2023-08-14 06:22:54+00:00
  edited: true
  hidden: false
  id: 64d9d64ec79ca7ce7705b63b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-08-14T23:38:58.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9578250050544739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>But this is what I''m talking about - there aren''t any 33B models
          out there lately, it''s all either 13B or 70B.</p>

          <p>IMHO 33B is the sweet spot for the 24GB VRAM cards (e.g. 3090) that most
          people could afford or find. The 48GB cards (A6000, etc) are at least 4-5x
          more expensive than a 3090.</p>

          '
        raw: 'But this is what I''m talking about - there aren''t any 33B models out
          there lately, it''s all either 13B or 70B.


          IMHO 33B is the sweet spot for the 24GB VRAM cards (e.g. 3090) that most
          people could afford or find. The 48GB cards (A6000, etc) are at least 4-5x
          more expensive than a 3090.'
        updatedAt: '2023-08-14T23:38:58.111Z'
      numEdits: 0
      reactions: []
    id: 64dabb123a7ab21ea7d8c79c
    type: comment
  author: mancub
  content: 'But this is what I''m talking about - there aren''t any 33B models out
    there lately, it''s all either 13B or 70B.


    IMHO 33B is the sweet spot for the 24GB VRAM cards (e.g. 3090) that most people
    could afford or find. The 48GB cards (A6000, etc) are at least 4-5x more expensive
    than a 3090.'
  created_at: 2023-08-14 22:38:58+00:00
  edited: false
  hidden: false
  id: 64dabb123a7ab21ea7d8c79c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/db033781eed53913abe12c2d30004f66.svg
      fullname: Nick Perez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nkpz
      type: user
    createdAt: '2023-08-15T00:19:24.000Z'
    data:
      edited: true
      editors:
      - nkpz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9611114263534546
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/db033781eed53913abe12c2d30004f66.svg
          fullname: Nick Perez
          isHf: false
          isPro: false
          name: nkpz
          type: user
        html: '<blockquote>

          <p>But this is what I''m talking about - there aren''t any 33B models out
          there lately, it''s all either 13B or 70B.</p>

          <p>IMHO 33B is the sweet spot for the 24GB VRAM cards (e.g. 3090) that most
          people could afford or find. The 48GB cards (A6000, etc) are at least 4-5x
          more expensive than a 3090.</p>

          </blockquote>

          <p>because they''re llama 2 based, which is only 7b, 13b and 70b. </p>

          '
        raw: "> But this is what I'm talking about - there aren't any 33B models out\
          \ there lately, it's all either 13B or 70B.\n> \n> IMHO 33B is the sweet\
          \ spot for the 24GB VRAM cards (e.g. 3090) that most people could afford\
          \ or find. The 48GB cards (A6000, etc) are at least 4-5x more expensive\
          \ than a 3090.\n\nbecause they're llama 2 based, which is only 7b, 13b and\
          \ 70b. "
        updatedAt: '2023-08-15T00:19:57.235Z'
      numEdits: 1
      reactions: []
    id: 64dac48c978d1f9fafc5b7be
    type: comment
  author: nkpz
  content: "> But this is what I'm talking about - there aren't any 33B models out\
    \ there lately, it's all either 13B or 70B.\n> \n> IMHO 33B is the sweet spot\
    \ for the 24GB VRAM cards (e.g. 3090) that most people could afford or find. The\
    \ 48GB cards (A6000, etc) are at least 4-5x more expensive than a 3090.\n\nbecause\
    \ they're llama 2 based, which is only 7b, 13b and 70b. "
  created_at: 2023-08-14 23:19:24+00:00
  edited: true
  hidden: false
  id: 64dac48c978d1f9fafc5b7be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-08-16T00:36:14.000Z'
    data:
      edited: true
      editors:
      - mancub
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4861912131309509
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p><a rel="nofollow" href="https://old.reddit.com/r/LocalLLaMA/comments/15mrrnm/do_you_guys_think_the_34b_llama_2_model_has_been/">https://old.reddit.com/r/LocalLLaMA/comments/15mrrnm/do_you_guys_think_the_34b_llama_2_model_has_been/</a></p>

          <p><a rel="nofollow" href="https://github.com/facebookresearch/llama/issues/590">https://github.com/facebookresearch/llama/issues/590</a></p>

          '
        raw: 'https://old.reddit.com/r/LocalLLaMA/comments/15mrrnm/do_you_guys_think_the_34b_llama_2_model_has_been/


          https://github.com/facebookresearch/llama/issues/590'
        updatedAt: '2023-08-16T00:38:37.531Z'
      numEdits: 1
      reactions: []
    id: 64dc19fe70446182be8e2ce2
    type: comment
  author: mancub
  content: 'https://old.reddit.com/r/LocalLLaMA/comments/15mrrnm/do_you_guys_think_the_34b_llama_2_model_has_been/


    https://github.com/facebookresearch/llama/issues/590'
  created_at: 2023-08-15 23:36:14+00:00
  edited: true
  hidden: false
  id: 64dc19fe70446182be8e2ce2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/LlongOrca-7B-16K-GPTQ
repo_type: model
status: open
target_branch: null
title: Lack of 33B models?
