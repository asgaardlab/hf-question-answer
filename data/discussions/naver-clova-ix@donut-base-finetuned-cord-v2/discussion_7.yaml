!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Samartha27
conflicting_files: null
created_at: 2024-01-24 21:27:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fe30ee0f974b56137f3abe63584aaecf.svg
      fullname: Samartha R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Samartha27
      type: user
    createdAt: '2024-01-24T21:27:59.000Z'
    data:
      edited: false
      editors:
      - Samartha27
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6360448598861694
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fe30ee0f974b56137f3abe63584aaecf.svg
          fullname: Samartha R
          isHf: false
          isPro: false
          name: Samartha27
          type: user
        html: '<p>model = DonutModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")</p>

          <p>''''''<br>model = DonutModel.from_pretrained("naver-clova-ix/donut-base-finetuned-cord-v2")<br>  File
          "/home/sam/code/donut/donut/model.py", line 597, in from_pretrained<br>    model
          = super(DonutModel, cls).from_pretrained(pretrained_model_name_or_path,
          revision="official", *model_args, **kwargs)<br>  File "/home/sam/anaconda3/envs/donut_official/lib/python3.7/site-packages/transformers/modeling_utils.py",
          line 2896, in from_pretrained<br>    keep_in_fp32_modules=keep_in_fp32_modules,<br>  File
          "/home/sam/anaconda3/envs/donut_official/lib/python3.7/site-packages/transformers/modeling_utils.py",
          line 3278, in _load_pretrained_model<br>    raise RuntimeError(f"Error(s)
          in loading state_dict for {model.<strong>class</strong>.<strong>name</strong>}:\n\t{error_msg}")<br>RuntimeError:
          Error(s) in loading state_dict for DonutModel:<br>    size mismatch for
          encoder.model.layers.1.downsample.norm.weight: copying a param with shape
          torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).<br>    size
          mismatch for encoder.model.layers.1.downsample.norm.bias: copying a param
          with shape torch.Size([1024]) from checkpoint, the shape in current model
          is torch.Size([512]).<br>    size mismatch for encoder.model.layers.1.downsample.reduction.weight:
          copying a param with shape torch.Size([512, 1024]) from checkpoint, the
          shape in current model is torch.Size([256, 512]).<br>    size mismatch for
          encoder.model.layers.2.downsample.norm.weight: copying a param with shape
          torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).<br>    size
          mismatch for encoder.model.layers.2.downsample.norm.bias: copying a param
          with shape torch.Size([2048]) from checkpoint, the shape in current model
          is torch.Size([1024]).<br>    size mismatch for encoder.model.layers.2.downsample.reduction.weight:
          copying a param with shape torch.Size([1024, 2048]) from checkpoint, the
          shape in current model is torch.Size([512, 1024]).<br>    You may consider
          adding <code>ignore_mismatched_sizes=True</code> in the model <code>from_pretrained</code>
          method. ''''''</p>

          '
        raw: "model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\"\
          )\r\n\r\n\r\n\r\n\r\n'''\r\nmodel = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\"\
          )\r\n  File \"/home/sam/code/donut/donut/model.py\", line 597, in from_pretrained\r\
          \n    model = super(DonutModel, cls).from_pretrained(pretrained_model_name_or_path,\
          \ revision=\"official\", *model_args, **kwargs)\r\n  File \"/home/sam/anaconda3/envs/donut_official/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
          , line 2896, in from_pretrained\r\n    keep_in_fp32_modules=keep_in_fp32_modules,\r\
          \n  File \"/home/sam/anaconda3/envs/donut_official/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
          , line 3278, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\
          )\r\nRuntimeError: Error(s) in loading state_dict for DonutModel:\r\n\t\
          size mismatch for encoder.model.layers.1.downsample.norm.weight: copying\
          \ a param with shape torch.Size([1024]) from checkpoint, the shape in current\
          \ model is torch.Size([512]).\r\n\tsize mismatch for encoder.model.layers.1.downsample.norm.bias:\
          \ copying a param with shape torch.Size([1024]) from checkpoint, the shape\
          \ in current model is torch.Size([512]).\r\n\tsize mismatch for encoder.model.layers.1.downsample.reduction.weight:\
          \ copying a param with shape torch.Size([512, 1024]) from checkpoint, the\
          \ shape in current model is torch.Size([256, 512]).\r\n\tsize mismatch for\
          \ encoder.model.layers.2.downsample.norm.weight: copying a param with shape\
          \ torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\r\
          \n\tsize mismatch for encoder.model.layers.2.downsample.norm.bias: copying\
          \ a param with shape torch.Size([2048]) from checkpoint, the shape in current\
          \ model is torch.Size([1024]).\r\n\tsize mismatch for encoder.model.layers.2.downsample.reduction.weight:\
          \ copying a param with shape torch.Size([1024, 2048]) from checkpoint, the\
          \ shape in current model is torch.Size([512, 1024]).\r\n\tYou may consider\
          \ adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\
          \ '''\r\n\r\n"
        updatedAt: '2024-01-24T21:27:59.223Z'
      numEdits: 0
      reactions: []
    id: 65b180df54fa188a2f9b465e
    type: comment
  author: Samartha27
  content: "model = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\"\
    )\r\n\r\n\r\n\r\n\r\n'''\r\nmodel = DonutModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\"\
    )\r\n  File \"/home/sam/code/donut/donut/model.py\", line 597, in from_pretrained\r\
    \n    model = super(DonutModel, cls).from_pretrained(pretrained_model_name_or_path,\
    \ revision=\"official\", *model_args, **kwargs)\r\n  File \"/home/sam/anaconda3/envs/donut_official/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
    , line 2896, in from_pretrained\r\n    keep_in_fp32_modules=keep_in_fp32_modules,\r\
    \n  File \"/home/sam/anaconda3/envs/donut_official/lib/python3.7/site-packages/transformers/modeling_utils.py\"\
    , line 3278, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s) in\
    \ loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\r\nRuntimeError:\
    \ Error(s) in loading state_dict for DonutModel:\r\n\tsize mismatch for encoder.model.layers.1.downsample.norm.weight:\
    \ copying a param with shape torch.Size([1024]) from checkpoint, the shape in\
    \ current model is torch.Size([512]).\r\n\tsize mismatch for encoder.model.layers.1.downsample.norm.bias:\
    \ copying a param with shape torch.Size([1024]) from checkpoint, the shape in\
    \ current model is torch.Size([512]).\r\n\tsize mismatch for encoder.model.layers.1.downsample.reduction.weight:\
    \ copying a param with shape torch.Size([512, 1024]) from checkpoint, the shape\
    \ in current model is torch.Size([256, 512]).\r\n\tsize mismatch for encoder.model.layers.2.downsample.norm.weight:\
    \ copying a param with shape torch.Size([2048]) from checkpoint, the shape in\
    \ current model is torch.Size([1024]).\r\n\tsize mismatch for encoder.model.layers.2.downsample.norm.bias:\
    \ copying a param with shape torch.Size([2048]) from checkpoint, the shape in\
    \ current model is torch.Size([1024]).\r\n\tsize mismatch for encoder.model.layers.2.downsample.reduction.weight:\
    \ copying a param with shape torch.Size([1024, 2048]) from checkpoint, the shape\
    \ in current model is torch.Size([512, 1024]).\r\n\tYou may consider adding `ignore_mismatched_sizes=True`\
    \ in the model `from_pretrained` method. '''\r\n\r\n"
  created_at: 2024-01-24 21:27:59+00:00
  edited: false
  hidden: false
  id: 65b180df54fa188a2f9b465e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: naver-clova-ix/donut-base-finetuned-cord-v2
repo_type: model
status: open
target_branch: null
title: Model weights available from Hugging face are twice in embedding dimension
  when trying to load it
