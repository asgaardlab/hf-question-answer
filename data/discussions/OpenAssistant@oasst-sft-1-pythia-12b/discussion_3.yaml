!!python/object:huggingface_hub.community.DiscussionWithDetails
author: eeyrw
conflicting_files: null
created_at: 2023-03-16 04:15:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
      fullname: Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eeyrw
      type: user
    createdAt: '2023-03-16T05:15:09.000Z'
    data:
      edited: false
      editors:
      - eeyrw
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
          fullname: Yuan
          isHf: false
          isPro: false
          name: eeyrw
          type: user
        html: '<p>Such as 12G*8?</p>

          '
        raw: Such as 12G*8?
        updatedAt: '2023-03-16T05:15:09.959Z'
      numEdits: 0
      reactions: []
    id: 6412a5dd00634c4fe9874451
    type: comment
  author: eeyrw
  content: Such as 12G*8?
  created_at: 2023-03-16 04:15:09+00:00
  edited: false
  hidden: false
  id: 6412a5dd00634c4fe9874451
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/13efc01665a3155eed454cac2e62fad9.svg
      fullname: Yuan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eeyrw
      type: user
    createdAt: '2023-03-16T05:15:22.000Z'
    data:
      from: How can I the model with multiple GPUs?
      to: How can I run the model with multiple GPUs?
    id: 6412a5eaeef867ce8f6f325c
    type: title-change
  author: eeyrw
  created_at: 2023-03-16 04:15:22+00:00
  id: 6412a5eaeef867ce8f6f325c
  new_title: How can I run the model with multiple GPUs?
  old_title: How can I the model with multiple GPUs?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-03-16T18:04:40.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>You can use <a rel="nofollow" href="https://github.com/huggingface/text-generation-inference"><code>text-generation-inference</code></a>:</p>

          <pre><code class="language-shell">model=OpenAssistant/oasst-sft-1-pythia-12b

          num_shard=4 # number of GPUs you want to use

          volume=$PWD/data # share a volume with the Docker container to avoid downloading
          weights every run


          docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
          --model-id $model --num-shard $num_shard

          </code></pre>

          <p>Then you can query the model with <a rel="nofollow" href="https://pypi.org/project/text-generation/"><code>text-generation</code></a>:</p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          text_generation <span class="hljs-keyword">import</span> Client


          client = Client(<span class="hljs-string">"http://localhost:8080"</span>)


          client.generate(<span class="hljs-string">"&lt;|prompter|&gt;What''s the
          Earth total population&lt;|endoftext|&gt;&lt;|assistant|&gt;"</span>).generated_text

          </code></pre>

          '
        raw: 'You can use [`text-generation-inference`](https://github.com/huggingface/text-generation-inference):


          ```shell

          model=OpenAssistant/oasst-sft-1-pythia-12b

          num_shard=4 # number of GPUs you want to use

          volume=$PWD/data # share a volume with the Docker container to avoid downloading
          weights every run


          docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
          --model-id $model --num-shard $num_shard

          ```


          Then you can query the model with [`text-generation`](https://pypi.org/project/text-generation/):


          ```python

          from text_generation import Client


          client = Client("http://localhost:8080")


          client.generate("<|prompter|>What''s the Earth total population<|endoftext|><|assistant|>").generated_text

          ```'
        updatedAt: '2023-03-16T18:04:40.073Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
        - kastan
    id: 64135a3874a580779ad43acf
    type: comment
  author: olivierdehaene
  content: 'You can use [`text-generation-inference`](https://github.com/huggingface/text-generation-inference):


    ```shell

    model=OpenAssistant/oasst-sft-1-pythia-12b

    num_shard=4 # number of GPUs you want to use

    volume=$PWD/data # share a volume with the Docker container to avoid downloading
    weights every run


    docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:latest
    --model-id $model --num-shard $num_shard

    ```


    Then you can query the model with [`text-generation`](https://pypi.org/project/text-generation/):


    ```python

    from text_generation import Client


    client = Client("http://localhost:8080")


    client.generate("<|prompter|>What''s the Earth total population<|endoftext|><|assistant|>").generated_text

    ```'
  created_at: 2023-03-16 17:04:40+00:00
  edited: false
  hidden: false
  id: 64135a3874a580779ad43acf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: OpenAssistant/oasst-sft-1-pythia-12b
repo_type: model
status: open
target_branch: null
title: How can I run the model with multiple GPUs?
