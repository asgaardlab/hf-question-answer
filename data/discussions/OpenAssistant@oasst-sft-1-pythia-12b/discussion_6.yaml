!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 93simonster
conflicting_files: null
created_at: 2023-04-12 08:52:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eb59759fca2c727350cc3ffd1e125b9c.svg
      fullname: Simone Martin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 93simonster
      type: user
    createdAt: '2023-04-12T09:52:01.000Z'
    data:
      edited: false
      editors:
      - 93simonster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eb59759fca2c727350cc3ffd1e125b9c.svg
          fullname: Simone Martin
          isHf: false
          isPro: false
          name: 93simonster
          type: user
        html: '<p>Hi all,</p>

          <p>I am writing today regarding an issue I encountered while attempting
          to run the "OpenAssistant/oasst-sft-1-pythia-12b" model in Google Colab
          using the provided template from Hugging Face. I used the following code:</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("OpenAssistant/oasst-sft-1-pythia-12b")<br>model
          = AutoModelForCausalLM.from_pretrained("OpenAssistant/oasst-sft-1-pythia-12b")</p>

          <p>While the model was loading, I encountered a "session crashed" message
          due to Google Colab running out of memory.<br>I am writing to ask if anyone
          has suggestions for possible workarounds that would allow me to run this
          model within Google Colab, or if there are alternative approaches to loading
          the model that would be more feasible given the resource constraints of
          Colab.</p>

          <p>Thank you in advance for your assistance.</p>

          <p>Best regards, Martin</p>

          '
        raw: "Hi all,\r\n\r\nI am writing today regarding an issue I encountered while\
          \ attempting to run the \"OpenAssistant/oasst-sft-1-pythia-12b\" model in\
          \ Google Colab using the provided template from Hugging Face. I used the\
          \ following code:\r\n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-1-pythia-12b\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-1-pythia-12b\"\
          )\r\n\r\nWhile the model was loading, I encountered a \"session crashed\"\
          \ message due to Google Colab running out of memory.\r\nI am writing to\
          \ ask if anyone has suggestions for possible workarounds that would allow\
          \ me to run this model within Google Colab, or if there are alternative\
          \ approaches to loading the model that would be more feasible given the\
          \ resource constraints of Colab.\r\n\r\nThank you in advance for your assistance.\r\
          \n\r\nBest regards, Martin"
        updatedAt: '2023-04-12T09:52:01.071Z'
      numEdits: 0
      reactions: []
    id: 64367f410b2c0d86d0b17246
    type: comment
  author: 93simonster
  content: "Hi all,\r\n\r\nI am writing today regarding an issue I encountered while\
    \ attempting to run the \"OpenAssistant/oasst-sft-1-pythia-12b\" model in Google\
    \ Colab using the provided template from Hugging Face. I used the following code:\r\
    \n\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"OpenAssistant/oasst-sft-1-pythia-12b\")\r\n\
    model = AutoModelForCausalLM.from_pretrained(\"OpenAssistant/oasst-sft-1-pythia-12b\"\
    )\r\n\r\nWhile the model was loading, I encountered a \"session crashed\" message\
    \ due to Google Colab running out of memory.\r\nI am writing to ask if anyone\
    \ has suggestions for possible workarounds that would allow me to run this model\
    \ within Google Colab, or if there are alternative approaches to loading the model\
    \ that would be more feasible given the resource constraints of Colab.\r\n\r\n\
    Thank you in advance for your assistance.\r\n\r\nBest regards, Martin"
  created_at: 2023-04-12 08:52:01+00:00
  edited: false
  hidden: false
  id: 64367f410b2c0d86d0b17246
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: OpenAssistant/oasst-sft-1-pythia-12b
repo_type: model
status: open
target_branch: null
title: How to run in Google Colab?
