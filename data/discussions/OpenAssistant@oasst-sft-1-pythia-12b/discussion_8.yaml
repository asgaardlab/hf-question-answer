!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ltechno
conflicting_files: null
created_at: 2023-05-05 08:24:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4af4f8b2912f5addffbb67c47bc6b547.svg
      fullname: lavaraja
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltechno
      type: user
    createdAt: '2023-05-05T09:24:33.000Z'
    data:
      edited: false
      editors:
      - ltechno
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4af4f8b2912f5addffbb67c47bc6b547.svg
          fullname: lavaraja
          isHf: false
          isPro: false
          name: ltechno
          type: user
        html: '<p>Tried running this model with AWS Sagemaker.  Have received the
          below error.</p>

          <p><code>An error occurred (ModelError) when calling the InvokeEndpoint
          operation: Received client error (400) from primary with message "{   "code":
          400,   "type": "InternalServerException",   "message": "Could not load model
          /.sagemaker/mms/models/OpenAssistant__oasst-sft-1-pythia-12b with any of
          the following classes: (\u003cclass \u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\u0027\u003e,
          \u003cclass \u0027transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM\u0027\u003e)."
          }</code><br>Tried with transformers 4.26.1 as well as latest version 4.28.0.</p>

          '
        raw: "Tried running this model with AWS Sagemaker.  Have received the below\
          \ error.\r\n\r\n``\r\nAn error occurred (ModelError) when calling the InvokeEndpoint\
          \ operation: Received client error (400) from primary with message \"{\r\
          \n  \"code\": 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\"\
          : \"Could not load model /.sagemaker/mms/models/OpenAssistant__oasst-sft-1-pythia-12b\
          \ with any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
          u0027\\u003e, \\u003cclass \\u0027transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM\\\
          u0027\\u003e).\"\r\n}\r\n``\r\nTried with transformers 4.26.1 as well as\
          \ latest version 4.28.0.\r\n"
        updatedAt: '2023-05-05T09:24:33.427Z'
      numEdits: 0
      reactions: []
    id: 6454cb511a543cf97b1b71a6
    type: comment
  author: ltechno
  content: "Tried running this model with AWS Sagemaker.  Have received the below\
    \ error.\r\n\r\n``\r\nAn error occurred (ModelError) when calling the InvokeEndpoint\
    \ operation: Received client error (400) from primary with message \"{\r\n  \"\
    code\": 400,\r\n  \"type\": \"InternalServerException\",\r\n  \"message\": \"\
    Could not load model /.sagemaker/mms/models/OpenAssistant__oasst-sft-1-pythia-12b\
    \ with any of the following classes: (\\u003cclass \\u0027transformers.models.auto.modeling_auto.AutoModelForCausalLM\\\
    u0027\\u003e, \\u003cclass \\u0027transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM\\\
    u0027\\u003e).\"\r\n}\r\n``\r\nTried with transformers 4.26.1 as well as latest\
    \ version 4.28.0.\r\n"
  created_at: 2023-05-05 08:24:33+00:00
  edited: false
  hidden: false
  id: 6454cb511a543cf97b1b71a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: OpenAssistant/oasst-sft-1-pythia-12b
repo_type: model
status: open
target_branch: null
title: Unable to run the model on Sagemaker with cpu instance.
