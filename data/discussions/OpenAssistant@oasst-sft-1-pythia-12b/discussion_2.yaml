!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Matan1905
conflicting_files: null
created_at: 2023-03-12 11:10:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545337cd3da0499d5de504bf164401ba.svg
      fullname: Matan Ellhayani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Matan1905
      type: user
    createdAt: '2023-03-12T12:10:00.000Z'
    data:
      edited: false
      editors:
      - Matan1905
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545337cd3da0499d5de504bf164401ba.svg
          fullname: Matan Ellhayani
          isHf: false
          isPro: false
          name: Matan1905
          type: user
        html: '<p>Hey, I''m new to hugging face and I tried to create a GPU gradio
          space using the deploy button, but it doesn''t work like it does in this
          main repo.<br>What am I missing?</p>

          '
        raw: "Hey, I'm new to hugging face and I tried to create a GPU gradio space\
          \ using the deploy button, but it doesn't work like it does in this main\
          \ repo.\r\nWhat am I missing?"
        updatedAt: '2023-03-12T12:10:00.297Z'
      numEdits: 0
      reactions: []
    id: 640dc1183830fd441c295cb3
    type: comment
  author: Matan1905
  content: "Hey, I'm new to hugging face and I tried to create a GPU gradio space\
    \ using the deploy button, but it doesn't work like it does in this main repo.\r\
    \nWhat am I missing?"
  created_at: 2023-03-12 11:10:00+00:00
  edited: false
  hidden: false
  id: 640dc1183830fd441c295cb3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d0597ff2341424c808b771/XsJsVaLVmqivOG6WxY7Uz.png?w=200&h=200&f=face
      fullname: oobabooga
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oobabooga
      type: user
    createdAt: '2023-03-12T13:55:50.000Z'
    data:
      edited: false
      editors:
      - oobabooga
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d0597ff2341424c808b771/XsJsVaLVmqivOG6WxY7Uz.png?w=200&h=200&f=face
          fullname: oobabooga
          isHf: false
          isPro: false
          name: oobabooga
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/253">https://github.com/oobabooga/text-generation-webui/issues/253</a></p>

          <p>It takes 13213MiB VRAM in 8-bit mode.</p>

          '
        raw: 'https://github.com/oobabooga/text-generation-webui/issues/253


          It takes 13213MiB VRAM in 8-bit mode.'
        updatedAt: '2023-03-12T13:55:50.207Z'
      numEdits: 0
      reactions: []
    id: 640dd9e69d4f48b8c722cf2e
    type: comment
  author: oobabooga
  content: 'https://github.com/oobabooga/text-generation-webui/issues/253


    It takes 13213MiB VRAM in 8-bit mode.'
  created_at: 2023-03-12 12:55:50+00:00
  edited: false
  hidden: false
  id: 640dd9e69d4f48b8c722cf2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-03-13T19:43:52.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: "<p>Juste use the HuggingFace <code>text-generation</code> client:</p>\n\
          <pre><code class=\"language-shell\">pip install text-generation\n</code></pre>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ text_generation <span class=\"hljs-keyword\">import</span> InferenceAPIClient\n\
          \nclient = InferenceAPIClient(<span class=\"hljs-string\">\"OpenAssistant/oasst-sft-1-pythia-12b\"\
          </span>)\ntext = client.generate(<span class=\"hljs-string\">\"&lt;|prompter|&gt;Why\
          \ is the sky blue?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"</span>).generated_text\n\
          <span class=\"hljs-built_in\">print</span>(text)\n\n<span class=\"hljs-comment\"\
          ># Token Streaming</span>\ntext = <span class=\"hljs-string\">\"\"</span>\n\
          <span class=\"hljs-keyword\">for</span> response <span class=\"hljs-keyword\"\
          >in</span> client.generate_stream(<span class=\"hljs-string\">\"&lt;|prompter|&gt;Why\
          \ is the sky blue?&lt;|endoftext|&gt;&lt;|assistant|&gt;\"</span>):\n  \
          \ <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\">not</span>\
          \ response.token.special:\n       <span class=\"hljs-built_in\">print</span>(response.token.text)\n\
          \       text += response.token.text\n<span class=\"hljs-built_in\">print</span>(text)\n\
          </code></pre>\n"
        raw: "Juste use the HuggingFace `text-generation` client:\n\n```shell\npip\
          \ install text-generation\n```\n\n```python\nfrom text_generation import\
          \ InferenceAPIClient\n\nclient = InferenceAPIClient(\"OpenAssistant/oasst-sft-1-pythia-12b\"\
          )\ntext = client.generate(\"<|prompter|>Why is the sky blue?<|endoftext|><|assistant|>\"\
          ).generated_text\nprint(text)\n\n# Token Streaming\ntext = \"\"\nfor response\
          \ in client.generate_stream(\"<|prompter|>Why is the sky blue?<|endoftext|><|assistant|>\"\
          ):\n   if not response.token.special:\n       print(response.token.text)\n\
          \       text += response.token.text\nprint(text)\n```"
        updatedAt: '2023-03-13T19:43:52.955Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - antonovmaxim
        - lewiswatson
        - MaziyarPanahi
        - iamrobotbear
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - iamrobotbear
    id: 640f7cf8308cb20cd4ebc923
    type: comment
  author: olivierdehaene
  content: "Juste use the HuggingFace `text-generation` client:\n\n```shell\npip install\
    \ text-generation\n```\n\n```python\nfrom text_generation import InferenceAPIClient\n\
    \nclient = InferenceAPIClient(\"OpenAssistant/oasst-sft-1-pythia-12b\")\ntext\
    \ = client.generate(\"<|prompter|>Why is the sky blue?<|endoftext|><|assistant|>\"\
    ).generated_text\nprint(text)\n\n# Token Streaming\ntext = \"\"\nfor response\
    \ in client.generate_stream(\"<|prompter|>Why is the sky blue?<|endoftext|><|assistant|>\"\
    ):\n   if not response.token.special:\n       print(response.token.text)\n   \
    \    text += response.token.text\nprint(text)\n```"
  created_at: 2023-03-13 18:43:52+00:00
  edited: false
  hidden: false
  id: 640f7cf8308cb20cd4ebc923
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/545337cd3da0499d5de504bf164401ba.svg
      fullname: Matan Ellhayani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Matan1905
      type: user
    createdAt: '2023-03-13T20:04:01.000Z'
    data:
      edited: false
      editors:
      - Matan1905
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/545337cd3da0499d5de504bf164401ba.svg
          fullname: Matan Ellhayani
          isHf: false
          isPro: false
          name: Matan1905
          type: user
        html: '<p>Thanks!</p>

          '
        raw: Thanks!
        updatedAt: '2023-03-13T20:04:01.833Z'
      numEdits: 0
      reactions: []
    id: 640f81b1308cb20cd4ebf3a6
    type: comment
  author: Matan1905
  content: Thanks!
  created_at: 2023-03-13 19:04:01+00:00
  edited: false
  hidden: false
  id: 640f81b1308cb20cd4ebf3a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-03-13T21:23:16.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>It seems we have a small issue with the model we are stopping the
          api-inference support for now while we figure out what''s wrong.</p>

          '
        raw: It seems we have a small issue with the model we are stopping the api-inference
          support for now while we figure out what's wrong.
        updatedAt: '2023-03-13T21:23:16.228Z'
      numEdits: 0
      reactions: []
    id: 640f9444df68b86bf8ecfa64
    type: comment
  author: olivierdehaene
  content: It seems we have a small issue with the model we are stopping the api-inference
    support for now while we figure out what's wrong.
  created_at: 2023-03-13 20:23:16+00:00
  edited: false
  hidden: false
  id: 640f9444df68b86bf8ecfa64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634c48a63d11eaedd88c7c4b/R1yITk1vNVbqQ9yNpdagm.png?w=200&h=200&f=face
      fullname: MrlolDev
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrlolDev
      type: user
    createdAt: '2023-03-14T05:22:08.000Z'
    data:
      edited: false
      editors:
      - MrlolDev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/634c48a63d11eaedd88c7c4b/R1yITk1vNVbqQ9yNpdagm.png?w=200&h=200&f=face
          fullname: MrlolDev
          isHf: false
          isPro: false
          name: MrlolDev
          type: user
        html: '<p>Any  estimated time when it will come back?</p>

          '
        raw: Any  estimated time when it will come back?
        updatedAt: '2023-03-14T05:22:08.659Z'
      numEdits: 0
      reactions: []
    id: 64100480308cb20cd4f02d82
    type: comment
  author: MrlolDev
  content: Any  estimated time when it will come back?
  created_at: 2023-03-14 04:22:08+00:00
  edited: false
  hidden: false
  id: 64100480308cb20cd4f02d82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-03-15T09:47:12.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>The issue is quite deep in Transformers. You can track its evolution
          here: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/22161">https://github.com/huggingface/transformers/issues/22161</a><br>While
          we find a solution, inference API will unfortunately stay off for this model.</p>

          '
        raw: 'The issue is quite deep in Transformers. You can track its evolution
          here: https://github.com/huggingface/transformers/issues/22161

          While we find a solution, inference API will unfortunately stay off for
          this model.'
        updatedAt: '2023-03-15T09:47:12.390Z'
      numEdits: 0
      reactions: []
    id: 6411942083efe367dc7a67ca
    type: comment
  author: olivierdehaene
  content: 'The issue is quite deep in Transformers. You can track its evolution here:
    https://github.com/huggingface/transformers/issues/22161

    While we find a solution, inference API will unfortunately stay off for this model.'
  created_at: 2023-03-15 08:47:12+00:00
  edited: false
  hidden: false
  id: 6411942083efe367dc7a67ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-03-15T13:08:01.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>The issue should be fixed and the inference API is back online.<br>If
          you see any weird outputs were the model seem to always repeat the same
          token, please inform us here.</p>

          '
        raw: 'The issue should be fixed and the inference API is back online.

          If you see any weird outputs were the model seem to always repeat the same
          token, please inform us here.'
        updatedAt: '2023-03-15T13:08:01.729Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - CactiStaccingCrane
        - andreaskoepf
        - Kagerage
        - MaziyarPanahi
    id: 6411c3313a623ab3c8b2a872
    type: comment
  author: olivierdehaene
  content: 'The issue should be fixed and the inference API is back online.

    If you see any weird outputs were the model seem to always repeat the same token,
    please inform us here.'
  created_at: 2023-03-15 12:08:01+00:00
  edited: false
  hidden: false
  id: 6411c3313a623ab3c8b2a872
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
      fullname: coyote79
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt79
      type: user
    createdAt: '2023-04-25T17:08:31.000Z'
    data:
      edited: false
      editors:
      - cyt79
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
          fullname: coyote79
          isHf: false
          isPro: false
          name: cyt79
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ Is there a way to generate multiple sequence with client.generete call?\
          \ (such as setting num_return_sequences parameter to a number greater than\
          \ 1 in model.generate() call)</p>\n"
        raw: '@olivierdehaene Is there a way to generate multiple sequence with client.generete
          call? (such as setting num_return_sequences parameter to a number greater
          than 1 in model.generate() call)'
        updatedAt: '2023-04-25T17:08:31.253Z'
      numEdits: 0
      reactions: []
    id: 6448090f3e498d66919596df
    type: comment
  author: cyt79
  content: '@olivierdehaene Is there a way to generate multiple sequence with client.generete
    call? (such as setting num_return_sequences parameter to a number greater than
    1 in model.generate() call)'
  created_at: 2023-04-25 16:08:31+00:00
  edited: false
  hidden: false
  id: 6448090f3e498d66919596df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-04-25T17:10:50.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: '<p>You can use the best of parameter.<br>Or simply do multiple calls.</p>

          '
        raw: "You can use the best of parameter. \nOr simply do multiple calls."
        updatedAt: '2023-04-25T17:10:50.195Z'
      numEdits: 0
      reactions: []
    id: 6448099ae54b488070b1f033
    type: comment
  author: olivierdehaene
  content: "You can use the best of parameter. \nOr simply do multiple calls."
  created_at: 2023-04-25 16:10:50+00:00
  edited: false
  hidden: false
  id: 6448099ae54b488070b1f033
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
      fullname: coyote79
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt79
      type: user
    createdAt: '2023-04-26T06:14:27.000Z'
    data:
      edited: false
      editors:
      - cyt79
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
          fullname: coyote79
          isHf: false
          isPro: false
          name: cyt79
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;olivierdehaene&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/olivierdehaene\"\
          >@<span class=\"underline\">olivierdehaene</span></a></span>\n\n\t</span></span>\
          \ thanks for your reply. I can increase best of parameter up to 2. Another\
          \ quesiton is, how can I increase the max length of generated text? It seems\
          \ like the client.generate() method doesn't have \"max_length\" parameter.</p>\n"
        raw: '@olivierdehaene thanks for your reply. I can increase best of parameter
          up to 2. Another quesiton is, how can I increase the max length of generated
          text? It seems like the client.generate() method doesn''t have "max_length"
          parameter.'
        updatedAt: '2023-04-26T06:14:27.009Z'
      numEdits: 0
      reactions: []
    id: 6448c143f88f1495f08e727d
    type: comment
  author: cyt79
  content: '@olivierdehaene thanks for your reply. I can increase best of parameter
    up to 2. Another quesiton is, how can I increase the max length of generated text?
    It seems like the client.generate() method doesn''t have "max_length" parameter.'
  created_at: 2023-04-26 05:14:27+00:00
  edited: false
  hidden: false
  id: 6448c143f88f1495f08e727d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
      fullname: Olivier Dehaene
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: olivierdehaene
      type: user
    createdAt: '2023-04-26T07:42:49.000Z'
    data:
      edited: false
      editors:
      - olivierdehaene
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a093d63e7d1dda047039fc/QGpVSKuJLwl2EsiffCYML.jpeg?w=200&h=200&f=face
          fullname: Olivier Dehaene
          isHf: true
          isPro: false
          name: olivierdehaene
          type: user
        html: "<pre><code class=\"language-python\"><span class=\"hljs-keyword\">from</span>\
          \ text_generation <span class=\"hljs-keyword\">import</span> Client\n\n\
          Client.generate?\n</code></pre>\n<p>Gives you: </p>\n<pre><code class=\"\
          language-python\">Signature:\nClient.generate(\n    self,\n    prompt: <span\
          \ class=\"hljs-built_in\">str</span>,\n    do_sample: <span class=\"hljs-built_in\"\
          >bool</span> = <span class=\"hljs-literal\">False</span>,\n    max_new_tokens:\
          \ <span class=\"hljs-built_in\">int</span> = <span class=\"hljs-number\"\
          >20</span>,\n    best_of: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-built_in\">int</span>] = <span class=\"hljs-literal\">None</span>,\n\
          \    repetition_penalty: <span class=\"hljs-type\">Optional</span>[<span\
          \ class=\"hljs-built_in\">float</span>] = <span class=\"hljs-literal\">None</span>,\n\
          \    return_full_text: <span class=\"hljs-built_in\">bool</span> = <span\
          \ class=\"hljs-literal\">False</span>,\n    seed: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-built_in\">int</span>] = <span class=\"\
          hljs-literal\">None</span>,\n    stop_sequences: <span class=\"hljs-type\"\
          >Optional</span>[<span class=\"hljs-type\">List</span>[<span class=\"hljs-built_in\"\
          >str</span>]] = <span class=\"hljs-literal\">None</span>,\n    temperature:\
          \ <span class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\"\
          >float</span>] = <span class=\"hljs-literal\">None</span>,\n    top_k: <span\
          \ class=\"hljs-type\">Optional</span>[<span class=\"hljs-built_in\">int</span>]\
          \ = <span class=\"hljs-literal\">None</span>,\n    top_p: <span class=\"\
          hljs-type\">Optional</span>[<span class=\"hljs-built_in\">float</span>]\
          \ = <span class=\"hljs-literal\">None</span>,\n    truncate: <span class=\"\
          hljs-type\">Optional</span>[<span class=\"hljs-built_in\">int</span>] =\
          \ <span class=\"hljs-literal\">None</span>,\n    typical_p: <span class=\"\
          hljs-type\">Optional</span>[<span class=\"hljs-built_in\">float</span>]\
          \ = <span class=\"hljs-literal\">None</span>,\n    watermark: <span class=\"\
          hljs-built_in\">bool</span> = <span class=\"hljs-literal\">False</span>,\n\
          ) -&gt; text_generation.types.Response\nDocstring:\nGiven a prompt, generate\
          \ the following text\n\nArgs:\n    prompt (`<span class=\"hljs-built_in\"\
          >str</span>`):\n        Input text\n    do_sample (`<span class=\"hljs-built_in\"\
          >bool</span>`):\n        Activate logits sampling\n    max_new_tokens (`<span\
          \ class=\"hljs-built_in\">int</span>`):\n        Maximum number of generated\
          \ tokens\n    best_of (`<span class=\"hljs-built_in\">int</span>`):\n  \
          \      Generate best_of sequences <span class=\"hljs-keyword\">and</span>\
          \ <span class=\"hljs-keyword\">return</span> the one <span class=\"hljs-keyword\"\
          >if</span> the highest token logprobs\n    repetition_penalty (`<span class=\"\
          hljs-built_in\">float</span>`):\n        The parameter <span class=\"hljs-keyword\"\
          >for</span> repetition penalty. <span class=\"hljs-number\">1.0</span> means\
          \ no penalty. See [this\n        paper](https://arxiv.org/pdf/<span class=\"\
          hljs-number\">1909.05858</span>.pdf) <span class=\"hljs-keyword\">for</span>\
          \ more details.\n    return_full_text (`<span class=\"hljs-built_in\">bool</span>`):\n\
          \        Whether to prepend the prompt to the generated text\n    seed (`<span\
          \ class=\"hljs-built_in\">int</span>`):\n        Random sampling seed\n\
          \    stop_sequences (`<span class=\"hljs-type\">List</span>[<span class=\"\
          hljs-built_in\">str</span>]`):\n        Stop generating tokens <span class=\"\
          hljs-keyword\">if</span> a member of `stop_sequences` <span class=\"hljs-keyword\"\
          >is</span> generated\n    temperature (`<span class=\"hljs-built_in\">float</span>`):\n\
          \        The value used to module the logits distribution.\n    top_k (`<span\
          \ class=\"hljs-built_in\">int</span>`):\n        The number of highest probability\
          \ vocabulary tokens to keep <span class=\"hljs-keyword\">for</span> top-k-filtering.\n\
          \    top_p (`<span class=\"hljs-built_in\">float</span>`):\n        If <span\
          \ class=\"hljs-built_in\">set</span> to &lt; <span class=\"hljs-number\"\
          >1</span>, only the smallest <span class=\"hljs-built_in\">set</span> of\
          \ most probable tokens <span class=\"hljs-keyword\">with</span> probabilities\
          \ that add up to `top_p` <span class=\"hljs-keyword\">or</span>\n      \
          \  higher are kept <span class=\"hljs-keyword\">for</span> generation.\n\
          \    truncate (`<span class=\"hljs-built_in\">int</span>`):\n        Truncate\
          \ inputs tokens to the given size\n    typical_p (`<span class=\"hljs-built_in\"\
          >float</span>`):\n        Typical Decoding mass\n        See [Typical Decoding\
          \ <span class=\"hljs-keyword\">for</span> Natural Language Generation](https://arxiv.org/<span\
          \ class=\"hljs-built_in\">abs</span>/<span class=\"hljs-number\">2202.00666</span>)\
          \ <span class=\"hljs-keyword\">for</span> more information\n    watermark\
          \ (`<span class=\"hljs-built_in\">bool</span>`):\n        Watermarking <span\
          \ class=\"hljs-keyword\">with</span> [A Watermark <span class=\"hljs-keyword\"\
          >for</span> Large Language Models](https://arxiv.org/<span class=\"hljs-built_in\"\
          >abs</span>/<span class=\"hljs-number\">2301.10226</span>)\n\nReturns:\n\
          \    Response: generated response\n</code></pre>\n<p>You need to use the\
          \ <code>max_new_tokens</code> parameter.</p>\n"
        raw: "```python\nfrom text_generation import Client\n\nClient.generate?\n\
          ```\nGives you: \n\n```python\nSignature:\nClient.generate(\n    self,\n\
          \    prompt: str,\n    do_sample: bool = False,\n    max_new_tokens: int\
          \ = 20,\n    best_of: Optional[int] = None,\n    repetition_penalty: Optional[float]\
          \ = None,\n    return_full_text: bool = False,\n    seed: Optional[int]\
          \ = None,\n    stop_sequences: Optional[List[str]] = None,\n    temperature:\
          \ Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p:\
          \ Optional[float] = None,\n    truncate: Optional[int] = None,\n    typical_p:\
          \ Optional[float] = None,\n    watermark: bool = False,\n) -> text_generation.types.Response\n\
          Docstring:\nGiven a prompt, generate the following text\n\nArgs:\n    prompt\
          \ (`str`):\n        Input text\n    do_sample (`bool`):\n        Activate\
          \ logits sampling\n    max_new_tokens (`int`):\n        Maximum number of\
          \ generated tokens\n    best_of (`int`):\n        Generate best_of sequences\
          \ and return the one if the highest token logprobs\n    repetition_penalty\
          \ (`float`):\n        The parameter for repetition penalty. 1.0 means no\
          \ penalty. See [this\n        paper](https://arxiv.org/pdf/1909.05858.pdf)\
          \ for more details.\n    return_full_text (`bool`):\n        Whether to\
          \ prepend the prompt to the generated text\n    seed (`int`):\n        Random\
          \ sampling seed\n    stop_sequences (`List[str]`):\n        Stop generating\
          \ tokens if a member of `stop_sequences` is generated\n    temperature (`float`):\n\
          \        The value used to module the logits distribution.\n    top_k (`int`):\n\
          \        The number of highest probability vocabulary tokens to keep for\
          \ top-k-filtering.\n    top_p (`float`):\n        If set to < 1, only the\
          \ smallest set of most probable tokens with probabilities that add up to\
          \ `top_p` or\n        higher are kept for generation.\n    truncate (`int`):\n\
          \        Truncate inputs tokens to the given size\n    typical_p (`float`):\n\
          \        Typical Decoding mass\n        See [Typical Decoding for Natural\
          \ Language Generation](https://arxiv.org/abs/2202.00666) for more information\n\
          \    watermark (`bool`):\n        Watermarking with [A Watermark for Large\
          \ Language Models](https://arxiv.org/abs/2301.10226)\n\nReturns:\n    Response:\
          \ generated response\n```\n\nYou need to use the `max_new_tokens` parameter."
        updatedAt: '2023-04-26T07:42:49.334Z'
      numEdits: 0
      reactions: []
    id: 6448d5f9d16a70c0158b85ec
    type: comment
  author: olivierdehaene
  content: "```python\nfrom text_generation import Client\n\nClient.generate?\n```\n\
    Gives you: \n\n```python\nSignature:\nClient.generate(\n    self,\n    prompt:\
    \ str,\n    do_sample: bool = False,\n    max_new_tokens: int = 20,\n    best_of:\
    \ Optional[int] = None,\n    repetition_penalty: Optional[float] = None,\n   \
    \ return_full_text: bool = False,\n    seed: Optional[int] = None,\n    stop_sequences:\
    \ Optional[List[str]] = None,\n    temperature: Optional[float] = None,\n    top_k:\
    \ Optional[int] = None,\n    top_p: Optional[float] = None,\n    truncate: Optional[int]\
    \ = None,\n    typical_p: Optional[float] = None,\n    watermark: bool = False,\n\
    ) -> text_generation.types.Response\nDocstring:\nGiven a prompt, generate the\
    \ following text\n\nArgs:\n    prompt (`str`):\n        Input text\n    do_sample\
    \ (`bool`):\n        Activate logits sampling\n    max_new_tokens (`int`):\n \
    \       Maximum number of generated tokens\n    best_of (`int`):\n        Generate\
    \ best_of sequences and return the one if the highest token logprobs\n    repetition_penalty\
    \ (`float`):\n        The parameter for repetition penalty. 1.0 means no penalty.\
    \ See [this\n        paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n\
    \    return_full_text (`bool`):\n        Whether to prepend the prompt to the\
    \ generated text\n    seed (`int`):\n        Random sampling seed\n    stop_sequences\
    \ (`List[str]`):\n        Stop generating tokens if a member of `stop_sequences`\
    \ is generated\n    temperature (`float`):\n        The value used to module the\
    \ logits distribution.\n    top_k (`int`):\n        The number of highest probability\
    \ vocabulary tokens to keep for top-k-filtering.\n    top_p (`float`):\n     \
    \   If set to < 1, only the smallest set of most probable tokens with probabilities\
    \ that add up to `top_p` or\n        higher are kept for generation.\n    truncate\
    \ (`int`):\n        Truncate inputs tokens to the given size\n    typical_p (`float`):\n\
    \        Typical Decoding mass\n        See [Typical Decoding for Natural Language\
    \ Generation](https://arxiv.org/abs/2202.00666) for more information\n    watermark\
    \ (`bool`):\n        Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)\n\
    \nReturns:\n    Response: generated response\n```\n\nYou need to use the `max_new_tokens`\
    \ parameter."
  created_at: 2023-04-26 06:42:49+00:00
  edited: false
  hidden: false
  id: 6448d5f9d16a70c0158b85ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2023-05-12T09:45:24.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: '<p>What''s the best practice to have external context/input in this
          prompt format? How to convert something like this:</p>

          <pre><code>Answer the question based on the context below. If the

          question cannot be answered using the information provided answer

          with "I don''t know".


          Context: Large Language Models (LLMs) are the latest models used in NLP.

          Their superior performance over smaller models has made them incredibly

          useful for developers building NLP enabled applications. These models

          can be accessed via Hugging Face''s `transformers` library, via OpenAI

          using the `openai` library, and via Spark NLP using the `spark-nlp` library.


          Question: Which libraries and model providers offer LLMs?


          Answer:

          </code></pre>

          '
        raw: 'What''s the best practice to have external context/input in this prompt
          format? How to convert something like this:


          ```

          Answer the question based on the context below. If the

          question cannot be answered using the information provided answer

          with "I don''t know".


          Context: Large Language Models (LLMs) are the latest models used in NLP.

          Their superior performance over smaller models has made them incredibly

          useful for developers building NLP enabled applications. These models

          can be accessed via Hugging Face''s `transformers` library, via OpenAI

          using the `openai` library, and via Spark NLP using the `spark-nlp` library.


          Question: Which libraries and model providers offer LLMs?


          Answer:

          ```'
        updatedAt: '2023-05-12T09:45:24.899Z'
      numEdits: 0
      reactions: []
    id: 645e0ab4d323ebc559bc8056
    type: comment
  author: MaziyarPanahi
  content: 'What''s the best practice to have external context/input in this prompt
    format? How to convert something like this:


    ```

    Answer the question based on the context below. If the

    question cannot be answered using the information provided answer

    with "I don''t know".


    Context: Large Language Models (LLMs) are the latest models used in NLP.

    Their superior performance over smaller models has made them incredibly

    useful for developers building NLP enabled applications. These models

    can be accessed via Hugging Face''s `transformers` library, via OpenAI

    using the `openai` library, and via Spark NLP using the `spark-nlp` library.


    Question: Which libraries and model providers offer LLMs?


    Answer:

    ```'
  created_at: 2023-05-12 08:45:24+00:00
  edited: false
  hidden: false
  id: 645e0ab4d323ebc559bc8056
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: OpenAssistant/oasst-sft-1-pythia-12b
repo_type: model
status: open
target_branch: null
title: How can I run this?
