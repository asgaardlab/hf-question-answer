!!python/object:huggingface_hub.community.DiscussionWithDetails
author: russellsparadox
conflicting_files: null
created_at: 2023-04-07 12:50:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667582983668-63654b9f98da81987e215a5b.jpeg?w=200&h=200&f=face
      fullname: Sergey Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: russellsparadox
      type: user
    createdAt: '2023-04-07T13:50:06.000Z'
    data:
      edited: false
      editors:
      - russellsparadox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667582983668-63654b9f98da81987e215a5b.jpeg?w=200&h=200&f=face
          fullname: Sergey Ivanov
          isHf: false
          isPro: false
          name: russellsparadox
          type: user
        html: '<p>All the time I use the model, it generates only &lt;|endoftext|&gt;.
          How can I fix it? </p>

          <pre><code>from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM,
          GPTNeoXForCausalLM, AutoTokenizer


          checkpoint = "OpenAssistant/oasst-sft-1-pythia-12b"


          tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=''models_hf'')

          model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto",
          load_in_8bit=True, cache_dir=''models_hf'')


          message = "Hello, I am"

          inp = "&lt;|prompter|&gt;"+message+"&lt;|endoftext|&gt;&lt;|assistant|&gt;"

          data = tokenizer([inp], return_tensors="pt")

          data = {k: v.to(model.device) for k, v in data.items() if k in ("input_ids",
          "attention_mask")}

          outputs = model.generate(**data)

          print(tokenizer.decode(outputs[0]))

          </code></pre>

          <p><code>&lt;|prompter|&gt;Hello, I am&lt;|endoftext|&gt;&lt;|assistant|&gt;&lt;|endoftext|&gt;</code></p>

          '
        raw: "All the time I use the model, it generates only <|endoftext|>. How can\
          \ I fix it? \r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoModelForSeq2SeqLM, GPTNeoXForCausalLM, AutoTokenizer\r\n\r\ncheckpoint\
          \ = \"OpenAssistant/oasst-sft-1-pythia-12b\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint,\
          \ cache_dir='models_hf')\r\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ device_map=\"auto\", load_in_8bit=True, cache_dir='models_hf')\r\n\r\n\
          message = \"Hello, I am\"\r\ninp = \"<|prompter|>\"+message+\"<|endoftext|><|assistant|>\"\
          \r\ndata = tokenizer([inp], return_tensors=\"pt\")\r\ndata = {k: v.to(model.device)\
          \ for k, v in data.items() if k in (\"input_ids\", \"attention_mask\")}\r\
          \noutputs = model.generate(**data)\r\nprint(tokenizer.decode(outputs[0]))\r\
          \n```\r\n\r\n`<|prompter|>Hello, I am<|endoftext|><|assistant|><|endoftext|>`"
        updatedAt: '2023-04-07T13:50:06.837Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - hanifabdlh
    id: 64301f8ea043f0ac7df10c90
    type: comment
  author: russellsparadox
  content: "All the time I use the model, it generates only <|endoftext|>. How can\
    \ I fix it? \r\n\r\n```\r\nfrom transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM,\
    \ GPTNeoXForCausalLM, AutoTokenizer\r\n\r\ncheckpoint = \"OpenAssistant/oasst-sft-1-pythia-12b\"\
    \r\n\r\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir='models_hf')\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\"\
    , load_in_8bit=True, cache_dir='models_hf')\r\n\r\nmessage = \"Hello, I am\"\r\
    \ninp = \"<|prompter|>\"+message+\"<|endoftext|><|assistant|>\"\r\ndata = tokenizer([inp],\
    \ return_tensors=\"pt\")\r\ndata = {k: v.to(model.device) for k, v in data.items()\
    \ if k in (\"input_ids\", \"attention_mask\")}\r\noutputs = model.generate(**data)\r\
    \nprint(tokenizer.decode(outputs[0]))\r\n```\r\n\r\n`<|prompter|>Hello, I am<|endoftext|><|assistant|><|endoftext|>`"
  created_at: 2023-04-07 12:50:06+00:00
  edited: false
  hidden: false
  id: 64301f8ea043f0ac7df10c90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d0cfd2485856cd71131c31/kbmt5cO6Gu3iDyJxhYiJ8.jpeg?w=200&h=200&f=face
      fullname: Hanif Yuli Abdillah P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hanifabdlh
      type: user
    createdAt: '2023-04-12T08:48:43.000Z'
    data:
      edited: false
      editors:
      - hanifabdlh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d0cfd2485856cd71131c31/kbmt5cO6Gu3iDyJxhYiJ8.jpeg?w=200&h=200&f=face
          fullname: Hanif Yuli Abdillah P
          isHf: false
          isPro: false
          name: hanifabdlh
          type: user
        html: '<p>same issues, i use example code from here : <a href="https://huggingface.co/EleutherAI/pythia-12b-deduped">https://huggingface.co/EleutherAI/pythia-12b-deduped</a></p>

          '
        raw: 'same issues, i use example code from here : https://huggingface.co/EleutherAI/pythia-12b-deduped'
        updatedAt: '2023-04-12T08:48:43.644Z'
      numEdits: 0
      reactions: []
    id: 6436706b89abcb5f842df23f
    type: comment
  author: hanifabdlh
  content: 'same issues, i use example code from here : https://huggingface.co/EleutherAI/pythia-12b-deduped'
  created_at: 2023-04-12 07:48:43+00:00
  edited: false
  hidden: false
  id: 6436706b89abcb5f842df23f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aae741d00ed1f5ead516c07543e59f3e.svg
      fullname: yotam eshel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yeshel
      type: user
    createdAt: '2023-04-18T13:15:55.000Z'
    data:
      edited: false
      editors:
      - yeshel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aae741d00ed1f5ead516c07543e59f3e.svg
          fullname: yotam eshel
          isHf: false
          isPro: false
          name: yeshel
          type: user
        html: '<p>Having the same issue. </p>

          <p>My two cents -<br>One cent: If I run a forward pass, the logits are all
          nan<br>Two cents: Running the cpu version works fine</p>

          '
        raw: "Having the same issue. \n\nMy two cents -\nOne cent: If I run a forward\
          \ pass, the logits are all nan\nTwo cents: Running the cpu version works\
          \ fine"
        updatedAt: '2023-04-18T13:15:55.392Z'
      numEdits: 0
      reactions: []
    id: 643e980bab725487d8eaa0de
    type: comment
  author: yeshel
  content: "Having the same issue. \n\nMy two cents -\nOne cent: If I run a forward\
    \ pass, the logits are all nan\nTwo cents: Running the cpu version works fine"
  created_at: 2023-04-18 12:15:55+00:00
  edited: false
  hidden: false
  id: 643e980bab725487d8eaa0de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aae741d00ed1f5ead516c07543e59f3e.svg
      fullname: yotam eshel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yeshel
      type: user
    createdAt: '2023-04-18T13:42:56.000Z'
    data:
      edited: false
      editors:
      - yeshel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aae741d00ed1f5ead516c07543e59f3e.svg
          fullname: yotam eshel
          isHf: false
          isPro: false
          name: yeshel
          type: user
        html: '<p>Three cents: works well without load_in_8bit=True. (using 2 gpus
          in my case)</p>

          '
        raw: 'Three cents: works well without load_in_8bit=True. (using 2 gpus in
          my case)'
        updatedAt: '2023-04-18T13:42:56.605Z'
      numEdits: 0
      reactions: []
    id: 643e9e6052885f3d3f5670db
    type: comment
  author: yeshel
  content: 'Three cents: works well without load_in_8bit=True. (using 2 gpus in my
    case)'
  created_at: 2023-04-18 12:42:56+00:00
  edited: false
  hidden: false
  id: 643e9e6052885f3d3f5670db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d830c33c744fcbf8eef250915d938223.svg
      fullname: David Hung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davidhung
      type: user
    createdAt: '2023-05-30T16:13:34.000Z'
    data:
      edited: false
      editors:
      - davidhung
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d830c33c744fcbf8eef250915d938223.svg
          fullname: David Hung
          isHf: false
          isPro: false
          name: davidhung
          type: user
        html: '<p>I have the same problem. It doens''t work with 8 bit loader</p>

          '
        raw: I have the same problem. It doens't work with 8 bit loader
        updatedAt: '2023-05-30T16:13:34.400Z'
      numEdits: 0
      reactions: []
    id: 647620aeb2ef3a5a82000a5b
    type: comment
  author: davidhung
  content: I have the same problem. It doens't work with 8 bit loader
  created_at: 2023-05-30 15:13:34+00:00
  edited: false
  hidden: false
  id: 647620aeb2ef3a5a82000a5b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: OpenAssistant/oasst-sft-1-pythia-12b
repo_type: model
status: open
target_branch: null
title: Does not generate text
