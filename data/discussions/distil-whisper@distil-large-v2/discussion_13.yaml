!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kadriu
conflicting_files: null
created_at: 2023-11-07 13:28:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3cc94b6475f4551cfa431409e3baf249.svg
      fullname: kadriu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kadriu
      type: user
    createdAt: '2023-11-07T13:28:11.000Z'
    data:
      edited: false
      editors:
      - kadriu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7978333234786987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3cc94b6475f4551cfa431409e3baf249.svg
          fullname: kadriu
          isHf: false
          isPro: false
          name: kadriu
          type: user
        html: '<p>Can it be fine-tuned using MPS backend?</p>

          '
        raw: Can it be fine-tuned using MPS backend?
        updatedAt: '2023-11-07T13:28:11.167Z'
      numEdits: 0
      reactions: []
    id: 654a3b6b94d809b4e73fd641
    type: comment
  author: kadriu
  content: Can it be fine-tuned using MPS backend?
  created_at: 2023-11-07 13:28:11+00:00
  edited: false
  hidden: false
  id: 654a3b6b94d809b4e73fd641
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ce49364403c1cf482dc5c51d9a7428b.svg
      fullname: wentao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iPanda
      type: user
    createdAt: '2023-11-10T03:06:05.000Z'
    data:
      edited: false
      editors:
      - iPanda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3078993558883667
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ce49364403c1cf482dc5c51d9a7428b.svg
          fullname: wentao
          isHf: false
          isPro: false
          name: iPanda
          type: user
        html: "<p>yes\uFF0Cyou can. On macOS you can change the following code<br>\
          \ <code><br>device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\
          </code></p><code>\n</code><p><code>torch_dtype = torch.float16 if torch.cuda.is_available()\
          \ else torch.float32<br></code><br>to this one:<br><code><br>device = \"\
          mps\" if torch.backends.mps.is_available() else \"cpu\"</code></p><code>\n\
          </code><p><code>torch_dtype = torch.float16 if torch.backends.mps.is_available()\
          \ else torch.float32<br></code></p>\n"
        raw: "yes\uFF0Cyou can. On macOS you can change the following code \n <code>\n\
          device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntorch_dtype\
          \ = torch.float16 if torch.cuda.is_available() else torch.float32\n</code>\n\
          to this one:\n<code>\ndevice = \"mps\" if torch.backends.mps.is_available()\
          \ else \"cpu\"\n\ntorch_dtype = torch.float16 if torch.backends.mps.is_available()\
          \ else torch.float32\n</code>"
        updatedAt: '2023-11-10T03:06:05.245Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - sanchit-gandhi
        - kadriu
    id: 654d9e1da7a4001e61e23761
    type: comment
  author: iPanda
  content: "yes\uFF0Cyou can. On macOS you can change the following code \n <code>\n\
    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\ntorch_dtype =\
    \ torch.float16 if torch.cuda.is_available() else torch.float32\n</code>\nto this\
    \ one:\n<code>\ndevice = \"mps\" if torch.backends.mps.is_available() else \"\
    cpu\"\n\ntorch_dtype = torch.float16 if torch.backends.mps.is_available() else\
    \ torch.float32\n</code>"
  created_at: 2023-11-10 03:06:05+00:00
  edited: false
  hidden: false
  id: 654d9e1da7a4001e61e23761
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-13T14:47:36.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7435714602470398
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>You should be able to then run fine-tuning as per this <a href="https://huggingface.co/blog/fine-tune-whisper">blog
          post</a> with the HF Trainer - simply swap out the model checkpoint <code>openai/whisper-small</code>
          for <code>distil-whisper/distil-large-v2</code></p>

          '
        raw: You should be able to then run fine-tuning as per this [blog post](https://huggingface.co/blog/fine-tune-whisper)
          with the HF Trainer - simply swap out the model checkpoint `openai/whisper-small`
          for `distil-whisper/distil-large-v2`
        updatedAt: '2023-11-13T14:47:36.019Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kadriu
    id: 6552370813e78467edd4cc5e
    type: comment
  author: sanchit-gandhi
  content: You should be able to then run fine-tuning as per this [blog post](https://huggingface.co/blog/fine-tune-whisper)
    with the HF Trainer - simply swap out the model checkpoint `openai/whisper-small`
    for `distil-whisper/distil-large-v2`
  created_at: 2023-11-13 14:47:36+00:00
  edited: false
  hidden: false
  id: 6552370813e78467edd4cc5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3cc94b6475f4551cfa431409e3baf249.svg
      fullname: kadriu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kadriu
      type: user
    createdAt: '2023-12-11T16:57:52.000Z'
    data:
      edited: false
      editors:
      - kadriu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8680569529533386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3cc94b6475f4551cfa431409e3baf249.svg
          fullname: kadriu
          isHf: false
          isPro: false
          name: kadriu
          type: user
        html: '<p>When swapping out the model checkpoint openai/whisper-small for
          distil-whisper/distil-large-v2, the fine-tune process is much slower than
          when using  openai/whisper-small. Does this have to do with pytorch with
          MPS backend, some operators (used with distill-whisper) not implemented?</p>

          '
        raw: When swapping out the model checkpoint openai/whisper-small for distil-whisper/distil-large-v2,
          the fine-tune process is much slower than when using  openai/whisper-small.
          Does this have to do with pytorch with MPS backend, some operators (used
          with distill-whisper) not implemented?
        updatedAt: '2023-12-11T16:57:52.382Z'
      numEdits: 0
      reactions: []
    id: 65773f90ebcaf87a0bf8511b
    type: comment
  author: kadriu
  content: When swapping out the model checkpoint openai/whisper-small for distil-whisper/distil-large-v2,
    the fine-tune process is much slower than when using  openai/whisper-small. Does
    this have to do with pytorch with MPS backend, some operators (used with distill-whisper)
    not implemented?
  created_at: 2023-12-11 16:57:52+00:00
  edited: false
  hidden: false
  id: 65773f90ebcaf87a0bf8511b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-12-12T18:10:05.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8724716901779175
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>All the operators are the same, since there is no code change going
          from <code>openai/whisper-small</code> -&gt; <code>distil-whisper/distil-large-v2</code>.
          Both use this modelling file: <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py</a></p>

          <p>Whisper small is 242M parameters, whereas Distil-Whisper large-v2 is
          756M. This is because the encoder for Distil-Whisper large-v2 is much wider
          and deeper (32 layers) than Whisper small (12 layers). That means the forward
          and backward propagation through the encoder takes much longer, which is
          likely the reason for slower training.</p>

          <p>In short: Distil-Whisper large-v2 is a larger model than Whisper small
          (even though it''s faster), so training takes longer.</p>

          '
        raw: 'All the operators are the same, since there is no code change going
          from `openai/whisper-small` -> `distil-whisper/distil-large-v2`. Both use
          this modelling file: https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py


          Whisper small is 242M parameters, whereas Distil-Whisper large-v2 is 756M.
          This is because the encoder for Distil-Whisper large-v2 is much wider and
          deeper (32 layers) than Whisper small (12 layers). That means the forward
          and backward propagation through the encoder takes much longer, which is
          likely the reason for slower training.


          In short: Distil-Whisper large-v2 is a larger model than Whisper small (even
          though it''s faster), so training takes longer.'
        updatedAt: '2023-12-12T18:10:05.759Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kadriu
    id: 6578a1fdcfb2207f1107c807
    type: comment
  author: sanchit-gandhi
  content: 'All the operators are the same, since there is no code change going from
    `openai/whisper-small` -> `distil-whisper/distil-large-v2`. Both use this modelling
    file: https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/modeling_whisper.py


    Whisper small is 242M parameters, whereas Distil-Whisper large-v2 is 756M. This
    is because the encoder for Distil-Whisper large-v2 is much wider and deeper (32
    layers) than Whisper small (12 layers). That means the forward and backward propagation
    through the encoder takes much longer, which is likely the reason for slower training.


    In short: Distil-Whisper large-v2 is a larger model than Whisper small (even though
    it''s faster), so training takes longer.'
  created_at: 2023-12-12 18:10:05+00:00
  edited: false
  hidden: false
  id: 6578a1fdcfb2207f1107c807
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 13
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: open
target_branch: null
title: MPS backend
