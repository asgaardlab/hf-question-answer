!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Venkatesh4342
conflicting_files: null
created_at: 2023-12-14 08:29:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c8b92a3adea3e2cc47d1b955fef8708.svg
      fullname: venkatesh R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Venkatesh4342
      type: user
    createdAt: '2023-12-14T08:29:26.000Z'
    data:
      edited: false
      editors:
      - Venkatesh4342
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9724419116973877
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c8b92a3adea3e2cc47d1b955fef8708.svg
          fullname: venkatesh R
          isHf: false
          isPro: false
          name: Venkatesh4342
          type: user
        html: '<p>till yesterday it was supporting now above error is popping up.</p>

          '
        raw: till yesterday it was supporting now above error is popping up.
        updatedAt: '2023-12-14T08:29:26.685Z'
      numEdits: 0
      reactions: []
    id: 657abce6e1113b5af517c5b1
    type: comment
  author: Venkatesh4342
  content: till yesterday it was supporting now above error is popping up.
  created_at: 2023-12-14 08:29:26+00:00
  edited: false
  hidden: false
  id: 657abce6e1113b5af517c5b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-12-29T10:22:16.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7157551050186157
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Venkatesh4342&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Venkatesh4342\"\
          >@<span class=\"underline\">Venkatesh4342</span></a></span>\n\n\t</span></span>!\
          \ Whisper now has native support for PyTorch SDPA flash attention. To use\
          \ it, first upgrade your version of PyTorch to 2.1.2: <a rel=\"nofollow\"\
          \ href=\"https://pytorch.org/get-started/locally/\">https://pytorch.org/get-started/locally/</a></p>\n\
          <p>Then update Transformers to use main: <a href=\"https://huggingface.co/docs/transformers/installation#install-from-source\"\
          >https://huggingface.co/docs/transformers/installation#install-from-source</a></p>\n\
          <p>Transformers will then use PyTorch SDPA by default, alongside faster\
          \ Torch STFT pre-processing, which should give you a nice speed-up overall:\
          \ <a href=\"https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention\"\
          >https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention</a></p>\n\
          <p>Otherwise, using the latest version of Optimum should resolve the issue\
          \ with BetterTransformer.</p>\n"
        raw: 'Hey @Venkatesh4342! Whisper now has native support for PyTorch SDPA
          flash attention. To use it, first upgrade your version of PyTorch to 2.1.2:
          https://pytorch.org/get-started/locally/


          Then update Transformers to use main: https://huggingface.co/docs/transformers/installation#install-from-source


          Transformers will then use PyTorch SDPA by default, alongside faster Torch
          STFT pre-processing, which should give you a nice speed-up overall: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention


          Otherwise, using the latest version of Optimum should resolve the issue
          with BetterTransformer.'
        updatedAt: '2023-12-29T10:22:42.245Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Venkatesh4342
      relatedEventId: 658e9dd9f5a209eeacc538da
    id: 658e9dd8f5a209eeacc538d3
    type: comment
  author: sanchit-gandhi
  content: 'Hey @Venkatesh4342! Whisper now has native support for PyTorch SDPA flash
    attention. To use it, first upgrade your version of PyTorch to 2.1.2: https://pytorch.org/get-started/locally/


    Then update Transformers to use main: https://huggingface.co/docs/transformers/installation#install-from-source


    Transformers will then use PyTorch SDPA by default, alongside faster Torch STFT
    pre-processing, which should give you a nice speed-up overall: https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-and-memory-efficient-attention-through-pytorchs-scaleddotproductattention


    Otherwise, using the latest version of Optimum should resolve the issue with BetterTransformer.'
  created_at: 2023-12-29 10:22:16+00:00
  edited: true
  hidden: false
  id: 658e9dd8f5a209eeacc538d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-12-29T10:22:17.000Z'
    data:
      status: closed
    id: 658e9dd9f5a209eeacc538da
    type: status-change
  author: sanchit-gandhi
  created_at: 2023-12-29 10:22:17+00:00
  id: 658e9dd9f5a209eeacc538da
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c8b92a3adea3e2cc47d1b955fef8708.svg
      fullname: venkatesh R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Venkatesh4342
      type: user
    createdAt: '2024-01-14T15:16:43.000Z'
    data:
      edited: false
      editors:
      - Venkatesh4342
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9402412176132202
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c8b92a3adea3e2cc47d1b955fef8708.svg
          fullname: venkatesh R
          isHf: false
          isPro: false
          name: Venkatesh4342
          type: user
        html: "<p>Thanks for the quick respone <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \  it worked.</p>\n"
        raw: 'Thanks for the quick respone @sanchit-gandhi  it worked.

          '
        updatedAt: '2024-01-14T15:16:43.909Z'
      numEdits: 0
      reactions: []
    id: 65a3fadb41b6ef119c03cb9a
    type: comment
  author: Venkatesh4342
  content: 'Thanks for the quick respone @sanchit-gandhi  it worked.

    '
  created_at: 2024-01-14 15:16:43+00:00
  edited: false
  hidden: false
  id: 65a3fadb41b6ef119c03cb9a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: closed
target_branch: null
title: The model type whisper is not supported to be used with BetterTransformer
