!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MelanieKoe
conflicting_files: null
created_at: 2023-11-08 06:05:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa47218dc81240a1f52d452c5e65babb.svg
      fullname: "Melanie K\xF6ppel"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MelanieKoe
      type: user
    createdAt: '2023-11-08T06:05:07.000Z'
    data:
      edited: false
      editors:
      - MelanieKoe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8837680816650391
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa47218dc81240a1f52d452c5e65babb.svg
          fullname: "Melanie K\xF6ppel"
          isHf: false
          isPro: false
          name: MelanieKoe
          type: user
        html: '<p>Whisper models of openai offer the possibility of promping (via
          prompt_ids that are given to the model or processor) - is something similar
          possible for this model?</p>

          '
        raw: Whisper models of openai offer the possibility of promping (via prompt_ids
          that are given to the model or processor) - is something similar possible
          for this model?
        updatedAt: '2023-11-08T06:05:07.217Z'
      numEdits: 0
      reactions: []
    id: 654b2513407c209b61562c0d
    type: comment
  author: MelanieKoe
  content: Whisper models of openai offer the possibility of promping (via prompt_ids
    that are given to the model or processor) - is something similar possible for
    this model?
  created_at: 2023-11-08 06:05:07+00:00
  edited: false
  hidden: false
  id: 654b2513407c209b61562c0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-22T17:43:16.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.642641544342041
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: '<p>Yes, currently for batch size 1: </p>

          <pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> WhisperProcessor,
          WhisperForConditionalGeneration

          <span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span>
          load_dataset


          dataset = load_dataset(<span class="hljs-string">"hf-internal-testing/librispeech_asr_dummy"</span>,
          <span class="hljs-string">"clean"</span>, split=<span class="hljs-string">"validation"</span>)

          input_speech = dataset[<span class="hljs-number">3</span>][<span class="hljs-string">"audio"</span>][<span
          class="hljs-string">"array"</span>]


          processor = WhisperProcessor.from_pretrained(<span class="hljs-string">"distil-whisper/distil-large-v2"</span>)

          model = WhisperForConditionalGeneration.from_pretrained(<span class="hljs-string">"distil-whisper/distil-large-v2"</span>)

          input_features = processor(input_speech, return_tensors=<span class="hljs-string">"pt"</span>).input_features


          <span class="hljs-comment"># --- Without prompt ---</span>

          output_without_prompt = model.generate(input_features)

          <span class="hljs-built_in">print</span>(processor.decode(output_without_prompt[<span
          class="hljs-number">0</span>]))

          <span class="hljs-comment"># &lt;|startoftranscript|&gt;&lt;|en|&gt;&lt;|transcribe|&gt;&lt;|notimestamps|&gt;
          He has grave doubts whether Sir Frederick Leighton''s work is really Greek
          after all, and can discover in it but little of Rocky Ithaca.&lt;|endoftext|&gt;</span>


          <span class="hljs-comment"># --- With prompt ---</span>

          <span class="hljs-comment">#&nbsp;Let''s change the spelling of "Leighton"
          -&gt; "Layton" by passing it as a prompt</span>

          prompt_ids = processor.get_prompt_ids(<span class="hljs-string">"Layton"</span>)

          output_with_prompt = model.generate(input_features, prompt_ids=prompt_ids)

          <span class="hljs-built_in">print</span>(processor.decode(output_with_prompt[<span
          class="hljs-number">0</span>]))

          <span class="hljs-comment"># &lt;|startofprev|&gt; Layton&lt;|startoftranscript|&gt;&lt;|en|&gt;&lt;|transcribe|&gt;&lt;|notimestamps|&gt;
          He has grave doubts whether Sir Frederick Layton''s work is really Greek
          after all, and can discover in it but little of Rocky Ithaca.&lt;|endoftext|&gt;</span>

          </code></pre>

          '
        raw: "Yes, currently for batch size 1: \n```python\nfrom transformers import\
          \ WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import\
          \ load_dataset\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
          , \"clean\", split=\"validation\")\ninput_speech = dataset[3][\"audio\"\
          ][\"array\"]\n\nprocessor = WhisperProcessor.from_pretrained(\"distil-whisper/distil-large-v2\"\
          )\nmodel = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-large-v2\"\
          )\ninput_features = processor(input_speech, return_tensors=\"pt\").input_features\n\
          \n# --- Without prompt ---\noutput_without_prompt = model.generate(input_features)\n\
          print(processor.decode(output_without_prompt[0]))\n# <|startoftranscript|><|en|><|transcribe|><|notimestamps|>\
          \ He has grave doubts whether Sir Frederick Leighton's work is really Greek\
          \ after all, and can discover in it but little of Rocky Ithaca.<|endoftext|>\n\
          \n# --- With prompt ---\n#\_Let's change the spelling of \"Leighton\" ->\
          \ \"Layton\" by passing it as a prompt\nprompt_ids = processor.get_prompt_ids(\"\
          Layton\")\noutput_with_prompt = model.generate(input_features, prompt_ids=prompt_ids)\n\
          print(processor.decode(output_with_prompt[0]))\n# <|startofprev|> Layton<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\
          \ He has grave doubts whether Sir Frederick Layton's work is really Greek\
          \ after all, and can discover in it but little of Rocky Ithaca.<|endoftext|>\n\
          ```"
        updatedAt: '2023-11-22T17:43:16.280Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - MelanieKoe
        - abdullah
    id: 655e3db4afee0e00786af3cd
    type: comment
  author: sanchit-gandhi
  content: "Yes, currently for batch size 1: \n```python\nfrom transformers import\
    \ WhisperProcessor, WhisperForConditionalGeneration\nfrom datasets import load_dataset\n\
    \ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\"\
    , split=\"validation\")\ninput_speech = dataset[3][\"audio\"][\"array\"]\n\nprocessor\
    \ = WhisperProcessor.from_pretrained(\"distil-whisper/distil-large-v2\")\nmodel\
    \ = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-large-v2\"\
    )\ninput_features = processor(input_speech, return_tensors=\"pt\").input_features\n\
    \n# --- Without prompt ---\noutput_without_prompt = model.generate(input_features)\n\
    print(processor.decode(output_without_prompt[0]))\n# <|startoftranscript|><|en|><|transcribe|><|notimestamps|>\
    \ He has grave doubts whether Sir Frederick Leighton's work is really Greek after\
    \ all, and can discover in it but little of Rocky Ithaca.<|endoftext|>\n\n# ---\
    \ With prompt ---\n#\_Let's change the spelling of \"Leighton\" -> \"Layton\"\
    \ by passing it as a prompt\nprompt_ids = processor.get_prompt_ids(\"Layton\"\
    )\noutput_with_prompt = model.generate(input_features, prompt_ids=prompt_ids)\n\
    print(processor.decode(output_with_prompt[0]))\n# <|startofprev|> Layton<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\
    \ He has grave doubts whether Sir Frederick Layton's work is really Greek after\
    \ all, and can discover in it but little of Rocky Ithaca.<|endoftext|>\n```"
  created_at: 2023-11-22 17:43:16+00:00
  edited: false
  hidden: false
  id: 655e3db4afee0e00786af3cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: open
target_branch: null
title: Is Prompting possible?
