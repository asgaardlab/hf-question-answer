!!python/object:huggingface_hub.community.DiscussionWithDetails
author: seeknndestroy
conflicting_files: null
created_at: 2023-12-13 11:38:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ef11837e0723dd5f1765a7985915d40.svg
      fullname: Talha SARI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: seeknndestroy
      type: user
    createdAt: '2023-12-13T11:38:27.000Z'
    data:
      edited: true
      editors:
      - seeknndestroy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8108637928962708
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ef11837e0723dd5f1765a7985915d40.svg
          fullname: Talha SARI
          isHf: false
          isPro: false
          name: seeknndestroy
          type: user
        html: "<p>Hey there!</p>\n<p>I've recently deployed the <code>distil-whisper/distil-large-v2</code>\
          \ model for ASR on Hugging Face's Inference Endpoints, using an AWS instance\
          \ with a GPU Nvidia Tesla T4. However, I'm encountering a <code>UserWarning</code>\
          \ related to <code>max_length</code>, and it's got us a bit puzzled.</p>\n\
          <p>Here's the code snippet I'm using from examples given at call examples:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ requests\n\nAPI_URL = <span class=\"hljs-string\">\"https://ovibb90ga7zdc5qa.us-east-1.aws.endpoints.huggingface.cloud\"\
          </span>\nheaders = {\n    <span class=\"hljs-string\">\"Authorization\"\
          </span>: <span class=\"hljs-string\">\"Bearer XXXXXX\"</span>,\n    <span\
          \ class=\"hljs-string\">\"Content-Type\"</span>: <span class=\"hljs-string\"\
          >\"audio/flac\"</span>\n}\n\n<span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">query</span>(<span class=\"hljs-params\"\
          >filename</span>):\n    <span class=\"hljs-keyword\">with</span> <span class=\"\
          hljs-built_in\">open</span>(filename, <span class=\"hljs-string\">\"rb\"\
          </span>) <span class=\"hljs-keyword\">as</span> f:\n        data = f.read()\n\
          \    response = requests.post(API_URL, headers=headers, data=data)\n   \
          \ <span class=\"hljs-keyword\">return</span> response.json()\n\noutput =\
          \ query(<span class=\"hljs-string\">\"sample1.flac\"</span>)\n</code></pre>\n\
          <p>Here is the full warning message:</p>\n<pre><code>2023/12/13 14:22:36\
          \ ~ /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1369:\
          \ UserWarning: Using `max_length`'s default (448) to control the generation\
          \ length. This behaviour is deprecated and will be removed from the config\
          \ in v5 of Transformers -- we recommend using `max_new_tokens` to control\
          \ the maximum length of the generation.\n</code></pre>\n<p>I need to ensure\
          \ my longer audio files get fully transcribed. Any advice or insights on\
          \ how to best handle this warning and maintain my transcription quality\
          \ would be much appreciated!</p>\n<p>Thanks a bunch!</p>\n"
        raw: "Hey there!\n\nI've recently deployed the `distil-whisper/distil-large-v2`\
          \ model for ASR on Hugging Face's Inference Endpoints, using an AWS instance\
          \ with a GPU Nvidia Tesla T4. However, I'm encountering a `UserWarning`\
          \ related to `max_length`, and it's got us a bit puzzled.\n\nHere's the\
          \ code snippet I'm using from examples given at call examples:\n```python\n\
          import requests\n\nAPI_URL = \"https://ovibb90ga7zdc5qa.us-east-1.aws.endpoints.huggingface.cloud\"\
          \nheaders = {\n\t\"Authorization\": \"Bearer XXXXXX\",\n\t\"Content-Type\"\
          : \"audio/flac\"\n}\n\ndef query(filename):\n    with open(filename, \"\
          rb\") as f:\n        data = f.read()\n    response = requests.post(API_URL,\
          \ headers=headers, data=data)\n    return response.json()\n\noutput = query(\"\
          sample1.flac\")\n```\n\nHere is the full warning message:\n```\n2023/12/13\
          \ 14:22:36 ~ /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1369:\
          \ UserWarning: Using `max_length`'s default (448) to control the generation\
          \ length. This behaviour is deprecated and will be removed from the config\
          \ in v5 of Transformers -- we recommend using `max_new_tokens` to control\
          \ the maximum length of the generation.\n```\n\nI need to ensure my longer\
          \ audio files get fully transcribed. Any advice or insights on how to best\
          \ handle this warning and maintain my transcription quality would be much\
          \ appreciated!\n\nThanks a bunch!"
        updatedAt: '2023-12-13T15:22:40.318Z'
      numEdits: 1
      reactions: []
    id: 657997b3635caf910dace965
    type: comment
  author: seeknndestroy
  content: "Hey there!\n\nI've recently deployed the `distil-whisper/distil-large-v2`\
    \ model for ASR on Hugging Face's Inference Endpoints, using an AWS instance with\
    \ a GPU Nvidia Tesla T4. However, I'm encountering a `UserWarning` related to\
    \ `max_length`, and it's got us a bit puzzled.\n\nHere's the code snippet I'm\
    \ using from examples given at call examples:\n```python\nimport requests\n\n\
    API_URL = \"https://ovibb90ga7zdc5qa.us-east-1.aws.endpoints.huggingface.cloud\"\
    \nheaders = {\n\t\"Authorization\": \"Bearer XXXXXX\",\n\t\"Content-Type\": \"\
    audio/flac\"\n}\n\ndef query(filename):\n    with open(filename, \"rb\") as f:\n\
    \        data = f.read()\n    response = requests.post(API_URL, headers=headers,\
    \ data=data)\n    return response.json()\n\noutput = query(\"sample1.flac\")\n\
    ```\n\nHere is the full warning message:\n```\n2023/12/13 14:22:36 ~ /opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1369:\
    \ UserWarning: Using `max_length`'s default (448) to control the generation length.\
    \ This behaviour is deprecated and will be removed from the config in v5 of Transformers\
    \ -- we recommend using `max_new_tokens` to control the maximum length of the\
    \ generation.\n```\n\nI need to ensure my longer audio files get fully transcribed.\
    \ Any advice or insights on how to best handle this warning and maintain my transcription\
    \ quality would be much appreciated!\n\nThanks a bunch!"
  created_at: 2023-12-13 11:38:27+00:00
  edited: true
  hidden: false
  id: 657997b3635caf910dace965
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: open
target_branch: null
title: UserWarning on`max_length`when deployed at Inference Endpoints
