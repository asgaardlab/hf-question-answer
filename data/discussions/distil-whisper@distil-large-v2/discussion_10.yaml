!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Hank012
conflicting_files: null
created_at: 2023-11-04 21:55:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/061f74fd7232c6257e27a22c620c0c9d.svg
      fullname: Hank
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hank012
      type: user
    createdAt: '2023-11-04T22:55:16.000Z'
    data:
      edited: false
      editors:
      - Hank012
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3186545670032501
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/061f74fd7232c6257e27a22c620c0c9d.svg
          fullname: Hank
          isHf: false
          isPro: false
          name: Hank012
          type: user
        html: '<p>Hello,</p>

          <p>Dynamically quantizing and replacing the linear weights only with int8
          significantly impacts the performance of the model. Is there anything that
          should be taken into consideration in this instance?</p>

          <pre><code>+-------------------------------------------------------+------------+

          |                        Modules                        | Parameters |

          +-------------------------------------------------------+------------+

          |               model.encoder.conv1.weight              |   245760   |

          |                model.encoder.conv1.bias               |    1024    |

          |               model.encoder.conv2.weight              |  3145728   |

          |                model.encoder.conv2.bias               |    1024    |

          |   model.encoder.layers.0.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.0.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.0.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.0.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.1.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.1.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.1.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.1.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.2.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.2.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.2.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.2.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.3.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.3.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.3.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.3.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.4.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.4.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.4.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.4.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.5.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.5.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.5.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.5.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.6.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.6.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.6.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.6.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.7.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.7.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.7.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.7.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.8.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.8.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.8.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.8.final_layer_norm.bias     |    1024    |

          |   model.encoder.layers.9.self_attn_layer_norm.weight  |    1024    |

          |    model.encoder.layers.9.self_attn_layer_norm.bias   |    1024    |

          |     model.encoder.layers.9.final_layer_norm.weight    |    1024    |

          |      model.encoder.layers.9.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.10.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.10.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.10.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.10.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.11.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.11.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.11.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.11.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.12.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.12.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.12.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.12.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.13.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.13.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.13.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.13.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.14.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.14.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.14.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.14.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.15.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.15.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.15.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.15.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.16.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.16.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.16.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.16.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.17.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.17.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.17.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.17.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.18.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.18.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.18.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.18.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.19.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.19.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.19.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.19.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.20.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.20.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.20.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.20.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.21.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.21.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.21.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.21.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.22.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.22.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.22.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.22.final_layer_norm.bias     |    1024    |

          |  model.encoder.layers.23.self_attn_layer_norm.weight  |    1024    |

          |   model.encoder.layers.23.self_attn_layer_norm.bias   |    1024    |

          |    model.encoder.layers.23.final_layer_norm.weight    |    1024    |

          |     model.encoder.layers.23.final_layer_norm.bias     |    1024    |

          |            model.encoder.layer_norm.weight            |    1024    |

          |             model.encoder.layer_norm.bias             |    1024    |

          |           model.decoder.embed_tokens.weight           |  53108736  |

          |          model.decoder.embed_positions.weight         |   458752   |

          |   model.decoder.layers.0.self_attn_layer_norm.weight  |    1024    |

          |    model.decoder.layers.0.self_attn_layer_norm.bias   |    1024    |

          | model.decoder.layers.0.encoder_attn_layer_norm.weight |    1024    |

          |  model.decoder.layers.0.encoder_attn_layer_norm.bias  |    1024    |

          |     model.decoder.layers.0.final_layer_norm.weight    |    1024    |

          |      model.decoder.layers.0.final_layer_norm.bias     |    1024    |

          |   model.decoder.layers.1.self_attn_layer_norm.weight  |    1024    |

          |    model.decoder.layers.1.self_attn_layer_norm.bias   |    1024    |

          | model.decoder.layers.1.encoder_attn_layer_norm.weight |    1024    |

          |  model.decoder.layers.1.encoder_attn_layer_norm.bias  |    1024    |

          |     model.decoder.layers.1.final_layer_norm.weight    |    1024    |

          |      model.decoder.layers.1.final_layer_norm.bias     |    1024    |

          |            model.decoder.layer_norm.weight            |    1024    |

          |             model.decoder.layer_norm.bias             |    1024    |

          +-------------------------------------------------------+------------+

          Total Trainable Params: 57075712

          57075712

          </code></pre>

          <p>Output:</p>

          <pre><code>Special tokens have been added in the vocabulary, make sure the
          associated word embeddings are fine-tuned or trained.

          !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

          Transcription time: 0.612983226776123 seconds

          </code></pre>

          '
        raw: "Hello,\r\n\r\nDynamically quantizing and replacing the linear weights\
          \ only with int8 significantly impacts the performance of the model. Is\
          \ there anything that should be taken into consideration in this instance?\r\
          \n```\r\n+-------------------------------------------------------+------------+\r\
          \n|                        Modules                        | Parameters |\r\
          \n+-------------------------------------------------------+------------+\r\
          \n|               model.encoder.conv1.weight              |   245760   |\r\
          \n|                model.encoder.conv1.bias               |    1024    |\r\
          \n|               model.encoder.conv2.weight              |  3145728   |\r\
          \n|                model.encoder.conv2.bias               |    1024    |\r\
          \n|   model.encoder.layers.0.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.0.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.0.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.0.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.1.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.1.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.1.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.1.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.2.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.2.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.2.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.2.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.3.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.3.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.3.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.3.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.4.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.4.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.4.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.4.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.5.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.5.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.5.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.5.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.6.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.6.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.6.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.6.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.7.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.7.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.7.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.7.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.8.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.8.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.8.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.8.final_layer_norm.bias     |    1024    |\r\
          \n|   model.encoder.layers.9.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.encoder.layers.9.self_attn_layer_norm.bias   |    1024    |\r\
          \n|     model.encoder.layers.9.final_layer_norm.weight    |    1024    |\r\
          \n|      model.encoder.layers.9.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.10.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.10.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.10.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.10.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.11.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.11.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.11.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.11.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.12.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.12.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.12.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.12.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.13.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.13.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.13.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.13.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.14.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.14.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.14.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.14.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.15.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.15.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.15.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.15.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.16.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.16.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.16.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.16.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.17.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.17.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.17.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.17.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.18.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.18.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.18.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.18.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.19.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.19.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.19.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.19.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.20.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.20.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.20.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.20.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.21.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.21.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.21.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.21.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.22.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.22.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.22.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.22.final_layer_norm.bias     |    1024    |\r\
          \n|  model.encoder.layers.23.self_attn_layer_norm.weight  |    1024    |\r\
          \n|   model.encoder.layers.23.self_attn_layer_norm.bias   |    1024    |\r\
          \n|    model.encoder.layers.23.final_layer_norm.weight    |    1024    |\r\
          \n|     model.encoder.layers.23.final_layer_norm.bias     |    1024    |\r\
          \n|            model.encoder.layer_norm.weight            |    1024    |\r\
          \n|             model.encoder.layer_norm.bias             |    1024    |\r\
          \n|           model.decoder.embed_tokens.weight           |  53108736  |\r\
          \n|          model.decoder.embed_positions.weight         |   458752   |\r\
          \n|   model.decoder.layers.0.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.decoder.layers.0.self_attn_layer_norm.bias   |    1024    |\r\
          \n| model.decoder.layers.0.encoder_attn_layer_norm.weight |    1024    |\r\
          \n|  model.decoder.layers.0.encoder_attn_layer_norm.bias  |    1024    |\r\
          \n|     model.decoder.layers.0.final_layer_norm.weight    |    1024    |\r\
          \n|      model.decoder.layers.0.final_layer_norm.bias     |    1024    |\r\
          \n|   model.decoder.layers.1.self_attn_layer_norm.weight  |    1024    |\r\
          \n|    model.decoder.layers.1.self_attn_layer_norm.bias   |    1024    |\r\
          \n| model.decoder.layers.1.encoder_attn_layer_norm.weight |    1024    |\r\
          \n|  model.decoder.layers.1.encoder_attn_layer_norm.bias  |    1024    |\r\
          \n|     model.decoder.layers.1.final_layer_norm.weight    |    1024    |\r\
          \n|      model.decoder.layers.1.final_layer_norm.bias     |    1024    |\r\
          \n|            model.decoder.layer_norm.weight            |    1024    |\r\
          \n|             model.decoder.layer_norm.bias             |    1024    |\r\
          \n+-------------------------------------------------------+------------+\r\
          \nTotal Trainable Params: 57075712\r\n57075712\r\n```\r\nOutput:\r\n```\r\
          \nSpecial tokens have been added in the vocabulary, make sure the associated\
          \ word embeddings are fine-tuned or trained.\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\
          \nTranscription time: 0.612983226776123 seconds\r\n```"
        updatedAt: '2023-11-04T22:55:16.967Z'
      numEdits: 0
      reactions: []
    id: 6546cbd449feb5082b4bd27c
    type: comment
  author: Hank012
  content: "Hello,\r\n\r\nDynamically quantizing and replacing the linear weights\
    \ only with int8 significantly impacts the performance of the model. Is there\
    \ anything that should be taken into consideration in this instance?\r\n```\r\n\
    +-------------------------------------------------------+------------+\r\n|  \
    \                      Modules                        | Parameters |\r\n+-------------------------------------------------------+------------+\r\
    \n|               model.encoder.conv1.weight              |   245760   |\r\n|\
    \                model.encoder.conv1.bias               |    1024    |\r\n|  \
    \             model.encoder.conv2.weight              |  3145728   |\r\n|    \
    \            model.encoder.conv2.bias               |    1024    |\r\n|   model.encoder.layers.0.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|    model.encoder.layers.0.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|     model.encoder.layers.0.final_layer_norm.weight    |  \
    \  1024    |\r\n|      model.encoder.layers.0.final_layer_norm.bias     |    1024\
    \    |\r\n|   model.encoder.layers.1.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|    model.encoder.layers.1.self_attn_layer_norm.bias   |    1024    |\r\
    \n|     model.encoder.layers.1.final_layer_norm.weight    |    1024    |\r\n|\
    \      model.encoder.layers.1.final_layer_norm.bias     |    1024    |\r\n|  \
    \ model.encoder.layers.2.self_attn_layer_norm.weight  |    1024    |\r\n|    model.encoder.layers.2.self_attn_layer_norm.bias\
    \   |    1024    |\r\n|     model.encoder.layers.2.final_layer_norm.weight   \
    \ |    1024    |\r\n|      model.encoder.layers.2.final_layer_norm.bias     |\
    \    1024    |\r\n|   model.encoder.layers.3.self_attn_layer_norm.weight  |  \
    \  1024    |\r\n|    model.encoder.layers.3.self_attn_layer_norm.bias   |    1024\
    \    |\r\n|     model.encoder.layers.3.final_layer_norm.weight    |    1024  \
    \  |\r\n|      model.encoder.layers.3.final_layer_norm.bias     |    1024    |\r\
    \n|   model.encoder.layers.4.self_attn_layer_norm.weight  |    1024    |\r\n|\
    \    model.encoder.layers.4.self_attn_layer_norm.bias   |    1024    |\r\n|  \
    \   model.encoder.layers.4.final_layer_norm.weight    |    1024    |\r\n|    \
    \  model.encoder.layers.4.final_layer_norm.bias     |    1024    |\r\n|   model.encoder.layers.5.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|    model.encoder.layers.5.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|     model.encoder.layers.5.final_layer_norm.weight    |  \
    \  1024    |\r\n|      model.encoder.layers.5.final_layer_norm.bias     |    1024\
    \    |\r\n|   model.encoder.layers.6.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|    model.encoder.layers.6.self_attn_layer_norm.bias   |    1024    |\r\
    \n|     model.encoder.layers.6.final_layer_norm.weight    |    1024    |\r\n|\
    \      model.encoder.layers.6.final_layer_norm.bias     |    1024    |\r\n|  \
    \ model.encoder.layers.7.self_attn_layer_norm.weight  |    1024    |\r\n|    model.encoder.layers.7.self_attn_layer_norm.bias\
    \   |    1024    |\r\n|     model.encoder.layers.7.final_layer_norm.weight   \
    \ |    1024    |\r\n|      model.encoder.layers.7.final_layer_norm.bias     |\
    \    1024    |\r\n|   model.encoder.layers.8.self_attn_layer_norm.weight  |  \
    \  1024    |\r\n|    model.encoder.layers.8.self_attn_layer_norm.bias   |    1024\
    \    |\r\n|     model.encoder.layers.8.final_layer_norm.weight    |    1024  \
    \  |\r\n|      model.encoder.layers.8.final_layer_norm.bias     |    1024    |\r\
    \n|   model.encoder.layers.9.self_attn_layer_norm.weight  |    1024    |\r\n|\
    \    model.encoder.layers.9.self_attn_layer_norm.bias   |    1024    |\r\n|  \
    \   model.encoder.layers.9.final_layer_norm.weight    |    1024    |\r\n|    \
    \  model.encoder.layers.9.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.10.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.10.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.10.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.10.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.11.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.11.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.11.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.11.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.12.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.12.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.12.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.12.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.13.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.13.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.13.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.13.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.14.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.14.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.14.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.14.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.15.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.15.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.15.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.15.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.16.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.16.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.16.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.16.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.17.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.17.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.17.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.17.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.18.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.18.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.18.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.18.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.19.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.19.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.19.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.19.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.20.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.20.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.20.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.20.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.21.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.21.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.21.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.21.final_layer_norm.bias     |    1024    |\r\n|  model.encoder.layers.22.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|   model.encoder.layers.22.self_attn_layer_norm.bias   |\
    \    1024    |\r\n|    model.encoder.layers.22.final_layer_norm.weight    |  \
    \  1024    |\r\n|     model.encoder.layers.22.final_layer_norm.bias     |    1024\
    \    |\r\n|  model.encoder.layers.23.self_attn_layer_norm.weight  |    1024  \
    \  |\r\n|   model.encoder.layers.23.self_attn_layer_norm.bias   |    1024    |\r\
    \n|    model.encoder.layers.23.final_layer_norm.weight    |    1024    |\r\n|\
    \     model.encoder.layers.23.final_layer_norm.bias     |    1024    |\r\n|  \
    \          model.encoder.layer_norm.weight            |    1024    |\r\n|    \
    \         model.encoder.layer_norm.bias             |    1024    |\r\n|      \
    \     model.decoder.embed_tokens.weight           |  53108736  |\r\n|        \
    \  model.decoder.embed_positions.weight         |   458752   |\r\n|   model.decoder.layers.0.self_attn_layer_norm.weight\
    \  |    1024    |\r\n|    model.decoder.layers.0.self_attn_layer_norm.bias   |\
    \    1024    |\r\n| model.decoder.layers.0.encoder_attn_layer_norm.weight |  \
    \  1024    |\r\n|  model.decoder.layers.0.encoder_attn_layer_norm.bias  |    1024\
    \    |\r\n|     model.decoder.layers.0.final_layer_norm.weight    |    1024  \
    \  |\r\n|      model.decoder.layers.0.final_layer_norm.bias     |    1024    |\r\
    \n|   model.decoder.layers.1.self_attn_layer_norm.weight  |    1024    |\r\n|\
    \    model.decoder.layers.1.self_attn_layer_norm.bias   |    1024    |\r\n| model.decoder.layers.1.encoder_attn_layer_norm.weight\
    \ |    1024    |\r\n|  model.decoder.layers.1.encoder_attn_layer_norm.bias  |\
    \    1024    |\r\n|     model.decoder.layers.1.final_layer_norm.weight    |  \
    \  1024    |\r\n|      model.decoder.layers.1.final_layer_norm.bias     |    1024\
    \    |\r\n|            model.decoder.layer_norm.weight            |    1024  \
    \  |\r\n|             model.decoder.layer_norm.bias             |    1024    |\r\
    \n+-------------------------------------------------------+------------+\r\nTotal\
    \ Trainable Params: 57075712\r\n57075712\r\n```\r\nOutput:\r\n```\r\nSpecial tokens\
    \ have been added in the vocabulary, make sure the associated word embeddings\
    \ are fine-tuned or trained.\r\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\
    \nTranscription time: 0.612983226776123 seconds\r\n```"
  created_at: 2023-11-04 21:55:16+00:00
  edited: false
  hidden: false
  id: 6546cbd449feb5082b4bd27c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-13T14:44:41.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8696447014808655
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;Hank012&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Hank012\"\
          >@<span class=\"underline\">Hank012</span></a></span>\n\n\t</span></span>\
          \ - do you have any further details on how you've quantised the model? E.g.\
          \ library, hyper parameters, etc? A reproducible code snippet would be awesome\
          \ here!</p>\n"
        raw: Hey @Hank012 - do you have any further details on how you've quantised
          the model? E.g. library, hyper parameters, etc? A reproducible code snippet
          would be awesome here!
        updatedAt: '2023-11-13T14:44:41.655Z'
      numEdits: 0
      reactions: []
    id: 65523659db48a7ea5615a88c
    type: comment
  author: sanchit-gandhi
  content: Hey @Hank012 - do you have any further details on how you've quantised
    the model? E.g. library, hyper parameters, etc? A reproducible code snippet would
    be awesome here!
  created_at: 2023-11-13 14:44:41+00:00
  edited: false
  hidden: false
  id: 65523659db48a7ea5615a88c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: open
target_branch: null
title: INT8 Weight Only Quantization
