!!python/object:huggingface_hub.community.DiscussionWithDetails
author: hxypqr
conflicting_files: null
created_at: 2024-01-12 07:43:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/df68ed85cdb2e29d5c45c5eb8dc429e2.svg
      fullname: xy h
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hxypqr
      type: user
    createdAt: '2024-01-12T07:43:54.000Z'
    data:
      edited: false
      editors:
      - hxypqr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.923916757106781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/df68ed85cdb2e29d5c45c5eb8dc429e2.svg
          fullname: xy h
          isHf: false
          isPro: false
          name: hxypqr
          type: user
        html: "<p>I don't quite understand how knowledge distillation is implemented\
          \ here.</p>\n<p>Whisper is trained on 680,000 hours of untagged data for\
          \ autoregression. According to the content of the fourth section of the\
          \ paper, our model is trained on 21,170 hours of data with pseudo-labels\
          \ generated by Whisper, with the first and 32nd layer parameters frozen\
          \ based on Whisper. This means that our model only needs to go through 21,170\
          \ hours of data with pseudo-labels and a model structure similar to Whisper,\
          \ freezing the first and 32nd layers, using weighted KL divergence and label\
          \ cross-entropy to achieve good results\uFF1F</p>\n<p>If this is the case,\
          \ it is indeed a significant discovery, indicating that we can always reduce\
          \ the model's parameters and inference time after pre-training the model\
          \ using similar methods, without significant loss of accuracy.</p>\n<p>Thank\
          \ you in advance</p>\n"
        raw: "I don't quite understand how knowledge distillation is implemented here.\r\
          \n\r\nWhisper is trained on 680,000 hours of untagged data for autoregression.\
          \ According to the content of the fourth section of the paper, our model\
          \ is trained on 21,170 hours of data with pseudo-labels generated by Whisper,\
          \ with the first and 32nd layer parameters frozen based on Whisper. This\
          \ means that our model only needs to go through 21,170 hours of data with\
          \ pseudo-labels and a model structure similar to Whisper, freezing the first\
          \ and 32nd layers, using weighted KL divergence and label cross-entropy\
          \ to achieve good results\uFF1F\r\n\r\nIf this is the case, it is indeed\
          \ a significant discovery, indicating that we can always reduce the model's\
          \ parameters and inference time after pre-training the model using similar\
          \ methods, without significant loss of accuracy.\r\n\r\nThank you in advance"
        updatedAt: '2024-01-12T07:43:54.410Z'
      numEdits: 0
      reactions: []
    id: 65a0edbafbad78ab681aaf4c
    type: comment
  author: hxypqr
  content: "I don't quite understand how knowledge distillation is implemented here.\r\
    \n\r\nWhisper is trained on 680,000 hours of untagged data for autoregression.\
    \ According to the content of the fourth section of the paper, our model is trained\
    \ on 21,170 hours of data with pseudo-labels generated by Whisper, with the first\
    \ and 32nd layer parameters frozen based on Whisper. This means that our model\
    \ only needs to go through 21,170 hours of data with pseudo-labels and a model\
    \ structure similar to Whisper, freezing the first and 32nd layers, using weighted\
    \ KL divergence and label cross-entropy to achieve good results\uFF1F\r\n\r\n\
    If this is the case, it is indeed a significant discovery, indicating that we\
    \ can always reduce the model's parameters and inference time after pre-training\
    \ the model using similar methods, without significant loss of accuracy.\r\n\r\
    \nThank you in advance"
  created_at: 2024-01-12 07:43:54+00:00
  edited: false
  hidden: false
  id: 65a0edbafbad78ab681aaf4c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: open
target_branch: null
title: "How the knowledge organization is implemented\uFF1F"
