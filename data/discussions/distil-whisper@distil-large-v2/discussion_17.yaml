!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jaxmetaverse
conflicting_files: null
created_at: 2023-11-25 09:58:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f959e44fedeb4ff0a6b8c49222107b0e.svg
      fullname: jax
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaxmetaverse
      type: user
    createdAt: '2023-11-25T09:58:32.000Z'
    data:
      edited: true
      editors:
      - jaxmetaverse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.894034206867218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f959e44fedeb4ff0a6b8c49222107b0e.svg
          fullname: jax
          isHf: false
          isPro: false
          name: jaxmetaverse
          type: user
        html: '<p>because sounds are not always has fixed chunk size,  if fix the
          chunk size and  the semantic can not be split two chunk, the information
          will be dropped.</p>

          '
        raw: because sounds are not always has fixed chunk size,  if fix the chunk
          size and  the semantic can not be split two chunk, the information will
          be dropped.
        updatedAt: '2023-11-25T09:59:03.878Z'
      numEdits: 1
      reactions: []
    id: 6561c54884a9fbe322c346a5
    type: comment
  author: jaxmetaverse
  content: because sounds are not always has fixed chunk size,  if fix the chunk size
    and  the semantic can not be split two chunk, the information will be dropped.
  created_at: 2023-11-25 09:58:32+00:00
  edited: true
  hidden: false
  id: 6561c54884a9fbe322c346a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-27T15:47:44.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8874269127845764
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;jaxmetaverse&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jaxmetaverse\"\
          >@<span class=\"underline\">jaxmetaverse</span></a></span>\n\n\t</span></span>\
          \ - there's a stride (overlap) equal to <code>chunk_length / 6</code> that\
          \ we use between chunks. This stride ensures that we get consistent transcriptions\
          \ across chunks. For more details, refer to the blog post: <a href=\"https://huggingface.co/blog/asr-chunking\"\
          >Making automatic speech recognition work on large files with Wav2Vec2 in\
          \ \U0001F917 Transformers</a></p>\n"
        raw: "Hey @jaxmetaverse - there's a stride (overlap) equal to `chunk_length\
          \ / 6` that we use between chunks. This stride ensures that we get consistent\
          \ transcriptions across chunks. For more details, refer to the blog post:\
          \ [Making automatic speech recognition work on large files with Wav2Vec2\
          \ in \U0001F917 Transformers](https://huggingface.co/blog/asr-chunking)"
        updatedAt: '2023-11-27T15:47:44.031Z'
      numEdits: 0
      reactions: []
    id: 6564ba201027d23026e5fb93
    type: comment
  author: sanchit-gandhi
  content: "Hey @jaxmetaverse - there's a stride (overlap) equal to `chunk_length\
    \ / 6` that we use between chunks. This stride ensures that we get consistent\
    \ transcriptions across chunks. For more details, refer to the blog post: [Making\
    \ automatic speech recognition work on large files with Wav2Vec2 in \U0001F917\
    \ Transformers](https://huggingface.co/blog/asr-chunking)"
  created_at: 2023-11-27 15:47:44+00:00
  edited: false
  hidden: false
  id: 6564ba201027d23026e5fb93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f959e44fedeb4ff0a6b8c49222107b0e.svg
      fullname: jax
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jaxmetaverse
      type: user
    createdAt: '2023-11-29T06:29:50.000Z'
    data:
      edited: true
      editors:
      - jaxmetaverse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8439880013465881
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f959e44fedeb4ff0a6b8c49222107b0e.svg
          fullname: jax
          isHf: false
          isPro: false
          name: jaxmetaverse
          type: user
        html: '<p>thanks,   The result I get  is  that  large-v2  is better than distil-large-v2  by
          test  this  video "<a rel="nofollow" href="https://www.youtube.com/watch?v=xguam0TKMw8&quot;">https://www.youtube.com/watch?v=xguam0TKMw8"</a>.<br>some
          env as follows:</p>

          <ol>

          <li>distil-large-v2 chunk_size is 30s .</li>

          <li>use whisper.cpp .</li>

          </ol>

          <p>I try to test more times. </p>

          '
        raw: 'thanks,   The result I get  is  that  large-v2  is better than distil-large-v2  by
          test  this  video "https://www.youtube.com/watch?v=xguam0TKMw8".

          some env as follows:

          1. distil-large-v2 chunk_size is 30s .

          2. use whisper.cpp .


          I try to test more times. '
        updatedAt: '2023-11-29T06:32:22.812Z'
      numEdits: 1
      reactions: []
    id: 6566da5e751591e4fae5b72b
    type: comment
  author: jaxmetaverse
  content: 'thanks,   The result I get  is  that  large-v2  is better than distil-large-v2  by
    test  this  video "https://www.youtube.com/watch?v=xguam0TKMw8".

    some env as follows:

    1. distil-large-v2 chunk_size is 30s .

    2. use whisper.cpp .


    I try to test more times. '
  created_at: 2023-11-29 06:29:50+00:00
  edited: true
  hidden: false
  id: 6566da5e751591e4fae5b72b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2023-11-29T16:18:01.000Z'
    data:
      edited: true
      editors:
      - sanchit-gandhi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6742273569107056
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>It's best to use <code>chunk_length_s=15</code> for <code>distil-large-v2</code>\
          \ with <code>return_timestamps=False</code>:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\n<span class=\"hljs-keyword\">from</span> datasets\
          \ <span class=\"hljs-keyword\">import</span> load_dataset\n\n\ndevice =\
          \ <span class=\"hljs-string\">\"cuda:0\"</span> <span class=\"hljs-keyword\"\
          >if</span> torch.cuda.is_available() <span class=\"hljs-keyword\">else</span>\
          \ <span class=\"hljs-string\">\"cpu\"</span>\ntorch_dtype = torch.float16\
          \ <span class=\"hljs-keyword\">if</span> torch.cuda.is_available() <span\
          \ class=\"hljs-keyword\">else</span> torch.float32\n\nmodel_id = <span class=\"\
          hljs-string\">\"distil-whisper/distil-large-v2\"</span>\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
          \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=<span class=\"\
          hljs-literal\">True</span>, use_safetensors=<span class=\"hljs-literal\"\
          >True</span>\n)\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\
          \npipe = pipeline(\n    <span class=\"hljs-string\">\"automatic-speech-recognition\"\
          </span>,\n    model=model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=<span class=\"hljs-number\">128</span>,\n    chunk_length_s=<span\
          \ class=\"hljs-number\">15</span>,\n    batch_size=<span class=\"hljs-number\"\
          >16</span>,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset\
          \ = load_dataset(<span class=\"hljs-string\">\"hf-internal-testing/librispeech_asr_dummy\"\
          </span>, <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"\
          hljs-string\">\"validation\"</span>)\nsample = dataset[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-string\">\"audio\"</span>]\n\nresult = pipe(sample)\n\
          <span class=\"hljs-built_in\">print</span>(result[<span class=\"hljs-string\"\
          >\"text\"</span>])\n</code></pre>\n<p>And <code>chunk_length_s=30</code>\
          \ for <code>large-v2</code> with <code>return_timestamps=True</code>:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n\
          <span class=\"hljs-keyword\">from</span> datasets <span class=\"hljs-keyword\"\
          >import</span> load_dataset\n\n\ndevice = <span class=\"hljs-string\">\"\
          cuda:0\"</span> <span class=\"hljs-keyword\">if</span> torch.cuda.is_available()\
          \ <span class=\"hljs-keyword\">else</span> <span class=\"hljs-string\">\"\
          cpu\"</span>\ntorch_dtype = torch.float16 <span class=\"hljs-keyword\">if</span>\
          \ torch.cuda.is_available() <span class=\"hljs-keyword\">else</span> torch.float32\n\
          \nmodel_id = <span class=\"hljs-string\">\"openai/whisper-large-v2\"</span>\n\
          \nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n    model_id, torch_dtype=torch_dtype,\
          \ low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>, use_safetensors=<span\
          \ class=\"hljs-literal\">True</span>\n)\nmodel.to(device)\n\nprocessor =\
          \ AutoProcessor.from_pretrained(model_id)\n\npipe = pipeline(\n    <span\
          \ class=\"hljs-string\">\"automatic-speech-recognition\"</span>,\n    model=model,\n\
          \    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=<span class=\"hljs-number\">128</span>,\n    chunk_length_s=<span\
          \ class=\"hljs-number\">30</span>,\n    batch_size=<span class=\"hljs-number\"\
          >16</span>,\n    torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset\
          \ = load_dataset(<span class=\"hljs-string\">\"hf-internal-testing/librispeech_asr_dummy\"\
          </span>, <span class=\"hljs-string\">\"clean\"</span>, split=<span class=\"\
          hljs-string\">\"validation\"</span>)\nsample = dataset[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-string\">\"audio\"</span>]\n\nresult = pipe(sample,\
          \ return_timestamps=<span class=\"hljs-literal\">True</span>)\n<span class=\"\
          hljs-built_in\">print</span>(result[<span class=\"hljs-string\">\"text\"\
          </span>])\n</code></pre>\n"
        raw: "It's best to use `chunk_length_s=15` for `distil-large-v2` with `return_timestamps=False`:\n\
          ```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice\
          \ = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype =\
          \ torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id\
          \ = \"distil-whisper/distil-large-v2\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
          \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n\
          )\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\
          \npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n\
          \    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=128,\n    chunk_length_s=15,\n    batch_size=16,\n \
          \   torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"\
          hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\
          )\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample)\nprint(result[\"\
          text\"])\n```\n\nAnd `chunk_length_s=30` for `large-v2` with `return_timestamps=True`:\n\
          ```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq,\
          \ AutoProcessor, pipeline\nfrom datasets import load_dataset\n\n\ndevice\
          \ = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype =\
          \ torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id\
          \ = \"openai/whisper-large-v2\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
          \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n\
          )\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\
          \npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n\
          \    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
          \    max_new_tokens=128,\n    chunk_length_s=30,\n    batch_size=16,\n \
          \   torch_dtype=torch_dtype,\n    device=device,\n)\n\ndataset = load_dataset(\"\
          hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\
          )\nsample = dataset[0][\"audio\"]\n\nresult = pipe(sample, return_timestamps=True)\n\
          print(result[\"text\"])\n```"
        updatedAt: '2023-11-29T16:19:33.305Z'
      numEdits: 2
      reactions: []
    id: 65676439461af93fca9d9ddd
    type: comment
  author: sanchit-gandhi
  content: "It's best to use `chunk_length_s=15` for `distil-large-v2` with `return_timestamps=False`:\n\
    ```python\nimport torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor,\
    \ pipeline\nfrom datasets import load_dataset\n\n\ndevice = \"cuda:0\" if torch.cuda.is_available()\
    \ else \"cpu\"\ntorch_dtype = torch.float16 if torch.cuda.is_available() else\
    \ torch.float32\n\nmodel_id = \"distil-whisper/distil-large-v2\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
    \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n\
    )\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n\
    pipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n  \
    \  tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
    \    max_new_tokens=128,\n    chunk_length_s=15,\n    batch_size=16,\n    torch_dtype=torch_dtype,\n\
    \    device=device,\n)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
    , \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult =\
    \ pipe(sample)\nprint(result[\"text\"])\n```\n\nAnd `chunk_length_s=30` for `large-v2`\
    \ with `return_timestamps=True`:\n```python\nimport torch\nfrom transformers import\
    \ AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\nfrom datasets import load_dataset\n\
    \n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch_dtype\
    \ = torch.float16 if torch.cuda.is_available() else torch.float32\n\nmodel_id\
    \ = \"openai/whisper-large-v2\"\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\n\
    \    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n\
    )\nmodel.to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n\n\
    pipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=model,\n  \
    \  tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n\
    \    max_new_tokens=128,\n    chunk_length_s=30,\n    batch_size=16,\n    torch_dtype=torch_dtype,\n\
    \    device=device,\n)\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\"\
    , \"clean\", split=\"validation\")\nsample = dataset[0][\"audio\"]\n\nresult =\
    \ pipe(sample, return_timestamps=True)\nprint(result[\"text\"])\n```"
  created_at: 2023-11-29 16:18:01+00:00
  edited: true
  hidden: false
  id: 65676439461af93fca9d9ddd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: distil-whisper/distil-large-v2
repo_type: model
status: open
target_branch: null
title: chunk size  is fixed could drop some information
