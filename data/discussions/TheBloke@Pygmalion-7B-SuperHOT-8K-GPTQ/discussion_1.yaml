!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Lordwind
conflicting_files: null
created_at: 2023-08-04 10:57:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
      fullname: Holger Brunken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lordwind
      type: user
    createdAt: '2023-08-04T11:57:39.000Z'
    data:
      edited: false
      editors:
      - Lordwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45994746685028076
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
          fullname: Holger Brunken
          isHf: false
          isPro: false
          name: Lordwind
          type: user
        html: '<p>Hello,<br>I downloaded the model which was completet but when I
          want to load it, it gives following error message:</p>

          <p>2023-08-04 13:53:52 INFO:Loading TheBloke_Pygmalion-7B-SuperHOT-8K-GPTQ...<br>2023-08-04
          13:53:53 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>  File "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\text-generation-webui\server.py",
          line 68, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\models.py",
          line 78, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\models.py",
          line 293, in ExLlama_loader<br>    model, tokenizer = ExllamaModel.from_pretrained(model_name)<br>  File
          "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\exllama.py",
          line 68, in from_pretrained<br>    tokenizer = ExLlamaTokenizer(str(tokenizer_model_path))<br>  File
          "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\exllama\tokenizer.py",
          line 10, in <strong>init</strong><br>    self.tokenizer = SentencePieceProcessor(model_file
          = self.path)<br>  File "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\sentencepiece_<em>init</em><em>.py",
          line 447, in Init<br>    self.Load(model_file=model_file, model_proto=model_proto)<br>  File
          "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\sentencepiece_<em>init</em></em>.py",
          line 905, in Load<br>    return self.LoadFromFile(model_file)<br>  File
          "C:\Users\User\Desktop\oobabooga_windows\oobabooga_windows\installer_files\env\lib\site-packages\sentencepiece_<em>init</em>_.py",
          line 310, in LoadFromFile<br>    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,
          arg)<br>RuntimeError: Internal: unk is not defined.</p>

          <p>It can''t be loaded . How can I solve this problem.<br>I run a new machine
          Windows 11 with Nvidia RTX 4080</p>

          <p>Thank you<br>Holger</p>

          '
        raw: "Hello,\r\nI downloaded the model which was completet but when I want\
          \ to load it, it gives following error message:\r\n\r\n2023-08-04 13:53:52\
          \ INFO:Loading TheBloke_Pygmalion-7B-SuperHOT-8K-GPTQ...\r\n2023-08-04 13:53:53\
          \ ERROR:Failed to load the model.\r\nTraceback (most recent call last):\r\
          \n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\server.py\", line 68, in load_model_wrapper\r\n \
          \   shared.model, shared.tokenizer = load_model(shared.model_name, loader)\r\
          \n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 78, in load_model\r\n\
          \    output = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\\
          User\\Desktop\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          modules\\models.py\", line 293, in ExLlama_loader\r\n    model, tokenizer\
          \ = ExllamaModel.from_pretrained(model_name)\r\n  File \"C:\\Users\\User\\\
          Desktop\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\\
          exllama.py\", line 68, in from_pretrained\r\n    tokenizer = ExLlamaTokenizer(str(tokenizer_model_path))\r\
          \n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\exllama\\tokenizer.py\", line\
          \ 10, in __init__\r\n    self.tokenizer = SentencePieceProcessor(model_file\
          \ = self.path)\r\n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\sentencepiece\\\
          __init__.py\", line 447, in Init\r\n    self.Load(model_file=model_file,\
          \ model_proto=model_proto)\r\n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\\
          oobabooga_windows\\installer_files\\env\\lib\\site-packages\\sentencepiece\\\
          __init__.py\", line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\
          \n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\\
          installer_files\\env\\lib\\site-packages\\sentencepiece\\__init__.py\",\
          \ line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\nRuntimeError: Internal: unk is not defined.\r\n\r\nIt can't be\
          \ loaded . How can I solve this problem. \r\nI run a new machine Windows\
          \ 11 with Nvidia RTX 4080\r\n\r\nThank you\r\nHolger\r\n"
        updatedAt: '2023-08-04T11:57:39.040Z'
      numEdits: 0
      reactions: []
    id: 64cce7b37221ef3c7e7a50b5
    type: comment
  author: Lordwind
  content: "Hello,\r\nI downloaded the model which was completet but when I want to\
    \ load it, it gives following error message:\r\n\r\n2023-08-04 13:53:52 INFO:Loading\
    \ TheBloke_Pygmalion-7B-SuperHOT-8K-GPTQ...\r\n2023-08-04 13:53:53 ERROR:Failed\
    \ to load the model.\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\\
    User\\Desktop\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\server.py\"\
    , line 68, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 78, in load_model\r\n    output\
    \ = load_func_map[loader](model_name)\r\n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\\
    oobabooga_windows\\text-generation-webui\\modules\\models.py\", line 293, in ExLlama_loader\r\
    \n    model, tokenizer = ExllamaModel.from_pretrained(model_name)\r\n  File \"\
    C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    modules\\exllama.py\", line 68, in from_pretrained\r\n    tokenizer = ExLlamaTokenizer(str(tokenizer_model_path))\r\
    \n  File \"C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\exllama\\tokenizer.py\", line 10, in __init__\r\n   \
    \ self.tokenizer = SentencePieceProcessor(model_file = self.path)\r\n  File \"\
    C:\\Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\sentencepiece\\__init__.py\", line 447, in Init\r\n \
    \   self.Load(model_file=model_file, model_proto=model_proto)\r\n  File \"C:\\\
    Users\\User\\Desktop\\oobabooga_windows\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\sentencepiece\\__init__.py\", line 905, in Load\r\n \
    \   return self.LoadFromFile(model_file)\r\n  File \"C:\\Users\\User\\Desktop\\\
    oobabooga_windows\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    sentencepiece\\__init__.py\", line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\nRuntimeError: Internal: unk is not defined.\r\n\r\nIt can't be loaded\
    \ . How can I solve this problem. \r\nI run a new machine Windows 11 with Nvidia\
    \ RTX 4080\r\n\r\nThank you\r\nHolger\r\n"
  created_at: 2023-08-04 10:57:39+00:00
  edited: false
  hidden: false
  id: 64cce7b37221ef3c7e7a50b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
      fullname: Holger Brunken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lordwind
      type: user
    createdAt: '2023-08-04T11:58:49.000Z'
    data:
      from: Bug with this model under Oogabooga
      to: 'Bug with this model under Oobabooga "Internal: unk is not defined"'
    id: 64cce7f900377e2848a80b90
    type: title-change
  author: Lordwind
  created_at: 2023-08-04 10:58:49+00:00
  id: 64cce7f900377e2848a80b90
  new_title: 'Bug with this model under Oobabooga "Internal: unk is not defined"'
  old_title: Bug with this model under Oogabooga
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T09:32:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9505876898765564
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>First thing to do is try downloading again, to confirm all model
          files are correctly downloaded.  </p>

          '
        raw: 'First thing to do is try downloading again, to confirm all model files
          are correctly downloaded.  '
        updatedAt: '2023-08-05T09:32:20.818Z'
      numEdits: 0
      reactions: []
    id: 64ce1724995a0b5d592edf35
    type: comment
  author: TheBloke
  content: 'First thing to do is try downloading again, to confirm all model files
    are correctly downloaded.  '
  created_at: 2023-08-05 08:32:20+00:00
  edited: false
  hidden: false
  id: 64ce1724995a0b5d592edf35
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
      fullname: Holger Brunken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lordwind
      type: user
    createdAt: '2023-08-05T11:23:01.000Z'
    data:
      edited: false
      editors:
      - Lordwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8949199318885803
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
          fullname: Holger Brunken
          isHf: false
          isPro: false
          name: Lordwind
          type: user
        html: '<p>I downloaded the 7B version 4 times and the 13B version 2 times
          :-) I also tried other forks. Everythingabover Pygmaliion 6 results in the
          same issue.  All from Huggingface. I even disabled all kind of Firewall
          and Antivirus to secure a uninterrupted download. I think (but that''s rather
          a guess), that the references in the actual Oobabooga refer to divergent
          folders. If you have an Oobabooga version where these references work, it
          would be helpful. Or maybe I have to change the references in thew code.
          But I have no idea what I shall replace there with what :-) . My oobabooga
          is the actual version. </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/643e863eb56b683f744b1232/N-saWaP9vR2Kd6cp_gWln.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/643e863eb56b683f744b1232/N-saWaP9vR2Kd6cp_gWln.png"></a></p>

          '
        raw: "I downloaded the 7B version 4 times and the 13B version 2 times :-)\
          \ I also tried other forks. Everythingabover Pygmaliion 6 results in the\
          \ same issue.  All from Huggingface. I even disabled all kind of Firewall\
          \ and Antivirus to secure a uninterrupted download. I think (but that's\
          \ rather a guess), that the references in the actual Oobabooga refer to\
          \ divergent folders. If you have an Oobabooga version where these references\
          \ work, it would be helpful. Or maybe I have to change the references in\
          \ thew code. But I have no idea what I shall replace there with what :-)\
          \ . My oobabooga is the actual version. \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643e863eb56b683f744b1232/N-saWaP9vR2Kd6cp_gWln.png)\n"
        updatedAt: '2023-08-05T11:23:01.252Z'
      numEdits: 0
      reactions: []
    id: 64ce31157e20ec9ea08b6a66
    type: comment
  author: Lordwind
  content: "I downloaded the 7B version 4 times and the 13B version 2 times :-) I\
    \ also tried other forks. Everythingabover Pygmaliion 6 results in the same issue.\
    \  All from Huggingface. I even disabled all kind of Firewall and Antivirus to\
    \ secure a uninterrupted download. I think (but that's rather a guess), that the\
    \ references in the actual Oobabooga refer to divergent folders. If you have an\
    \ Oobabooga version where these references work, it would be helpful. Or maybe\
    \ I have to change the references in thew code. But I have no idea what I shall\
    \ replace there with what :-) . My oobabooga is the actual version. \n\n\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643e863eb56b683f744b1232/N-saWaP9vR2Kd6cp_gWln.png)\n"
  created_at: 2023-08-05 10:23:01+00:00
  edited: false
  hidden: false
  id: 64ce31157e20ec9ea08b6a66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-05T20:57:04.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9833431839942932
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah OK I just saw someone else reporting this as well in a different
          context. I don''t yet know what the issue is.  I think maybe some code has
          updated in one of the libraries that ooba uses and now it makes certain
          older model configs no longer work.  But I don''t know what exactly, as
          this model does have an <code>&lt;unk&gt;</code> token defined.  If I learn
          what to change  I''ll update the model</p>

          '
        raw: Yeah OK I just saw someone else reporting this as well in a different
          context. I don't yet know what the issue is.  I think maybe some code has
          updated in one of the libraries that ooba uses and now it makes certain
          older model configs no longer work.  But I don't know what exactly, as this
          model does have an `<unk>` token defined.  If I learn what to change  I'll
          update the model
        updatedAt: '2023-08-05T20:57:04.608Z'
      numEdits: 0
      reactions: []
    id: 64ceb7a0072225e7f0ac026f
    type: comment
  author: TheBloke
  content: Yeah OK I just saw someone else reporting this as well in a different context.
    I don't yet know what the issue is.  I think maybe some code has updated in one
    of the libraries that ooba uses and now it makes certain older model configs no
    longer work.  But I don't know what exactly, as this model does have an `<unk>`
    token defined.  If I learn what to change  I'll update the model
  created_at: 2023-08-05 19:57:04+00:00
  edited: false
  hidden: false
  id: 64ceb7a0072225e7f0ac026f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/17fb3f2f3514da058e54de0e76ba52be.svg
      fullname: Preston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: imagineaiuser
      type: user
    createdAt: '2023-08-05T21:14:24.000Z'
    data:
      edited: false
      editors:
      - imagineaiuser
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7798129916191101
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/17fb3f2f3514da058e54de0e76ba52be.svg
          fullname: Preston
          isHf: false
          isPro: false
          name: imagineaiuser
          type: user
        html: "<p>I haven\u2019t gotten this error but seen a lot of Wierd errors\
          \ in Ooga today </p>\n"
        raw: "I haven\u2019t gotten this error but seen a lot of Wierd errors in Ooga\
          \ today "
        updatedAt: '2023-08-05T21:14:24.925Z'
      numEdits: 0
      reactions: []
    id: 64cebbb0e8df1f66dd5bcf53
    type: comment
  author: imagineaiuser
  content: "I haven\u2019t gotten this error but seen a lot of Wierd errors in Ooga\
    \ today "
  created_at: 2023-08-05 20:14:24+00:00
  edited: false
  hidden: false
  id: 64cebbb0e8df1f66dd5bcf53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
      fullname: Holger Brunken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lordwind
      type: user
    createdAt: '2023-08-08T11:05:39.000Z'
    data:
      edited: false
      editors:
      - Lordwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9874635934829712
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
          fullname: Holger Brunken
          isHf: false
          isPro: false
          name: Lordwind
          type: user
        html: '<p>Thank you for taking care here. The problem seems tohappen with
          all kind of Llama 2 models. GPT I didn''t try. Please send a notification
          when you found the issue. I think many user of your model (and other models)
          would be happy :-)</p>

          '
        raw: Thank you for taking care here. The problem seems tohappen with all kind
          of Llama 2 models. GPT I didn't try. Please send a notification when you
          found the issue. I think many user of your model (and other models) would
          be happy :-)
        updatedAt: '2023-08-08T11:05:39.752Z'
      numEdits: 0
      reactions: []
    id: 64d22183b0b5c832525dfa42
    type: comment
  author: Lordwind
  content: Thank you for taking care here. The problem seems tohappen with all kind
    of Llama 2 models. GPT I didn't try. Please send a notification when you found
    the issue. I think many user of your model (and other models) would be happy :-)
  created_at: 2023-08-08 10:05:39+00:00
  edited: false
  hidden: false
  id: 64d22183b0b5c832525dfa42
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
      fullname: Holger Brunken
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lordwind
      type: user
    createdAt: '2023-08-10T16:02:47.000Z'
    data:
      edited: false
      editors:
      - Lordwind
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9704247117042542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1ae7a264271f08f4aa9e60fadd52c485.svg
          fullname: Holger Brunken
          isHf: false
          isPro: false
          name: Lordwind
          type: user
        html: '<p>Hello. I got the answer by Oobabooga. </p>

          <p>wrong repository</p>

          <p>But this was all. Does it help to solve this issue?</p>

          <p>Thank you for help. Really great</p>

          <p>Holger</p>

          '
        raw: "Hello. I got the answer by Oobabooga. \n\nwrong repository\n\nBut this\
          \ was all. Does it help to solve this issue?\n\nThank you for help. Really\
          \ great\n\nHolger"
        updatedAt: '2023-08-10T16:02:47.875Z'
      numEdits: 0
      reactions: []
    id: 64d50a27d6f3b8810df495c9
    type: comment
  author: Lordwind
  content: "Hello. I got the answer by Oobabooga. \n\nwrong repository\n\nBut this\
    \ was all. Does it help to solve this issue?\n\nThank you for help. Really great\n\
    \nHolger"
  created_at: 2023-08-10 15:02:47+00:00
  edited: false
  hidden: false
  id: 64d50a27d6f3b8810df495c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8d15b249589b2c44c61eb94b0ca0f197.svg
      fullname: Sanaz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sanazkh
      type: user
    createdAt: '2023-11-18T07:14:15.000Z'
    data:
      edited: false
      editors:
      - Sanazkh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9900840520858765
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8d15b249589b2c44c61eb94b0ca0f197.svg
          fullname: Sanaz
          isHf: false
          isPro: false
          name: Sanazkh
          type: user
        html: '<p>I had this error too.<br>For me, the weight files (.h5 and .bin)
          had zero volume. I downloaded them again, and now they are correct and It
          work.</p>

          '
        raw: 'I had this error too.

          For me, the weight files (.h5 and .bin) had zero volume. I downloaded them
          again, and now they are correct and It work.'
        updatedAt: '2023-11-18T07:14:15.856Z'
      numEdits: 0
      reactions: []
    id: 6558644730ad83ad6b1b2284
    type: comment
  author: Sanazkh
  content: 'I had this error too.

    For me, the weight files (.h5 and .bin) had zero volume. I downloaded them again,
    and now they are correct and It work.'
  created_at: 2023-11-18 07:14:15+00:00
  edited: false
  hidden: false
  id: 6558644730ad83ad6b1b2284
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Pygmalion-7B-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: 'Bug with this model under Oobabooga "Internal: unk is not defined"'
