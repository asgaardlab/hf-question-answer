!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheBacteria
conflicting_files: null
created_at: 2023-11-27 22:31:37+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/553d1ef96a3ffc2b226b04434fa4b53b.svg
      fullname: Bacteria
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheBacteria
      type: user
    createdAt: '2023-11-27T22:31:37.000Z'
    data:
      edited: false
      editors:
      - TheBacteria
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9077169895172119
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/553d1ef96a3ffc2b226b04434fa4b53b.svg
          fullname: Bacteria
          isHf: false
          isPro: false
          name: TheBacteria
          type: user
        html: '<p>What are some of the potential approaches or arguments that can
          help reduce the inference latency on a CPU cluster?</p>

          '
        raw: What are some of the potential approaches or arguments that can help
          reduce the inference latency on a CPU cluster?
        updatedAt: '2023-11-27T22:31:37.953Z'
      numEdits: 0
      reactions: []
    id: 656518c9e5aac326bfa917e3
    type: comment
  author: TheBacteria
  content: What are some of the potential approaches or arguments that can help reduce
    the inference latency on a CPU cluster?
  created_at: 2023-11-27 22:31:37+00:00
  edited: false
  hidden: false
  id: 656518c9e5aac326bfa917e3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cdf266604b24bc9ae599aa2def8debd.svg
      fullname: lvkaokao
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lvkaokao
      type: user
    createdAt: '2023-11-30T05:38:16.000Z'
    data:
      edited: true
      editors:
      - lvkaokao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.770484447479248
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cdf266604b24bc9ae599aa2def8debd.svg
          fullname: lvkaokao
          isHf: false
          isPro: false
          name: lvkaokao
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;TheBacteria&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBacteria\"\
          >@<span class=\"underline\">TheBacteria</span></a></span>\n\n\t</span></span>,\
          \ Intel provides the effective LLM quantization tool (Intel Neural Compressor\
          \ <a rel=\"nofollow\" href=\"https://github.com/intel/neural-compressor\"\
          >https://github.com/intel/neural-compressor</a> ) to generate low-bit model\
          \ (e.g., INT4/FP4/NF4, and INT8) and LLM runtime (Intel Extension for Transformers\
          \ <a rel=\"nofollow\" href=\"https://github.com/intel/intel-extension-for-transformers/tree/main\"\
          >https://github.com/intel/intel-extension-for-transformers/tree/main</a>\
          \ ) to demonstrate the inference efficiency on Intel platforms by extending\
          \ Hugging Face Transformers APIs.</p>\n<p>you can also refer the paper:\
          \ <a href=\"https://huggingface.co/papers/2311.16133\">https://huggingface.co/papers/2311.16133</a>\
          \ and blog: <a rel=\"nofollow\" href=\"https://medium.com/intel-analytics-software/efficient-streaming-llm-with-intel-extension-for-transformers-runtime-31ee24577d26\"\
          >https://medium.com/intel-analytics-software/efficient-streaming-llm-with-intel-extension-for-transformers-runtime-31ee24577d26</a>\
          \ </p>\n"
        raw: 'hi @TheBacteria, Intel provides the effective LLM quantization tool
          (Intel Neural Compressor https://github.com/intel/neural-compressor ) to
          generate low-bit model (e.g., INT4/FP4/NF4, and INT8) and LLM runtime (Intel
          Extension for Transformers https://github.com/intel/intel-extension-for-transformers/tree/main
          ) to demonstrate the inference efficiency on Intel platforms by extending
          Hugging Face Transformers APIs.


          you can also refer the paper: https://huggingface.co/papers/2311.16133 and
          blog: https://medium.com/intel-analytics-software/efficient-streaming-llm-with-intel-extension-for-transformers-runtime-31ee24577d26 '
        updatedAt: '2023-11-30T05:59:46.083Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBacteria
    id: 65681fc883a448555e2f3c6b
    type: comment
  author: lvkaokao
  content: 'hi @TheBacteria, Intel provides the effective LLM quantization tool (Intel
    Neural Compressor https://github.com/intel/neural-compressor ) to generate low-bit
    model (e.g., INT4/FP4/NF4, and INT8) and LLM runtime (Intel Extension for Transformers
    https://github.com/intel/intel-extension-for-transformers/tree/main ) to demonstrate
    the inference efficiency on Intel platforms by extending Hugging Face Transformers
    APIs.


    you can also refer the paper: https://huggingface.co/papers/2311.16133 and blog:
    https://medium.com/intel-analytics-software/efficient-streaming-llm-with-intel-extension-for-transformers-runtime-31ee24577d26 '
  created_at: 2023-11-30 05:38:16+00:00
  edited: true
  hidden: false
  id: 65681fc883a448555e2f3c6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/553d1ef96a3ffc2b226b04434fa4b53b.svg
      fullname: Bacteria
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheBacteria
      type: user
    createdAt: '2023-11-30T13:32:37.000Z'
    data:
      edited: true
      editors:
      - TheBacteria
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8350498080253601
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/553d1ef96a3ffc2b226b04434fa4b53b.svg
          fullname: Bacteria
          isHf: false
          isPro: false
          name: TheBacteria
          type: user
        html: "<p>Fantastic! Thanks so much <span data-props=\"{&quot;user&quot;:&quot;lvkaokao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lvkaokao\"\
          >@<span class=\"underline\">lvkaokao</span></a></span>\n\n\t</span></span>\
          \ for such a helpful response.</p>\n"
        raw: Fantastic! Thanks so much @lvkaokao for such a helpful response.
        updatedAt: '2023-11-30T13:34:31.134Z'
      numEdits: 2
      reactions: []
    id: 65688ef5b733e539ae8be6a0
    type: comment
  author: TheBacteria
  content: Fantastic! Thanks so much @lvkaokao for such a helpful response.
  created_at: 2023-11-30 13:32:37+00:00
  edited: true
  hidden: false
  id: 65688ef5b733e539ae8be6a0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: Intel/neural-chat-7b-v3-1
repo_type: model
status: open
target_branch: null
title: Potential ways to reduce inference latency on CPU cluster?
