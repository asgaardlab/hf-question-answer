!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fahadh4ilyas
conflicting_files: null
created_at: 2023-07-02 15:03:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-02T16:03:01.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8895228505134583
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>When I''m looking at the <code>config.json</code> from longchat,
          the value of <code>max_position_embeddings</code> still 2048, but the <code>max_sequence_length</code>
          is set to 16384. I don''t understand what is the difference? And why your
          <code>config.json</code> did not contain <code>max_sequence_length</code>?</p>

          '
        raw: When I'm looking at the `config.json` from longchat, the value of `max_position_embeddings`
          still 2048, but the `max_sequence_length` is set to 16384. I don't understand
          what is the difference? And why your `config.json` did not contain `max_sequence_length`?
        updatedAt: '2023-07-02T16:03:01.775Z'
      numEdits: 0
      reactions: []
    id: 64a19fb5849787adb112c162
    type: comment
  author: fahadh4ilyas
  content: When I'm looking at the `config.json` from longchat, the value of `max_position_embeddings`
    still 2048, but the `max_sequence_length` is set to 16384. I don't understand
    what is the difference? And why your `config.json` did not contain `max_sequence_length`?
  created_at: 2023-07-02 15:03:01+00:00
  edited: false
  hidden: false
  id: 64a19fb5849787adb112c162
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-08T01:30:36.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7736342549324036
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>From what i understand those values are what the model needs to
          run, you can just change the actual values for the model to generate text
          at 8k or 16k tokens inside text generation web ui and not worry about the
          config files</p>

          '
        raw: From what i understand those values are what the model needs to run,
          you can just change the actual values for the model to generate text at
          8k or 16k tokens inside text generation web ui and not worry about the config
          files
        updatedAt: '2023-07-08T01:30:36.578Z'
      numEdits: 0
      reactions: []
    id: 64a8bc3cd14398e2708dadb1
    type: comment
  author: rombodawg
  content: From what i understand those values are what the model needs to run, you
    can just change the actual values for the model to generate text at 8k or 16k
    tokens inside text generation web ui and not worry about the config files
  created_at: 2023-07-08 00:30:36+00:00
  edited: false
  hidden: false
  id: 64a8bc3cd14398e2708dadb1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T08:22:43.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8908501267433167
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes you don''t need to touch config.json if using text-generation-webui
          with ExLlama, as it has UI parameters for sequence length and compression
          emb.</p>

          <p>But you do need to change max_position_embeddings if you''re using AutoGPTQ,
          as that''s how it knows what sequence length to use. That''s detailed in
          my README.</p>

          '
        raw: 'Yes you don''t need to touch config.json if using text-generation-webui
          with ExLlama, as it has UI parameters for sequence length and compression
          emb.


          But you do need to change max_position_embeddings if you''re using AutoGPTQ,
          as that''s how it knows what sequence length to use. That''s detailed in
          my README.'
        updatedAt: '2023-07-08T08:22:43.539Z'
      numEdits: 0
      reactions: []
    id: 64a91cd3d14398e2709795ff
    type: comment
  author: TheBloke
  content: 'Yes you don''t need to touch config.json if using text-generation-webui
    with ExLlama, as it has UI parameters for sequence length and compression emb.


    But you do need to change max_position_embeddings if you''re using AutoGPTQ, as
    that''s how it knows what sequence length to use. That''s detailed in my README.'
  created_at: 2023-07-08 07:22:43+00:00
  edited: false
  hidden: false
  id: 64a91cd3d14398e2709795ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-08T08:28:04.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8985763192176819
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>Yeah, after seeing how longchat load its model, It seems they use
          ratio parameter to change max_position_embeddings from 2048 to 16k. Kinda
          weird why they didn''t set max_position_embedding to 16k at the beginning.</p>

          '
        raw: Yeah, after seeing how longchat load its model, It seems they use ratio
          parameter to change max_position_embeddings from 2048 to 16k. Kinda weird
          why they didn't set max_position_embedding to 16k at the beginning.
        updatedAt: '2023-07-08T08:28:04.262Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64a91e14e831371424eb5c70
    id: 64a91e14e831371424eb5c6f
    type: comment
  author: fahadh4ilyas
  content: Yeah, after seeing how longchat load its model, It seems they use ratio
    parameter to change max_position_embeddings from 2048 to 16k. Kinda weird why
    they didn't set max_position_embedding to 16k at the beginning.
  created_at: 2023-07-08 07:28:04+00:00
  edited: false
  hidden: false
  id: 64a91e14e831371424eb5c6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2023-07-08T08:28:04.000Z'
    data:
      status: closed
    id: 64a91e14e831371424eb5c70
    type: status-change
  author: fahadh4ilyas
  created_at: 2023-07-08 07:28:04+00:00
  id: 64a91e14e831371424eb5c70
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/LongChat-13B-GPTQ
repo_type: model
status: closed
target_branch: null
title: Does max_position_embeddings really the parameter to be changed?
