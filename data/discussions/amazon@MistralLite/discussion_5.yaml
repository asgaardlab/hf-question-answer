!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-10-27 19:10:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-10-27T20:10:17.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8283872008323669
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I have created and refined open source dataset named "LosslessmegacodeV3"
          (Linked at the end) which I believe could create one of the best open source
          ai models if trained on the right base model. However seeing as I am severally
          lacking in funding (Aka im broke af) I haven''t been able to do the training
          myself. I''m curious if your team would be willing to take on the challenge
          of training one ai model on my dataset for coding and non-coding tasks (the
          dataset is made for both) to create possibly one of the best ai models available.
          If you are up for the challenge, here is a list of the top models I would
          recommend training with my dataset in order or highest priority (Note that
          I would only ask you to train 1 model, I am merely giving multiple options):</p>

          <p>1: WizardLM/WizardCoder-Python-13B-V1.0</p>

          <ul>

          <li><a href="https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0">https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0</a></li>

          </ul>

          <p>2: amazon/MistralLite</p>

          <ul>

          <li><a href="https://huggingface.co/amazon/MistralLite">https://huggingface.co/amazon/MistralLite</a></li>

          </ul>

          <p>3: WizardLM/WizardCoder-Python-34B-V1.0 (<a href="/amazon/MistralLite/discussions/3">#3</a>
          and <a href="/amazon/MistralLite/discussions/4">#4</a> are equal in priority)</p>

          <ul>

          <li><a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0">https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0</a></li>

          </ul>

          <p>4: Phind/Phind-CodeLlama-34B-v2 (<a href="/amazon/MistralLite/discussions/3">#3</a>
          and <a href="/amazon/MistralLite/discussions/4">#4</a> are equal in priority)</p>

          <ul>

          <li><a href="https://huggingface.co/Phind/Phind-CodeLlama-34B-v2">https://huggingface.co/Phind/Phind-CodeLlama-34B-v2</a></li>

          </ul>

          <p>5: jondurbin/airoboros-l2-c70b-3.1.2</p>

          <ul>

          <li><a href="https://huggingface.co/jondurbin/airoboros-l2-c70b-3.1.2">https://huggingface.co/jondurbin/airoboros-l2-c70b-3.1.2</a></li>

          </ul>

          <hr>

          <p>If you agree I have some names for the model that would release if you
          would allow me. I''ve listed them bellow. Lossless and V3 referring to the
          datasets That were used to train the models on.</p>

          <p>1: LosslessWizardCoder-Python-13B-V3</p>

          <p>2: LosslessMistralLitecoderV3</p>

          <p>3: LosslessWizardCoder-Python-34B-V3 </p>

          <p>4: LoesslessPhind-LlamaCoder-34B-V3</p>

          <p>5: LosslessAiroborosCoder-l2-c70b-V3</p>

          <hr>

          <p>Dataset link:</p>

          <ul>

          <li><a href="https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol">https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol</a></li>

          </ul>

          '
        raw: "I have created and refined open source dataset named \"LosslessmegacodeV3\"\
          \ (Linked at the end) which I believe could create one of the best open\
          \ source ai models if trained on the right base model. However seeing as\
          \ I am severally lacking in funding (Aka im broke af) I haven't been able\
          \ to do the training myself. I'm curious if your team would be willing to\
          \ take on the challenge of training one ai model on my dataset for coding\
          \ and non-coding tasks (the dataset is made for both) to create possibly\
          \ one of the best ai models available. If you are up for the challenge,\
          \ here is a list of the top models I would recommend training with my dataset\
          \ in order or highest priority (Note that I would only ask you to train\
          \ 1 model, I am merely giving multiple options):\r\n\r\n1: WizardLM/WizardCoder-Python-13B-V1.0\r\
          \n- https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0\r\n\r\n\
          2: amazon/MistralLite\r\n- https://huggingface.co/amazon/MistralLite\r\n\
          \r\n3: WizardLM/WizardCoder-Python-34B-V1.0 (#3 and #4 are equal in priority)\r\
          \n- https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0\r\n\r\n\
          4: Phind/Phind-CodeLlama-34B-v2 (#3 and #4 are equal in priority)\r\n- https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\r\
          \n\r\n5: jondurbin/airoboros-l2-c70b-3.1.2\r\n- https://huggingface.co/jondurbin/airoboros-l2-c70b-3.1.2\r\
          \n\r\n_______________________________________________________________________________________________________________________________\r\
          \n\r\nIf you agree I have some names for the model that would release if\
          \ you would allow me. I've listed them bellow. Lossless and V3 referring\
          \ to the datasets That were used to train the models on.\r\n\r\n1: LosslessWizardCoder-Python-13B-V3\r\
          \n\r\n2: LosslessMistralLitecoderV3\r\n\r\n3: LosslessWizardCoder-Python-34B-V3\
          \ \r\n\r\n4: LoesslessPhind-LlamaCoder-34B-V3\r\n\r\n5: LosslessAiroborosCoder-l2-c70b-V3\r\
          \n_______________________________________________________________________________________________________________________________\r\
          \n\r\nDataset link:\r\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol"
        updatedAt: '2023-10-27T20:10:17.327Z'
      numEdits: 0
      reactions: []
    id: 653c1929b110c9180d6c87e1
    type: comment
  author: rombodawg
  content: "I have created and refined open source dataset named \"LosslessmegacodeV3\"\
    \ (Linked at the end) which I believe could create one of the best open source\
    \ ai models if trained on the right base model. However seeing as I am severally\
    \ lacking in funding (Aka im broke af) I haven't been able to do the training\
    \ myself. I'm curious if your team would be willing to take on the challenge of\
    \ training one ai model on my dataset for coding and non-coding tasks (the dataset\
    \ is made for both) to create possibly one of the best ai models available. If\
    \ you are up for the challenge, here is a list of the top models I would recommend\
    \ training with my dataset in order or highest priority (Note that I would only\
    \ ask you to train 1 model, I am merely giving multiple options):\r\n\r\n1: WizardLM/WizardCoder-Python-13B-V1.0\r\
    \n- https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0\r\n\r\n2: amazon/MistralLite\r\
    \n- https://huggingface.co/amazon/MistralLite\r\n\r\n3: WizardLM/WizardCoder-Python-34B-V1.0\
    \ (#3 and #4 are equal in priority)\r\n- https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0\r\
    \n\r\n4: Phind/Phind-CodeLlama-34B-v2 (#3 and #4 are equal in priority)\r\n- https://huggingface.co/Phind/Phind-CodeLlama-34B-v2\r\
    \n\r\n5: jondurbin/airoboros-l2-c70b-3.1.2\r\n- https://huggingface.co/jondurbin/airoboros-l2-c70b-3.1.2\r\
    \n\r\n_______________________________________________________________________________________________________________________________\r\
    \n\r\nIf you agree I have some names for the model that would release if you would\
    \ allow me. I've listed them bellow. Lossless and V3 referring to the datasets\
    \ That were used to train the models on.\r\n\r\n1: LosslessWizardCoder-Python-13B-V3\r\
    \n\r\n2: LosslessMistralLitecoderV3\r\n\r\n3: LosslessWizardCoder-Python-34B-V3\
    \ \r\n\r\n4: LoesslessPhind-LlamaCoder-34B-V3\r\n\r\n5: LosslessAiroborosCoder-l2-c70b-V3\r\
    \n_______________________________________________________________________________________________________________________________\r\
    \n\r\nDataset link:\r\n- https://huggingface.co/datasets/rombodawg/LosslessMegaCodeTrainingV3_1.6m_Evol"
  created_at: 2023-10-27 19:10:17+00:00
  edited: false
  hidden: false
  id: 653c1929b110c9180d6c87e1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
      fullname: Yin Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yinsong1986
      type: user
    createdAt: '2023-10-30T22:59:25.000Z'
    data:
      edited: false
      editors:
      - yinsong1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9281235933303833
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
          fullname: Yin Song
          isHf: false
          isPro: false
          name: yinsong1986
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;rombodawg&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rombodawg\"\
          >@<span class=\"underline\">rombodawg</span></a></span>\n\n\t</span></span>\
          \ Thanks for the advice!</p>\n<p>Unfortunately, we have our internal process\
          \ to work on this topic.</p>\n<p>Based on your description, <a rel=\"nofollow\"\
          \ href=\"https://aws.amazon.com/codewhisperer/\">codewhisperer</a> might\
          \ be sth you are interested. Please have a try :)</p>\n"
        raw: 'Hi @rombodawg Thanks for the advice!


          Unfortunately, we have our internal process to work on this topic.


          Based on your description, [codewhisperer](https://aws.amazon.com/codewhisperer/)
          might be sth you are interested. Please have a try :)'
        updatedAt: '2023-10-30T22:59:25.279Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rombodawg
    id: 6540354d6e87243178399b13
    type: comment
  author: yinsong1986
  content: 'Hi @rombodawg Thanks for the advice!


    Unfortunately, we have our internal process to work on this topic.


    Based on your description, [codewhisperer](https://aws.amazon.com/codewhisperer/)
    might be sth you are interested. Please have a try :)'
  created_at: 2023-10-30 21:59:25+00:00
  edited: false
  hidden: false
  id: 6540354d6e87243178399b13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
      fullname: Yin Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yinsong1986
      type: user
    createdAt: '2023-11-04T01:22:08.000Z'
    data:
      status: closed
    id: 65459cc0f7dd43aaa97599e5
    type: status-change
  author: yinsong1986
  created_at: 2023-11-04 00:22:08+00:00
  id: 65459cc0f7dd43aaa97599e5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: amazon/MistralLite
repo_type: model
status: closed
target_branch: null
title: 'Request: Would the amazon team be willing to train a model on my high quality
  dataset?'
