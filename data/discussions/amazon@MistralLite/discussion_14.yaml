!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krecceg
conflicting_files: null
created_at: 2023-11-29 10:01:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7dbc24c8312f7038b68a3ee418d0043d.svg
      fullname: Goh Chia Wei Kenneth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krecceg
      type: user
    createdAt: '2023-11-29T10:01:19.000Z'
    data:
      edited: false
      editors:
      - krecceg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.902808427810669
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7dbc24c8312f7038b68a3ee418d0043d.svg
          fullname: Goh Chia Wei Kenneth
          isHf: false
          isPro: false
          name: krecceg
          type: user
        html: '<p>I have been testing with mistral lite on various platforms and it
          seems that using text generation inference or transformers directly, the
          output starts to break when the context length exceeds a certain threshold
          -- roughly 16k. Is there any possible reason why this is the case since
          it should theoretically support 32k context length?</p>

          '
        raw: I have been testing with mistral lite on various platforms and it seems
          that using text generation inference or transformers directly, the output
          starts to break when the context length exceeds a certain threshold -- roughly
          16k. Is there any possible reason why this is the case since it should theoretically
          support 32k context length?
        updatedAt: '2023-11-29T10:01:19.364Z'
      numEdits: 0
      reactions: []
    id: 65670bef751591e4faf0cc54
    type: comment
  author: krecceg
  content: I have been testing with mistral lite on various platforms and it seems
    that using text generation inference or transformers directly, the output starts
    to break when the context length exceeds a certain threshold -- roughly 16k. Is
    there any possible reason why this is the case since it should theoretically support
    32k context length?
  created_at: 2023-11-29 10:01:19+00:00
  edited: false
  hidden: false
  id: 65670bef751591e4faf0cc54
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
      fullname: Yin Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yinsong1986
      type: user
    createdAt: '2023-11-29T11:25:12.000Z'
    data:
      edited: false
      editors:
      - yinsong1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8443236947059631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
          fullname: Yin Song
          isHf: false
          isPro: false
          name: yinsong1986
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;krecceg&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/krecceg\">@<span class=\"\
          underline\">krecceg</span></a></span>\n\n\t</span></span> </p>\n<p>Thanks\
          \ for the feedback! Can you clarify if the break was out of memory or just\
          \ output gibberish text? Cheers!</p>\n"
        raw: "Hi @krecceg \n\nThanks for the feedback! Can you clarify if the break\
          \ was out of memory or just output gibberish text? Cheers!"
        updatedAt: '2023-11-29T11:25:12.371Z'
      numEdits: 0
      reactions: []
    id: 65671f98944a6c35cf8f7c24
    type: comment
  author: yinsong1986
  content: "Hi @krecceg \n\nThanks for the feedback! Can you clarify if the break\
    \ was out of memory or just output gibberish text? Cheers!"
  created_at: 2023-11-29 11:25:12+00:00
  edited: false
  hidden: false
  id: 65671f98944a6c35cf8f7c24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef4da027e12e05f5ee186b3d1499f2e.svg
      fullname: nicholas like
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicklikets
      type: user
    createdAt: '2023-12-01T16:46:58.000Z'
    data:
      edited: false
      editors:
      - nicklikets
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5242414474487305
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef4da027e12e05f5ee186b3d1499f2e.svg
          fullname: nicholas like
          isHf: false
          isPro: false
          name: nicklikets
          type: user
        html: "<p>I am also getting this issue. I am instantiating and running my\
          \ model using the snippet below:</p>\n<pre><code>model = AutoModelForCausalLM.from_pretrained(\n\
          \    \"amazon/MistralLite\",\n    torch_dtype=torch.bfloat16,\n    use_flash_attention_2=True,\n\
          \    device_map=\"auto\"\n        )\ntokeniser = AutoTokenizer.from_pretrained(\n\
          \    \"amazon/MistralLite\",\n    max_length=model_config.max_seq_len\n\
          )\npipe = pipeline(\n     \"text-generation\",\n     model=_model,\n   \
          \  tokenizer=_tokeniser,\n     max_length=model_config.max_seq_len\n)\n\
          pipe.padding_side = \"left\"\n</code></pre>\n<p>And then I am generating\
          \ my output using this snippet:</p>\n<pre><code>with torch.no_grad():\n\
          \    output = pipe(\n    input,\n    return_full_text=False,\n    do_sample=False,\n\
          \    use_cache=True,\n    eos_token_id=tokeniser.eos_token_id,\n    num_return_sequences=1,\n\
          )\n</code></pre>\n<p>And when the input token length is above 16k I get\
          \ this error:</p>\n<pre><code>File ~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:374,\
          \ in MistralFlashAttention2.forward(self, hidden_states, attention_mask,\
          \ position_ids, past_key_value, output_attentions, use_cache, **kwargs)\n\
          \    371 past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n\
          \    373 if past_key.shape[-2] != self.config.sliding_window - 1:\n--&gt;\
          \ 374     raise ValueError(\n    375         f\"past key much have a shape\
          \ of (`batch_size, num_heads, self.config.sliding_window-1, head_dim`),\
          \ got\"\n    376         f\" {past_key.shape}\"\n    377     )\n    379\
          \ past_key_value = (past_key, past_value)\n    381 if attention_mask is\
          \ not None:\n\nValueError: past key much have a shape of (`batch_size, num_heads,\
          \ self.config.sliding_window-1, head_dim`), got torch.Size([1, 8, 8318,\
          \ 128])\n</code></pre>\n<p>Unless I set this parameter</p>\n<pre><code>pipe.model.config.sliding_window\
          \ = 32000\n</code></pre>\n<p>The question being, is this the intended behaviour?\
          \ What is the ideal size of config.sliding_window? Why does it crash without\
          \ this being explicitly set?</p>\n"
        raw: "I am also getting this issue. I am instantiating and running my model\
          \ using the snippet below:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    \"amazon/MistralLite\",\n    torch_dtype=torch.bfloat16,\n    use_flash_attention_2=True,\n\
          \    device_map=\"auto\"\n        )\ntokeniser = AutoTokenizer.from_pretrained(\n\
          \    \"amazon/MistralLite\",\n    max_length=model_config.max_seq_len\n\
          )\npipe = pipeline(\n     \"text-generation\",\n     model=_model,\n   \
          \  tokenizer=_tokeniser,\n     max_length=model_config.max_seq_len\n)\n\
          pipe.padding_side = \"left\"\n```\nAnd then I am generating my output using\
          \ this snippet:\n```\nwith torch.no_grad():\n    output = pipe(\n    input,\n\
          \    return_full_text=False,\n    do_sample=False,\n    use_cache=True,\n\
          \    eos_token_id=tokeniser.eos_token_id,\n    num_return_sequences=1,\n\
          )\n```\nAnd when the input token length is above 16k I get this error:\n\
          ```\nFile ~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:374,\
          \ in MistralFlashAttention2.forward(self, hidden_states, attention_mask,\
          \ position_ids, past_key_value, output_attentions, use_cache, **kwargs)\n\
          \    371 past_value = past_value[:, :, slicing_tokens:, :].contiguous()\n\
          \    373 if past_key.shape[-2] != self.config.sliding_window - 1:\n--> 374\
          \     raise ValueError(\n    375         f\"past key much have a shape of\
          \ (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\"\
          \n    376         f\" {past_key.shape}\"\n    377     )\n    379 past_key_value\
          \ = (past_key, past_value)\n    381 if attention_mask is not None:\n\nValueError:\
          \ past key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,\
          \ head_dim`), got torch.Size([1, 8, 8318, 128])\n```\n\nUnless I set this\
          \ parameter\n```\npipe.model.config.sliding_window = 32000\n```\n\nThe question\
          \ being, is this the intended behaviour? What is the ideal size of config.sliding_window?\
          \ Why does it crash without this being explicitly set?"
        updatedAt: '2023-12-01T16:46:58.378Z'
      numEdits: 0
      reactions: []
    id: 656a0e02903e16e62b74ad96
    type: comment
  author: nicklikets
  content: "I am also getting this issue. I am instantiating and running my model\
    \ using the snippet below:\n```\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    \"amazon/MistralLite\",\n    torch_dtype=torch.bfloat16,\n    use_flash_attention_2=True,\n\
    \    device_map=\"auto\"\n        )\ntokeniser = AutoTokenizer.from_pretrained(\n\
    \    \"amazon/MistralLite\",\n    max_length=model_config.max_seq_len\n)\npipe\
    \ = pipeline(\n     \"text-generation\",\n     model=_model,\n     tokenizer=_tokeniser,\n\
    \     max_length=model_config.max_seq_len\n)\npipe.padding_side = \"left\"\n```\n\
    And then I am generating my output using this snippet:\n```\nwith torch.no_grad():\n\
    \    output = pipe(\n    input,\n    return_full_text=False,\n    do_sample=False,\n\
    \    use_cache=True,\n    eos_token_id=tokeniser.eos_token_id,\n    num_return_sequences=1,\n\
    )\n```\nAnd when the input token length is above 16k I get this error:\n```\n\
    File ~/.local/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:374,\
    \ in MistralFlashAttention2.forward(self, hidden_states, attention_mask, position_ids,\
    \ past_key_value, output_attentions, use_cache, **kwargs)\n    371 past_value\
    \ = past_value[:, :, slicing_tokens:, :].contiguous()\n    373 if past_key.shape[-2]\
    \ != self.config.sliding_window - 1:\n--> 374     raise ValueError(\n    375 \
    \        f\"past key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,\
    \ head_dim`), got\"\n    376         f\" {past_key.shape}\"\n    377     )\n \
    \   379 past_key_value = (past_key, past_value)\n    381 if attention_mask is\
    \ not None:\n\nValueError: past key much have a shape of (`batch_size, num_heads,\
    \ self.config.sliding_window-1, head_dim`), got torch.Size([1, 8, 8318, 128])\n\
    ```\n\nUnless I set this parameter\n```\npipe.model.config.sliding_window = 32000\n\
    ```\n\nThe question being, is this the intended behaviour? What is the ideal size\
    \ of config.sliding_window? Why does it crash without this being explicitly set?"
  created_at: 2023-12-01 16:46:58+00:00
  edited: false
  hidden: false
  id: 656a0e02903e16e62b74ad96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
      fullname: Yin Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yinsong1986
      type: user
    createdAt: '2023-12-02T00:55:54.000Z'
    data:
      edited: true
      editors:
      - yinsong1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9339766502380371
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
          fullname: Yin Song
          isHf: false
          isPro: false
          name: yinsong1986
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;nicklikets&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nicklikets\"\
          >@<span class=\"underline\">nicklikets</span></a></span>\n\n\t</span></span>\
          \ Thanks for sharing this!</p>\n<p>I was using the code snnipet below</p>\n\
          <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import transformers\nimport torch\n\nmodel_id = \"amazon/MistralLite\"\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
          \                                             torch_dtype=torch.bfloat16,\n\
          \                                             use_flash_attention_2=True,\n\
          \                                             device_map=\"auto\",)\npipeline\
          \ = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n)\n\n# replace with your long context 16K\nprompt\
          \ = \".....................\"\n\nsequences = pipeline(\n    prompt,\n  \
          \  max_new_tokens=400,\n    do_sample=False,\n    return_full_text=False,\n\
          \    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n</code></pre>\n\
          <p>It seems to be okay for input &gt;16K, the transformers version I was\
          \ using is <code>4.35.2</code>.</p>\n<p>Can you double check if your code\
          \ was configuring correctly? Or paste a more comprehensive code snippet\
          \ here. Thank you!</p>\n"
        raw: "Hi @nicklikets Thanks for sharing this!\n\nI was using the code snnipet\
          \ below\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import transformers\nimport torch\n\nmodel_id = \"amazon/MistralLite\"\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
          \                                             torch_dtype=torch.bfloat16,\n\
          \                                             use_flash_attention_2=True,\n\
          \                                             device_map=\"auto\",)\npipeline\
          \ = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n)\n\n# replace with your long context 16K\nprompt\
          \ = \".....................\"\n\nsequences = pipeline(\n    prompt,\n  \
          \  max_new_tokens=400,\n    do_sample=False,\n    return_full_text=False,\n\
          \    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n```\n\
          It seems to be okay for input >16K, the transformers version I was using\
          \ is `4.35.2`.\n\nCan you double check if your code was configuring correctly?\
          \ Or paste a more comprehensive code snippet here. Thank you!"
        updatedAt: '2023-12-02T00:59:10.349Z'
      numEdits: 1
      reactions: []
    id: 656a809a18123781412e9240
    type: comment
  author: yinsong1986
  content: "Hi @nicklikets Thanks for sharing this!\n\nI was using the code snnipet\
    \ below\n```\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport\
    \ transformers\nimport torch\n\nmodel_id = \"amazon/MistralLite\"\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
    \                                             torch_dtype=torch.bfloat16,\n  \
    \                                           use_flash_attention_2=True,\n    \
    \                                         device_map=\"auto\",)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\n#\
    \ replace with your long context 16K\nprompt = \".....................\"\n\nsequences\
    \ = pipeline(\n    prompt,\n    max_new_tokens=400,\n    do_sample=False,\n  \
    \  return_full_text=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
    )\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n```\nIt seems\
    \ to be okay for input >16K, the transformers version I was using is `4.35.2`.\n\
    \nCan you double check if your code was configuring correctly? Or paste a more\
    \ comprehensive code snippet here. Thank you!"
  created_at: 2023-12-02 00:55:54+00:00
  edited: true
  hidden: false
  id: 656a809a18123781412e9240
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ef4da027e12e05f5ee186b3d1499f2e.svg
      fullname: nicholas like
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nicklikets
      type: user
    createdAt: '2023-12-04T10:48:56.000Z'
    data:
      edited: true
      editors:
      - nicklikets
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46041688323020935
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ef4da027e12e05f5ee186b3d1499f2e.svg
          fullname: nicholas like
          isHf: false
          isPro: false
          name: nicklikets
          type: user
        html: "<p>Still getting that error from using your snippet on contexts longer\
          \ that 16k tokens in length (specifically 24455 tokens).<br>I am using these\
          \ package versions:</p>\n<pre><code>torch                         2.1.1\n\
          transformers                  4.35.2\nflash-attn                    2.3.6\n\
          </code></pre>\n<p>And below is my code snippet, and the console output:\
          \ </p>\n<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\
          import transformers\nimport torch\n\nmodel_id = \"amazon/MistralLite\"\n\
          \ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
          \                                             torch_dtype=torch.bfloat16,\n\
          \                                             use_flash_attention_2=True,\n\
          \                                             device_map=\"auto\",)\npipeline\
          \ = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n)\n\n\nwith open(\"./data/longcontext.txt\",\
          \ \"r\") as f:\n    doc = f.read()\nprompt = f\"&lt;|prompter|&gt;{doc},\
          \ Summarise this document&lt;/s&gt;&lt;|assistant|&gt;\"\n\ntokens = tokenizer(prompt)\n\
          print(len(tokens[0]))\n\nsequences = pipeline(\n    prompt,\n    max_new_tokens=400,\n\
          \    do_sample=False,\n    return_full_text=False,\n    num_return_sequences=1,\n\
          \    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n  \
          \  print(f\"{seq['generated_text']}\")\n\n----- CONSOLE OUTPUT -----\nLoading\
          \ checkpoint shards: 100%\n24455\n\nSetting `pad_token_id` to `eos_token_id`:2\
          \ for open-end generation.\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[4], line 26\n     23 tokens = tokenizer(prompt)\n     24 print(len(tokens[0]))\n\
          ---&gt; 26 sequences = pipeline(\n     27     prompt,\n     28     max_new_tokens=400,\n\
          \     29     do_sample=False,\n     30     return_full_text=False,\n   \
          \  31     num_return_sequences=1,\n     32     eos_token_id=tokenizer.eos_token_id,\n\
          \     33 )\n     34 for seq in sequences:\n     35     print(f\"{seq['generated_text']}\"\
          )\n...\nValueError: past key much have a shape of (`batch_size, num_heads,\
          \ self.config.sliding_window-1, head_dim`), got torch.Size([1, 8, 8312,\
          \ 128])\n</code></pre>\n"
        raw: "Still getting that error from using your snippet on contexts longer\
          \ that 16k tokens in length (specifically 24455 tokens).\nI am using these\
          \ package versions:\n```\ntorch                         2.1.1\ntransformers\
          \                  4.35.2\nflash-attn                    2.3.6\n```\nAnd\
          \ below is my code snippet, and the console output: \n```\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\nimport transformers\nimport\
          \ torch\n\nmodel_id = \"amazon/MistralLite\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\
          model = AutoModelForCausalLM.from_pretrained(model_id,\n               \
          \                              torch_dtype=torch.bfloat16,\n           \
          \                                  use_flash_attention_2=True,\n       \
          \                                      device_map=\"auto\",)\npipeline =\
          \ transformers.pipeline(\n    \"text-generation\",\n    model=model,\n \
          \   tokenizer=tokenizer,\n)\n\n\nwith open(\"./data/longcontext.txt\", \"\
          r\") as f:\n    doc = f.read()\nprompt = f\"<|prompter|>{doc}, Summarise\
          \ this document</s><|assistant|>\"\n\ntokens = tokenizer(prompt)\nprint(len(tokens[0]))\n\
          \nsequences = pipeline(\n    prompt,\n    max_new_tokens=400,\n    do_sample=False,\n\
          \    return_full_text=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n\
          )\nfor seq in sequences:\n    print(f\"{seq['generated_text']}\")\n\n-----\
          \ CONSOLE OUTPUT -----\nLoading checkpoint shards: 100%\n24455\n\nSetting\
          \ `pad_token_id` to `eos_token_id`:2 for open-end generation.\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          Cell In[4], line 26\n     23 tokens = tokenizer(prompt)\n     24 print(len(tokens[0]))\n\
          ---> 26 sequences = pipeline(\n     27     prompt,\n     28     max_new_tokens=400,\n\
          \     29     do_sample=False,\n     30     return_full_text=False,\n   \
          \  31     num_return_sequences=1,\n     32     eos_token_id=tokenizer.eos_token_id,\n\
          \     33 )\n     34 for seq in sequences:\n     35     print(f\"{seq['generated_text']}\"\
          )\n...\nValueError: past key much have a shape of (`batch_size, num_heads,\
          \ self.config.sliding_window-1, head_dim`), got torch.Size([1, 8, 8312,\
          \ 128])\n```"
        updatedAt: '2023-12-04T13:25:14.079Z'
      numEdits: 1
      reactions: []
    id: 656dae98ede71189ad722082
    type: comment
  author: nicklikets
  content: "Still getting that error from using your snippet on contexts longer that\
    \ 16k tokens in length (specifically 24455 tokens).\nI am using these package\
    \ versions:\n```\ntorch                         2.1.1\ntransformers          \
    \        4.35.2\nflash-attn                    2.3.6\n```\nAnd below is my code\
    \ snippet, and the console output: \n```\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\nimport transformers\nimport torch\n\nmodel_id = \"amazon/MistralLite\"\
    \n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\n\
    \                                             torch_dtype=torch.bfloat16,\n  \
    \                                           use_flash_attention_2=True,\n    \
    \                                         device_map=\"auto\",)\npipeline = transformers.pipeline(\n\
    \    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\n\n\
    with open(\"./data/longcontext.txt\", \"r\") as f:\n    doc = f.read()\nprompt\
    \ = f\"<|prompter|>{doc}, Summarise this document</s><|assistant|>\"\n\ntokens\
    \ = tokenizer(prompt)\nprint(len(tokens[0]))\n\nsequences = pipeline(\n    prompt,\n\
    \    max_new_tokens=400,\n    do_sample=False,\n    return_full_text=False,\n\
    \    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor\
    \ seq in sequences:\n    print(f\"{seq['generated_text']}\")\n\n----- CONSOLE\
    \ OUTPUT -----\nLoading checkpoint shards: 100%\n24455\n\nSetting `pad_token_id`\
    \ to `eos_token_id`:2 for open-end generation.\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    Cell In[4], line 26\n     23 tokens = tokenizer(prompt)\n     24 print(len(tokens[0]))\n\
    ---> 26 sequences = pipeline(\n     27     prompt,\n     28     max_new_tokens=400,\n\
    \     29     do_sample=False,\n     30     return_full_text=False,\n     31  \
    \   num_return_sequences=1,\n     32     eos_token_id=tokenizer.eos_token_id,\n\
    \     33 )\n     34 for seq in sequences:\n     35     print(f\"{seq['generated_text']}\"\
    )\n...\nValueError: past key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,\
    \ head_dim`), got torch.Size([1, 8, 8312, 128])\n```"
  created_at: 2023-12-04 10:48:56+00:00
  edited: true
  hidden: false
  id: 656dae98ede71189ad722082
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-08T12:25:37.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8383914232254028
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Yes, I have the same error for context above the sliding window
          length of 16k.</p>

          <p>The same error is there on Mistral models too when exceeding the sliding
          window length.</p>

          '
        raw: 'Yes, I have the same error for context above the sliding window length
          of 16k.


          The same error is there on Mistral models too when exceeding the sliding
          window length.'
        updatedAt: '2023-12-08T12:25:37.099Z'
      numEdits: 0
      reactions: []
    id: 65730b41a0dbf64030449102
    type: comment
  author: RonanMcGovern
  content: 'Yes, I have the same error for context above the sliding window length
    of 16k.


    The same error is there on Mistral models too when exceeding the sliding window
    length.'
  created_at: 2023-12-08 12:25:37+00:00
  edited: false
  hidden: false
  id: 65730b41a0dbf64030449102
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
      fullname: Yin Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yinsong1986
      type: user
    createdAt: '2023-12-13T06:12:33.000Z'
    data:
      edited: false
      editors:
      - yinsong1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6868242025375366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
          fullname: Yin Song
          isHf: false
          isPro: false
          name: yinsong1986
          type: user
        html: "<p>hi <span data-props=\"{&quot;user&quot;:&quot;nicklikets&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nicklikets\"\
          >@<span class=\"underline\">nicklikets</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;RonanMcGovern&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/RonanMcGovern\"\
          >@<span class=\"underline\">RonanMcGovern</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Thank you for the feedback! If possible, can you provide the\
          \ example long context file to me so I can reproduce the error?</p>\n<p>Cheers!</p>\n"
        raw: "hi @nicklikets and @RonanMcGovern \n\nThank you for the feedback! If\
          \ possible, can you provide the example long context file to me so I can\
          \ reproduce the error?\n\nCheers!"
        updatedAt: '2023-12-13T06:12:33.440Z'
      numEdits: 0
      reactions: []
    id: 65794b51dd2996f01a23093b
    type: comment
  author: yinsong1986
  content: "hi @nicklikets and @RonanMcGovern \n\nThank you for the feedback! If possible,\
    \ can you provide the example long context file to me so I can reproduce the error?\n\
    \nCheers!"
  created_at: 2023-12-13 06:12:33+00:00
  edited: false
  hidden: false
  id: 65794b51dd2996f01a23093b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-13T11:24:36.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5070189237594604
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>The text file is <a rel=\"nofollow\" href=\"https://github.com/TrelisResearch/install-guides/blob/main/berkshire23.txt\"\
          >here</a>.</p>\n<p>And the code to test is below.</p>\n<p>FWIW:</p>\n<ul>\n\
          <li>The same issue happens on Mistral v0.1 when running a context longer\
          \ than 4k input tokens (the sliding window size for the model).</li>\n<li>I\
          \ notice for Mistral v0.2 that the sliding window length is set to null\
          \ (?)</li>\n</ul>\n<pre><code># Installing required packages\n!pip install\
          \ -U -q transformers\n!pip install -q -U accelerate\n!pip install -U flash-attn\
          \ -q\n!pip install scipy -q\n\n# Importing libraries\nimport transformers\n\
          import torch\nimport json\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ AutoConfig\nfrom IPython.display import display, HTML\nimport gc\n\n#\
          \ Make sure to define these variables before using them\nmodel_name_A =\
          \ \"amazon/MistralLite\"  # Replace with your actual model name\ncache_dir\
          \ = \"\"  # Replace with your cache directory\n\n# Model loading\nmodel_A\
          \ = AutoModelForCausalLM.from_pretrained(\n    model_name_A,\n    device_map='auto',\n\
          \    torch_dtype=torch.bfloat16,\n    use_flash_attention_2=True,  # works\
          \ with Llama models and reduces memory reqs\n    cache_dir=cache_dir)\n\n\
          tokenizer_A = AutoTokenizer.from_pretrained(model_name_A, cache_dir=cache_dir,\
          \ use_fast=True)\n\n# Function to generate text\ndef generate(model, tokenizer,\
          \ user_prompt, system_prompt):\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\
          \n    B_SYS, E_SYS = \"&lt;&lt;SYS&gt;&gt;\", \"&lt;&lt;/SYS&gt;&gt;\"\n\
          \n    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()}\
          \ {E_INST}\\n\\n\"\n\n    inputs = tokenizer([prompt], return_tensors=\"\
          pt\").to('cuda')\n    shape = inputs.input_ids.shape\n    print(f\"Length\
          \ of input is {shape[1]}\")\n    result = model.generate(**inputs, max_new_tokens=750,\
          \ pad_token_id=tokenizer.eos_token_id, do_sample=False)\n\n    result_str\
          \ = tokenizer.decode(result[0], skip_special_tokens=False)\n    \n    torch.cuda.empty_cache()\n\
          \    gc.collect()\n    \n    return result_str\n\n# Reading from a text\
          \ file\ntext_file = 'berkshire23.txt'\nlen_limit = int(16000*1*4)  # Set\
          \ the limit in characters\n\ntry:\n    with open(text_file, 'r') as file:\n\
          \        text = file.read()[:len_limit]\nexcept FileNotFoundError:\n   \
          \ print(f\"File {text_file} not found.\")\n\n# Test with Model A\nsystem_prompt\
          \ = \"\"  # Define your system prompt, or leave blank\nuser_prompt = f'Respond\
          \ only with a brief summary of the below text.\\n\\n{text}\\n\\nRespond\
          \ only with a brief summary of the above text.'\n\nresult = generate(model_A,\
          \ tokenizer_A, user_prompt, system_prompt)\ndisplay(HTML(f\"&lt;b&gt;{model_name_A}:&lt;/b&gt;&lt;br&gt;\"\
          ))\nprint(result)\n</code></pre>\n"
        raw: "The text file is [here](https://github.com/TrelisResearch/install-guides/blob/main/berkshire23.txt).\n\
          \nAnd the code to test is below.\n\nFWIW:\n- The same issue happens on Mistral\
          \ v0.1 when running a context longer than 4k input tokens (the sliding window\
          \ size for the model).\n- I notice for Mistral v0.2 that the sliding window\
          \ length is set to null (?)\n\n```\n# Installing required packages\n!pip\
          \ install -U -q transformers\n!pip install -q -U accelerate\n!pip install\
          \ -U flash-attn -q\n!pip install scipy -q\n\n# Importing libraries\nimport\
          \ transformers\nimport torch\nimport json\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, AutoConfig\nfrom IPython.display import display,\
          \ HTML\nimport gc\n\n# Make sure to define these variables before using\
          \ them\nmodel_name_A = \"amazon/MistralLite\"  # Replace with your actual\
          \ model name\ncache_dir = \"\"  # Replace with your cache directory\n\n\
          # Model loading\nmodel_A = AutoModelForCausalLM.from_pretrained(\n    model_name_A,\n\
          \    device_map='auto',\n    torch_dtype=torch.bfloat16,\n    use_flash_attention_2=True,\
          \  # works with Llama models and reduces memory reqs\n    cache_dir=cache_dir)\n\
          \ntokenizer_A = AutoTokenizer.from_pretrained(model_name_A, cache_dir=cache_dir,\
          \ use_fast=True)\n\n# Function to generate text\ndef generate(model, tokenizer,\
          \ user_prompt, system_prompt):\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\
          \n    B_SYS, E_SYS = \"<<SYS>>\", \"<</SYS>>\"\n\n    prompt = f\"{B_INST}\
          \ {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\\
          n\\n\"\n\n    inputs = tokenizer([prompt], return_tensors=\"pt\").to('cuda')\n\
          \    shape = inputs.input_ids.shape\n    print(f\"Length of input is {shape[1]}\"\
          )\n    result = model.generate(**inputs, max_new_tokens=750, pad_token_id=tokenizer.eos_token_id,\
          \ do_sample=False)\n\n    result_str = tokenizer.decode(result[0], skip_special_tokens=False)\n\
          \    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return\
          \ result_str\n\n# Reading from a text file\ntext_file = 'berkshire23.txt'\n\
          len_limit = int(16000*1*4)  # Set the limit in characters\n\ntry:\n    with\
          \ open(text_file, 'r') as file:\n        text = file.read()[:len_limit]\n\
          except FileNotFoundError:\n    print(f\"File {text_file} not found.\")\n\
          \n# Test with Model A\nsystem_prompt = \"\"  # Define your system prompt,\
          \ or leave blank\nuser_prompt = f'Respond only with a brief summary of the\
          \ below text.\\n\\n{text}\\n\\nRespond only with a brief summary of the\
          \ above text.'\n\nresult = generate(model_A, tokenizer_A, user_prompt, system_prompt)\n\
          display(HTML(f\"<b>{model_name_A}:</b><br>\"))\nprint(result)\n```\n"
        updatedAt: '2023-12-13T11:24:36.568Z'
      numEdits: 0
      reactions: []
    id: 6579947447ae87ca727004d8
    type: comment
  author: RonanMcGovern
  content: "The text file is [here](https://github.com/TrelisResearch/install-guides/blob/main/berkshire23.txt).\n\
    \nAnd the code to test is below.\n\nFWIW:\n- The same issue happens on Mistral\
    \ v0.1 when running a context longer than 4k input tokens (the sliding window\
    \ size for the model).\n- I notice for Mistral v0.2 that the sliding window length\
    \ is set to null (?)\n\n```\n# Installing required packages\n!pip install -U -q\
    \ transformers\n!pip install -q -U accelerate\n!pip install -U flash-attn -q\n\
    !pip install scipy -q\n\n# Importing libraries\nimport transformers\nimport torch\n\
    import json\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n\
    from IPython.display import display, HTML\nimport gc\n\n# Make sure to define\
    \ these variables before using them\nmodel_name_A = \"amazon/MistralLite\"  #\
    \ Replace with your actual model name\ncache_dir = \"\"  # Replace with your cache\
    \ directory\n\n# Model loading\nmodel_A = AutoModelForCausalLM.from_pretrained(\n\
    \    model_name_A,\n    device_map='auto',\n    torch_dtype=torch.bfloat16,\n\
    \    use_flash_attention_2=True,  # works with Llama models and reduces memory\
    \ reqs\n    cache_dir=cache_dir)\n\ntokenizer_A = AutoTokenizer.from_pretrained(model_name_A,\
    \ cache_dir=cache_dir, use_fast=True)\n\n# Function to generate text\ndef generate(model,\
    \ tokenizer, user_prompt, system_prompt):\n    B_INST, E_INST = \"[INST]\", \"\
    [/INST]\"\n    B_SYS, E_SYS = \"<<SYS>>\", \"<</SYS>>\"\n\n    prompt = f\"{B_INST}\
    \ {B_SYS}{system_prompt.strip()}{E_SYS}{user_prompt.strip()} {E_INST}\\n\\n\"\n\
    \n    inputs = tokenizer([prompt], return_tensors=\"pt\").to('cuda')\n    shape\
    \ = inputs.input_ids.shape\n    print(f\"Length of input is {shape[1]}\")\n  \
    \  result = model.generate(**inputs, max_new_tokens=750, pad_token_id=tokenizer.eos_token_id,\
    \ do_sample=False)\n\n    result_str = tokenizer.decode(result[0], skip_special_tokens=False)\n\
    \    \n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    return result_str\n\
    \n# Reading from a text file\ntext_file = 'berkshire23.txt'\nlen_limit = int(16000*1*4)\
    \  # Set the limit in characters\n\ntry:\n    with open(text_file, 'r') as file:\n\
    \        text = file.read()[:len_limit]\nexcept FileNotFoundError:\n    print(f\"\
    File {text_file} not found.\")\n\n# Test with Model A\nsystem_prompt = \"\"  #\
    \ Define your system prompt, or leave blank\nuser_prompt = f'Respond only with\
    \ a brief summary of the below text.\\n\\n{text}\\n\\nRespond only with a brief\
    \ summary of the above text.'\n\nresult = generate(model_A, tokenizer_A, user_prompt,\
    \ system_prompt)\ndisplay(HTML(f\"<b>{model_name_A}:</b><br>\"))\nprint(result)\n\
    ```\n"
  created_at: 2023-12-13 11:24:36+00:00
  edited: false
  hidden: false
  id: 6579947447ae87ca727004d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
      fullname: Yin Song
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: yinsong1986
      type: user
    createdAt: '2023-12-20T00:16:28.000Z'
    data:
      edited: false
      editors:
      - yinsong1986
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8476524949073792
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5af9961560bf7b4108640e1c205234a.svg
          fullname: Yin Song
          isHf: false
          isPro: false
          name: yinsong1986
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;krecceg&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/krecceg\">@<span class=\"\
          underline\">krecceg</span></a></span>\n\n\t</span></span> and <span data-props=\"\
          {&quot;user&quot;:&quot;nicklikets&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/nicklikets\">@<span class=\"underline\"\
          >nicklikets</span></a></span>\n\n\t</span></span> </p>\n<p>I have updated\
          \ the sliding_window in the config.json to <code>null</code>, and it should\
          \ fix the issue you mentioned.</p>\n<p>Since <code>MistralLite</code> was\
          \ fine tuned using the prompt like f\"&lt;|prompter|&gt;{instruction}&lt;|assistant|&gt;\"\
          . It is recommended to use prompt like below to get a valid summary as below:</p>\n\
          <pre><code>with open(\"berkshire23.txt\", \"r\") as fin:\n    text = fin.read()[:80000]\n\
          prompt = f\"&lt;|prompter|&gt;{text}\\n\\nRespond only with a brief summary\
          \ of the above text.&lt;/s&gt;&lt;|assistant|&gt;\"\n</code></pre>\n<p>I\
          \ tested, it should give some valid results even token size is &gt;16000.\
          \ Pls give a try and let me know if you still have further issues and thank\
          \ you!</p>\n"
        raw: "Hi @krecceg and @nicklikets \n\nI have updated the sliding_window in\
          \ the config.json to `null`, and it should fix the issue you mentioned.\n\
          \nSince `MistralLite` was fine tuned using the prompt like f\"<|prompter|>{instruction}</s><|assistant|>\"\
          . It is recommended to use prompt like below to get a valid summary as below:\n\
          \n```\nwith open(\"berkshire23.txt\", \"r\") as fin:\n    text = fin.read()[:80000]\n\
          prompt = f\"<|prompter|>{text}\\n\\nRespond only with a brief summary of\
          \ the above text.</s><|assistant|>\"\n```\n\nI tested, it should give some\
          \ valid results even token size is >16000. Pls give a try and let me know\
          \ if you still have further issues and thank you!"
        updatedAt: '2023-12-20T00:16:28.890Z'
      numEdits: 0
      reactions: []
    id: 6582325c7cec0a2080c4a5c8
    type: comment
  author: yinsong1986
  content: "Hi @krecceg and @nicklikets \n\nI have updated the sliding_window in the\
    \ config.json to `null`, and it should fix the issue you mentioned.\n\nSince `MistralLite`\
    \ was fine tuned using the prompt like f\"<|prompter|>{instruction}</s><|assistant|>\"\
    . It is recommended to use prompt like below to get a valid summary as below:\n\
    \n```\nwith open(\"berkshire23.txt\", \"r\") as fin:\n    text = fin.read()[:80000]\n\
    prompt = f\"<|prompter|>{text}\\n\\nRespond only with a brief summary of the above\
    \ text.</s><|assistant|>\"\n```\n\nI tested, it should give some valid results\
    \ even token size is >16000. Pls give a try and let me know if you still have\
    \ further issues and thank you!"
  created_at: 2023-12-20 00:16:28+00:00
  edited: false
  hidden: false
  id: 6582325c7cec0a2080c4a5c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-20T10:04:09.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.944246768951416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Thanks. Yeah that makes sense. Using null would have broken transformers
          but with the latest version that should work. And thanks for clarifying
          the prompt.</p>

          <p>As an aside, probably you''re aware that Mistral v0.2 now covers 32k
          context.</p>

          '
        raw: 'Thanks. Yeah that makes sense. Using null would have broken transformers
          but with the latest version that should work. And thanks for clarifying
          the prompt.


          As an aside, probably you''re aware that Mistral v0.2 now covers 32k context.'
        updatedAt: '2023-12-20T10:04:09.693Z'
      numEdits: 0
      reactions: []
    id: 6582bc1905c177eea3b8ac53
    type: comment
  author: RonanMcGovern
  content: 'Thanks. Yeah that makes sense. Using null would have broken transformers
    but with the latest version that should work. And thanks for clarifying the prompt.


    As an aside, probably you''re aware that Mistral v0.2 now covers 32k context.'
  created_at: 2023-12-20 10:04:09+00:00
  edited: false
  hidden: false
  id: 6582bc1905c177eea3b8ac53
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: amazon/MistralLite
repo_type: model
status: open
target_branch: null
title: Output breaks above 16k context length.
