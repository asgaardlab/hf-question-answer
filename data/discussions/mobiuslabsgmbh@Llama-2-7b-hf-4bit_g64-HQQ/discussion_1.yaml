!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sudhir2016
conflicting_files: null
created_at: 2023-12-31 12:14:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c3b817a7b558e0667652ead31747967.svg
      fullname: Sudhir Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sudhir2016
      type: user
    createdAt: '2023-12-31T12:14:05.000Z'
    data:
      edited: true
      editors:
      - sudhir2016
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45247820019721985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c3b817a7b558e0667652ead31747967.svg
          fullname: Sudhir Gupta
          isHf: false
          isPro: false
          name: sudhir2016
          type: user
        html: '<p>Load model<br>model_id = ''mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ''</p>

          <p>from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer<br>tokenizer
          = AutoTokenizer.from_pretrained(model_id)<br>model     = HQQModelForCausalLM.from_quantized(model_id)</p>

          <p>Generate<br>prompt = "Capital of India"<br>inputs = tokenizer(prompt,
          return_tensors="pt")<br>generate_ids = model.generate(inputs.input_ids,
          max_length=30)<br>tokenizer.batch_decode(generate_ids, skip_special_tokens=True,
          clean_up_tokenization_spaces=False)[0]</p>

          <p>This is the error.<br>RuntimeError: Expected all tensors to be on the
          same device, but found at least two devices, cuda:0 and cpu! (when checking
          argument for argument index in method wrapper_CUDA__index_select)</p>

          '
        raw: 'Load model

          model_id = ''mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ''


          from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          model     = HQQModelForCausalLM.from_quantized(model_id)


          Generate

          prompt = "Capital of India"

          inputs = tokenizer(prompt, return_tensors="pt")

          generate_ids = model.generate(inputs.input_ids, max_length=30)

          tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


          This is the error.

          RuntimeError: Expected all tensors to be on the same device, but found at
          least two devices, cuda:0 and cpu! (when checking argument for argument
          index in method wrapper_CUDA__index_select)

          '
        updatedAt: '2023-12-31T12:15:36.360Z'
      numEdits: 1
      reactions: []
    id: 65915b0d57a556fbe1d2271d
    type: comment
  author: sudhir2016
  content: 'Load model

    model_id = ''mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ''


    from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model     = HQQModelForCausalLM.from_quantized(model_id)


    Generate

    prompt = "Capital of India"

    inputs = tokenizer(prompt, return_tensors="pt")

    generate_ids = model.generate(inputs.input_ids, max_length=30)

    tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]


    This is the error.

    RuntimeError: Expected all tensors to be on the same device, but found at least
    two devices, cuda:0 and cpu! (when checking argument for argument index in method
    wrapper_CUDA__index_select)

    '
  created_at: 2023-12-31 12:14:05+00:00
  edited: true
  hidden: false
  id: 65915b0d57a556fbe1d2271d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/9c3b817a7b558e0667652ead31747967.svg
      fullname: Sudhir Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sudhir2016
      type: user
    createdAt: '2023-12-31T12:14:30.000Z'
    data:
      from: Error in using this model for inference on in Google Colab
      to: Error in using this model for inference  in Google Colab
    id: 65915b2643971eed453106e0
    type: title-change
  author: sudhir2016
  created_at: 2023-12-31 12:14:30+00:00
  id: 65915b2643971eed453106e0
  new_title: Error in using this model for inference  in Google Colab
  old_title: Error in using this model for inference on in Google Colab
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-31T12:26:27.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5572481751441956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: '<p>You forgot to put the tokenized input on the gpu</p>

          <pre><code class="language-python">model_id = <span class="hljs-string">''mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ''</span>


          <span class="hljs-keyword">from</span> hqq.engine.hf <span class="hljs-keyword">import</span>
          HQQModelForCausalLM, AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          model = HQQModelForCausalLM.from_quantized(model_id)


          prompt = <span class="hljs-string">"Capital of India"</span>

          inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">"pt"</span>).to(<span
          class="hljs-string">''cuda''</span>)

          generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)[<span
          class="hljs-number">0</span>]

          <span class="hljs-built_in">print</span>(tokenizer.decode(generate_ids))

          </code></pre>

          <p>Output:</p>

          <pre><code>&lt;s&gt; Capital of India, Delhi is a city of contrasts. surely,
          the city is a blend of the old and the new. The

          </code></pre>

          '
        raw: 'You forgot to put the tokenized input on the gpu

          ``` python

          model_id = ''mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ''


          from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(model_id)

          model = HQQModelForCausalLM.from_quantized(model_id)


          prompt = "Capital of India"

          inputs = tokenizer(prompt, return_tensors="pt").to(''cuda'')

          generate_ids = model.generate(inputs.input_ids, max_length=30)[0]

          print(tokenizer.decode(generate_ids))

          ```

          Output:

          ```

          <s> Capital of India, Delhi is a city of contrasts. surely, the city is
          a blend of the old and the new. The

          ```'
        updatedAt: '2023-12-31T12:26:27.660Z'
      numEdits: 0
      reactions: []
    id: 65915df316227c7a2d5147e2
    type: comment
  author: mobicham
  content: 'You forgot to put the tokenized input on the gpu

    ``` python

    model_id = ''mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ''


    from hqq.engine.hf import HQQModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_id)

    model = HQQModelForCausalLM.from_quantized(model_id)


    prompt = "Capital of India"

    inputs = tokenizer(prompt, return_tensors="pt").to(''cuda'')

    generate_ids = model.generate(inputs.input_ids, max_length=30)[0]

    print(tokenizer.decode(generate_ids))

    ```

    Output:

    ```

    <s> Capital of India, Delhi is a city of contrasts. surely, the city is a blend
    of the old and the new. The

    ```'
  created_at: 2023-12-31 12:26:27+00:00
  edited: false
  hidden: false
  id: 65915df316227c7a2d5147e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9c3b817a7b558e0667652ead31747967.svg
      fullname: Sudhir Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sudhir2016
      type: user
    createdAt: '2024-01-01T09:30:48.000Z'
    data:
      edited: false
      editors:
      - sudhir2016
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9405332803726196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9c3b817a7b558e0667652ead31747967.svg
          fullname: Sudhir Gupta
          isHf: false
          isPro: false
          name: sudhir2016
          type: user
        html: '<p>Thank you so much it works now !!</p>

          '
        raw: Thank you so much it works now !!
        updatedAt: '2024-01-01T09:30:48.499Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65928648315340de5f8906f7
    id: 65928648315340de5f8906f4
    type: comment
  author: sudhir2016
  content: Thank you so much it works now !!
  created_at: 2024-01-01 09:30:48+00:00
  edited: false
  hidden: false
  id: 65928648315340de5f8906f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9c3b817a7b558e0667652ead31747967.svg
      fullname: Sudhir Gupta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sudhir2016
      type: user
    createdAt: '2024-01-01T09:30:48.000Z'
    data:
      status: closed
    id: 65928648315340de5f8906f7
    type: status-change
  author: sudhir2016
  created_at: 2024-01-01 09:30:48+00:00
  id: 65928648315340de5f8906f7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mobiuslabsgmbh/Llama-2-7b-hf-4bit_g64-HQQ
repo_type: model
status: closed
target_branch: null
title: Error in using this model for inference  in Google Colab
