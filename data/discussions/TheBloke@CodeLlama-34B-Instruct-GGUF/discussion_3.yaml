!!python/object:huggingface_hub.community.DiscussionWithDetails
author: goodromka
conflicting_files: null
created_at: 2023-09-12 14:30:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: goodromka
      type: user
    createdAt: '2023-09-12T15:30:12.000Z'
    data:
      edited: false
      editors:
      - goodromka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7680670022964478
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2717029158d8df88b3ac0f0b789622f.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: goodromka
          type: user
        html: '<p>I fine-tuned CodeLlama-34B-Instruct and now I want to convert it
          to the codellama-34b-instruct.Q6_K.gguf  format. (HF to GGUF)<br>Could you
          please tell me how I can do this? What script or method can I use to achieve
          this? </p>

          <p>I used to do it earlier using this: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py">https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py</a></p>

          '
        raw: "I fine-tuned CodeLlama-34B-Instruct and now I want to convert it to\
          \ the codellama-34b-instruct.Q6_K.gguf  format. (HF to GGUF)\r\nCould you\
          \ please tell me how I can do this? What script or method can I use to achieve\
          \ this? \r\n\r\nI used to do it earlier using this: https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py"
        updatedAt: '2023-09-12T15:30:12.750Z'
      numEdits: 0
      reactions: []
    id: 65008404990aaba7bb482868
    type: comment
  author: goodromka
  content: "I fine-tuned CodeLlama-34B-Instruct and now I want to convert it to the\
    \ codellama-34b-instruct.Q6_K.gguf  format. (HF to GGUF)\r\nCould you please tell\
    \ me how I can do this? What script or method can I use to achieve this? \r\n\r\
    \nI used to do it earlier using this: https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py"
  created_at: 2023-09-12 14:30:12+00:00
  edited: false
  hidden: false
  id: 65008404990aaba7bb482868
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24e0a925c5398ad4544163e126af0e5c.svg
      fullname: JT Pennington
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: q5sys
      type: user
    createdAt: '2023-09-12T15:35:14.000Z'
    data:
      edited: false
      editors:
      - q5sys
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9716861248016357
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24e0a925c5398ad4544163e126af0e5c.svg
          fullname: JT Pennington
          isHf: false
          isPro: false
          name: q5sys
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;goodromka&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/goodromka\">@<span class=\"\
          underline\">goodromka</span></a></span>\n\n\t</span></span> I believe it's\
          \ in <a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\"\
          >https://github.com/ggerganov/llama.cpp/tree/master/gguf-py</a>  I was poking\
          \ around in the llama.cpp repo last night that that stuck in my mind. I\
          \ didn't look through the script in that dir, so that might not be it. \
          \ But that's somewhere to start.\n </p>\n"
        raw: "@goodromka I believe it's in https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\
          \  I was poking around in the llama.cpp repo last night that that stuck\
          \ in my mind. I didn't look through the script in that dir, so that might\
          \ not be it.  But that's somewhere to start.\n "
        updatedAt: '2023-09-12T15:35:14.568Z'
      numEdits: 0
      reactions: []
    id: 65008532520f8d7b4d0975c2
    type: comment
  author: q5sys
  content: "@goodromka I believe it's in https://github.com/ggerganov/llama.cpp/tree/master/gguf-py\
    \  I was poking around in the llama.cpp repo last night that that stuck in my\
    \ mind. I didn't look through the script in that dir, so that might not be it.\
    \  But that's somewhere to start.\n "
  created_at: 2023-09-12 14:35:14+00:00
  edited: false
  hidden: false
  id: 65008532520f8d7b4d0975c2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-12T16:10:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9285175204277039
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p><code>make-ggml.py</code> should still work fine.  Just edit this
          line first: <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py#L76">https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py#L76</a>
          and change it to:</p>

          <pre><code>outfile = f"{outdir}/{outname}.{type}.gguf"

          </code></pre>

          <p>You could also change line 66 to:</p>

          <pre><code>fp16 = f"{outdir}/{outname}.fp16.gguf"

          </code></pre>

          <p>TBH neither change should actually be needed - you could also just rename
          the quant files afterwards.   I don''t think that llama.cpp''s <code>quantize</code>
          cares what the output file is called, and nor does llama.cpp care what the
          input file is called.  But to avoid confusion, edit the script to give them
          a <code>.gguf</code> extension.</p>

          '
        raw: '`make-ggml.py` should still work fine.  Just edit this line first: https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py#L76
          and change it to:

          ```

          outfile = f"{outdir}/{outname}.{type}.gguf"

          ```


          You could also change line 66 to:

          ```

          fp16 = f"{outdir}/{outname}.fp16.gguf"

          ```


          TBH neither change should actually be needed - you could also just rename
          the quant files afterwards.   I don''t think that llama.cpp''s `quantize`
          cares what the output file is called, and nor does llama.cpp care what the
          input file is called.  But to avoid confusion, edit the script to give them
          a `.gguf` extension.'
        updatedAt: '2023-09-12T16:10:03.488Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - q5sys
        - xss
        - tpiperatgod
    id: 65008d5bb0de1665785c5e94
    type: comment
  author: TheBloke
  content: '`make-ggml.py` should still work fine.  Just edit this line first: https://github.com/ggerganov/llama.cpp/blob/master/examples/make-ggml.py#L76
    and change it to:

    ```

    outfile = f"{outdir}/{outname}.{type}.gguf"

    ```


    You could also change line 66 to:

    ```

    fp16 = f"{outdir}/{outname}.fp16.gguf"

    ```


    TBH neither change should actually be needed - you could also just rename the
    quant files afterwards.   I don''t think that llama.cpp''s `quantize` cares what
    the output file is called, and nor does llama.cpp care what the input file is
    called.  But to avoid confusion, edit the script to give them a `.gguf` extension.'
  created_at: 2023-09-12 15:10:03+00:00
  edited: false
  hidden: false
  id: 65008d5bb0de1665785c5e94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24e0a925c5398ad4544163e126af0e5c.svg
      fullname: JT Pennington
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: q5sys
      type: user
    createdAt: '2023-09-12T18:23:21.000Z'
    data:
      edited: false
      editors:
      - q5sys
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8362678289413452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24e0a925c5398ad4544163e126af0e5c.svg
          fullname: JT Pennington
          isHf: false
          isPro: false
          name: q5sys
          type: user
        html: '<blockquote>

          <blockquote>

          <p>make-ggml.py should still work fine. Just edit...</p>

          </blockquote>

          </blockquote>

          <p>Thanks for the correction. TIL.</p>

          '
        raw: '>> make-ggml.py should still work fine. Just edit...


          Thanks for the correction. TIL.

          '
        updatedAt: '2023-09-12T18:23:21.543Z'
      numEdits: 0
      reactions: []
    id: 6500ac99e850c952d44f7e08
    type: comment
  author: q5sys
  content: '>> make-ggml.py should still work fine. Just edit...


    Thanks for the correction. TIL.

    '
  created_at: 2023-09-12 17:23:21+00:00
  edited: false
  hidden: false
  id: 6500ac99e850c952d44f7e08
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/CodeLlama-34B-Instruct-GGUF
repo_type: model
status: open
target_branch: null
title: Converting hf format model codeLlama34B to GGUF
