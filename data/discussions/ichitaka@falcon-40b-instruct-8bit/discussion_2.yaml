!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mahmoudajawad
conflicting_files: null
created_at: 2023-06-19 12:46:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/efc82ec2e17837c0d0528332324a3dfb.svg
      fullname: Mahmoud Abduljawad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahmoudajawad
      type: user
    createdAt: '2023-06-19T13:46:20.000Z'
    data:
      edited: false
      editors:
      - mahmoudajawad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8491157293319702
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/efc82ec2e17837c0d0528332324a3dfb.svg
          fullname: Mahmoud Abduljawad
          isHf: false
          isPro: false
          name: mahmoudajawad
          type: user
        html: '<p>Passing <code>load_in_8bit=True</code> fails. Does this mean the
          model runs in 8-bit mode without the need to pass the argument? What <code>dtype</code>
          should be?</p>

          '
        raw: Passing `load_in_8bit=True` fails. Does this mean the model runs in 8-bit
          mode without the need to pass the argument? What `dtype` should be?
        updatedAt: '2023-06-19T13:46:20.776Z'
      numEdits: 0
      reactions: []
    id: 64905c2c175db2a6ed1a04fb
    type: comment
  author: mahmoudajawad
  content: Passing `load_in_8bit=True` fails. Does this mean the model runs in 8-bit
    mode without the need to pass the argument? What `dtype` should be?
  created_at: 2023-06-19 12:46:20+00:00
  edited: false
  hidden: false
  id: 64905c2c175db2a6ed1a04fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2704aa0f012d94f88c349c532eb403cb.svg
      fullname: T T
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ichitaka
      type: user
    createdAt: '2023-06-19T15:09:30.000Z'
    data:
      edited: false
      editors:
      - ichitaka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9982304573059082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2704aa0f012d94f88c349c532eb403cb.svg
          fullname: T T
          isHf: false
          isPro: false
          name: ichitaka
          type: user
        html: '<p>What is the issue that you are facing?</p>

          '
        raw: What is the issue that you are facing?
        updatedAt: '2023-06-19T15:09:30.696Z'
      numEdits: 0
      reactions: []
    id: 64906faa2549fd68a7623a41
    type: comment
  author: ichitaka
  content: What is the issue that you are facing?
  created_at: 2023-06-19 14:09:30+00:00
  edited: false
  hidden: false
  id: 64906faa2549fd68a7623a41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
      fullname: Viktor Ferenczi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: viktor-ferenczi
      type: user
    createdAt: '2023-06-29T00:01:35.000Z'
    data:
      edited: false
      editors:
      - viktor-ferenczi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8401241898536682
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/xV4Xlk01BsqfRqxAWsO8Z.png?w=200&h=200&f=face
          fullname: Viktor Ferenczi
          isHf: false
          isPro: false
          name: viktor-ferenczi
          type: user
        html: '<p>Could you please provide a script to load this model in plain Python?</p>

          <p>It failed to load into oobabooga/text-generation-webui for CPU inference:<br>RuntimeError:
          No GPU found. A GPU is needed for quantization.</p>

          '
        raw: 'Could you please provide a script to load this model in plain Python?


          It failed to load into oobabooga/text-generation-webui for CPU inference:

          RuntimeError: No GPU found. A GPU is needed for quantization.'
        updatedAt: '2023-06-29T00:01:35.796Z'
      numEdits: 0
      reactions: []
    id: 649cc9df91241fbd22be5a4c
    type: comment
  author: viktor-ferenczi
  content: 'Could you please provide a script to load this model in plain Python?


    It failed to load into oobabooga/text-generation-webui for CPU inference:

    RuntimeError: No GPU found. A GPU is needed for quantization.'
  created_at: 2023-06-28 23:01:35+00:00
  edited: false
  hidden: false
  id: 649cc9df91241fbd22be5a4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2704aa0f012d94f88c349c532eb403cb.svg
      fullname: T T
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ichitaka
      type: user
    createdAt: '2023-06-29T15:00:33.000Z'
    data:
      edited: false
      editors:
      - ichitaka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.832818865776062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2704aa0f012d94f88c349c532eb403cb.svg
          fullname: T T
          isHf: false
          isPro: false
          name: ichitaka
          type: user
        html: '<p>bitsandbytes 8-Bit Quantization requires a GPU that can hold the
          whole model, it is not compatible with CPU inference.</p>

          '
        raw: bitsandbytes 8-Bit Quantization requires a GPU that can hold the whole
          model, it is not compatible with CPU inference.
        updatedAt: '2023-06-29T15:00:33.024Z'
      numEdits: 0
      reactions: []
    id: 649d9c91dd8bb1e7f36d8463
    type: comment
  author: ichitaka
  content: bitsandbytes 8-Bit Quantization requires a GPU that can hold the whole
    model, it is not compatible with CPU inference.
  created_at: 2023-06-29 14:00:33+00:00
  edited: false
  hidden: false
  id: 649d9c91dd8bb1e7f36d8463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652562749455-noauth.png?w=200&h=200&f=face
      fullname: Talha Yousuf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaffalPoosh
      type: user
    createdAt: '2023-07-04T10:48:00.000Z'
    data:
      edited: false
      editors:
      - SaffalPoosh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8585919737815857
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1652562749455-noauth.png?w=200&h=200&f=face
          fullname: Talha Yousuf
          isHf: false
          isPro: false
          name: SaffalPoosh
          type: user
        html: '<p>the script you provided in README is loading model in full precision,
          do we need to pass load_in_8bit ? As of now, it downloads full checkpoints
          when loaded using <code>.from_pretrained ( .... )</code></p>

          '
        raw: the script you provided in README is loading model in full precision,
          do we need to pass load_in_8bit ? As of now, it downloads full checkpoints
          when loaded using `.from_pretrained ( .... )`
        updatedAt: '2023-07-04T10:48:00.741Z'
      numEdits: 0
      reactions: []
    id: 64a3f8e00a4ffdd34e08a3d3
    type: comment
  author: SaffalPoosh
  content: the script you provided in README is loading model in full precision, do
    we need to pass load_in_8bit ? As of now, it downloads full checkpoints when loaded
    using `.from_pretrained ( .... )`
  created_at: 2023-07-04 09:48:00+00:00
  edited: false
  hidden: false
  id: 64a3f8e00a4ffdd34e08a3d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2704aa0f012d94f88c349c532eb403cb.svg
      fullname: T T
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ichitaka
      type: user
    createdAt: '2023-07-05T08:08:51.000Z'
    data:
      edited: false
      editors:
      - ichitaka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8495362997055054
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2704aa0f012d94f88c349c532eb403cb.svg
          fullname: T T
          isHf: false
          isPro: false
          name: ichitaka
          type: user
        html: '<p>The weight files are only 40+ GB instead of 90+ GB for the full
          precision mode. I usually pass load_in_8bit when loading as well but it
          should be impossible to load the original weights using this repo.</p>

          '
        raw: The weight files are only 40+ GB instead of 90+ GB for the full precision
          mode. I usually pass load_in_8bit when loading as well but it should be
          impossible to load the original weights using this repo.
        updatedAt: '2023-07-05T08:08:51.053Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - SaffalPoosh
    id: 64a52513ba8c29ea3b56bdd9
    type: comment
  author: ichitaka
  content: The weight files are only 40+ GB instead of 90+ GB for the full precision
    mode. I usually pass load_in_8bit when loading as well but it should be impossible
    to load the original weights using this repo.
  created_at: 2023-07-05 07:08:51+00:00
  edited: false
  hidden: false
  id: 64a52513ba8c29ea3b56bdd9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ichitaka/falcon-40b-instruct-8bit
repo_type: model
status: open
target_branch: null
title: Is the snippet in README loading the model in 8-bit mode?
