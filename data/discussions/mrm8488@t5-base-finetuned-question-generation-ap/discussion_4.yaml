!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lovodkin93
conflicting_files: null
created_at: 2023-03-27 18:04:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/242e344dca08057bdf1eef09f69b41b2.svg
      fullname: Aviv Slobodkin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lovodkin93
      type: user
    createdAt: '2023-03-27T19:04:05.000Z'
    data:
      edited: false
      editors:
      - lovodkin93
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/242e344dca08057bdf1eef09f69b41b2.svg
          fullname: Aviv Slobodkin
          isHf: false
          isPro: false
          name: lovodkin93
          type: user
        html: '<p>Hello,<br>I would like to run your model from a linux server, but
          I keep getting errors related to the torch-xla.<br>At first the errors stated
          that I need to separately download torch-xla, which I did.<br>Now, the error
          says:</p>

          <pre><code>ImportError: /lib64/libm.so.6: version `GLIBC_2.23'' not found
          (required by /home/nlp/sloboda1/anaconda3/envs/hug_original/lib/python3.7/site-packages/torch_xla/lib/libxla_computation_client.so)

          </code></pre>

          <p>After searching online, it appears the problem can be solved if I were
          able to run a docker, but unfortunately I cannot in my server.</p>

          <p>So, my question is, is there a way to disable the need for torch-xla,
          and run it using torch?</p>

          '
        raw: "Hello,\r\nI would like to run your model from a linux server, but I\
          \ keep getting errors related to the torch-xla.\r\nAt first the errors stated\
          \ that I need to separately download torch-xla, which I did.\r\nNow, the\
          \ error says:\r\n```\r\nImportError: /lib64/libm.so.6: version `GLIBC_2.23'\
          \ not found (required by /home/nlp/sloboda1/anaconda3/envs/hug_original/lib/python3.7/site-packages/torch_xla/lib/libxla_computation_client.so)\r\
          \n```\r\n\r\nAfter searching online, it appears the problem can be solved\
          \ if I were able to run a docker, but unfortunately I cannot in my server.\r\
          \n\r\nSo, my question is, is there a way to disable the need for torch-xla,\
          \ and run it using torch?"
        updatedAt: '2023-03-27T19:04:05.784Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - isiG
    id: 6421e8a5eaad1bcb28b1f308
    type: comment
  author: lovodkin93
  content: "Hello,\r\nI would like to run your model from a linux server, but I keep\
    \ getting errors related to the torch-xla.\r\nAt first the errors stated that\
    \ I need to separately download torch-xla, which I did.\r\nNow, the error says:\r\
    \n```\r\nImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required\
    \ by /home/nlp/sloboda1/anaconda3/envs/hug_original/lib/python3.7/site-packages/torch_xla/lib/libxla_computation_client.so)\r\
    \n```\r\n\r\nAfter searching online, it appears the problem can be solved if I\
    \ were able to run a docker, but unfortunately I cannot in my server.\r\n\r\n\
    So, my question is, is there a way to disable the need for torch-xla, and run\
    \ it using torch?"
  created_at: 2023-03-27 18:04:05+00:00
  edited: false
  hidden: false
  id: 6421e8a5eaad1bcb28b1f308
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: mrm8488/t5-base-finetuned-question-generation-ap
repo_type: model
status: open
target_branch: null
title: use model without the need for torch-xla
