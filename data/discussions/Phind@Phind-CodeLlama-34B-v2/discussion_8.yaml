!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wrtn2
conflicting_files: null
created_at: 2023-09-04 13:00:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c24bc7a868dd9526a0e62e5e92cc9b4f.svg
      fullname: wrtn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wrtn2
      type: user
    createdAt: '2023-09-04T14:00:43.000Z'
    data:
      edited: false
      editors:
      - wrtn2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9244729280471802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c24bc7a868dd9526a0e62e5e92cc9b4f.svg
          fullname: wrtn
          isHf: false
          isPro: false
          name: wrtn2
          type: user
        html: '<p>This is excellent, really impressive! Are there any plans to release
          smaller model sizes? For example 3B or 7B trained against the same data
          set could be useful for speculative decoding optimization. TIA</p>

          '
        raw: This is excellent, really impressive! Are there any plans to release
          smaller model sizes? For example 3B or 7B trained against the same data
          set could be useful for speculative decoding optimization. TIA
        updatedAt: '2023-09-04T14:00:43.242Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - gnomealone
        - mbuet2ner
        - eshack94
        - TNTOutburst
        - djedu28
        - Mandarin
        - bakezq
        - gabrielgrant
        - vmirea
    id: 64f5e30bdf2a7ec1cdd517cc
    type: comment
  author: wrtn2
  content: This is excellent, really impressive! Are there any plans to release smaller
    model sizes? For example 3B or 7B trained against the same data set could be useful
    for speculative decoding optimization. TIA
  created_at: 2023-09-04 13:00:43+00:00
  edited: false
  hidden: false
  id: 64f5e30bdf2a7ec1cdd517cc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: Phind/Phind-CodeLlama-34B-v2
repo_type: model
status: open
target_branch: null
title: Smaller model sizes to allow for speculative decoding
