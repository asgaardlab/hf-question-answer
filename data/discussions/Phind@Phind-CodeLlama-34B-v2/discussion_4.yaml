!!python/object:huggingface_hub.community.DiscussionWithDetails
author: emrgnt-cmplxty
conflicting_files: null
created_at: 2023-08-29 02:28:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-08-29T03:28:54.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5448321104049683
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: "<p>Hi all,</p>\n<p>I'm looking to replicate the HumanEval result for\
          \ this model so that I can then go on to testing on interesting orthogonal\
          \ benchmarks.</p>\n<p>Unfortunately, I find that the model goes off the\
          \ rails frequently, and is likely far from Phind's quoted performance when\
          \ i attempt to replicate. Does anyone see an obvious bug here - <a rel=\"\
          nofollow\" href=\"https://github.com/emrgnt-cmplxty/zero-shot-replication/blob/main/zero_shot_replication/model/hugging_face_model/phind_model.py\"\
          >https://github.com/emrgnt-cmplxty/zero-shot-replication/blob/main/zero_shot_replication/model/hugging_face_model/phind_model.py</a>?</p>\n\
          <p>For reference, I am seeing output like that shown:</p>\n<pre><code class=\"\
          language-python\">\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">is_multiply_prime</span>(<span class=\"hljs-params\"\
          >a</span>):\n    <span class=\"hljs-string\">\"\"\"Write a function that\
          \ returns true if the given number is the multiplication of 3 prime numbers</span>\n\
          <span class=\"hljs-string\">    and false otherwise.</span>\n<span class=\"\
          hljs-string\">    Knowing that (a) is less then 100. </span>\n<span class=\"\
          hljs-string\">    Example:</span>\n<span class=\"hljs-string\">    is_multiply_prime(30)\
          \ == True</span>\n<span class=\"hljs-string\">    30 = 2 * 3 * 5</span>\n\
          <span class=\"hljs-string\">    \"\"\"</span>\n\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">is_prime</span>(<span class=\"\
          hljs-params\">n</span>))):\n        <span class=\"hljs-keyword\">if</span>\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n\n</code></pre>\n"
        raw: "Hi all,\r\n\r\nI'm looking to replicate the HumanEval result for this\
          \ model so that I can then go on to testing on interesting orthogonal benchmarks.\r\
          \n\r\nUnfortunately, I find that the model goes off the rails frequently,\
          \ and is likely far from Phind's quoted performance when i attempt to replicate.\
          \ Does anyone see an obvious bug here - https://github.com/emrgnt-cmplxty/zero-shot-replication/blob/main/zero_shot_replication/model/hugging_face_model/phind_model.py?\r\
          \n\r\n\r\nFor reference, I am seeing output like that shown:\r\n\r\n```python\r\
          \n\r\ndef is_multiply_prime(a):\r\n    \"\"\"Write a function that returns\
          \ true if the given number is the multiplication of 3 prime numbers\r\n\
          \    and false otherwise.\r\n    Knowing that (a) is less then 100. \r\n\
          \    Example:\r\n    is_multiply_prime(30) == True\r\n    30 = 2 * 3 * 5\r\
          \n    \"\"\"\r\n\r\n    def is_prime(n))):\r\n        if n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
          \ n n n n n n n n n n n n n n\r\n```"
        updatedAt: '2023-08-29T03:28:54.851Z'
      numEdits: 0
      reactions: []
    id: 64ed65f660f6345da700eada
    type: comment
  author: emrgnt-cmplxty
  content: "Hi all,\r\n\r\nI'm looking to replicate the HumanEval result for this\
    \ model so that I can then go on to testing on interesting orthogonal benchmarks.\r\
    \n\r\nUnfortunately, I find that the model goes off the rails frequently, and\
    \ is likely far from Phind's quoted performance when i attempt to replicate. Does\
    \ anyone see an obvious bug here - https://github.com/emrgnt-cmplxty/zero-shot-replication/blob/main/zero_shot_replication/model/hugging_face_model/phind_model.py?\r\
    \n\r\n\r\nFor reference, I am seeing output like that shown:\r\n\r\n```python\r\
    \n\r\ndef is_multiply_prime(a):\r\n    \"\"\"Write a function that returns true\
    \ if the given number is the multiplication of 3 prime numbers\r\n    and false\
    \ otherwise.\r\n    Knowing that (a) is less then 100. \r\n    Example:\r\n  \
    \  is_multiply_prime(30) == True\r\n    30 = 2 * 3 * 5\r\n    \"\"\"\r\n\r\n \
    \   def is_prime(n))):\r\n        if n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n n\
    \ n n n n n n n n n\r\n```"
  created_at: 2023-08-29 02:28:54+00:00
  edited: false
  hidden: false
  id: 64ed65f660f6345da700eada
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
      fullname: Bohan Du
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: acrastt
      type: user
    createdAt: '2023-08-29T03:37:42.000Z'
    data:
      edited: false
      editors:
      - acrastt
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9222445487976074
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4d38fcaa72d9f9cb04ba8e7f72211e34.svg
          fullname: Bohan Du
          isHf: false
          isPro: false
          name: acrastt
          type: user
        html: '<p>This model have the Theta of 1000000. Is there any way to implement
          that in the script?</p>

          '
        raw: This model have the Theta of 1000000. Is there any way to implement that
          in the script?
        updatedAt: '2023-08-29T03:37:42.223Z'
      numEdits: 0
      reactions: []
    id: 64ed68065554408940182212
    type: comment
  author: acrastt
  content: This model have the Theta of 1000000. Is there any way to implement that
    in the script?
  created_at: 2023-08-29 02:37:42+00:00
  edited: false
  hidden: false
  id: 64ed68065554408940182212
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624223414317-noauth.jpeg?w=200&h=200&f=face
      fullname: Michael Royzen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: michaelroyzen
      type: user
    createdAt: '2023-08-29T04:04:33.000Z'
    data:
      edited: false
      editors:
      - michaelroyzen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9582287073135376
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624223414317-noauth.jpeg?w=200&h=200&f=face
          fullname: Michael Royzen
          isHf: false
          isPro: true
          name: michaelroyzen
          type: user
        html: '<p>Thanks for reporting, we''ll investigate</p>

          '
        raw: Thanks for reporting, we'll investigate
        updatedAt: '2023-08-29T04:04:33.212Z'
      numEdits: 0
      reactions: []
    id: 64ed6e51f67d7dd49f8769da
    type: comment
  author: michaelroyzen
  content: Thanks for reporting, we'll investigate
  created_at: 2023-08-29 03:04:33+00:00
  edited: false
  hidden: false
  id: 64ed6e51f67d7dd49f8769da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624223414317-noauth.jpeg?w=200&h=200&f=face
      fullname: Michael Royzen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: michaelroyzen
      type: user
    createdAt: '2023-08-29T04:23:17.000Z'
    data:
      edited: false
      editors:
      - michaelroyzen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9874492287635803
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624223414317-noauth.jpeg?w=200&h=200&f=face
          fullname: Michael Royzen
          isHf: false
          isPro: true
          name: michaelroyzen
          type: user
        html: '<p>The eval code in the model card just worked for me. Could you please
          let me know if that works for you?</p>

          '
        raw: The eval code in the model card just worked for me. Could you please
          let me know if that works for you?
        updatedAt: '2023-08-29T04:23:17.251Z'
      numEdits: 0
      reactions: []
    id: 64ed72b5a489318e667f321b
    type: comment
  author: michaelroyzen
  content: The eval code in the model card just worked for me. Could you please let
    me know if that works for you?
  created_at: 2023-08-29 03:23:17+00:00
  edited: false
  hidden: false
  id: 64ed72b5a489318e667f321b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-08-29T04:42:53.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9686557054519653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: '<p>I will test explicitly tomorrow, I don''t think there are any significant
          diffs w.r.t what I am doing, but this can help pinpoint.</p>

          '
        raw: I will test explicitly tomorrow, I don't think there are any significant
          diffs w.r.t what I am doing, but this can help pinpoint.
        updatedAt: '2023-08-29T04:42:53.654Z'
      numEdits: 0
      reactions: []
    id: 64ed774db9984ff7757b93bb
    type: comment
  author: emrgnt-cmplxty
  content: I will test explicitly tomorrow, I don't think there are any significant
    diffs w.r.t what I am doing, but this can help pinpoint.
  created_at: 2023-08-29 03:42:53+00:00
  edited: false
  hidden: false
  id: 64ed774db9984ff7757b93bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/891e1c2074e4bd651af370d2d4ee1e64.svg
      fullname: jay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: waytohou
      type: user
    createdAt: '2023-08-29T07:42:40.000Z'
    data:
      edited: false
      editors:
      - waytohou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.980049192905426
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/891e1c2074e4bd651af370d2d4ee1e64.svg
          fullname: jay
          isHf: false
          isPro: false
          name: waytohou
          type: user
        html: '<blockquote>

          <p>The eval code in the model card just worked for me. Could you please
          let me know if that works for you?</p>

          </blockquote>

          <p>same here, every outputs end with same words, it seems there is no end_token
          here</p>

          '
        raw: '> The eval code in the model card just worked for me. Could you please
          let me know if that works for you?


          same here, every outputs end with same words, it seems there is no end_token
          here'
        updatedAt: '2023-08-29T07:42:40.241Z'
      numEdits: 0
      reactions: []
    id: 64eda17093b11f432184da01
    type: comment
  author: waytohou
  content: '> The eval code in the model card just worked for me. Could you please
    let me know if that works for you?


    same here, every outputs end with same words, it seems there is no end_token here'
  created_at: 2023-08-29 06:42:40+00:00
  edited: false
  hidden: false
  id: 64eda17093b11f432184da01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-08-29T11:47:41.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8944144248962402
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: '<p>There is some commentary in the reddit thread here -&gt; <a rel="nofollow"
          href="https://www.reddit.com/r/LocalLLaMA/comments/164754t/wizardcoder_eval_results_vs_chatgpt_and_claude_on/">https://www.reddit.com/r/LocalLLaMA/comments/164754t/wizardcoder_eval_results_vs_chatgpt_and_claude_on/</a></p>

          <p>It does seem that the issue is related to transformers version.</p>

          '
        raw: 'There is some commentary in the reddit thread here -> https://www.reddit.com/r/LocalLLaMA/comments/164754t/wizardcoder_eval_results_vs_chatgpt_and_claude_on/


          It does seem that the issue is related to transformers version.'
        updatedAt: '2023-08-29T11:47:41.780Z'
      numEdits: 0
      reactions: []
    id: 64eddaddd44c8a56d15e7562
    type: comment
  author: emrgnt-cmplxty
  content: 'There is some commentary in the reddit thread here -> https://www.reddit.com/r/LocalLLaMA/comments/164754t/wizardcoder_eval_results_vs_chatgpt_and_claude_on/


    It does seem that the issue is related to transformers version.'
  created_at: 2023-08-29 10:47:41+00:00
  edited: false
  hidden: false
  id: 64eddaddd44c8a56d15e7562
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/21276de914739614c2940207de6495e4.svg
      fullname: Ilian P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ilianos
      type: user
    createdAt: '2023-08-29T12:21:54.000Z'
    data:
      edited: false
      editors:
      - Ilianos
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3652684688568115
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/21276de914739614c2940207de6495e4.svg
          fullname: Ilian P
          isHf: false
          isPro: false
          name: Ilianos
          type: user
        html: '<p><a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions/13">https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions/13</a></p>

          '
        raw: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions/13
        updatedAt: '2023-08-29T12:21:54.657Z'
      numEdits: 0
      reactions: []
    id: 64ede2e2420467062a9d1f10
    type: comment
  author: Ilianos
  content: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions/13
  created_at: 2023-08-29 11:21:54+00:00
  edited: false
  hidden: false
  id: 64ede2e2420467062a9d1f10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-08-29T12:23:32.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8986432552337646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: '<p>Can confirm, running off transformers main brach commit worked.</p>

          '
        raw: Can confirm, running off transformers main brach commit worked.
        updatedAt: '2023-08-29T12:23:32.521Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - GaussianMixture
    id: 64ede344cd1f882ed940456a
    type: comment
  author: emrgnt-cmplxty
  content: Can confirm, running off transformers main brach commit worked.
  created_at: 2023-08-29 11:23:32+00:00
  edited: false
  hidden: false
  id: 64ede344cd1f882ed940456a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/decf2cb9ea2c3008aadafae59c392199.svg
      fullname: Satyanarayana Reddy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Satya4093
      type: user
    createdAt: '2023-09-04T05:33:41.000Z'
    data:
      edited: false
      editors:
      - Satya4093
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6201685667037964
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/decf2cb9ea2c3008aadafae59c392199.svg
          fullname: Satyanarayana Reddy
          isHf: false
          isPro: false
          name: Satya4093
          type: user
        html: "<p>I tried this code on single gpu. but getting bad results.</p>\n\
          <pre><code class=\"language-python\">   <span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ LlamaForCausalLM\n   <span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> BitsAndBytesConfig\n   <span\
          \ class=\"hljs-keyword\">import</span> torch\n   <span class=\"hljs-keyword\"\
          >import</span> os \n\n   model_path = <span class=\"hljs-string\">\"Phind/Phind-CodeLlama-34B-v2\"\
          </span>\n   model = LlamaForCausalLM.from_pretrained(model_path, load_in_8bit=<span\
          \ class=\"hljs-literal\">True</span>, device_map=<span class=\"hljs-string\"\
          >\"auto\"</span>)\n   <span class=\"hljs-comment\">#model = LlamaForCausalLM.from_pretrained(model_path,\
          \ quantization_config=nf4_config)</span>\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ legacy=<span class=\"hljs-literal\">True</span>)\n    tokenizer.pad_token_id\
          \ = tokenizer.eos_token_id\n\n   text = <span class=\"hljs-string\">\"Write\
          \ a code in python for Inferecing large language models using Transformers\
          \ library. Give step by step approach.\"</span>\n\n   inputs = tokenizer(text,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(<span class=\"\
          hljs-string\">\"cuda:0\"</span>)\n\n   out = model.generate(**inputs, max_length=<span\
          \ class=\"hljs-number\">200</span>, temperature=<span class=\"hljs-number\"\
          >0.9</span>, repetition_penalty=<span class=\"hljs-number\">1.5</span>,\
          \ do_sample=<span class=\"hljs-literal\">True</span>)\n   <span class=\"\
          hljs-built_in\">print</span>(tokenizer.decode(out[<span class=\"hljs-number\"\
          >0</span>][<span class=\"hljs-built_in\">len</span>(inputs[<span class=\"\
          hljs-string\">'input_ids'</span>][<span class=\"hljs-number\">0</span>]):]))\n\
          </code></pre>\n<p>This is the output i am getting.</p>\n<pre><code class=\"\
          language-output\">In order to inferencing with transformer model, we need\
          \ use the Hugging Face's pytorch-transformers Library.\nStep 1: Installation\
          \ of Libraries\nYou can install this required useful very necessary important\
          \ big huge immense massive monstrous enormous vast colossal portentious\
          \ prodigious sizeable sizable mammoth mind mouth multitudinously numberless\
          \ numb numerous novel nones none non non nonsensical senseless insignificant\
          \ inconsequentialist unimportant small sm\npython\n# Importing Necessary\
          \ nec es ess ent en env e environments  needed environment environments\
          \ environments\nimport torch\nfrom transformers import AutoModelForMaskedLM,AutoTokenizerFastBert\
          \ BertConfigP\nclass Class Config Model Token BERT For\nconfig = class Auto\n\
          </code></pre>\n<p>Can someone suggest?</p>\n"
        raw: "I tried this code on single gpu. but getting bad results.\n\n```python\n\
          \   from transformers import AutoTokenizer, LlamaForCausalLM\n   from transformers\
          \ import BitsAndBytesConfig\n   import torch\n   import os \n\n   model_path\
          \ = \"Phind/Phind-CodeLlama-34B-v2\"\n   model = LlamaForCausalLM.from_pretrained(model_path,\
          \ load_in_8bit=True, device_map=\"auto\")\n   #model = LlamaForCausalLM.from_pretrained(model_path,\
          \ quantization_config=nf4_config)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
          \ legacy=True)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n\
          \   text = \"Write a code in python for Inferecing large language models\
          \ using Transformers library. Give step by step approach.\"\n\n   inputs\
          \ = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")\n\n   out = model.generate(**inputs,\
          \ max_length=200, temperature=0.9, repetition_penalty=1.5, do_sample=True)\n\
          \   print(tokenizer.decode(out[0][len(inputs['input_ids'][0]):]))\n```\n\
          This is the output i am getting.\n```output\nIn order to inferencing with\
          \ transformer model, we need use the Hugging Face's pytorch-transformers\
          \ Library.\nStep 1: Installation of Libraries\nYou can install this required\
          \ useful very necessary important big huge immense massive monstrous enormous\
          \ vast colossal portentious prodigious sizeable sizable mammoth mind mouth\
          \ multitudinously numberless numb numerous novel nones none non non nonsensical\
          \ senseless insignificant inconsequentialist unimportant small sm\npython\n\
          # Importing Necessary nec es ess ent en env e environments  needed environment\
          \ environments environments\nimport torch\nfrom transformers import AutoModelForMaskedLM,AutoTokenizerFastBert\
          \ BertConfigP\nclass Class Config Model Token BERT For\nconfig = class Auto\n\
          ```\n\nCan someone suggest?"
        updatedAt: '2023-09-04T05:33:41.984Z'
      numEdits: 0
      reactions: []
    id: 64f56c350a822d0433cf3c7f
    type: comment
  author: Satya4093
  content: "I tried this code on single gpu. but getting bad results.\n\n```python\n\
    \   from transformers import AutoTokenizer, LlamaForCausalLM\n   from transformers\
    \ import BitsAndBytesConfig\n   import torch\n   import os \n\n   model_path =\
    \ \"Phind/Phind-CodeLlama-34B-v2\"\n   model = LlamaForCausalLM.from_pretrained(model_path,\
    \ load_in_8bit=True, device_map=\"auto\")\n   #model = LlamaForCausalLM.from_pretrained(model_path,\
    \ quantization_config=nf4_config)\n\n    tokenizer = AutoTokenizer.from_pretrained(model_path,\
    \ legacy=True)\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n   text\
    \ = \"Write a code in python for Inferecing large language models using Transformers\
    \ library. Give step by step approach.\"\n\n   inputs = tokenizer(text, return_tensors=\"\
    pt\").to(\"cuda:0\")\n\n   out = model.generate(**inputs, max_length=200, temperature=0.9,\
    \ repetition_penalty=1.5, do_sample=True)\n   print(tokenizer.decode(out[0][len(inputs['input_ids'][0]):]))\n\
    ```\nThis is the output i am getting.\n```output\nIn order to inferencing with\
    \ transformer model, we need use the Hugging Face's pytorch-transformers Library.\n\
    Step 1: Installation of Libraries\nYou can install this required useful very necessary\
    \ important big huge immense massive monstrous enormous vast colossal portentious\
    \ prodigious sizeable sizable mammoth mind mouth multitudinously numberless numb\
    \ numerous novel nones none non non nonsensical senseless insignificant inconsequentialist\
    \ unimportant small sm\npython\n# Importing Necessary nec es ess ent en env e\
    \ environments  needed environment environments environments\nimport torch\nfrom\
    \ transformers import AutoModelForMaskedLM,AutoTokenizerFastBert BertConfigP\n\
    class Class Config Model Token BERT For\nconfig = class Auto\n```\n\nCan someone\
    \ suggest?"
  created_at: 2023-09-04 04:33:41+00:00
  edited: false
  hidden: false
  id: 64f56c350a822d0433cf3c7f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Phind/Phind-CodeLlama-34B-v2
repo_type: model
status: open
target_branch: null
title: Issue/Bug replicating HumanEval result
