!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vmirea
conflicting_files: null
created_at: 2023-10-29 19:28:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
      fullname: Viorel Mirea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmirea
      type: user
    createdAt: '2023-10-29T20:28:02.000Z'
    data:
      edited: false
      editors:
      - vmirea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9355757236480713
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
          fullname: Viorel Mirea
          isHf: false
          isPro: false
          name: vmirea
          type: user
        html: '<p>I am testing the model on a GTX 4090 24G and it''s great. I use
          TheBoke/Phind-CodeLlama-34B-v2-GGUF/phind-codellama-34b-v2.Q5_0.gguf<br>I
          would have loved to fine tune the model with my code but I''m getting OOM
          issues.<br>Because of this I would have loved to have a smaller version
          to try to fine tune using my personal data to see the results.</p>

          '
        raw: "I am testing the model on a GTX 4090 24G and it's great. I use TheBoke/Phind-CodeLlama-34B-v2-GGUF/phind-codellama-34b-v2.Q5_0.gguf\r\
          \nI would have loved to fine tune the model with my code but I'm getting\
          \ OOM issues.\r\nBecause of this I would have loved to have a smaller version\
          \ to try to fine tune using my personal data to see the results.\r\n"
        updatedAt: '2023-10-29T20:28:02.820Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Sleepwave
    id: 653ec052885338b011fa5740
    type: comment
  author: vmirea
  content: "I am testing the model on a GTX 4090 24G and it's great. I use TheBoke/Phind-CodeLlama-34B-v2-GGUF/phind-codellama-34b-v2.Q5_0.gguf\r\
    \nI would have loved to fine tune the model with my code but I'm getting OOM issues.\r\
    \nBecause of this I would have loved to have a smaller version to try to fine\
    \ tune using my personal data to see the results.\r\n"
  created_at: 2023-10-29 19:28:02+00:00
  edited: false
  hidden: false
  id: 653ec052885338b011fa5740
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
      fullname: Viorel Mirea
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vmirea
      type: user
    createdAt: '2023-11-24T01:10:40.000Z'
    data:
      edited: false
      editors:
      - vmirea
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9756650328636169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24fc7bc36aa696921e0d8399e09a84d0.svg
          fullname: Viorel Mirea
          isHf: false
          isPro: false
          name: vmirea
          type: user
        html: '<p>Meanwhile I have found out that you can load the model and say that
          you use 30 layers on GPU an the rest on CPU.<br>Any idea if this could be
          done somehow during the training? This could fix the OOM errors but I don''t
          know if it''s possible and how.</p>

          '
        raw: "Meanwhile I have found out that you can load the model and say that\
          \ you use 30 layers on GPU an the rest on CPU. \nAny idea if this could\
          \ be done somehow during the training? This could fix the OOM errors but\
          \ I don't know if it's possible and how."
        updatedAt: '2023-11-24T01:10:40.390Z'
      numEdits: 0
      reactions: []
    id: 655ff8102b4d05bcbac6eb3d
    type: comment
  author: vmirea
  content: "Meanwhile I have found out that you can load the model and say that you\
    \ use 30 layers on GPU an the rest on CPU. \nAny idea if this could be done somehow\
    \ during the training? This could fix the OOM errors but I don't know if it's\
    \ possible and how."
  created_at: 2023-11-24 01:10:40+00:00
  edited: false
  hidden: false
  id: 655ff8102b4d05bcbac6eb3d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: Phind/Phind-CodeLlama-34B-v2
repo_type: model
status: open
target_branch: null
title: Smaller model for fine tunning
