!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-09-07 13:12:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-07T14:12:38.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9051188826560974
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I have created LimitlessCodeTraining, my most refined and filtered
          coding dataset. with over 640,000 lines of pure, high quality coding data.
          I invite you to further fine tune  your model on this data, or add it to
          your own dataset and retrain your model with it. </p>

          <p>Cheers,<br>Rombodawg</p>

          <p>link:</p>

          <ul>

          <li><a href="https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining">https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining</a></li>

          </ul>

          '
        raw: "I have created LimitlessCodeTraining, my most refined and filtered coding\
          \ dataset. with over 640,000 lines of pure, high quality coding data. I\
          \ invite you to further fine tune  your model on this data, or add it to\
          \ your own dataset and retrain your model with it. \n\nCheers,\nRombodawg\n\
          \nlink:\n- https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining"
        updatedAt: '2023-09-07T14:13:28.286Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - djedu28
        - gnomealone
        - YaTharThShaRma999
        - compressllama
    id: 64f9da56d33bf34f5e94862c
    type: comment
  author: rombodawg
  content: "I have created LimitlessCodeTraining, my most refined and filtered coding\
    \ dataset. with over 640,000 lines of pure, high quality coding data. I invite\
    \ you to further fine tune  your model on this data, or add it to your own dataset\
    \ and retrain your model with it. \n\nCheers,\nRombodawg\n\nlink:\n- https://huggingface.co/datasets/rombodawg/LimitlessCodeTraining"
  created_at: 2023-09-07 13:12:38+00:00
  edited: true
  hidden: false
  id: 64f9da56d33bf34f5e94862c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-07T14:15:08.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9188529253005981
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p>Oh and Please make a 13b model. I know you guys dont want to, but\
          \ It really helps people who dont have 3090/4090 gpus. Like my 3080 10gb\
          \ can only run 13b param models when quantized in 4bit. </p>\n<p>So please\
          \ \U0001F64F\U0001F64F</p>\n"
        raw: "Oh and Please make a 13b model. I know you guys dont want to, but It\
          \ really helps people who dont have 3090/4090 gpus. Like my 3080 10gb can\
          \ only run 13b param models when quantized in 4bit. \n\nSo please \U0001F64F\
          \U0001F64F"
        updatedAt: '2023-09-07T14:15:08.329Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - djedu28
        - edebonneuil
        - eramax
        - Sleepwave
    id: 64f9daecbded58b26fdd3e84
    type: comment
  author: rombodawg
  content: "Oh and Please make a 13b model. I know you guys dont want to, but It really\
    \ helps people who dont have 3090/4090 gpus. Like my 3080 10gb can only run 13b\
    \ param models when quantized in 4bit. \n\nSo please \U0001F64F\U0001F64F"
  created_at: 2023-09-07 13:15:08+00:00
  edited: false
  hidden: false
  id: 64f9daecbded58b26fdd3e84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23f65e8620f80d4b7a67b168892a1795.svg
      fullname: SK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 5ven1
      type: user
    createdAt: '2023-09-11T14:00:56.000Z'
    data:
      edited: false
      editors:
      - 5ven1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7616998553276062
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23f65e8620f80d4b7a67b168892a1795.svg
          fullname: SK
          isHf: false
          isPro: false
          name: 5ven1
          type: user
        html: "<blockquote>\n<p>Oh and Please make a 13b model. I know you guys dont\
          \ want to, but It really helps people who dont have 3090/4090 gpus. Like\
          \ my 3080 10gb can only run 13b param models when quantized in 4bit. </p>\n\
          <p>So please \U0001F64F\U0001F64F</p>\n</blockquote>\n<p>My PC has 64GB\
          \ of RAM and the GPU is 4080 16GB along with i7 13700K. I am using Ubuntu\
          \ 22.04.</p>\n<p>Both llama.cpp and oobabooga work surprisingly well with\
          \ these 30B and 34B models. Here is the command I used for this CodeLlama,\
          \ getting 3 tokens/s - quite happy about it!</p>\n<p>./main -t 14 -ngl 32\
          \ -m models/phind-codellama-34b-v2.Q5_K_M.gguf --color -c 2048 --temp 0.7\
          \ --repeat_penalty 1.1 -n -1 -i -ins --batch-size 256</p>\n"
        raw: "> Oh and Please make a 13b model. I know you guys dont want to, but\
          \ It really helps people who dont have 3090/4090 gpus. Like my 3080 10gb\
          \ can only run 13b param models when quantized in 4bit. \n> \n> So please\
          \ \U0001F64F\U0001F64F\n\nMy PC has 64GB of RAM and the GPU is 4080 16GB\
          \ along with i7 13700K. I am using Ubuntu 22.04.\n\nBoth llama.cpp and oobabooga\
          \ work surprisingly well with these 30B and 34B models. Here is the command\
          \ I used for this CodeLlama, getting 3 tokens/s - quite happy about it!\n\
          \n./main -t 14 -ngl 32 -m models/phind-codellama-34b-v2.Q5_K_M.gguf --color\
          \ -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -i -ins --batch-size 256"
        updatedAt: '2023-09-11T14:00:56.967Z'
      numEdits: 0
      reactions: []
    id: 64ff1d98c31316ec1dcc6eb6
    type: comment
  author: 5ven1
  content: "> Oh and Please make a 13b model. I know you guys dont want to, but It\
    \ really helps people who dont have 3090/4090 gpus. Like my 3080 10gb can only\
    \ run 13b param models when quantized in 4bit. \n> \n> So please \U0001F64F\U0001F64F\
    \n\nMy PC has 64GB of RAM and the GPU is 4080 16GB along with i7 13700K. I am\
    \ using Ubuntu 22.04.\n\nBoth llama.cpp and oobabooga work surprisingly well with\
    \ these 30B and 34B models. Here is the command I used for this CodeLlama, getting\
    \ 3 tokens/s - quite happy about it!\n\n./main -t 14 -ngl 32 -m models/phind-codellama-34b-v2.Q5_K_M.gguf\
    \ --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -i -ins --batch-size 256"
  created_at: 2023-09-11 13:00:56+00:00
  edited: false
  hidden: false
  id: 64ff1d98c31316ec1dcc6eb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-11T16:15:20.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9150840044021606
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;5ven1&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/5ven1\">@<span class=\"\
          underline\">5ven1</span></a></span>\n\n\t</span></span>  bruh 3 tokens per\
          \ second? Some people have entire projects they need to work on with ai\
          \ for coding and we need 15-20 tokens per second minimum. </p>\n"
        raw: '@5ven1  bruh 3 tokens per second? Some people have entire projects they
          need to work on with ai for coding and we need 15-20 tokens per second minimum. '
        updatedAt: '2023-09-11T16:15:20.084Z'
      numEdits: 0
      reactions: []
    id: 64ff3d18ce88cdd9baeb98dd
    type: comment
  author: rombodawg
  content: '@5ven1  bruh 3 tokens per second? Some people have entire projects they
    need to work on with ai for coding and we need 15-20 tokens per second minimum. '
  created_at: 2023-09-11 15:15:20+00:00
  edited: false
  hidden: false
  id: 64ff3d18ce88cdd9baeb98dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14570e9514625800a440f40fec36ed6e.svg
      fullname: Camilo Martin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CamiloMM
      type: user
    createdAt: '2023-09-18T12:33:46.000Z'
    data:
      edited: false
      editors:
      - CamiloMM
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9346827268600464
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14570e9514625800a440f40fec36ed6e.svg
          fullname: Camilo Martin
          isHf: false
          isPro: false
          name: CamiloMM
          type: user
        html: '<p>As someone who has a 3090, even I like 13b models just because of
          how damn fast they run.</p>

          <p>That said maybe EXL2 will save the day for you:<br><a href="https://huggingface.co/latimar/Phind-Codellama-34B-v2-exl2">https://huggingface.co/latimar/Phind-Codellama-34B-v2-exl2</a></p>

          '
        raw: 'As someone who has a 3090, even I like 13b models just because of how
          damn fast they run.


          That said maybe EXL2 will save the day for you:

          https://huggingface.co/latimar/Phind-Codellama-34B-v2-exl2'
        updatedAt: '2023-09-18T12:33:46.218Z'
      numEdits: 0
      reactions: []
    id: 650843aa14302b1d76e44b89
    type: comment
  author: CamiloMM
  content: 'As someone who has a 3090, even I like 13b models just because of how
    damn fast they run.


    That said maybe EXL2 will save the day for you:

    https://huggingface.co/latimar/Phind-Codellama-34B-v2-exl2'
  created_at: 2023-09-18 11:33:46+00:00
  edited: false
  hidden: false
  id: 650843aa14302b1d76e44b89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-09-19T03:00:07.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9186820983886719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Just go for gptq 4 bit made by The Bloke, you only need 20G~ish
          VRAM, which stuffed the whole 3090, it is about 20tokens/s with exllama2
          on textgen webui</p>

          '
        raw: Just go for gptq 4 bit made by The Bloke, you only need 20G~ish VRAM,
          which stuffed the whole 3090, it is about 20tokens/s with exllama2 on textgen
          webui
        updatedAt: '2023-09-19T03:00:07.793Z'
      numEdits: 0
      reactions: []
    id: 65090eb7ad753305deb3beee
    type: comment
  author: Yhyu13
  content: Just go for gptq 4 bit made by The Bloke, you only need 20G~ish VRAM, which
    stuffed the whole 3090, it is about 20tokens/s with exllama2 on textgen webui
  created_at: 2023-09-19 02:00:07+00:00
  edited: false
  hidden: false
  id: 65090eb7ad753305deb3beee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-09-19T20:52:28.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9711766839027405
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> I only have a 3080,\
          \ and not everyone can afford to go out and buy a new graphics card at any\
          \ day of the week</p>\n"
        raw: '@Yhyu13 I only have a 3080, and not everyone can afford to go out and
          buy a new graphics card at any day of the week'
        updatedAt: '2023-09-19T20:52:28.636Z'
      numEdits: 0
      reactions: []
    id: 650a0a0ce0850b3ff04bc3ff
    type: comment
  author: rombodawg
  content: '@Yhyu13 I only have a 3080, and not everyone can afford to go out and
    buy a new graphics card at any day of the week'
  created_at: 2023-09-19 19:52:28+00:00
  edited: false
  hidden: false
  id: 650a0a0ce0850b3ff04bc3ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23f65e8620f80d4b7a67b168892a1795.svg
      fullname: SK
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 5ven1
      type: user
    createdAt: '2023-09-28T22:57:17.000Z'
    data:
      edited: false
      editors:
      - 5ven1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9469764828681946
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23f65e8620f80d4b7a67b168892a1795.svg
          fullname: SK
          isHf: false
          isPro: false
          name: 5ven1
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;5ven1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/5ven1\"\
          >@<span class=\"underline\">5ven1</span></a></span>\n\n\t</span></span>\
          \  bruh 3 tokens per second? Some people have entire projects they need\
          \ to work on with ai for coding and we need 15-20 tokens per second minimum.</p>\n\
          </blockquote>\n<p>That seemed okay for light use cases like writing shorter\
          \ scripts. It would be painful for sure for large projects. Awaiting for\
          \ my 3090 and will then re-test.</p>\n"
        raw: '> @5ven1  bruh 3 tokens per second? Some people have entire projects
          they need to work on with ai for coding and we need 15-20 tokens per second
          minimum.


          That seemed okay for light use cases like writing shorter scripts. It would
          be painful for sure for large projects. Awaiting for my 3090 and will then
          re-test.'
        updatedAt: '2023-09-28T22:57:17.231Z'
      numEdits: 0
      reactions: []
    id: 651604cd4256270d6fa8dada
    type: comment
  author: 5ven1
  content: '> @5ven1  bruh 3 tokens per second? Some people have entire projects they
    need to work on with ai for coding and we need 15-20 tokens per second minimum.


    That seemed okay for light use cases like writing shorter scripts. It would be
    painful for sure for large projects. Awaiting for my 3090 and will then re-test.'
  created_at: 2023-09-28 21:57:17+00:00
  edited: false
  hidden: false
  id: 651604cd4256270d6fa8dada
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: Phind/Phind-CodeLlama-34B-v2
repo_type: model
status: open
target_branch: null
title: '(Possibly) the Highest quality coding dataset on hugging face. And its free
  for you to use. '
