!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cypherfox
conflicting_files: null
created_at: 2023-09-18 23:02:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
      fullname: Morgan Schweers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cypherfox
      type: user
    createdAt: '2023-09-19T00:02:11.000Z'
    data:
      edited: true
      editors:
      - Cypherfox
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9681490659713745
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8efac25a624c04c98e1ebedf69a95c48.svg
          fullname: Morgan Schweers
          isHf: false
          isPro: false
          name: Cypherfox
          type: user
        html: '<p>Greetings,<br>A few small tests have suggested that this is a very
          good model from a coding perspective, but on an L40 with 48GB I''m still
          having to run it in 8-bit quantization mode, and getting around 3-4 tokens
          per second. I know the dataset doesn''t necessarily scale to smaller models,
          but if you''d be willing to share the dataset I''d love to try fine-tuning
          the 13B code llama model on it myself. (If I can get the GPUs for it. :P
          )</p>

          <p>The nice part is that 13B models both train and infer faster, so hopefully
          it won''t take quite as long.</p>

          <p>I understand if it''s from internal sources, and can''t be shared, I
          just thought I''d ask.</p>

          <p>In any case, whether you can share the dataset or not, is there any chance
          you can share any information about how you trained it? What tooling, and
          parameters, for example?</p>

          <p>There''s a lot of broken models out there that perform well under limited
          circumstances, or with specific prompts, but fail in more general use in
          ways that are only fixable with retraining. (E.g. finishing their response,
          and then just...flowing into random text generation, or dumping unrelated
          code, which is usually due to the training dataset not using stop tokens.)
          So I''d like to learn more about how to build models that are solid performers,
          and don''t have those bugs.</p>

          <p>Thanks for any help you can give!</p>

          <p>--  Morgan</p>

          '
        raw: 'Greetings,

          A few small tests have suggested that this is a very good model from a coding
          perspective, but on an L40 with 48GB I''m still having to run it in 8-bit
          quantization mode, and getting around 3-4 tokens per second. I know the
          dataset doesn''t necessarily scale to smaller models, but if you''d be willing
          to share the dataset I''d love to try fine-tuning the 13B code llama model
          on it myself. (If I can get the GPUs for it. :P )


          The nice part is that 13B models both train and infer faster, so hopefully
          it won''t take quite as long.


          I understand if it''s from internal sources, and can''t be shared, I just
          thought I''d ask.


          In any case, whether you can share the dataset or not, is there any chance
          you can share any information about how you trained it? What tooling, and
          parameters, for example?


          There''s a lot of broken models out there that perform well under limited
          circumstances, or with specific prompts, but fail in more general use in
          ways that are only fixable with retraining. (E.g. finishing their response,
          and then just...flowing into random text generation, or dumping unrelated
          code, which is usually due to the training dataset not using stop tokens.)
          So I''d like to learn more about how to build models that are solid performers,
          and don''t have those bugs.


          Thanks for any help you can give!


          --  Morgan'
        updatedAt: '2023-09-19T01:26:37.841Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Sleepwave
        - bartekupartek
    id: 6508e503ed23af2c5d05faa8
    type: comment
  author: Cypherfox
  content: 'Greetings,

    A few small tests have suggested that this is a very good model from a coding
    perspective, but on an L40 with 48GB I''m still having to run it in 8-bit quantization
    mode, and getting around 3-4 tokens per second. I know the dataset doesn''t necessarily
    scale to smaller models, but if you''d be willing to share the dataset I''d love
    to try fine-tuning the 13B code llama model on it myself. (If I can get the GPUs
    for it. :P )


    The nice part is that 13B models both train and infer faster, so hopefully it
    won''t take quite as long.


    I understand if it''s from internal sources, and can''t be shared, I just thought
    I''d ask.


    In any case, whether you can share the dataset or not, is there any chance you
    can share any information about how you trained it? What tooling, and parameters,
    for example?


    There''s a lot of broken models out there that perform well under limited circumstances,
    or with specific prompts, but fail in more general use in ways that are only fixable
    with retraining. (E.g. finishing their response, and then just...flowing into
    random text generation, or dumping unrelated code, which is usually due to the
    training dataset not using stop tokens.) So I''d like to learn more about how
    to build models that are solid performers, and don''t have those bugs.


    Thanks for any help you can give!


    --  Morgan'
  created_at: 2023-09-18 23:02:11+00:00
  edited: true
  hidden: false
  id: 6508e503ed23af2c5d05faa8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0754557d76df10b60072dbed86001d2b.svg
      fullname: Manish Vidyasagar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: compressllama
      type: user
    createdAt: '2023-09-27T02:20:56.000Z'
    data:
      edited: false
      editors:
      - compressllama
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8563582897186279
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0754557d76df10b60072dbed86001d2b.svg
          fullname: Manish Vidyasagar
          isHf: false
          isPro: false
          name: compressllama
          type: user
        html: '<p>Any luck getting the dataset for fine tuning?</p>

          '
        raw: Any luck getting the dataset for fine tuning?
        updatedAt: '2023-09-27T02:20:56.874Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - bartekupartek
    id: 65139188cc7684c9e41676a6
    type: comment
  author: compressllama
  content: Any luck getting the dataset for fine tuning?
  created_at: 2023-09-27 01:20:56+00:00
  edited: false
  hidden: false
  id: 65139188cc7684c9e41676a6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 16
repo_id: Phind/Phind-CodeLlama-34B-v2
repo_type: model
status: open
target_branch: null
title: Willing to share the dataset for someone else to tune a 13b model?
