!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pfmm
conflicting_files: null
created_at: 2023-05-17 00:07:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7f2022deca47486398b25a1c5c095fa.svg
      fullname: llll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pfmm
      type: user
    createdAt: '2023-05-17T01:07:24.000Z'
    data:
      edited: false
      editors:
      - pfmm
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7f2022deca47486398b25a1c5c095fa.svg
          fullname: llll
          isHf: false
          isPro: false
          name: pfmm
          type: user
        html: '<p>Hello,<br>I am using windows 10 with Python 3.10.6.  I have installed
          the requirements with pip install -r requirements.txt and that went well.  I
          have also added the ckpt file (v2-1_768-ema-pruned.ckpt) to my Checkpoints
          folder. </p>

          <p>When I attempt to run the txt2img script using the supplied example,
          modified with my real paths (C:\Users\User01\stablediffusion&gt;python scripts/txt2img.py
          --prompt "a professional photograph of an astronaut riding a horse" --ckpt
          C:\Users\User01\stablediffusion\checkpoints\v2-1<br>_768-ema-pruned.ckpt
          --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768) I
          get this error:</p>

          <h2 id="output-below">Output Below:</h2>

          <p>C:\Users\User01\stablediffusion&gt;python scripts/txt2img.py --prompt
          "a professional photograph of an astronaut riding a horse" --ckpt C:\Users\User01\stablediffusion\checkpoints\v2-1<br>_768-ema-pruned.ckpt
          --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768<br>Global
          seed set to 42<br>Loading model from C:\Users\User01\stablediffusion\checkpoints\v2-1_768-ema-pruned.ckpt<br>Global
          Step: 110000<br>WARNING[XFORMERS]: xFormers can''t load C++/CUDA extensions.
          xFormers was built for:<br>    PyTorch 1.13.1+cu117 with CUDA 1107 (you
          have 2.0.1+cpu)<br>    Python  3.10.9 (you have 3.10.6)<br>  Please reinstall
          xformers (see <a rel="nofollow" href="https://github.com/facebookresearch/xformers#installing-xformers">https://github.com/facebookresearch/xformers#installing-xformers</a>)<br>  Memory-efficient
          attention, SwiGLU, sparse and more won''t be available.<br>  Set XFORMERS_MORE_DETAILS=1
          for more details<br>LatentDiffusion: Running in v-prediction mode<br>Setting
          up MemoryEfficientCrossAttention. Query dim is 320, context_dim is None
          and using 5 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim
          is 320, context_dim is 1024 and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 320, context_dim is None and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 320, context_dim is 1024 and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 640, context_dim is None and using 10 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and
          using 10 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          640, context_dim is None and using 10 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 640, context_dim is 1024 and using 10 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and
          using 20 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          1280, context_dim is 1024 and using 20 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 1280, context_dim is None and using 20 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and
          using 20 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          1280, context_dim is None and using 20 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 1280, context_dim is 1024 and using 20 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and
          using 20 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          1280, context_dim is 1024 and using 20 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 1280, context_dim is None and using 20 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024 and
          using 20 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          1280, context_dim is None and using 20 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 1280, context_dim is 1024 and using 20 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and
          using 10 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          640, context_dim is 1024 and using 10 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 640, context_dim is None and using 10 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and
          using 10 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          640, context_dim is None and using 10 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 640, context_dim is 1024 and using 10 heads.<br>Setting up
          MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and
          using 5 heads.<br>Setting up MemoryEfficientCrossAttention. Query dim is
          320, context_dim is 1024 and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 320, context_dim is None and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 320, context_dim is 1024 and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 320, context_dim is None and using 5 heads.<br>Setting up MemoryEfficientCrossAttention.
          Query dim is 320, context_dim is 1024 and using 5 heads.<br>DiffusionWrapper
          has 865.91 M params.<br>making attention of type ''vanilla-xformers'' with
          512 in_channels<br>building MemoryEfficientAttnBlock with 512 in_channels...<br>Working
          with z of shape (1, 4, 32, 32) = 4096 dimensions.<br>making attention of
          type ''vanilla-xformers'' with 512 in_channels<br>building MemoryEfficientAttnBlock
          with 512 in_channels...<br>Creating invisible watermark encoder (see <a
          rel="nofollow" href="https://github.com/ShieldMnt/invisible-watermark">https://github.com/ShieldMnt/invisible-watermark</a>)...<br>data:   0%|                                                                                                                                                   |
          0/1 [00:01&lt;?, ?it/s]<br>Sampling:   0%|                                                                                                                                               |
          0/3 [00:01&lt;?, ?it/s]<br>Traceback (most recent call last):<br>  File
          "C:\Users\User01\stablediffusion\scripts\txt2img.py", line 388, in <br>    main(opt)<br>  File
          "C:\Users\User01\stablediffusion\scripts\txt2img.py", line 342, in main<br>    uc
          = model.get_learned_conditioning(batch_size * [""])<br>  File "c:\users\user01\stablediffusion\ldm\models\diffusion\ddpm.py",
          line 665, in get_learned_conditioning<br>    c = self.cond_stage_model.encode(c)<br>  File
          "c:\users\user01\stablediffusion\ldm\modules\encoders\modules.py", line
          236, in encode<br>    return self(text)<br>  File "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "c:\users\user01\stablediffusion\ldm\modules\encoders\modules.py", line
          213, in forward<br>    z = self.encode_with_transformer(tokens.to(self.device))<br>  File
          "c:\users\user01\stablediffusion\ldm\modules\encoders\modules.py", line
          220, in encode_with_transformer<br>    x = self.text_transformer_forward(x,
          attn_mask=self.model.attn_mask)<br>  File "c:\users\user01\stablediffusion\ldm\modules\encoders\modules.py",
          line 232, in text_transformer_forward<br>    x = r(x, attn_mask=attn_mask)<br>  File
          "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\open_clip\transformer.py",
          line 154, in forward<br>    x = x + self.ls_1(self.attention(self.ln_1(x),
          attn_mask=attn_mask))<br>  File "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\open_clip\transformer.py",
          line 151, in attention<br>    return self.attn(x, x, x, need_weights=False,
          attn_mask=attn_mask)[0]<br>  File "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\activation.py",
          line 1205, in forward<br>    attn_output, attn_output_weights = F.multi_head_attention_forward(<br>  File
          "C:\Users\User01\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\functional.py",
          line 5373, in multi_head_attention_forward<br>    attn_output = scaled_dot_product_attention(q,
          k, v, attn_mask, dropout_p, is_causal)<br>RuntimeError: Expected attn_mask
          dtype to be bool or to match query dtype, but got attn_mask.dtype: float
          and  query.dtype: struct c10::BFloat16 instead.</p>

          <hr>

          <p>End Output</p>

          <p>Does anyone have any suggestions for me?  Thanks very much.</p>

          '
        raw: "Hello,\r\nI am using windows 10 with Python 3.10.6.  I have installed\
          \ the requirements with pip install -r requirements.txt and that went well.\
          \  I have also added the ckpt file (v2-1_768-ema-pruned.ckpt) to my Checkpoints\
          \ folder. \r\n\r\nWhen I attempt to run the txt2img script using the supplied\
          \ example, modified with my real paths (C:\\Users\\User01\\stablediffusion>python\
          \ scripts/txt2img.py --prompt \"a professional photograph of an astronaut\
          \ riding a horse\" --ckpt C:\\Users\\User01\\stablediffusion\\checkpoints\\\
          v2-1\r\n_768-ema-pruned.ckpt --config configs/stable-diffusion/v2-inference-v.yaml\
          \ --H 768 --W 768) I get this error:\r\n\r\n\r\nOutput Below:\r\n--------------------------------------------------------------------------------\r\
          \n\r\nC:\\Users\\User01\\stablediffusion>python scripts/txt2img.py --prompt\
          \ \"a professional photograph of an astronaut riding a horse\" --ckpt C:\\\
          Users\\User01\\stablediffusion\\checkpoints\\v2-1\r\n_768-ema-pruned.ckpt\
          \ --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768\r\
          \nGlobal seed set to 42\r\nLoading model from C:\\Users\\User01\\stablediffusion\\\
          checkpoints\\v2-1_768-ema-pruned.ckpt\r\nGlobal Step: 110000\r\nWARNING[XFORMERS]:\
          \ xFormers can't load C++/CUDA extensions. xFormers was built for:\r\n \
          \   PyTorch 1.13.1+cu117 with CUDA 1107 (you have 2.0.1+cpu)\r\n    Python\
          \  3.10.9 (you have 3.10.6)\r\n  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\r\
          \n  Memory-efficient attention, SwiGLU, sparse and more won't be available.\r\
          \n  Set XFORMERS_MORE_DETAILS=1 for more details\r\nLatentDiffusion: Running\
          \ in v-prediction mode\r\nSetting up MemoryEfficientCrossAttention. Query\
          \ dim is 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nSetting up\
          \ MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and\
          \ using 5 heads.\r\nSetting up MemoryEfficientCrossAttention. Query dim\
          \ is 320, context_dim is 1024 and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up\
          \ MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and\
          \ using 10 heads.\r\nSetting up MemoryEfficientCrossAttention. Query dim\
          \ is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up\
          \ MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None\
          \ and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention. Query\
          \ dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting\
          \ up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024\
          \ and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention. Query\
          \ dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting\
          \ up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None\
          \ and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention. Query\
          \ dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting\
          \ up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 1024\
          \ and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention. Query\
          \ dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting\
          \ up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None\
          \ and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention. Query\
          \ dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up\
          \ MemoryEfficientCrossAttention. Query dim is 640, context_dim is 1024 and\
          \ using 10 heads.\r\nSetting up MemoryEfficientCrossAttention. Query dim\
          \ is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up\
          \ MemoryEfficientCrossAttention. Query dim is 320, context_dim is None and\
          \ using 5 heads.\r\nSetting up MemoryEfficientCrossAttention. Query dim\
          \ is 320, context_dim is 1024 and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 320, context_dim is None and using 5 heads.\r\nSetting up\
          \ MemoryEfficientCrossAttention. Query dim is 320, context_dim is 1024 and\
          \ using 5 heads.\r\nSetting up MemoryEfficientCrossAttention. Query dim\
          \ is 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
          \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nDiffusionWrapper\
          \ has 865.91 M params.\r\nmaking attention of type 'vanilla-xformers' with\
          \ 512 in_channels\r\nbuilding MemoryEfficientAttnBlock with 512 in_channels...\r\
          \nWorking with z of shape (1, 4, 32, 32) = 4096 dimensions.\r\nmaking attention\
          \ of type 'vanilla-xformers' with 512 in_channels\r\nbuilding MemoryEfficientAttnBlock\
          \ with 512 in_channels...\r\nCreating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\r\
          \ndata:   0%|                                                          \
          \                                                                      \
          \                   | 0/1 [00:01<?, ?it/s]\r\nSampling:   0%|          \
          \                                                                      \
          \                                                               | 0/3 [00:01<?,\
          \ ?it/s]\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\User01\\\
          stablediffusion\\scripts\\txt2img.py\", line 388, in <module>\r\n    main(opt)\r\
          \n  File \"C:\\Users\\User01\\stablediffusion\\scripts\\txt2img.py\", line\
          \ 342, in main\r\n    uc = model.get_learned_conditioning(batch_size * [\"\
          \"])\r\n  File \"c:\\users\\user01\\stablediffusion\\ldm\\models\\diffusion\\\
          ddpm.py\", line 665, in get_learned_conditioning\r\n    c = self.cond_stage_model.encode(c)\r\
          \n  File \"c:\\users\\user01\\stablediffusion\\ldm\\modules\\encoders\\\
          modules.py\", line 236, in encode\r\n    return self(text)\r\n  File \"\
          C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"c:\\users\\user01\\stablediffusion\\\
          ldm\\modules\\encoders\\modules.py\", line 213, in forward\r\n    z = self.encode_with_transformer(tokens.to(self.device))\r\
          \n  File \"c:\\users\\user01\\stablediffusion\\ldm\\modules\\encoders\\\
          modules.py\", line 220, in encode_with_transformer\r\n    x = self.text_transformer_forward(x,\
          \ attn_mask=self.model.attn_mask)\r\n  File \"c:\\users\\user01\\stablediffusion\\\
          ldm\\modules\\encoders\\modules.py\", line 232, in text_transformer_forward\r\
          \n    x = r(x, attn_mask=attn_mask)\r\n  File \"C:\\Users\\User01\\AppData\\\
          Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\\
          module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\\
          Python310\\lib\\site-packages\\open_clip\\transformer.py\", line 154, in\
          \ forward\r\n    x = x + self.ls_1(self.attention(self.ln_1(x), attn_mask=attn_mask))\r\
          \n  File \"C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\\
          lib\\site-packages\\open_clip\\transformer.py\", line 151, in attention\r\
          \n    return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\r\
          \n  File \"C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\\
          lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
          \n    return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\User01\\\
          AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\\
          nn\\modules\\activation.py\", line 1205, in forward\r\n    attn_output,\
          \ attn_output_weights = F.multi_head_attention_forward(\r\n  File \"C:\\\
          Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\\
          torch\\nn\\functional.py\", line 5373, in multi_head_attention_forward\r\
          \n    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p,\
          \ is_causal)\r\nRuntimeError: Expected attn_mask dtype to be bool or to\
          \ match query dtype, but got attn_mask.dtype: float and  query.dtype: struct\
          \ c10::BFloat16 instead.\r\n\r\n---------------------------------------------\r\
          \nEnd Output\r\n\r\n\r\nDoes anyone have any suggestions for me?  Thanks\
          \ very much."
        updatedAt: '2023-05-17T01:07:24.685Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ebrar
    id: 646428cceca41ed5029c604b
    type: comment
  author: pfmm
  content: "Hello,\r\nI am using windows 10 with Python 3.10.6.  I have installed\
    \ the requirements with pip install -r requirements.txt and that went well.  I\
    \ have also added the ckpt file (v2-1_768-ema-pruned.ckpt) to my Checkpoints folder.\
    \ \r\n\r\nWhen I attempt to run the txt2img script using the supplied example,\
    \ modified with my real paths (C:\\Users\\User01\\stablediffusion>python scripts/txt2img.py\
    \ --prompt \"a professional photograph of an astronaut riding a horse\" --ckpt\
    \ C:\\Users\\User01\\stablediffusion\\checkpoints\\v2-1\r\n_768-ema-pruned.ckpt\
    \ --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768) I get\
    \ this error:\r\n\r\n\r\nOutput Below:\r\n--------------------------------------------------------------------------------\r\
    \n\r\nC:\\Users\\User01\\stablediffusion>python scripts/txt2img.py --prompt \"\
    a professional photograph of an astronaut riding a horse\" --ckpt C:\\Users\\\
    User01\\stablediffusion\\checkpoints\\v2-1\r\n_768-ema-pruned.ckpt --config configs/stable-diffusion/v2-inference-v.yaml\
    \ --H 768 --W 768\r\nGlobal seed set to 42\r\nLoading model from C:\\Users\\User01\\\
    stablediffusion\\checkpoints\\v2-1_768-ema-pruned.ckpt\r\nGlobal Step: 110000\r\
    \nWARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built\
    \ for:\r\n    PyTorch 1.13.1+cu117 with CUDA 1107 (you have 2.0.1+cpu)\r\n   \
    \ Python  3.10.9 (you have 3.10.6)\r\n  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\r\
    \n  Memory-efficient attention, SwiGLU, sparse and more won't be available.\r\n\
    \  Set XFORMERS_MORE_DETAILS=1 for more details\r\nLatentDiffusion: Running in\
    \ v-prediction mode\r\nSetting up MemoryEfficientCrossAttention. Query dim is\
    \ 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is None and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 1280, context_dim is 1024 and using 20 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is None and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 640, context_dim is 1024 and using 10 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is None and using 5 heads.\r\nSetting up MemoryEfficientCrossAttention.\
    \ Query dim is 320, context_dim is 1024 and using 5 heads.\r\nDiffusionWrapper\
    \ has 865.91 M params.\r\nmaking attention of type 'vanilla-xformers' with 512\
    \ in_channels\r\nbuilding MemoryEfficientAttnBlock with 512 in_channels...\r\n\
    Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\r\nmaking attention\
    \ of type 'vanilla-xformers' with 512 in_channels\r\nbuilding MemoryEfficientAttnBlock\
    \ with 512 in_channels...\r\nCreating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...\r\
    \ndata:   0%|                                                                \
    \                                                                            \
    \       | 0/1 [00:01<?, ?it/s]\r\nSampling:   0%|                            \
    \                                                                            \
    \                                       | 0/3 [00:01<?, ?it/s]\r\nTraceback (most\
    \ recent call last):\r\n  File \"C:\\Users\\User01\\stablediffusion\\scripts\\\
    txt2img.py\", line 388, in <module>\r\n    main(opt)\r\n  File \"C:\\Users\\User01\\\
    stablediffusion\\scripts\\txt2img.py\", line 342, in main\r\n    uc = model.get_learned_conditioning(batch_size\
    \ * [\"\"])\r\n  File \"c:\\users\\user01\\stablediffusion\\ldm\\models\\diffusion\\\
    ddpm.py\", line 665, in get_learned_conditioning\r\n    c = self.cond_stage_model.encode(c)\r\
    \n  File \"c:\\users\\user01\\stablediffusion\\ldm\\modules\\encoders\\modules.py\"\
    , line 236, in encode\r\n    return self(text)\r\n  File \"C:\\Users\\User01\\\
    AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
    \n  File \"c:\\users\\user01\\stablediffusion\\ldm\\modules\\encoders\\modules.py\"\
    , line 213, in forward\r\n    z = self.encode_with_transformer(tokens.to(self.device))\r\
    \n  File \"c:\\users\\user01\\stablediffusion\\ldm\\modules\\encoders\\modules.py\"\
    , line 220, in encode_with_transformer\r\n    x = self.text_transformer_forward(x,\
    \ attn_mask=self.model.attn_mask)\r\n  File \"c:\\users\\user01\\stablediffusion\\\
    ldm\\modules\\encoders\\modules.py\", line 232, in text_transformer_forward\r\n\
    \    x = r(x, attn_mask=attn_mask)\r\n  File \"C:\\Users\\User01\\AppData\\Local\\\
    Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
    , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\\
    open_clip\\transformer.py\", line 154, in forward\r\n    x = x + self.ls_1(self.attention(self.ln_1(x),\
    \ attn_mask=attn_mask))\r\n  File \"C:\\Users\\User01\\AppData\\Local\\Programs\\\
    Python\\Python310\\lib\\site-packages\\open_clip\\transformer.py\", line 151,\
    \ in attention\r\n    return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]\r\
    \n  File \"C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\\
    site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n \
    \   return forward_call(*args, **kwargs)\r\n  File \"C:\\Users\\User01\\AppData\\\
    Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py\"\
    , line 1205, in forward\r\n    attn_output, attn_output_weights = F.multi_head_attention_forward(\r\
    \n  File \"C:\\Users\\User01\\AppData\\Local\\Programs\\Python\\Python310\\lib\\\
    site-packages\\torch\\nn\\functional.py\", line 5373, in multi_head_attention_forward\r\
    \n    attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p,\
    \ is_causal)\r\nRuntimeError: Expected attn_mask dtype to be bool or to match\
    \ query dtype, but got attn_mask.dtype: float and  query.dtype: struct c10::BFloat16\
    \ instead.\r\n\r\n---------------------------------------------\r\nEnd Output\r\
    \n\r\n\r\nDoes anyone have any suggestions for me?  Thanks very much."
  created_at: 2023-05-17 00:07:24+00:00
  edited: false
  hidden: false
  id: 646428cceca41ed5029c604b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9afcb2b2d6dded5866c2e37deb86837c.svg
      fullname: Ebrar Eke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ebrar
      type: user
    createdAt: '2023-07-30T22:46:52.000Z'
    data:
      edited: false
      editors:
      - ebrar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9958534240722656
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9afcb2b2d6dded5866c2e37deb86837c.svg
          fullname: Ebrar Eke
          isHf: false
          isPro: false
          name: ebrar
          type: user
        html: '<p>have you solved this? I have the same issue</p>

          '
        raw: have you solved this? I have the same issue
        updatedAt: '2023-07-30T22:46:52.598Z'
      numEdits: 0
      reactions: []
    id: 64c6e85c4c9bebfa6adb712b
    type: comment
  author: ebrar
  content: have you solved this? I have the same issue
  created_at: 2023-07-30 21:46:52+00:00
  edited: false
  hidden: false
  id: 64c6e85c4c9bebfa6adb712b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 64
repo_id: stabilityai/stable-diffusion-2
repo_type: model
status: open
target_branch: null
title: 'Can''t seem to get txt2img to work properly  Details:'
