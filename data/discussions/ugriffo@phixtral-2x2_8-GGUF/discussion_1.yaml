!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jeiku
conflicting_files: null
created_at: 2024-01-22 01:02:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/a4AlmzpXRwLW_Z93Velts.jpeg?w=200&h=200&f=face
      fullname: jeiku
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeiku
      type: user
    createdAt: '2024-01-22T01:02:21.000Z'
    data:
      edited: false
      editors:
      - jeiku
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9318113923072815
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/a4AlmzpXRwLW_Z93Velts.jpeg?w=200&h=200&f=face
          fullname: jeiku
          isHf: false
          isPro: false
          name: jeiku
          type: user
        html: '<p>Hello, I have recently followed the Phixtral recipe with another
          architecture and every step has been successful until I came up against
          conversion. I notice that the Phixtral and greater Phi-2 model does not
          have a ''tokenizer.model'' file. I am attempting to convert my StableLM
          MOE using the ''convert-hf-to-gguf.py'' script and I am faced with this
          error: </p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/sdCboCuXzhhvYKQHOWUFw.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/sdCboCuXzhhvYKQHOWUFw.png"></a></p>

          <p>This file does not exist in the StableLM architecture, and as far as
          I can tell, it does not exist in the Phi architecture. Can you please share
          with me how you converted and quantized this model without that file? Thank
          you.</p>

          '
        raw: "Hello, I have recently followed the Phixtral recipe with another architecture\
          \ and every step has been successful until I came up against conversion.\
          \ I notice that the Phixtral and greater Phi-2 model does not have a 'tokenizer.model'\
          \ file. I am attempting to convert my StableLM MOE using the 'convert-hf-to-gguf.py'\
          \ script and I am faced with this error: \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/sdCboCuXzhhvYKQHOWUFw.png)\r\
          \n\r\nThis file does not exist in the StableLM architecture, and as far\
          \ as I can tell, it does not exist in the Phi architecture. Can you please\
          \ share with me how you converted and quantized this model without that\
          \ file? Thank you."
        updatedAt: '2024-01-22T01:02:21.321Z'
      numEdits: 0
      reactions: []
    id: 65adbe9dd6b10af911987e2f
    type: comment
  author: jeiku
  content: "Hello, I have recently followed the Phixtral recipe with another architecture\
    \ and every step has been successful until I came up against conversion. I notice\
    \ that the Phixtral and greater Phi-2 model does not have a 'tokenizer.model'\
    \ file. I am attempting to convert my StableLM MOE using the 'convert-hf-to-gguf.py'\
    \ script and I am faced with this error: \r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/626dfb8786671a29c715f8a9/sdCboCuXzhhvYKQHOWUFw.png)\r\
    \n\r\nThis file does not exist in the StableLM architecture, and as far as I can\
    \ tell, it does not exist in the Phi architecture. Can you please share with me\
    \ how you converted and quantized this model without that file? Thank you."
  created_at: 2024-01-22 01:02:21+00:00
  edited: false
  hidden: false
  id: 65adbe9dd6b10af911987e2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66bb73b10405c1275ad488c7aa7eb4fa.svg
      fullname: Umberto Griffo
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ugriffo
      type: user
    createdAt: '2024-01-22T08:52:06.000Z'
    data:
      edited: false
      editors:
      - ugriffo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9068698883056641
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66bb73b10405c1275ad488c7aa7eb4fa.svg
          fullname: Umberto Griffo
          isHf: false
          isPro: false
          name: ugriffo
          type: user
        html: '<p>Here is how I did it <a rel="nofollow" href="https://colab.research.google.com/drive/130HcbgMiHQC7yadIDfZZrnnPz-K9Ynqc#scrollTo=QQAVuZ8kGTTq">https://colab.research.google.com/drive/130HcbgMiHQC7yadIDfZZrnnPz-K9Ynqc#scrollTo=QQAVuZ8kGTTq</a><br>But
          be aware that I don''t think the model actually works. I got answers with
          random stuff.</p>

          '
        raw: 'Here is how I did it https://colab.research.google.com/drive/130HcbgMiHQC7yadIDfZZrnnPz-K9Ynqc#scrollTo=QQAVuZ8kGTTq

          But be aware that I don''t think the model actually works. I got answers
          with random stuff.'
        updatedAt: '2024-01-22T08:52:06.024Z'
      numEdits: 0
      reactions: []
    id: 65ae2cb614d782df06f3aa99
    type: comment
  author: ugriffo
  content: 'Here is how I did it https://colab.research.google.com/drive/130HcbgMiHQC7yadIDfZZrnnPz-K9Ynqc#scrollTo=QQAVuZ8kGTTq

    But be aware that I don''t think the model actually works. I got answers with
    random stuff.'
  created_at: 2024-01-22 08:52:06+00:00
  edited: false
  hidden: false
  id: 65ae2cb614d782df06f3aa99
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66bb73b10405c1275ad488c7aa7eb4fa.svg
      fullname: Umberto Griffo
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ugriffo
      type: user
    createdAt: '2024-01-22T08:53:31.000Z'
    data:
      edited: false
      editors:
      - ugriffo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9351806044578552
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66bb73b10405c1275ad488c7aa7eb4fa.svg
          fullname: Umberto Griffo
          isHf: false
          isPro: false
          name: ugriffo
          type: user
        html: '<p>I''ve used this <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/pull/4912">PR</a>
          that is still a draft.</p>

          '
        raw: I've used this [PR](https://github.com/ggerganov/llama.cpp/pull/4912)
          that is still a draft.
        updatedAt: '2024-01-22T08:53:31.023Z'
      numEdits: 0
      reactions: []
    id: 65ae2d0bf8111f40c084cb2d
    type: comment
  author: ugriffo
  content: I've used this [PR](https://github.com/ggerganov/llama.cpp/pull/4912) that
    is still a draft.
  created_at: 2024-01-22 08:53:31+00:00
  edited: false
  hidden: false
  id: 65ae2d0bf8111f40c084cb2d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ugriffo/phixtral-2x2_8-GGUF
repo_type: model
status: open
target_branch: null
title: Please help me understand how you quantized this.
