!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dvruette
conflicting_files: null
created_at: 2024-01-09 14:31:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678365850657-640501091a3babee78e41e99.jpeg?w=200&h=200&f=face
      fullname: Dimitri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dvruette
      type: user
    createdAt: '2024-01-09T14:31:19.000Z'
    data:
      edited: false
      editors:
      - dvruette
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7373623251914978
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678365850657-640501091a3babee78e41e99.jpeg?w=200&h=200&f=face
          fullname: Dimitri
          isHf: false
          isPro: false
          name: dvruette
          type: user
        html: "<p>The <code>special_tokens_map.json</code> specifies the <code>eos</code>\
          \ and <code>pad</code> tokens as <code>#</code> and <code>\"</code> respectively,\
          \ which seems like a weird choice.</p>\n<pre><code class=\"language-json\"\
          ><span class=\"hljs-punctuation\">{</span>\n  <span class=\"hljs-attr\"\
          >\"eos_token\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-string\">\"#\"</span><span class=\"hljs-punctuation\">,</span>\n  <span\
          \ class=\"hljs-attr\">\"pad_token\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"\\\"\"</span><span class=\"hljs-punctuation\"\
          >,</span>\n  <span class=\"hljs-attr\">\"unk_token\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n\
          \    <span class=\"hljs-attr\">\"content\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-string\">\"&lt;|endoftext|&gt;\"</span><span\
          \ class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"\
          lstrip\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"\
          hljs-literal\"><span class=\"hljs-keyword\">false</span></span><span class=\"\
          hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"normalized\"\
          </span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"\
          ><span class=\"hljs-keyword\">true</span></span><span class=\"hljs-punctuation\"\
          >,</span>\n    <span class=\"hljs-attr\">\"rstrip\"</span><span class=\"\
          hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"\
          hljs-keyword\">false</span></span><span class=\"hljs-punctuation\">,</span>\n\
          \    <span class=\"hljs-attr\">\"single_word\"</span><span class=\"hljs-punctuation\"\
          >:</span> <span class=\"hljs-literal\"><span class=\"hljs-keyword\">false</span></span>\n\
          \  <span class=\"hljs-punctuation\">}</span>\n<span class=\"hljs-punctuation\"\
          >}</span>\n</code></pre>\n<p>Is this correct? Has the model been trained\
          \ on these token maps? Has the model seen the <code>&lt;|endoftext|&gt;</code>\
          \ token during training?</p>\n"
        raw: "The `special_tokens_map.json` specifies the `eos` and `pad` tokens as\
          \ `#` and `\"` respectively, which seems like a weird choice.\r\n\r\n```json\r\
          \n{\r\n  \"eos_token\": \"#\",\r\n  \"pad_token\": \"\\\"\",\r\n  \"unk_token\"\
          : {\r\n    \"content\": \"<|endoftext|>\",\r\n    \"lstrip\": false,\r\n\
          \    \"normalized\": true,\r\n    \"rstrip\": false,\r\n    \"single_word\"\
          : false\r\n  }\r\n}\r\n```\r\n\r\nIs this correct? Has the model been trained\
          \ on these token maps? Has the model seen the `<|endoftext|>` token during\
          \ training?"
        updatedAt: '2024-01-09T14:31:19.408Z'
      numEdits: 0
      reactions: []
    id: 659d58b70873806b65ecd0a9
    type: comment
  author: dvruette
  content: "The `special_tokens_map.json` specifies the `eos` and `pad` tokens as\
    \ `#` and `\"` respectively, which seems like a weird choice.\r\n\r\n```json\r\
    \n{\r\n  \"eos_token\": \"#\",\r\n  \"pad_token\": \"\\\"\",\r\n  \"unk_token\"\
    : {\r\n    \"content\": \"<|endoftext|>\",\r\n    \"lstrip\": false,\r\n    \"\
    normalized\": true,\r\n    \"rstrip\": false,\r\n    \"single_word\": false\r\n\
    \  }\r\n}\r\n```\r\n\r\nIs this correct? Has the model been trained on these token\
    \ maps? Has the model seen the `<|endoftext|>` token during training?"
  created_at: 2024-01-09 14:31:19+00:00
  edited: false
  hidden: false
  id: 659d58b70873806b65ecd0a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2024-01-11T00:48:49.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9977598786354065
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: '<p>I''m also seeing that, I don''t know how that would affect the future,
          also I don''t see a template</p>

          '
        raw: I'm also seeing that, I don't know how that would affect the future,
          also I don't see a template
        updatedAt: '2024-01-11T00:48:49.248Z'
      numEdits: 0
      reactions: []
    id: 659f3af19f682a3147b19863
    type: comment
  author: NickyNicky
  content: I'm also seeing that, I don't know how that would affect the future, also
    I don't see a template
  created_at: 2024-01-11 00:48:49+00:00
  edited: false
  hidden: false
  id: 659f3af19f682a3147b19863
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
      fullname: M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maykeye
      type: user
    createdAt: '2024-01-11T04:33:13.000Z'
    data:
      edited: false
      editors:
      - Maykeye
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6455869674682617
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
          fullname: M
          isHf: false
          isPro: false
          name: Maykeye
          type: user
        html: '<p>IME after finetuning it leads to model preferring single quotes
          instead of double quotes as it really confuses DataCollatorForLanguageModeling.</p>

          <pre><code class="language-python">collator = DataCollatorForLanguageModeling(tokenizer,
          mlm=<span class="hljs-literal">False</span>)

          features = tokenizer(<span class="hljs-string">''I said "Hi"''</span>, return_tensors=<span
          class="hljs-string">"pt"</span>)

          collator([features])

          </code></pre>

          <p>produces<br>"{''input_ids'': tensor([[[   40,   531,   220,     1, 17250,     1]]]),
          ''attention_mask'': tensor([[[1, 1, 1, 1, 1, 1]]]), ''labels'': tensor([[[   40,   531,   220,
          <strong>-100</strong>, 17250, <strong>-100</strong>]]])}"</p>

          <p> the model never learns to output a single double quote. </p>

          <blockquote>

          <p>also I don''t see a template</p>

          </blockquote>

          <p>It''s not a chat model.</p>

          '
        raw: "IME after finetuning it leads to model preferring single quotes instead\
          \ of double quotes as it really confuses DataCollatorForLanguageModeling.\n\
          \n```python\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\
          features = tokenizer('I said \"Hi\"', return_tensors=\"pt\")\ncollator([features])\n\
          ```\nproduces\n\"{'input_ids': tensor([[[   40,   531,   220,     1, 17250,\
          \     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1]]]), 'labels':\
          \ tensor([[[   40,   531,   220, **-100**, 17250, **-100**]]])}\"\n\n the\
          \ model never learns to output a single double quote. \n\n>also I don't\
          \ see a template\n\nIt's not a chat model.\n"
        updatedAt: '2024-01-11T04:33:13.080Z'
      numEdits: 0
      reactions: []
    id: 659f6f894c074ce5e4e9532c
    type: comment
  author: Maykeye
  content: "IME after finetuning it leads to model preferring single quotes instead\
    \ of double quotes as it really confuses DataCollatorForLanguageModeling.\n\n\
    ```python\ncollator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\
    features = tokenizer('I said \"Hi\"', return_tensors=\"pt\")\ncollator([features])\n\
    ```\nproduces\n\"{'input_ids': tensor([[[   40,   531,   220,     1, 17250,  \
    \   1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1]]]), 'labels': tensor([[[\
    \   40,   531,   220, **-100**, 17250, **-100**]]])}\"\n\n the model never learns\
    \ to output a single double quote. \n\n>also I don't see a template\n\nIt's not\
    \ a chat model.\n"
  created_at: 2024-01-11 04:33:13+00:00
  edited: false
  hidden: false
  id: 659f6f894c074ce5e4e9532c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: ahxt/LiteLlama-460M-1T
repo_type: model
status: open
target_branch: null
title: EOS and PAD tokens
