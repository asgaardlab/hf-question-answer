!!python/object:huggingface_hub.community.DiscussionWithDetails
author: foowaa
conflicting_files: null
created_at: 2024-01-08 08:55:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKCDdRzcqX71kr-muz2s_.jpeg?w=200&h=200&f=face
      fullname: alex tiann
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: foowaa
      type: user
    createdAt: '2024-01-08T08:55:11.000Z'
    data:
      edited: false
      editors:
      - foowaa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6319820284843445
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKCDdRzcqX71kr-muz2s_.jpeg?w=200&h=200&f=face
          fullname: alex tiann
          isHf: false
          isPro: false
          name: foowaa
          type: user
        html: '<p>When using the demo code, the answer is not the same as inference
          API in HuggingFace which is boring.</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span>
          AutoTokenizer, AutoModelForCausalLM


          model_path = <span class="hljs-string">''/data0/LiteLlama''</span>


          model = AutoModelForCausalLM.from_pretrained(model_path)

          tokenizer = AutoTokenizer.from_pretrained(model_path)

          model.<span class="hljs-built_in">eval</span>()


          prompt = <span class="hljs-string">''Q: Hello, what is your name?\nA:''</span>

          input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">"pt"</span>).input_ids

          tokens = model.generate(input_ids, max_length=<span class="hljs-number">20</span>)

          <span class="hljs-built_in">print</span>( tokenizer.decode(tokens[<span
          class="hljs-number">0</span>].tolist(), skip_special_tokens=<span class="hljs-literal">True</span>)
          )

          </code></pre>

          <p>Result is:</p>

          <pre><code>Q: What is the largest bird?

          A: The largest bird is the largest bird.

          </code></pre>

          '
        raw: "When using the demo code, the answer is not the same as inference API\
          \ in HuggingFace which is boring.\r\n```python\r\nimport torch\r\nfrom transformers\
          \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_path = '/data0/LiteLlama'\r\
          \n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_path)\r\nmodel.eval()\r\n\r\nprompt\
          \ = 'Q: Hello, what is your name?\\nA:'\r\ninput_ids = tokenizer(prompt,\
          \ return_tensors=\"pt\").input_ids\r\ntokens = model.generate(input_ids,\
          \ max_length=20)\r\nprint( tokenizer.decode(tokens[0].tolist(), skip_special_tokens=True)\
          \ )\r\n```\r\nResult is:\r\n```\r\nQ: What is the largest bird?\r\nA: The\
          \ largest bird is the largest bird.\r\n```"
        updatedAt: '2024-01-08T08:55:11.396Z'
      numEdits: 0
      reactions: []
    id: 659bb86f58608c404430d972
    type: comment
  author: foowaa
  content: "When using the demo code, the answer is not the same as inference API\
    \ in HuggingFace which is boring.\r\n```python\r\nimport torch\r\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_path = '/data0/LiteLlama'\r\
    \n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\r\ntokenizer =\
    \ AutoTokenizer.from_pretrained(model_path)\r\nmodel.eval()\r\n\r\nprompt = 'Q:\
    \ Hello, what is your name?\\nA:'\r\ninput_ids = tokenizer(prompt, return_tensors=\"\
    pt\").input_ids\r\ntokens = model.generate(input_ids, max_length=20)\r\nprint(\
    \ tokenizer.decode(tokens[0].tolist(), skip_special_tokens=True) )\r\n```\r\n\
    Result is:\r\n```\r\nQ: What is the largest bird?\r\nA: The largest bird is the\
    \ largest bird.\r\n```"
  created_at: 2024-01-08 08:55:11+00:00
  edited: false
  hidden: false
  id: 659bb86f58608c404430d972
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: ahxt/LiteLlama-460M-1T
repo_type: model
status: open
target_branch: null
title: Boring answer
