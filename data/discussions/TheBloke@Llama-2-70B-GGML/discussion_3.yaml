!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zuhashaik
conflicting_files: null
created_at: 2023-09-04 05:56:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
      fullname: zuhair hasan shaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuhashaik
      type: user
    createdAt: '2023-09-04T06:56:32.000Z'
    data:
      edited: false
      editors:
      - zuhashaik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49577924609184265
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
          fullname: zuhair hasan shaik
          isHf: false
          isPro: false
          name: zuhashaik
          type: user
        html: '<h2 id="i-initially-loaded-the-ggml-version-by-mistake-instead-of-gguf-and-discovered-that-llamacpp-doesnt-support-ggml-i-then-converted-it-to-gguf-using-the-llamacpp-repository-but-im-still-encountering-the-same-error">I
          initially loaded the GGML version by mistake instead of GGUF and discovered
          that LLama.cpp doesn''t support GGML. I then converted it to GGUF using
          the LLama.cpp repository, but I''m still encountering the same error.</h2>

          <p>modelq2gguf=''/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin''<br>llm
          = LlamaCpp(<br>    model_path=modelq2gguf,<br>    temperature=0.75,<br>    max_tokens=2000,<br>    top_p=1,<br>    callback_manager=callback_manager,<br>    verbose=True,
          # Verbose is required to pass to the callback manager<br>)</p>

          <hr>

          <p>ValidationError: 1 validation error for LlamaCpp<br><strong>root</strong><br>  Could
          not load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.
          Received error  (type=value_error)</p>

          '
        raw: "I initially loaded the GGML version by mistake instead of GGUF and discovered\
          \ that LLama.cpp doesn't support GGML. I then converted it to GGUF using\
          \ the LLama.cpp repository, but I'm still encountering the same error.\r\
          \n-------------------------------------------------------------------------------------------------------------------------\r\
          \nmodelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'\r\
          \nllm = LlamaCpp(\r\n    model_path=modelq2gguf,\r\n    temperature=0.75,\r\
          \n    max_tokens=2000,\r\n    top_p=1,\r\n    callback_manager=callback_manager,\
          \ \r\n    verbose=True, # Verbose is required to pass to the callback manager\r\
          \n)\r\n-------------------------------------------------------------------------------------------------------------------------\r\
          \nValidationError: 1 validation error for LlamaCpp\r\n__root__\r\n  Could\
          \ not load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.\
          \ Received error  (type=value_error)\r\n"
        updatedAt: '2023-09-04T06:56:32.103Z'
      numEdits: 0
      reactions: []
    id: 64f57fa06e335217131806b7
    type: comment
  author: zuhashaik
  content: "I initially loaded the GGML version by mistake instead of GGUF and discovered\
    \ that LLama.cpp doesn't support GGML. I then converted it to GGUF using the LLama.cpp\
    \ repository, but I'm still encountering the same error.\r\n-------------------------------------------------------------------------------------------------------------------------\r\
    \nmodelq2gguf='/media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin'\r\
    \nllm = LlamaCpp(\r\n    model_path=modelq2gguf,\r\n    temperature=0.75,\r\n\
    \    max_tokens=2000,\r\n    top_p=1,\r\n    callback_manager=callback_manager,\
    \ \r\n    verbose=True, # Verbose is required to pass to the callback manager\r\
    \n)\r\n-------------------------------------------------------------------------------------------------------------------------\r\
    \nValidationError: 1 validation error for LlamaCpp\r\n__root__\r\n  Could not\
    \ load Llama model from path: /media/iiit/Karvalo/zuhair/llama/llama70b_q2/llama-2-70b.gguf.q2_K.bin.\
    \ Received error  (type=value_error)\r\n"
  created_at: 2023-09-04 05:56:32+00:00
  edited: false
  hidden: false
  id: 64f57fa06e335217131806b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/92502e8c2c0758511f7f7cbcba6decd7.svg
      fullname: zuhair hasan shaik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuhashaik
      type: user
    createdAt: '2023-09-05T02:22:57.000Z'
    data:
      status: closed
    id: 64f69101ef2049bdac3d20ea
    type: status-change
  author: zuhashaik
  created_at: 2023-09-05 01:22:57+00:00
  id: 64f69101ef2049bdac3d20ea
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/Llama-2-70B-GGML
repo_type: model
status: closed
target_branch: null
title: Error while loading llama-2
