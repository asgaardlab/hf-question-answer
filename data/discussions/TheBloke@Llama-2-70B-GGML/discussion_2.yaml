!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mikeee
conflicting_files: null
created_at: 2023-07-26 23:39:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/99552e88b72164becbb7b60fcaa31a11.svg
      fullname: mikeee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeee
      type: user
    createdAt: '2023-07-27T00:39:26.000Z'
    data:
      edited: false
      editors:
      - mikeee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6424625515937805
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/99552e88b72164becbb7b60fcaa31a11.svg
          fullname: mikeee
          isHf: false
          isPro: false
          name: mikeee
          type: user
        html: "<pre><code>error loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
          \ has wrong shape; expected  8192 x  8192, got  8192 x1024\nllama_init_from_file:\
          \ failed to load model\nTraceback (most recent call last):\n  File \"/home/mu2018/github/llama2-7b-chat-ggml/app.py\"\
          , line 126, in &lt;module&gt;\n    LLM = AutoModelForCausalLM.from_pretrained(\n\
          \  File \"/home/mu2018/github/llama2-7b-chat-ggml/.venv/lib/python3.10/site-packages/ctransformers/hub.py\"\
          , line 157, in from_pretrained\n    return LLM(\n  File \"/home/mu2018/github/llama2-7b-chat-ggml/.venv/lib/python3.10/site-packages/ctransformers/llm.py\"\
          , line 214, in __init__\n    raise RuntimeError(\nRuntimeError: Failed to\
          \ create LLM 'llama' from 'models/llama-2-70b.ggmlv3.q4_K_S.bin'.\n</code></pre>\n\
          <p>Thanks very much for providing all these ggml models. Really awesome!</p>\n\
          <p>I tried some of the llama-2-7b,  llama-2-13b ggml  models and all run\
          \ without a problem.</p>\n<p>llama-2-70b.ggmlv3.q4_K_S.bin however seems\
          \ to have some problem. I tried it with Python 3.10.6 and ctransformers\
          \ in Ubuntu 22. I'll probably  give another try with llama-2-70b.ggmlv3.q4_0.bin.</p>\n"
        raw: "```\r\nerror loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
          \ has wrong shape; expected  8192 x  8192, got  8192 x1024\r\nllama_init_from_file:\
          \ failed to load model\r\nTraceback (most recent call last):\r\n  File \"\
          /home/mu2018/github/llama2-7b-chat-ggml/app.py\", line 126, in <module>\r\
          \n    LLM = AutoModelForCausalLM.from_pretrained(\r\n  File \"/home/mu2018/github/llama2-7b-chat-ggml/.venv/lib/python3.10/site-packages/ctransformers/hub.py\"\
          , line 157, in from_pretrained\r\n    return LLM(\r\n  File \"/home/mu2018/github/llama2-7b-chat-ggml/.venv/lib/python3.10/site-packages/ctransformers/llm.py\"\
          , line 214, in __init__\r\n    raise RuntimeError(\r\nRuntimeError: Failed\
          \ to create LLM 'llama' from 'models/llama-2-70b.ggmlv3.q4_K_S.bin'.\r\n\
          ```\r\nThanks very much for providing all these ggml models. Really awesome!\r\
          \n\r\nI tried some of the llama-2-7b,  llama-2-13b ggml  models and all\
          \ run without a problem.\r\n\r\nllama-2-70b.ggmlv3.q4_K_S.bin however seems\
          \ to have some problem. I tried it with Python 3.10.6 and ctransformers\
          \ in Ubuntu 22. I'll probably  give another try with llama-2-70b.ggmlv3.q4_0.bin.\r\
          \n\r\n"
        updatedAt: '2023-07-27T00:39:26.844Z'
      numEdits: 0
      reactions: []
    id: 64c1bcbe7179ef53b87c1ca5
    type: comment
  author: mikeee
  content: "```\r\nerror loading model: llama.cpp: tensor 'layers.0.attention.wk.weight'\
    \ has wrong shape; expected  8192 x  8192, got  8192 x1024\r\nllama_init_from_file:\
    \ failed to load model\r\nTraceback (most recent call last):\r\n  File \"/home/mu2018/github/llama2-7b-chat-ggml/app.py\"\
    , line 126, in <module>\r\n    LLM = AutoModelForCausalLM.from_pretrained(\r\n\
    \  File \"/home/mu2018/github/llama2-7b-chat-ggml/.venv/lib/python3.10/site-packages/ctransformers/hub.py\"\
    , line 157, in from_pretrained\r\n    return LLM(\r\n  File \"/home/mu2018/github/llama2-7b-chat-ggml/.venv/lib/python3.10/site-packages/ctransformers/llm.py\"\
    , line 214, in __init__\r\n    raise RuntimeError(\r\nRuntimeError: Failed to\
    \ create LLM 'llama' from 'models/llama-2-70b.ggmlv3.q4_K_S.bin'.\r\n```\r\nThanks\
    \ very much for providing all these ggml models. Really awesome!\r\n\r\nI tried\
    \ some of the llama-2-7b,  llama-2-13b ggml  models and all run without a problem.\r\
    \n\r\nllama-2-70b.ggmlv3.q4_K_S.bin however seems to have some problem. I tried\
    \ it with Python 3.10.6 and ctransformers in Ubuntu 22. I'll probably  give another\
    \ try with llama-2-70b.ggmlv3.q4_0.bin.\r\n\r\n"
  created_at: 2023-07-26 23:39:26+00:00
  edited: false
  hidden: false
  id: 64c1bcbe7179ef53b87c1ca5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/99552e88b72164becbb7b60fcaa31a11.svg
      fullname: mikeee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeee
      type: user
    createdAt: '2023-07-29T03:38:00.000Z'
    data:
      edited: false
      editors:
      - mikeee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9431027770042419
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/99552e88b72164becbb7b60fcaa31a11.svg
          fullname: mikeee
          isHf: false
          isPro: false
          name: mikeee
          type: user
        html: '<p>This is what I fonud out and the way I understand it: the <code>70b
          model</code> is slightly different, needs  to set <code>-gqa 8</code>. But
          <code>ctransformers 0.2.15</code> is able to handle <code>70b</code> model
          now.</p>

          '
        raw: 'This is what I fonud out and the way I understand it: the `70b model`
          is slightly different, needs  to set `-gqa 8`. But `ctransformers 0.2.15`
          is able to handle `70b` model now.'
        updatedAt: '2023-07-29T03:38:00.366Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64c489985cd9506edf2be29b
    id: 64c489985cd9506edf2be29a
    type: comment
  author: mikeee
  content: 'This is what I fonud out and the way I understand it: the `70b model`
    is slightly different, needs  to set `-gqa 8`. But `ctransformers 0.2.15` is able
    to handle `70b` model now.'
  created_at: 2023-07-29 02:38:00+00:00
  edited: false
  hidden: false
  id: 64c489985cd9506edf2be29a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/99552e88b72164becbb7b60fcaa31a11.svg
      fullname: mikeee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikeee
      type: user
    createdAt: '2023-07-29T03:38:00.000Z'
    data:
      status: closed
    id: 64c489985cd9506edf2be29b
    type: status-change
  author: mikeee
  created_at: 2023-07-29 02:38:00+00:00
  id: 64c489985cd9506edf2be29b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-70B-GGML
repo_type: model
status: closed
target_branch: null
title: 'llama-2-70b.ggmlv3.q4_K_S.bin :  wrong shape errors '
