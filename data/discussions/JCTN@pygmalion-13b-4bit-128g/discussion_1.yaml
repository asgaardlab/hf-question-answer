!!python/object:huggingface_hub.community.DiscussionWithDetails
author: StableDiffusion69
conflicting_files: null
created_at: 2023-07-02 07:08:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
      fullname: Diffusion
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: StableDiffusion69
      type: user
    createdAt: '2023-07-02T08:08:27.000Z'
    data:
      edited: false
      editors:
      - StableDiffusion69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49554145336151123
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d21e05e5e616f57b17b43244d561411c.svg
          fullname: Diffusion
          isHf: false
          isPro: false
          name: StableDiffusion69
          type: user
        html: "<p>I can load the model in oobabooga with the cpu and desc_act switch\
          \ on my 8GB VRAM card. But when I enter something, there is no response\
          \ and I get this error:</p>\n<p>2023-07-02 09:03:45 INFO:Loading JCTN_pygmalion-13b-4bit-128g...<br>2023-07-02\
          \ 09:03:45 INFO:The AutoGPTQ params are: {'model_basename': '4bit-128g',\
          \ 'device': 'cpu', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': BaseQuantizeConfig(bits=4,\
          \ group_size=128, damp_percent=0.01, desc_act=True, sym=True, true_sequential=True,\
          \ model_name_or_path=None, model_file_base_name=None), 'use_cuda_fp16':\
          \ True}<br>2023-07-02 09:03:45 WARNING:The model weights are not tied. Please\
          \ use the <code>tie_weights</code> method before using the <code>infer_auto_device</code>\
          \ function.<br>2023-07-02 09:03:45 WARNING:The safetensors archive passed\
          \ at models\\JCTN_pygmalion-13b-4bit-128g\\4bit-128g.safetensors does not\
          \ contain metadata. Make sure to save your model with the <code>save_pretrained</code>\
          \ method. Defaulting to 'pt' metadata.<br>2023-07-02 09:04:32 WARNING:skip\
          \ module injection for FusedLlamaMLPForQuantizedModel not support integrate\
          \ without triton yet.<br>2023-07-02 09:04:32 INFO:Loaded the model in 47.16\
          \ seconds.</p>\n<p>============================================================<br>Traceback\
          \ (most recent call last):<br>  File \"F:\\Programme\\oobabooga_windows\\\
          text-generation-webui\\modules\\callbacks.py\", line 55, in gentask<br>\
          \    ret = self.mfunc(callback=_callback, *args, **self.kwargs)<br>  File\
          \ \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 289, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling_base.py\", line 422, in generate<br>\
          \    with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):<br>\
          \  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling_base.py\", line 411, in device<br>  \
          \  device = [d for d in self.hf_device_map.values() if d not in {'cpu',\
          \ 'disk'}][0]<br>IndexError: list index out of range<br>Output generated\
          \ in 0.32 seconds (0.00 tokens/s, 0 tokens, context 1056, seed 1992812162)</p>\n\
          <p>Any ideas on how to fix this, please? \U0001F914</p>\n"
        raw: "I can load the model in oobabooga with the cpu and desc_act switch on\
          \ my 8GB VRAM card. But when I enter something, there is no response and\
          \ I get this error:\r\n\r\n2023-07-02 09:03:45 INFO:Loading JCTN_pygmalion-13b-4bit-128g...\r\
          \n2023-07-02 09:03:45 INFO:The AutoGPTQ params are: {'model_basename': '4bit-128g',\
          \ 'device': 'cpu', 'use_triton': False, 'inject_fused_attention': True,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': None, 'quantize_config': BaseQuantizeConfig(bits=4,\
          \ group_size=128, damp_percent=0.01, desc_act=True, sym=True, true_sequential=True,\
          \ model_name_or_path=None, model_file_base_name=None), 'use_cuda_fp16':\
          \ True}\r\n2023-07-02 09:03:45 WARNING:The model weights are not tied. Please\
          \ use the `tie_weights` method before using the `infer_auto_device` function.\r\
          \n2023-07-02 09:03:45 WARNING:The safetensors archive passed at models\\\
          JCTN_pygmalion-13b-4bit-128g\\4bit-128g.safetensors does not contain metadata.\
          \ Make sure to save your model with the `save_pretrained` method. Defaulting\
          \ to 'pt' metadata.\r\n2023-07-02 09:04:32 WARNING:skip module injection\
          \ for FusedLlamaMLPForQuantizedModel not support integrate without triton\
          \ yet.\r\n2023-07-02 09:04:32 INFO:Loaded the model in 47.16 seconds.\r\n\
          \r\n============================================================\r\nTraceback\
          \ (most recent call last):\r\n  File \"F:\\Programme\\oobabooga_windows\\\
          text-generation-webui\\modules\\callbacks.py\", line 55, in gentask\r\n\
          \    ret = self.mfunc(callback=_callback, *args, **self.kwargs)\r\n  File\
          \ \"F:\\Programme\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 289, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling\\_base.py\", line 422, in generate\r\n\
          \    with torch.inference_mode(), torch.amp.autocast(device_type=self.device.type):\r\
          \n  File \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\\
          site-packages\\auto_gptq\\modeling\\_base.py\", line 411, in device\r\n\
          \    device = [d for d in self.hf_device_map.values() if d not in {'cpu',\
          \ 'disk'}][0]\r\nIndexError: list index out of range\r\nOutput generated\
          \ in 0.32 seconds (0.00 tokens/s, 0 tokens, context 1056, seed 1992812162)\r\
          \n\r\nAny ideas on how to fix this, please? \U0001F914"
        updatedAt: '2023-07-02T08:08:27.503Z'
      numEdits: 0
      reactions: []
    id: 64a1307b33631b0574d88169
    type: comment
  author: StableDiffusion69
  content: "I can load the model in oobabooga with the cpu and desc_act switch on\
    \ my 8GB VRAM card. But when I enter something, there is no response and I get\
    \ this error:\r\n\r\n2023-07-02 09:03:45 INFO:Loading JCTN_pygmalion-13b-4bit-128g...\r\
    \n2023-07-02 09:03:45 INFO:The AutoGPTQ params are: {'model_basename': '4bit-128g',\
    \ 'device': 'cpu', 'use_triton': False, 'inject_fused_attention': True, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': None,\
    \ 'quantize_config': BaseQuantizeConfig(bits=4, group_size=128, damp_percent=0.01,\
    \ desc_act=True, sym=True, true_sequential=True, model_name_or_path=None, model_file_base_name=None),\
    \ 'use_cuda_fp16': True}\r\n2023-07-02 09:03:45 WARNING:The model weights are\
    \ not tied. Please use the `tie_weights` method before using the `infer_auto_device`\
    \ function.\r\n2023-07-02 09:03:45 WARNING:The safetensors archive passed at models\\\
    JCTN_pygmalion-13b-4bit-128g\\4bit-128g.safetensors does not contain metadata.\
    \ Make sure to save your model with the `save_pretrained` method. Defaulting to\
    \ 'pt' metadata.\r\n2023-07-02 09:04:32 WARNING:skip module injection for FusedLlamaMLPForQuantizedModel\
    \ not support integrate without triton yet.\r\n2023-07-02 09:04:32 INFO:Loaded\
    \ the model in 47.16 seconds.\r\n\r\n============================================================\r\
    \nTraceback (most recent call last):\r\n  File \"F:\\Programme\\oobabooga_windows\\\
    text-generation-webui\\modules\\callbacks.py\", line 55, in gentask\r\n    ret\
    \ = self.mfunc(callback=_callback, *args, **self.kwargs)\r\n  File \"F:\\Programme\\\
    oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\", line\
    \ 289, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\n  File\
    \ \"F:\\Programme\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\\
    auto_gptq\\modeling\\_base.py\", line 422, in generate\r\n    with torch.inference_mode(),\
    \ torch.amp.autocast(device_type=self.device.type):\r\n  File \"F:\\Programme\\\
    oobabooga_windows\\installer_files\\env\\lib\\site-packages\\auto_gptq\\modeling\\\
    _base.py\", line 411, in device\r\n    device = [d for d in self.hf_device_map.values()\
    \ if d not in {'cpu', 'disk'}][0]\r\nIndexError: list index out of range\r\nOutput\
    \ generated in 0.32 seconds (0.00 tokens/s, 0 tokens, context 1056, seed 1992812162)\r\
    \n\r\nAny ideas on how to fix this, please? \U0001F914"
  created_at: 2023-07-02 07:08:27+00:00
  edited: false
  hidden: false
  id: 64a1307b33631b0574d88169
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: JCTN/pygmalion-13b-4bit-128g
repo_type: model
status: open
target_branch: null
title: Output failure
