!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YaTharThShaRma999
conflicting_files: null
created_at: 2023-11-06 23:01:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-06T23:01:32.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9893212914466858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: "<p>I really appreciate that you made Lora\u2019s of the model. That\
          \ really helped me to run the helixnet model. </p>\n<p>However one thing\
          \ I saw is that you say extract Lora\u2019s? How did you exactly do that?</p>\n"
        raw: "I really appreciate that you made Lora\u2019s of the model. That really\
          \ helped me to run the helixnet model. \r\n\r\nHowever one thing I saw is\
          \ that you say extract Lora\u2019s? How did you exactly do that?"
        updatedAt: '2023-11-06T23:01:32.632Z'
      numEdits: 0
      reactions: []
    id: 6549704c2a292b3e2e4e89d8
    type: comment
  author: YaTharThShaRma999
  content: "I really appreciate that you made Lora\u2019s of the model. That really\
    \ helped me to run the helixnet model. \r\n\r\nHowever one thing I saw is that\
    \ you say extract Lora\u2019s? How did you exactly do that?"
  created_at: 2023-11-06 23:01:32+00:00
  edited: false
  hidden: false
  id: 6549704c2a292b3e2e4e89d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313247d289cf15634c514bd/Qoev8yYtZBor4xws6MIE6.png?w=200&h=200&f=face
      fullname: Rhys Jones
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: rhysjones
      type: user
    createdAt: '2023-11-06T23:21:44.000Z'
    data:
      edited: false
      editors:
      - rhysjones
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9173769354820251
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6313247d289cf15634c514bd/Qoev8yYtZBor4xws6MIE6.png?w=200&h=200&f=face
          fullname: Rhys Jones
          isHf: false
          isPro: false
          name: rhysjones
          type: user
        html: '<p>The process is to compare the fine-tuned model with the original
          model (Mistral in this case), creating a delta of changes for each of the
          model''s layers. You then factorize this using SVD with a given rank, which
          will eventually give you the A,B matrix to what an equivalent LoRA would
          be.</p>

          <p>There''s a github project that I followed here: <a rel="nofollow" href="https://github.com/uukuguy/multi_loras#quick-start">https://github.com/uukuguy/multi_loras</a>
          - I ended up generating a range of LoRAs of different ranks to try (32,
          48, 64 and 128) and settled on the 64  one for use in this project. The
          higher the rank, the greater the LoRA''s fidelity but at the expense of
          size.</p>

          '
        raw: 'The process is to compare the fine-tuned model with the original model
          (Mistral in this case), creating a delta of changes for each of the model''s
          layers. You then factorize this using SVD with a given rank, which will
          eventually give you the A,B matrix to what an equivalent LoRA would be.


          There''s a github project that I followed here: [https://github.com/uukuguy/multi_loras](https://github.com/uukuguy/multi_loras#quick-start)
          - I ended up generating a range of LoRAs of different ranks to try (32,
          48, 64 and 128) and settled on the 64  one for use in this project. The
          higher the rank, the greater the LoRA''s fidelity but at the expense of
          size.'
        updatedAt: '2023-11-06T23:21:44.315Z'
      numEdits: 0
      reactions: []
    id: 654975088fde27109bbf87cb
    type: comment
  author: rhysjones
  content: 'The process is to compare the fine-tuned model with the original model
    (Mistral in this case), creating a delta of changes for each of the model''s layers.
    You then factorize this using SVD with a given rank, which will eventually give
    you the A,B matrix to what an equivalent LoRA would be.


    There''s a github project that I followed here: [https://github.com/uukuguy/multi_loras](https://github.com/uukuguy/multi_loras#quick-start)
    - I ended up generating a range of LoRAs of different ranks to try (32, 48, 64
    and 128) and settled on the 64  one for use in this project. The higher the rank,
    the greater the LoRA''s fidelity but at the expense of size.'
  created_at: 2023-11-06 23:21:44+00:00
  edited: false
  hidden: false
  id: 654975088fde27109bbf87cb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rhysjones/HelixNet-LMoE-Actor
repo_type: model
status: open
target_branch: null
title: Thanks but how did you extract the Lora??
