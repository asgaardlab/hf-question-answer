!!python/object:huggingface_hub.community.DiscussionWithDetails
author: narphorium
conflicting_files: null
created_at: 2022-11-27 20:18:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35f92bea6fe3f214abf971e28bc36e5b.svg
      fullname: Shawn Simister
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: narphorium
      type: user
    createdAt: '2022-11-27T20:18:18.000Z'
    data:
      edited: true
      editors:
      - narphorium
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35f92bea6fe3f214abf971e28bc36e5b.svg
          fullname: Shawn Simister
          isHf: false
          isPro: false
          name: narphorium
          type: user
        html: '<p>Could you please upload the tokenizer with the &lt;NME&gt;, &lt;BEF&gt;,
          &lt;MSG&gt; and &lt;DFF&gt; tokens so that they can be used in inference.</p>

          '
        raw: Could you please upload the tokenizer with the &lt;NME>, &lt;BEF>, &lt;MSG>
          and &lt;DFF> tokens so that they can be used in inference.
        updatedAt: '2022-11-27T20:18:46.133Z'
      numEdits: 1
      reactions: []
    id: 6383c60a830bbad7b91691ed
    type: comment
  author: narphorium
  content: Could you please upload the tokenizer with the &lt;NME>, &lt;BEF>, &lt;MSG>
    and &lt;DFF> tokens so that they can be used in inference.
  created_at: 2022-11-27 20:18:18+00:00
  edited: true
  hidden: false
  id: 6383c60a830bbad7b91691ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e3769763c0e3f3ff83f3020aef60768.svg
      fullname: Honglu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: quintic
      type: user
    createdAt: '2022-12-09T21:32:47.000Z'
    data:
      edited: false
      editors:
      - quintic
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e3769763c0e3f3ff83f3020aef60768.svg
          fullname: Honglu
          isHf: false
          isPro: false
          name: quintic
          type: user
        html: '<blockquote>

          <p>Could you please upload the tokenizer with the &lt;NME&gt;, &lt;BEF&gt;,
          &lt;MSG&gt; and &lt;DFF&gt; tokens so that they can be used in inference.</p>

          </blockquote>

          <p>There were some discussions and in the end we didn''t add them as special
          tokens. They are a combination of a few tokens and it still seems to be
          reasonably capable of recognizing/using them in the prompt completion.<br>But
          this is really just a preliminary model and we are currently doing more
          training and testing.</p>

          '
        raw: '> Could you please upload the tokenizer with the &lt;NME>, &lt;BEF>,
          &lt;MSG> and &lt;DFF> tokens so that they can be used in inference.


          There were some discussions and in the end we didn''t add them as special
          tokens. They are a combination of a few tokens and it still seems to be
          reasonably capable of recognizing/using them in the prompt completion.

          But this is really just a preliminary model and we are currently doing more
          training and testing.'
        updatedAt: '2022-12-09T21:32:47.138Z'
      numEdits: 0
      reactions: []
    id: 6393a97fcb235f9a8a8d9d79
    type: comment
  author: quintic
  content: '> Could you please upload the tokenizer with the &lt;NME>, &lt;BEF>, &lt;MSG>
    and &lt;DFF> tokens so that they can be used in inference.


    There were some discussions and in the end we didn''t add them as special tokens.
    They are a combination of a few tokens and it still seems to be reasonably capable
    of recognizing/using them in the prompt completion.

    But this is really just a preliminary model and we are currently doing more training
    and testing.'
  created_at: 2022-12-09 21:32:47+00:00
  edited: false
  hidden: false
  id: 6393a97fcb235f9a8a8d9d79
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: CarperAI/diff-codegen-350m-v1
repo_type: model
status: open
target_branch: null
title: Missing special tokens
