!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mahdi-b
conflicting_files: null
created_at: 2022-12-04 22:20:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0c6250689237e4e359d35373997ddbb.svg
      fullname: Madi B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahdi-b
      type: user
    createdAt: '2022-12-04T22:20:13.000Z'
    data:
      edited: true
      editors:
      - mahdi-b
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0c6250689237e4e359d35373997ddbb.svg
          fullname: Madi B
          isHf: false
          isPro: false
          name: mahdi-b
          type: user
        html: "<p>I am trying to load the pre-trained model using <code>device_map=\"\
          auto\",</code> but I get an error saying that:</p>\n<pre><code>...\nFile\
          \ ~/anaconda3/envs/esm_2/lib/python3.9/site-packages/transformers/modeling_utils.py:2406,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2404 # Dispatch model with hooks on all devices\
          \ if necessary\n   2405 if device_map is not None:\n-&gt; 2406     dispatch_model(model,\
          \ device_map=device_map, offload_dir=offload_folder, offload_index=offload_index)\n\
          \   2408 if output_loading_info:\n   2409     if loading_info is None:\n\
          \nTypeError: dispatch_model() got an unexpected keyword argument 'offload_index'\n\
          </code></pre>\n<p>Looks likes indeed the function <code>dispatch_model</code>\
          \ does not have an 'offload_index'. I tried removing <code>offload_index</code>\
          \ from the function call, it crashed my server \U0001F602.<br>Also, It seems\
          \ that I cannot even generate a device map for the Facebook ESM model. Trying\
          \ the following:</p>\n<pre><code>from accelerate import init_empty_weights\n\
          from transformers import AutoConfig, AutoModel, AutoTokenizer\n\nconfig\
          \ = AutoConfig.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n\nwith init_empty_weights():\n\
          \    model = AutoModel.from_config(config)\ndevice_map = infer_auto_device_map(model)\n\
          divice_map\n</code></pre>\n<p>returns </p>\n<pre><code>{'': 0}\n</code></pre>\n\
          <p>While running a small model like <code>facebook/esm2_t6_8M_UR50D</code>\
          \ is not an issue, I am afraid that the larger model (3B or 15B) will not\
          \ be useable unless one can split the weights across GPUs. Any thoughts\
          \ about the issue above would be greatly appreciated.</p>\n<p>Thank you!</p>\n"
        raw: "I am trying to load the pre-trained model using `device_map=\"auto\"\
          ,` but I get an error saying that:\n\n```\n...\nFile ~/anaconda3/envs/esm_2/lib/python3.9/site-packages/transformers/modeling_utils.py:2406,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n   2404 # Dispatch model with hooks on all devices\
          \ if necessary\n   2405 if device_map is not None:\n-> 2406     dispatch_model(model,\
          \ device_map=device_map, offload_dir=offload_folder, offload_index=offload_index)\n\
          \   2408 if output_loading_info:\n   2409     if loading_info is None:\n\
          \nTypeError: dispatch_model() got an unexpected keyword argument 'offload_index'\n\
          ```\nLooks likes indeed the function `dispatch_model` does not have an 'offload_index'.\
          \ I tried removing `offload_index` from the function call, it crashed my\
          \ server \U0001F602.\nAlso, It seems that I cannot even generate a device\
          \ map for the Facebook ESM model. Trying the following:\n\n```\nfrom accelerate\
          \ import init_empty_weights\nfrom transformers import AutoConfig, AutoModel,\
          \ AutoTokenizer\n\nconfig = AutoConfig.from_pretrained(\"facebook/esm2_t6_8M_UR50D\"\
          )\n\nwith init_empty_weights():\n    model = AutoModel.from_config(config)\n\
          device_map = infer_auto_device_map(model)\ndivice_map\n```\nreturns \n```\n\
          {'': 0}\n```\n\nWhile running a small model like `facebook/esm2_t6_8M_UR50D`\
          \ is not an issue, I am afraid that the larger model (3B or 15B) will not\
          \ be useable unless one can split the weights across GPUs. Any thoughts\
          \ about the issue above would be greatly appreciated.\n\nThank you!"
        updatedAt: '2022-12-04T23:35:37.855Z'
      numEdits: 2
      reactions: []
    id: 638d1d1d5869d78eb7e65598
    type: comment
  author: mahdi-b
  content: "I am trying to load the pre-trained model using `device_map=\"auto\",`\
    \ but I get an error saying that:\n\n```\n...\nFile ~/anaconda3/envs/esm_2/lib/python3.9/site-packages/transformers/modeling_utils.py:2406,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\n   2404 # Dispatch model with hooks on all devices if necessary\n\
    \   2405 if device_map is not None:\n-> 2406     dispatch_model(model, device_map=device_map,\
    \ offload_dir=offload_folder, offload_index=offload_index)\n   2408 if output_loading_info:\n\
    \   2409     if loading_info is None:\n\nTypeError: dispatch_model() got an unexpected\
    \ keyword argument 'offload_index'\n```\nLooks likes indeed the function `dispatch_model`\
    \ does not have an 'offload_index'. I tried removing `offload_index` from the\
    \ function call, it crashed my server \U0001F602.\nAlso, It seems that I cannot\
    \ even generate a device map for the Facebook ESM model. Trying the following:\n\
    \n```\nfrom accelerate import init_empty_weights\nfrom transformers import AutoConfig,\
    \ AutoModel, AutoTokenizer\n\nconfig = AutoConfig.from_pretrained(\"facebook/esm2_t6_8M_UR50D\"\
    )\n\nwith init_empty_weights():\n    model = AutoModel.from_config(config)\ndevice_map\
    \ = infer_auto_device_map(model)\ndivice_map\n```\nreturns \n```\n{'': 0}\n```\n\
    \nWhile running a small model like `facebook/esm2_t6_8M_UR50D` is not an issue,\
    \ I am afraid that the larger model (3B or 15B) will not be useable unless one\
    \ can split the weights across GPUs. Any thoughts about the issue above would\
    \ be greatly appreciated.\n\nThank you!"
  created_at: 2022-12-04 22:20:13+00:00
  edited: true
  hidden: false
  id: 638d1d1d5869d78eb7e65598
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: facebook/esm2_t6_8M_UR50D
repo_type: model
status: open
target_branch: null
title: inferring device map for model
