!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ihave1
conflicting_files: null
created_at: 2023-12-11 05:11:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3d4375be30a5607ce2a901ef74766dae.svg
      fullname: Gi Jeong Cho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ihave1
      type: user
    createdAt: '2023-12-11T05:11:58.000Z'
    data:
      edited: false
      editors:
      - ihave1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6803587675094604
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3d4375be30a5607ce2a901ef74766dae.svg
          fullname: Gi Jeong Cho
          isHf: false
          isPro: false
          name: ihave1
          type: user
        html: '<p>Where can I download the dpo model?</p>

          '
        raw: Where can I download the dpo model?
        updatedAt: '2023-12-11T05:11:58.670Z'
      numEdits: 0
      reactions: []
    id: 65769a1eb951d40e7a79021c
    type: comment
  author: ihave1
  content: Where can I download the dpo model?
  created_at: 2023-12-11 05:11:58+00:00
  edited: false
  hidden: false
  id: 65769a1eb951d40e7a79021c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64241c3d774cc340797429fc/6aItlelkKm96bfucgwU6J.png?w=200&h=200&f=face
      fullname: davidkim205
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: davidkim205
      type: user
    createdAt: '2023-12-13T06:02:12.000Z'
    data:
      edited: false
      editors:
      - davidkim205
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3028941750526428
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64241c3d774cc340797429fc/6aItlelkKm96bfucgwU6J.png?w=200&h=200&f=face
          fullname: davidkim205
          isHf: false
          isPro: false
          name: davidkim205
          type: user
        html: "<p>in here..<br><a href=\"https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/tree/main\"\
          >https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/tree/main</a></p>\n\
          <p>or, in the code<br><a href=\"https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo#usage\"\
          >https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/blob/4b28ab03b2f9355aaf77cea57b8ed0dcc052cc90/adapter_config.json#L4\n\
          </a></p>\n<pre><code>import torch\n\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel,\
          \ PeftConfig\nfrom transformers import TextStreamer, GenerationConfig\n\n\
          \nmodel='davidkim205/komt-mistral-7b-v1'\npeft_model_name = 'davidkim205/komt-mistral-7b-v1-dpo'\n\
          config = PeftConfig.from_pretrained(peft_model_name)\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nconfig.base_model_name_or_path\
          \ =model\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\
          \ quantization_config=bnb_config, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model,\
          \ peft_model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\
          </code></pre>\n"
        raw: "in here..\nhttps://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/tree/main\n\
          \n\n\nor, in the code\n[https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/blob/4b28ab03b2f9355aaf77cea57b8ed0dcc052cc90/adapter_config.json#L4\n\
          ](https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo#usage)\n```\n\
          import torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM,\
          \ BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\nfrom transformers\
          \ import TextStreamer, GenerationConfig\n\n\nmodel='davidkim205/komt-mistral-7b-v1'\n\
          peft_model_name = 'davidkim205/komt-mistral-7b-v1-dpo'\nconfig = PeftConfig.from_pretrained(peft_model_name)\n\
          bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
          \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
          )\nconfig.base_model_name_or_path =model\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\
          \ quantization_config=bnb_config, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model,\
          \ peft_model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\
          ```"
        updatedAt: '2023-12-13T06:02:12.688Z'
      numEdits: 0
      reactions: []
    id: 657948e432150025ace2722d
    type: comment
  author: davidkim205
  content: "in here..\nhttps://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/tree/main\n\
    \n\n\nor, in the code\n[https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo/blob/4b28ab03b2f9355aaf77cea57b8ed0dcc052cc90/adapter_config.json#L4\n\
    ](https://huggingface.co/davidkim205/komt-mistral-7b-v1-dpo#usage)\n```\nimport\
    \ torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\
    from peft import PeftModel, PeftConfig\nfrom transformers import TextStreamer,\
    \ GenerationConfig\n\n\nmodel='davidkim205/komt-mistral-7b-v1'\npeft_model_name\
    \ = 'davidkim205/komt-mistral-7b-v1-dpo'\nconfig = PeftConfig.from_pretrained(peft_model_name)\n\
    bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
    \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
    )\nconfig.base_model_name_or_path =model\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\
    \ quantization_config=bnb_config, device_map=\"auto\")\nmodel = PeftModel.from_pretrained(model,\
    \ peft_model_name)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\
    ```"
  created_at: 2023-12-13 06:02:12+00:00
  edited: false
  hidden: false
  id: 657948e432150025ace2722d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: davidkim205/komt-mistral-7b-v1-dpo
repo_type: model
status: open
target_branch: null
title: Where can I download the dpo model?
