!!python/object:huggingface_hub.community.DiscussionWithDetails
author: perelmanych
conflicting_files: null
created_at: 2023-07-15 19:11:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-07-15T20:11:05.000Z'
    data:
      edited: false
      editors:
      - perelmanych
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5243526101112366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<p>I have tried to load model with llama AVX2 version and with cublas
          version but I failed. Llama-cpp is from the latest release. Here is the
          output</p>

          <p>C:\AI\llama&gt;main -i --color --interactive-first -r "### Human:" -r
          "### Input:" -r "(Input)" -r "### Instruction:" -r "### User:" -r "User:"
          -r "USER:" -r "=============" --temp 0 --ctx_size 2048 --n_predict -1 --ignore-eos
          --repeat_penalty 1.2 --instruct -m wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin
          --threads 8<br>main: build = 843 (6e7cca4)<br>main: seed  = 1689451684<br>llama.cpp:
          loading model from wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin<br>error
          loading model: unexpectedly reached end of file<br>llama_load_model_from_file:
          failed to load model<br>llama_init_from_gpt_params: error: failed to load
          model ''wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin''<br>main: error: unable
          to load model</p>

          '
        raw: "I have tried to load model with llama AVX2 version and with cublas version\
          \ but I failed. Llama-cpp is from the latest release. Here is the output\r\
          \n\r\nC:\\AI\\llama>main -i --color --interactive-first -r \"### Human:\"\
          \ -r \"### Input:\" -r \"(Input)\" -r \"### Instruction:\" -r \"### User:\"\
          \ -r \"User:\" -r \"USER:\" -r \"=============\" --temp 0 --ctx_size 2048\
          \ --n_predict -1 --ignore-eos --repeat_penalty 1.2 --instruct -m wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin\
          \ --threads 8\r\nmain: build = 843 (6e7cca4)\r\nmain: seed  = 1689451684\r\
          \nllama.cpp: loading model from wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin\r\
          \nerror loading model: unexpectedly reached end of file\r\nllama_load_model_from_file:\
          \ failed to load model\r\nllama_init_from_gpt_params: error: failed to load\
          \ model 'wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin'\r\nmain: error: unable\
          \ to load model"
        updatedAt: '2023-07-15T20:11:05.057Z'
      numEdits: 0
      reactions: []
    id: 64b2fd594e5902e2fdc66e2b
    type: comment
  author: perelmanych
  content: "I have tried to load model with llama AVX2 version and with cublas version\
    \ but I failed. Llama-cpp is from the latest release. Here is the output\r\n\r\
    \nC:\\AI\\llama>main -i --color --interactive-first -r \"### Human:\" -r \"###\
    \ Input:\" -r \"(Input)\" -r \"### Instruction:\" -r \"### User:\" -r \"User:\"\
    \ -r \"USER:\" -r \"=============\" --temp 0 --ctx_size 2048 --n_predict -1 --ignore-eos\
    \ --repeat_penalty 1.2 --instruct -m wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin\
    \ --threads 8\r\nmain: build = 843 (6e7cca4)\r\nmain: seed  = 1689451684\r\nllama.cpp:\
    \ loading model from wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin\r\nerror loading\
    \ model: unexpectedly reached end of file\r\nllama_load_model_from_file: failed\
    \ to load model\r\nllama_init_from_gpt_params: error: failed to load model 'wizardcoder-guanaco-15b-v1.1.ggmlv1.q5_1.bin'\r\
    \nmain: error: unable to load model"
  created_at: 2023-07-15 19:11:05+00:00
  edited: false
  hidden: false
  id: 64b2fd594e5902e2fdc66e2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-07-15T20:19:44.000Z'
    data:
      from: Can't load model
      to: Can't load q5_1 model
    id: 64b2ff60cba3235a05eb4647
    type: title-change
  author: perelmanych
  created_at: 2023-07-15 19:19:44+00:00
  id: 64b2ff60cba3235a05eb4647
  new_title: Can't load q5_1 model
  old_title: Can't load model
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0444e484b4d510a612bb27af16f247da.svg
      fullname: Jeff Meloy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeffmeloy
      type: user
    createdAt: '2023-07-15T21:44:24.000Z'
    data:
      edited: false
      editors:
      - jeffmeloy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5103827714920044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0444e484b4d510a612bb27af16f247da.svg
          fullname: Jeff Meloy
          isHf: false
          isPro: false
          name: jeffmeloy
          type: user
        html: '<p>From readme:<br>Compatibilty: These files are not compatible with
          llama.cpp, text-generation-webui or llama-cpp-python.</p>

          <p>Model works for me using  ctransformers (<a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a>)</p>

          '
        raw: "From readme: \nCompatibilty: These files are not compatible with llama.cpp,\
          \ text-generation-webui or llama-cpp-python.\n\nModel works for me using\
          \  ctransformers (https://github.com/marella/ctransformers)"
        updatedAt: '2023-07-15T21:44:24.571Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 64b31338094a60c78bfa9baf
    type: comment
  author: jeffmeloy
  content: "From readme: \nCompatibilty: These files are not compatible with llama.cpp,\
    \ text-generation-webui or llama-cpp-python.\n\nModel works for me using  ctransformers\
    \ (https://github.com/marella/ctransformers)"
  created_at: 2023-07-15 20:44:24+00:00
  edited: false
  hidden: false
  id: 64b31338094a60c78bfa9baf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-07-16T11:16:10.000Z'
    data:
      edited: true
      editors:
      - perelmanych
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8372104167938232
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<blockquote>

          <p>From readme:<br>Compatibilty: These files are not compatible with llama.cpp,
          text-generation-webui or llama-cpp-python.</p>

          <p>Model works for me using  ctransformers (<a rel="nofollow" href="https://github.com/marella/ctransformers">https://github.com/marella/ctransformers</a>)</p>

          </blockquote>

          <p>Oh, sorry. Got used that all ggml models run well on these 3. The problem
          is that I need openAI API. Kobold has its own API. About the rest I have
          no idea. TheBloke, can we expect quantized models compatible with those
          3 tools?</p>

          <p>PS: Just found out that LM Studio should have OpenAI API. So I will try
          it.</p>

          '
        raw: "> From readme: \n> Compatibilty: These files are not compatible with\
          \ llama.cpp, text-generation-webui or llama-cpp-python.\n> \n> Model works\
          \ for me using  ctransformers (https://github.com/marella/ctransformers)\n\
          \nOh, sorry. Got used that all ggml models run well on these 3. The problem\
          \ is that I need openAI API. Kobold has its own API. About the rest I have\
          \ no idea. TheBloke, can we expect quantized models compatible with those\
          \ 3 tools?\n\nPS: Just found out that LM Studio should have OpenAI API.\
          \ So I will try it."
        updatedAt: '2023-07-16T11:28:23.029Z'
      numEdits: 1
      reactions: []
    id: 64b3d17affed52962ac22ba4
    type: comment
  author: perelmanych
  content: "> From readme: \n> Compatibilty: These files are not compatible with llama.cpp,\
    \ text-generation-webui or llama-cpp-python.\n> \n> Model works for me using \
    \ ctransformers (https://github.com/marella/ctransformers)\n\nOh, sorry. Got used\
    \ that all ggml models run well on these 3. The problem is that I need openAI\
    \ API. Kobold has its own API. About the rest I have no idea. TheBloke, can we\
    \ expect quantized models compatible with those 3 tools?\n\nPS: Just found out\
    \ that LM Studio should have OpenAI API. So I will try it."
  created_at: 2023-07-16 10:16:10+00:00
  edited: true
  hidden: false
  id: 64b3d17affed52962ac22ba4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-16T14:00:19.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9118636846542358
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah lm studio is good. And ctransformers can also provide an open
          AI api I believe. </p>

          '
        raw: 'Yeah lm studio is good. And ctransformers can also provide an open AI
          api I believe. '
        updatedAt: '2023-07-16T14:00:19.877Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - perelmanych
    id: 64b3f7f34c3cc95a7563b5ca
    type: comment
  author: TheBloke
  content: 'Yeah lm studio is good. And ctransformers can also provide an open AI
    api I believe. '
  created_at: 2023-07-16 13:00:19+00:00
  edited: false
  hidden: false
  id: 64b3f7f34c3cc95a7563b5ca
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/WizardCoder-Guanaco-15B-V1.1-GGML
repo_type: model
status: open
target_branch: null
title: Can't load q5_1 model
