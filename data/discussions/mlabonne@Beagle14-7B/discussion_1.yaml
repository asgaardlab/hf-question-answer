!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SimSim93
conflicting_files: null
created_at: 2024-01-15 18:28:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c666e897b275a56bf7edd2ed6993647.svg
      fullname: Simon Andreas Egli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SimSim93
      type: user
    createdAt: '2024-01-15T18:28:32.000Z'
    data:
      edited: false
      editors:
      - SimSim93
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9783074855804443
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c666e897b275a56bf7edd2ed6993647.svg
          fullname: Simon Andreas Egli
          isHf: false
          isPro: false
          name: SimSim93
          type: user
        html: "<p>First of all: Thank you for this awesome model! It seems to perform\
          \ really well. Small models are great, as one can run them locally. =D</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Could you create\
          \ a gguf Version of this model?</p>\n<p>(By the way, I have access to a\
          \ computer with two rtx 3090 - I am not quite sure how to create gguf versions,\
          \ but if its doable, I could perhaps help.)<br>I could also use the old\
          \ server of a company of a friend. It is equipped with 3 M40 GPUs. While\
          \ beeing a bit old, they still got some Vram. Don't know if this is usefull.</p>\n"
        raw: "First of all: Thank you for this awesome model! It seems to perform\
          \ really well. Small models are great, as one can run them locally. =D\r\
          \n\r\n@TheBloke Could you create a gguf Version of this model?\r\n\r\n(By\
          \ the way, I have access to a computer with two rtx 3090 - I am not quite\
          \ sure how to create gguf versions, but if its doable, I could perhaps help.)\r\
          \nI could also use the old server of a company of a friend. It is equipped\
          \ with 3 M40 GPUs. While beeing a bit old, they still got some Vram. Don't\
          \ know if this is usefull.\r\n\r\n"
        updatedAt: '2024-01-15T18:28:32.439Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - mlabonne
    id: 65a57950f6cfc4b24a59938a
    type: comment
  author: SimSim93
  content: "First of all: Thank you for this awesome model! It seems to perform really\
    \ well. Small models are great, as one can run them locally. =D\r\n\r\n@TheBloke\
    \ Could you create a gguf Version of this model?\r\n\r\n(By the way, I have access\
    \ to a computer with two rtx 3090 - I am not quite sure how to create gguf versions,\
    \ but if its doable, I could perhaps help.)\r\nI could also use the old server\
    \ of a company of a friend. It is equipped with 3 M40 GPUs. While beeing a bit\
    \ old, they still got some Vram. Don't know if this is usefull.\r\n\r\n"
  created_at: 2024-01-15 18:28:32+00:00
  edited: false
  hidden: false
  id: 65a57950f6cfc4b24a59938a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
      fullname: Maxime Labonne
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mlabonne
      type: user
    createdAt: '2024-01-15T19:40:45.000Z'
    data:
      edited: false
      editors:
      - mlabonne
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8696142435073853
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61b8e2ba285851687028d395/4XZP5aVsMWwzGx_313cqd.jpeg?w=200&h=200&f=face
          fullname: Maxime Labonne
          isHf: false
          isPro: false
          name: mlabonne
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;SimSim93&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SimSim93\"\
          >@<span class=\"underline\">SimSim93</span></a></span>\n\n\t</span></span>!\
          \ I'm currently evaluating a DPO version of this model, it should be even\
          \ better.</p>\n<p>If you want to make GGUF versions of a 7B model, you don't\
          \ need any big hardware. I created this notebook to automate this process\
          \ (T4 GPU): <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k\"\
          >https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k</a></p>\n"
        raw: 'Thanks @SimSim93! I''m currently evaluating a DPO version of this model,
          it should be even better.


          If you want to make GGUF versions of a 7B model, you don''t need any big
          hardware. I created this notebook to automate this process (T4 GPU): https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k'
        updatedAt: '2024-01-15T19:40:45.503Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PrimeD
    id: 65a58a3d0a2ac7c42e28229d
    type: comment
  author: mlabonne
  content: 'Thanks @SimSim93! I''m currently evaluating a DPO version of this model,
    it should be even better.


    If you want to make GGUF versions of a 7B model, you don''t need any big hardware.
    I created this notebook to automate this process (T4 GPU): https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k'
  created_at: 2024-01-15 19:40:45+00:00
  edited: false
  hidden: false
  id: 65a58a3d0a2ac7c42e28229d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aca2fcf1f113e384e724b1bbfb010116.svg
      fullname: Avatest
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: testerav
      type: user
    createdAt: '2024-01-16T02:53:07.000Z'
    data:
      edited: false
      editors:
      - testerav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9564250707626343
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aca2fcf1f113e384e724b1bbfb010116.svg
          fullname: Avatest
          isHf: false
          isPro: false
          name: testerav
          type: user
        html: '<blockquote>

          <p>I''m currently evaluating a DPO version of this model, it should be even
          better.</p>

          </blockquote>

          <p>Not much Difference in Scores.</p>

          '
        raw: '> I''m currently evaluating a DPO version of this model, it should be
          even better.


          Not much Difference in Scores.


          '
        updatedAt: '2024-01-16T02:53:07.847Z'
      numEdits: 0
      reactions: []
    id: 65a5ef9393d165b6e5edd73b
    type: comment
  author: testerav
  content: '> I''m currently evaluating a DPO version of this model, it should be
    even better.


    Not much Difference in Scores.


    '
  created_at: 2024-01-16 02:53:07+00:00
  edited: false
  hidden: false
  id: 65a5ef9393d165b6e5edd73b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mlabonne/Beagle14-7B
repo_type: model
status: open
target_branch: null
title: GGUF Version of the best 7B LLM!
