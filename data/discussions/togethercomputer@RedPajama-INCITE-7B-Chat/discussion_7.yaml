!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nik8482
conflicting_files: null
created_at: 2023-06-14 10:47:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a4e29be7f9f7a0926e3181bc3a788317.svg
      fullname: Nikhil Modha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nik8482
      type: user
    createdAt: '2023-06-14T11:47:04.000Z'
    data:
      edited: false
      editors:
      - nik8482
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9155186414718628
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a4e29be7f9f7a0926e3181bc3a788317.svg
          fullname: Nikhil Modha
          isHf: false
          isPro: false
          name: nik8482
          type: user
        html: '<p>Hi,</p>

          <p>Is there anyway to stop the model from asking itself questions after
          generating an initial response? This is an example of responses I''m currently
          getting</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6489a794837ad032c6d13e2b/SdSCg8ZLPUOk63yRw5Gw2.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/6489a794837ad032c6d13e2b/SdSCg8ZLPUOk63yRw5Gw2.png"></a></p>

          <p>Ideally I want the model to stop generating after the first response,
          I could reduce the max tokens length however I get responses which seem
          to get cut off midway</p>

          <p>Thanks for the help!</p>

          '
        raw: "Hi,\r\n\r\nIs there anyway to stop the model from asking itself questions\
          \ after generating an initial response? This is an example of responses\
          \ I'm currently getting\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6489a794837ad032c6d13e2b/SdSCg8ZLPUOk63yRw5Gw2.png)\r\
          \n\r\nIdeally I want the model to stop generating after the first response,\
          \ I could reduce the max tokens length however I get responses which seem\
          \ to get cut off midway\r\n\r\nThanks for the help!"
        updatedAt: '2023-06-14T11:47:04.865Z'
      numEdits: 0
      reactions: []
    id: 6489a8b83c62e45ab03b0740
    type: comment
  author: nik8482
  content: "Hi,\r\n\r\nIs there anyway to stop the model from asking itself questions\
    \ after generating an initial response? This is an example of responses I'm currently\
    \ getting\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6489a794837ad032c6d13e2b/SdSCg8ZLPUOk63yRw5Gw2.png)\r\
    \n\r\nIdeally I want the model to stop generating after the first response, I\
    \ could reduce the max tokens length however I get responses which seem to get\
    \ cut off midway\r\n\r\nThanks for the help!"
  created_at: 2023-06-14 10:47:04+00:00
  edited: false
  hidden: false
  id: 6489a8b83c62e45ab03b0740
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
      fullname: Ralph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: szeta
      type: user
    createdAt: '2023-06-14T11:53:11.000Z'
    data:
      edited: true
      editors:
      - szeta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.36107489466667175
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
          fullname: Ralph
          isHf: false
          isPro: false
          name: szeta
          type: user
        html: "<p>Add  \"<code>&lt;human&gt;:</code>\" as a stop word.</p>\n<p>Example:\
          \ (here I have <code>[\"&lt;human&gt;:\"]</code> inside the variable <code>self.model_conf['stop_words']</code>)</p>\n\
          <pre><code>            self.stop_words_ids = [\n                self.tokenizer(stop_word,\
          \ return_tensors=\"pt\")[\"input_ids\"].squeeze()\n                for stop_word\
          \ in self.model_conf[\"stop_words\"]\n            ]\n            self.stopping_criteria\
          \ = StoppingCriteriaList(\n                [StoppingCriteriaSub(stops=self.stop_words_ids)]\n\
          \            )\n</code></pre>\n<p>and then add this as parameter to your\
          \ <code>generate()</code> call:</p>\n<p><code>stopping_criteria=self.stopping_criteria,</code></p>\n"
        raw: "Add  \"`<human>:`\" as a stop word.\n\nExample: (here I have `[\"<human>:\"\
          ]` inside the variable `self.model_conf['stop_words']`)\n\n```\n       \
          \     self.stop_words_ids = [\n                self.tokenizer(stop_word,\
          \ return_tensors=\"pt\")[\"input_ids\"].squeeze()\n                for stop_word\
          \ in self.model_conf[\"stop_words\"]\n            ]\n            self.stopping_criteria\
          \ = StoppingCriteriaList(\n                [StoppingCriteriaSub(stops=self.stop_words_ids)]\n\
          \            )\n```\n\nand then add this as parameter to your `generate()`\
          \ call:\n\n`stopping_criteria=self.stopping_criteria,`"
        updatedAt: '2023-06-14T11:53:30.908Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nik8482
    id: 6489aa27d0b2fd1f329a1e1e
    type: comment
  author: szeta
  content: "Add  \"`<human>:`\" as a stop word.\n\nExample: (here I have `[\"<human>:\"\
    ]` inside the variable `self.model_conf['stop_words']`)\n\n```\n            self.stop_words_ids\
    \ = [\n                self.tokenizer(stop_word, return_tensors=\"pt\")[\"input_ids\"\
    ].squeeze()\n                for stop_word in self.model_conf[\"stop_words\"]\n\
    \            ]\n            self.stopping_criteria = StoppingCriteriaList(\n \
    \               [StoppingCriteriaSub(stops=self.stop_words_ids)]\n           \
    \ )\n```\n\nand then add this as parameter to your `generate()` call:\n\n`stopping_criteria=self.stopping_criteria,`"
  created_at: 2023-06-14 10:53:11+00:00
  edited: true
  hidden: false
  id: 6489aa27d0b2fd1f329a1e1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
      fullname: Sven Heyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sven00
      type: user
    createdAt: '2023-06-16T08:33:53.000Z'
    data:
      edited: false
      editors:
      - Sven00
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7577821612358093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
          fullname: Sven Heyer
          isHf: false
          isPro: false
          name: Sven00
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;szeta&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/szeta\">@<span class=\"\
          underline\">szeta</span></a></span>\n\n\t</span></span> can you please provide\
          \ a step-by-step guide for less technical savvy folks? thank you</p>\n"
        raw: '@szeta can you please provide a step-by-step guide for less technical
          savvy folks? thank you'
        updatedAt: '2023-06-16T08:33:53.796Z'
      numEdits: 0
      reactions: []
    id: 648c1e713ec197b5e9a72e22
    type: comment
  author: Sven00
  content: '@szeta can you please provide a step-by-step guide for less technical
    savvy folks? thank you'
  created_at: 2023-06-16 07:33:53+00:00
  edited: false
  hidden: false
  id: 648c1e713ec197b5e9a72e22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
      fullname: Ralph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: szeta
      type: user
    createdAt: '2023-06-16T09:37:24.000Z'
    data:
      edited: false
      editors:
      - szeta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9731952548027039
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
          fullname: Ralph
          isHf: false
          isPro: false
          name: szeta
          type: user
        html: '<p>Sven i think we work on the same project. Let''s catch up. :-)</p>

          '
        raw: Sven i think we work on the same project. Let's catch up. :-)
        updatedAt: '2023-06-16T09:37:24.781Z'
      numEdits: 0
      reactions: []
    id: 648c2d54a912482f4a44dadd
    type: comment
  author: szeta
  content: Sven i think we work on the same project. Let's catch up. :-)
  created_at: 2023-06-16 08:37:24+00:00
  edited: false
  hidden: false
  id: 648c2d54a912482f4a44dadd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: togethercomputer/RedPajama-INCITE-7B-Chat
repo_type: model
status: open
target_branch: null
title: Open Ended Generation
