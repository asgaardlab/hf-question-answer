!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Karthik1611
conflicting_files: null
created_at: 2023-06-28 10:45:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19af38dc05560dbeae7b38ab43e4a01e.svg
      fullname: Karthik Bala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Karthik1611
      type: user
    createdAt: '2023-06-28T11:45:45.000Z'
    data:
      edited: false
      editors:
      - Karthik1611
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9286160469055176
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19af38dc05560dbeae7b38ab43e4a01e.svg
          fullname: Karthik Bala
          isHf: false
          isPro: false
          name: Karthik1611
          type: user
        html: '<p>I get the Keyerror: falcon when trying to load the model from huggingface
          hub directly. Can someone please help me in resolving this issue?</p>

          '
        raw: 'I get the Keyerror: falcon when trying to load the model from huggingface
          hub directly. Can someone please help me in resolving this issue?'
        updatedAt: '2023-06-28T11:45:45.908Z'
      numEdits: 0
      reactions: []
    id: 649c1d691d8893c7192fa419
    type: comment
  author: Karthik1611
  content: 'I get the Keyerror: falcon when trying to load the model from huggingface
    hub directly. Can someone please help me in resolving this issue?'
  created_at: 2023-06-28 10:45:45+00:00
  edited: false
  hidden: false
  id: 649c1d691d8893c7192fa419
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/19af38dc05560dbeae7b38ab43e4a01e.svg
      fullname: Karthik Bala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Karthik1611
      type: user
    createdAt: '2023-06-28T11:54:05.000Z'
    data:
      from: 'Keyerror: falcon when loading the model from the hub directly'
      to: 'Keyerror: falcon when loading the model from the hub directly using ctransformers'
    id: 649c1f5dcc0cc07dc95789a1
    type: title-change
  author: Karthik1611
  created_at: 2023-06-28 10:54:05+00:00
  id: 649c1f5dcc0cc07dc95789a1
  new_title: 'Keyerror: falcon when loading the model from the hub directly using
    ctransformers'
  old_title: 'Keyerror: falcon when loading the model from the hub directly'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-28T12:00:36.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8555687665939331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can''t load GGML models directly from transformers or with Hugging
          Face''s Text Generation Inference.</p>

          <p>To load Falcon GGMLs from Python code you should use <a rel="nofollow"
          href="https://github.com/marella/ctransformers">ctransformers</a>. It is
          designed to work in the same way as Transformers, but for GGML models.</p>

          '
        raw: 'You can''t load GGML models directly from transformers or with Hugging
          Face''s Text Generation Inference.


          To load Falcon GGMLs from Python code you should use [ctransformers](https://github.com/marella/ctransformers).
          It is designed to work in the same way as Transformers, but for GGML models.'
        updatedAt: '2023-06-28T12:01:09.598Z'
      numEdits: 1
      reactions: []
    id: 649c20e4dc167adfa85286c5
    type: comment
  author: TheBloke
  content: 'You can''t load GGML models directly from transformers or with Hugging
    Face''s Text Generation Inference.


    To load Falcon GGMLs from Python code you should use [ctransformers](https://github.com/marella/ctransformers).
    It is designed to work in the same way as Transformers, but for GGML models.'
  created_at: 2023-06-28 11:00:36+00:00
  edited: true
  hidden: false
  id: 649c20e4dc167adfa85286c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19af38dc05560dbeae7b38ab43e4a01e.svg
      fullname: Karthik Bala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Karthik1611
      type: user
    createdAt: '2023-06-30T09:00:06.000Z'
    data:
      status: closed
    id: 649e9996054717b1a395ab02
    type: status-change
  author: Karthik1611
  created_at: 2023-06-30 08:00:06+00:00
  id: 649e9996054717b1a395ab02
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/falcon-40b-instruct-GGML
repo_type: model
status: closed
target_branch: null
title: 'Keyerror: falcon when loading the model from the hub directly using ctransformers'
