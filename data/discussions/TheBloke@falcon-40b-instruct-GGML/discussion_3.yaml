!!python/object:huggingface_hub.community.DiscussionWithDetails
author: drago1717
conflicting_files: null
created_at: 2023-06-23 10:10:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f502bc9a4bbfb97a4951592404d016c5.svg
      fullname: rachid saidane
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: drago1717
      type: user
    createdAt: '2023-06-23T11:10:40.000Z'
    data:
      edited: false
      editors:
      - drago1717
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5108797550201416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f502bc9a4bbfb97a4951592404d016c5.svg
          fullname: rachid saidane
          isHf: false
          isPro: false
          name: drago1717
          type: user
        html: '<p>I tried to load falcon40b-instruct.ggmlv3.q8_0.bin directly into
          text-generation-webui but I got this error.<br>Is there a way to solve this
          problem?</p>

          <p>2023-06-23 13:07:29 INFO:llama.cpp weights detected: models/falcon40b-instruct.ggmlv3.q8_0.bin</p>

          <p>2023-06-23 13:07:29 INFO:Cache capacity is 0 bytes<br>llama.cpp: loading
          model from models/falcon40b-instruct.ggmlv3.q8_0.bin<br>error loading model:
          missing tok_embeddings.weight<br>llama_init_from_file: failed to load model<br>2023-06-23
          13:07:29 ERROR:Failed to load the model.<br>Traceback (most recent call
          last):<br>  File "/lhome/saidaner/text-generation-webui/server.py", line
          62, in load_model_wrapper<br>    shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>  File "/lhome/saidaner/text-generation-webui/modules/models.py",
          line 66, in load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>  File
          "/lhome/saidaner/text-generation-webui/modules/models.py", line 247, in
          llamacpp_loader<br>    model, tokenizer = LlamaCppModel.from_pretrained(model_file)<br>  File
          "/lhome/saidaner/text-generation-webui/modules/llamacpp_model.py", line
          55, in from_pretrained<br>    result.model = Llama(**params)<br>  File "/lhome/saidaner/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py",
          line 287, in <strong>init</strong><br>    assert self.ctx is not None<br>AssertionError</p>

          <p>Exception ignored in: &lt;function LlamaCppModel.__del__ at 0x155414974c10&gt;<br>Traceback
          (most recent call last):<br>  File "/lhome/saidaner/text-generation-webui/modules/llamacpp_model.py",
          line 29, in <strong>del</strong><br>    self.model.<strong>del</strong>()<br>AttributeError:
          ''LlamaCppModel'' object has no attribute ''model''</p>

          '
        raw: "I tried to load falcon40b-instruct.ggmlv3.q8_0.bin directly into text-generation-webui\
          \ but I got this error.\r\nIs there a way to solve this problem?\r\n\r\n\
          2023-06-23 13:07:29 INFO:llama.cpp weights detected: models/falcon40b-instruct.ggmlv3.q8_0.bin\r\
          \n\r\n2023-06-23 13:07:29 INFO:Cache capacity is 0 bytes\r\nllama.cpp: loading\
          \ model from models/falcon40b-instruct.ggmlv3.q8_0.bin\r\nerror loading\
          \ model: missing tok_embeddings.weight\r\nllama_init_from_file: failed to\
          \ load model\r\n2023-06-23 13:07:29 ERROR:Failed to load the model.\r\n\
          Traceback (most recent call last):\r\n  File \"/lhome/saidaner/text-generation-webui/server.py\"\
          , line 62, in load_model_wrapper\r\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\r\n  File \"/lhome/saidaner/text-generation-webui/modules/models.py\"\
          , line 66, in load_model\r\n    output = load_func_map[loader](model_name)\r\
          \n  File \"/lhome/saidaner/text-generation-webui/modules/models.py\", line\
          \ 247, in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n  File \"/lhome/saidaner/text-generation-webui/modules/llamacpp_model.py\"\
          , line 55, in from_pretrained\r\n    result.model = Llama(**params)\r\n\
          \  File \"/lhome/saidaner/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
          , line 287, in __init__\r\n    assert self.ctx is not None\r\nAssertionError\r\
          \n\r\nException ignored in: <function LlamaCppModel.__del__ at 0x155414974c10>\r\
          \nTraceback (most recent call last):\r\n  File \"/lhome/saidaner/text-generation-webui/modules/llamacpp_model.py\"\
          , line 29, in __del__\r\n    self.model.__del__()\r\nAttributeError: 'LlamaCppModel'\
          \ object has no attribute 'model'\r\n"
        updatedAt: '2023-06-23T11:10:40.361Z'
      numEdits: 0
      reactions: []
    id: 64957db0ecc232fffde6fb21
    type: comment
  author: drago1717
  content: "I tried to load falcon40b-instruct.ggmlv3.q8_0.bin directly into text-generation-webui\
    \ but I got this error.\r\nIs there a way to solve this problem?\r\n\r\n2023-06-23\
    \ 13:07:29 INFO:llama.cpp weights detected: models/falcon40b-instruct.ggmlv3.q8_0.bin\r\
    \n\r\n2023-06-23 13:07:29 INFO:Cache capacity is 0 bytes\r\nllama.cpp: loading\
    \ model from models/falcon40b-instruct.ggmlv3.q8_0.bin\r\nerror loading model:\
    \ missing tok_embeddings.weight\r\nllama_init_from_file: failed to load model\r\
    \n2023-06-23 13:07:29 ERROR:Failed to load the model.\r\nTraceback (most recent\
    \ call last):\r\n  File \"/lhome/saidaner/text-generation-webui/server.py\", line\
    \ 62, in load_model_wrapper\r\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\n  File \"/lhome/saidaner/text-generation-webui/modules/models.py\"\
    , line 66, in load_model\r\n    output = load_func_map[loader](model_name)\r\n\
    \  File \"/lhome/saidaner/text-generation-webui/modules/models.py\", line 247,\
    \ in llamacpp_loader\r\n    model, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n  File \"/lhome/saidaner/text-generation-webui/modules/llamacpp_model.py\",\
    \ line 55, in from_pretrained\r\n    result.model = Llama(**params)\r\n  File\
    \ \"/lhome/saidaner/miniconda3/envs/textgen/lib/python3.10/site-packages/llama_cpp/llama.py\"\
    , line 287, in __init__\r\n    assert self.ctx is not None\r\nAssertionError\r\
    \n\r\nException ignored in: <function LlamaCppModel.__del__ at 0x155414974c10>\r\
    \nTraceback (most recent call last):\r\n  File \"/lhome/saidaner/text-generation-webui/modules/llamacpp_model.py\"\
    , line 29, in __del__\r\n    self.model.__del__()\r\nAttributeError: 'LlamaCppModel'\
    \ object has no attribute 'model'\r\n"
  created_at: 2023-06-23 10:10:40+00:00
  edited: false
  hidden: false
  id: 64957db0ecc232fffde6fb21
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-06-23T11:12:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8085352182388306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Please see the README. text-generation-webui does not support Falcon
          GGML at this time.</p>

          <p>You can try <a rel="nofollow" href="https://github.com/ParisNeo/lollms-webui">LoLLMS
          Web UI</a> which just added support for Falcon GGML.</p>

          '
        raw: 'Please see the README. text-generation-webui does not support Falcon
          GGML at this time.


          You can try [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui) which
          just added support for Falcon GGML.'
        updatedAt: '2023-06-23T11:12:00.555Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - drago1717
        - iHaag
    id: 64957e007cf6d684c4634b63
    type: comment
  author: TheBloke
  content: 'Please see the README. text-generation-webui does not support Falcon GGML
    at this time.


    You can try [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui) which just
    added support for Falcon GGML.'
  created_at: 2023-06-23 10:12:00+00:00
  edited: false
  hidden: false
  id: 64957e007cf6d684c4634b63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30c59d1d488116d6db1c43b749fa5422.svg
      fullname: Wouter Tichelaar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Azamorn
      type: user
    createdAt: '2023-07-05T19:35:13.000Z'
    data:
      edited: false
      editors:
      - Azamorn
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6388750672340393
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30c59d1d488116d6db1c43b749fa5422.svg
          fullname: Wouter Tichelaar
          isHf: false
          isPro: false
          name: Azamorn
          type: user
        html: '<p>There is a pull request to let it work in text-generation-webui,
          see here<br><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/pull/2828">https://github.com/oobabooga/text-generation-webui/pull/2828</a></p>

          '
        raw: 'There is a pull request to let it work in text-generation-webui, see
          here

          https://github.com/oobabooga/text-generation-webui/pull/2828'
        updatedAt: '2023-07-05T19:35:13.752Z'
      numEdits: 0
      reactions: []
    id: 64a5c5f1d3838b4a0038ead7
    type: comment
  author: Azamorn
  content: 'There is a pull request to let it work in text-generation-webui, see here

    https://github.com/oobabooga/text-generation-webui/pull/2828'
  created_at: 2023-07-05 18:35:13+00:00
  edited: false
  hidden: false
  id: 64a5c5f1d3838b4a0038ead7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T09:15:32.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9684032797813416
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Azamorn&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Azamorn\">@<span class=\"\
          underline\">Azamorn</span></a></span>\n\n\t</span></span> interesting, thanks\
          \ for the link.  That's for a different model backend. It won't support\
          \ this Falcon model but would support other Falcon models made with CTranslate2.\
          \ I will keep an eye on that and if it's merged I will look at some making\
          \ CTranslate2 models.</p>\n"
        raw: '@Azamorn interesting, thanks for the link.  That''s for a different
          model backend. It won''t support this Falcon model but would support other
          Falcon models made with CTranslate2. I will keep an eye on that and if it''s
          merged I will look at some making CTranslate2 models.'
        updatedAt: '2023-07-07T09:15:32.475Z'
      numEdits: 0
      reactions: []
    id: 64a7d7b467df7ef7a783b8ad
    type: comment
  author: TheBloke
  content: '@Azamorn interesting, thanks for the link.  That''s for a different model
    backend. It won''t support this Falcon model but would support other Falcon models
    made with CTranslate2. I will keep an eye on that and if it''s merged I will look
    at some making CTranslate2 models.'
  created_at: 2023-07-07 08:15:32+00:00
  edited: false
  hidden: false
  id: 64a7d7b467df7ef7a783b8ad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/falcon-40b-instruct-GGML
repo_type: model
status: open
target_branch: null
title: 'I tried to load falcon40b-instruct.ggmlv3.q8_0.bin directly into text-generation-webui
  but I got this error '
