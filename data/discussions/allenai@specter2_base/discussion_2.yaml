!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cormak
conflicting_files: null
created_at: 2023-04-18 09:12:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58b96b631094d71d01a06c1468d58bc0.svg
      fullname: Jean-Baptiste
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cormak
      type: user
    createdAt: '2023-04-18T10:12:00.000Z'
    data:
      edited: false
      editors:
      - cormak
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58b96b631094d71d01a06c1468d58bc0.svg
          fullname: Jean-Baptiste
          isHf: false
          isPro: false
          name: cormak
          type: user
        html: '<p>Hello AllenAI, thank you for the great release,<br>I am highly interested
          in Specter, as to my knowledge it is one of the best models to do academic
          document embedding representation. This is probably due to the reference-aware
          triplet loss, which I think brings desirable properties to the generated
          embeddings. </p>

          <p>However, I wonder if using SciBERT as a basis is still a good idea, the
          model is getting a bit old, and I think the vocabulary would also benefit
          from an update (I think it did not change since the original SciBERT paper
          from 2019) so, for instance, there is no token about coronavirus. Also,
          limiting the number of numeric tokens is maybe a good idea. For instance,
          I noted some tokens, such as "30)", "[48]", "1981", "##/10.100", ... those
          actually take a sizeable part of the vocabulary and are maybe not very useful
          in explaining the final model accuracy.</p>

          <p>Thanks again for the great contribution to the academic NLP community,
          and I hope my suggestions are useful.</p>

          '
        raw: "Hello AllenAI, thank you for the great release,\r\nI am highly interested\
          \ in Specter, as to my knowledge it is one of the best models to do academic\
          \ document embedding representation. This is probably due to the reference-aware\
          \ triplet loss, which I think brings desirable properties to the generated\
          \ embeddings. \r\n\r\nHowever, I wonder if using SciBERT as a basis is still\
          \ a good idea, the model is getting a bit old, and I think the vocabulary\
          \ would also benefit from an update (I think it did not change since the\
          \ original SciBERT paper from 2019) so, for instance, there is no token\
          \ about coronavirus. Also, limiting the number of numeric tokens is maybe\
          \ a good idea. For instance, I noted some tokens, such as \"30)\", \"[48]\"\
          , \"1981\", \"##/10.100\", ... those actually take a sizeable part of the\
          \ vocabulary and are maybe not very useful in explaining the final model\
          \ accuracy.\r\n\r\nThanks again for the great contribution to the academic\
          \ NLP community, and I hope my suggestions are useful."
        updatedAt: '2023-04-18T10:12:00.493Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - pathwaymedical
    id: 643e6cf0e08e86aec10ed463
    type: comment
  author: cormak
  content: "Hello AllenAI, thank you for the great release,\r\nI am highly interested\
    \ in Specter, as to my knowledge it is one of the best models to do academic document\
    \ embedding representation. This is probably due to the reference-aware triplet\
    \ loss, which I think brings desirable properties to the generated embeddings.\
    \ \r\n\r\nHowever, I wonder if using SciBERT as a basis is still a good idea,\
    \ the model is getting a bit old, and I think the vocabulary would also benefit\
    \ from an update (I think it did not change since the original SciBERT paper from\
    \ 2019) so, for instance, there is no token about coronavirus. Also, limiting\
    \ the number of numeric tokens is maybe a good idea. For instance, I noted some\
    \ tokens, such as \"30)\", \"[48]\", \"1981\", \"##/10.100\", ... those actually\
    \ take a sizeable part of the vocabulary and are maybe not very useful in explaining\
    \ the final model accuracy.\r\n\r\nThanks again for the great contribution to\
    \ the academic NLP community, and I hope my suggestions are useful."
  created_at: 2023-04-18 09:12:00+00:00
  edited: false
  hidden: false
  id: 643e6cf0e08e86aec10ed463
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/58b96b631094d71d01a06c1468d58bc0.svg
      fullname: Jean-Baptiste
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cormak
      type: user
    createdAt: '2023-09-07T09:48:29.000Z'
    data:
      edited: false
      editors:
      - cormak
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9509058594703674
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/58b96b631094d71d01a06c1468d58bc0.svg
          fullname: Jean-Baptiste
          isHf: false
          isPro: false
          name: cormak
          type: user
        html: '<p>Hello,<br>Thank you very much for listening to my suggestion, I
          will try the refreshed version ASAP.</p>

          <p>Best regards,</p>

          '
        raw: 'Hello,

          Thank you very much for listening to my suggestion, I will try the refreshed
          version ASAP.


          Best regards,


          '
        updatedAt: '2023-09-07T09:48:29.539Z'
      numEdits: 0
      reactions: []
    id: 64f99c6d2e0cda71e7b3d942
    type: comment
  author: cormak
  content: 'Hello,

    Thank you very much for listening to my suggestion, I will try the refreshed version
    ASAP.


    Best regards,


    '
  created_at: 2023-09-07 08:48:29+00:00
  edited: false
  hidden: false
  id: 64f99c6d2e0cda71e7b3d942
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: allenai/specter2_base
repo_type: model
status: open
target_branch: null
title: Is using SciBERT as a basis still a good idea?
