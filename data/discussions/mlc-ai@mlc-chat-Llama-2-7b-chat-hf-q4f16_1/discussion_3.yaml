!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kishorep1
conflicting_files: null
created_at: 2023-09-13 15:43:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24807ae2c8ed4dfe7d3e90cabd321ba0.svg
      fullname: Kishore Prakash
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kishorep1
      type: user
    createdAt: '2023-09-13T16:43:12.000Z'
    data:
      edited: false
      editors:
      - kishorep1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4030959904193878
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24807ae2c8ed4dfe7d3e90cabd321ba0.svg
          fullname: Kishore Prakash
          isHf: false
          isPro: false
          name: kishorep1
          type: user
        html: '<p>I was trying this collab notebook out: <a rel="nofollow" href="https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb</a></p>

          <p>and we are getting this error when we run the first code cell under Let''s
          Chat:</p>

          <hr>

          <p>InternalError                             Traceback (most recent call
          last)<br> in &lt;cell line: 4&gt;()<br>      2 from mlc_chat.callback import
          StreamToStdout<br>      3<br>----&gt; 4 cm = ChatModule(model="mlc-chat-Llama-2-7b-chat-hf-q4f16_1")</p>

          <p>3 frames<br>tvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.PackedFuncBase.<strong>call</strong>()</p>

          <p>tvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall()</p>

          <p>tvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall3()</p>

          <p>tvm/_ffi/_cython/./base.pxi in tvm._ffi._cy3.core.CHECK_CALL()</p>

          <p>/workspace/mlc-llm/cpp/llm_chat.cc in mlc::llm::LLMChatModule::GetFunction(tvm::runtime::String
          const&amp;, tvm::runtime::ObjectPtr<a rel="nofollow">tvm::runtime::Object</a>
          const&amp;)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs,
          tvm::runtime::TVMRetValue*) const()</p>

          <p>InternalError: Traceback (most recent call last):<br>  3: mlc::llm::LLMChatModule::GetFunction(tvm::runtime::String
          const&amp;, tvm::runtime::ObjectPtr<a rel="nofollow">tvm::runtime::Object</a>
          const&amp;)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs,
          tvm::runtime::TVMRetValue*) const<br>        at /workspace/mlc-llm/cpp/llm_chat.cc:1164<br>  2:
          tvm::runtime::PackedFuncValueConverter<a rel="nofollow">tvm::runtime::String</a>::From(tvm::runtime::TVMArgValue
          const&amp;)<br>  1: tvm::runtime::TVMArgValue::operator std::__cxx11::basic_string&lt;char,
          std::char_traits, std::allocator &gt;() const<br>  0: _ZN3tvm7runtime6deta<br>  File
          "/workspace/tvm/include/tvm/runtime/packed_func.h", line 683<br>InternalError:
          Check failed: (IsObjectRef<a rel="nofollow">tvm::runtime::String</a>())
          is false: Could not convert TVM object of type runtime.Closure to a string.</p>

          '
        raw: "I was trying this collab notebook out: https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\r\
          \n\r\nand we are getting this error when we run the first code cell under\
          \ Let's Chat:\r\n\r\n---------------------------------------------------------------------------\r\
          \nInternalError                             Traceback (most recent call\
          \ last)\r\n<ipython-input-2-f2f969a533aa> in <cell line: 4>()\r\n      2\
          \ from mlc_chat.callback import StreamToStdout\r\n      3 \r\n----> 4 cm\
          \ = ChatModule(model=\"mlc-chat-Llama-2-7b-chat-hf-q4f16_1\")\r\n\r\n3 frames\r\
          \ntvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.PackedFuncBase.__call__()\r\
          \n\r\ntvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall()\r\
          \n\r\ntvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall3()\r\
          \n\r\ntvm/_ffi/_cython/./base.pxi in tvm._ffi._cy3.core.CHECK_CALL()\r\n\
          \r\n/workspace/mlc-llm/cpp/llm_chat.cc in mlc::llm::LLMChatModule::GetFunction(tvm::runtime::String\
          \ const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs,\
          \ tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\
          \ const()\r\n\r\nInternalError: Traceback (most recent call last):\r\n \
          \ 3: mlc::llm::LLMChatModule::GetFunction(tvm::runtime::String const&, tvm::runtime::ObjectPtr<tvm::runtime::Object>\
          \ const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs,\
          \ tvm::runtime::TVMRetValue*) const\r\n        at /workspace/mlc-llm/cpp/llm_chat.cc:1164\r\
          \n  2: tvm::runtime::PackedFuncValueConverter<tvm::runtime::String>::From(tvm::runtime::TVMArgValue\
          \ const&)\r\n  1: tvm::runtime::TVMArgValue::operator std::__cxx11::basic_string<char,\
          \ std::char_traits<char>, std::allocator<char> >() const\r\n  0: _ZN3tvm7runtime6deta\r\
          \n  File \"/workspace/tvm/include/tvm/runtime/packed_func.h\", line 683\r\
          \nInternalError: Check failed: (IsObjectRef<tvm::runtime::String>()) is\
          \ false: Could not convert TVM object of type runtime.Closure to a string."
        updatedAt: '2023-09-13T16:43:12.543Z'
      numEdits: 0
      reactions: []
    id: 6501e6a0ca579f83bb58a1a1
    type: comment
  author: kishorep1
  content: "I was trying this collab notebook out: https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb\r\
    \n\r\nand we are getting this error when we run the first code cell under Let's\
    \ Chat:\r\n\r\n---------------------------------------------------------------------------\r\
    \nInternalError                             Traceback (most recent call last)\r\
    \n<ipython-input-2-f2f969a533aa> in <cell line: 4>()\r\n      2 from mlc_chat.callback\
    \ import StreamToStdout\r\n      3 \r\n----> 4 cm = ChatModule(model=\"mlc-chat-Llama-2-7b-chat-hf-q4f16_1\"\
    )\r\n\r\n3 frames\r\ntvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.PackedFuncBase.__call__()\r\
    \n\r\ntvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall()\r\n\r\
    \ntvm/_ffi/_cython/./packed_func.pxi in tvm._ffi._cy3.core.FuncCall3()\r\n\r\n\
    tvm/_ffi/_cython/./base.pxi in tvm._ffi._cy3.core.CHECK_CALL()\r\n\r\n/workspace/mlc-llm/cpp/llm_chat.cc\
    \ in mlc::llm::LLMChatModule::GetFunction(tvm::runtime::String const&, tvm::runtime::ObjectPtr<tvm::runtime::Object>\
    \ const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs,\
    \ tvm::runtime::TVMRetValue*) const()\r\n\r\nInternalError: Traceback (most recent\
    \ call last):\r\n  3: mlc::llm::LLMChatModule::GetFunction(tvm::runtime::String\
    \ const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs,\
    \ tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\
    \ const\r\n        at /workspace/mlc-llm/cpp/llm_chat.cc:1164\r\n  2: tvm::runtime::PackedFuncValueConverter<tvm::runtime::String>::From(tvm::runtime::TVMArgValue\
    \ const&)\r\n  1: tvm::runtime::TVMArgValue::operator std::__cxx11::basic_string<char,\
    \ std::char_traits<char>, std::allocator<char> >() const\r\n  0: _ZN3tvm7runtime6deta\r\
    \n  File \"/workspace/tvm/include/tvm/runtime/packed_func.h\", line 683\r\nInternalError:\
    \ Check failed: (IsObjectRef<tvm::runtime::String>()) is false: Could not convert\
    \ TVM object of type runtime.Closure to a string."
  created_at: 2023-09-13 15:43:12+00:00
  edited: false
  hidden: false
  id: 6501e6a0ca579f83bb58a1a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
      fullname: Charlie Ruan
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: CharlieFRuan
      type: user
    createdAt: '2023-09-15T23:28:41.000Z'
    data:
      edited: false
      editors:
      - CharlieFRuan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9568333029747009
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57567fa8f741e2f177dbbd3a5ac8ff32.svg
          fullname: Charlie Ruan
          isHf: false
          isPro: false
          name: CharlieFRuan
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kishorep1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kishorep1\"\
          >@<span class=\"underline\">kishorep1</span></a></span>\n\n\t</span></span>,\
          \ thanks for raising the issue! This should be fixed now, could you retry\
          \ and let us know?</p>\n"
        raw: Hi @kishorep1, thanks for raising the issue! This should be fixed now,
          could you retry and let us know?
        updatedAt: '2023-09-15T23:28:41.104Z'
      numEdits: 0
      reactions: []
    id: 6504e8a97de4710bafb42a27
    type: comment
  author: CharlieFRuan
  content: Hi @kishorep1, thanks for raising the issue! This should be fixed now,
    could you retry and let us know?
  created_at: 2023-09-15 22:28:41+00:00
  edited: false
  hidden: false
  id: 6504e8a97de4710bafb42a27
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
repo_type: model
status: open
target_branch: null
title: Facing issue with cm = ChatModule(model="mlc-chat-Llama-2-7b-chat-hf-q4f16_1")
