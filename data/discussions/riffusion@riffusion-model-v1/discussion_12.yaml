!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jerobich
conflicting_files: null
created_at: 2022-12-22 16:12:30+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f3b6d6751d644be17df6efae4f773cd8.svg
      fullname: Jean-Philippe Robichaud
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jerobich
      type: user
    createdAt: '2022-12-22T16:12:30.000Z'
    data:
      edited: false
      editors:
      - jerobich
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f3b6d6751d644be17df6efae4f773cd8.svg
          fullname: Jean-Philippe Robichaud
          isHf: false
          isPro: false
          name: jerobich
          type: user
        html: '<p>Thanks for this great experiment, it is thought-provoking!</p>

          <p>I would be interested in pushing the limits a bit, seeing how we can
          produce "events" that last less than 10 seconds.  Saying like : groovy rock
          music with a 4 seconds sax reel".  I''m curious how we could deal with ambiophonic
          events, like dog barking or bird signing?  What do you suggest we do if
          we have smaller audio clips, (like a 2 sec bird song)?  Should we just loop
          the sound to reach 10 secs or pad the audio with silence?</p>

          <p>What if we''re trying to capture something that exceed 10 secs, like
          long fog horn? I understand the 10 secs is a choice you made to match the
          image size, but once we pick a limit, we''re kind of stuck with it...</p>

          <p>Finally, any chance you would share your fine-tuning setup?  It would
          save us lots of time to try to push the envelope of what you''ve accomplished!</p>

          <p>Thanks again!</p>

          '
        raw: "Thanks for this great experiment, it is thought-provoking!\r\n\r\nI\
          \ would be interested in pushing the limits a bit, seeing how we can produce\
          \ \"events\" that last less than 10 seconds.  Saying like : groovy rock\
          \ music with a 4 seconds sax reel\".  I'm curious how we could deal with\
          \ ambiophonic events, like dog barking or bird signing?  What do you suggest\
          \ we do if we have smaller audio clips, (like a 2 sec bird song)?  Should\
          \ we just loop the sound to reach 10 secs or pad the audio with silence?\r\
          \n\r\nWhat if we're trying to capture something that exceed 10 secs, like\
          \ long fog horn? I understand the 10 secs is a choice you made to match\
          \ the image size, but once we pick a limit, we're kind of stuck with it...\r\
          \n\r\nFinally, any chance you would share your fine-tuning setup?  It would\
          \ save us lots of time to try to push the envelope of what you've accomplished!\r\
          \n\r\nThanks again!\r\n\r\n\r\n\r\n"
        updatedAt: '2022-12-22T16:12:30.675Z'
      numEdits: 0
      reactions: []
    id: 63a481ee27f1f64ed7255350
    type: comment
  author: jerobich
  content: "Thanks for this great experiment, it is thought-provoking!\r\n\r\nI would\
    \ be interested in pushing the limits a bit, seeing how we can produce \"events\"\
    \ that last less than 10 seconds.  Saying like : groovy rock music with a 4 seconds\
    \ sax reel\".  I'm curious how we could deal with ambiophonic events, like dog\
    \ barking or bird signing?  What do you suggest we do if we have smaller audio\
    \ clips, (like a 2 sec bird song)?  Should we just loop the sound to reach 10\
    \ secs or pad the audio with silence?\r\n\r\nWhat if we're trying to capture something\
    \ that exceed 10 secs, like long fog horn? I understand the 10 secs is a choice\
    \ you made to match the image size, but once we pick a limit, we're kind of stuck\
    \ with it...\r\n\r\nFinally, any chance you would share your fine-tuning setup?\
    \  It would save us lots of time to try to push the envelope of what you've accomplished!\r\
    \n\r\nThanks again!\r\n\r\n\r\n\r\n"
  created_at: 2022-12-22 16:12:30+00:00
  edited: false
  hidden: false
  id: 63a481ee27f1f64ed7255350
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/972272987cbb3ef75b94c6ef9f7677dc.svg
      fullname: James Maxwell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jbmaxwell
      type: user
    createdAt: '2023-01-15T20:02:35.000Z'
    data:
      edited: false
      editors:
      - jbmaxwell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/972272987cbb3ef75b94c6ef9f7677dc.svg
          fullname: James Maxwell
          isHf: false
          isPro: false
          name: jbmaxwell
          type: user
        html: "<p>I can get fine-tuning to run just using the <code>examples/train_text_to_image.py</code>\
          \ script. You do need to format your data, but that's in the HF docs so\
          \ not a huge issue.<br>In my case I had to download the riffusion model\u2014\
          I was getting errors when trying to get it directly from HF (i.e., in code).\
          \ So I used git to download a local copy (you need <code>lfs</code>).</p>\n\
          <p>The problem I'm having is that, when trying to run the model, I get a\
          \ type error: <code>RuntimeError: Input type (c10::Half) and bias type (float)\
          \ should be the same</code><br>This is happening in <code>F.conv2d()</code>.\
          \ Full trace:</p>\n<pre><code>Traceback (most recent call last):\n  File\
          \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py\"\
          , line 565, in _run_script\n    exec(code, module.__dict__)\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/pages/text_to_audio.py\"\
          , line 102, in &lt;module&gt;\n    render_text_to_audio()\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/pages/text_to_audio.py\"\
          , line 78, in render_text_to_audio\n    image = streamlit_util.run_txt2img(\n\
          \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 428, in wrapper\n    return get_or_create_cached_value()\n  File\
          \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 401, in get_or_create_cached_value\n    return_value = func(*args,\
          \ **kwargs)\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/util.py\"\
          , line 103, in run_txt2img\n    output = pipeline(\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\"\
          , line 531, in __call__\n    noise_pred = self.unet(latent_model_input,\
          \ t, encoder_hidden_states=text_embeddings).sample\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py\"\
          , line 421, in forward\n    sample = self.conv_in(sample)\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py\"\
          , line 463, in forward\n    return self._conv_forward(input, self.weight,\
          \ self.bias)\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py\"\
          , line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n\
          RuntimeError: Input type (c10::Half) and bias type (float) should be the\
          \ same\n</code></pre>\n<p>Any thoughts as to what might be going on?</p>\n"
        raw: "I can get fine-tuning to run just using the `examples/train_text_to_image.py`\
          \ script. You do need to format your data, but that's in the HF docs so\
          \ not a huge issue.\nIn my case I had to download the riffusion model\u2014\
          I was getting errors when trying to get it directly from HF (i.e., in code).\
          \ So I used git to download a local copy (you need `lfs`).\n\nThe problem\
          \ I'm having is that, when trying to run the model, I get a type error:\
          \ `RuntimeError: Input type (c10::Half) and bias type (float) should be\
          \ the same`\nThis is happening in `F.conv2d()`. Full trace:\n\n```\nTraceback\
          \ (most recent call last):\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py\"\
          , line 565, in _run_script\n    exec(code, module.__dict__)\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/pages/text_to_audio.py\"\
          , line 102, in <module>\n    render_text_to_audio()\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/pages/text_to_audio.py\"\
          , line 78, in render_text_to_audio\n    image = streamlit_util.run_txt2img(\n\
          \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 428, in wrapper\n    return get_or_create_cached_value()\n  File\
          \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py\"\
          , line 401, in get_or_create_cached_value\n    return_value = func(*args,\
          \ **kwargs)\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/util.py\"\
          , line 103, in run_txt2img\n    output = pipeline(\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\"\
          , line 531, in __call__\n    noise_pred = self.unet(latent_model_input,\
          \ t, encoder_hidden_states=text_embeddings).sample\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py\"\
          , line 421, in forward\n    sample = self.conv_in(sample)\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
          , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n\
          \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py\"\
          , line 463, in forward\n    return self._conv_forward(input, self.weight,\
          \ self.bias)\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py\"\
          , line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n\
          RuntimeError: Input type (c10::Half) and bias type (float) should be the\
          \ same\n```\n\nAny thoughts as to what might be going on?"
        updatedAt: '2023-01-15T20:02:35.031Z'
      numEdits: 0
      reactions: []
    id: 63c45bdb6e793fba087ffecf
    type: comment
  author: jbmaxwell
  content: "I can get fine-tuning to run just using the `examples/train_text_to_image.py`\
    \ script. You do need to format your data, but that's in the HF docs so not a\
    \ huge issue.\nIn my case I had to download the riffusion model\u2014I was getting\
    \ errors when trying to get it directly from HF (i.e., in code). So I used git\
    \ to download a local copy (you need `lfs`).\n\nThe problem I'm having is that,\
    \ when trying to run the model, I get a type error: `RuntimeError: Input type\
    \ (c10::Half) and bias type (float) should be the same`\nThis is happening in\
    \ `F.conv2d()`. Full trace:\n\n```\nTraceback (most recent call last):\n  File\
    \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py\"\
    , line 565, in _run_script\n    exec(code, module.__dict__)\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/pages/text_to_audio.py\"\
    , line 102, in <module>\n    render_text_to_audio()\n  File \"/home/james/src/somms/riffusion/riffusion/streamlit/pages/text_to_audio.py\"\
    , line 78, in render_text_to_audio\n    image = streamlit_util.run_txt2img(\n\
    \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 428, in wrapper\n    return get_or_create_cached_value()\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/streamlit/runtime/caching/cache_utils.py\"\
    , line 401, in get_or_create_cached_value\n    return_value = func(*args, **kwargs)\n\
    \  File \"/home/james/src/somms/riffusion/riffusion/streamlit/util.py\", line\
    \ 103, in run_txt2img\n    output = pipeline(\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\"\
    , line 531, in __call__\n    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\
    \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py\"\
    , line 421, in forward\n    sample = self.conv_in(sample)\n  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/module.py\"\
    , line 1194, in _call_impl\n    return forward_call(*input, **kwargs)\n  File\
    \ \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py\"\
    , line 463, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n\
    \  File \"/home/james/anaconda3/envs/riffusion/lib/python3.9/site-packages/torch/nn/modules/conv.py\"\
    , line 459, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\n\
    RuntimeError: Input type (c10::Half) and bias type (float) should be the same\n\
    ```\n\nAny thoughts as to what might be going on?"
  created_at: 2023-01-15 20:02:35+00:00
  edited: false
  hidden: false
  id: 63c45bdb6e793fba087ffecf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/972272987cbb3ef75b94c6ef9f7677dc.svg
      fullname: James Maxwell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jbmaxwell
      type: user
    createdAt: '2023-01-15T20:37:43.000Z'
    data:
      edited: false
      editors:
      - jbmaxwell
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/972272987cbb3ef75b94c6ef9f7677dc.svg
          fullname: James Maxwell
          isHf: false
          isPro: false
          name: jbmaxwell
          type: user
        html: '<p>Ah, false alarm. I had hacked some stuff yesterday while trying
          to get things working... removing my hacks it works fine.</p>

          '
        raw: Ah, false alarm. I had hacked some stuff yesterday while trying to get
          things working... removing my hacks it works fine.
        updatedAt: '2023-01-15T20:37:43.032Z'
      numEdits: 0
      reactions: []
    id: 63c4641786485272992d98b3
    type: comment
  author: jbmaxwell
  content: Ah, false alarm. I had hacked some stuff yesterday while trying to get
    things working... removing my hacks it works fine.
  created_at: 2023-01-15 20:37:43+00:00
  edited: false
  hidden: false
  id: 63c4641786485272992d98b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2171c33d1c47882db1ced45c1daa6215.svg
      fullname: dusty
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dustyatx
      type: user
    createdAt: '2023-01-17T23:31:36.000Z'
    data:
      edited: false
      editors:
      - dustyatx
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2171c33d1c47882db1ced45c1daa6215.svg
          fullname: dusty
          isHf: false
          isPro: false
          name: dustyatx
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jbmaxwell&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jbmaxwell\">@<span class=\"\
          underline\">jbmaxwell</span></a></span>\n\n\t</span></span> I'm working\
          \ on the same thing.. What's a good way to connect with you so we can share\
          \ what we learn along the way?</p>\n"
        raw: '@jbmaxwell I''m working on the same thing.. What''s a good way to connect
          with you so we can share what we learn along the way?'
        updatedAt: '2023-01-17T23:31:36.864Z'
      numEdits: 0
      reactions: []
    id: 63c72fd8523e3884e33b767c
    type: comment
  author: dustyatx
  content: '@jbmaxwell I''m working on the same thing.. What''s a good way to connect
    with you so we can share what we learn along the way?'
  created_at: 2023-01-17 23:31:36+00:00
  edited: false
  hidden: false
  id: 63c72fd8523e3884e33b767c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: riffusion/riffusion-model-v1
repo_type: model
status: open
target_branch: null
title: Finetuning details/code
