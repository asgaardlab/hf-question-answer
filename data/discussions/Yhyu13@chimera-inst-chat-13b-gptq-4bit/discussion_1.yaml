!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kikeavi36
conflicting_files: null
created_at: 2023-05-24 15:00:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf9b112fb8c4b7142b05b734bb98479d.svg
      fullname: Enrique jimenez sanchez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kikeavi36
      type: user
    createdAt: '2023-05-24T16:00:34.000Z'
    data:
      edited: false
      editors:
      - kikeavi36
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf9b112fb8c4b7142b05b734bb98479d.svg
          fullname: Enrique jimenez sanchez
          isHf: false
          isPro: false
          name: kikeavi36
          type: user
        html: '<p>Hello, can we talk by email or something similar?</p>

          '
        raw: Hello, can we talk by email or something similar?
        updatedAt: '2023-05-24T16:00:34.731Z'
      numEdits: 0
      reactions: []
    id: 646e34a23f1812ab1bc9e0af
    type: comment
  author: kikeavi36
  content: Hello, can we talk by email or something similar?
  created_at: 2023-05-24 15:00:34+00:00
  edited: false
  hidden: false
  id: 646e34a23f1812ab1bc9e0af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-25T02:50:09.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Sure, contact me at <a rel="nofollow" href="mailto:yohan680919@163.com">yohan680919@163.com</a></p>

          '
        raw: Sure, contact me at yohan680919@163.com
        updatedAt: '2023-05-25T02:50:09.288Z'
      numEdits: 0
      reactions: []
      relatedEventId: 646ecce11a427e26304722f3
    id: 646ecce11a427e26304722f2
    type: comment
  author: Yhyu13
  content: Sure, contact me at yohan680919@163.com
  created_at: 2023-05-25 01:50:09+00:00
  edited: false
  hidden: false
  id: 646ecce11a427e26304722f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-25T02:50:09.000Z'
    data:
      status: closed
    id: 646ecce11a427e26304722f3
    type: status-change
  author: Yhyu13
  created_at: 2023-05-25 01:50:09+00:00
  id: 646ecce11a427e26304722f3
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-05-25T10:10:12.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p>Just in case you have not received my replay:</p>\n<p>Let me briefly\
          \ setup the context here:</p>\n<p>1,<br>The Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-gptq-4bit\
          \ was made compatible with No actor order so that it is compatible with\
          \ Ooba's GPTQ fork which lag behind w/o feature support for actor order<br>In\
          \ practice, you can load the gptq-4bit model with either Ooba's fork or\
          \ with the latest cuda branch of GPTQ-for-LLAMA (qwopqwop200/GPTQ-for-LLaMa\
          \ at cuda (github.com))<br>But in general, Ooba's fork is more performant\
          \ for some reasons I don't entirely understand.<br>2,<br>The next step,\
          \ you will need to copy your GPTQ repo (either Ooba' fork or the qwopa's\
          \ cuda branch or your own modification) into ooba'a textgen-webui's repository\
          \ folder, the \"repositories\" folder is not created by default, you need\
          \ to mkdir it.<br>SO it's like \"/yourpath/text-generation-webui/repositories/GPTQ-for-LLaMa\"\
          ,<br>3,<br>You need to install the GPTQ-for-LLaMa's py whl, it is generated\
          \ from Ooba's docker script during its setup. But you can check out my fork\
          \ of GTPQ-for-LLaMA (yhyu13/GPTQ-for-LLaMa at cuda_ooba (github.com)) where<br>I\
          \ created a docker_build_whl.sh script to help build the whl, you need to\
          \ modify TORCH_ARCH_LIST and choose your own CUDA image so to match your\
          \ rig.<br>The whl is produced under result/ folder, you need to install\
          \ it in order to load GPTQ quantized llama models</p>\n<p>In summary, in\
          \ order to use Ooba's textgen webui with GPTQ-for-LLaMa, you need to copy\
          \ the GPTQ repo under repositories folder, and install the correct, locally\
          \ built GPTQ whl.</p>\n<p>4,<br>The final step is to load the gptq model\
          \ with correct arg. you can checkout my fork of textgen-webui here (text-generation-webui/.env.example\
          \ at dev \xB7 yhyu13/text-generation-webui \xB7 GitHub)<br>I provided the\
          \ argument that I used locally.</p>\n"
        raw: "Just in case you have not received my replay:\n\nLet me briefly setup\
          \ the context here:\n\n1, \nThe Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-gptq-4bit\
          \ was made compatible with No actor order so that it is compatible with\
          \ Ooba's GPTQ fork which lag behind w/o feature support for actor order\n\
          In practice, you can load the gptq-4bit model with either Ooba's fork or\
          \ with the latest cuda branch of GPTQ-for-LLAMA (qwopqwop200/GPTQ-for-LLaMa\
          \ at cuda (github.com))\nBut in general, Ooba's fork is more performant\
          \ for some reasons I don't entirely understand.\n2,\nThe next step, you\
          \ will need to copy your GPTQ repo (either Ooba' fork or the qwopa's cuda\
          \ branch or your own modification) into ooba'a textgen-webui's repository\
          \ folder, the \"repositories\" folder is not created by default, you need\
          \ to mkdir it.\nSO it's like \"/yourpath/text-generation-webui/repositories/GPTQ-for-LLaMa\"\
          , \n3,\nYou need to install the GPTQ-for-LLaMa's py whl, it is generated\
          \ from Ooba's docker script during its setup. But you can check out my fork\
          \ of GTPQ-for-LLaMA (yhyu13/GPTQ-for-LLaMa at cuda_ooba (github.com)) where\n\
          I created a docker_build_whl.sh script to help build the whl, you need to\
          \ modify TORCH_ARCH_LIST and choose your own CUDA image so to match your\
          \ rig.\nThe whl is produced under result/ folder, you need to install it\
          \ in order to load GPTQ quantized llama models\n\nIn summary, in order to\
          \ use Ooba's textgen webui with GPTQ-for-LLaMa, you need to copy the GPTQ\
          \ repo under repositories folder, and install the correct, locally built\
          \ GPTQ whl.\n\n4,\nThe final step is to load the gptq model with correct\
          \ arg. you can checkout my fork of textgen-webui here (text-generation-webui/.env.example\
          \ at dev \xB7 yhyu13/text-generation-webui \xB7 GitHub)\nI provided the\
          \ argument that I used locally."
        updatedAt: '2023-05-25T10:10:12.350Z'
      numEdits: 0
      reactions: []
    id: 646f3404e2a72c647b6836cf
    type: comment
  author: Yhyu13
  content: "Just in case you have not received my replay:\n\nLet me briefly setup\
    \ the context here:\n\n1, \nThe Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-gptq-4bit\
    \ was made compatible with No actor order so that it is compatible with Ooba's\
    \ GPTQ fork which lag behind w/o feature support for actor order\nIn practice,\
    \ you can load the gptq-4bit model with either Ooba's fork or with the latest\
    \ cuda branch of GPTQ-for-LLAMA (qwopqwop200/GPTQ-for-LLaMa at cuda (github.com))\n\
    But in general, Ooba's fork is more performant for some reasons I don't entirely\
    \ understand.\n2,\nThe next step, you will need to copy your GPTQ repo (either\
    \ Ooba' fork or the qwopa's cuda branch or your own modification) into ooba'a\
    \ textgen-webui's repository folder, the \"repositories\" folder is not created\
    \ by default, you need to mkdir it.\nSO it's like \"/yourpath/text-generation-webui/repositories/GPTQ-for-LLaMa\"\
    , \n3,\nYou need to install the GPTQ-for-LLaMa's py whl, it is generated from\
    \ Ooba's docker script during its setup. But you can check out my fork of GTPQ-for-LLaMA\
    \ (yhyu13/GPTQ-for-LLaMa at cuda_ooba (github.com)) where\nI created a docker_build_whl.sh\
    \ script to help build the whl, you need to modify TORCH_ARCH_LIST and choose\
    \ your own CUDA image so to match your rig.\nThe whl is produced under result/\
    \ folder, you need to install it in order to load GPTQ quantized llama models\n\
    \nIn summary, in order to use Ooba's textgen webui with GPTQ-for-LLaMa, you need\
    \ to copy the GPTQ repo under repositories folder, and install the correct, locally\
    \ built GPTQ whl.\n\n4,\nThe final step is to load the gptq model with correct\
    \ arg. you can checkout my fork of textgen-webui here (text-generation-webui/.env.example\
    \ at dev \xB7 yhyu13/text-generation-webui \xB7 GitHub)\nI provided the argument\
    \ that I used locally."
  created_at: 2023-05-25 09:10:12+00:00
  edited: false
  hidden: false
  id: 646f3404e2a72c647b6836cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf9b112fb8c4b7142b05b734bb98479d.svg
      fullname: Enrique jimenez sanchez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kikeavi36
      type: user
    createdAt: '2023-05-25T11:51:05.000Z'
    data:
      edited: false
      editors:
      - kikeavi36
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf9b112fb8c4b7142b05b734bb98479d.svg
          fullname: Enrique jimenez sanchez
          isHf: false
          isPro: false
          name: kikeavi36
          type: user
        html: '<p>Hello, if I get it, I''m in it. In the end, I''m changing to ubuntu
          on my server in windows, I''m not capable.<br>In these days I''m sure I
          can  :)</p>

          '
        raw: 'Hello, if I get it, I''m in it. In the end, I''m changing to ubuntu
          on my server in windows, I''m not capable.

          In these days I''m sure I can  :)'
        updatedAt: '2023-05-25T11:51:05.660Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Yhyu13
    id: 646f4ba9e2a72c647b6c6a16
    type: comment
  author: kikeavi36
  content: 'Hello, if I get it, I''m in it. In the end, I''m changing to ubuntu on
    my server in windows, I''m not capable.

    In these days I''m sure I can  :)'
  created_at: 2023-05-25 10:51:05+00:00
  edited: false
  hidden: false
  id: 646f4ba9e2a72c647b6c6a16
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bf9b112fb8c4b7142b05b734bb98479d.svg
      fullname: Enrique jimenez sanchez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kikeavi36
      type: user
    createdAt: '2023-05-25T11:51:38.000Z'
    data:
      edited: false
      editors:
      - kikeavi36
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bf9b112fb8c4b7142b05b734bb98479d.svg
          fullname: Enrique jimenez sanchez
          isHf: false
          isPro: false
          name: kikeavi36
          type: user
        html: '<p>Thanks for all the explanation.</p>

          '
        raw: Thanks for all the explanation.
        updatedAt: '2023-05-25T11:51:38.149Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Yhyu13
    id: 646f4bcafa9253f3ae7e1bb9
    type: comment
  author: kikeavi36
  content: Thanks for all the explanation.
  created_at: 2023-05-25 10:51:38+00:00
  edited: false
  hidden: false
  id: 646f4bcafa9253f3ae7e1bb9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Yhyu13/chimera-inst-chat-13b-gptq-4bit
repo_type: model
status: closed
target_branch: null
title: Hello, can we talk by email or something similar?
