!!python/object:huggingface_hub.community.DiscussionWithDetails
author: minlik
conflicting_files: null
created_at: 2023-07-06 08:09:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
      fullname: kuan li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: minlik
      type: user
    createdAt: '2023-07-06T09:09:59.000Z'
    data:
      edited: false
      editors:
      - minlik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.971878707408905
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
          fullname: kuan li
          isHf: false
          isPro: false
          name: minlik
          type: user
        html: '<p>Chinese alpaca 33b was trained with additional chinese tokenizer<br>Kaio
          Ken''s SuperHOT 8K was trained with the original llama tokenizer<br>Is it
          ok to merge them together?</p>

          '
        raw: "Chinese alpaca 33b was trained with additional chinese tokenizer\r\n\
          Kaio Ken's SuperHOT 8K was trained with the original llama tokenizer\r\n\
          Is it ok to merge them together?"
        updatedAt: '2023-07-06T09:09:59.630Z'
      numEdits: 0
      reactions: []
    id: 64a684e7bec7a639234dfde3
    type: comment
  author: minlik
  content: "Chinese alpaca 33b was trained with additional chinese tokenizer\r\nKaio\
    \ Ken's SuperHOT 8K was trained with the original llama tokenizer\r\nIs it ok\
    \ to merge them together?"
  created_at: 2023-07-06 08:09:59+00:00
  edited: false
  hidden: false
  id: 64a684e7bec7a639234dfde3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T09:14:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.99749356508255
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hmm, actually I don''t know!  I didn''t look at that, I just did
          the merge.</p>

          <p>Have you tried it, does it produce OK results?</p>

          '
        raw: 'Hmm, actually I don''t know!  I didn''t look at that, I just did the
          merge.


          Have you tried it, does it produce OK results?'
        updatedAt: '2023-07-07T09:14:10.080Z'
      numEdits: 0
      reactions: []
    id: 64a7d762a31eeb933a451a81
    type: comment
  author: TheBloke
  content: 'Hmm, actually I don''t know!  I didn''t look at that, I just did the merge.


    Have you tried it, does it produce OK results?'
  created_at: 2023-07-07 08:14:10+00:00
  edited: false
  hidden: false
  id: 64a7d762a31eeb933a451a81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fdd9d80d99b22187d4e2875a3f7dba.svg
      fullname: Yi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charleyzhuyi
      type: user
    createdAt: '2023-07-07T10:55:32.000Z'
    data:
      edited: false
      editors:
      - charleyzhuyi
      hidden: false
      identifiedLanguage:
        language: pms
        probability: 0.3688162863254547
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fdd9d80d99b22187d4e2875a3f7dba.svg
          fullname: Yi
          isHf: false
          isPro: false
          name: charleyzhuyi
          type: user
        html: "<p>it is producing some garbage when I tested it.....something like\
          \ \u4F60\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D</p>\n"
        raw: "it is producing some garbage when I tested it.....something like \u4F60\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
          \u597D\u597D\u597D\u597D\u597D\u597D"
        updatedAt: '2023-07-07T10:55:32.612Z'
      numEdits: 0
      reactions: []
    id: 64a7ef246279203100ab890a
    type: comment
  author: charleyzhuyi
  content: "it is producing some garbage when I tested it.....something like \u4F60\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\
    \u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D\u597D"
  created_at: 2023-07-07 09:55:32+00:00
  edited: false
  hidden: false
  id: 64a7ef246279203100ab890a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-07T10:58:32.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9237962961196899
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>This can happen with any SuperHOT model when you don''t have the
          context size set correctly.  How did you test it? If using ExLlama, make
          sure context size is &gt;2048 and compress_emb is set appropriately,  eg
          context_size 4096 and compress 2</p>

          <p>If using AutoGPTQ, make sure you used <code>trust_remote_code=True</code></p>

          '
        raw: 'This can happen with any SuperHOT model when you don''t have the context
          size set correctly.  How did you test it? If using ExLlama, make sure context
          size is >2048 and compress_emb is set appropriately,  eg context_size 4096
          and compress 2


          If using AutoGPTQ, make sure you used `trust_remote_code=True`'
        updatedAt: '2023-07-07T11:00:49.940Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - songzhen
    id: 64a7efd8eb58940b45f09e82
    type: comment
  author: TheBloke
  content: 'This can happen with any SuperHOT model when you don''t have the context
    size set correctly.  How did you test it? If using ExLlama, make sure context
    size is >2048 and compress_emb is set appropriately,  eg context_size 4096 and
    compress 2


    If using AutoGPTQ, make sure you used `trust_remote_code=True`'
  created_at: 2023-07-07 09:58:32+00:00
  edited: true
  hidden: false
  id: 64a7efd8eb58940b45f09e82
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
      fullname: kuan li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: minlik
      type: user
    createdAt: '2023-07-07T17:07:31.000Z'
    data:
      edited: false
      editors:
      - minlik
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9135745763778687
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a0a04f38a00b11b9752ad8d1fc0708e.svg
          fullname: kuan li
          isHf: false
          isPro: false
          name: minlik
          type: user
        html: '<p>The Alpaca template should be used for inferecing.</p>

          <p>And I tested the model with some simple scenarios, and it works fine
          with AutoGPTQ. I will further test the model''s ability with longer context
          sizes and more complex scenarios.</p>

          <p>I think It''s interesting that different Lora adapters trained on different
          base models with different tokenizers  can be merged successfully</p>

          '
        raw: 'The Alpaca template should be used for inferecing.


          And I tested the model with some simple scenarios, and it works fine with
          AutoGPTQ. I will further test the model''s ability with longer context sizes
          and more complex scenarios.


          I think It''s interesting that different Lora adapters trained on different
          base models with different tokenizers  can be merged successfully'
        updatedAt: '2023-07-07T17:07:31.547Z'
      numEdits: 0
      reactions: []
    id: 64a846535a69e2ca8880018b
    type: comment
  author: minlik
  content: 'The Alpaca template should be used for inferecing.


    And I tested the model with some simple scenarios, and it works fine with AutoGPTQ.
    I will further test the model''s ability with longer context sizes and more complex
    scenarios.


    I think It''s interesting that different Lora adapters trained on different base
    models with different tokenizers  can be merged successfully'
  created_at: 2023-07-07 16:07:31+00:00
  edited: false
  hidden: false
  id: 64a846535a69e2ca8880018b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70fdd9d80d99b22187d4e2875a3f7dba.svg
      fullname: Yi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charleyzhuyi
      type: user
    createdAt: '2023-07-08T00:28:06.000Z'
    data:
      edited: false
      editors:
      - charleyzhuyi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9187941551208496
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70fdd9d80d99b22187d4e2875a3f7dba.svg
          fullname: Yi
          isHf: false
          isPro: false
          name: charleyzhuyi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> thank you for\
          \ your reply, I indeed forgot to set trust_remote_code=True in AutoGPTQ.\
          \ After set it, I am able to get correct output now.  </p>\n<p>For ExLlama,\
          \ I also tested with 8192 context size and 4 compress_pos_emb and able to\
          \ get correct output. ( and it is much faster than autoGPTQ!)</p>\n<p>P.S.\
          \ Normall, I perfer ExLlama than  AutoGPTQ since the former one's performance\
          \ is much better. For  But large mode that can not fit into VRAM. like this\
          \ 33B-8K, AutoGPTQ is my only choice. (since EXLLama will give CUDA out\
          \ of memory if load this model with 8k context)</p>\n<p>However, today when\
          \ i retested it using oobabooga  text-generation-web-ui , I found out I\
          \ can even load this model with 8k context by using ExLlama only under Windows.\
          \  The windows task manager shows there is an additional 16GB \u201CShared\
          \ GPU memory\"  besides the 24GB Dedicated GPU memory, so once the dedicated\
          \ GPU memory has been filled, it automatically starting to use Shared GPU\
          \ memory.</p>\n<p>This only works under Windows . In  Linux, I still get\
          \ \"CUDA out of memory\" when I load this model with 8K context  using ExLLama.</p>\n\
          <p>I did a quick search and it seems that <a rel=\"nofollow\" href=\"https://www.reddit.com/r/LocalLLaMA/comments/14fb9c0/last_nvidia_drivers_let_you_use_the_shared_memory/\"\
          >https://www.reddit.com/r/LocalLLaMA/comments/14fb9c0/last_nvidia_drivers_let_you_use_the_shared_memory/</a>\
          \ mentioned the latest  someone got it work on linux as well, but not my\
          \ case though.</p>\n"
        raw: "@TheBloke thank you for your reply, I indeed forgot to set trust_remote_code=True\
          \ in AutoGPTQ. After set it, I am able to get correct output now.  \n\n\
          For ExLlama, I also tested with 8192 context size and 4 compress_pos_emb\
          \ and able to get correct output. ( and it is much faster than autoGPTQ!)\n\
          \nP.S. Normall, I perfer ExLlama than  AutoGPTQ since the former one's performance\
          \ is much better. For  But large mode that can not fit into VRAM. like this\
          \ 33B-8K, AutoGPTQ is my only choice. (since EXLLama will give CUDA out\
          \ of memory if load this model with 8k context)\n\nHowever, today when i\
          \ retested it using oobabooga  text-generation-web-ui , I found out I can\
          \ even load this model with 8k context by using ExLlama only under Windows.\
          \  The windows task manager shows there is an additional 16GB \u201CShared\
          \ GPU memory\"  besides the 24GB Dedicated GPU memory, so once the dedicated\
          \ GPU memory has been filled, it automatically starting to use Shared GPU\
          \ memory.\n\nThis only works under Windows . In  Linux, I still get \"CUDA\
          \ out of memory\" when I load this model with 8K context  using ExLLama.\n\
          \nI did a quick search and it seems that https://www.reddit.com/r/LocalLLaMA/comments/14fb9c0/last_nvidia_drivers_let_you_use_the_shared_memory/\
          \ mentioned the latest  someone got it work on linux as well, but not my\
          \ case though.\n"
        updatedAt: '2023-07-08T00:28:06.021Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - songzhen
    id: 64a8ad96eb47b35522911c36
    type: comment
  author: charleyzhuyi
  content: "@TheBloke thank you for your reply, I indeed forgot to set trust_remote_code=True\
    \ in AutoGPTQ. After set it, I am able to get correct output now.  \n\nFor ExLlama,\
    \ I also tested with 8192 context size and 4 compress_pos_emb and able to get\
    \ correct output. ( and it is much faster than autoGPTQ!)\n\nP.S. Normall, I perfer\
    \ ExLlama than  AutoGPTQ since the former one's performance is much better. For\
    \  But large mode that can not fit into VRAM. like this 33B-8K, AutoGPTQ is my\
    \ only choice. (since EXLLama will give CUDA out of memory if load this model\
    \ with 8k context)\n\nHowever, today when i retested it using oobabooga  text-generation-web-ui\
    \ , I found out I can even load this model with 8k context by using ExLlama only\
    \ under Windows.  The windows task manager shows there is an additional 16GB \u201C\
    Shared GPU memory\"  besides the 24GB Dedicated GPU memory, so once the dedicated\
    \ GPU memory has been filled, it automatically starting to use Shared GPU memory.\n\
    \nThis only works under Windows . In  Linux, I still get \"CUDA out of memory\"\
    \ when I load this model with 8K context  using ExLLama.\n\nI did a quick search\
    \ and it seems that https://www.reddit.com/r/LocalLLaMA/comments/14fb9c0/last_nvidia_drivers_let_you_use_the_shared_memory/\
    \ mentioned the latest  someone got it work on linux as well, but not my case\
    \ though.\n"
  created_at: 2023-07-07 23:28:06+00:00
  edited: false
  hidden: false
  id: 64a8ad96eb47b35522911c36
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Chinese-Alpaca-33B-SuperHOT-8K-GPTQ
repo_type: model
status: open
target_branch: null
title: question about merging chinese-alpaca-33b with SuperHOT 8K lora
