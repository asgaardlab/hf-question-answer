!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ReXommendation
conflicting_files: null
created_at: 2023-07-02 09:16:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb50f47c5235ec070041a4a9f78fe396.svg
      fullname: ReXommendation
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ReXommendation
      type: user
    createdAt: '2023-07-02T10:16:34.000Z'
    data:
      edited: false
      editors:
      - ReXommendation
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8727487921714783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb50f47c5235ec070041a4a9f78fe396.svg
          fullname: ReXommendation
          isHf: false
          isPro: false
          name: ReXommendation
          type: user
        html: '<p>No error (not enough time for it), only insane amounts of memory
          consumption before crashing the terminal or my system. This is with 32 GB
          of real ram and 16GB of swap. This is the GPTQ plugin I''m using: <a rel="nofollow"
          href="https://github.com/0cc4m/GPTQ-for-LLaMa">https://github.com/0cc4m/GPTQ-for-LLaMa</a>.</p>

          '
        raw: 'No error (not enough time for it), only insane amounts of memory consumption
          before crashing the terminal or my system. This is with 32 GB of real ram
          and 16GB of swap. This is the GPTQ plugin I''m using: https://github.com/0cc4m/GPTQ-for-LLaMa.'
        updatedAt: '2023-07-02T10:16:34.866Z'
      numEdits: 0
      reactions: []
    id: 64a14e82f824e15453c33e89
    type: comment
  author: ReXommendation
  content: 'No error (not enough time for it), only insane amounts of memory consumption
    before crashing the terminal or my system. This is with 32 GB of real ram and
    16GB of swap. This is the GPTQ plugin I''m using: https://github.com/0cc4m/GPTQ-for-LLaMa.'
  created_at: 2023-07-02 09:16:34+00:00
  edited: false
  hidden: false
  id: 64a14e82f824e15453c33e89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb50f47c5235ec070041a4a9f78fe396.svg
      fullname: ReXommendation
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ReXommendation
      type: user
    createdAt: '2023-07-02T10:23:43.000Z'
    data:
      edited: false
      editors:
      - ReXommendation
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5032263994216919
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb50f47c5235ec070041a4a9f78fe396.svg
          fullname: ReXommendation
          isHf: false
          isPro: false
          name: ReXommendation
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62cd70b0c4eb470b622a5501/WTwCRXJtDZFuD1K932Vnb.jpeg"><img
          alt="20230702_062035.jpg" src="https://cdn-uploads.huggingface.co/production/uploads/62cd70b0c4eb470b622a5501/WTwCRXJtDZFuD1K932Vnb.jpeg"></a><br>External
          photo because of the crash.</p>

          '
        raw: '

          ![20230702_062035.jpg](https://cdn-uploads.huggingface.co/production/uploads/62cd70b0c4eb470b622a5501/WTwCRXJtDZFuD1K932Vnb.jpeg)

          External photo because of the crash.'
        updatedAt: '2023-07-02T10:23:43.638Z'
      numEdits: 0
      reactions: []
    id: 64a1502f4d55bc91b1c28d91
    type: comment
  author: ReXommendation
  content: '

    ![20230702_062035.jpg](https://cdn-uploads.huggingface.co/production/uploads/62cd70b0c4eb470b622a5501/WTwCRXJtDZFuD1K932Vnb.jpeg)

    External photo because of the crash.'
  created_at: 2023-07-02 09:23:43+00:00
  edited: false
  hidden: false
  id: 64a1502f4d55bc91b1c28d91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-08T09:36:42.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9421384930610657
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Can''t really help without error messages. But it''s normal to see
          RAM usage increase - it has to load the model to RAM before it can move
          it to the GPU.  Try increasing your swap/Pagefile size to around 100GB,
          that usually helps people load 30B models.</p>

          '
        raw: Can't really help without error messages. But it's normal to see RAM
          usage increase - it has to load the model to RAM before it can move it to
          the GPU.  Try increasing your swap/Pagefile size to around 100GB, that usually
          helps people load 30B models.
        updatedAt: '2023-07-08T09:36:42.609Z'
      numEdits: 0
      reactions: []
    id: 64a92e2acbc2ba10ffa1773c
    type: comment
  author: TheBloke
  content: Can't really help without error messages. But it's normal to see RAM usage
    increase - it has to load the model to RAM before it can move it to the GPU.  Try
    increasing your swap/Pagefile size to around 100GB, that usually helps people
    load 30B models.
  created_at: 2023-07-08 08:36:42+00:00
  edited: false
  hidden: false
  id: 64a92e2acbc2ba10ffa1773c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/GPlatty-30B-GPTQ
repo_type: model
status: open
target_branch: null
title: Issues attempting to load model.
