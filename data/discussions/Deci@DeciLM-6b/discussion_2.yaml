!!python/object:huggingface_hub.community.DiscussionWithDetails
author: philschmid
conflicting_files: null
created_at: 2023-09-14 04:27:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
      fullname: Philipp Schmid
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: philschmid
      type: user
    createdAt: '2023-09-14T05:27:39.000Z'
    data:
      edited: false
      editors:
      - philschmid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9441397786140442
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624629516652-5ff5d596f244529b3ec0fb89.png?w=200&h=200&f=face
          fullname: Philipp Schmid
          isHf: true
          isPro: false
          name: philschmid
          type: user
        html: '<p>Hey Philipp here, </p>

          <p>Congrats on the new model release looks awesome! But I am curious to
          know how you created the benchmark numbers for inference Endpoints. Since
          it seems that the model needs "remote_trust_code" and there is no <code>handler.py</code>
          to deploy it. </p>

          '
        raw: "Hey Philipp here, \r\n\r\nCongrats on the new model release looks awesome!\
          \ But I am curious to know how you created the benchmark numbers for inference\
          \ Endpoints. Since it seems that the model needs \"remote_trust_code\" and\
          \ there is no `handler.py` to deploy it. \r\n"
        updatedAt: '2023-09-14T05:27:39.855Z'
      numEdits: 0
      reactions: []
    id: 650299cb27c4cfdbc691dad7
    type: comment
  author: philschmid
  content: "Hey Philipp here, \r\n\r\nCongrats on the new model release looks awesome!\
    \ But I am curious to know how you created the benchmark numbers for inference\
    \ Endpoints. Since it seems that the model needs \"remote_trust_code\" and there\
    \ is no `handler.py` to deploy it. \r\n"
  created_at: 2023-09-14 04:27:39+00:00
  edited: false
  hidden: false
  id: 650299cb27c4cfdbc691dad7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/028ebd51b2b71598f1a070117f109eaa.svg
      fullname: Itay  Levy
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: itay-levy
      type: user
    createdAt: '2023-09-14T06:49:53.000Z'
    data:
      edited: false
      editors:
      - itay-levy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8572822213172913
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/028ebd51b2b71598f1a070117f109eaa.svg
          fullname: Itay  Levy
          isHf: false
          isPro: false
          name: itay-levy
          type: user
        html: '<p>Hey Philipp!<br>Apologies for the oversight. The baseline numbers
          in the README file refer to benchmarking results without a managed solution
          - just HF + PyTorch, not HF Inference Endpoints. We''ve updated the README
          to clarify. Thanks for pointing it out<br>Also, a heartfelt thank you for
          all your open-source contributions!</p>

          '
        raw: 'Hey Philipp!

          Apologies for the oversight. The baseline numbers in the README file refer
          to benchmarking results without a managed solution - just HF + PyTorch,
          not HF Inference Endpoints. We''ve updated the README to clarify. Thanks
          for pointing it out

          Also, a heartfelt thank you for all your open-source contributions!'
        updatedAt: '2023-09-14T06:49:53.480Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - philschmid
    id: 6502ad115b67fa2846dc8a6b
    type: comment
  author: itay-levy
  content: 'Hey Philipp!

    Apologies for the oversight. The baseline numbers in the README file refer to
    benchmarking results without a managed solution - just HF + PyTorch, not HF Inference
    Endpoints. We''ve updated the README to clarify. Thanks for pointing it out

    Also, a heartfelt thank you for all your open-source contributions!'
  created_at: 2023-09-14 05:49:53+00:00
  edited: false
  hidden: false
  id: 6502ad115b67fa2846dc8a6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4f09dc6f994e3899cc3f0a0000490b13.svg
      fullname: Joseph William
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josepfg
      type: user
    createdAt: '2023-09-17T17:57:24.000Z'
    data:
      edited: true
      editors:
      - josepfg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9089662432670593
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4f09dc6f994e3899cc3f0a0000490b13.svg
          fullname: Joseph William
          isHf: false
          isPro: false
          name: josepfg
          type: user
        html: '<p>Performance of inference endpoints is a crucial aspect of any machine
          learning deployment, and it''s something that demands careful consideration.
          Here are some thoughts on the topic of inference endpoint performance:</p>

          <p>Latency Matters: One of the key performance metrics for inference endpoints
          is latency. Low latency is essential for real-time applications, such as
          chatbots or autonomous vehicles. It ensures that predictions are delivered
          quickly, enhancing user experience.</p>

          <p>Throughput and Scalability: In addition to low latency, high throughput
          and scalability are equally important. As the number of requests increases,
          the inference endpoint should be able to handle them efficiently without
          a significant drop in performance.</p>

          <p>Resource Utilization: Efficient resource utilization is critical for
          cost-effectiveness. You want to make sure that <a rel="nofollow" href="https://essaycreator.org/">https://essaycreator.org/</a>
          your inference endpoint is using hardware resources (CPU/GPU) optimally
          to avoid unnecessary costs.</p>

          <p>Model Optimization: Optimizing the model architecture can significantly
          impact performance. Smaller and quantized models can lead to faster inference
          times, which is especially important for edge devices with limited resources.</p>

          <p>Monitoring and Profiling: Regular monitoring and profiling of your inference
          endpoints are essential. Tools like Prometheus and Grafana can help you
          keep an eye on latency, throughput, and resource utilization.</p>

          '
        raw: 'Performance of inference endpoints is a crucial aspect of any machine
          learning deployment, and it''s something that demands careful consideration.
          Here are some thoughts on the topic of inference endpoint performance:


          Latency Matters: One of the key performance metrics for inference endpoints
          is latency. Low latency is essential for real-time applications, such as
          chatbots or autonomous vehicles. It ensures that predictions are delivered
          quickly, enhancing user experience.


          Throughput and Scalability: In addition to low latency, high throughput
          and scalability are equally important. As the number of requests increases,
          the inference endpoint should be able to handle them efficiently without
          a significant drop in performance.


          Resource Utilization: Efficient resource utilization is critical for cost-effectiveness.
          You want to make sure that https://essaycreator.org/ your inference endpoint
          is using hardware resources (CPU/GPU) optimally to avoid unnecessary costs.


          Model Optimization: Optimizing the model architecture can significantly
          impact performance. Smaller and quantized models can lead to faster inference
          times, which is especially important for edge devices with limited resources.


          Monitoring and Profiling: Regular monitoring and profiling of your inference
          endpoints are essential. Tools like Prometheus and Grafana can help you
          keep an eye on latency, throughput, and resource utilization.'
        updatedAt: '2023-10-26T11:09:47.127Z'
      numEdits: 2
      reactions: []
    id: 65073e04c53e1a7f17ca7ada
    type: comment
  author: josepfg
  content: 'Performance of inference endpoints is a crucial aspect of any machine
    learning deployment, and it''s something that demands careful consideration. Here
    are some thoughts on the topic of inference endpoint performance:


    Latency Matters: One of the key performance metrics for inference endpoints is
    latency. Low latency is essential for real-time applications, such as chatbots
    or autonomous vehicles. It ensures that predictions are delivered quickly, enhancing
    user experience.


    Throughput and Scalability: In addition to low latency, high throughput and scalability
    are equally important. As the number of requests increases, the inference endpoint
    should be able to handle them efficiently without a significant drop in performance.


    Resource Utilization: Efficient resource utilization is critical for cost-effectiveness.
    You want to make sure that https://essaycreator.org/ your inference endpoint is
    using hardware resources (CPU/GPU) optimally to avoid unnecessary costs.


    Model Optimization: Optimizing the model architecture can significantly impact
    performance. Smaller and quantized models can lead to faster inference times,
    which is especially important for edge devices with limited resources.


    Monitoring and Profiling: Regular monitoring and profiling of your inference endpoints
    are essential. Tools like Prometheus and Grafana can help you keep an eye on latency,
    throughput, and resource utilization.'
  created_at: 2023-09-17 16:57:24+00:00
  edited: true
  hidden: false
  id: 65073e04c53e1a7f17ca7ada
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/028ebd51b2b71598f1a070117f109eaa.svg
      fullname: Itay  Levy
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: itay-levy
      type: user
    createdAt: '2023-09-21T07:24:35.000Z'
    data:
      status: closed
    id: 650befb37c99ca283e5cb387
    type: status-change
  author: itay-levy
  created_at: 2023-09-21 06:24:35+00:00
  id: 650befb37c99ca283e5cb387
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Deci/DeciLM-6b
repo_type: model
status: closed
target_branch: null
title: Inference Endpoints Performance
