!!python/object:huggingface_hub.community.DiscussionWithDetails
author: neuralworm
conflicting_files: null
created_at: 2023-08-20 13:49:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/574d055ca3ee7ecab70385e44b8944ef.svg
      fullname: neural_worm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: neuralworm
      type: user
    createdAt: '2023-08-20T14:49:24.000Z'
    data:
      edited: false
      editors:
      - neuralworm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.26406919956207275
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/574d055ca3ee7ecab70385e44b8944ef.svg
          fullname: neural_worm
          isHf: false
          isPro: false
          name: neuralworm
          type: user
        html: "<p>I tried to run the example in the model card, and the generation\
          \ works, but the transformers pipeline generation doesnt work.<br>I have\
          \ the latest transformers and auto_gptq</p>\n<p>Code:</p>\n<pre><code class=\"\
          language-py\"><span class=\"hljs-keyword\">from</span> transformers <span\
          \ class=\"hljs-keyword\">import</span> AutoTokenizer, pipeline, logging\n\
          <span class=\"hljs-keyword\">from</span> auto_gptq <span class=\"hljs-keyword\"\
          >import</span> AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path\
          \ = <span class=\"hljs-string\">\"TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ\"\
          </span>\nuse_triton = <span class=\"hljs-literal\">False</span>\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=<span class=\"\
          hljs-literal\">True</span>)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     trust_remote_code=<span class=\"hljs-literal\">False</span>,\n   \
          \     device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n        use_triton=use_triton,\n\
          \        quantize_config=<span class=\"hljs-literal\">None</span>)\n\n<span\
          \ class=\"hljs-string\">\"\"\"</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,</span>\n\
          <span class=\"hljs-string\">        use_safetensors=True,</span>\n<span\
          \ class=\"hljs-string\">        trust_remote_code=False,</span>\n<span class=\"\
          hljs-string\">        device=\"cuda:0\",</span>\n<span class=\"hljs-string\"\
          >        quantize_config=None)</span>\n<span class=\"hljs-string\">\"\"\"\
          </span>\n\nprompt = <span class=\"hljs-string\">\"Wo steht der Eifelturm?\"\
          </span>\nprompt_template=<span class=\"hljs-string\">f'''### User: <span\
          \ class=\"hljs-subst\">{prompt}</span></span>\n<span class=\"hljs-string\"\
          >### Assistant:</span>\n<span class=\"hljs-string\">'''</span>\n\n<span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\\
          n\\n*** Generate:\"</span>)\n\ninput_ids = tokenizer(prompt_template, return_tensors=<span\
          \ class=\"hljs-string\">'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ max_new_tokens=<span class=\"hljs-number\">512</span>)\n<span class=\"\
          hljs-built_in\">print</span>(tokenizer.decode(output[<span class=\"hljs-number\"\
          >0</span>]))\n\n<span class=\"hljs-comment\"># Inference can also be done\
          \ using transformers' pipeline</span>\n\n<span class=\"hljs-comment\">#\
          \ Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ</span>\nlogging.set_verbosity(logging.CRITICAL)\n\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"\
          </span>)\npipe = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>,\n)\n\n<span class=\"hljs-built_in\"\
          >print</span>(pipe(prompt_template)[<span class=\"hljs-number\">0</span>][<span\
          \ class=\"hljs-string\">'generated_text'</span>])&lt;/code&gt;\n</code></pre>\n\
          <p>Output:</p>\n<pre><code>*** Generate:\n&lt;s&gt; ### User: Wo steht der\
          \ Eifelturm?\n### Assistant:\n\nDer Eifelturm steht in Paris, Frankreich.\
          \ Er ist ein ber\xFChmter Aussichtsturm und ein Wahrzeichen der Stadt. Der\
          \ Turm wurde im 19. Jahrhundert erbaut und ist seitdem ein beliebter Ort\
          \ f\xFCr Touristen und Einheimische.\n&lt;/s&gt;\n*** Pipeline:\n### User:\
          \ Wo steht der Eifelturm?\n### Assistant:\n</code></pre>\n"
        raw: "I tried to run the example in the model card, and the generation works,\
          \ but the transformers pipeline generation doesnt work.\r\nI have the latest\
          \ transformers and auto_gptq\r\n\r\nCode:\r\n```py\r\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\r\n\r\nmodel_name_or_path = \"TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ\"\
          \r\nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        use_safetensors=True,\r\n        trust_remote_code=False,\r\n\
          \        device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n      \
          \  quantize_config=None)\r\n\r\n\"\"\"\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        use_safetensors=True,\r\n        trust_remote_code=False,\r\n\
          \        device=\"cuda:0\",\r\n        quantize_config=None)\r\n\"\"\"\r\
          \n\r\nprompt = \"Wo steht der Eifelturm?\"\r\nprompt_template=f'''### User:\
          \ {prompt}\r\n### Assistant:\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\"\
          )\r\n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
          \noutput = model.generate(inputs=input_ids, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n\r\n# Inference can also be done using transformers' pipeline\r\n\r\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\n\r\nprint(\"***\
          \ Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])</code>\r\
          \n```\r\n\r\nOutput:\r\n```\r\n*** Generate:\r\n<s> ### User: Wo steht der\
          \ Eifelturm?\r\n### Assistant:\r\n\r\nDer Eifelturm steht in Paris, Frankreich.\
          \ Er ist ein ber\xFChmter Aussichtsturm und ein Wahrzeichen der Stadt. Der\
          \ Turm wurde im 19. Jahrhundert erbaut und ist seitdem ein beliebter Ort\
          \ f\xFCr Touristen und Einheimische.\r\n</s>\r\n*** Pipeline:\r\n### User:\
          \ Wo steht der Eifelturm?\r\n### Assistant:\r\n```"
        updatedAt: '2023-08-20T14:49:24.901Z'
      numEdits: 0
      reactions: []
    id: 64e227f43209bf4194b2c4ad
    type: comment
  author: neuralworm
  content: "I tried to run the example in the model card, and the generation works,\
    \ but the transformers pipeline generation doesnt work.\r\nI have the latest transformers\
    \ and auto_gptq\r\n\r\nCode:\r\n```py\r\nfrom transformers import AutoTokenizer,\
    \ pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\
    \n\r\nmodel_name_or_path = \"TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ\"\r\
    \nuse_triton = False\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        use_safetensors=True,\r\n        trust_remote_code=False,\r\n      \
    \  device=\"cuda:0\",\r\n        use_triton=use_triton,\r\n        quantize_config=None)\r\
    \n\r\n\"\"\"\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        use_safetensors=True,\r\n        trust_remote_code=False,\r\n      \
    \  device=\"cuda:0\",\r\n        quantize_config=None)\r\n\"\"\"\r\n\r\nprompt\
    \ = \"Wo steht der Eifelturm?\"\r\nprompt_template=f'''### User: {prompt}\r\n\
    ### Assistant:\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids =\
    \ tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\noutput =\
    \ model.generate(inputs=input_ids, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
    \n\r\n# Inference can also be done using transformers' pipeline\r\n\r\n# Prevent\
    \ printing spurious transformers error when using pipeline with AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\
    \n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\
    \n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])</code>\r\n```\r\n\
    \r\nOutput:\r\n```\r\n*** Generate:\r\n<s> ### User: Wo steht der Eifelturm?\r\
    \n### Assistant:\r\n\r\nDer Eifelturm steht in Paris, Frankreich. Er ist ein ber\xFC\
    hmter Aussichtsturm und ein Wahrzeichen der Stadt. Der Turm wurde im 19. Jahrhundert\
    \ erbaut und ist seitdem ein beliebter Ort f\xFCr Touristen und Einheimische.\r\
    \n</s>\r\n*** Pipeline:\r\n### User: Wo steht der Eifelturm?\r\n### Assistant:\r\
    \n```"
  created_at: 2023-08-20 13:49:24+00:00
  edited: false
  hidden: false
  id: 64e227f43209bf4194b2c4ad
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-13B-German-Assistant-v4-GPTQ
repo_type: model
status: open
target_branch: null
title: transformers pipeline doesnt output text
