!!python/object:huggingface_hub.community.DiscussionWithDetails
author: obscureagent
conflicting_files: null
created_at: 2023-09-13 11:28:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/02134e3b4d97bf60805d0020f5b40b8e.svg
      fullname: Gerald Villaran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: obscureagent
      type: user
    createdAt: '2023-09-13T12:28:46.000Z'
    data:
      edited: false
      editors:
      - obscureagent
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9058179259300232
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/02134e3b4d97bf60805d0020f5b40b8e.svg
          fullname: Gerald Villaran
          isHf: false
          isPro: false
          name: obscureagent
          type: user
        html: '<p>How do I make an inference using the model in pth format?<br>Specially
          the model that is acquired though the download.sh from the meta repo.</p>

          '
        raw: "How do I make an inference using the model in pth format?\r\nSpecially\
          \ the model that is acquired though the download.sh from the meta repo."
        updatedAt: '2023-09-13T12:28:46.007Z'
      numEdits: 0
      reactions: []
    id: 6501aafea37c86bad8f9d2db
    type: comment
  author: obscureagent
  content: "How do I make an inference using the model in pth format?\r\nSpecially\
    \ the model that is acquired though the download.sh from the meta repo."
  created_at: 2023-09-13 11:28:46+00:00
  edited: false
  hidden: false
  id: 6501aafea37c86bad8f9d2db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-09-13T23:30:50.000Z'
    data:
      edited: false
      editors:
      - mrfakename
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8830375075340271
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
          fullname: mrfakename
          isHf: false
          isPro: false
          name: mrfakename
          type: user
        html: '<p>Hello Mr. Villaran,</p>

          <p>If I am understanding correctly, you''re interested in inferencing the
          LLaMA 1 (not Llama 2) model downloaded from Meta''s GitHub repo. This project
          is not affiliated with Meta Platforms, Inc. and does not distribute LLaMA
          models (this project distributes OpenLLaMA and its derivatives), so I recommend
          opening this issue in the official <a rel="nofollow" href="https://github.com/facebookresearch/llama">Llama</a>
          repository.</p>

          <p>If you would like to inference OpenLLaMA/OpenAlpaca PTH files, they are
          in the same format as LLaMA 1 models, so I recommend using <a rel="nofollow"
          href="https://github.com/facebookresearch/llama/tree/llama_v1">the LLaMA
          1 branch of the Llama repository</a> to inference it. If you have any other
          questions, please do not hesitate to let me know.</p>

          <p>Thank you!</p>

          <p><em>Please note that this project is not affiliated with Meta Platforms,
          Inc.</em></p>

          '
        raw: 'Hello Mr. Villaran,


          If I am understanding correctly, you''re interested in inferencing the LLaMA
          1 (not Llama 2) model downloaded from Meta''s GitHub repo. This project
          is not affiliated with Meta Platforms, Inc. and does not distribute LLaMA
          models (this project distributes OpenLLaMA and its derivatives), so I recommend
          opening this issue in the official [Llama](https://github.com/facebookresearch/llama)
          repository.


          If you would like to inference OpenLLaMA/OpenAlpaca PTH files, they are
          in the same format as LLaMA 1 models, so I recommend using [the LLaMA 1
          branch of the Llama repository](https://github.com/facebookresearch/llama/tree/llama_v1)
          to inference it. If you have any other questions, please do not hesitate
          to let me know.


          Thank you!


          _Please note that this project is not affiliated with Meta Platforms, Inc._'
        updatedAt: '2023-09-13T23:30:50.923Z'
      numEdits: 0
      reactions: []
    id: 6502462a7764af3f3fb4b057
    type: comment
  author: mrfakename
  content: 'Hello Mr. Villaran,


    If I am understanding correctly, you''re interested in inferencing the LLaMA 1
    (not Llama 2) model downloaded from Meta''s GitHub repo. This project is not affiliated
    with Meta Platforms, Inc. and does not distribute LLaMA models (this project distributes
    OpenLLaMA and its derivatives), so I recommend opening this issue in the official
    [Llama](https://github.com/facebookresearch/llama) repository.


    If you would like to inference OpenLLaMA/OpenAlpaca PTH files, they are in the
    same format as LLaMA 1 models, so I recommend using [the LLaMA 1 branch of the
    Llama repository](https://github.com/facebookresearch/llama/tree/llama_v1) to
    inference it. If you have any other questions, please do not hesitate to let me
    know.


    Thank you!


    _Please note that this project is not affiliated with Meta Platforms, Inc._'
  created_at: 2023-09-13 22:30:50+00:00
  edited: false
  hidden: false
  id: 6502462a7764af3f3fb4b057
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62e54f0eae9d3f10acb95cb9/VAyk05hqB3OZWXEZW-B0q.png?w=200&h=200&f=face
      fullname: mrfakename
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mrfakename
      type: user
    createdAt: '2023-09-13T23:30:56.000Z'
    data:
      status: closed
    id: 65024630c62f8710cd653db0
    type: status-change
  author: mrfakename
  created_at: 2023-09-13 22:30:56+00:00
  id: 65024630c62f8710cd653db0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: llama-research/openalpaca-7b-pth
repo_type: model
status: closed
target_branch: null
title: How do I make an inference using the model in pth format?
