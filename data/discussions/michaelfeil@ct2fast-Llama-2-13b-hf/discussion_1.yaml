!!python/object:huggingface_hub.community.DiscussionWithDetails
author: limcheekin
conflicting_files: null
created_at: 2023-07-20 08:15:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
      fullname: Lim Chee Kin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: limcheekin
      type: user
    createdAt: '2023-07-20T09:15:48.000Z'
    data:
      edited: false
      editors:
      - limcheekin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.812092661857605
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/24a8c63c897efdd980ef9d4805cbff7b.svg
          fullname: Lim Chee Kin
          isHf: false
          isPro: false
          name: limcheekin
          type: user
        html: '<p>Thanks for convert and sharing the model.</p>

          <p>First, given the sample code of the model card, it will raise the following
          error:</p>

          <pre><code>ValueError: no suitable Tokenizer found.

          </code></pre>

          <p>Set the <code>tokenizer</code> using the <code>AutoTokenizer</code> class
          doesn''t help either.</p>

          <p>Hence, I used the <code>LlamaTokenizer</code> and it required the installation
          of the <code>sentencepiece</code> package.</p>

          <p>Lastly, the <code>sentencepiece</code> package will raise the <code>TypeError:
          not a string</code>, after added the <code>tokenizer.model</code> file (which
          downloaded using the <a rel="nofollow" href="https://github.com/facebookresearch/llama/blob/main/download.sh">download.sh</a>)
          to the model directory, finally it is working fine.</p>

          <p>Please test it out and update the model card. I hope it save couple of
          productive hours of other developers time.</p>

          <p>Best regards.</p>

          '
        raw: "Thanks for convert and sharing the model.\r\n\r\nFirst, given the sample\
          \ code of the model card, it will raise the following error:\r\n``` \r\n\
          ValueError: no suitable Tokenizer found.\r\n```\r\nSet the `tokenizer` using\
          \ the `AutoTokenizer` class doesn't help either.\r\n\r\nHence, I used the\
          \ `LlamaTokenizer` and it required the installation of the `sentencepiece`\
          \ package.\r\n\r\nLastly, the `sentencepiece` package will raise the `TypeError:\
          \ not a string`, after added the `tokenizer.model` file (which downloaded\
          \ using the [download.sh](https://github.com/facebookresearch/llama/blob/main/download.sh))\
          \ to the model directory, finally it is working fine.\r\n\r\nPlease test\
          \ it out and update the model card. I hope it save couple of productive\
          \ hours of other developers time.\r\n\r\nBest regards."
        updatedAt: '2023-07-20T09:15:48.972Z'
      numEdits: 0
      reactions: []
    id: 64b8fb44acfb31c3f7ae7ef7
    type: comment
  author: limcheekin
  content: "Thanks for convert and sharing the model.\r\n\r\nFirst, given the sample\
    \ code of the model card, it will raise the following error:\r\n``` \r\nValueError:\
    \ no suitable Tokenizer found.\r\n```\r\nSet the `tokenizer` using the `AutoTokenizer`\
    \ class doesn't help either.\r\n\r\nHence, I used the `LlamaTokenizer` and it\
    \ required the installation of the `sentencepiece` package.\r\n\r\nLastly, the\
    \ `sentencepiece` package will raise the `TypeError: not a string`, after added\
    \ the `tokenizer.model` file (which downloaded using the [download.sh](https://github.com/facebookresearch/llama/blob/main/download.sh))\
    \ to the model directory, finally it is working fine.\r\n\r\nPlease test it out\
    \ and update the model card. I hope it save couple of productive hours of other\
    \ developers time.\r\n\r\nBest regards."
  created_at: 2023-07-20 08:15:48+00:00
  edited: false
  hidden: false
  id: 64b8fb44acfb31c3f7ae7ef7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-07-20T12:11:03.000Z'
    data:
      edited: true
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9770597219467163
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: '<p>Thanks - added the 7B tokenizer now. Should resolve your error?</p>

          <p>Edit: Thanks for your detailed descriptions!</p>

          '
        raw: 'Thanks - added the 7B tokenizer now. Should resolve your error?


          Edit: Thanks for your detailed descriptions!'
        updatedAt: '2023-07-20T21:45:38.152Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - limcheekin
    id: 64b9245703124195cd95918b
    type: comment
  author: michaelfeil
  content: 'Thanks - added the 7B tokenizer now. Should resolve your error?


    Edit: Thanks for your detailed descriptions!'
  created_at: 2023-07-20 11:11:03+00:00
  edited: true
  hidden: false
  id: 64b9245703124195cd95918b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-07-21T15:04:32.000Z'
    data:
      status: closed
    id: 64ba9e80bc7873649693f302
    type: status-change
  author: michaelfeil
  created_at: 2023-07-21 14:04:32+00:00
  id: 64ba9e80bc7873649693f302
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: michaelfeil/ct2fast-Llama-2-13b-hf
repo_type: model
status: closed
target_branch: null
title: Missing tokenizer.model file and other errors
