!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mstachow
conflicting_files: null
created_at: 2023-07-20 14:14:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-07-20T15:14:34.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9537006616592407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>CPU generation is fairly slow. Can I move this model to CUDA?</p>

          '
        raw: CPU generation is fairly slow. Can I move this model to CUDA?
        updatedAt: '2023-07-20T15:14:34.139Z'
      numEdits: 0
      reactions: []
    id: 64b94f5a49bde5d9483a2d34
    type: comment
  author: mstachow
  content: CPU generation is fairly slow. Can I move this model to CUDA?
  created_at: 2023-07-20 14:14:34+00:00
  edited: false
  hidden: false
  id: 64b94f5a49bde5d9483a2d34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3a75844a801c5bd9953d77e102384d87.svg
      fullname: "Johannes Sj\xF6lund"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jsjolund
      type: user
    createdAt: '2023-07-20T23:58:46.000Z'
    data:
      edited: false
      editors:
      - jsjolund
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5740191340446472
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3a75844a801c5bd9953d77e102384d87.svg
          fullname: "Johannes Sj\xF6lund"
          isHf: false
          isPro: false
          name: jsjolund
          type: user
        html: "<p>You can do it like this</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoProcessor, AutoModel\n\nprocessor = AutoProcessor.from_pretrained(<span\
          \ class=\"hljs-string\">\"suno/bark-small\"</span>)\nmodel = AutoModel.from_pretrained(<span\
          \ class=\"hljs-string\">\"suno/bark-small\"</span>).to(<span class=\"hljs-string\"\
          >\"cuda\"</span>)\n\ninputs = processor(\n    text=[<span class=\"hljs-string\"\
          >\"Hello, my name is Suno. And, uh \u2014 and I like pizza. [laughs] But\
          \ I also have other interests such as playing tic tac toe.\"</span>],\n\
          \    return_tensors=<span class=\"hljs-string\">\"pt\"</span>,\n)\ninputs\
          \ = inputs.to(<span class=\"hljs-string\">\"cuda\"</span>)\nspeech_values\
          \ = model.generate(**inputs, do_sample=<span class=\"hljs-literal\">True</span>)\n\
          \n<span class=\"hljs-keyword\">from</span> IPython.display <span class=\"\
          hljs-keyword\">import</span> Audio\n\nsampling_rate = model.generation_config.sample_rate\n\
          Audio(speech_values.cpu().numpy().squeeze(), rate=sampling_rate)\n</code></pre>\n"
        raw: "You can do it like this\n\n```python\nfrom transformers import AutoProcessor,\
          \ AutoModel\n\nprocessor = AutoProcessor.from_pretrained(\"suno/bark-small\"\
          )\nmodel = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\")\n\
          \ninputs = processor(\n    text=[\"Hello, my name is Suno. And, uh \u2014\
          \ and I like pizza. [laughs] But I also have other interests such as playing\
          \ tic tac toe.\"],\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"\
          cuda\")\nspeech_values = model.generate(**inputs, do_sample=True)\n\nfrom\
          \ IPython.display import Audio\n\nsampling_rate = model.generation_config.sample_rate\n\
          Audio(speech_values.cpu().numpy().squeeze(), rate=sampling_rate)\n```"
        updatedAt: '2023-07-20T23:58:46.640Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F917"
        users:
        - matteopilotto
        - snys98
        - AndreLearning
        - skoll520
    id: 64b9ca369d8360edd985a148
    type: comment
  author: jsjolund
  content: "You can do it like this\n\n```python\nfrom transformers import AutoProcessor,\
    \ AutoModel\n\nprocessor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n\
    model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\")\n\ninputs\
    \ = processor(\n    text=[\"Hello, my name is Suno. And, uh \u2014 and I like\
    \ pizza. [laughs] But I also have other interests such as playing tic tac toe.\"\
    ],\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\nspeech_values\
    \ = model.generate(**inputs, do_sample=True)\n\nfrom IPython.display import Audio\n\
    \nsampling_rate = model.generation_config.sample_rate\nAudio(speech_values.cpu().numpy().squeeze(),\
    \ rate=sampling_rate)\n```"
  created_at: 2023-07-20 22:58:46+00:00
  edited: false
  hidden: false
  id: 64b9ca369d8360edd985a148
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-07-21T13:06:34.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46368739008903503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>Thank you!</p>

          '
        raw: Thank you!
        updatedAt: '2023-07-21T13:06:34.528Z'
      numEdits: 0
      reactions: []
    id: 64ba82da5e13c0d8659913b6
    type: comment
  author: mstachow
  content: Thank you!
  created_at: 2023-07-21 12:06:34+00:00
  edited: false
  hidden: false
  id: 64ba82da5e13c0d8659913b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
      fullname: Greg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: porky10111
      type: user
    createdAt: '2023-08-31T20:47:32.000Z'
    data:
      edited: false
      editors:
      - porky10111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.45362624526023865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
          fullname: Greg
          isHf: false
          isPro: false
          name: porky10111
          type: user
        html: "<p>I tried this and I get this error, any help please! :</p>\n<hr>\n\
          <p>AssertionError                            Traceback (most recent call\
          \ last)<br>Cell In[15], line 4<br>      1 from transformers import AutoProcessor,\
          \ AutoModel<br>      3 processor = AutoProcessor.from_pretrained(\"suno/bark-small\"\
          )<br>----&gt; 4 model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"\
          cuda\")<br>      6 inputs = processor(<br>      7     text=[\"Hello, my\
          \ name is Suno. And, uh \u2014 and I like pizza. [laughs] But I also have\
          \ other interests such as playing tic tac toe.\"],<br>      8     return_tensors=\"\
          pt\",<br>      9 )<br>     10 inputs = inputs.to(\"cuda\")</p>\n<p>File\
          \ ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\\
          modeling_utils.py:2014, in PreTrainedModel.to(self, *args, **kwargs)<br>\
          \   2009     raise ValueError(<br>   2010         \"<code>.to</code> is\
          \ not supported for <code>4-bit</code> or <code>8-bit</code> bitsandbytes\
          \ models. Please use the model as it is, since the\"<br>   2011        \
          \ \" model has already been set to the correct devices and casted to the\
          \ correct <code>dtype</code>.\"<br>   2012     )<br>   2013 else:<br>-&gt;\
          \ 2014     return super().to(*args, **kwargs)</p>\n<p>File ~\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\\
          module.py:1145, in Module.to(self, *args, **kwargs)<br>   1141         return\
          \ t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,<br>\
          \   1142                     non_blocking, memory_format=convert_to_format)<br>\
          \   1143     return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
          \ else None, non_blocking)<br>-&gt; 1145 return self._apply(convert)</p>\n\
          <p>File ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)<br>    795\
          \ def _apply(self, fn):<br>    796     for module in self.children():<br>--&gt;\
          \ 797         module._apply(fn)<br>    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):<br>    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):<br>    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,<br>    802             # the current\
          \ behavior is to change the tensor in-place using <code>.data =</code>,<br>\
          \   (...)<br>    807             # global flag to let the user control whether\
          \ they want the future<br>    808             # behavior of overwriting\
          \ the existing tensor or not.</p>\n<p>File ~\\AppData\\Local\\Programs\\\
          Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
          \ in Module._apply(self, fn)<br>    795 def _apply(self, fn):<br>    796\
          \     for module in self.children():<br>--&gt; 797         module._apply(fn)<br>\
          \    799     def compute_should_use_set_data(tensor, tensor_applied):<br>\
          \    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):<br>\
          \    801             # If the new tensor has compatible tensor type as the\
          \ existing tensor,<br>    802             # the current behavior is to change\
          \ the tensor in-place using <code>.data =</code>,<br>   (...)<br>    807\
          \             # global flag to let the user control whether they want the\
          \ future<br>    808             # behavior of overwriting the existing tensor\
          \ or not.</p>\n<p>File ~\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self,\
          \ fn)<br>    816 # Tensors stored in modules are graph leaves, and we don't\
          \ want to<br>    817 # track autograd history of <code>param_applied</code>,\
          \ so we have to use<br>    818 # <code>with torch.no_grad():</code><br>\
          \    819 with torch.no_grad():<br>--&gt; 820     param_applied = fn(param)<br>\
          \    821 should_use_set_data = compute_should_use_set_data(param, param_applied)<br>\
          \    822 if should_use_set_data:</p>\n<p>File ~\\AppData\\Local\\Programs\\\
          Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
          \ in Module.to..convert(t)<br>   1140 if convert_to_format is not None and\
          \ t.dim() in (4, 5):<br>   1141     return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None,<br>   1142                 non_blocking,\
          \ memory_format=convert_to_format)<br>-&gt; 1143 return t.to(device, dtype\
          \ if t.is_floating_point() or t.is_complex() else None, non_blocking)</p>\n\
          <p>File ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\cuda_<em>init</em>_.py:239, in _lazy_init()<br>    235     raise\
          \ RuntimeError(<br>    236         \"Cannot re-initialize CUDA in forked\
          \ subprocess. To use CUDA with \"<br>    237         \"multiprocessing,\
          \ you must use the 'spawn' start method\")<br>    238 if not hasattr(torch._C,\
          \ '_cuda_getDeviceCount'):<br>--&gt; 239     raise AssertionError(\"Torch\
          \ not compiled with CUDA enabled\")<br>    240 if _cudart is None:<br> \
          \   241     raise AssertionError(<br>    242         \"libcudart functions\
          \ unavailable. It looks like you have a broken build?\")</p>\n<p>AssertionError:\
          \ Torch not compiled with CUDA enabled</p>\n"
        raw: "I tried this and I get this error, any help please! :\n\n---------------------------------------------------------------------------\n\
          AssertionError                            Traceback (most recent call last)\n\
          Cell In[15], line 4\n      1 from transformers import AutoProcessor, AutoModel\n\
          \      3 processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n\
          ----> 4 model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\"\
          )\n      6 inputs = processor(\n      7     text=[\"Hello, my name is Suno.\
          \ And, uh \u2014 and I like pizza. [laughs] But I also have other interests\
          \ such as playing tic tac toe.\"],\n      8     return_tensors=\"pt\",\n\
          \      9 )\n     10 inputs = inputs.to(\"cuda\")\n\nFile ~\\AppData\\Local\\\
          Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2014,\
          \ in PreTrainedModel.to(self, *args, **kwargs)\n   2009     raise ValueError(\n\
          \   2010         \"`.to` is not supported for `4-bit` or `8-bit` bitsandbytes\
          \ models. Please use the model as it is, since the\"\n   2011         \"\
          \ model has already been set to the correct devices and casted to the correct\
          \ `dtype`.\"\n   2012     )\n   2013 else:\n-> 2014     return super().to(*args,\
          \ **kwargs)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
          site-packages\\torch\\nn\\modules\\module.py:1145, in Module.to(self, *args,\
          \ **kwargs)\n   1141         return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None,\n   1142                     non_blocking,\
          \ memory_format=convert_to_format)\n   1143     return t.to(device, dtype\
          \ if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
          -> 1145 return self._apply(convert)\n\nFile ~\\AppData\\Local\\Programs\\\
          Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
          \ in Module._apply(self, fn)\n    795 def _apply(self, fn):\n    796   \
          \  for module in self.children():\n--> 797         module._apply(fn)\n \
          \   799     def compute_should_use_set_data(tensor, tensor_applied):\n \
          \   800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    801             # If the new tensor has compatible tensor type as the\
          \ existing tensor,\n    802             # the current behavior is to change\
          \ the tensor in-place using `.data =`,\n   (...)\n    807             #\
          \ global flag to let the user control whether they want the future\n   \
          \ 808             # behavior of overwriting the existing tensor or not.\n\
          \nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)\n    795 def\
          \ _apply(self, fn):\n    796     for module in self.children():\n--> 797\
          \         module._apply(fn)\n    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):\n    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,\n    802             # the current\
          \ behavior is to change the tensor in-place using `.data =`,\n   (...)\n\
          \    807             # global flag to let the user control whether they\
          \ want the future\n    808             # behavior of overwriting the existing\
          \ tensor or not.\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self,\
          \ fn)\n    816 # Tensors stored in modules are graph leaves, and we don't\
          \ want to\n    817 # track autograd history of `param_applied`, so we have\
          \ to use\n    818 # `with torch.no_grad():`\n    819 with torch.no_grad():\n\
          --> 820     param_applied = fn(param)\n    821 should_use_set_data = compute_should_use_set_data(param,\
          \ param_applied)\n    822 if should_use_set_data:\n\nFile ~\\AppData\\Local\\\
          Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
          \ in Module.to.<locals>.convert(t)\n   1140 if convert_to_format is not\
          \ None and t.dim() in (4, 5):\n   1141     return t.to(device, dtype if\
          \ t.is_floating_point() or t.is_complex() else None,\n   1142          \
          \       non_blocking, memory_format=convert_to_format)\n-> 1143 return t.to(device,\
          \ dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
          \nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\cuda\\__init__.py:239, in _lazy_init()\n    235     raise RuntimeError(\n\
          \    236         \"Cannot re-initialize CUDA in forked subprocess. To use\
          \ CUDA with \"\n    237         \"multiprocessing, you must use the 'spawn'\
          \ start method\")\n    238 if not hasattr(torch._C, '_cuda_getDeviceCount'):\n\
          --> 239     raise AssertionError(\"Torch not compiled with CUDA enabled\"\
          )\n    240 if _cudart is None:\n    241     raise AssertionError(\n    242\
          \         \"libcudart functions unavailable. It looks like you have a broken\
          \ build?\")\n\nAssertionError: Torch not compiled with CUDA enabled"
        updatedAt: '2023-08-31T20:47:32.718Z'
      numEdits: 0
      reactions: []
    id: 64f0fc64ca02fd28d67b51fd
    type: comment
  author: porky10111
  content: "I tried this and I get this error, any help please! :\n\n---------------------------------------------------------------------------\n\
    AssertionError                            Traceback (most recent call last)\n\
    Cell In[15], line 4\n      1 from transformers import AutoProcessor, AutoModel\n\
    \      3 processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n---->\
    \ 4 model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\")\n    \
    \  6 inputs = processor(\n      7     text=[\"Hello, my name is Suno. And, uh\
    \ \u2014 and I like pizza. [laughs] But I also have other interests such as playing\
    \ tic tac toe.\"],\n      8     return_tensors=\"pt\",\n      9 )\n     10 inputs\
    \ = inputs.to(\"cuda\")\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\\
    Lib\\site-packages\\transformers\\modeling_utils.py:2014, in PreTrainedModel.to(self,\
    \ *args, **kwargs)\n   2009     raise ValueError(\n   2010         \"`.to` is\
    \ not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model\
    \ as it is, since the\"\n   2011         \" model has already been set to the\
    \ correct devices and casted to the correct `dtype`.\"\n   2012     )\n   2013\
    \ else:\n-> 2014     return super().to(*args, **kwargs)\n\nFile ~\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1145,\
    \ in Module.to(self, *args, **kwargs)\n   1141         return t.to(device, dtype\
    \ if t.is_floating_point() or t.is_complex() else None,\n   1142             \
    \        non_blocking, memory_format=convert_to_format)\n   1143     return t.to(device,\
    \ dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
    -> 1145 return self._apply(convert)\n\nFile ~\\AppData\\Local\\Programs\\Python\\\
    Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797, in Module._apply(self,\
    \ fn)\n    795 def _apply(self, fn):\n    796     for module in self.children():\n\
    --> 797         module._apply(fn)\n    799     def compute_should_use_set_data(tensor,\
    \ tensor_applied):\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
    \ tensor_applied):\n    801             # If the new tensor has compatible tensor\
    \ type as the existing tensor,\n    802             # the current behavior is\
    \ to change the tensor in-place using `.data =`,\n   (...)\n    807          \
    \   # global flag to let the user control whether they want the future\n    808\
    \             # behavior of overwriting the existing tensor or not.\n\nFile ~\\\
    AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\\
    module.py:797, in Module._apply(self, fn)\n    795 def _apply(self, fn):\n   \
    \ 796     for module in self.children():\n--> 797         module._apply(fn)\n\
    \    799     def compute_should_use_set_data(tensor, tensor_applied):\n    800\
    \         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
    \    801             # If the new tensor has compatible tensor type as the existing\
    \ tensor,\n    802             # the current behavior is to change the tensor\
    \ in-place using `.data =`,\n   (...)\n    807             # global flag to let\
    \ the user control whether they want the future\n    808             # behavior\
    \ of overwriting the existing tensor or not.\n\nFile ~\\AppData\\Local\\Programs\\\
    Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self,\
    \ fn)\n    816 # Tensors stored in modules are graph leaves, and we don't want\
    \ to\n    817 # track autograd history of `param_applied`, so we have to use\n\
    \    818 # `with torch.no_grad():`\n    819 with torch.no_grad():\n--> 820   \
    \  param_applied = fn(param)\n    821 should_use_set_data = compute_should_use_set_data(param,\
    \ param_applied)\n    822 if should_use_set_data:\n\nFile ~\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
    \ in Module.to.<locals>.convert(t)\n   1140 if convert_to_format is not None and\
    \ t.dim() in (4, 5):\n   1141     return t.to(device, dtype if t.is_floating_point()\
    \ or t.is_complex() else None,\n   1142                 non_blocking, memory_format=convert_to_format)\n\
    -> 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else\
    \ None, non_blocking)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\\
    Lib\\site-packages\\torch\\cuda\\__init__.py:239, in _lazy_init()\n    235   \
    \  raise RuntimeError(\n    236         \"Cannot re-initialize CUDA in forked\
    \ subprocess. To use CUDA with \"\n    237         \"multiprocessing, you must\
    \ use the 'spawn' start method\")\n    238 if not hasattr(torch._C, '_cuda_getDeviceCount'):\n\
    --> 239     raise AssertionError(\"Torch not compiled with CUDA enabled\")\n \
    \   240 if _cudart is None:\n    241     raise AssertionError(\n    242      \
    \   \"libcudart functions unavailable. It looks like you have a broken build?\"\
    )\n\nAssertionError: Torch not compiled with CUDA enabled"
  created_at: 2023-08-31 19:47:32+00:00
  edited: false
  hidden: false
  id: 64f0fc64ca02fd28d67b51fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
      fullname: Greg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: porky10111
      type: user
    createdAt: '2023-08-31T21:03:41.000Z'
    data:
      edited: false
      editors:
      - porky10111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7905194759368896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
          fullname: Greg
          isHf: false
          isPro: false
          name: porky10111
          type: user
        html: '<p>I tried running this command to see if CUDA is enabled :</p>

          <p>python -c "import torch; print(torch.cuda.is_available())"</p>

          <p>I get "False". </p>

          <p>I tried installing CUDA with pip installs but I get the same error?</p>

          '
        raw: "I tried running this command to see if CUDA is enabled :\n\npython -c\
          \ \"import torch; print(torch.cuda.is_available())\"\n\n\nI get \"False\"\
          . \n\nI tried installing CUDA with pip installs but I get the same error?"
        updatedAt: '2023-08-31T21:03:41.300Z'
      numEdits: 0
      reactions: []
    id: 64f1002d7c83fd1d21677e59
    type: comment
  author: porky10111
  content: "I tried running this command to see if CUDA is enabled :\n\npython -c\
    \ \"import torch; print(torch.cuda.is_available())\"\n\n\nI get \"False\". \n\n\
    I tried installing CUDA with pip installs but I get the same error?"
  created_at: 2023-08-31 20:03:41+00:00
  edited: false
  hidden: false
  id: 64f1002d7c83fd1d21677e59
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-08-31T21:51:39.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9355096220970154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>What''s your os? Did you install the CUDA version of torch?</p>

          '
        raw: What's your os? Did you install the CUDA version of torch?
        updatedAt: '2023-08-31T21:51:39.760Z'
      numEdits: 0
      reactions: []
    id: 64f10b6b00b032576ea5cb05
    type: comment
  author: mstachow
  content: What's your os? Did you install the CUDA version of torch?
  created_at: 2023-08-31 20:51:39+00:00
  edited: false
  hidden: false
  id: 64f10b6b00b032576ea5cb05
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
      fullname: Greg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: porky10111
      type: user
    createdAt: '2023-08-31T21:58:46.000Z'
    data:
      edited: false
      editors:
      - porky10111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9430825710296631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
          fullname: Greg
          isHf: false
          isPro: false
          name: porky10111
          type: user
        html: '<p>Windows 11. </p>

          <p>Im not sure I have installed loads of dependencies. I did check and it
          was on my system. Anyway you could share commands that I can install the
          correct version of cuda plus any dependencies im missing?</p>

          '
        raw: "Windows 11. \n\nIm not sure I have installed loads of dependencies.\
          \ I did check and it was on my system. Anyway you could share commands that\
          \ I can install the correct version of cuda plus any dependencies im missing?"
        updatedAt: '2023-08-31T21:58:46.162Z'
      numEdits: 0
      reactions: []
    id: 64f10d16492f469e22688e0a
    type: comment
  author: porky10111
  content: "Windows 11. \n\nIm not sure I have installed loads of dependencies. I\
    \ did check and it was on my system. Anyway you could share commands that I can\
    \ install the correct version of cuda plus any dependencies im missing?"
  created_at: 2023-08-31 20:58:46+00:00
  edited: false
  hidden: false
  id: 64f10d16492f469e22688e0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
      fullname: Mike Cooper-Stachowsky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mstachow
      type: user
    createdAt: '2023-09-01T02:49:08.000Z'
    data:
      edited: false
      editors:
      - mstachow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9457284808158875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ab05f77592cc3da78c2af2e77c409e9d.svg
          fullname: Mike Cooper-Stachowsky
          isHf: false
          isPro: false
          name: mstachow
          type: user
        html: '<p>It''s not CUDA I think it''s torch. If you just do pip install torch
          it isn''t the right version. Google  pytorch install with CUDA and you will
          get to the right website with exactly the code you need</p>

          '
        raw: It's not CUDA I think it's torch. If you just do pip install torch it
          isn't the right version. Google  pytorch install with CUDA and you will
          get to the right website with exactly the code you need
        updatedAt: '2023-09-01T02:49:08.346Z'
      numEdits: 0
      reactions: []
    id: 64f1512481f3e474c6b9946d
    type: comment
  author: mstachow
  content: It's not CUDA I think it's torch. If you just do pip install torch it isn't
    the right version. Google  pytorch install with CUDA and you will get to the right
    website with exactly the code you need
  created_at: 2023-09-01 01:49:08+00:00
  edited: false
  hidden: false
  id: 64f1512481f3e474c6b9946d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
      fullname: Greg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: porky10111
      type: user
    createdAt: '2023-09-01T08:38:05.000Z'
    data:
      edited: false
      editors:
      - porky10111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9593815803527832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
          fullname: Greg
          isHf: false
          isPro: false
          name: porky10111
          type: user
        html: '<p>Ok ill try that thanks!</p>

          '
        raw: 'Ok ill try that thanks!

          '
        updatedAt: '2023-09-01T08:38:05.897Z'
      numEdits: 0
      reactions: []
    id: 64f1a2ed737ef9c47e8a5cb7
    type: comment
  author: porky10111
  content: 'Ok ill try that thanks!

    '
  created_at: 2023-09-01 07:38:05+00:00
  edited: false
  hidden: false
  id: 64f1a2ed737ef9c47e8a5cb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
      fullname: Greg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: porky10111
      type: user
    createdAt: '2023-09-01T08:42:08.000Z'
    data:
      edited: true
      editors:
      - porky10111
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7547750473022461
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c4d80b9970c640601fd847a81aa67234.svg
          fullname: Greg
          isHf: false
          isPro: false
          name: porky10111
          type: user
        html: "<p>I am getting the same assertion error.<br>I did what you said and\
          \ I then checked my version and I get this below (note : I am still getting\
          \ the assertion error) : </p>\n<p>Name: torch<br>Version: 2.0.0<br>Summary:\
          \ Tensors and Dynamic neural networks in Python with strong GPU acceleration<br>Home-page:\
          \ <a rel=\"nofollow\" href=\"https://pytorch.org/\">https://pytorch.org/</a><br>Author:\
          \ PyTorch Team<br>Author-email: <a rel=\"nofollow\" href=\"mailto:packages@pytorch.org\"\
          >packages@pytorch.org</a><br>License: BSD-3<br>Location: C:\\Users\\Greg\\\
          AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages<br>Requires:\
          \ filelock, jinja2, networkx, sympy, typing-extensions<br>Required-by: accelerate,\
          \ fairscale, optimum, speechbrain, torchaudio, torchvision</p>\n<p>Here\
          \ is full error : </p>\n<p>C:\\Users\\Greg\\AppData\\Local\\Programs\\Python\\\
          Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress\
          \ not found. Please update jupyter and ipywidgets. See <a rel=\"nofollow\"\
          \ href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\"\
          >https://ipywidgets.readthedocs.io/en/stable/user_install.html</a><br> \
          \ from .autonotebook import tqdm as notebook_tqdm</p>\n<hr>\n<p>AssertionError\
          \                            Traceback (most recent call last)<br>Cell In[1],\
          \ line 4<br>      1 from transformers import AutoProcessor, AutoModel<br>\
          \      3 processor = AutoProcessor.from_pretrained(\"suno/bark-small\")<br>----&gt;\
          \ 4 model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\")<br>\
          \      6 inputs = processor(<br>      7     text=[\"Hello, my name is Suno.\
          \ And, uh \u2014 and I like pizza. [laughs] But I also have other interests\
          \ such as playing tic tac toe.\"],<br>      8     return_tensors=\"pt\"\
          ,<br>      9 )<br>     10 inputs = inputs.to(\"cuda\")</p>\n<p>File ~\\\
          AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\\
          modeling_utils.py:2014, in PreTrainedModel.to(self, *args, **kwargs)<br>\
          \   2009     raise ValueError(<br>   2010         \"<code>.to</code> is\
          \ not supported for <code>4-bit</code> or <code>8-bit</code> bitsandbytes\
          \ models. Please use the model as it is, since the\"<br>   2011        \
          \ \" model has already been set to the correct devices and casted to the\
          \ correct <code>dtype</code>.\"<br>   2012     )<br>   2013 else:<br>-&gt;\
          \ 2014     return super().to(*args, **kwargs)</p>\n<p>File ~\\AppData\\\
          Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\\
          module.py:1145, in Module.to(self, *args, **kwargs)<br>   1141         return\
          \ t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,<br>\
          \   1142                     non_blocking, memory_format=convert_to_format)<br>\
          \   1143     return t.to(device, dtype if t.is_floating_point() or t.is_complex()\
          \ else None, non_blocking)<br>-&gt; 1145 return self._apply(convert)</p>\n\
          <p>File ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)<br>    795\
          \ def _apply(self, fn):<br>    796     for module in self.children():<br>--&gt;\
          \ 797         module._apply(fn)<br>    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):<br>    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):<br>    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,<br>    802             # the current\
          \ behavior is to change the tensor in-place using <code>.data =</code>,<br>\
          \   (...)<br>    807             # global flag to let the user control whether\
          \ they want the future<br>    808             # behavior of overwriting\
          \ the existing tensor or not.</p>\n<p>File ~\\AppData\\Local\\Programs\\\
          Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
          \ in Module._apply(self, fn)<br>    795 def _apply(self, fn):<br>    796\
          \     for module in self.children():<br>--&gt; 797         module._apply(fn)<br>\
          \    799     def compute_should_use_set_data(tensor, tensor_applied):<br>\
          \    800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):<br>\
          \    801             # If the new tensor has compatible tensor type as the\
          \ existing tensor,<br>    802             # the current behavior is to change\
          \ the tensor in-place using <code>.data =</code>,<br>   (...)<br>    807\
          \             # global flag to let the user control whether they want the\
          \ future<br>    808             # behavior of overwriting the existing tensor\
          \ or not.</p>\n<p>File ~\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self,\
          \ fn)<br>    816 # Tensors stored in modules are graph leaves, and we don't\
          \ want to<br>    817 # track autograd history of <code>param_applied</code>,\
          \ so we have to use<br>    818 # <code>with torch.no_grad():</code><br>\
          \    819 with torch.no_grad():<br>--&gt; 820     param_applied = fn(param)<br>\
          \    821 should_use_set_data = compute_should_use_set_data(param, param_applied)<br>\
          \    822 if should_use_set_data:</p>\n<p>File ~\\AppData\\Local\\Programs\\\
          Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
          \ in Module.to..convert(t)<br>   1140 if convert_to_format is not None and\
          \ t.dim() in (4, 5):<br>   1141     return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None,<br>   1142                 non_blocking,\
          \ memory_format=convert_to_format)<br>-&gt; 1143 return t.to(device, dtype\
          \ if t.is_floating_point() or t.is_complex() else None, non_blocking)</p>\n\
          <p>File ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\cuda_<em>init</em>_.py:239, in _lazy_init()<br>    235     raise\
          \ RuntimeError(<br>    236         \"Cannot re-initialize CUDA in forked\
          \ subprocess. To use CUDA with \"<br>    237         \"multiprocessing,\
          \ you must use the 'spawn' start method\")<br>    238 if not hasattr(torch._C,\
          \ '_cuda_getDeviceCount'):<br>--&gt; 239     raise AssertionError(\"Torch\
          \ not compiled with CUDA enabled\")<br>    240 if _cudart is None:<br> \
          \   241     raise AssertionError(<br>    242         \"libcudart functions\
          \ unavailable. It looks like you have a broken build?\")</p>\n<p>AssertionError:\
          \ Torch not compiled with CUDA enabled</p>\n"
        raw: "I am getting the same assertion error. \nI did what you said and I then\
          \ checked my version and I get this below (note : I am still getting the\
          \ assertion error) : \n\nName: torch\nVersion: 2.0.0\nSummary: Tensors and\
          \ Dynamic neural networks in Python with strong GPU acceleration\nHome-page:\
          \ https://pytorch.org/\nAuthor: PyTorch Team\nAuthor-email: packages@pytorch.org\n\
          License: BSD-3\nLocation: C:\\Users\\Greg\\AppData\\Local\\Programs\\Python\\\
          Python311\\Lib\\site-packages\nRequires: filelock, jinja2, networkx, sympy,\
          \ typing-extensions\nRequired-by: accelerate, fairscale, optimum, speechbrain,\
          \ torchaudio, torchvision\n\n\nHere is full error : \n\nC:\\Users\\Greg\\\
          AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\\
          auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and\
          \ ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\
          \  from .autonotebook import tqdm as notebook_tqdm\n---------------------------------------------------------------------------\n\
          AssertionError                            Traceback (most recent call last)\n\
          Cell In[1], line 4\n      1 from transformers import AutoProcessor, AutoModel\n\
          \      3 processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n\
          ----> 4 model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\"\
          )\n      6 inputs = processor(\n      7     text=[\"Hello, my name is Suno.\
          \ And, uh \u2014 and I like pizza. [laughs] But I also have other interests\
          \ such as playing tic tac toe.\"],\n      8     return_tensors=\"pt\",\n\
          \      9 )\n     10 inputs = inputs.to(\"cuda\")\n\nFile ~\\AppData\\Local\\\
          Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2014,\
          \ in PreTrainedModel.to(self, *args, **kwargs)\n   2009     raise ValueError(\n\
          \   2010         \"`.to` is not supported for `4-bit` or `8-bit` bitsandbytes\
          \ models. Please use the model as it is, since the\"\n   2011         \"\
          \ model has already been set to the correct devices and casted to the correct\
          \ `dtype`.\"\n   2012     )\n   2013 else:\n-> 2014     return super().to(*args,\
          \ **kwargs)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\\
          site-packages\\torch\\nn\\modules\\module.py:1145, in Module.to(self, *args,\
          \ **kwargs)\n   1141         return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None,\n   1142                     non_blocking,\
          \ memory_format=convert_to_format)\n   1143     return t.to(device, dtype\
          \ if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
          -> 1145 return self._apply(convert)\n\nFile ~\\AppData\\Local\\Programs\\\
          Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797,\
          \ in Module._apply(self, fn)\n    795 def _apply(self, fn):\n    796   \
          \  for module in self.children():\n--> 797         module._apply(fn)\n \
          \   799     def compute_should_use_set_data(tensor, tensor_applied):\n \
          \   800         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
          \    801             # If the new tensor has compatible tensor type as the\
          \ existing tensor,\n    802             # the current behavior is to change\
          \ the tensor in-place using `.data =`,\n   (...)\n    807             #\
          \ global flag to let the user control whether they want the future\n   \
          \ 808             # behavior of overwriting the existing tensor or not.\n\
          \nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\nn\\modules\\module.py:797, in Module._apply(self, fn)\n    795 def\
          \ _apply(self, fn):\n    796     for module in self.children():\n--> 797\
          \         module._apply(fn)\n    799     def compute_should_use_set_data(tensor,\
          \ tensor_applied):\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
          \ tensor_applied):\n    801             # If the new tensor has compatible\
          \ tensor type as the existing tensor,\n    802             # the current\
          \ behavior is to change the tensor in-place using `.data =`,\n   (...)\n\
          \    807             # global flag to let the user control whether they\
          \ want the future\n    808             # behavior of overwriting the existing\
          \ tensor or not.\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\\
          Lib\\site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self,\
          \ fn)\n    816 # Tensors stored in modules are graph leaves, and we don't\
          \ want to\n    817 # track autograd history of `param_applied`, so we have\
          \ to use\n    818 # `with torch.no_grad():`\n    819 with torch.no_grad():\n\
          --> 820     param_applied = fn(param)\n    821 should_use_set_data = compute_should_use_set_data(param,\
          \ param_applied)\n    822 if should_use_set_data:\n\nFile ~\\AppData\\Local\\\
          Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
          \ in Module.to.<locals>.convert(t)\n   1140 if convert_to_format is not\
          \ None and t.dim() in (4, 5):\n   1141     return t.to(device, dtype if\
          \ t.is_floating_point() or t.is_complex() else None,\n   1142          \
          \       non_blocking, memory_format=convert_to_format)\n-> 1143 return t.to(device,\
          \ dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
          \nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
          torch\\cuda\\__init__.py:239, in _lazy_init()\n    235     raise RuntimeError(\n\
          \    236         \"Cannot re-initialize CUDA in forked subprocess. To use\
          \ CUDA with \"\n    237         \"multiprocessing, you must use the 'spawn'\
          \ start method\")\n    238 if not hasattr(torch._C, '_cuda_getDeviceCount'):\n\
          --> 239     raise AssertionError(\"Torch not compiled with CUDA enabled\"\
          )\n    240 if _cudart is None:\n    241     raise AssertionError(\n    242\
          \         \"libcudart functions unavailable. It looks like you have a broken\
          \ build?\")\n\nAssertionError: Torch not compiled with CUDA enabled"
        updatedAt: '2023-09-01T08:42:52.362Z'
      numEdits: 1
      reactions: []
    id: 64f1a3e079dc71a66af7c0bd
    type: comment
  author: porky10111
  content: "I am getting the same assertion error. \nI did what you said and I then\
    \ checked my version and I get this below (note : I am still getting the assertion\
    \ error) : \n\nName: torch\nVersion: 2.0.0\nSummary: Tensors and Dynamic neural\
    \ networks in Python with strong GPU acceleration\nHome-page: https://pytorch.org/\n\
    Author: PyTorch Team\nAuthor-email: packages@pytorch.org\nLicense: BSD-3\nLocation:\
    \ C:\\Users\\Greg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n\
    Requires: filelock, jinja2, networkx, sympy, typing-extensions\nRequired-by: accelerate,\
    \ fairscale, optimum, speechbrain, torchaudio, torchvision\n\n\nHere is full error\
    \ : \n\nC:\\Users\\Greg\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\\
    tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and\
    \ ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n\
    \  from .autonotebook import tqdm as notebook_tqdm\n---------------------------------------------------------------------------\n\
    AssertionError                            Traceback (most recent call last)\n\
    Cell In[1], line 4\n      1 from transformers import AutoProcessor, AutoModel\n\
    \      3 processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n---->\
    \ 4 model = AutoModel.from_pretrained(\"suno/bark-small\").to(\"cuda\")\n    \
    \  6 inputs = processor(\n      7     text=[\"Hello, my name is Suno. And, uh\
    \ \u2014 and I like pizza. [laughs] But I also have other interests such as playing\
    \ tic tac toe.\"],\n      8     return_tensors=\"pt\",\n      9 )\n     10 inputs\
    \ = inputs.to(\"cuda\")\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\\
    Lib\\site-packages\\transformers\\modeling_utils.py:2014, in PreTrainedModel.to(self,\
    \ *args, **kwargs)\n   2009     raise ValueError(\n   2010         \"`.to` is\
    \ not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model\
    \ as it is, since the\"\n   2011         \" model has already been set to the\
    \ correct devices and casted to the correct `dtype`.\"\n   2012     )\n   2013\
    \ else:\n-> 2014     return super().to(*args, **kwargs)\n\nFile ~\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1145,\
    \ in Module.to(self, *args, **kwargs)\n   1141         return t.to(device, dtype\
    \ if t.is_floating_point() or t.is_complex() else None,\n   1142             \
    \        non_blocking, memory_format=convert_to_format)\n   1143     return t.to(device,\
    \ dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n\
    -> 1145 return self._apply(convert)\n\nFile ~\\AppData\\Local\\Programs\\Python\\\
    Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797, in Module._apply(self,\
    \ fn)\n    795 def _apply(self, fn):\n    796     for module in self.children():\n\
    --> 797         module._apply(fn)\n    799     def compute_should_use_set_data(tensor,\
    \ tensor_applied):\n    800         if torch._has_compatible_shallow_copy_type(tensor,\
    \ tensor_applied):\n    801             # If the new tensor has compatible tensor\
    \ type as the existing tensor,\n    802             # the current behavior is\
    \ to change the tensor in-place using `.data =`,\n   (...)\n    807          \
    \   # global flag to let the user control whether they want the future\n    808\
    \             # behavior of overwriting the existing tensor or not.\n\nFile ~\\\
    AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\\
    module.py:797, in Module._apply(self, fn)\n    795 def _apply(self, fn):\n   \
    \ 796     for module in self.children():\n--> 797         module._apply(fn)\n\
    \    799     def compute_should_use_set_data(tensor, tensor_applied):\n    800\
    \         if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\
    \    801             # If the new tensor has compatible tensor type as the existing\
    \ tensor,\n    802             # the current behavior is to change the tensor\
    \ in-place using `.data =`,\n   (...)\n    807             # global flag to let\
    \ the user control whether they want the future\n    808             # behavior\
    \ of overwriting the existing tensor or not.\n\nFile ~\\AppData\\Local\\Programs\\\
    Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:820, in Module._apply(self,\
    \ fn)\n    816 # Tensors stored in modules are graph leaves, and we don't want\
    \ to\n    817 # track autograd history of `param_applied`, so we have to use\n\
    \    818 # `with torch.no_grad():`\n    819 with torch.no_grad():\n--> 820   \
    \  param_applied = fn(param)\n    821 should_use_set_data = compute_should_use_set_data(param,\
    \ param_applied)\n    822 if should_use_set_data:\n\nFile ~\\AppData\\Local\\\
    Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1143,\
    \ in Module.to.<locals>.convert(t)\n   1140 if convert_to_format is not None and\
    \ t.dim() in (4, 5):\n   1141     return t.to(device, dtype if t.is_floating_point()\
    \ or t.is_complex() else None,\n   1142                 non_blocking, memory_format=convert_to_format)\n\
    -> 1143 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else\
    \ None, non_blocking)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\\
    Lib\\site-packages\\torch\\cuda\\__init__.py:239, in _lazy_init()\n    235   \
    \  raise RuntimeError(\n    236         \"Cannot re-initialize CUDA in forked\
    \ subprocess. To use CUDA with \"\n    237         \"multiprocessing, you must\
    \ use the 'spawn' start method\")\n    238 if not hasattr(torch._C, '_cuda_getDeviceCount'):\n\
    --> 239     raise AssertionError(\"Torch not compiled with CUDA enabled\")\n \
    \   240 if _cudart is None:\n    241     raise AssertionError(\n    242      \
    \   \"libcudart functions unavailable. It looks like you have a broken build?\"\
    )\n\nAssertionError: Torch not compiled with CUDA enabled"
  created_at: 2023-09-01 07:42:08+00:00
  edited: true
  hidden: false
  id: 64f1a3e079dc71a66af7c0bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6K-SdVza7Q4Q842Swkzd8.jpeg?w=200&h=200&f=face
      fullname: JamesLiu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesljl
      type: user
    createdAt: '2024-01-04T08:02:22.000Z'
    data:
      edited: true
      editors:
      - jamesljl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7569475173950195
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/6K-SdVza7Q4Q842Swkzd8.jpeg?w=200&h=200&f=face
          fullname: JamesLiu
          isHf: false
          isPro: false
          name: jamesljl
          type: user
        html: '<p>"Torch not compiled with CUDA enabled" means your torch version
          is not compatible with your cuda version or your nvidia driver. download
          a gpu version torch instead of a cpu version. <a rel="nofollow" href="https://download.pytorch.org/whl/torch_stable.html">https://download.pytorch.org/whl/torch_stable.html</a><br>use
          "nvidia-smi"  and  "nvcc -V"  cmd to check cuda and nvidia driver</p>

          '
        raw: '"Torch not compiled with CUDA enabled" means your torch version is not
          compatible with your cuda version or your nvidia driver. download a gpu
          version torch instead of a cpu version. https://download.pytorch.org/whl/torch_stable.html

          use "nvidia-smi"  and  "nvcc -V"  cmd to check cuda and nvidia driver'
        updatedAt: '2024-01-04T08:03:29.175Z'
      numEdits: 1
      reactions: []
    id: 6596660e7cda5685b34cbf3c
    type: comment
  author: jamesljl
  content: '"Torch not compiled with CUDA enabled" means your torch version is not
    compatible with your cuda version or your nvidia driver. download a gpu version
    torch instead of a cpu version. https://download.pytorch.org/whl/torch_stable.html

    use "nvidia-smi"  and  "nvcc -V"  cmd to check cuda and nvidia driver'
  created_at: 2024-01-04 08:02:22+00:00
  edited: true
  hidden: false
  id: 6596660e7cda5685b34cbf3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 12
repo_id: suno/bark
repo_type: model
status: open
target_branch: null
title: Possibility to use on CUDA?
