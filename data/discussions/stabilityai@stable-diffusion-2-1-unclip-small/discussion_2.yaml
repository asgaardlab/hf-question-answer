!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jfelectron
conflicting_files: null
created_at: 2023-10-31 17:01:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b22dbc905444c8d3ab5035a8e2e99b3.svg
      fullname: Jonathan E Foley
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jfelectron
      type: user
    createdAt: '2023-10-31T18:01:02.000Z'
    data:
      edited: false
      editors:
      - jfelectron
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9569803476333618
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b22dbc905444c8d3ab5035a8e2e99b3.svg
          fullname: Jonathan E Foley
          isHf: false
          isPro: false
          name: jfelectron
          type: user
        html: '<p>We''re using unCLIP for img2img and the StableUnCLIPImg2Img2Pipeline
          works well enough but we think it would perform better if fine tuned on
          our dataset. It''s not clear to me how to do this. From poking around the
          docs, it would seem like the necessary component models (TextEncoder, ImageEncoder,
          Tokenizer..) could be loaded somehow but I haven''t been able to determine
          how. </p>

          '
        raw: 'We''re using unCLIP for img2img and the StableUnCLIPImg2Img2Pipeline
          works well enough but we think it would perform better if fine tuned on
          our dataset. It''s not clear to me how to do this. From poking around the
          docs, it would seem like the necessary component models (TextEncoder, ImageEncoder,
          Tokenizer..) could be loaded somehow but I haven''t been able to determine
          how. '
        updatedAt: '2023-10-31T18:01:02.782Z'
      numEdits: 0
      reactions: []
    id: 654140de76262f49967ebc75
    type: comment
  author: jfelectron
  content: 'We''re using unCLIP for img2img and the StableUnCLIPImg2Img2Pipeline works
    well enough but we think it would perform better if fine tuned on our dataset.
    It''s not clear to me how to do this. From poking around the docs, it would seem
    like the necessary component models (TextEncoder, ImageEncoder, Tokenizer..) could
    be loaded somehow but I haven''t been able to determine how. '
  created_at: 2023-10-31 17:01:02+00:00
  edited: false
  hidden: false
  id: 654140de76262f49967ebc75
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: stabilityai/stable-diffusion-2-1-unclip-small
repo_type: model
status: open
target_branch: null
title: fine tuning on domain specific images/prompts
