!!python/object:huggingface_hub.community.DiscussionWithDetails
author: downquark
conflicting_files: null
created_at: 2023-12-18 16:08:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
      fullname: Bob Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: downquark
      type: user
    createdAt: '2023-12-18T16:08:52.000Z'
    data:
      edited: false
      editors:
      - downquark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7027326226234436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
          fullname: Bob Ross
          isHf: false
          isPro: false
          name: downquark
          type: user
        html: '<p>Hi. Trying to loading this model using the oobabooga web-ui, results
          in an error:</p>

          <blockquote>

          <p>ValueError: ## Could not find model.layers.0.mlp.down_proj.* in model</p>

          </blockquote>

          <p>This is the commit I''m using<br><a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/commit/b28020a9e444ce9a3a872b7b507d771b8e653198">https://github.com/oobabooga/text-generation-webui/commit/b28020a9e444ce9a3a872b7b507d771b8e653198</a></p>

          <p>Any advice how this model can be loaded? Thanks</p>

          '
        raw: "Hi. Trying to loading this model using the oobabooga web-ui, results\
          \ in an error:\r\n> ValueError: ## Could not find model.layers.0.mlp.down_proj.*\
          \ in model\r\n\r\nThis is the commit I'm using\r\nhttps://github.com/oobabooga/text-generation-webui/commit/b28020a9e444ce9a3a872b7b507d771b8e653198\r\
          \n\r\nAny advice how this model can be loaded? Thanks"
        updatedAt: '2023-12-18T16:08:52.053Z'
      numEdits: 0
      reactions: []
    id: 65806e94c15f9833025ed851
    type: comment
  author: downquark
  content: "Hi. Trying to loading this model using the oobabooga web-ui, results in\
    \ an error:\r\n> ValueError: ## Could not find model.layers.0.mlp.down_proj.*\
    \ in model\r\n\r\nThis is the commit I'm using\r\nhttps://github.com/oobabooga/text-generation-webui/commit/b28020a9e444ce9a3a872b7b507d771b8e653198\r\
    \n\r\nAny advice how this model can be loaded? Thanks"
  created_at: 2023-12-18 16:08:52+00:00
  edited: false
  hidden: false
  id: 65806e94c15f9833025ed851
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
      fullname: Bob Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: downquark
      type: user
    createdAt: '2023-12-18T16:10:32.000Z'
    data:
      edited: false
      editors:
      - downquark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3574206531047821
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
          fullname: Bob Ross
          isHf: false
          isPro: false
          name: downquark
          type: user
        html: '<p>Full stacktrace</p>

          <pre><code> File "/workspace/text-generation-webui/modules/ui_model_menu.py",
          line 210, in load_model_wrapper

          shared.model, shared.tokenizer = load_model(selected_model, loader)


          File "/workspace/text-generation-webui/modules/models.py", line 89, in load_model

          output = load_func_map[loader](model_name)


          File "/workspace/text-generation-webui/modules/models.py", line 411, in
          ExLlamav2_HF_loader

          return Exllamav2HF.from_pretrained(model_name)


          File "/workspace/text-generation-webui/modules/exllamav2_hf.py", line 162,
          in from_pretrained

          config.prepare()


          File "/usr/local/lib/python3.10/dist-packages/exllamav2/config.py", line
          156, in prepare

          raise ValueError(f" ## Could not find {prefix}.* in model")


          ValueError: ## Could not find model.layers.0.mlp.down_proj.* in model

          </code></pre>

          '
        raw: "Full stacktrace\n\n```\n File \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
          , line 210, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(selected_model,\
          \ loader)\n\nFile \"/workspace/text-generation-webui/modules/models.py\"\
          , line 89, in load_model\noutput = load_func_map[loader](model_name)\n\n\
          File \"/workspace/text-generation-webui/modules/models.py\", line 411, in\
          \ ExLlamav2_HF_loader\nreturn Exllamav2HF.from_pretrained(model_name)\n\n\
          File \"/workspace/text-generation-webui/modules/exllamav2_hf.py\", line\
          \ 162, in from_pretrained\nconfig.prepare()\n\nFile \"/usr/local/lib/python3.10/dist-packages/exllamav2/config.py\"\
          , line 156, in prepare\nraise ValueError(f\" ## Could not find {prefix}.*\
          \ in model\")\n\nValueError: ## Could not find model.layers.0.mlp.down_proj.*\
          \ in model\n```\n"
        updatedAt: '2023-12-18T16:10:32.491Z'
      numEdits: 0
      reactions: []
    id: 65806ef8d0dfc3caf6b3392d
    type: comment
  author: downquark
  content: "Full stacktrace\n\n```\n File \"/workspace/text-generation-webui/modules/ui_model_menu.py\"\
    , line 210, in load_model_wrapper\nshared.model, shared.tokenizer = load_model(selected_model,\
    \ loader)\n\nFile \"/workspace/text-generation-webui/modules/models.py\", line\
    \ 89, in load_model\noutput = load_func_map[loader](model_name)\n\nFile \"/workspace/text-generation-webui/modules/models.py\"\
    , line 411, in ExLlamav2_HF_loader\nreturn Exllamav2HF.from_pretrained(model_name)\n\
    \nFile \"/workspace/text-generation-webui/modules/exllamav2_hf.py\", line 162,\
    \ in from_pretrained\nconfig.prepare()\n\nFile \"/usr/local/lib/python3.10/dist-packages/exllamav2/config.py\"\
    , line 156, in prepare\nraise ValueError(f\" ## Could not find {prefix}.* in model\"\
    )\n\nValueError: ## Could not find model.layers.0.mlp.down_proj.* in model\n```\n"
  created_at: 2023-12-18 16:10:32+00:00
  edited: false
  hidden: false
  id: 65806ef8d0dfc3caf6b3392d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
      fullname: Bob Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: downquark
      type: user
    createdAt: '2023-12-18T16:20:26.000Z'
    data:
      edited: false
      editors:
      - downquark
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9443971514701843
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
          fullname: Bob Ross
          isHf: false
          isPro: false
          name: downquark
          type: user
        html: '<p>It seems this is also happening with other EXL2 quants.<br>I''ve
          raised an issue with oobabooga <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/4981">https://github.com/oobabooga/text-generation-webui/issues/4981</a></p>

          '
        raw: 'It seems this is also happening with other EXL2 quants.

          I''ve raised an issue with oobabooga https://github.com/oobabooga/text-generation-webui/issues/4981'
        updatedAt: '2023-12-18T16:20:26.234Z'
      numEdits: 0
      reactions: []
    id: 6580714a4cdbd00e3dc23104
    type: comment
  author: downquark
  content: 'It seems this is also happening with other EXL2 quants.

    I''ve raised an issue with oobabooga https://github.com/oobabooga/text-generation-webui/issues/4981'
  created_at: 2023-12-18 16:20:26+00:00
  edited: false
  hidden: false
  id: 6580714a4cdbd00e3dc23104
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-12-18T18:00:17.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9338895082473755
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>Check to make sure that your exllamav2 module version is at least
          0.0.11 to support mixtral models.</p>

          '
        raw: Check to make sure that your exllamav2 module version is at least 0.0.11
          to support mixtral models.
        updatedAt: '2023-12-18T18:00:17.427Z'
      numEdits: 0
      reactions: []
    id: 658088b17c71acb6464fda33
    type: comment
  author: LoneStriker
  content: Check to make sure that your exllamav2 module version is at least 0.0.11
    to support mixtral models.
  created_at: 2023-12-18 18:00:17+00:00
  edited: false
  hidden: false
  id: 658088b17c71acb6464fda33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/d5d838a5b54f506f5d76212482e03767.svg
      fullname: Bob Ross
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: downquark
      type: user
    createdAt: '2023-12-18T19:22:22.000Z'
    data:
      status: closed
    id: 65809beeabafd960c831f5c6
    type: status-change
  author: downquark
  created_at: 2023-12-18 19:22:22+00:00
  id: 65809beeabafd960c831f5c6
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/dolphin-2.5-mixtral-8x7b-3.5bpw-h6-exl2-2
repo_type: model
status: closed
target_branch: null
title: '"ValueError: ## Could not find model.layers.0.mlp.down_proj.* in model" when
  loading in oobabooga webui'
