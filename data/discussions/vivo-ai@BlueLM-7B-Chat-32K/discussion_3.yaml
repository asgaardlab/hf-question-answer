!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dafen
conflicting_files: null
created_at: 2023-11-13 14:15:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d3e8c861245d0e48324a4bcb7921158.svg
      fullname: fenjun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dafen
      type: user
    createdAt: '2023-11-13T14:15:06.000Z'
    data:
      edited: false
      editors:
      - dafen
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.5795819163322449
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d3e8c861245d0e48324a4bcb7921158.svg
          fullname: fenjun
          isHf: false
          isPro: false
          name: dafen
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://github.com/vivo-ai-lab/BlueLM#vllm-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%A4%BA%E4%BE%8B\"\
          >https://github.com/vivo-ai-lab/BlueLM#vllm-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%A4%BA%E4%BE%8B</a><br>\u6211\
          \u770B\u6700\u65B0\u7684vllm\u5DF2\u7ECF\u4FEE\u590D\u4E86\uFF0Cbluelm\u80FD\
          \u9002\u914D\u4E00\u4E0B\u6700\u65B0\u7684vllm\u5417<br><a rel=\"nofollow\"\
          \ href=\"https://github.com/vllm-project/vllm/pull/1577\">https://github.com/vllm-project/vllm/pull/1577</a></p>\n"
        raw: "https://github.com/vivo-ai-lab/BlueLM#vllm-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%A4%BA%E4%BE%8B\r\
          \n\u6211\u770B\u6700\u65B0\u7684vllm\u5DF2\u7ECF\u4FEE\u590D\u4E86\uFF0C\
          bluelm\u80FD\u9002\u914D\u4E00\u4E0B\u6700\u65B0\u7684vllm\u5417\r\nhttps://github.com/vllm-project/vllm/pull/1577"
        updatedAt: '2023-11-13T14:15:06.948Z'
      numEdits: 0
      reactions: []
    id: 65522f6a2b0763c3b05409d3
    type: comment
  author: dafen
  content: "https://github.com/vivo-ai-lab/BlueLM#vllm-%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E7%A4%BA%E4%BE%8B\r\
    \n\u6211\u770B\u6700\u65B0\u7684vllm\u5DF2\u7ECF\u4FEE\u590D\u4E86\uFF0Cbluelm\u80FD\
    \u9002\u914D\u4E00\u4E0B\u6700\u65B0\u7684vllm\u5417\r\nhttps://github.com/vllm-project/vllm/pull/1577"
  created_at: 2023-11-13 14:15:06+00:00
  edited: false
  hidden: false
  id: 65522f6a2b0763c3b05409d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5d3e8c861245d0e48324a4bcb7921158.svg
      fullname: fenjun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dafen
      type: user
    createdAt: '2023-11-14T08:49:46.000Z'
    data:
      edited: true
      editors:
      - dafen
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.4421073794364929
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5d3e8c861245d0e48324a4bcb7921158.svg
          fullname: fenjun
          isHf: false
          isPro: false
          name: dafen
          type: user
        html: "<h1 id=\"hf\u7248\uFF1A\">HF\u7248\uFF1A</h1>\n<p>import torch<br>from\
          \ transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig<br>model_dir\
          \ = 'model/vivo-ai/BlueLM-7B-Chat-32K'<br>tokenizer = AutoTokenizer.from_pretrained(model_dir,\
          \ trust_remote_code=True)<br>model = AutoModelForCausalLM.from_pretrained(model_dir,\
          \ torch_dtype=torch.bfloat16,trust_remote_code=True,device_map=\"cuda:0\"\
          )#, device_map=\"auto\"<br>model.generation_config = GenerationConfig.from_pretrained(model_dir\
          \ )<br>model.generation_config.temperature = 0.01<br>model.generation_config.top_p\
          \ =0.01<br>model.generation_config.repetition_penalty = 1.0</p>\n<p>inputs\
          \ = tokenizer(\"\"\"[|Human|]:\u5C0F\u660E\u7684\u7238\u7238\u6709\u4E09\
          \u4E2A\u513F\u5B50\uFF0C\u5C0F\u660E1992\u5E74\u51FA\u751F\uFF0C\u5927\u513F\
          \u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\
          \u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\u95EE\u4E09\u513F\u5B50\u4ECA\u5E74\
          \u591A\u5927\u4E86[|AI|]:\"\"\", return_tensors=\"pt\")<br>inputs = inputs.to(\"\
          cuda:0\")<br>pred = model.generate(**inputs, max_new_tokens=512)<br>print(tokenizer.decode(pred.cpu()[0],\
          \ skip_special_tokens=True).split('[|AI|]:')[-1])</p>\n<h2 id=\"result\uFF1A\
          \">result\uFF1A</h2>\n<p>\u5C0F\u660E\u7684\u7238\u7238\u6709\u4E09\u4E2A\
          \u513F\u5B50\uFF0C\u5C0F\u660E1992\u5E74\u51FA\u751F\uFF0C\u5927\u513F\u5B50\
          \u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\
          \u5E74\u662F2023\u5E74\uFF0C\u8BF7\u95EE\u4E09\u513F\u5B50\u4ECA\u5E74\u591A\
          \u5927\u4E86 \u6839\u636E\u9898\u76EE\u6240\u7ED9\u7684\u4FE1\u606F\uFF0C\
          \u53EF\u4EE5\u8BA1\u7B97\u51FA\u5C0F\u660E\u7684\u4E09\u513F\u5B50\u4ECA\
          \u5E74\u7684\u5E74\u9F84\u3002</p>\n<p>\u5DF2\u77E5\u5C0F\u660E\u51FA\u751F\
          \u4E8E1992\u5E74\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\
          \u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u56E0\u6B64\
          \u5C0F\u660E\u7684\u4E09\u513F\u5B50\u51FA\u751F\u4E8E\uFF1A$2023 - 1992\
          \ = 31$\u5E74\uFF1B</p>\n<p>\u6240\u4EE5\uFF0C\u5C0F\u660E\u7684\u4E09\u513F\
          \u5B50\u4ECA\u5E7431\u5C81\u4E86\u3002</p>\n<h1 id=\"vllm\u7248\uFF1A\"\
          >VLLM\u7248\uFF1A</h1>\n<p>prompts = [<br>\"\"\"[|Human|]:\u5C0F\u660E\u7684\
          \u7238\u7238\u6709\u4E09\u4E2A\u513F\u5B50\uFF0C\u5C0F\u660E1992\u5E74\u51FA\
          \u751F\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\u5B50\
          \u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\u95EE\u4E09\
          \u513F\u5B50\u4ECA\u5E74\u591A\u5927\u4E86[|AI|]:\"\"\"<br>]<br>sampling_params\
          \ = SamplingParams(temperature=0.01, top_p=0.01,repetition_penalty=1.0,max_tokens=512)<br>outputs\
          \ = llm.generate(prompts,sampling_params=sampling_params)<br>for output\
          \ in outputs:<br>    prompt = output.prompt<br>    generated_text = output.outputs[0].text<br>\
          \    print(f\"Generated text: {generated_text!r}\")</p>\n<h2 id=\"result\uFF1A\
          -1\">result\uFF1A</h2>\n<p>' \u6839\u636E\u9898\u76EE\u4E2D\u7684\u4FE1\u606F\
          \"\u4E09\u4E2A\u4EE3\u8868\u548C\u548C*\"\u53EF\u77E5\uFF0C\u9898\u76EE\u4E2D\
          \u7684\u4FE1\u606F\u4E0D'</p>\n<p>\u4E8B\u5B9E\u4E0Avllm\u7248\u6BCF\u6B21\
          \u6267\u884C\u7ED3\u679C\u90FD\u4E0D\u4E00\u6837</p>\n"
        raw: "# HF\u7248\uFF1A\nimport torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer, GenerationConfig\nmodel_dir = 'model/vivo-ai/BlueLM-7B-Chat-32K'\n\
          tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n\
          model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16,trust_remote_code=True,device_map=\"\
          cuda:0\")#, device_map=\"auto\"\nmodel.generation_config = GenerationConfig.from_pretrained(model_dir\
          \ )\nmodel.generation_config.temperature = 0.01 \nmodel.generation_config.top_p\
          \ =0.01\nmodel.generation_config.repetition_penalty = 1.0\n\ninputs = tokenizer(\"\
          \"\"[|Human|]:\u5C0F\u660E\u7684\u7238\u7238\u6709\u4E09\u4E2A\u513F\u5B50\
          \uFF0C\u5C0F\u660E1992\u5E74\u51FA\u751F\uFF0C\u5927\u513F\u5B50\u53EB\u5927\
          \u6BDB\uFF0C\u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F\
          2023\u5E74\uFF0C\u8BF7\u95EE\u4E09\u513F\u5B50\u4ECA\u5E74\u591A\u5927\u4E86\
          [|AI|]:\"\"\", return_tensors=\"pt\")\ninputs = inputs.to(\"cuda:0\")\n\
          pred = model.generate(**inputs, max_new_tokens=512)\nprint(tokenizer.decode(pred.cpu()[0],\
          \ skip_special_tokens=True).split('[|AI|]:')[-1])\n## result\uFF1A\n\u5C0F\
          \u660E\u7684\u7238\u7238\u6709\u4E09\u4E2A\u513F\u5B50\uFF0C\u5C0F\u660E\
          1992\u5E74\u51FA\u751F\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\
          \u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\
          \u95EE\u4E09\u513F\u5B50\u4ECA\u5E74\u591A\u5927\u4E86 \u6839\u636E\u9898\
          \u76EE\u6240\u7ED9\u7684\u4FE1\u606F\uFF0C\u53EF\u4EE5\u8BA1\u7B97\u51FA\
          \u5C0F\u660E\u7684\u4E09\u513F\u5B50\u4ECA\u5E74\u7684\u5E74\u9F84\u3002\
          \n\n\u5DF2\u77E5\u5C0F\u660E\u51FA\u751F\u4E8E1992\u5E74\uFF0C\u5927\u513F\
          \u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\
          \u4ECA\u5E74\u662F2023\u5E74\uFF0C\u56E0\u6B64\u5C0F\u660E\u7684\u4E09\u513F\
          \u5B50\u51FA\u751F\u4E8E\uFF1A$2023 - 1992 = 31$\u5E74\uFF1B\n\n\u6240\u4EE5\
          \uFF0C\u5C0F\u660E\u7684\u4E09\u513F\u5B50\u4ECA\u5E7431\u5C81\u4E86\u3002\
          \n\n# VLLM\u7248\uFF1A\nprompts = [\n\"\"\"[|Human|]:\u5C0F\u660E\u7684\u7238\
          \u7238\u6709\u4E09\u4E2A\u513F\u5B50\uFF0C\u5C0F\u660E1992\u5E74\u51FA\u751F\
          \uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\u5B50\u53EB\
          \u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\u95EE\u4E09\u513F\
          \u5B50\u4ECA\u5E74\u591A\u5927\u4E86[|AI|]:\"\"\"\n]\nsampling_params =\
          \ SamplingParams(temperature=0.01, top_p=0.01,repetition_penalty=1.0,max_tokens=512)\n\
          outputs = llm.generate(prompts,sampling_params=sampling_params)\nfor output\
          \ in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n\
          \    print(f\"Generated text: {generated_text!r}\")\n## result\uFF1A\n'\
          \ \u6839\u636E\u9898\u76EE\u4E2D\u7684\u4FE1\u606F\"\u4E09\u4E2A\u4EE3\u8868\
          \u548C\u548C*\"\u53EF\u77E5\uFF0C\u9898\u76EE\u4E2D\u7684\u4FE1\u606F\u4E0D\
          '\n\n\u4E8B\u5B9E\u4E0Avllm\u7248\u6BCF\u6B21\u6267\u884C\u7ED3\u679C\u90FD\
          \u4E0D\u4E00\u6837"
        updatedAt: '2023-11-14T09:25:37.767Z'
      numEdits: 1
      reactions: []
    id: 655334aad3b9b5be9399d029
    type: comment
  author: dafen
  content: "# HF\u7248\uFF1A\nimport torch\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, GenerationConfig\nmodel_dir = 'model/vivo-ai/BlueLM-7B-Chat-32K'\n\
    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n\
    model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.bfloat16,trust_remote_code=True,device_map=\"\
    cuda:0\")#, device_map=\"auto\"\nmodel.generation_config = GenerationConfig.from_pretrained(model_dir\
    \ )\nmodel.generation_config.temperature = 0.01 \nmodel.generation_config.top_p\
    \ =0.01\nmodel.generation_config.repetition_penalty = 1.0\n\ninputs = tokenizer(\"\
    \"\"[|Human|]:\u5C0F\u660E\u7684\u7238\u7238\u6709\u4E09\u4E2A\u513F\u5B50\uFF0C\
    \u5C0F\u660E1992\u5E74\u51FA\u751F\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\
    \u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\
    \u95EE\u4E09\u513F\u5B50\u4ECA\u5E74\u591A\u5927\u4E86[|AI|]:\"\"\", return_tensors=\"\
    pt\")\ninputs = inputs.to(\"cuda:0\")\npred = model.generate(**inputs, max_new_tokens=512)\n\
    print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True).split('[|AI|]:')[-1])\n\
    ## result\uFF1A\n\u5C0F\u660E\u7684\u7238\u7238\u6709\u4E09\u4E2A\u513F\u5B50\uFF0C\
    \u5C0F\u660E1992\u5E74\u51FA\u751F\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\
    \u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\
    \u95EE\u4E09\u513F\u5B50\u4ECA\u5E74\u591A\u5927\u4E86 \u6839\u636E\u9898\u76EE\
    \u6240\u7ED9\u7684\u4FE1\u606F\uFF0C\u53EF\u4EE5\u8BA1\u7B97\u51FA\u5C0F\u660E\
    \u7684\u4E09\u513F\u5B50\u4ECA\u5E74\u7684\u5E74\u9F84\u3002\n\n\u5DF2\u77E5\u5C0F\
    \u660E\u51FA\u751F\u4E8E1992\u5E74\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\
    \u4E8C\u513F\u5B50\u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u56E0\
    \u6B64\u5C0F\u660E\u7684\u4E09\u513F\u5B50\u51FA\u751F\u4E8E\uFF1A$2023 - 1992\
    \ = 31$\u5E74\uFF1B\n\n\u6240\u4EE5\uFF0C\u5C0F\u660E\u7684\u4E09\u513F\u5B50\u4ECA\
    \u5E7431\u5C81\u4E86\u3002\n\n# VLLM\u7248\uFF1A\nprompts = [\n\"\"\"[|Human|]:\u5C0F\
    \u660E\u7684\u7238\u7238\u6709\u4E09\u4E2A\u513F\u5B50\uFF0C\u5C0F\u660E1992\u5E74\
    \u51FA\u751F\uFF0C\u5927\u513F\u5B50\u53EB\u5927\u6BDB\uFF0C\u4E8C\u513F\u5B50\
    \u53EB\u4E8C\u6BDB\uFF0C\u4ECA\u5E74\u662F2023\u5E74\uFF0C\u8BF7\u95EE\u4E09\u513F\
    \u5B50\u4ECA\u5E74\u591A\u5927\u4E86[|AI|]:\"\"\"\n]\nsampling_params = SamplingParams(temperature=0.01,\
    \ top_p=0.01,repetition_penalty=1.0,max_tokens=512)\noutputs = llm.generate(prompts,sampling_params=sampling_params)\n\
    for output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n\
    \    print(f\"Generated text: {generated_text!r}\")\n## result\uFF1A\n' \u6839\
    \u636E\u9898\u76EE\u4E2D\u7684\u4FE1\u606F\"\u4E09\u4E2A\u4EE3\u8868\u548C\u548C\
    *\"\u53EF\u77E5\uFF0C\u9898\u76EE\u4E2D\u7684\u4FE1\u606F\u4E0D'\n\n\u4E8B\u5B9E\
    \u4E0Avllm\u7248\u6BCF\u6B21\u6267\u884C\u7ED3\u679C\u90FD\u4E0D\u4E00\u6837"
  created_at: 2023-11-14 08:49:46+00:00
  edited: true
  hidden: false
  id: 655334aad3b9b5be9399d029
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b4312d553a59ad5246b2b88c43847d9.svg
      fullname: LiHongbin
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: JoeyHeisenberg
      type: user
    createdAt: '2023-11-22T03:58:45.000Z'
    data:
      edited: false
      editors:
      - JoeyHeisenberg
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.6149471402168274
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b4312d553a59ad5246b2b88c43847d9.svg
          fullname: LiHongbin
          isHf: false
          isPro: false
          name: JoeyHeisenberg
          type: user
        html: "<p>github\u4ED3\u5E93 vllm\u7248\u672C\u5DF2\u4FEE\u590D</p>\n"
        raw: "github\u4ED3\u5E93 vllm\u7248\u672C\u5DF2\u4FEE\u590D"
        updatedAt: '2023-11-22T03:58:45.625Z'
      numEdits: 0
      reactions: []
    id: 655d7c75d36a195f6624a8a7
    type: comment
  author: JoeyHeisenberg
  content: "github\u4ED3\u5E93 vllm\u7248\u672C\u5DF2\u4FEE\u590D"
  created_at: 2023-11-22 03:58:45+00:00
  edited: false
  hidden: false
  id: 655d7c75d36a195f6624a8a7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/870724e20dc1c221fbe9c6656720ab9b.svg
      fullname: Pororo
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: jeffreygao
      type: user
    createdAt: '2023-11-23T11:27:10.000Z'
    data:
      status: closed
    id: 655f370ec073ea9cf864904e
    type: status-change
  author: jeffreygao
  created_at: 2023-11-23 11:27:10+00:00
  id: 655f370ec073ea9cf864904e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: vivo-ai/BlueLM-7B-Chat-32K
repo_type: model
status: closed
target_branch: null
title: "\u6211\u5C1D\u8BD5\u91C7\u7528\u4F60\u4EEC\u5B98\u65B9\u7684vllm\u63A8\u7406\
  \uFF0C\u53D1\u73B0\u548Chf\u63A8\u7406\u7684\u7ED3\u679C\u76F8\u5DEE\u975E\u5E38\
  \u5927"
