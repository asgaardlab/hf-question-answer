!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alfredplpl
conflicting_files: null
created_at: 2023-11-03 18:26:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-11-03T19:26:12.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.9325330853462219
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>\u3053\u3093\u306B\u3061\u306F</p>\n<p>\u8CEA\u554F\u304C\u3042\u308A\
          \u307E\u3059\u3002<br>\u3082\u3068\u306E\u30E2\u30C7\u30EB\u306Fbfloat16\u306A\
          \u3093\u3067\u3059\u304C\u3001<br>float16\u3067\u6B63\u5E38\u306B\u52D5\u4F5C\
          \u3067\u304D\u308B\u306E\u3067\u3057\u3087\u3046\u304B\uFF1F</p>\n<p>\u3044\
          \u308D\u3044\u308D\u8A66\u884C\u932F\u8AA4\u3057\u3066\u30A8\u30E9\u30FC\
          \u3092\u306A\u304F\u3057\u307E\u3057\u305F\u3002<br>\u307E\u305A\u3001\u3053\
          \u306E\u30E2\u30C7\u30EB\u306Ftriton\u304C\u5FC5\u8981\u3089\u3057\u3044\
          \u306E\u3067\u304C\u5FC5\u8981\u306A\u306E\u3067\u3001</p>\n<pre><code class=\"\
          language-bash\">pip install auto-gptq[triton]==0.4.2\n</code></pre>\n<p>\u3092\
          \u3057\u307E\u3057\u305F\u3002\u6700\u65B0\u306E0.5.0\u3067\u306F\u30D0\u30B0\
          \u308B\u3088\u3046\u3067\u3059\u3002<br>\u6B21\u306B\u30D7\u30ED\u30F3\u30D7\
          \u30C8\u304C\u5BFE\u8A71\u7528\u306B\u306A\u3063\u3066\u306A\u304B\u3063\
          \u305F\u306E\u3067\u4FEE\u6B63\u3057\u307E\u3057\u305F\u3002</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-comment\"># Check transformers\
          \ version</span>\n<span class=\"hljs-keyword\">assert</span> transformers.__version__\
          \ &gt;= <span class=\"hljs-string\">\"4.34.1\"</span>\n\n<span class=\"\
          hljs-comment\"># Tokenizer</span>\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
          \n<span class=\"hljs-comment\"># Model</span>\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     inject_fused_attention=<span class=\"hljs-literal\">True</span>,\n\
          \        inject_fused_mlp=<span class=\"hljs-literal\">True</span>,\n  \
          \      device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n        use_triton=<span\
          \ class=\"hljs-literal\">True</span>,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>,\n        use_cuda_fp16=<span class=\"hljs-literal\"\
          >True</span>,\n        torch_dtype=torch.float16)\n\n<span class=\"hljs-comment\"\
          ># Your test prompt</span>\nprompt = <span class=\"hljs-string\">\"\"\"\
          </span>\n<span class=\"hljs-string\">USER: \u4ECA\u65E5\u306E\u5915\u98DF\
          \u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\
          \u3044\u3002</span>\n<span class=\"hljs-string\">ASSISTANT: </span>\n<span\
          \ class=\"hljs-string\">\"\"\"</span>\ninput_ids = tokenizer.encode(prompt,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>)\noutput_ids=model.generate(\n\
          \        input_ids=input_ids.to(model.device),\n        max_new_tokens=<span\
          \ class=\"hljs-number\">128</span>\n)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output_ids[<span\
          \ class=\"hljs-number\">0</span>]))\n<span class=\"hljs-string\">\"\"\"\
          </span>\n</code></pre>\n<p>\u3068\u3057\u305F\u3068\u3053\u308D\u3001\u30A8\
          \u30E9\u30FC\u306A\u304F\u6B21\u306E\u3088\u3046\u306B\u51FA\u529B\u3057\
          \u307E\u3057\u305F\u3002</p>\n<pre><code class=\"language-bash\">USER: \u4ECA\
          \u65E5\u306E\u5915\u98DF\u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\
          \u3066\u304F\u3060\u3055\u3044\u3002\nASSISTANT: \n&lt;|endoftext|&gt;\n\
          </code></pre>\n<p>\u3057\u304B\u3057\u3001\u660E\u3089\u304B\u306B\u304A\
          \u304B\u3057\u3044\u306E\u3067\u3001model.generate(...,do_sample=True)\u3092\
          \u5165\u308C\u308B\u3068\u3001</p>\n<pre><code class=\"language-bash\">RuntimeError:\
          \ probability tensor contains either `inf`, `nan` or element &lt; 0\n</code></pre>\n\
          <p>\u3068\u306A\u308A\u3001\u8A08\u7B97\u4E0D\u80FD\u306B\u306A\u308A\u307E\
          \u3057\u305F\u3002<br>\u3053\u306E\u539F\u56E0\u306F\u304A\u305D\u3089\u304F\
          bfloat16\u306E\u5024\u57DF\u304Cfloat16\u3088\u308A\u3082\u5E83\u3044\u305F\
          \u3081\u306B\u8D77\u3053\u3063\u3066\u3044\u307E\u3059\u3002</p>\n<p>\u305D\
          \u3053\u3067\u8CEA\u554F\u306A\u306E\u3067\u3059\u304C\u3001\u3082\u3068\
          \u306E\u30E2\u30C7\u30EB\u306Fbfloat16\u306A\u3093\u3067\u3059\u304C\u3001\
          float16\u3067\u6B63\u5E38\u306B\u52D5\u4F5C\u3067\u304D\u308B\u306E\u3067\
          \u3057\u3087\u3046\u304B\uFF1F</p>\n<p>\u3088\u308D\u3057\u304F\u304A\u9858\
          \u3044\u3057\u307E\u3059\u3002</p>\n"
        raw: "\u3053\u3093\u306B\u3061\u306F\r\n\r\n\u8CEA\u554F\u304C\u3042\u308A\
          \u307E\u3059\u3002\r\n\u3082\u3068\u306E\u30E2\u30C7\u30EB\u306Fbfloat16\u306A\
          \u3093\u3067\u3059\u304C\u3001\r\nfloat16\u3067\u6B63\u5E38\u306B\u52D5\u4F5C\
          \u3067\u304D\u308B\u306E\u3067\u3057\u3087\u3046\u304B\uFF1F\r\n\r\n\u3044\
          \u308D\u3044\u308D\u8A66\u884C\u932F\u8AA4\u3057\u3066\u30A8\u30E9\u30FC\
          \u3092\u306A\u304F\u3057\u307E\u3057\u305F\u3002\r\n\u307E\u305A\u3001\u3053\
          \u306E\u30E2\u30C7\u30EB\u306Ftriton\u304C\u5FC5\u8981\u3089\u3057\u3044\
          \u306E\u3067\u304C\u5FC5\u8981\u306A\u306E\u3067\u3001\r\n\r\n```bash\r\n\
          pip install auto-gptq[triton]==0.4.2\r\n```\r\n\u3092\u3057\u307E\u3057\u305F\
          \u3002\u6700\u65B0\u306E0.5.0\u3067\u306F\u30D0\u30B0\u308B\u3088\u3046\u3067\
          \u3059\u3002\r\n\u6B21\u306B\u30D7\u30ED\u30F3\u30D7\u30C8\u304C\u5BFE\u8A71\
          \u7528\u306B\u306A\u3063\u3066\u306A\u304B\u3063\u305F\u306E\u3067\u4FEE\
          \u6B63\u3057\u307E\u3057\u305F\u3002\r\n\r\n```python\r\n# Check transformers\
          \ version\r\nassert transformers.__version__ >= \"4.34.1\"\r\n\r\n# Tokenizer\r\
          \ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\n\r\n\
          # Model\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
          \n        use_safetensors=True,\r\n        inject_fused_attention=True,\r\
          \n        inject_fused_mlp=True,\r\n        device=\"cuda:0\",\r\n     \
          \   use_triton=True,\r\n        quantize_config=None,\r\n        use_cuda_fp16=True,\r\
          \n        torch_dtype=torch.float16)\r\n\r\n# Your test prompt\r\nprompt\
          \ = \"\"\"\r\nUSER: \u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\u30B7\u30D4\
          \u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\u3044\u3002\r\nASSISTANT:\
          \ \r\n\"\"\"\r\ninput_ids = tokenizer.encode(prompt, return_tensors=\"pt\"\
          )\r\noutput_ids=model.generate(\r\n        input_ids=input_ids.to(model.device),\r\
          \n        max_new_tokens=128\r\n)\r\nprint(tokenizer.decode(output_ids[0]))\r\
          \n\"\"\"\r\n```\r\n\r\n\u3068\u3057\u305F\u3068\u3053\u308D\u3001\u30A8\u30E9\
          \u30FC\u306A\u304F\u6B21\u306E\u3088\u3046\u306B\u51FA\u529B\u3057\u307E\
          \u3057\u305F\u3002\r\n\r\n```bash\r\nUSER: \u4ECA\u65E5\u306E\u5915\u98DF\
          \u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\
          \u3044\u3002\r\nASSISTANT: \r\n<|endoftext|>\r\n```\r\n\r\n\u3057\u304B\u3057\
          \u3001\u660E\u3089\u304B\u306B\u304A\u304B\u3057\u3044\u306E\u3067\u3001\
          model.generate(...,do_sample=True)\u3092\u5165\u308C\u308B\u3068\u3001\r\
          \n\r\n```bash\r\nRuntimeError: probability tensor contains either `inf`,\
          \ `nan` or element < 0\r\n```\r\n\r\n\u3068\u306A\u308A\u3001\u8A08\u7B97\
          \u4E0D\u80FD\u306B\u306A\u308A\u307E\u3057\u305F\u3002\r\n\u3053\u306E\u539F\
          \u56E0\u306F\u304A\u305D\u3089\u304Fbfloat16\u306E\u5024\u57DF\u304Cfloat16\u3088\
          \u308A\u3082\u5E83\u3044\u305F\u3081\u306B\u8D77\u3053\u3063\u3066\u3044\
          \u307E\u3059\u3002\r\n\r\n\u305D\u3053\u3067\u8CEA\u554F\u306A\u306E\u3067\
          \u3059\u304C\u3001\u3082\u3068\u306E\u30E2\u30C7\u30EB\u306Fbfloat16\u306A\
          \u3093\u3067\u3059\u304C\u3001float16\u3067\u6B63\u5E38\u306B\u52D5\u4F5C\
          \u3067\u304D\u308B\u306E\u3067\u3057\u3087\u3046\u304B\uFF1F\r\n\r\n\u3088\
          \u308D\u3057\u304F\u304A\u9858\u3044\u3057\u307E\u3059\u3002\r\n"
        updatedAt: '2023-11-03T19:26:12.926Z'
      numEdits: 0
      reactions: []
    id: 65454954bfae9d8476b4f100
    type: comment
  author: alfredplpl
  content: "\u3053\u3093\u306B\u3061\u306F\r\n\r\n\u8CEA\u554F\u304C\u3042\u308A\u307E\
    \u3059\u3002\r\n\u3082\u3068\u306E\u30E2\u30C7\u30EB\u306Fbfloat16\u306A\u3093\
    \u3067\u3059\u304C\u3001\r\nfloat16\u3067\u6B63\u5E38\u306B\u52D5\u4F5C\u3067\u304D\
    \u308B\u306E\u3067\u3057\u3087\u3046\u304B\uFF1F\r\n\r\n\u3044\u308D\u3044\u308D\
    \u8A66\u884C\u932F\u8AA4\u3057\u3066\u30A8\u30E9\u30FC\u3092\u306A\u304F\u3057\
    \u307E\u3057\u305F\u3002\r\n\u307E\u305A\u3001\u3053\u306E\u30E2\u30C7\u30EB\u306F\
    triton\u304C\u5FC5\u8981\u3089\u3057\u3044\u306E\u3067\u304C\u5FC5\u8981\u306A\
    \u306E\u3067\u3001\r\n\r\n```bash\r\npip install auto-gptq[triton]==0.4.2\r\n\
    ```\r\n\u3092\u3057\u307E\u3057\u305F\u3002\u6700\u65B0\u306E0.5.0\u3067\u306F\
    \u30D0\u30B0\u308B\u3088\u3046\u3067\u3059\u3002\r\n\u6B21\u306B\u30D7\u30ED\u30F3\
    \u30D7\u30C8\u304C\u5BFE\u8A71\u7528\u306B\u306A\u3063\u3066\u306A\u304B\u3063\
    \u305F\u306E\u3067\u4FEE\u6B63\u3057\u307E\u3057\u305F\u3002\r\n\r\n```python\r\
    \n# Check transformers version\r\nassert transformers.__version__ >= \"4.34.1\"\
    \r\n\r\n# Tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\r\
    \n\r\n# Model\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\r\
    \n        use_safetensors=True,\r\n        inject_fused_attention=True,\r\n  \
    \      inject_fused_mlp=True,\r\n        device=\"cuda:0\",\r\n        use_triton=True,\r\
    \n        quantize_config=None,\r\n        use_cuda_fp16=True,\r\n        torch_dtype=torch.float16)\r\
    \n\r\n# Your test prompt\r\nprompt = \"\"\"\r\nUSER: \u4ECA\u65E5\u306E\u5915\u98DF\
    \u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\u3044\
    \u3002\r\nASSISTANT: \r\n\"\"\"\r\ninput_ids = tokenizer.encode(prompt, return_tensors=\"\
    pt\")\r\noutput_ids=model.generate(\r\n        input_ids=input_ids.to(model.device),\r\
    \n        max_new_tokens=128\r\n)\r\nprint(tokenizer.decode(output_ids[0]))\r\n\
    \"\"\"\r\n```\r\n\r\n\u3068\u3057\u305F\u3068\u3053\u308D\u3001\u30A8\u30E9\u30FC\
    \u306A\u304F\u6B21\u306E\u3088\u3046\u306B\u51FA\u529B\u3057\u307E\u3057\u305F\
    \u3002\r\n\r\n```bash\r\nUSER: \u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\u30B7\
    \u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\u3044\u3002\r\nASSISTANT:\
    \ \r\n<|endoftext|>\r\n```\r\n\r\n\u3057\u304B\u3057\u3001\u660E\u3089\u304B\u306B\
    \u304A\u304B\u3057\u3044\u306E\u3067\u3001model.generate(...,do_sample=True)\u3092\
    \u5165\u308C\u308B\u3068\u3001\r\n\r\n```bash\r\nRuntimeError: probability tensor\
    \ contains either `inf`, `nan` or element < 0\r\n```\r\n\r\n\u3068\u306A\u308A\
    \u3001\u8A08\u7B97\u4E0D\u80FD\u306B\u306A\u308A\u307E\u3057\u305F\u3002\r\n\u3053\
    \u306E\u539F\u56E0\u306F\u304A\u305D\u3089\u304Fbfloat16\u306E\u5024\u57DF\u304C\
    float16\u3088\u308A\u3082\u5E83\u3044\u305F\u3081\u306B\u8D77\u3053\u3063\u3066\
    \u3044\u307E\u3059\u3002\r\n\r\n\u305D\u3053\u3067\u8CEA\u554F\u306A\u306E\u3067\
    \u3059\u304C\u3001\u3082\u3068\u306E\u30E2\u30C7\u30EB\u306Fbfloat16\u306A\u3093\
    \u3067\u3059\u304C\u3001float16\u3067\u6B63\u5E38\u306B\u52D5\u4F5C\u3067\u304D\
    \u308B\u306E\u3067\u3057\u3087\u3046\u304B\uFF1F\r\n\r\n\u3088\u308D\u3057\u304F\
    \u304A\u9858\u3044\u3057\u307E\u3059\u3002\r\n"
  created_at: 2023-11-03 18:26:12+00:00
  edited: false
  hidden: false
  id: 65454954bfae9d8476b4f100
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-11-03T19:28:16.000Z'
    data:
      from: "bfloat16\u3067\u306A\u304Ffloat16\u306B\u3088\u308B\u91CF\u5B50\u5316\
        \u3078\u306E\u8CEA\u554F"
      to: "bfloat16\u3067\u306A\u304Ffloat16\u306B\u3088\u308B\u91CF\u5B50\u5316"
    id: 654549d0e3486f8a5e603463
    type: title-change
  author: alfredplpl
  created_at: 2023-11-03 18:28:16+00:00
  id: 654549d0e3486f8a5e603463
  new_title: "bfloat16\u3067\u306A\u304Ffloat16\u306B\u3088\u308B\u91CF\u5B50\u5316"
  old_title: "bfloat16\u3067\u306A\u304Ffloat16\u306B\u3088\u308B\u91CF\u5B50\u5316\
    \u3078\u306E\u8CEA\u554F"
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630faef1982455e61ccc10fd/V2bJ83SV-AfvX4fkEtFNi.jpeg?w=200&h=200&f=face
      fullname: momonga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: mmnga
      type: user
    createdAt: '2023-11-03T21:13:57.000Z'
    data:
      edited: false
      editors:
      - mmnga
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 1.0000284910202026
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/630faef1982455e61ccc10fd/V2bJ83SV-AfvX4fkEtFNi.jpeg?w=200&h=200&f=face
          fullname: momonga
          isHf: false
          isPro: false
          name: mmnga
          type: user
        html: "<p>\u3054\u8CEA\u554F\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\
          \u307E\u3059\u3002<br>\u3054\u6307\u6458\u306E\u3068\u304A\u308A\u3001\u5143\
          \u30E2\u30C7\u30EB\u306Fbfloat16\u3067float16\u306B\u306A\u3063\u3066\u3044\
          \u308B\u3053\u3068\u306B\u3088\u308A\u6B63\u5E38\u306B\u52D5\u4F5C\u3057\
          \u3066\u304A\u308A\u307E\u305B\u3093\u3067\u3057\u305F\u306E\u3067\u3001\
          <br>bfloat16\u3067\u4F5C\u308A\u76F4\u3057\u3066\u518D\u5EA6\u30A2\u30C3\
          \u30D7\u30ED\u30FC\u30C9\u3044\u305F\u3057\u307E\u3057\u305F\u3002<br>\u307E\
          \u305F\u3001README.md\u306E\u8A18\u8F09\u30B3\u30FC\u30C9\u3082\u4FEE\u6B63\
          \u3057\u307E\u3057\u305F\u3002</p>\n<p>use_triton=True\u306Fcolab\u306E\
          T4\u3067\u518D\u8D77\u52D5\u304C\u304B\u304B\u3063\u3066\u3057\u307E\u3046\
          \u8B0E\u306E\u6319\u52D5\u3068\u306A\u3063\u3066\u3057\u307E\u3046\u305F\
          \u3081\u3001<br>warning\u306F\u51FA\u307E\u3059\u304C\u3072\u3068\u307E\u305A\
          \u5916\u3057\u3066\u3042\u308A\u307E\u3059\u3002</p>\n<p>\u304A\u624B\u6570\
          \u304A\u304B\u3051\u3057\u3066\u7533\u3057\u8A33\u306A\u3044\u3067\u3059\
          \u3002<br>\u52A9\u304B\u308A\u307E\u3057\u305F\u3001\u3042\u308A\u304C\u3068\
          \u3046\u3054\u3056\u3044\u307E\u3059\uFF01</p>\n"
        raw: "\u3054\u8CEA\u554F\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\
          \u3059\u3002\n\u3054\u6307\u6458\u306E\u3068\u304A\u308A\u3001\u5143\u30E2\
          \u30C7\u30EB\u306Fbfloat16\u3067float16\u306B\u306A\u3063\u3066\u3044\u308B\
          \u3053\u3068\u306B\u3088\u308A\u6B63\u5E38\u306B\u52D5\u4F5C\u3057\u3066\
          \u304A\u308A\u307E\u305B\u3093\u3067\u3057\u305F\u306E\u3067\u3001\nbfloat16\u3067\
          \u4F5C\u308A\u76F4\u3057\u3066\u518D\u5EA6\u30A2\u30C3\u30D7\u30ED\u30FC\
          \u30C9\u3044\u305F\u3057\u307E\u3057\u305F\u3002\n\u307E\u305F\u3001README.md\u306E\
          \u8A18\u8F09\u30B3\u30FC\u30C9\u3082\u4FEE\u6B63\u3057\u307E\u3057\u305F\
          \u3002\n\nuse_triton=True\u306Fcolab\u306ET4\u3067\u518D\u8D77\u52D5\u304C\
          \u304B\u304B\u3063\u3066\u3057\u307E\u3046\u8B0E\u306E\u6319\u52D5\u3068\
          \u306A\u3063\u3066\u3057\u307E\u3046\u305F\u3081\u3001\nwarning\u306F\u51FA\
          \u307E\u3059\u304C\u3072\u3068\u307E\u305A\u5916\u3057\u3066\u3042\u308A\
          \u307E\u3059\u3002\n\n\u304A\u624B\u6570\u304A\u304B\u3051\u3057\u3066\u7533\
          \u3057\u8A33\u306A\u3044\u3067\u3059\u3002\n\u52A9\u304B\u308A\u307E\u3057\
          \u305F\u3001\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\u3059\
          \uFF01"
        updatedAt: '2023-11-03T21:13:57.007Z'
      numEdits: 0
      reactions: []
    id: 6545629562fae6b2a719638f
    type: comment
  author: mmnga
  content: "\u3054\u8CEA\u554F\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\
    \u3059\u3002\n\u3054\u6307\u6458\u306E\u3068\u304A\u308A\u3001\u5143\u30E2\u30C7\
    \u30EB\u306Fbfloat16\u3067float16\u306B\u306A\u3063\u3066\u3044\u308B\u3053\u3068\
    \u306B\u3088\u308A\u6B63\u5E38\u306B\u52D5\u4F5C\u3057\u3066\u304A\u308A\u307E\
    \u305B\u3093\u3067\u3057\u305F\u306E\u3067\u3001\nbfloat16\u3067\u4F5C\u308A\u76F4\
    \u3057\u3066\u518D\u5EA6\u30A2\u30C3\u30D7\u30ED\u30FC\u30C9\u3044\u305F\u3057\
    \u307E\u3057\u305F\u3002\n\u307E\u305F\u3001README.md\u306E\u8A18\u8F09\u30B3\u30FC\
    \u30C9\u3082\u4FEE\u6B63\u3057\u307E\u3057\u305F\u3002\n\nuse_triton=True\u306F\
    colab\u306ET4\u3067\u518D\u8D77\u52D5\u304C\u304B\u304B\u3063\u3066\u3057\u307E\
    \u3046\u8B0E\u306E\u6319\u52D5\u3068\u306A\u3063\u3066\u3057\u307E\u3046\u305F\
    \u3081\u3001\nwarning\u306F\u51FA\u307E\u3059\u304C\u3072\u3068\u307E\u305A\u5916\
    \u3057\u3066\u3042\u308A\u307E\u3059\u3002\n\n\u304A\u624B\u6570\u304A\u304B\u3051\
    \u3057\u3066\u7533\u3057\u8A33\u306A\u3044\u3067\u3059\u3002\n\u52A9\u304B\u308A\
    \u307E\u3057\u305F\u3001\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\
    \u3059\uFF01"
  created_at: 2023-11-03 20:13:57+00:00
  edited: false
  hidden: false
  id: 6545629562fae6b2a719638f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-11-04T00:17:23.000Z'
    data:
      edited: false
      editors:
      - alfredplpl
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.9475858211517334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
          fullname: Yasunori Ozaki
          isHf: false
          isPro: false
          name: alfredplpl
          type: user
        html: "<p>\u3054\u5BFE\u5FDC\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\
          \u307E\u3057\u305F\u3002\u304A\u9670\u69D8\u30674bit\u91CF\u5B50\u5316\u3057\
          \u305F\u30E2\u30C7\u30EB\u304C\u4F7F\u3048\u308B\u3088\u3046\u306B\u306A\
          \u308A\u307E\u3057\u305F\u3002<br>\u306A\u304A\u3001\u52D5\u4F5C\u78BA\u8A8D\
          \u3068\u3057\u3066\u4F7F\u3063\u305F\u6B63\u5E38\u52D5\u4F5C\u3059\u308B\
          \u30B5\u30F3\u30D7\u30EB\u30B3\u30FC\u30C9\u3068\u6B63\u5E38\u52D5\u4F5C\
          \u3057\u305F\u3057\u305F\u7D50\u679C\u3092\u8CBC\u308A\u307E\u3059\u3002\
          </p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >from</span> auto_gptq <span class=\"hljs-keyword\">import</span> AutoGPTQForCausalLM\n\
          <span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer\n\nmodel_name_or_path = <span class=\"hljs-string\"\
          >\"mmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1k\"</span>\n\n<span class=\"\
          hljs-comment\"># Tokenizer</span>\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
          \n<span class=\"hljs-comment\"># Model</span>\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=<span class=\"hljs-literal\">True</span>,\n   \
          \     device=<span class=\"hljs-string\">\"cuda:0\"</span>,\n        use_triton=<span\
          \ class=\"hljs-literal\">True</span>,\n        quantize_config=<span class=\"\
          hljs-literal\">None</span>)\n\n<span class=\"hljs-comment\"># Your test\
          \ prompt</span>\nprompt = <span class=\"hljs-string\">\"\"\"</span>\n<span\
          \ class=\"hljs-string\">USER: \u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\
          \u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\u3044\u3002\
          </span>\n<span class=\"hljs-string\">ASSISTANT: </span>\n<span class=\"\
          hljs-string\">\"\"\"</span>\ninput_ids = tokenizer.encode(prompt, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>)\noutput_ids=model.generate(\n   \
          \ input_ids=input_ids.to(model.device),\n    max_new_tokens=<span class=\"\
          hljs-number\">300</span>,\n    do_sample=<span class=\"hljs-literal\">True</span>,\n\
          \    temperature=<span class=\"hljs-number\">0.7</span>,\n)\n<span class=\"\
          hljs-built_in\">print</span>(tokenizer.decode(output_ids[<span class=\"\
          hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n</code></pre>\n<pre><code>USER: \u4ECA\u65E5\u306E\u5915\
          \u98DF\u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\
          \u3055\u3044\u3002\nASSISTANT: \n\n\u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\
          \u30B7\u30D4\u306F\u300C\u9D8F\u8089\u3068\u91CE\u83DC\u306E\u7092\u3081\
          \u7269\u300D\u3067\u3059\u3002\n\n\u3010\u6750\u6599\u3011\n\u30FB\u9D8F\
          \u3082\u3082\u8089\u3000150g\n\u30FB\u30AD\u30E3\u30D9\u30C4\u30004\u5206\
          \u306E1\u7389\n\u30FB\u7389\u306D\u304E\u30001\u500B\n\u30FB\u306B\u3093\
          \u3058\u3093\u30001\u672C\n\u30FB\u30D4\u30FC\u30DE\u30F3\u30002\u500B\n\
          \u30FB\u30B5\u30E9\u30C0\u6CB9\u3000\u5927\u3055\u30581\n\u30FB\u9152\u3000\
          \u5927\u3055\u30581\n\u30FB\u91A4\u6CB9\u3000\u5927\u3055\u30581\n\u30FB\
          \u7802\u7CD6\u3000\u5927\u3055\u30581\n\u30FB\u5869\u3000\u5C0F\u3055\u3058\
          1/4\n\u30FB\u30B3\u30B7\u30E7\u30A6\u3000\u5C11\u3005\n\n\u3010\u4F5C\u308A\
          \u65B9\u3011\n\n1. \u91CE\u83DC\u3092\u98DF\u3079\u3084\u3059\u3044\u5927\
          \u304D\u3055\u306B\u5207\u308A\u307E\u3059\u3002\n2. \u30D5\u30E9\u30A4\u30D1\
          \u30F3\u306B\u30B5\u30E9\u30C0\u6CB9\u3092\u71B1\u3057\u3001\u9D8F\u8089\
          \u3092\u7092\u3081\u307E\u3059\u3002\n3. \u9D8F\u8089\u306B\u706B\u304C\u901A\
          \u3063\u305F\u3089\u3001\u91CE\u83DC\u3092\u52A0\u3048\u3066\u7092\u3081\
          \u307E\u3059\u3002\n4. \u91CE\u83DC\u304C\u3057\u3093\u306A\u308A\u3057\u3066\
          \u304D\u305F\u3089\u3001\u9152\u3068\u91A4\u6CB9\u3092\u52A0\u3048\u3001\
          \u7802\u7CD6\u3068\u5869\u3092\u52A0\u3048\u3001\u3055\u3089\u306B\u7092\
          \u3081\u307E\u3059\u3002\n5. \u6700\u5F8C\u306B\u30B3\u30B7\u30E7\u30A6\u3067\
          \u5473\u3092\u6574\u3048\u305F\u3089\u5B8C\u6210\u3067\u3059\u3002\n\n\u3010\
          \u30DD\u30A4\u30F3\u30C8\u3011\n\u9D8F\u8089\u306F\u76AE\u76EE\u304B\u3089\
          \u713C\u304F\u3053\u3068\u3067\u3001\u30AB\u30EA\u30AB\u30EA\u306B\u713C\
          \u304D\u4E0A\u304C\u308A\u307E\u3059\u3002\u91CE\u83DC\u306F\u706B\u304C\
          \u901A\u308A\u3084\u3059\u3044\u3088\u3046\u306B\u5C0F\u3055\u3081\u306B\
          \u5207\u308B\u3053\u3068\u304C\u30DD\u30A4\u30F3\u30C8\u3067\u3059\u3002\
          \u7518\u8F9B\u3044\u5473\u4ED8\u3051\u3067\u3054\u98EF\u304C\u9032\u3080\
          \u4E00\u54C1\u3067\u3059\u3002\n</code></pre>\n"
        raw: "\u3054\u5BFE\u5FDC\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\
          \u3057\u305F\u3002\u304A\u9670\u69D8\u30674bit\u91CF\u5B50\u5316\u3057\u305F\
          \u30E2\u30C7\u30EB\u304C\u4F7F\u3048\u308B\u3088\u3046\u306B\u306A\u308A\
          \u307E\u3057\u305F\u3002\n\u306A\u304A\u3001\u52D5\u4F5C\u78BA\u8A8D\u3068\
          \u3057\u3066\u4F7F\u3063\u305F\u6B63\u5E38\u52D5\u4F5C\u3059\u308B\u30B5\
          \u30F3\u30D7\u30EB\u30B3\u30FC\u30C9\u3068\u6B63\u5E38\u52D5\u4F5C\u3057\
          \u305F\u3057\u305F\u7D50\u679C\u3092\u8CBC\u308A\u307E\u3059\u3002\n\n```python\n\
          from auto_gptq import AutoGPTQForCausalLM\nfrom transformers import AutoTokenizer\n\
          \nmodel_name_or_path = \"mmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1k\"\
          \n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
          \n# Model\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        use_safetensors=True,\n        device=\"cuda:0\",\n        use_triton=True,\n\
          \        quantize_config=None)\n\n# Your test prompt\nprompt = \"\"\"\n\
          USER: \u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\u30B7\u30D4\u3092\u7D39\
          \u4ECB\u3057\u3066\u304F\u3060\u3055\u3044\u3002\nASSISTANT: \n\"\"\"\n\
          input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\noutput_ids=model.generate(\n\
          \    input_ids=input_ids.to(model.device),\n    max_new_tokens=300,\n  \
          \  do_sample=True,\n    temperature=0.7,\n)\nprint(tokenizer.decode(output_ids[0],\
          \ skip_special_tokens=True))\n```\n\n```\nUSER: \u4ECA\u65E5\u306E\u5915\
          \u98DF\u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\
          \u3055\u3044\u3002\nASSISTANT: \n\n\u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\
          \u30B7\u30D4\u306F\u300C\u9D8F\u8089\u3068\u91CE\u83DC\u306E\u7092\u3081\
          \u7269\u300D\u3067\u3059\u3002\n\n\u3010\u6750\u6599\u3011\n\u30FB\u9D8F\
          \u3082\u3082\u8089\u3000150g\n\u30FB\u30AD\u30E3\u30D9\u30C4\u30004\u5206\
          \u306E1\u7389\n\u30FB\u7389\u306D\u304E\u30001\u500B\n\u30FB\u306B\u3093\
          \u3058\u3093\u30001\u672C\n\u30FB\u30D4\u30FC\u30DE\u30F3\u30002\u500B\n\
          \u30FB\u30B5\u30E9\u30C0\u6CB9\u3000\u5927\u3055\u30581\n\u30FB\u9152\u3000\
          \u5927\u3055\u30581\n\u30FB\u91A4\u6CB9\u3000\u5927\u3055\u30581\n\u30FB\
          \u7802\u7CD6\u3000\u5927\u3055\u30581\n\u30FB\u5869\u3000\u5C0F\u3055\u3058\
          1/4\n\u30FB\u30B3\u30B7\u30E7\u30A6\u3000\u5C11\u3005\n\n\u3010\u4F5C\u308A\
          \u65B9\u3011\n\n1. \u91CE\u83DC\u3092\u98DF\u3079\u3084\u3059\u3044\u5927\
          \u304D\u3055\u306B\u5207\u308A\u307E\u3059\u3002\n2. \u30D5\u30E9\u30A4\u30D1\
          \u30F3\u306B\u30B5\u30E9\u30C0\u6CB9\u3092\u71B1\u3057\u3001\u9D8F\u8089\
          \u3092\u7092\u3081\u307E\u3059\u3002\n3. \u9D8F\u8089\u306B\u706B\u304C\u901A\
          \u3063\u305F\u3089\u3001\u91CE\u83DC\u3092\u52A0\u3048\u3066\u7092\u3081\
          \u307E\u3059\u3002\n4. \u91CE\u83DC\u304C\u3057\u3093\u306A\u308A\u3057\u3066\
          \u304D\u305F\u3089\u3001\u9152\u3068\u91A4\u6CB9\u3092\u52A0\u3048\u3001\
          \u7802\u7CD6\u3068\u5869\u3092\u52A0\u3048\u3001\u3055\u3089\u306B\u7092\
          \u3081\u307E\u3059\u3002\n5. \u6700\u5F8C\u306B\u30B3\u30B7\u30E7\u30A6\u3067\
          \u5473\u3092\u6574\u3048\u305F\u3089\u5B8C\u6210\u3067\u3059\u3002\n\n\u3010\
          \u30DD\u30A4\u30F3\u30C8\u3011\n\u9D8F\u8089\u306F\u76AE\u76EE\u304B\u3089\
          \u713C\u304F\u3053\u3068\u3067\u3001\u30AB\u30EA\u30AB\u30EA\u306B\u713C\
          \u304D\u4E0A\u304C\u308A\u307E\u3059\u3002\u91CE\u83DC\u306F\u706B\u304C\
          \u901A\u308A\u3084\u3059\u3044\u3088\u3046\u306B\u5C0F\u3055\u3081\u306B\
          \u5207\u308B\u3053\u3068\u304C\u30DD\u30A4\u30F3\u30C8\u3067\u3059\u3002\
          \u7518\u8F9B\u3044\u5473\u4ED8\u3051\u3067\u3054\u98EF\u304C\u9032\u3080\
          \u4E00\u54C1\u3067\u3059\u3002\n```"
        updatedAt: '2023-11-04T00:17:23.741Z'
      numEdits: 0
      reactions: []
    id: 65458d936070dd5df723a6f0
    type: comment
  author: alfredplpl
  content: "\u3054\u5BFE\u5FDC\u3042\u308A\u304C\u3068\u3046\u3054\u3056\u3044\u307E\
    \u3057\u305F\u3002\u304A\u9670\u69D8\u30674bit\u91CF\u5B50\u5316\u3057\u305F\u30E2\
    \u30C7\u30EB\u304C\u4F7F\u3048\u308B\u3088\u3046\u306B\u306A\u308A\u307E\u3057\
    \u305F\u3002\n\u306A\u304A\u3001\u52D5\u4F5C\u78BA\u8A8D\u3068\u3057\u3066\u4F7F\
    \u3063\u305F\u6B63\u5E38\u52D5\u4F5C\u3059\u308B\u30B5\u30F3\u30D7\u30EB\u30B3\
    \u30FC\u30C9\u3068\u6B63\u5E38\u52D5\u4F5C\u3057\u305F\u3057\u305F\u7D50\u679C\
    \u3092\u8CBC\u308A\u307E\u3059\u3002\n\n```python\nfrom auto_gptq import AutoGPTQForCausalLM\n\
    from transformers import AutoTokenizer\n\nmodel_name_or_path = \"mmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1k\"\
    \n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\
    \n# Model\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n  \
    \      use_safetensors=True,\n        device=\"cuda:0\",\n        use_triton=True,\n\
    \        quantize_config=None)\n\n# Your test prompt\nprompt = \"\"\"\nUSER: \u4ECA\
    \u65E5\u306E\u5915\u98DF\u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\
    \u304F\u3060\u3055\u3044\u3002\nASSISTANT: \n\"\"\"\ninput_ids = tokenizer.encode(prompt,\
    \ return_tensors=\"pt\")\noutput_ids=model.generate(\n    input_ids=input_ids.to(model.device),\n\
    \    max_new_tokens=300,\n    do_sample=True,\n    temperature=0.7,\n)\nprint(tokenizer.decode(output_ids[0],\
    \ skip_special_tokens=True))\n```\n\n```\nUSER: \u4ECA\u65E5\u306E\u5915\u98DF\
    \u306E\u30EC\u30B7\u30D4\u3092\u7D39\u4ECB\u3057\u3066\u304F\u3060\u3055\u3044\
    \u3002\nASSISTANT: \n\n\u4ECA\u65E5\u306E\u5915\u98DF\u306E\u30EC\u30B7\u30D4\u306F\
    \u300C\u9D8F\u8089\u3068\u91CE\u83DC\u306E\u7092\u3081\u7269\u300D\u3067\u3059\
    \u3002\n\n\u3010\u6750\u6599\u3011\n\u30FB\u9D8F\u3082\u3082\u8089\u3000150g\n\
    \u30FB\u30AD\u30E3\u30D9\u30C4\u30004\u5206\u306E1\u7389\n\u30FB\u7389\u306D\u304E\
    \u30001\u500B\n\u30FB\u306B\u3093\u3058\u3093\u30001\u672C\n\u30FB\u30D4\u30FC\
    \u30DE\u30F3\u30002\u500B\n\u30FB\u30B5\u30E9\u30C0\u6CB9\u3000\u5927\u3055\u3058\
    1\n\u30FB\u9152\u3000\u5927\u3055\u30581\n\u30FB\u91A4\u6CB9\u3000\u5927\u3055\
    \u30581\n\u30FB\u7802\u7CD6\u3000\u5927\u3055\u30581\n\u30FB\u5869\u3000\u5C0F\
    \u3055\u30581/4\n\u30FB\u30B3\u30B7\u30E7\u30A6\u3000\u5C11\u3005\n\n\u3010\u4F5C\
    \u308A\u65B9\u3011\n\n1. \u91CE\u83DC\u3092\u98DF\u3079\u3084\u3059\u3044\u5927\
    \u304D\u3055\u306B\u5207\u308A\u307E\u3059\u3002\n2. \u30D5\u30E9\u30A4\u30D1\u30F3\
    \u306B\u30B5\u30E9\u30C0\u6CB9\u3092\u71B1\u3057\u3001\u9D8F\u8089\u3092\u7092\
    \u3081\u307E\u3059\u3002\n3. \u9D8F\u8089\u306B\u706B\u304C\u901A\u3063\u305F\u3089\
    \u3001\u91CE\u83DC\u3092\u52A0\u3048\u3066\u7092\u3081\u307E\u3059\u3002\n4. \u91CE\
    \u83DC\u304C\u3057\u3093\u306A\u308A\u3057\u3066\u304D\u305F\u3089\u3001\u9152\
    \u3068\u91A4\u6CB9\u3092\u52A0\u3048\u3001\u7802\u7CD6\u3068\u5869\u3092\u52A0\
    \u3048\u3001\u3055\u3089\u306B\u7092\u3081\u307E\u3059\u3002\n5. \u6700\u5F8C\u306B\
    \u30B3\u30B7\u30E7\u30A6\u3067\u5473\u3092\u6574\u3048\u305F\u3089\u5B8C\u6210\
    \u3067\u3059\u3002\n\n\u3010\u30DD\u30A4\u30F3\u30C8\u3011\n\u9D8F\u8089\u306F\
    \u76AE\u76EE\u304B\u3089\u713C\u304F\u3053\u3068\u3067\u3001\u30AB\u30EA\u30AB\
    \u30EA\u306B\u713C\u304D\u4E0A\u304C\u308A\u307E\u3059\u3002\u91CE\u83DC\u306F\
    \u706B\u304C\u901A\u308A\u3084\u3059\u3044\u3088\u3046\u306B\u5C0F\u3055\u3081\
    \u306B\u5207\u308B\u3053\u3068\u304C\u30DD\u30A4\u30F3\u30C8\u3067\u3059\u3002\
    \u7518\u8F9B\u3044\u5473\u4ED8\u3051\u3067\u3054\u98EF\u304C\u9032\u3080\u4E00\
    \u54C1\u3067\u3059\u3002\n```"
  created_at: 2023-11-03 23:17:23+00:00
  edited: false
  hidden: false
  id: 65458d936070dd5df723a6f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670594087059-630412d57373aacccd88af95.jpeg?w=200&h=200&f=face
      fullname: Yasunori Ozaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alfredplpl
      type: user
    createdAt: '2023-11-04T00:33:41.000Z'
    data:
      status: closed
    id: 65459165c6dadd513fe53680
    type: status-change
  author: alfredplpl
  created_at: 2023-11-03 23:33:41+00:00
  id: 65459165c6dadd513fe53680
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mmnga/cyberagent-calm2-7b-chat-GPTQ-calib-ja-1k
repo_type: model
status: closed
target_branch: null
title: "bfloat16\u3067\u306A\u304Ffloat16\u306B\u3088\u308B\u91CF\u5B50\u5316"
