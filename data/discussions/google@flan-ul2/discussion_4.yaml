!!python/object:huggingface_hub.community.DiscussionWithDetails
author: abacaj
conflicting_files: null
created_at: 2023-03-03 16:11:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677644903536-62ceeb27e7f6014c0e9d9268.jpeg?w=200&h=200&f=face
      fullname: Anton Bacaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abacaj
      type: user
    createdAt: '2023-03-03T16:11:22.000Z'
    data:
      edited: false
      editors:
      - abacaj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677644903536-62ceeb27e7f6014c0e9d9268.jpeg?w=200&h=200&f=face
          fullname: Anton Bacaj
          isHf: false
          isPro: false
          name: abacaj
          type: user
        html: '<p>Hi, I think this model is still limited to the original 512 positions
          as well as a 512 tokenizer</p>

          '
        raw: Hi, I think this model is still limited to the original 512 positions
          as well as a 512 tokenizer
        updatedAt: '2023-03-03T16:11:22.172Z'
      numEdits: 0
      reactions: []
    id: 64021c2a3e3d0f2745b042da
    type: comment
  author: abacaj
  content: Hi, I think this model is still limited to the original 512 positions as
    well as a 512 tokenizer
  created_at: 2023-03-03 16:11:22+00:00
  edited: false
  hidden: false
  id: 64021c2a3e3d0f2745b042da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-03-03T16:28:49.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;abacaj&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/abacaj\"\
          >@<span class=\"underline\">abacaj</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I think that it works fine, I did a quick check using this Space:\
          \ <a href=\"https://huggingface.co/spaces/ybelkada/i-like-flan-ul2\">https://huggingface.co/spaces/ybelkada/i-like-flan-ul2</a>\
          \ and fed an input of length <code>966</code>:</p>\n<pre><code>Summarize\
          \ the following text: On February 6, 2023, earthquakes measuring 7.7 and\
          \ 7.6 hit South Eastern Turkey, affecting 10 cities and resulting in more\
          \ than 42,000 deaths and 120,000 injured as of February 21.\n\nA few hours\
          \ after the earthquake, a group of programmers started a Discord server\
          \ to roll out an application called afetharita, literally meaning, disaster\
          \ map. This application would serve search &amp; rescue teams and volunteers\
          \ to find survivors and bring them help. The need for such an app arose\
          \ when survivors posted screenshots of texts with their addresses and what\
          \ they needed (including rescue) on social media. Some survivors also tweeted\
          \ what they needed so their relatives knew they were alive and that they\
          \ need rescue. Needing to extract information from these tweets, we developed\
          \ various applications to turn them into structured data and raced against\
          \ time in developing and deploying these apps.\n\nWhen I got invited to\
          \ the discord server, there was quite a lot of chaos regarding how we (volunteers)\
          \ would operate and what we would do. We decided to collaboratively train\
          \ models so we needed a model and dataset registry. We opened a Hugging\
          \ Face organization account and collaborated through pull requests as to\
          \ build ML-based applications to receive and process information.\n\nWe\
          \ had been told by volunteers in other teams that there's a need for an\
          \ application to post screenshots, extract information from the screenshots,\
          \ structure it and write the structured information to the database. We\
          \ started developing an application that would take a given image, extract\
          \ the text first, and from text, extract a name, telephone number, and address\
          \ and write these informations to a database that would be handed to authorities.\
          \ After experimenting with various open-source OCR tools, we started using\
          \ easyocr for OCR part and Gradio for building an interface for this application.\
          \ We were asked to build a standalone application for OCR as well so we\
          \ opened endpoints from the interface. The text output from OCR is parsed\
          \ using transformers-based fine-tuned NER model.\n\nTo collaborate and improve\
          \ the application, we hosted it on Hugging Face Spaces and we've received\
          \ a GPU grant to keep the application up and running. Hugging Face Hub team\
          \ has set us up a CI bot for us to have an ephemeral environment, so we\
          \ could see how a pull request would affect the Space, and it helped us\
          \ during pull request reviews.\n\nLater on, we were given labeled content\
          \ from various channels (e.g. twitter, discord) with raw tweets of survivors'\
          \ calls for help, along with the addresses and personal information extracted\
          \ from them. We started experimenting both with few-shot prompting of closed-source\
          \ models and fine-tuning our own token classification model from transformers.\
          \ We\u2019ve used bert-base-turkish-cased as a base model for token classification\
          \ and came up with the first address extraction model.\n\nThe model was\
          \ later used in afetharita to extract addresses. The parsed addresses would\
          \ be sent to a geocoding API to obtain longitude and latitude, and the geolocation\
          \ would then be displayed on the front-end map. For inference, we have used\
          \ Inference API, which is an API that hosts model for inference and is automatically\
          \ enabled when the model is pushed to Hugging Face Hub. Using Inference\
          \ API for serving has saved us from pulling the model, writing an app, building\
          \ a docker image, setting up CI/CD, and deploying the model to a cloud instance,\
          \ where it would be extra overhead work for the DevOps and cloud teams as\
          \ well. Hugging Face teams have provided us with more replicas so that there\
          \ would be no downtime and the application would be robust against a lot\
          \ of traffic.\n\nLater on, we were asked if we could extract what earthquake\
          \ survivors need from a given tweet. We were given data with multiple labels\
          \ for multiple needs in a given tweet, and these needs could be shelter,\
          \ food, or logistics, as it was freezing cold over there. We\u2019ve started\
          \ experimenting first with zero-shot experimentations with open-source NLI\
          \ models on Hugging Face Hub and few-shot experimentations with closed-source\
          \ generative model endpoints. We have tried xlm-roberta-large-xnli and convbert-base-turkish-mc4-cased-allnli_tr.\
          \ \n\n&gt;&gt;&gt; len(tokenizer(text).input_ids)\n&gt;&gt;&gt; 966\n</code></pre>\n\
          <p>(Btw  the text that I got from this blogpost: <a href=\"https://huggingface.co/blog/using-ml-for-disasters\"\
          >https://huggingface.co/blog/using-ml-for-disasters</a> )</p>\n<p>when I\
          \ fed it to the model I got:<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1677860860412-62441d1d9fdefb55a0b7d12c.png\"\
          ><img alt=\"Screenshot 2023-03-03 at 17.25.56.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1677860860412-62441d1d9fdefb55a0b7d12c.png\"\
          ></a></p>\n<p>Note that you can't pass an input that is larger than 1000\
          \ tokens on inference endpoints</p>\n"
        raw: "Hello @abacaj \n\nI think that it works fine, I did a quick check using\
          \ this Space: https://huggingface.co/spaces/ybelkada/i-like-flan-ul2 and\
          \ fed an input of length `966`:\n\n```\nSummarize the following text: On\
          \ February 6, 2023, earthquakes measuring 7.7 and 7.6 hit South Eastern\
          \ Turkey, affecting 10 cities and resulting in more than 42,000 deaths and\
          \ 120,000 injured as of February 21.\n\nA few hours after the earthquake,\
          \ a group of programmers started a Discord server to roll out an application\
          \ called afetharita, literally meaning, disaster map. This application would\
          \ serve search & rescue teams and volunteers to find survivors and bring\
          \ them help. The need for such an app arose when survivors posted screenshots\
          \ of texts with their addresses and what they needed (including rescue)\
          \ on social media. Some survivors also tweeted what they needed so their\
          \ relatives knew they were alive and that they need rescue. Needing to extract\
          \ information from these tweets, we developed various applications to turn\
          \ them into structured data and raced against time in developing and deploying\
          \ these apps.\n\nWhen I got invited to the discord server, there was quite\
          \ a lot of chaos regarding how we (volunteers) would operate and what we\
          \ would do. We decided to collaboratively train models so we needed a model\
          \ and dataset registry. We opened a Hugging Face organization account and\
          \ collaborated through pull requests as to build ML-based applications to\
          \ receive and process information.\n\nWe had been told by volunteers in\
          \ other teams that there's a need for an application to post screenshots,\
          \ extract information from the screenshots, structure it and write the structured\
          \ information to the database. We started developing an application that\
          \ would take a given image, extract the text first, and from text, extract\
          \ a name, telephone number, and address and write these informations to\
          \ a database that would be handed to authorities. After experimenting with\
          \ various open-source OCR tools, we started using easyocr for OCR part and\
          \ Gradio for building an interface for this application. We were asked to\
          \ build a standalone application for OCR as well so we opened endpoints\
          \ from the interface. The text output from OCR is parsed using transformers-based\
          \ fine-tuned NER model.\n\nTo collaborate and improve the application, we\
          \ hosted it on Hugging Face Spaces and we've received a GPU grant to keep\
          \ the application up and running. Hugging Face Hub team has set us up a\
          \ CI bot for us to have an ephemeral environment, so we could see how a\
          \ pull request would affect the Space, and it helped us during pull request\
          \ reviews.\n\nLater on, we were given labeled content from various channels\
          \ (e.g. twitter, discord) with raw tweets of survivors' calls for help,\
          \ along with the addresses and personal information extracted from them.\
          \ We started experimenting both with few-shot prompting of closed-source\
          \ models and fine-tuning our own token classification model from transformers.\
          \ We\u2019ve used bert-base-turkish-cased as a base model for token classification\
          \ and came up with the first address extraction model.\n\nThe model was\
          \ later used in afetharita to extract addresses. The parsed addresses would\
          \ be sent to a geocoding API to obtain longitude and latitude, and the geolocation\
          \ would then be displayed on the front-end map. For inference, we have used\
          \ Inference API, which is an API that hosts model for inference and is automatically\
          \ enabled when the model is pushed to Hugging Face Hub. Using Inference\
          \ API for serving has saved us from pulling the model, writing an app, building\
          \ a docker image, setting up CI/CD, and deploying the model to a cloud instance,\
          \ where it would be extra overhead work for the DevOps and cloud teams as\
          \ well. Hugging Face teams have provided us with more replicas so that there\
          \ would be no downtime and the application would be robust against a lot\
          \ of traffic.\n\nLater on, we were asked if we could extract what earthquake\
          \ survivors need from a given tweet. We were given data with multiple labels\
          \ for multiple needs in a given tweet, and these needs could be shelter,\
          \ food, or logistics, as it was freezing cold over there. We\u2019ve started\
          \ experimenting first with zero-shot experimentations with open-source NLI\
          \ models on Hugging Face Hub and few-shot experimentations with closed-source\
          \ generative model endpoints. We have tried xlm-roberta-large-xnli and convbert-base-turkish-mc4-cased-allnli_tr.\
          \ \n\n>>> len(tokenizer(text).input_ids)\n>>> 966\n```\n(Btw  the text that\
          \ I got from this blogpost: https://huggingface.co/blog/using-ml-for-disasters\
          \ )\n\nwhen I fed it to the model I got: \n![Screenshot 2023-03-03 at 17.25.56.png](https://cdn-uploads.huggingface.co/production/uploads/1677860860412-62441d1d9fdefb55a0b7d12c.png)\n\
          \nNote that you can't pass an input that is larger than 1000 tokens on inference\
          \ endpoints"
        updatedAt: '2023-03-03T16:29:16.725Z'
      numEdits: 1
      reactions: []
    id: 64022041c17fbf7a4f746af6
    type: comment
  author: ybelkada
  content: "Hello @abacaj \n\nI think that it works fine, I did a quick check using\
    \ this Space: https://huggingface.co/spaces/ybelkada/i-like-flan-ul2 and fed an\
    \ input of length `966`:\n\n```\nSummarize the following text: On February 6,\
    \ 2023, earthquakes measuring 7.7 and 7.6 hit South Eastern Turkey, affecting\
    \ 10 cities and resulting in more than 42,000 deaths and 120,000 injured as of\
    \ February 21.\n\nA few hours after the earthquake, a group of programmers started\
    \ a Discord server to roll out an application called afetharita, literally meaning,\
    \ disaster map. This application would serve search & rescue teams and volunteers\
    \ to find survivors and bring them help. The need for such an app arose when survivors\
    \ posted screenshots of texts with their addresses and what they needed (including\
    \ rescue) on social media. Some survivors also tweeted what they needed so their\
    \ relatives knew they were alive and that they need rescue. Needing to extract\
    \ information from these tweets, we developed various applications to turn them\
    \ into structured data and raced against time in developing and deploying these\
    \ apps.\n\nWhen I got invited to the discord server, there was quite a lot of\
    \ chaos regarding how we (volunteers) would operate and what we would do. We decided\
    \ to collaboratively train models so we needed a model and dataset registry. We\
    \ opened a Hugging Face organization account and collaborated through pull requests\
    \ as to build ML-based applications to receive and process information.\n\nWe\
    \ had been told by volunteers in other teams that there's a need for an application\
    \ to post screenshots, extract information from the screenshots, structure it\
    \ and write the structured information to the database. We started developing\
    \ an application that would take a given image, extract the text first, and from\
    \ text, extract a name, telephone number, and address and write these informations\
    \ to a database that would be handed to authorities. After experimenting with\
    \ various open-source OCR tools, we started using easyocr for OCR part and Gradio\
    \ for building an interface for this application. We were asked to build a standalone\
    \ application for OCR as well so we opened endpoints from the interface. The text\
    \ output from OCR is parsed using transformers-based fine-tuned NER model.\n\n\
    To collaborate and improve the application, we hosted it on Hugging Face Spaces\
    \ and we've received a GPU grant to keep the application up and running. Hugging\
    \ Face Hub team has set us up a CI bot for us to have an ephemeral environment,\
    \ so we could see how a pull request would affect the Space, and it helped us\
    \ during pull request reviews.\n\nLater on, we were given labeled content from\
    \ various channels (e.g. twitter, discord) with raw tweets of survivors' calls\
    \ for help, along with the addresses and personal information extracted from them.\
    \ We started experimenting both with few-shot prompting of closed-source models\
    \ and fine-tuning our own token classification model from transformers. We\u2019\
    ve used bert-base-turkish-cased as a base model for token classification and came\
    \ up with the first address extraction model.\n\nThe model was later used in afetharita\
    \ to extract addresses. The parsed addresses would be sent to a geocoding API\
    \ to obtain longitude and latitude, and the geolocation would then be displayed\
    \ on the front-end map. For inference, we have used Inference API, which is an\
    \ API that hosts model for inference and is automatically enabled when the model\
    \ is pushed to Hugging Face Hub. Using Inference API for serving has saved us\
    \ from pulling the model, writing an app, building a docker image, setting up\
    \ CI/CD, and deploying the model to a cloud instance, where it would be extra\
    \ overhead work for the DevOps and cloud teams as well. Hugging Face teams have\
    \ provided us with more replicas so that there would be no downtime and the application\
    \ would be robust against a lot of traffic.\n\nLater on, we were asked if we could\
    \ extract what earthquake survivors need from a given tweet. We were given data\
    \ with multiple labels for multiple needs in a given tweet, and these needs could\
    \ be shelter, food, or logistics, as it was freezing cold over there. We\u2019\
    ve started experimenting first with zero-shot experimentations with open-source\
    \ NLI models on Hugging Face Hub and few-shot experimentations with closed-source\
    \ generative model endpoints. We have tried xlm-roberta-large-xnli and convbert-base-turkish-mc4-cased-allnli_tr.\
    \ \n\n>>> len(tokenizer(text).input_ids)\n>>> 966\n```\n(Btw  the text that I\
    \ got from this blogpost: https://huggingface.co/blog/using-ml-for-disasters )\n\
    \nwhen I fed it to the model I got: \n![Screenshot 2023-03-03 at 17.25.56.png](https://cdn-uploads.huggingface.co/production/uploads/1677860860412-62441d1d9fdefb55a0b7d12c.png)\n\
    \nNote that you can't pass an input that is larger than 1000 tokens on inference\
    \ endpoints"
  created_at: 2023-03-03 16:28:49+00:00
  edited: true
  hidden: false
  id: 64022041c17fbf7a4f746af6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677644903536-62ceeb27e7f6014c0e9d9268.jpeg?w=200&h=200&f=face
      fullname: Anton Bacaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abacaj
      type: user
    createdAt: '2023-03-03T19:15:38.000Z'
    data:
      edited: false
      editors:
      - abacaj
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677644903536-62ceeb27e7f6014c0e9d9268.jpeg?w=200&h=200&f=face
          fullname: Anton Bacaj
          isHf: false
          isPro: false
          name: abacaj
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ thanks for following up. Looks like it is working, maybe I loaded wrong\
          \ config. Was seeing the warning:</p>\n<pre><code>Token indices sequence\
          \ length is longer than the specified maximum sequence length for this model\
          \ (1348 &gt; 512). Running this sequence through the model will result in\
          \ indexing errors\n</code></pre>\n"
        raw: 'Hi @ybelkada thanks for following up. Looks like it is working, maybe
          I loaded wrong config. Was seeing the warning:

          ```

          Token indices sequence length is longer than the specified maximum sequence
          length for this model (1348 > 512). Running this sequence through the model
          will result in indexing errors

          ```'
        updatedAt: '2023-03-03T19:15:38.476Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - will33am
    id: 6402475a4682c8c277b1c8e5
    type: comment
  author: abacaj
  content: 'Hi @ybelkada thanks for following up. Looks like it is working, maybe
    I loaded wrong config. Was seeing the warning:

    ```

    Token indices sequence length is longer than the specified maximum sequence length
    for this model (1348 > 512). Running this sequence through the model will result
    in indexing errors

    ```'
  created_at: 2023-03-03 19:15:38+00:00
  edited: false
  hidden: false
  id: 6402475a4682c8c277b1c8e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8d4f4520ae8a197c768aa3f58478cf2.svg
      fullname: Sachin Vernekar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: svernek
      type: user
    createdAt: '2023-03-05T12:15:47.000Z'
    data:
      edited: true
      editors:
      - svernek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8d4f4520ae8a197c768aa3f58478cf2.svg
          fullname: Sachin Vernekar
          isHf: false
          isPro: false
          name: svernek
          type: user
        html: '<p>Hi for me it still shows 512 as the max sequence length. How do
          you fix this? The model card for Flan-UL2 says it supports 2048 tokens!
          I am running it on my local machine.</p>

          '
        raw: Hi for me it still shows 512 as the max sequence length. How do you fix
          this? The model card for Flan-UL2 says it supports 2048 tokens! I am running
          it on my local machine.
        updatedAt: '2023-03-05T16:52:51.635Z'
      numEdits: 1
      reactions: []
    id: 640487f323d788411b59b299
    type: comment
  author: svernek
  content: Hi for me it still shows 512 as the max sequence length. How do you fix
    this? The model card for Flan-UL2 says it supports 2048 tokens! I am running it
    on my local machine.
  created_at: 2023-03-05 12:15:47+00:00
  edited: true
  hidden: false
  id: 640487f323d788411b59b299
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641203017724-noauth.png?w=200&h=200&f=face
      fullname: Joao Gante
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joaogante
      type: user
    createdAt: '2023-03-05T21:14:47.000Z'
    data:
      edited: false
      editors:
      - joaogante
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1641203017724-noauth.png?w=200&h=200&f=face
          fullname: Joao Gante
          isHf: true
          isPro: false
          name: joaogante
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> I believe changing\
          \ <a href=\"https://huggingface.co/google/flan-ul2/blob/5dfdc8c4a1ae47a6c1487a88636ac8a712e8808a/tokenizer_config.json#L106\"\
          >this line</a> to 2048 sorts the issue :) After changing it, everything\
          \ seems to work well, and there is no (misleading) warning.</p>\n<p>I've\
          \ opened a PR with the fix <a href=\"https://huggingface.co/google/flan-ul2/discussions/10\"\
          >here</a></p>\n"
        raw: '@ybelkada I believe changing [this line](https://huggingface.co/google/flan-ul2/blob/5dfdc8c4a1ae47a6c1487a88636ac8a712e8808a/tokenizer_config.json#L106)
          to 2048 sorts the issue :) After changing it, everything seems to work well,
          and there is no (misleading) warning.


          I''ve opened a PR with the fix [here](https://huggingface.co/google/flan-ul2/discussions/10)'
        updatedAt: '2023-03-05T21:14:47.966Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ybelkada
        - svernek
    id: 64050647ad24e26b447f4f2b
    type: comment
  author: joaogante
  content: '@ybelkada I believe changing [this line](https://huggingface.co/google/flan-ul2/blob/5dfdc8c4a1ae47a6c1487a88636ac8a712e8808a/tokenizer_config.json#L106)
    to 2048 sorts the issue :) After changing it, everything seems to work well, and
    there is no (misleading) warning.


    I''ve opened a PR with the fix [here](https://huggingface.co/google/flan-ul2/discussions/10)'
  created_at: 2023-03-05 21:14:47+00:00
  edited: false
  hidden: false
  id: 64050647ad24e26b447f4f2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-03-06T09:01:42.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks! Indeed I think the fix proposed by <span data-props=\"{&quot;user&quot;:&quot;joaogante&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/joaogante\"\
          >@<span class=\"underline\">joaogante</span></a></span>\n\n\t</span></span>\
          \ should fix the issue</p>\n"
        raw: Thanks! Indeed I think the fix proposed by @joaogante should fix the
          issue
        updatedAt: '2023-03-06T09:01:42.347Z'
      numEdits: 0
      reactions: []
    id: 6405abf674c2ddcd64f92c38
    type: comment
  author: ybelkada
  content: Thanks! Indeed I think the fix proposed by @joaogante should fix the issue
  created_at: 2023-03-06 09:01:42+00:00
  edited: false
  hidden: false
  id: 6405abf674c2ddcd64f92c38
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: google/flan-ul2
repo_type: model
status: open
target_branch: null
title: Max embedding size
