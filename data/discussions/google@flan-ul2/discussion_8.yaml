!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MoritzLaurer
conflicting_files: null
created_at: 2023-03-04 11:48:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2023-03-04T11:48:49.000Z'
    data:
      edited: true
      editors:
      - MoritzLaurer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
          fullname: Moritz Laurer
          isHf: true
          isPro: false
          name: MoritzLaurer
          type: user
        html: "<p>Loading the model in 8bit=True does not seem to work on google colab.</p>\n\
          <pre><code>model_name =  \"google/flan-ul2\"\nfrom transformers import T5Tokenizer,\
          \ T5ForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = T5ForConditionalGeneration.from_pretrained(\n    model_name, \n\
          \    #torch_dtype=torch.bfloat16, \n    load_in_8bit=True,\n    device_map=\"\
          auto\",\n    #offload_folder=\"offload\",  \n    #offload_state_dict=True,\n\
          )\n</code></pre>\n<p>This code leads to the following error. I'm running\
          \ it on a standard google colab GPU (Tesla T4, 15GB RAM).<br>(I had successfully\
          \ loaded the model with <code>torch_dtype=torch.bfloat16</code> and offloading\
          \ (accelerate and bitsandbytes is installed), but it doesn't seem to work\
          \ with 8bit)</p>\n<pre><code>---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          &lt;ipython-input-22-b20ea4cc84c4&gt; in &lt;module&gt;\n      3 from transformers\
          \ import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n      4\
          \ tokenizer = AutoTokenizer.from_pretrained(model_name)\n----&gt; 5 model\
          \ = T5ForConditionalGeneration.from_pretrained(\n      6     model_name,\n\
          \      7     #torch_dtype=torch.bfloat16,\n\n/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
          \   2423                 }\n   2424                 if \"cpu\" in device_map_without_lm_head.values()\
          \ or \"disk\" in device_map_without_lm_head.values():\n-&gt; 2425      \
          \               raise ValueError(\n   2426                         \"\"\"\
          \n   2427                         Some modules are dispatched on the CPU\
          \ or the disk. Make sure you have enough GPU RAM to fit\n\nValueError: \n\
          \                        Some modules are dispatched on the CPU or the disk.\
          \ Make sure you have enough GPU RAM to fit\n                        the\
          \ quantized model. If you have set a value for `max_memory` you should increase\
          \ that. To have\n                        an idea of the modules that are\
          \ set on the CPU or RAM you can print model.hf_device_map.\n           \
          \             \n</code></pre>\n"
        raw: "Loading the model in 8bit=True does not seem to work on google colab.\n\
          \n```\nmodel_name =  \"google/flan-ul2\"\nfrom transformers import T5Tokenizer,\
          \ T5ForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
          model = T5ForConditionalGeneration.from_pretrained(\n    model_name, \n\
          \    #torch_dtype=torch.bfloat16, \n    load_in_8bit=True,\n    device_map=\"\
          auto\",\n    #offload_folder=\"offload\",  \n    #offload_state_dict=True,\n\
          )\n```\n\nThis code leads to the following error. I'm running it on a standard\
          \ google colab GPU (Tesla T4, 15GB RAM). \n(I had successfully loaded the\
          \ model with `torch_dtype=torch.bfloat16` and offloading (accelerate and\
          \ bitsandbytes is installed), but it doesn't seem to work with 8bit)\n\n\
          ```\n---------------------------------------------------------------------------\n\
          ValueError                                Traceback (most recent call last)\n\
          <ipython-input-22-b20ea4cc84c4> in <module>\n      3 from transformers import\
          \ T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n      4 tokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\n----> 5 model = T5ForConditionalGeneration.from_pretrained(\n\
          \      6     model_name,\n      7     #torch_dtype=torch.bfloat16,\n\n/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\
          \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
          \   2423                 }\n   2424                 if \"cpu\" in device_map_without_lm_head.values()\
          \ or \"disk\" in device_map_without_lm_head.values():\n-> 2425         \
          \            raise ValueError(\n   2426                         \"\"\"\n\
          \   2427                         Some modules are dispatched on the CPU\
          \ or the disk. Make sure you have enough GPU RAM to fit\n\nValueError: \n\
          \                        Some modules are dispatched on the CPU or the disk.\
          \ Make sure you have enough GPU RAM to fit\n                        the\
          \ quantized model. If you have set a value for `max_memory` you should increase\
          \ that. To have\n                        an idea of the modules that are\
          \ set on the CPU or RAM you can print model.hf_device_map.\n           \
          \             \n```"
        updatedAt: '2023-03-04T11:49:50.969Z'
      numEdits: 1
      reactions: []
    id: 6403302156038547951de771
    type: comment
  author: MoritzLaurer
  content: "Loading the model in 8bit=True does not seem to work on google colab.\n\
    \n```\nmodel_name =  \"google/flan-ul2\"\nfrom transformers import T5Tokenizer,\
    \ T5ForConditionalGeneration, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\
    model = T5ForConditionalGeneration.from_pretrained(\n    model_name, \n    #torch_dtype=torch.bfloat16,\
    \ \n    load_in_8bit=True,\n    device_map=\"auto\",\n    #offload_folder=\"offload\"\
    ,  \n    #offload_state_dict=True,\n)\n```\n\nThis code leads to the following\
    \ error. I'm running it on a standard google colab GPU (Tesla T4, 15GB RAM). \n\
    (I had successfully loaded the model with `torch_dtype=torch.bfloat16` and offloading\
    \ (accelerate and bitsandbytes is installed), but it doesn't seem to work with\
    \ 8bit)\n\n```\n---------------------------------------------------------------------------\n\
    ValueError                                Traceback (most recent call last)\n\
    <ipython-input-22-b20ea4cc84c4> in <module>\n      3 from transformers import\
    \ T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer\n      4 tokenizer =\
    \ AutoTokenizer.from_pretrained(model_name)\n----> 5 model = T5ForConditionalGeneration.from_pretrained(\n\
    \      6     model_name,\n      7     #torch_dtype=torch.bfloat16,\n\n/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\
    \ in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n\
    \   2423                 }\n   2424                 if \"cpu\" in device_map_without_lm_head.values()\
    \ or \"disk\" in device_map_without_lm_head.values():\n-> 2425               \
    \      raise ValueError(\n   2426                         \"\"\"\n   2427    \
    \                     Some modules are dispatched on the CPU or the disk. Make\
    \ sure you have enough GPU RAM to fit\n\nValueError: \n                      \
    \  Some modules are dispatched on the CPU or the disk. Make sure you have enough\
    \ GPU RAM to fit\n                        the quantized model. If you have set\
    \ a value for `max_memory` you should increase that. To have\n               \
    \         an idea of the modules that are set on the CPU or RAM you can print\
    \ model.hf_device_map.\n                        \n```"
  created_at: 2023-03-04 11:48:49+00:00
  edited: true
  hidden: false
  id: 6403302156038547951de771
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2023-03-04T12:00:19.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
          fullname: Moritz Laurer
          isHf: true
          isPro: false
          name: MoritzLaurer
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-03-04T12:01:24.739Z'
      numEdits: 0
      reactions: []
    id: 640332d3923fe7905861562d
    type: comment
  author: MoritzLaurer
  content: This comment has been hidden
  created_at: 2023-03-04 12:00:19+00:00
  edited: true
  hidden: true
  id: 640332d3923fe7905861562d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2023-03-04T12:00:19.000Z'
    data:
      status: closed
    id: 640332d3923fe7905861562e
    type: status-change
  author: MoritzLaurer
  created_at: 2023-03-04 12:00:19+00:00
  id: 640332d3923fe7905861562e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2023-03-04T12:00:22.000Z'
    data:
      status: open
    id: 640332d666f4a994f1cc75c7
    type: status-change
  author: MoritzLaurer
  created_at: 2023-03-04 12:00:22+00:00
  id: 640332d666f4a994f1cc75c7
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/11d226dcccdbd011d3daafecf1a96afa.svg
      fullname: chan yan hon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miltonc
      type: user
    createdAt: '2023-04-12T03:44:19.000Z'
    data:
      edited: false
      editors:
      - miltonc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/11d226dcccdbd011d3daafecf1a96afa.svg
          fullname: chan yan hon
          isHf: false
          isPro: false
          name: miltonc
          type: user
        html: '<p>same for my deployment in sagemaker using instance instance_type="ml.g4dn.4xlarge".
          Waiting for someone to help on this as well.</p>

          <p>my code:<br>def model_fn(model_dir):<br>    #load model and tokenizer<br>    model
          = T5ForConditionalGeneration.from_pretrained("google/flan-ul2",<br>                                                       load_in_8bit=True,
          device_map="auto", cache_dir="/tmp/model_cache/")<br>    tokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")<br>    return
          model, tokenizer</p>

          <p>from sagemaker.huggingface.model import HuggingFaceModel</p>

          <p>huggingface_model = HuggingFaceModel(<br>    model_data=s3_location,<br>    role=role,<br>    transformers_version="4.17",<br>    pytorch_version="1.10",<br>    py_version=''py38'',<br>)<br>from
          sagemaker.utils import name_from_base</p>

          <p>endpoint_name = name_from_base(model_name)</p>

          <p>predictor = huggingface_model.deploy(<br>    initial_instance_count=1,<br>    #instance_type="ml.g5.4xlarge",<br>    instance_type="ml.g4dn.4xlarge",<br>    endpoint_name=endpoint_name,<br>)<br>data
          = {<br>    "inputs": prompt,<br>    "min_length": 20,<br>    "max_length":
          50,<br>    "do_sample": True,<br>    "temperature": 0.6,<br>}</p>

          <p>res = predictor.predict(data=data)<br>print(res)</p>

          <p>Some modules are dispatched on the CPU or the disk. Make sure you have
          enough GPU RAM to fit\n                        the quantized model. If you
          have set a value for <code>max_memory</code> you should increase that. To
          have\n                        an idea of the modules that are set on the
          CPU or RAM you can print model.hf_device_map</p>

          '
        raw: "same for my deployment in sagemaker using instance instance_type=\"\
          ml.g4dn.4xlarge\". Waiting for someone to help on this as well.\n\nmy code:\
          \ \ndef model_fn(model_dir):\n    #load model and tokenizer\n    model =\
          \ T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\",\n    \
          \                                                   load_in_8bit=True, device_map=\"\
          auto\", cache_dir=\"/tmp/model_cache/\")\n    tokenizer = AutoTokenizer.from_pretrained(\"\
          google/flan-ul2\")\n    return model, tokenizer\n\n\nfrom sagemaker.huggingface.model\
          \ import HuggingFaceModel\n\nhuggingface_model = HuggingFaceModel(\n   \
          \ model_data=s3_location,\n    role=role,\n    transformers_version=\"4.17\"\
          ,\n    pytorch_version=\"1.10\",\n    py_version='py38',\n)\nfrom sagemaker.utils\
          \ import name_from_base\n\nendpoint_name = name_from_base(model_name)\n\n\
          predictor = huggingface_model.deploy(\n    initial_instance_count=1,\n \
          \   #instance_type=\"ml.g5.4xlarge\",\n    instance_type=\"ml.g4dn.4xlarge\"\
          ,\n    endpoint_name=endpoint_name,\n)\ndata = {\n    \"inputs\": prompt,\n\
          \    \"min_length\": 20,\n    \"max_length\": 50,\n    \"do_sample\": True,\n\
          \    \"temperature\": 0.6,\n}\n\nres = predictor.predict(data=data)\nprint(res)\n\
          \nSome modules are dispatched on the CPU or the disk. Make sure you have\
          \ enough GPU RAM to fit\\n                        the quantized model. If\
          \ you have set a value for `max_memory` you should increase that. To have\\\
          n                        an idea of the modules that are set on the CPU\
          \ or RAM you can print model.hf_device_map"
        updatedAt: '2023-04-12T03:44:19.916Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - MoritzLaurer
    id: 6436291379c45fcf1ade43b0
    type: comment
  author: miltonc
  content: "same for my deployment in sagemaker using instance instance_type=\"ml.g4dn.4xlarge\"\
    . Waiting for someone to help on this as well.\n\nmy code: \ndef model_fn(model_dir):\n\
    \    #load model and tokenizer\n    model = T5ForConditionalGeneration.from_pretrained(\"\
    google/flan-ul2\",\n                                                       load_in_8bit=True,\
    \ device_map=\"auto\", cache_dir=\"/tmp/model_cache/\")\n    tokenizer = AutoTokenizer.from_pretrained(\"\
    google/flan-ul2\")\n    return model, tokenizer\n\n\nfrom sagemaker.huggingface.model\
    \ import HuggingFaceModel\n\nhuggingface_model = HuggingFaceModel(\n    model_data=s3_location,\n\
    \    role=role,\n    transformers_version=\"4.17\",\n    pytorch_version=\"1.10\"\
    ,\n    py_version='py38',\n)\nfrom sagemaker.utils import name_from_base\n\nendpoint_name\
    \ = name_from_base(model_name)\n\npredictor = huggingface_model.deploy(\n    initial_instance_count=1,\n\
    \    #instance_type=\"ml.g5.4xlarge\",\n    instance_type=\"ml.g4dn.4xlarge\"\
    ,\n    endpoint_name=endpoint_name,\n)\ndata = {\n    \"inputs\": prompt,\n  \
    \  \"min_length\": 20,\n    \"max_length\": 50,\n    \"do_sample\": True,\n  \
    \  \"temperature\": 0.6,\n}\n\nres = predictor.predict(data=data)\nprint(res)\n\
    \nSome modules are dispatched on the CPU or the disk. Make sure you have enough\
    \ GPU RAM to fit\\n                        the quantized model. If you have set\
    \ a value for `max_memory` you should increase that. To have\\n              \
    \          an idea of the modules that are set on the CPU or RAM you can print\
    \ model.hf_device_map"
  created_at: 2023-04-12 02:44:19+00:00
  edited: false
  hidden: false
  id: 6436291379c45fcf1ade43b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-04-12T17:54:02.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;MoritzLaurer&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MoritzLaurer\"\
          >@<span class=\"underline\">MoritzLaurer</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;miltonc&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/miltonc\">@<span class=\"\
          underline\">miltonc</span></a></span>\n\n\t</span></span><br>As stated by\
          \ the error trace it seems that you don't have enough memory to fit the\
          \ model on a 15GB GPU. The model is a 20B parameters model so you would\
          \ need roughly 20GB GPU RAM at least, to run the model in int8<br>However,\
          \ you might be interested in dispatching the model between CPU and GPU,\
          \ and fit ~70% of the model weights on the GPU and the rest on CPU using\
          \ <code>BitsAndBytesConfig</code>. Please have a look at the following section:\
          \ <a href=\"https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\"\
          >https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu</a></p>\n"
        raw: "Hi @MoritzLaurer @miltonc \nAs stated by the error trace it seems that\
          \ you don't have enough memory to fit the model on a 15GB GPU. The model\
          \ is a 20B parameters model so you would need roughly 20GB GPU RAM at least,\
          \ to run the model in int8\nHowever, you might be interested in dispatching\
          \ the model between CPU and GPU, and fit ~70% of the model weights on the\
          \ GPU and the rest on CPU using `BitsAndBytesConfig`. Please have a look\
          \ at the following section: https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"
        updatedAt: '2023-04-12T17:54:02.014Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - nhung
    id: 6436f03af8962b4332ba2644
    type: comment
  author: ybelkada
  content: "Hi @MoritzLaurer @miltonc \nAs stated by the error trace it seems that\
    \ you don't have enough memory to fit the model on a 15GB GPU. The model is a\
    \ 20B parameters model so you would need roughly 20GB GPU RAM at least, to run\
    \ the model in int8\nHowever, you might be interested in dispatching the model\
    \ between CPU and GPU, and fit ~70% of the model weights on the GPU and the rest\
    \ on CPU using `BitsAndBytesConfig`. Please have a look at the following section:\
    \ https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu"
  created_at: 2023-04-12 16:54:02+00:00
  edited: false
  hidden: false
  id: 6436f03af8962b4332ba2644
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
      fullname: Moritz Laurer
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MoritzLaurer
      type: user
    createdAt: '2023-04-13T08:00:11.000Z'
    data:
      edited: false
      editors:
      - MoritzLaurer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1613511937628-5fb15d1e84389b139cf3b508.jpeg?w=200&h=200&f=face
          fullname: Moritz Laurer
          isHf: true
          isPro: false
          name: MoritzLaurer
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>,\
          \ yeah just increasing memory makes sense. What confused me is that it worked\
          \ with bf16, but the same setup did not work with int8. I thought bf16 requires\
          \ more memory than 8int. If that's the case, then I don't really understand\
          \ why I got the error with int8, especially since a central motivation for\
          \ using int8 is to decrease memory requirements. But maybe I misunderstand\
          \ something</p>\n"
        raw: Hi @ybelkada, yeah just increasing memory makes sense. What confused
          me is that it worked with bf16, but the same setup did not work with int8.
          I thought bf16 requires more memory than 8int. If that's the case, then
          I don't really understand why I got the error with int8, especially since
          a central motivation for using int8 is to decrease memory requirements.
          But maybe I misunderstand something
        updatedAt: '2023-04-13T08:00:11.655Z'
      numEdits: 0
      reactions: []
    id: 6437b68bf6a7e4778bb060d9
    type: comment
  author: MoritzLaurer
  content: Hi @ybelkada, yeah just increasing memory makes sense. What confused me
    is that it worked with bf16, but the same setup did not work with int8. I thought
    bf16 requires more memory than 8int. If that's the case, then I don't really understand
    why I got the error with int8, especially since a central motivation for using
    int8 is to decrease memory requirements. But maybe I misunderstand something
  created_at: 2023-04-13 07:00:11+00:00
  edited: false
  hidden: false
  id: 6437b68bf6a7e4778bb060d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e9b329c3241bc478df6d3f164c52632f.svg
      fullname: Jackson Cozzi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fredbananas
      type: user
    createdAt: '2023-10-19T14:30:59.000Z'
    data:
      edited: true
      editors:
      - fredbananas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9841902852058411
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e9b329c3241bc478df6d3f164c52632f.svg
          fullname: Jackson Cozzi
          isHf: false
          isPro: false
          name: fredbananas
          type: user
        html: "<p>Hey, <span data-props=\"{&quot;user&quot;:&quot;MoritzLaurer&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MoritzLaurer\"\
          >@<span class=\"underline\">MoritzLaurer</span></a></span>\n\n\t</span></span>\
          \ were you ever able to figure this out? I am having a similar problem in\
          \ the Google Colab workspace on a T4 GPU which has 16Gb of memory, but I\
          \ am loading my fine tuned Llama 2 7B hf model which should in theory work\
          \ but I run into the same error. Is it really as simple as I need more memory?\
          \ I would really like to remain in the free tier of google colab if at all\
          \ possible.</p>\n<p>EDIT: To clarify, I am even using the 4 bit quantization\
          \ using BitsAndBytes<br>MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"</p>\n\
          <p>bnb_config = BitsAndBytesConfig(<br>    load_in_4bit=True,<br>    bnb_4bit_use_double_quant=True,<br>\
          \    bnb_4bit_quant_type=\"nf4\",<br>    bnb_4bit_compute_dtype=torch.bfloat16<br>)</p>\n\
          <p>model = AutoModelForCausalLM.from_pretrained(<br>    MODEL_NAME,<br>\
          \    device_map=\"auto\",<br>    trust_remote_code=True,<br>    quantization_config=bnb_config<br>)</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)<br>tokenizer.pad_token\
          \ = tokenizer.eos_token</p>\n<p>training the model works fine but then when\
          \ I load the trained model I encounter the error.</p>\n"
        raw: "Hey, @MoritzLaurer were you ever able to figure this out? I am having\
          \ a similar problem in the Google Colab workspace on a T4 GPU which has\
          \ 16Gb of memory, but I am loading my fine tuned Llama 2 7B hf model which\
          \ should in theory work but I run into the same error. Is it really as simple\
          \ as I need more memory? I would really like to remain in the free tier\
          \ of google colab if at all possible.\n\nEDIT: To clarify, I am even using\
          \ the 4 bit quantization using BitsAndBytes\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\
          \n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n\
          \    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
          )\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n  \
          \  device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=bnb_config\n\
          )\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token\
          \ = tokenizer.eos_token\n\ntraining the model works fine but then when I\
          \ load the trained model I encounter the error."
        updatedAt: '2023-10-19T14:33:46.998Z'
      numEdits: 1
      reactions: []
    id: 65313da3e753fe12e96ea3a2
    type: comment
  author: fredbananas
  content: "Hey, @MoritzLaurer were you ever able to figure this out? I am having\
    \ a similar problem in the Google Colab workspace on a T4 GPU which has 16Gb of\
    \ memory, but I am loading my fine tuned Llama 2 7B hf model which should in theory\
    \ work but I run into the same error. Is it really as simple as I need more memory?\
    \ I would really like to remain in the free tier of google colab if at all possible.\n\
    \nEDIT: To clarify, I am even using the 4 bit quantization using BitsAndBytes\n\
    MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n\nbnb_config = BitsAndBytesConfig(\n\
    \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
    nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n\
    \    MODEL_NAME,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=bnb_config\n\
    )\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token\
    \ = tokenizer.eos_token\n\ntraining the model works fine but then when I load\
    \ the trained model I encounter the error."
  created_at: 2023-10-19 13:30:59+00:00
  edited: true
  hidden: false
  id: 65313da3e753fe12e96ea3a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/328124d13b7a5a74c90b8c26e483a9d0.svg
      fullname: Chena Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chenaclee
      type: user
    createdAt: '2023-10-22T08:28:57.000Z'
    data:
      edited: true
      editors:
      - chenaclee
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9789621829986572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/328124d13b7a5a74c90b8c26e483a9d0.svg
          fullname: Chena Lee
          isHf: false
          isPro: false
          name: chenaclee
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;fredbananas&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/fredbananas\"\
          >@<span class=\"underline\">fredbananas</span></a></span>\n\n\t</span></span><br>I\
          \ ran into the same error despite having aws g5.4xlarge instance. (using\
          \ a different model) For me it was because nvidia runtime wasn't up. If\
          \ you think that might be your issue too, try nvidia-smi and see if it's\
          \ working</p>\n"
        raw: "@fredbananas \nI ran into the same error despite having aws g5.4xlarge\
          \ instance. (using a different model) For me it was because nvidia runtime\
          \ wasn't up. If you think that might be your issue too, try nvidia-smi and\
          \ see if it's working"
        updatedAt: '2023-10-22T08:29:27.636Z'
      numEdits: 1
      reactions: []
    id: 6534dd49484d775cb0d363be
    type: comment
  author: chenaclee
  content: "@fredbananas \nI ran into the same error despite having aws g5.4xlarge\
    \ instance. (using a different model) For me it was because nvidia runtime wasn't\
    \ up. If you think that might be your issue too, try nvidia-smi and see if it's\
    \ working"
  created_at: 2023-10-22 07:28:57+00:00
  edited: true
  hidden: false
  id: 6534dd49484d775cb0d363be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-23T11:15:13.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7867410182952881
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;chenaclee&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/chenaclee\"\
          >@<span class=\"underline\">chenaclee</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;fredbananas&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/fredbananas\">@<span\
          \ class=\"underline\">fredbananas</span></a></span>\n\n\t</span></span><br>In\
          \ order to load the model on a free tier Gcolab instance, I recommend you\
          \ to use the sharded version of the model instead such as: <code>Trelis/Llama-2-7b-chat-hf-sharded-bf16\
          \ </code><br>You can find more sharded checkpoints on my personal collection:\
          \ <a href=\"https://huggingface.co/collections/ybelkada/sharded-checkpoints-64fefe78cccea7ce7b268310\"\
          >https://huggingface.co/collections/ybelkada/sharded-checkpoints-64fefe78cccea7ce7b268310</a></p>\n"
        raw: "Hi @chenaclee @fredbananas \nIn order to load the model on a free tier\
          \ Gcolab instance, I recommend you to use the sharded version of the model\
          \ instead such as: `Trelis/Llama-2-7b-chat-hf-sharded-bf16 `\nYou can find\
          \ more sharded checkpoints on my personal collection: https://huggingface.co/collections/ybelkada/sharded-checkpoints-64fefe78cccea7ce7b268310"
        updatedAt: '2023-10-23T11:15:13.676Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - fredbananas
    id: 653655c1ed74ace633d8cdef
    type: comment
  author: ybelkada
  content: "Hi @chenaclee @fredbananas \nIn order to load the model on a free tier\
    \ Gcolab instance, I recommend you to use the sharded version of the model instead\
    \ such as: `Trelis/Llama-2-7b-chat-hf-sharded-bf16 `\nYou can find more sharded\
    \ checkpoints on my personal collection: https://huggingface.co/collections/ybelkada/sharded-checkpoints-64fefe78cccea7ce7b268310"
  created_at: 2023-10-23 10:15:13+00:00
  edited: false
  hidden: false
  id: 653655c1ed74ace633d8cdef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f9944d4cca3cc32854e87232717989e.svg
      fullname: sari
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: egsari
      type: user
    createdAt: '2023-11-08T18:31:22.000Z'
    data:
      edited: false
      editors:
      - egsari
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9908076524734497
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f9944d4cca3cc32854e87232717989e.svg
          fullname: sari
          isHf: false
          isPro: false
          name: egsari
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;fredbananas&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/fredbananas\"\
          >@<span class=\"underline\">fredbananas</span></a></span>\n\n\t</span></span>\
          \ were you able to fix the error?</p>\n"
        raw: '@fredbananas were you able to fix the error?'
        updatedAt: '2023-11-08T18:31:22.393Z'
      numEdits: 0
      reactions: []
    id: 654bd3fae1671abcbc2bb20c
    type: comment
  author: egsari
  content: '@fredbananas were you able to fix the error?'
  created_at: 2023-11-08 18:31:22+00:00
  edited: false
  hidden: false
  id: 654bd3fae1671abcbc2bb20c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce725f0fb91bc7be4251614f74edf0b6.svg
      fullname: Anees Aslam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Anees-Aslam
      type: user
    createdAt: '2024-01-17T10:58:39.000Z'
    data:
      edited: false
      editors:
      - Anees-Aslam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9039876461029053
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce725f0fb91bc7be4251614f74edf0b6.svg
          fullname: Anees Aslam
          isHf: false
          isPro: false
          name: Anees-Aslam
          type: user
        html: '<p>Free Up GPU :)<br>Guys who are trying in Google Colab, you can restart
          session.<br>Then the GPU Memory is cleared for your use.</p>

          '
        raw: 'Free Up GPU :)

          Guys who are trying in Google Colab, you can restart session.

          Then the GPU Memory is cleared for your use.

          '
        updatedAt: '2024-01-17T10:58:39.499Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Anees-Aslam
    id: 65a7b2df3212568defad9679
    type: comment
  author: Anees-Aslam
  content: 'Free Up GPU :)

    Guys who are trying in Google Colab, you can restart session.

    Then the GPU Memory is cleared for your use.

    '
  created_at: 2024-01-17 10:58:39+00:00
  edited: false
  hidden: false
  id: 65a7b2df3212568defad9679
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: google/flan-ul2
repo_type: model
status: open
target_branch: null
title: 'ValueError:  Some modules are dispatched on the CPU or the disk. Make sure
  you have enough GPU RAM'
