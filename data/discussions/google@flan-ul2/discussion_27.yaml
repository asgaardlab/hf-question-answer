!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nelsonspbr
conflicting_files: null
created_at: 2023-06-27 11:38:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678398380100-noauth.png?w=200&h=200&f=face
      fullname: Nelson Gonzalez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nelsonspbr
      type: user
    createdAt: '2023-06-27T12:38:04.000Z'
    data:
      edited: false
      editors:
      - nelsonspbr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8338342308998108
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1678398380100-noauth.png?w=200&h=200&f=face
          fullname: Nelson Gonzalez
          isHf: false
          isPro: false
          name: nelsonspbr
          type: user
        html: '<p>I am running inference following <a href="https://huggingface.co/google/flan-ul2#running-the-model">https://huggingface.co/google/flan-ul2#running-the-model</a>.
          I tested both INT8 <code>load_in_8bit</code> and BF16 <code>torch_dtype=torch.bfloat16
          </code> methods. After running some experiments, INT8 is ~3x slower than
          BF16. For reference, these are the most executed kernels for INT8:</p>

          <pre><code>ampere_int32_i16832gemm_int8_256x128_ldg16_stages_64x3_nt

          ampere_sgemm_128x32_tn

          ampere_int32_i16832gemm_int8_128x128_ldg16_stages_64x3_nt

          </code></pre>

          <p>Is this "INT8" actually mixed precision? Would that start to explain
          why it is worse?</p>

          '
        raw: "I am running inference following https://huggingface.co/google/flan-ul2#running-the-model.\
          \ I tested both INT8 `load_in_8bit` and BF16 `torch_dtype=torch.bfloat16\
          \ ` methods. After running some experiments, INT8 is ~3x slower than BF16.\
          \ For reference, these are the most executed kernels for INT8:\r\n\r\n```\r\
          \nampere_int32_i16832gemm_int8_256x128_ldg16_stages_64x3_nt\r\nampere_sgemm_128x32_tn\r\
          \nampere_int32_i16832gemm_int8_128x128_ldg16_stages_64x3_nt\r\n```\r\n\r\
          \nIs this \"INT8\" actually mixed precision? Would that start to explain\
          \ why it is worse?"
        updatedAt: '2023-06-27T12:38:04.330Z'
      numEdits: 0
      reactions: []
    id: 649ad82cb9368db7f5b5d226
    type: comment
  author: nelsonspbr
  content: "I am running inference following https://huggingface.co/google/flan-ul2#running-the-model.\
    \ I tested both INT8 `load_in_8bit` and BF16 `torch_dtype=torch.bfloat16 ` methods.\
    \ After running some experiments, INT8 is ~3x slower than BF16. For reference,\
    \ these are the most executed kernels for INT8:\r\n\r\n```\r\nampere_int32_i16832gemm_int8_256x128_ldg16_stages_64x3_nt\r\
    \nampere_sgemm_128x32_tn\r\nampere_int32_i16832gemm_int8_128x128_ldg16_stages_64x3_nt\r\
    \n```\r\n\r\nIs this \"INT8\" actually mixed precision? Would that start to explain\
    \ why it is worse?"
  created_at: 2023-06-27 11:38:04+00:00
  edited: false
  hidden: false
  id: 649ad82cb9368db7f5b5d226
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 27
repo_id: google/flan-ul2
repo_type: model
status: open
target_branch: null
title: FLAN-UL2 performance INT8 worse than BF16
