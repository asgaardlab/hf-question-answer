!!python/object:huggingface_hub.community.DiscussionWithDetails
author: clang-kodex
conflicting_files: null
created_at: 2023-05-02 12:57:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9cc49426ca3c622de41df81a65211d67.svg
      fullname: Claus Lang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clang-kodex
      type: user
    createdAt: '2023-05-02T13:57:43.000Z'
    data:
      edited: false
      editors:
      - clang-kodex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9cc49426ca3c622de41df81a65211d67.svg
          fullname: Claus Lang
          isHf: false
          isPro: false
          name: clang-kodex
          type: user
        html: '<p>I tried deploying this as inference endpoint on a large GPU (4 x
          Nvidia Tesla T4). For any smaller instance I get <code>Warning: Model may
          be too large for the selected Instance Size.</code>. However on the large
          GPU, the build fails with this error message:</p>

          <p><code>Endpoint error: Endpoint failed to start, reason: Endpoint failed.
          Check logs or documentation more for more information</code></p>

          <p>And from the logs:</p>

          <p><code>bp6wn 2023-05-02T13:32:59.547Z torch.cuda.OutOfMemoryError: CUDA
          out of memory. Tried to allocate 256.00 MiB (GPU 0; 14.76 GiB total capacity;
          14.24 GiB already allocated; 23.75 MiB free; 14.24 GiB reserved in total
          by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting
          max_split_size_mb to avoid fragmentation. See documentation for Memory Management
          and PYTORCH_CUDA_ALLOC_CONF</code></p>

          <p>Does this mean I need a larger instance (Nvidia A100)?</p>

          '
        raw: "I tried deploying this as inference endpoint on a large GPU (4 x Nvidia\
          \ Tesla T4). For any smaller instance I get `Warning: Model may be too large\
          \ for the selected Instance Size.`. However on the large GPU, the build\
          \ fails with this error message:\r\n\r\n```Endpoint error: Endpoint failed\
          \ to start, reason: Endpoint failed. Check logs or documentation more for\
          \ more information```\r\n\r\nAnd from the logs:\r\n\r\n```bp6wn 2023-05-02T13:32:59.547Z\
          \ torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00\
          \ MiB (GPU 0; 14.76 GiB total capacity; 14.24 GiB already allocated; 23.75\
          \ MiB free; 14.24 GiB reserved in total by PyTorch) If reserved memory is\
          \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \ See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF```\r\
          \n\r\nDoes this mean I need a larger instance (Nvidia A100)?"
        updatedAt: '2023-05-02T13:57:43.600Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rragundez
    id: 645116d73eb5502631d148f9
    type: comment
  author: clang-kodex
  content: "I tried deploying this as inference endpoint on a large GPU (4 x Nvidia\
    \ Tesla T4). For any smaller instance I get `Warning: Model may be too large for\
    \ the selected Instance Size.`. However on the large GPU, the build fails with\
    \ this error message:\r\n\r\n```Endpoint error: Endpoint failed to start, reason:\
    \ Endpoint failed. Check logs or documentation more for more information```\r\n\
    \r\nAnd from the logs:\r\n\r\n```bp6wn 2023-05-02T13:32:59.547Z torch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 14.76 GiB total capacity;\
    \ 14.24 GiB already allocated; 23.75 MiB free; 14.24 GiB reserved in total by\
    \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF```\r\
    \n\r\nDoes this mean I need a larger instance (Nvidia A100)?"
  created_at: 2023-05-02 12:57:43+00:00
  edited: false
  hidden: false
  id: 645116d73eb5502631d148f9
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 18
repo_id: google/flan-ul2
repo_type: model
status: open
target_branch: null
title: 'What instance type do I need to deploy this as Inference Endpoint? '
