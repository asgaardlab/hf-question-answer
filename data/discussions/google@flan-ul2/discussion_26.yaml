!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ahatsham
conflicting_files: null
created_at: 2023-06-23 22:10:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/100ad3d63b1cbd10ab71814d76e18042.svg
      fullname: Hayat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ahatsham
      type: user
    createdAt: '2023-06-23T23:10:15.000Z'
    data:
      edited: false
      editors:
      - Ahatsham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.908632218837738
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/100ad3d63b1cbd10ab71814d76e18042.svg
          fullname: Hayat
          isHf: false
          isPro: false
          name: Ahatsham
          type: user
        html: '<p>Hi, I am trying to fine-tune the Flan-UL2 model on my dataset. I
          have two 32-GB GPU''s but whenever I am tyring to run my model. It shows,
          </p>

          <p>"RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU
          0; 31.74 GiB total capacity; 30.74 GiB already allocated; 136.88 MiB free;
          30.74 GiB reserved in total by PyTorch)."</p>

          <p>Can anybody help me with that?</p>

          '
        raw: "Hi, I am trying to fine-tune the Flan-UL2 model on my dataset. I have\
          \ two 32-GB GPU's but whenever I am tyring to run my model. It shows, \r\
          \n\r\n\"RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU\
          \ 0; 31.74 GiB total capacity; 30.74 GiB already allocated; 136.88 MiB free;\
          \ 30.74 GiB reserved in total by PyTorch).\"\r\n\r\nCan anybody help me\
          \ with that?"
        updatedAt: '2023-06-23T23:10:15.591Z'
      numEdits: 0
      reactions: []
    id: 6496265735406086330a63bc
    type: comment
  author: Ahatsham
  content: "Hi, I am trying to fine-tune the Flan-UL2 model on my dataset. I have\
    \ two 32-GB GPU's but whenever I am tyring to run my model. It shows, \r\n\r\n\
    \"RuntimeError: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 31.74\
    \ GiB total capacity; 30.74 GiB already allocated; 136.88 MiB free; 30.74 GiB\
    \ reserved in total by PyTorch).\"\r\n\r\nCan anybody help me with that?"
  created_at: 2023-06-23 22:10:15+00:00
  edited: false
  hidden: false
  id: 6496265735406086330a63bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c71878c87bce36d3193ea9e13ad23722.svg
      fullname: Venkata Bhanu Teja Pallakonda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pvbhanuteja
      type: user
    createdAt: '2023-07-08T02:46:58.000Z'
    data:
      edited: false
      editors:
      - pvbhanuteja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9655349850654602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c71878c87bce36d3193ea9e13ad23722.svg
          fullname: Venkata Bhanu Teja Pallakonda
          isHf: false
          isPro: false
          name: pvbhanuteja
          type: user
        html: '<p>Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).
          For batched training you need clusters. And maybe 64gb might suffice if
          you load model into both GPUs for inference I think you are trying to load
          whole model on device 0</p>

          '
        raw: Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).
          For batched training you need clusters. And maybe 64gb might suffice if
          you load model into both GPUs for inference I think you are trying to load
          whole model on device 0
        updatedAt: '2023-07-08T02:46:58.755Z'
      numEdits: 0
      reactions: []
    id: 64a8ce228a1a9187c2db76ee
    type: comment
  author: pvbhanuteja
  content: Flan-UL2 requires min GPU size of 50GB (that to maybe for inference). For
    batched training you need clusters. And maybe 64gb might suffice if you load model
    into both GPUs for inference I think you are trying to load whole model on device
    0
  created_at: 2023-07-08 01:46:58+00:00
  edited: false
  hidden: false
  id: 64a8ce228a1a9187c2db76ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/100ad3d63b1cbd10ab71814d76e18042.svg
      fullname: Hayat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ahatsham
      type: user
    createdAt: '2023-07-10T16:17:11.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/100ad3d63b1cbd10ab71814d76e18042.svg
          fullname: Hayat
          isHf: false
          isPro: false
          name: Ahatsham
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-07-10T16:17:55.658Z'
      numEdits: 0
      reactions: []
    id: 64ac2f07af539898ef3df92f
    type: comment
  author: Ahatsham
  content: This comment has been hidden
  created_at: 2023-07-10 15:17:11+00:00
  edited: true
  hidden: true
  id: 64ac2f07af539898ef3df92f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/100ad3d63b1cbd10ab71814d76e18042.svg
      fullname: Hayat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ahatsham
      type: user
    createdAt: '2023-07-10T16:17:40.000Z'
    data:
      edited: false
      editors:
      - Ahatsham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9665354490280151
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/100ad3d63b1cbd10ab71814d76e18042.svg
          fullname: Hayat
          isHf: false
          isPro: false
          name: Ahatsham
          type: user
        html: '<blockquote>

          <p>Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).
          For batched training you need clusters. And maybe 64gb might suffice if
          you load model into both GPUs for inference I think you are trying to load
          whole model on device 0</p>

          </blockquote>

          <p>Yes, you are right, I am new in this area, can you help me with how to
          use multiple GPU''s?</p>

          '
        raw: '> Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).
          For batched training you need clusters. And maybe 64gb might suffice if
          you load model into both GPUs for inference I think you are trying to load
          whole model on device 0


          Yes, you are right, I am new in this area, can you help me with how to use
          multiple GPU''s?


          '
        updatedAt: '2023-07-10T16:17:40.609Z'
      numEdits: 0
      reactions: []
    id: 64ac2f243215a18926f40da9
    type: comment
  author: Ahatsham
  content: '> Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).
    For batched training you need clusters. And maybe 64gb might suffice if you load
    model into both GPUs for inference I think you are trying to load whole model
    on device 0


    Yes, you are right, I am new in this area, can you help me with how to use multiple
    GPU''s?


    '
  created_at: 2023-07-10 15:17:40+00:00
  edited: false
  hidden: false
  id: 64ac2f243215a18926f40da9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c71878c87bce36d3193ea9e13ad23722.svg
      fullname: Venkata Bhanu Teja Pallakonda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pvbhanuteja
      type: user
    createdAt: '2023-07-10T23:28:05.000Z'
    data:
      edited: false
      editors:
      - pvbhanuteja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8965513110160828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c71878c87bce36d3193ea9e13ad23722.svg
          fullname: Venkata Bhanu Teja Pallakonda
          isHf: false
          isPro: false
          name: pvbhanuteja
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).
          For batched training you need clusters. And maybe 64gb might suffice if
          you load model into both GPUs for inference I think you are trying to load
          whole model on device 0</p>

          </blockquote>

          <p>Yes, you are right, I am new in this area, can you help me with how to
          use multiple GPU''s?</p>

          </blockquote>

          <p>Adding this parameter device_map="auto" while loading the pipeline should
          do the work <a href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device_map">https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device_map</a></p>

          '
        raw: "> > Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).\
          \ For batched training you need clusters. And maybe 64gb might suffice if\
          \ you load model into both GPUs for inference I think you are trying to\
          \ load whole model on device 0\n> \n> Yes, you are right, I am new in this\
          \ area, can you help me with how to use multiple GPU's?\n\nAdding this parameter\
          \ device_map=\"auto\" while loading the pipeline should do the work https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device_map"
        updatedAt: '2023-07-10T23:28:05.540Z'
      numEdits: 0
      reactions: []
    id: 64ac940567c1cee8dfdad92a
    type: comment
  author: pvbhanuteja
  content: "> > Flan-UL2 requires min GPU size of 50GB (that to maybe for inference).\
    \ For batched training you need clusters. And maybe 64gb might suffice if you\
    \ load model into both GPUs for inference I think you are trying to load whole\
    \ model on device 0\n> \n> Yes, you are right, I am new in this area, can you\
    \ help me with how to use multiple GPU's?\n\nAdding this parameter device_map=\"\
    auto\" while loading the pipeline should do the work https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device_map"
  created_at: 2023-07-10 22:28:05+00:00
  edited: false
  hidden: false
  id: 64ac940567c1cee8dfdad92a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 26
repo_id: google/flan-ul2
repo_type: model
status: open
target_branch: null
title: Error in Fine Tunning Flan-UL2 model
