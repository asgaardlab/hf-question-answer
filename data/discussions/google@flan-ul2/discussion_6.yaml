!!python/object:huggingface_hub.community.DiscussionWithDetails
author: will33am
conflicting_files: null
created_at: 2023-03-03 17:08:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656275265222-62503f03815d0fd28f847c19.jpeg?w=200&h=200&f=face
      fullname: William Berrios
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: will33am
      type: user
    createdAt: '2023-03-03T17:08:47.000Z'
    data:
      edited: false
      editors:
      - will33am
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1656275265222-62503f03815d0fd28f847c19.jpeg?w=200&h=200&f=face
          fullname: William Berrios
          isHf: false
          isPro: false
          name: will33am
          type: user
        html: "<p>I am trying to run the example code in a multi-gpu setting but it's\
          \ failing :( </p>\n<pre><code>from transformers import T5ForConditionalGeneration,\
          \ AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          google/flan-ul2\", device_map=\"auto\", load_in_8bit=True)             \
          \                                                    \ntokenizer = AutoTokenizer.from_pretrained(\"\
          google/flan-ul2\")\n\ninput_string = \"Answer the following question by\
          \ reasoning step by step. The cafeteria had 23 apples. If they used 20 for\
          \ lunch, and bought 6 more, how many apple do they have?\"             \
          \                                  \n\ninputs = tokenizer(input_string,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\noutputs = model.generate(inputs,\
          \ max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n</code></pre>\n\
          <p>Output:</p>\n<pre><code>RuntimeError: Expected all tensors to be on the\
          \ same device, but found at least two devices, cuda:0 and cuda:1! (when\
          \ checking argument for argument mat2 in method wrapper_mm)\n</code></pre>\n"
        raw: "I am trying to run the example code in a multi-gpu setting but it's\
          \ failing :( \r\n\r\n```\r\nfrom transformers import T5ForConditionalGeneration,\
          \ AutoTokenizer\r\nimport torch\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          google/flan-ul2\", device_map=\"auto\", load_in_8bit=True)             \
          \                                                    \r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          google/flan-ul2\")\r\n\r\ninput_string = \"Answer the following question\
          \ by reasoning step by step. The cafeteria had 23 apples. If they used 20\
          \ for lunch, and bought 6 more, how many apple do they have?\"         \
          \                                      \r\n\r\ninputs = tokenizer(input_string,\
          \ return_tensors=\"pt\").input_ids.to(\"cuda\")\r\noutputs = model.generate(inputs,\
          \ max_length=200)\r\n\r\nprint(tokenizer.decode(outputs[0]))\r\n\r\n```\r\
          \nOutput:\r\n\r\n```\r\nRuntimeError: Expected all tensors to be on the\
          \ same device, but found at least two devices, cuda:0 and cuda:1! (when\
          \ checking argument for argument mat2 in method wrapper_mm)\r\n```\r\n"
        updatedAt: '2023-03-03T17:08:48.000Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - WHL95
    id: 6402299f82bbdfe4b7c5d21d
    type: comment
  author: will33am
  content: "I am trying to run the example code in a multi-gpu setting but it's failing\
    \ :( \r\n\r\n```\r\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\r\
    \nimport torch\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\"\
    , device_map=\"auto\", load_in_8bit=True)                                    \
    \                             \r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    google/flan-ul2\")\r\n\r\ninput_string = \"Answer the following question by reasoning\
    \ step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought\
    \ 6 more, how many apple do they have?\"                                     \
    \          \r\n\r\ninputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"\
    cuda\")\r\noutputs = model.generate(inputs, max_length=200)\r\n\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n\r\n```\r\nOutput:\r\n\r\n```\r\nRuntimeError: Expected all tensors to be on\
    \ the same device, but found at least two devices, cuda:0 and cuda:1! (when checking\
    \ argument for argument mat2 in method wrapper_mm)\r\n```\r\n"
  created_at: 2023-03-03 17:08:47+00:00
  edited: false
  hidden: false
  id: 6402299f82bbdfe4b7c5d21d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-03-03T17:56:10.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hm very weird, can you try to use the latest versions of <code>transformers</code>\
          \ &amp;  <code>accelerate</code> ? </p>\n<pre><code class=\"language-bash\"\
          >pip install --upgrade accelerate\npip install --upgrade git+https://github.com/huggingface/transformers.git@\xF9\
          ain\n</code></pre>\n"
        raw: "Hm very weird, can you try to use the latest versions of `transformers`\
          \ &  `accelerate` ? \n```bash\npip install --upgrade accelerate\npip install\
          \ --upgrade git+https://github.com/huggingface/transformers.git@\xF9ain\n\
          ```"
        updatedAt: '2023-03-03T17:56:10.452Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - WHL95
    id: 640234bad7b02faed4ed37b2
    type: comment
  author: ybelkada
  content: "Hm very weird, can you try to use the latest versions of `transformers`\
    \ &  `accelerate` ? \n```bash\npip install --upgrade accelerate\npip install --upgrade\
    \ git+https://github.com/huggingface/transformers.git@\xF9ain\n```"
  created_at: 2023-03-03 17:56:10+00:00
  edited: false
  hidden: false
  id: 640234bad7b02faed4ed37b2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-04T04:56:02.000Z'
    data:
      edited: true
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;will33am&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/will33am\">@<span class=\"\
          underline\">will33am</span></a></span>\n\n\t</span></span> You need to play\
          \ around with <code>device_map</code> since UL2 includes T5 blocks with\
          \ residual connections which causes an error the blocks to be split across\
          \ multiple GPUs (Ref: <a rel=\"nofollow\" href=\"https://github.com/huggingface/blog/blob/main/accelerate-large-models.md\"\
          >https://github.com/huggingface/blog/blob/main/accelerate-large-models.md</a>).<br>Use\
          \ <code>no_split_module_classes=[\"T5Block\"]</code> and also map the <code>lm_head</code>\
          \ to the same device as the embedding layer. </p>\n<p>Here's an example\
          \ script that works on my env with 3 low memory (16GB) GPUs.<br><a rel=\"\
          nofollow\" href=\"https://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py\"\
          >https://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py</a></p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span>  This is the\
          \ same issue as what we encountered the other day when you were working\
          \ on fixing multi-gpu settings for BLIP-2 :)<br><a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/pull/21707\">https://github.com/huggingface/transformers/pull/21707</a></p>\n\
          <p>EDIT: Fixed some grammatical mistakes.</p>\n"
        raw: "@will33am You need to play around with `device_map` since UL2 includes\
          \ T5 blocks with residual connections which causes an error the blocks to\
          \ be split across multiple GPUs (Ref: https://github.com/huggingface/blog/blob/main/accelerate-large-models.md).\n\
          Use `no_split_module_classes=[\"T5Block\"]` and also map the `lm_head` to\
          \ the same device as the embedding layer. \n\nHere's an example script that\
          \ works on my env with 3 low memory (16GB) GPUs.\nhttps://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py\n\
          \n@ybelkada  This is the same issue as what we encountered the other day\
          \ when you were working on fixing multi-gpu settings for BLIP-2 :)\nhttps://github.com/huggingface/transformers/pull/21707\n\
          \nEDIT: Fixed some grammatical mistakes."
        updatedAt: '2023-03-04T13:14:29.055Z'
      numEdits: 6
      reactions: []
    id: 6402cf6217f1f2c384b1c123
    type: comment
  author: akkikiki
  content: "@will33am You need to play around with `device_map` since UL2 includes\
    \ T5 blocks with residual connections which causes an error the blocks to be split\
    \ across multiple GPUs (Ref: https://github.com/huggingface/blog/blob/main/accelerate-large-models.md).\n\
    Use `no_split_module_classes=[\"T5Block\"]` and also map the `lm_head` to the\
    \ same device as the embedding layer. \n\nHere's an example script that works\
    \ on my env with 3 low memory (16GB) GPUs.\nhttps://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py\n\
    \n@ybelkada  This is the same issue as what we encountered the other day when\
    \ you were working on fixing multi-gpu settings for BLIP-2 :)\nhttps://github.com/huggingface/transformers/pull/21707\n\
    \nEDIT: Fixed some grammatical mistakes."
  created_at: 2023-03-04 04:56:02+00:00
  edited: true
  hidden: false
  id: 6402cf6217f1f2c384b1c123
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
      fullname: Samuel Azran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelAzran
      type: user
    createdAt: '2023-03-04T08:27:16.000Z'
    data:
      edited: false
      editors:
      - SamuelAzran
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
          fullname: Samuel Azran
          isHf: false
          isPro: false
          name: SamuelAzran
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span><br>When I try\
          \ your script load_flan_ul2.py on a single 16GB GPU I get this error:</p>\n\
          <p>ValueError: If you want to offload some keys to <code>cpu</code> or <code>disk</code>,\
          \ you need to set <code>load_in_8bit_fp32_cpu_offload=True</code>. Note\
          \ that these modules will not be  converted to 8-bit but kept in 32-bit.</p>\n"
        raw: '@akkikiki

          When I try your script load_flan_ul2.py on a single 16GB GPU I get this
          error:


          ValueError: If you want to offload some keys to `cpu` or `disk`, you need
          to set `load_in_8bit_fp32_cpu_offload=True`. Note that these modules will
          not be  converted to 8-bit but kept in 32-bit.'
        updatedAt: '2023-03-04T08:27:16.176Z'
      numEdits: 0
      reactions: []
    id: 640300e4923fe79058454a1a
    type: comment
  author: SamuelAzran
  content: '@akkikiki

    When I try your script load_flan_ul2.py on a single 16GB GPU I get this error:


    ValueError: If you want to offload some keys to `cpu` or `disk`, you need to set
    `load_in_8bit_fp32_cpu_offload=True`. Note that these modules will not be  converted
    to 8-bit but kept in 32-bit.'
  created_at: 2023-03-04 08:27:16+00:00
  edited: false
  hidden: false
  id: 640300e4923fe79058454a1a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669710270688-noauth.jpeg?w=200&h=200&f=face
      fullname: Xing Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: diegomontoya
      type: user
    createdAt: '2023-03-04T12:04:26.000Z'
    data:
      edited: false
      editors:
      - diegomontoya
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669710270688-noauth.jpeg?w=200&h=200&f=face
          fullname: Xing Li
          isHf: false
          isPro: true
          name: diegomontoya
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> Does your load_flan_ul2.py\
          \ <code>max_memory[0]=10GiB</code> code waste 6GB on the first gpu by setting\
          \ a value below the max ram?</p>\n"
        raw: '@akkikiki Does your load_flan_ul2.py ```max_memory[0]=10GiB``` code
          waste 6GB on the first gpu by setting a value below the max ram?'
        updatedAt: '2023-03-04T12:04:26.069Z'
      numEdits: 0
      reactions: []
    id: 640333ca723a03e6267c263f
    type: comment
  author: diegomontoya
  content: '@akkikiki Does your load_flan_ul2.py ```max_memory[0]=10GiB``` code waste
    6GB on the first gpu by setting a value below the max ram?'
  created_at: 2023-03-04 12:04:26+00:00
  edited: false
  hidden: false
  id: 640333ca723a03e6267c263f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-04T13:05:49.000Z'
    data:
      edited: true
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;SamuelAzran&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SamuelAzran\"\
          >@<span class=\"underline\">SamuelAzran</span></a></span>\n\n\t</span></span>\
          \ Yeah, that script assumes you have four 16 GB RAM GPUs and you need to\
          \ offload it to CPU when you have only one. </p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;diegomontoya&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/diegomontoya\">@<span class=\"underline\"\
          >diegomontoya</span></a></span>\n\n\t</span></span> The reason for setting\
          \ <code>max_memory[0]=10GiB</code> is because of moving <code>lm_head</code>\
          \ to GPU 0 in an ad-hoc way (and loading the input tensor to GPU 0 before\
          \ running forward pass). Otherwise, it'll encounter the same <code>RuntimeError:\
          \ Expected all tensors to be on the same device,</code> when you run <code>model.generate</code>.<br>You\
          \ can play around with this max memory (it does not have to be 10GiB, and\
          \ there may be smarter ways of doing this), but without it, Accelerate does\
          \ not consider this action of ad-hoc moving of the <code>lm_head</code>\
          \ and causes GPU OOM on GPU 0.</p>\n"
        raw: "@SamuelAzran Yeah, that script assumes you have four 16 GB RAM GPUs\
          \ and you need to offload it to CPU when you have only one. \n\n@diegomontoya\
          \ The reason for setting `max_memory[0]=10GiB` is because of moving `lm_head`\
          \ to GPU 0 in an ad-hoc way (and loading the input tensor to GPU 0 before\
          \ running forward pass). Otherwise, it'll encounter the same `RuntimeError:\
          \ Expected all tensors to be on the same device,` when you run `model.generate`.\
          \ \nYou can play around with this max memory (it does not have to be 10GiB,\
          \ and there may be smarter ways of doing this), but without it, Accelerate\
          \ does not consider this action of ad-hoc moving of the `lm_head` and causes\
          \ GPU OOM on GPU 0."
        updatedAt: '2023-03-04T13:23:33.957Z'
      numEdits: 5
      reactions: []
    id: 6403422deec7c32754bdc47c
    type: comment
  author: akkikiki
  content: "@SamuelAzran Yeah, that script assumes you have four 16 GB RAM GPUs and\
    \ you need to offload it to CPU when you have only one. \n\n@diegomontoya The\
    \ reason for setting `max_memory[0]=10GiB` is because of moving `lm_head` to GPU\
    \ 0 in an ad-hoc way (and loading the input tensor to GPU 0 before running forward\
    \ pass). Otherwise, it'll encounter the same `RuntimeError: Expected all tensors\
    \ to be on the same device,` when you run `model.generate`. \nYou can play around\
    \ with this max memory (it does not have to be 10GiB, and there may be smarter\
    \ ways of doing this), but without it, Accelerate does not consider this action\
    \ of ad-hoc moving of the `lm_head` and causes GPU OOM on GPU 0."
  created_at: 2023-03-04 13:05:49+00:00
  edited: true
  hidden: false
  id: 6403422deec7c32754bdc47c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f95ed508a0e1c8c250640ea23a6c591.svg
      fullname: coyote78
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt78
      type: user
    createdAt: '2023-03-15T09:30:45.000Z'
    data:
      edited: true
      editors:
      - cyt78
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f95ed508a0e1c8c250640ea23a6c591.svg
          fullname: coyote78
          isHf: false
          isPro: false
          name: cyt78
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> Thanks for sharing\
          \ an example script to run flan-ul2 on multi gpu. I've tried it on an instance\
          \ with 4 V100 GPU (each has 16 GB memory). It didn't throw any error but\
          \ the output didn't look correct to me either.</p>\n<p>I got the following\
          \ output when I run your script (without change anything except file name):\
          \ </p>\n<pre><code>python flan_ul2_runbook.py\n/opt/conda/envs/flanul2/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:98:\
          \ UserWarning: /opt/conda/envs/flanul2 did not contain libcudart.so as expected!\
          \ Searching further paths...\n  warn(\nCUDA SETUP: CUDA path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n\
          CUDA_SETUP: Detected CUDA version 116\nCUDA_SETUP: Loading binary /opt/conda/envs/flanul2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n\
          &lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;&lt;pad&gt;\n\
          </code></pre>\n<p>In case it helps, here are my setup:</p>\n<pre><code>&gt;&gt;&gt;\
          \ torch.__version__\n'1.13.1+cu116'\n&gt;&gt;&gt; transformers.__version__\n\
          '4.26.1'\n&gt;&gt;&gt; accelerate.__version__\n'0.17.1\n</code></pre>\n\
          <p>Do you have any idea, what would be the problem?</p>\n"
        raw: "@akkikiki Thanks for sharing an example script to run flan-ul2 on multi\
          \ gpu. I've tried it on an instance with 4 V100 GPU (each has 16 GB memory).\
          \ It didn't throw any error but the output didn't look correct to me either.\n\
          \nI got the following output when I run your script (without change anything\
          \ except file name): \n\n```\npython flan_ul2_runbook.py\n/opt/conda/envs/flanul2/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:98:\
          \ UserWarning: /opt/conda/envs/flanul2 did not contain libcudart.so as expected!\
          \ Searching further paths...\n  warn(\nCUDA SETUP: CUDA path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n\
          CUDA_SETUP: Detected CUDA version 116\nCUDA_SETUP: Loading binary /opt/conda/envs/flanul2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n\
          <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n\
          ```\nIn case it helps, here are my setup:\n```\n>>> torch.__version__\n\
          '1.13.1+cu116'\n>>> transformers.__version__\n'4.26.1'\n>>> accelerate.__version__\n\
          '0.17.1\n```\nDo you have any idea, what would be the problem?"
        updatedAt: '2023-03-15T09:39:30.916Z'
      numEdits: 4
      reactions: []
    id: 64119045bb60ad6c6d0931f3
    type: comment
  author: cyt78
  content: "@akkikiki Thanks for sharing an example script to run flan-ul2 on multi\
    \ gpu. I've tried it on an instance with 4 V100 GPU (each has 16 GB memory). It\
    \ didn't throw any error but the output didn't look correct to me either.\n\n\
    I got the following output when I run your script (without change anything except\
    \ file name): \n\n```\npython flan_ul2_runbook.py\n/opt/conda/envs/flanul2/lib/python3.8/site-packages/bitsandbytes/cuda_setup/paths.py:98:\
    \ UserWarning: /opt/conda/envs/flanul2 did not contain libcudart.so as expected!\
    \ Searching further paths...\n  warn(\nCUDA SETUP: CUDA path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n\
    CUDA_SETUP: Detected CUDA version 116\nCUDA_SETUP: Loading binary /opt/conda/envs/flanul2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda116_nocublaslt.so...\n\
    <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n\
    ```\nIn case it helps, here are my setup:\n```\n>>> torch.__version__\n'1.13.1+cu116'\n\
    >>> transformers.__version__\n'4.26.1'\n>>> accelerate.__version__\n'0.17.1\n\
    ```\nDo you have any idea, what would be the problem?"
  created_at: 2023-03-15 08:30:45+00:00
  edited: true
  hidden: false
  id: 64119045bb60ad6c6d0931f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-15T12:57:02.000Z'
    data:
      edited: false
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cyt78&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cyt78\">@<span class=\"\
          underline\">cyt78</span></a></span>\n\n\t</span></span> Have a look at <a\
          \ rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/21987\"\
          >https://github.com/huggingface/transformers/issues/21987</a> :)<br>TL;\
          \ DR: Play around with <code>N</code> in <code>BitsAndBytesConfig(llm_int8_threshold=N)</code></p>\n"
        raw: '@cyt78 Have a look at https://github.com/huggingface/transformers/issues/21987
          :)

          TL; DR: Play around with `N` in `BitsAndBytesConfig(llm_int8_threshold=N)`'
        updatedAt: '2023-03-15T12:57:02.639Z'
      numEdits: 0
      reactions: []
    id: 6411c09e41ee111774ea2258
    type: comment
  author: akkikiki
  content: '@cyt78 Have a look at https://github.com/huggingface/transformers/issues/21987
    :)

    TL; DR: Play around with `N` in `BitsAndBytesConfig(llm_int8_threshold=N)`'
  created_at: 2023-03-15 11:57:02+00:00
  edited: false
  hidden: false
  id: 6411c09e41ee111774ea2258
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0f95ed508a0e1c8c250640ea23a6c591.svg
      fullname: coyote78
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt78
      type: user
    createdAt: '2023-03-15T16:09:44.000Z'
    data:
      edited: false
      editors:
      - cyt78
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0f95ed508a0e1c8c250640ea23a6c591.svg
          fullname: coyote78
          isHf: false
          isPro: false
          name: cyt78
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> thanks for your\
          \ reply! One follow up question: Do you have any recommendation for N? Also,\
          \ the script you shared above does not load model on int8. Is this still\
          \ necessary?</p>\n"
        raw: '@akkikiki thanks for your reply! One follow up question: Do you have
          any recommendation for N? Also, the script you shared above does not load
          model on int8. Is this still necessary?'
        updatedAt: '2023-03-15T16:09:44.971Z'
      numEdits: 0
      reactions: []
    id: 6411edc885e89e53c368c59e
    type: comment
  author: cyt78
  content: '@akkikiki thanks for your reply! One follow up question: Do you have any
    recommendation for N? Also, the script you shared above does not load model on
    int8. Is this still necessary?'
  created_at: 2023-03-15 15:09:44+00:00
  edited: false
  hidden: false
  id: 6411edc885e89e53c368c59e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-15T16:23:10.000Z'
    data:
      edited: false
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cyt78&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cyt78\">@<span class=\"\
          underline\">cyt78</span></a></span>\n\n\t</span></span> <code>N=5</code>\
          \ worked for me. Basically it's a trade-off betw. memory usage and accuracy\
          \ (on V100, which does not have int8 support on the hardware level. I believe\
          \ it's different story for A100 and others).<br>If you are talking about\
          \ <a rel=\"nofollow\" href=\"https://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py#L17\"\
          >https://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py#L17</a>\
          \ , then it does use load on int8 with <code>load_in_8bit=True</code></p>\n"
        raw: "@cyt78 `N=5` worked for me. Basically it's a trade-off betw. memory\
          \ usage and accuracy (on V100, which does not have int8 support on the hardware\
          \ level. I believe it's different story for A100 and others). \nIf you are\
          \ talking about https://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py#L17\
          \ , then it does use load on int8 with `load_in_8bit=True`"
        updatedAt: '2023-03-15T16:23:10.570Z'
      numEdits: 0
      reactions: []
    id: 6411f0ee61720088371a19e5
    type: comment
  author: akkikiki
  content: "@cyt78 `N=5` worked for me. Basically it's a trade-off betw. memory usage\
    \ and accuracy (on V100, which does not have int8 support on the hardware level.\
    \ I believe it's different story for A100 and others). \nIf you are talking about\
    \ https://github.com/akkikiki/huggingface_examples/blob/main/examples/load_flan_ul2.py#L17\
    \ , then it does use load on int8 with `load_in_8bit=True`"
  created_at: 2023-03-15 15:23:10+00:00
  edited: false
  hidden: false
  id: 6411f0ee61720088371a19e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
      fullname: coyote79
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt79
      type: user
    createdAt: '2023-03-15T16:56:34.000Z'
    data:
      edited: true
      editors:
      - cyt79
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
          fullname: coyote79
          isHf: false
          isPro: false
          name: cyt79
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> My account was\
          \ created today  and therefore I cant' post any more comments today. So,\
          \ I'll reply with this new account:).</p>\n<p> Yes, I was talking about\
          \ the script that you pointed out and realised that it indeed use int8.\
          \ My bad!  I've tried with couple of different N values ranging from 1.0\
          \ to 10.0 including 5.0 and I got CUDA out of memory error every single\
          \ time. I found this interesting since you mentioned that  you could manage\
          \ to run it on 3 GPUs with 16 gb memory each. I'm trying to run the exact\
          \ same script on 4 GPUs each has 16GB memory. Can you think of any possible\
          \ reason which might lead to Out of memory error in my case?</p>\n<p>Here\
          \ is the changes I did on the script to integrate your suggestion: </p>\n\
          <pre><code>from transformers import BitsAndBytesConfig\nquantization_config\
          \ = BitsAndBytesConfig(llm_int8_threshold=5.0)\nmodel = T5ForConditionalGeneration.from_pretrained(model_id,\
          \ device_map=device_map, load_in_8bit=True,quantization_config=quantization_config)\n\
          </code></pre>\n"
        raw: "@akkikiki My account was created today  and therefore I cant' post any\
          \ more comments today. So, I'll reply with this new account:).\n\n Yes,\
          \ I was talking about the script that you pointed out and realised that\
          \ it indeed use int8. My bad!  I've tried with couple of different N values\
          \ ranging from 1.0 to 10.0 including 5.0 and I got CUDA out of memory error\
          \ every single time. I found this interesting since you mentioned that \
          \ you could manage to run it on 3 GPUs with 16 gb memory each. I'm trying\
          \ to run the exact same script on 4 GPUs each has 16GB memory. Can you think\
          \ of any possible reason which might lead to Out of memory error in my case?\n\
          \nHere is the changes I did on the script to integrate your suggestion:\
          \ \n```\nfrom transformers import BitsAndBytesConfig\nquantization_config\
          \ = BitsAndBytesConfig(llm_int8_threshold=5.0)\nmodel = T5ForConditionalGeneration.from_pretrained(model_id,\
          \ device_map=device_map, load_in_8bit=True,quantization_config=quantization_config)\n\
          ```"
        updatedAt: '2023-03-15T17:03:13.843Z'
      numEdits: 1
      reactions: []
    id: 6411f8c285e89e53c36a742f
    type: comment
  author: cyt79
  content: "@akkikiki My account was created today  and therefore I cant' post any\
    \ more comments today. So, I'll reply with this new account:).\n\n Yes, I was\
    \ talking about the script that you pointed out and realised that it indeed use\
    \ int8. My bad!  I've tried with couple of different N values ranging from 1.0\
    \ to 10.0 including 5.0 and I got CUDA out of memory error every single time.\
    \ I found this interesting since you mentioned that  you could manage to run it\
    \ on 3 GPUs with 16 gb memory each. I'm trying to run the exact same script on\
    \ 4 GPUs each has 16GB memory. Can you think of any possible reason which might\
    \ lead to Out of memory error in my case?\n\nHere is the changes I did on the\
    \ script to integrate your suggestion: \n```\nfrom transformers import BitsAndBytesConfig\n\
    quantization_config = BitsAndBytesConfig(llm_int8_threshold=5.0)\nmodel = T5ForConditionalGeneration.from_pretrained(model_id,\
    \ device_map=device_map, load_in_8bit=True,quantization_config=quantization_config)\n\
    ```"
  created_at: 2023-03-15 15:56:34+00:00
  edited: true
  hidden: false
  id: 6411f8c285e89e53c36a742f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-15T18:06:00.000Z'
    data:
      edited: true
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cyt79&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cyt79\">@<span class=\"\
          underline\">cyt79</span></a></span>\n\n\t</span></span> Yeah, looks like\
          \ there's more memory usage if the quantization threshold is lower (not\
          \ 100% sure why), so the 3 GPUs example is just <strong>without</strong>\
          \ setting <code>BitsAndBytesConfig(llm_int8_threshold=5.0)</code>.  Best\
          \ to play around with lowering <code>max_memory</code> to avoid it (and\
          \ CPU offloading if needed).</p>\n"
        raw: '@cyt79 Yeah, looks like there''s more memory usage if the quantization
          threshold is lower (not 100% sure why), so the 3 GPUs example is just **without**
          setting `BitsAndBytesConfig(llm_int8_threshold=5.0)`.  Best to play around
          with lowering `max_memory` to avoid it (and CPU offloading if needed).'
        updatedAt: '2023-03-15T19:44:12.456Z'
      numEdits: 1
      reactions: []
    id: 64120908f1a8832626dbd0c3
    type: comment
  author: akkikiki
  content: '@cyt79 Yeah, looks like there''s more memory usage if the quantization
    threshold is lower (not 100% sure why), so the 3 GPUs example is just **without**
    setting `BitsAndBytesConfig(llm_int8_threshold=5.0)`.  Best to play around with
    lowering `max_memory` to avoid it (and CPU offloading if needed).'
  created_at: 2023-03-15 17:06:00+00:00
  edited: true
  hidden: false
  id: 64120908f1a8832626dbd0c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fddcd1212f62547e87e026a8354cd4d5.svg
      fullname: Ziyi Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YzyLmc
      type: user
    createdAt: '2023-03-15T20:22:08.000Z'
    data:
      edited: true
      editors:
      - YzyLmc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fddcd1212f62547e87e026a8354cd4d5.svg
          fullname: Ziyi Yang
          isHf: false
          isPro: false
          name: YzyLmc
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> Thanks for sharing\
          \ the example code and all explanations! I would like to ask a relevant\
          \ question about specifying the <code>no_split_module_classes</code> parameter\
          \ since there's no official documentation of <code>infer_auto_device_map</code>.\
          \ I looked into their source code, and it seems that it gets the components\
          \ of the model by checking <code>model.named_parameters()</code>. However,\
          \ I didn't find anything has the name <code>T5Block</code> in those parameters\
          \ of flan_ul2. I wonder if it's defined elsewhere and how you find a way\
          \ to specify this parameter properly?</p>\n<p>I also tried to run the original\
          \ T5 model basically by replacing <code>flan_ul2</code> with <code>t5-large</code>,\
          \ but it triggered the <code>Expected all tensors to be on the same device</code>\
          \ error too, and this time specifying <code>no_split_module_classes=[\"\
          T5Block\"]</code> wouldn't help, neither did moving <code>lm_head</code>\
          \ to <code>gpu:0</code>. Does that indicate each time we want to run a model\
          \ we'll have to check the source code and look for the residual connections\
          \ and preserve them by specifying <code>no_split_module_classes</code>?\
          \ Thanks!</p>\n<p>EDIT: There is no problem doing inference with <code>t5-large</code>\
          \ by swapping  <code>flan_ul2</code> with <code>t5-large</code>. The multiple\
          \ device error was caused by the fine-tuning part of my code.</p>\n"
        raw: '@akkikiki Thanks for sharing the example code and all explanations!
          I would like to ask a relevant question about specifying the `no_split_module_classes`
          parameter since there''s no official documentation of `infer_auto_device_map`.
          I looked into their source code, and it seems that it gets the components
          of the model by checking `model.named_parameters()`. However, I didn''t
          find anything has the name `T5Block` in those parameters of flan_ul2. I
          wonder if it''s defined elsewhere and how you find a way to specify this
          parameter properly?


          I also tried to run the original T5 model basically by replacing `flan_ul2`
          with `t5-large`, but it triggered the `Expected all tensors to be on the
          same device` error too, and this time specifying `no_split_module_classes=["T5Block"]`
          wouldn''t help, neither did moving `lm_head` to `gpu:0`. Does that indicate
          each time we want to run a model we''ll have to check the source code and
          look for the residual connections and preserve them by specifying `no_split_module_classes`?
          Thanks!


          EDIT: There is no problem doing inference with `t5-large` by swapping  `flan_ul2`
          with `t5-large`. The multiple device error was caused by the fine-tuning
          part of my code.'
        updatedAt: '2023-03-17T03:50:51.653Z'
      numEdits: 2
      reactions: []
    id: 641228f0cbe679e71ebbcc7c
    type: comment
  author: YzyLmc
  content: '@akkikiki Thanks for sharing the example code and all explanations! I
    would like to ask a relevant question about specifying the `no_split_module_classes`
    parameter since there''s no official documentation of `infer_auto_device_map`.
    I looked into their source code, and it seems that it gets the components of the
    model by checking `model.named_parameters()`. However, I didn''t find anything
    has the name `T5Block` in those parameters of flan_ul2. I wonder if it''s defined
    elsewhere and how you find a way to specify this parameter properly?


    I also tried to run the original T5 model basically by replacing `flan_ul2` with
    `t5-large`, but it triggered the `Expected all tensors to be on the same device`
    error too, and this time specifying `no_split_module_classes=["T5Block"]` wouldn''t
    help, neither did moving `lm_head` to `gpu:0`. Does that indicate each time we
    want to run a model we''ll have to check the source code and look for the residual
    connections and preserve them by specifying `no_split_module_classes`? Thanks!


    EDIT: There is no problem doing inference with `t5-large` by swapping  `flan_ul2`
    with `t5-large`. The multiple device error was caused by the fine-tuning part
    of my code.'
  created_at: 2023-03-15 19:22:08+00:00
  edited: true
  hidden: false
  id: 641228f0cbe679e71ebbcc7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-15T21:15:27.000Z'
    data:
      edited: false
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;YzyLmc&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/YzyLmc\">@<span class=\"\
          underline\">YzyLmc</span></a></span>\n\n\t</span></span> I believe you should\
          \ share your script on <code>t5-large</code> to share more context since\
          \ I did not have any trouble with swapping <code>flan_ul2</code> with <code>t5-large</code>.\
          \ As long as it's in the same T5 family, it's not the issue with <code>no_split_module_classes</code>\
          \  and the cause is different.</p>\n<p>How I found out is basically \"connecting\
          \ the dots\" from the Accelerate documentation (<a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/blog/blob/main/accelerate-large-models.md\"\
          >https://github.com/huggingface/blog/blob/main/accelerate-large-models.md</a>)\
          \ on <code>OPTDecoderLayer</code>,  reading the BLOOM blog esp. on the naive\
          \ pipeline parallelism section (<a href=\"https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism\"\
          >https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism</a>)\
          \ to understand the basic assumption that layers should not be dispatched\
          \ across multiple GPUs, my experience on the fix in loading Blip-2 Flan-T5-XL\
          \ with multiple GPUs <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/21707\"\
          >https://github.com/huggingface/transformers/pull/21707</a> (hinting from\
          \ what ybelkada@ raised as a warning in his PR), and getting my hands dirty\
          \ by actually debugging through printing out the <code>named_parameters</code>.</p>\n\
          <p>I believe with other types of multi-gpu parallelism (e.g., Tensor Parallelism\
          \ or TP) where we do not have to assume that same layer (or specifically\
          \ the weight Tensor associated with that layer) are be on same GPU (I guess,\
          \ not an expert with TP so somebody correct me if I'm wrong ), then probably\
          \ we do not have to much care about the <code>no_split_module_classes</code>\
          \ but somebody have to teach us how in a simple way :)</p>\n"
        raw: '@YzyLmc I believe you should share your script on `t5-large` to share
          more context since I did not have any trouble with swapping `flan_ul2` with
          `t5-large`. As long as it''s in the same T5 family, it''s not the issue
          with `no_split_module_classes`  and the cause is different.


          How I found out is basically "connecting the dots" from the Accelerate documentation
          (https://github.com/huggingface/blog/blob/main/accelerate-large-models.md)
          on `OPTDecoderLayer`,  reading the BLOOM blog esp. on the naive pipeline
          parallelism section (https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism)
          to understand the basic assumption that layers should not be dispatched
          across multiple GPUs, my experience on the fix in loading Blip-2 Flan-T5-XL
          with multiple GPUs https://github.com/huggingface/transformers/pull/21707
          (hinting from what ybelkada@ raised as a warning in his PR), and getting
          my hands dirty by actually debugging through printing out the `named_parameters`.


          I believe with other types of multi-gpu parallelism (e.g., Tensor Parallelism
          or TP) where we do not have to assume that same layer (or specifically the
          weight Tensor associated with that layer) are be on same GPU (I guess, not
          an expert with TP so somebody correct me if I''m wrong ), then probably
          we do not have to much care about the `no_split_module_classes` but somebody
          have to teach us how in a simple way :)'
        updatedAt: '2023-03-15T21:15:27.359Z'
      numEdits: 0
      reactions: []
    id: 6412356fbf081e031e928756
    type: comment
  author: akkikiki
  content: '@YzyLmc I believe you should share your script on `t5-large` to share
    more context since I did not have any trouble with swapping `flan_ul2` with `t5-large`.
    As long as it''s in the same T5 family, it''s not the issue with `no_split_module_classes`  and
    the cause is different.


    How I found out is basically "connecting the dots" from the Accelerate documentation
    (https://github.com/huggingface/blog/blob/main/accelerate-large-models.md) on
    `OPTDecoderLayer`,  reading the BLOOM blog esp. on the naive pipeline parallelism
    section (https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism)
    to understand the basic assumption that layers should not be dispatched across
    multiple GPUs, my experience on the fix in loading Blip-2 Flan-T5-XL with multiple
    GPUs https://github.com/huggingface/transformers/pull/21707 (hinting from what
    ybelkada@ raised as a warning in his PR), and getting my hands dirty by actually
    debugging through printing out the `named_parameters`.


    I believe with other types of multi-gpu parallelism (e.g., Tensor Parallelism
    or TP) where we do not have to assume that same layer (or specifically the weight
    Tensor associated with that layer) are be on same GPU (I guess, not an expert
    with TP so somebody correct me if I''m wrong ), then probably we do not have to
    much care about the `no_split_module_classes` but somebody have to teach us how
    in a simple way :)'
  created_at: 2023-03-15 20:15:27+00:00
  edited: false
  hidden: false
  id: 6412356fbf081e031e928756
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fddcd1212f62547e87e026a8354cd4d5.svg
      fullname: Ziyi Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YzyLmc
      type: user
    createdAt: '2023-03-16T01:11:23.000Z'
    data:
      edited: false
      editors:
      - YzyLmc
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fddcd1212f62547e87e026a8354cd4d5.svg
          fullname: Ziyi Yang
          isHf: false
          isPro: false
          name: YzyLmc
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> Thanks for your\
          \ quick response! I just ran more tests, and you are totally correct that\
          \ <code>no_split_module_classes</code> wasn't the issue in my case. I was\
          \ trying to fine-tune the model, and it was the fine-tuning part that caused\
          \ this error, which is a separate issue, and the inference worked perfectly\
          \ with swapping <code>flan_ul2</code> with <code>t5-large</code>.  I'll\
          \ edit my earlier post. Sorry about that!</p>\n<p>Also thank you for sharing\
          \ your experience and insights. I can imagine how much effort you have put\
          \ into this to make it work, and I hope huggingface people will make clear\
          \ documentation on this to make it less burdensome.</p>\n"
        raw: '@akkikiki Thanks for your quick response! I just ran more tests, and
          you are totally correct that `no_split_module_classes` wasn''t the issue
          in my case. I was trying to fine-tune the model, and it was the fine-tuning
          part that caused this error, which is a separate issue, and the inference
          worked perfectly with swapping `flan_ul2` with `t5-large`.  I''ll edit my
          earlier post. Sorry about that!


          Also thank you for sharing your experience and insights. I can imagine how
          much effort you have put into this to make it work, and I hope huggingface
          people will make clear documentation on this to make it less burdensome.'
        updatedAt: '2023-03-16T01:11:23.483Z'
      numEdits: 0
      reactions: []
    id: 64126cbb8a62a28e6444e455
    type: comment
  author: YzyLmc
  content: '@akkikiki Thanks for your quick response! I just ran more tests, and you
    are totally correct that `no_split_module_classes` wasn''t the issue in my case.
    I was trying to fine-tune the model, and it was the fine-tuning part that caused
    this error, which is a separate issue, and the inference worked perfectly with
    swapping `flan_ul2` with `t5-large`.  I''ll edit my earlier post. Sorry about
    that!


    Also thank you for sharing your experience and insights. I can imagine how much
    effort you have put into this to make it work, and I hope huggingface people will
    make clear documentation on this to make it less burdensome.'
  created_at: 2023-03-16 00:11:23+00:00
  edited: false
  hidden: false
  id: 64126cbb8a62a28e6444e455
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
      fullname: coyote79
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt79
      type: user
    createdAt: '2023-03-16T08:49:18.000Z'
    data:
      edited: false
      editors:
      - cyt79
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
          fullname: coyote79
          isHf: false
          isPro: false
          name: cyt79
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span>  Many thanks\
          \ for your reply! I've moved to a bigger instance which has 8 V100 GPUs\
          \ (each has 32GB memory). Here, I could run the official example code in\
          \ the model card  tab in btfloat16 and got the exacted result. Then, I tried\
          \ to run your script again. This time, I got a different error: </p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"flan_runbook.py\", line 18, in &lt;module&gt;\n\
          \    device_map['lm_head'] = device_map[\"decoder.embed_tokens\"]\nKeyError:\
          \ 'decoder.embed_tokens'\n</code></pre>\n<p>I guess \"decoder.embed_tokens\"\
          \ has been renamed  after you implemented this script. Do you know where\
          \ can I check the latest name of this?</p>\n"
        raw: "@akkikiki  Many thanks for your reply! I've moved to a bigger instance\
          \ which has 8 V100 GPUs (each has 32GB memory). Here, I could run the official\
          \ example code in the model card  tab in btfloat16 and got the exacted result.\
          \ Then, I tried to run your script again. This time, I got a different error:\
          \ \n\n```\nTraceback (most recent call last):\n  File \"flan_runbook.py\"\
          , line 18, in <module>\n    device_map['lm_head'] = device_map[\"decoder.embed_tokens\"\
          ]\nKeyError: 'decoder.embed_tokens'\n```\n\nI guess \"decoder.embed_tokens\"\
          \ has been renamed  after you implemented this script. Do you know where\
          \ can I check the latest name of this?"
        updatedAt: '2023-03-16T08:49:18.332Z'
      numEdits: 0
      reactions: []
    id: 6412d80eb9c4d0e1b73f7932
    type: comment
  author: cyt79
  content: "@akkikiki  Many thanks for your reply! I've moved to a bigger instance\
    \ which has 8 V100 GPUs (each has 32GB memory). Here, I could run the official\
    \ example code in the model card  tab in btfloat16 and got the exacted result.\
    \ Then, I tried to run your script again. This time, I got a different error:\
    \ \n\n```\nTraceback (most recent call last):\n  File \"flan_runbook.py\", line\
    \ 18, in <module>\n    device_map['lm_head'] = device_map[\"decoder.embed_tokens\"\
    ]\nKeyError: 'decoder.embed_tokens'\n```\n\nI guess \"decoder.embed_tokens\" has\
    \ been renamed  after you implemented this script. Do you know where can I check\
    \ the latest name of this?"
  created_at: 2023-03-16 07:49:18+00:00
  edited: false
  hidden: false
  id: 6412d80eb9c4d0e1b73f7932
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
      fullname: coyote79
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt79
      type: user
    createdAt: '2023-03-16T22:32:19.000Z'
    data:
      edited: true
      editors:
      - cyt79
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
          fullname: coyote79
          isHf: false
          isPro: false
          name: cyt79
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span> When possible,\
          \ can you give me some pointers on how to fix the above error? Many thanks\
          \ in advance!</p>\n"
        raw: '@akkikiki When possible, can you give me some pointers on how to fix
          the above error? Many thanks in advance!'
        updatedAt: '2023-03-16T22:32:32.377Z'
      numEdits: 1
      reactions: []
    id: 641398f374a580779ae6caa9
    type: comment
  author: cyt79
  content: '@akkikiki When possible, can you give me some pointers on how to fix the
    above error? Many thanks in advance!'
  created_at: 2023-03-16 21:32:19+00:00
  edited: true
  hidden: false
  id: 641398f374a580779ae6caa9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-03-17T03:53:21.000Z'
    data:
      edited: false
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cyt79&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cyt79\">@<span class=\"\
          underline\">cyt79</span></a></span>\n\n\t</span></span> It's either the\
          \ device map is empty (when you can fit on 1 GPU) or the parameter names\
          \ are different. Just print out the param names with e.g., <code>model.named_parameters()</code></p>\n"
        raw: '@cyt79 It''s either the device map is empty (when you can fit on 1 GPU)
          or the parameter names are different. Just print out the param names with
          e.g., `model.named_parameters()`'
        updatedAt: '2023-03-17T03:53:21.677Z'
      numEdits: 0
      reactions: []
    id: 6413e43147b37232c80e0b9d
    type: comment
  author: akkikiki
  content: '@cyt79 It''s either the device map is empty (when you can fit on 1 GPU)
    or the parameter names are different. Just print out the param names with e.g.,
    `model.named_parameters()`'
  created_at: 2023-03-17 02:53:21+00:00
  edited: false
  hidden: false
  id: 6413e43147b37232c80e0b9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/93b25084fda861d348e882a29c840284.svg
      fullname: weihl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WHL95
      type: user
    createdAt: '2023-04-15T01:50:29.000Z'
    data:
      edited: false
      editors:
      - WHL95
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/93b25084fda861d348e882a29c840284.svg
          fullname: weihl
          isHf: false
          isPro: false
          name: WHL95
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;akkikiki&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/akkikiki\">@<span class=\"\
          underline\">akkikiki</span></a></span>\n\n\t</span></span>  I ran inference\
          \ on blip2_flant5xxl model in a two 3090 environment. Following (<a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/pull/21707\"\
          >https://github.com/huggingface/transformers/pull/21707</a>), I use</p>\n\
          <pre><code class=\"language-python\">configuration = Blip2Config.from_pretrained(<span\
          \ class=\"hljs-string\">\"Salesforce/blip2-flan-t5-xxl\"</span>)\n<span\
          \ class=\"hljs-keyword\">with</span> init_empty_weights():\n    model =\
          \ Blip2ForConditionalGeneration(configuration)\n    device_map = infer_auto_device_map(model,\
          \ no_split_module_classes=[<span class=\"hljs-string\">\"T5Block\"</span>],\
          \ max_memory={<span class=\"hljs-number\">0</span>: <span class=\"hljs-string\"\
          >\"24GiB\"</span>, <span class=\"hljs-number\">1</span>: <span class=\"\
          hljs-string\">\"24GiB\"</span>})\ndevice_map[<span class=\"hljs-string\"\
          >'language_model.lm_head'</span>] = device_map[<span class=\"hljs-string\"\
          >\"language_model.decoder.embed_tokens\"</span>]  <span class=\"hljs-comment\"\
          ># to make the genearted tokens and input_ids to be on the same device</span>\n\
          model = Blip2ForConditionalGeneration(configuration).from_pretrained(<span\
          \ class=\"hljs-string\">\"Salesforce/blip2-flan-t5-xxl\"</span>, torch_dtype=torch.float16,\
          \ device_map=device_map, cache_dir=<span class=\"hljs-string\">\"/mnt/14T-disk/code/HF_model/hub\"\
          </span>)\n</code></pre>\n<p>and device_map is:</p>\n<pre><code class=\"\
          language-python\">{<span class=\"hljs-string\">'query_tokens'</span>: <span\
          \ class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'vision_model'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'qformer'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'language_projection'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'language_model.shared'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'language_model.decoder.embed_tokens'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'language_model.encoder'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'language_model.decoder.block.0'</span>:\
          \ <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">'language_model.decoder.block.1'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.2'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.3'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.4'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.5'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.6'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.7'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.8'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.9'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.10'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.11'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.12'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.13'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.14'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.15'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.16'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.17'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.18'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.19'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.20'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.21'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.22'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.block.23'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.final_layer_norm'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.decoder.dropout'</span>:\
          \ <span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'language_model.lm_head'</span>:\
          \ <span class=\"hljs-number\">0</span>}\n</code></pre>\n<p> However, when\
          \ I executed the inference, I received the following error message but still\
          \ got the inference result of the model. I don't understand why this happened.\
          \ Is the result reliable in this case?Thank you.<br>--- Logging error ---<br>Traceback\
          \ (most recent call last):<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/<strong>init</strong>.py\"\
          , line 1100, in emit<br>    msg = self.format(record)<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/<strong>init</strong>.py\"\
          , line 943, in format<br>    return fmt.format(record)<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/<strong>init</strong>.py\"\
          , line 678, in format<br>    record.message = record.getMessage()<br>  File\
          \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/<strong>init</strong>.py\"\
          , line 368, in getMessage<br>    msg = msg % self.args<br>TypeError: not\
          \ all arguments converted during string formatting<br>Call stack:<br>  File\
          \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/runpy.py\", line 196,\
          \ in _run_module_as_main<br>    return _run_code(code, main_globals, None,<br>\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/runpy.py\", line\
          \ 86, in _run_code<br>    exec(code, run_globals)<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel_launcher.py\"\
          , line 17, in <br>    app.launch_new_instance()<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/traitlets/config/application.py\"\
          , line 992, in launch_instance<br>    app.start()<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelapp.py\"\
          , line 711, in start<br>    self.io_loop.start()<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/tornado/platform/asyncio.py\"\
          , line 215, in start<br>    self.asyncio_loop.run_forever()<br>  File \"\
          /home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/base_events.py\",\
          \ line 603, in run_forever<br>    self._run_once()<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/base_events.py\"\
          , line 1906, in _run_once<br>    handle._run()<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run<br>    self._context.run(self._callback, *self._args)<br>\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 510, in dispatch_queue<br>    await self.process_one()<br>  File\
          \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 499, in process_one<br>    await dispatch(*args)<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 406, in dispatch_shell<br>    await result<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 729, in execute_request<br>    reply_content = await reply_content<br>\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/ipkernel.py\"\
          , line 411, in do_execute<br>    res = shell.run_cell(<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/zmqshell.py\"\
          , line 531, in run_cell<br>    return super().run_cell(*args, **kwargs)<br>\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 2961, in run_cell<br>    result = self._run_cell(<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3016, in _run_cell<br>    result = runner(coro)<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/async_helpers.py\"\
          , line 129, in <em>pseudo_sync_runner<br>    coro.send(None)<br>  File \"\
          /home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3221, in run_cell_async<br>    has_raised = await self.run_ast_nodes(code_ast.body,\
          \ cell_name,<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3400, in run_ast_nodes<br>    if await self.run_code(code, result,\
          \ async</em>=asy):<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3460, in run_code<br>    exec(code_obj, self.user_global_ns, self.user_ns)<br>\
          \  File \"/tmp/ipykernel_1716702/309436947.py\", line 11, in <br>    out\
          \ = model.generate(**inputs)<br>  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context<br>    return func(*args, **kwargs)<br> \
          \ File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py\"\
          , line 1805, in generate<br>    self._preprocess_accelerate()<br>  File\
          \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py\"\
          , line 1607, in _preprocess_accelerate<br>    logger.warning(<br>Message:\
          \ 'The <code>language_model</code> is not in the <code>hf_device_map</code>\
          \ dictionary and you are running your script in a multi-GPU environment.\
          \ this may lead to unexpected behavior when using <code>accelerate</code>.\
          \ Please pass a <code>device_map</code> that contains <code>language_model</code>\
          \ to remove this warning. Please refer to <a rel=\"nofollow\" href=\"https://github.com/huggingface/blog/blob/main/accelerate-large-models.md\"\
          >https://github.com/huggingface/blog/blob/main/accelerate-large-models.md</a>\
          \ for'<br>Arguments: (' more details on creating a <code>device_map</code>\
          \ for large models.',)</p>\n"
        raw: "@akkikiki  I ran inference on blip2_flant5xxl model in a two 3090 environment.\
          \ Following (https://github.com/huggingface/transformers/pull/21707), I\
          \ use\n```python\nconfiguration = Blip2Config.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\"\
          )\nwith init_empty_weights():\n    model = Blip2ForConditionalGeneration(configuration)\n\
          \    device_map = infer_auto_device_map(model, no_split_module_classes=[\"\
          T5Block\"], max_memory={0: \"24GiB\", 1: \"24GiB\"})\ndevice_map['language_model.lm_head']\
          \ = device_map[\"language_model.decoder.embed_tokens\"]  # to make the genearted\
          \ tokens and input_ids to be on the same device\nmodel = Blip2ForConditionalGeneration(configuration).from_pretrained(\"\
          Salesforce/blip2-flan-t5-xxl\", torch_dtype=torch.float16, device_map=device_map,\
          \ cache_dir=\"/mnt/14T-disk/code/HF_model/hub\")\n```\nand device_map is:\n\
          ```python\n{'query_tokens': 0, 'vision_model': 0, 'qformer': 0, 'language_projection':\
          \ 0, 'language_model.shared': 0, 'language_model.decoder.embed_tokens':\
          \ 0, 'language_model.encoder': 0, 'language_model.decoder.block.0': 0, 'language_model.decoder.block.1':\
          \ 1, 'language_model.decoder.block.2': 1, 'language_model.decoder.block.3':\
          \ 1, 'language_model.decoder.block.4': 1, 'language_model.decoder.block.5':\
          \ 1, 'language_model.decoder.block.6': 1, 'language_model.decoder.block.7':\
          \ 1, 'language_model.decoder.block.8': 1, 'language_model.decoder.block.9':\
          \ 1, 'language_model.decoder.block.10': 1, 'language_model.decoder.block.11':\
          \ 1, 'language_model.decoder.block.12': 1, 'language_model.decoder.block.13':\
          \ 1, 'language_model.decoder.block.14': 1, 'language_model.decoder.block.15':\
          \ 1, 'language_model.decoder.block.16': 1, 'language_model.decoder.block.17':\
          \ 1, 'language_model.decoder.block.18': 1, 'language_model.decoder.block.19':\
          \ 1, 'language_model.decoder.block.20': 1, 'language_model.decoder.block.21':\
          \ 1, 'language_model.decoder.block.22': 1, 'language_model.decoder.block.23':\
          \ 1, 'language_model.decoder.final_layer_norm': 1, 'language_model.decoder.dropout':\
          \ 1, 'language_model.lm_head': 0}\n```\n However, when I executed the inference,\
          \ I received the following error message but still got the inference result\
          \ of the model. I don't understand why this happened. Is the result reliable\
          \ in this case?Thank you.\n--- Logging error ---\nTraceback (most recent\
          \ call last):\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
          , line 1100, in emit\n    msg = self.format(record)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
          , line 943, in format\n    return fmt.format(record)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
          , line 678, in format\n    record.message = record.getMessage()\n  File\
          \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
          , line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all\
          \ arguments converted during string formatting\nCall stack:\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/runpy.py\"\
          , line 196, in _run_module_as_main\n    return _run_code(code, main_globals,\
          \ None,\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/runpy.py\"\
          , line 86, in _run_code\n    exec(code, run_globals)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel_launcher.py\"\
          , line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/traitlets/config/application.py\"\
          , line 992, in launch_instance\n    app.start()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelapp.py\"\
          , line 711, in start\n    self.io_loop.start()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/tornado/platform/asyncio.py\"\
          , line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/base_events.py\"\
          , line 603, in run_forever\n    self._run_once()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/base_events.py\"\
          , line 1906, in _run_once\n    handle._run()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/events.py\"\
          , line 80, in _run\n    self._context.run(self._callback, *self._args)\n\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 510, in dispatch_queue\n    await self.process_one()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 499, in process_one\n    await dispatch(*args)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 406, in dispatch_shell\n    await result\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
          , line 729, in execute_request\n    reply_content = await reply_content\n\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/ipkernel.py\"\
          , line 411, in do_execute\n    res = shell.run_cell(\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/zmqshell.py\"\
          , line 531, in run_cell\n    return super().run_cell(*args, **kwargs)\n\
          \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 2961, in run_cell\n    result = self._run_cell(\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3016, in _run_cell\n    result = runner(coro)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/async_helpers.py\"\
          , line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3221, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body,\
          \ cell_name,\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3400, in run_ast_nodes\n    if await self.run_code(code, result,\
          \ async_=asy):\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
          , line 3460, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\
          \  File \"/tmp/ipykernel_1716702/309436947.py\", line 11, in <module>\n\
          \    out = model.generate(**inputs)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py\"\
          , line 1805, in generate\n    self._preprocess_accelerate()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py\"\
          , line 1607, in _preprocess_accelerate\n    logger.warning(\nMessage: 'The\
          \ `language_model` is not in the `hf_device_map` dictionary and you are\
          \ running your script in a multi-GPU environment. this may lead to unexpected\
          \ behavior when using `accelerate`. Please pass a `device_map` that contains\
          \ `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md\
          \ for'\nArguments: (' more details on creating a `device_map` for large\
          \ models.',)"
        updatedAt: '2023-04-15T01:50:29.838Z'
      numEdits: 0
      reactions: []
    id: 643a02e5623c970188059c17
    type: comment
  author: WHL95
  content: "@akkikiki  I ran inference on blip2_flant5xxl model in a two 3090 environment.\
    \ Following (https://github.com/huggingface/transformers/pull/21707), I use\n\
    ```python\nconfiguration = Blip2Config.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\"\
    )\nwith init_empty_weights():\n    model = Blip2ForConditionalGeneration(configuration)\n\
    \    device_map = infer_auto_device_map(model, no_split_module_classes=[\"T5Block\"\
    ], max_memory={0: \"24GiB\", 1: \"24GiB\"})\ndevice_map['language_model.lm_head']\
    \ = device_map[\"language_model.decoder.embed_tokens\"]  # to make the genearted\
    \ tokens and input_ids to be on the same device\nmodel = Blip2ForConditionalGeneration(configuration).from_pretrained(\"\
    Salesforce/blip2-flan-t5-xxl\", torch_dtype=torch.float16, device_map=device_map,\
    \ cache_dir=\"/mnt/14T-disk/code/HF_model/hub\")\n```\nand device_map is:\n```python\n\
    {'query_tokens': 0, 'vision_model': 0, 'qformer': 0, 'language_projection': 0,\
    \ 'language_model.shared': 0, 'language_model.decoder.embed_tokens': 0, 'language_model.encoder':\
    \ 0, 'language_model.decoder.block.0': 0, 'language_model.decoder.block.1': 1,\
    \ 'language_model.decoder.block.2': 1, 'language_model.decoder.block.3': 1, 'language_model.decoder.block.4':\
    \ 1, 'language_model.decoder.block.5': 1, 'language_model.decoder.block.6': 1,\
    \ 'language_model.decoder.block.7': 1, 'language_model.decoder.block.8': 1, 'language_model.decoder.block.9':\
    \ 1, 'language_model.decoder.block.10': 1, 'language_model.decoder.block.11':\
    \ 1, 'language_model.decoder.block.12': 1, 'language_model.decoder.block.13':\
    \ 1, 'language_model.decoder.block.14': 1, 'language_model.decoder.block.15':\
    \ 1, 'language_model.decoder.block.16': 1, 'language_model.decoder.block.17':\
    \ 1, 'language_model.decoder.block.18': 1, 'language_model.decoder.block.19':\
    \ 1, 'language_model.decoder.block.20': 1, 'language_model.decoder.block.21':\
    \ 1, 'language_model.decoder.block.22': 1, 'language_model.decoder.block.23':\
    \ 1, 'language_model.decoder.final_layer_norm': 1, 'language_model.decoder.dropout':\
    \ 1, 'language_model.lm_head': 0}\n```\n However, when I executed the inference,\
    \ I received the following error message but still got the inference result of\
    \ the model. I don't understand why this happened. Is the result reliable in this\
    \ case?Thank you.\n--- Logging error ---\nTraceback (most recent call last):\n\
    \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
    , line 1100, in emit\n    msg = self.format(record)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
    , line 943, in format\n    return fmt.format(record)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
    , line 678, in format\n    record.message = record.getMessage()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/logging/__init__.py\"\
    , line 368, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments\
    \ converted during string formatting\nCall stack:\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/runpy.py\"\
    , line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n\
    \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/runpy.py\", line 86, in\
    \ _run_code\n    exec(code, run_globals)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel_launcher.py\"\
    , line 17, in <module>\n    app.launch_new_instance()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/traitlets/config/application.py\"\
    , line 992, in launch_instance\n    app.start()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelapp.py\"\
    , line 711, in start\n    self.io_loop.start()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/tornado/platform/asyncio.py\"\
    , line 215, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/base_events.py\"\
    , line 603, in run_forever\n    self._run_once()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/base_events.py\"\
    , line 1906, in _run_once\n    handle._run()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/asyncio/events.py\"\
    , line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File\
    \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
    , line 510, in dispatch_queue\n    await self.process_one()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
    , line 499, in process_one\n    await dispatch(*args)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
    , line 406, in dispatch_shell\n    await result\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/kernelbase.py\"\
    , line 729, in execute_request\n    reply_content = await reply_content\n  File\
    \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/ipkernel.py\"\
    , line 411, in do_execute\n    res = shell.run_cell(\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/ipykernel/zmqshell.py\"\
    , line 531, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File\
    \ \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 2961, in run_cell\n    result = self._run_cell(\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 3016, in _run_cell\n    result = runner(coro)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/async_helpers.py\"\
    , line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 3221, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body,\
    \ cell_name,\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 3400, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n\
    \  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/IPython/core/interactiveshell.py\"\
    , line 3460, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n\
    \  File \"/tmp/ipykernel_1716702/309436947.py\", line 11, in <module>\n    out\
    \ = model.generate(**inputs)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py\"\
    , line 1805, in generate\n    self._preprocess_accelerate()\n  File \"/home/whl/anaconda3/envs/blip2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py\"\
    , line 1607, in _preprocess_accelerate\n    logger.warning(\nMessage: 'The `language_model`\
    \ is not in the `hf_device_map` dictionary and you are running your script in\
    \ a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`.\
    \ Please pass a `device_map` that contains `language_model` to remove this warning.\
    \ Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md\
    \ for'\nArguments: (' more details on creating a `device_map` for large models.',)"
  created_at: 2023-04-15 00:50:29+00:00
  edited: false
  hidden: false
  id: 643a02e5623c970188059c17
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
      fullname: Yoshinari Fujinuma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: akkikiki
      type: user
    createdAt: '2023-04-15T05:41:08.000Z'
    data:
      edited: false
      editors:
      - akkikiki
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ec6f7b2fa24a284d592b71b9bd3ca5f3.svg
          fullname: Yoshinari Fujinuma
          isHf: false
          isPro: false
          name: akkikiki
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;WHL95&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/WHL95\">@<span class=\"\
          underline\">WHL95</span></a></span>\n\n\t</span></span> Probably good to\
          \ start a thread in the BLIP-2 model community rather than here :)<br>But\
          \ not sure what is happening there. Maybe an error related to log parsing?\
          \ If you can share the full script, that would help.</p>\n"
        raw: '@WHL95 Probably good to start a thread in the BLIP-2 model community
          rather than here :)

          But not sure what is happening there. Maybe an error related to log parsing?
          If you can share the full script, that would help.'
        updatedAt: '2023-04-15T05:41:08.937Z'
      numEdits: 0
      reactions: []
    id: 643a38f4623c97018806dc9e
    type: comment
  author: akkikiki
  content: '@WHL95 Probably good to start a thread in the BLIP-2 model community rather
    than here :)

    But not sure what is happening there. Maybe an error related to log parsing? If
    you can share the full script, that would help.'
  created_at: 2023-04-15 04:41:08+00:00
  edited: false
  hidden: false
  id: 643a38f4623c97018806dc9e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: google/flan-ul2
repo_type: model
status: open
target_branch: null
title: Error running the example code
