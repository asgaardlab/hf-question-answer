!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pengan
conflicting_files: null
created_at: 2023-10-04 07:27:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3898f3ac9b2daaab9959b0aeeff7e34.svg
      fullname: Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pengan
      type: user
    createdAt: '2023-10-04T08:27:19.000Z'
    data:
      edited: true
      editors:
      - Pengan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7530437707901001
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3898f3ac9b2daaab9959b0aeeff7e34.svg
          fullname: Zhou
          isHf: false
          isPro: false
          name: Pengan
          type: user
        html: '<p>First install the DirectML ONNX Runtime<br><code>pip install onnxruntime-directml</code><br>Then
          in the <code>model.py</code>file<br>Change<code>providers = ["CPUExecutionProvider"]</code>
          to <code>providers = ["DmlExecutionProvider"]</code><br>It will work with
          GPU, verified with RTX4060 8GB and 7.7GB VRAM consumed.<br>Also tested with
          Intel i5 7200U''s HD620 iGPU, running slowly at 5~6s per token but generate
          correct content with no problem.</p>

          '
        raw: "First install the DirectML ONNX Runtime\n`pip install onnxruntime-directml`\n\
          Then in the `model.py`file\nChange`providers = [\"CPUExecutionProvider\"\
          ]` to `providers = [\"DmlExecutionProvider\"]`\nIt will work with GPU, verified\
          \ with RTX4060 8GB and 7.7GB VRAM consumed. \nAlso tested with Intel i5\
          \ 7200U's HD620 iGPU, running slowly at 5~6s per token but generate correct\
          \ content with no problem."
        updatedAt: '2023-10-04T10:48:56.541Z'
      numEdits: 1
      reactions: []
    id: 651d21e7b69c6c5e30ef57f4
    type: comment
  author: Pengan
  content: "First install the DirectML ONNX Runtime\n`pip install onnxruntime-directml`\n\
    Then in the `model.py`file\nChange`providers = [\"CPUExecutionProvider\"]` to\
    \ `providers = [\"DmlExecutionProvider\"]`\nIt will work with GPU, verified with\
    \ RTX4060 8GB and 7.7GB VRAM consumed. \nAlso tested with Intel i5 7200U's HD620\
    \ iGPU, running slowly at 5~6s per token but generate correct content with no\
    \ problem."
  created_at: 2023-10-04 07:27:19+00:00
  edited: true
  hidden: false
  id: 651d21e7b69c6c5e30ef57f4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: K024/ChatGLM-6b-onnx-u8s8
repo_type: model
status: open
target_branch: null
title: Successfully run on GPU with DirectML
