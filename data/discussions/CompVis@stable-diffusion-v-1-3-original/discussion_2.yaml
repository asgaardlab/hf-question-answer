!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Donnyed
conflicting_files: null
created_at: 2022-08-13 14:41:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69bbca64b1f38b6ceec58ee077df5a98.svg
      fullname: Ed Daniels
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Donnyed
      type: user
    createdAt: '2022-08-13T15:41:35.000Z'
    data:
      edited: false
      editors:
      - Donnyed
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69bbca64b1f38b6ceec58ee077df5a98.svg
          fullname: Ed Daniels
          isHf: false
          isPro: false
          name: Donnyed
          type: user
        html: '<p>I heard this can be run on &gt;10 Vram card, I have a 3070 8gb and
          when I tried to generate a picture I ran out of Vram, any settings I can
          change to get it running ?</p>

          '
        raw: I heard this can be run on >10 Vram card, I have a 3070 8gb and when
          I tried to generate a picture I ran out of Vram, any settings I can change
          to get it running ?
        updatedAt: '2022-08-13T15:41:35.565Z'
      numEdits: 0
      reactions: []
    id: 62f7c62f81f596cfe3c69733
    type: comment
  author: Donnyed
  content: I heard this can be run on >10 Vram card, I have a 3070 8gb and when I
    tried to generate a picture I ran out of Vram, any settings I can change to get
    it running ?
  created_at: 2022-08-13 14:41:35+00:00
  edited: false
  hidden: false
  id: 62f7c62f81f596cfe3c69733
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654305103579-noauth.jpeg?w=200&h=200&f=face
      fullname: Aleksandr Kudriashov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: not-salieri
      type: user
    createdAt: '2022-08-13T16:22:13.000Z'
    data:
      edited: true
      editors:
      - not-salieri
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654305103579-noauth.jpeg?w=200&h=200&f=face
          fullname: Aleksandr Kudriashov
          isHf: false
          isPro: false
          name: not-salieri
          type: user
        html: '<p>It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers
          model on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in
          ~ 45 secs with 100 steps perfectly.<br>Are you generating several samples
          or one sample? It may ran out of vRAM if you try to generate several samples
          in one go.</p>

          '
        raw: 'It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers
          model on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in
          ~ 45 secs with 100 steps perfectly.

          Are you generating several samples or one sample? It may ran out of vRAM
          if you try to generate several samples in one go.'
        updatedAt: '2022-08-13T16:22:48.431Z'
      numEdits: 1
      reactions: []
    id: 62f7cfb5fe21cc48751f43cd
    type: comment
  author: not-salieri
  content: 'It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers model
    on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in ~ 45 secs
    with 100 steps perfectly.

    Are you generating several samples or one sample? It may ran out of vRAM if you
    try to generate several samples in one go.'
  created_at: 2022-08-13 15:22:13+00:00
  edited: true
  hidden: false
  id: 62f7cfb5fe21cc48751f43cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69bbca64b1f38b6ceec58ee077df5a98.svg
      fullname: Ed Daniels
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Donnyed
      type: user
    createdAt: '2022-08-13T16:24:20.000Z'
    data:
      edited: false
      editors:
      - Donnyed
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69bbca64b1f38b6ceec58ee077df5a98.svg
          fullname: Ed Daniels
          isHf: false
          isPro: false
          name: Donnyed
          type: user
        html: '<p>I  tried running the txt2img.py with default settings, what do you
          use to get it to work on your 2060?</p>

          '
        raw: I  tried running the txt2img.py with default settings, what do you use
          to get it to work on your 2060?
        updatedAt: '2022-08-13T16:24:20.028Z'
      numEdits: 0
      reactions: []
    id: 62f7d03404de855c35e1827a
    type: comment
  author: Donnyed
  content: I  tried running the txt2img.py with default settings, what do you use
    to get it to work on your 2060?
  created_at: 2022-08-13 15:24:20+00:00
  edited: false
  hidden: false
  id: 62f7d03404de855c35e1827a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/69bbca64b1f38b6ceec58ee077df5a98.svg
      fullname: Ed Daniels
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Donnyed
      type: user
    createdAt: '2022-08-13T16:24:55.000Z'
    data:
      edited: false
      editors:
      - Donnyed
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/69bbca64b1f38b6ceec58ee077df5a98.svg
          fullname: Ed Daniels
          isHf: false
          isPro: false
          name: Donnyed
          type: user
        html: '<blockquote>

          <p>It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers model
          on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in ~ 45
          secs with 100 steps perfectly.<br>Are you generating several samples or
          one sample? It may ran out of vRAM if you try to generate several samples
          in one go.</p>

          </blockquote>

          <p>I tried running the txt2img.py with default settings, what do you use
          to get it to work on your 2060?</p>

          '
        raw: '> It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers
          model on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in
          ~ 45 secs with 100 steps perfectly.

          > Are you generating several samples or one sample? It may ran out of vRAM
          if you try to generate several samples in one go.


          I tried running the txt2img.py with default settings, what do you use to
          get it to work on your 2060?'
        updatedAt: '2022-08-13T16:24:55.604Z'
      numEdits: 0
      reactions: []
    id: 62f7d057c5cf61a0142ee562
    type: comment
  author: Donnyed
  content: '> It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers
    model on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in ~ 45
    secs with 100 steps perfectly.

    > Are you generating several samples or one sample? It may ran out of vRAM if
    you try to generate several samples in one go.


    I tried running the txt2img.py with default settings, what do you use to get it
    to work on your 2060?'
  created_at: 2022-08-13 15:24:55+00:00
  edited: false
  hidden: false
  id: 62f7d057c5cf61a0142ee562
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654305103579-noauth.jpeg?w=200&h=200&f=face
      fullname: Aleksandr Kudriashov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: not-salieri
      type: user
    createdAt: '2022-08-13T16:38:51.000Z'
    data:
      edited: true
      editors:
      - not-salieri
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654305103579-noauth.jpeg?w=200&h=200&f=face
          fullname: Aleksandr Kudriashov
          isHf: false
          isPro: false
          name: not-salieri
          type: user
        html: '<blockquote>

          <blockquote>

          <p>It''s really strange. I''ve runned stable-diffusion-v-1-3-diffusers model
          on GeForce RTX 2060 SUPER (8 GB vRAM). It''s generates one image in ~ 45
          secs with 100 steps perfectly.<br>Are you generating several samples or
          one sample? It may ran out of vRAM if you try to generate several samples
          in one go.</p>

          </blockquote>

          <p>I tried running the txt2img.py with default settings, what do you use
          to get it to work on your 2060?</p>

          </blockquote>

          <p>Again, I am using diffusers model. So I think they may be different results.<br>My
          settings are:  <code>n_samples=1, num_inference_steps=50, guidance_scale=7</code></p>

          <p>Wallpaper Engine/any other GPU intensive apps if running can really influence
          the GPU capabilities, so consider disabling it if running.</p>

          '
        raw: "> > It's really strange. I've runned stable-diffusion-v-1-3-diffusers\
          \ model on GeForce RTX 2060 SUPER (8 GB vRAM). It's generates one image\
          \ in ~ 45 secs with 100 steps perfectly.\n> > Are you generating several\
          \ samples or one sample? It may ran out of vRAM if you try to generate several\
          \ samples in one go.\n> \n> I tried running the txt2img.py with default\
          \ settings, what do you use to get it to work on your 2060?\n\nAgain, I\
          \ am using diffusers model. So I think they may be different results.\n\
          My settings are:  `n_samples=1, num_inference_steps=50, guidance_scale=7`\n\
          \nWallpaper Engine/any other GPU intensive apps if running can really influence\
          \ the GPU capabilities, so consider disabling it if running."
        updatedAt: '2022-08-13T16:44:52.608Z'
      numEdits: 2
      reactions: []
    id: 62f7d39b04de855c35e1986a
    type: comment
  author: not-salieri
  content: "> > It's really strange. I've runned stable-diffusion-v-1-3-diffusers\
    \ model on GeForce RTX 2060 SUPER (8 GB vRAM). It's generates one image in ~ 45\
    \ secs with 100 steps perfectly.\n> > Are you generating several samples or one\
    \ sample? It may ran out of vRAM if you try to generate several samples in one\
    \ go.\n> \n> I tried running the txt2img.py with default settings, what do you\
    \ use to get it to work on your 2060?\n\nAgain, I am using diffusers model. So\
    \ I think they may be different results.\nMy settings are:  `n_samples=1, num_inference_steps=50,\
    \ guidance_scale=7`\n\nWallpaper Engine/any other GPU intensive apps if running\
    \ can really influence the GPU capabilities, so consider disabling it if running."
  created_at: 2022-08-13 15:38:51+00:00
  edited: true
  hidden: false
  id: 62f7d39b04de855c35e1986a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
      fullname: Basu Jindal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: basujindal
      type: user
    createdAt: '2022-08-13T17:10:03.000Z'
    data:
      edited: false
      editors:
      - basujindal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
          fullname: Basu Jindal
          isHf: false
          isPro: false
          name: basujindal
          type: user
        html: '<p><a rel="nofollow" href="https://twitter.com/EMostaque/status/1557862289394515973">https://twitter.com/EMostaque/status/1557862289394515973</a>  This
          tweet says that the current model can run on 5.1 Gb VRAM, but my 6Gb GPU
          is giving me out of memory error. Any suggestions : (?</p>

          '
        raw: 'https://twitter.com/EMostaque/status/1557862289394515973  This tweet
          says that the current model can run on 5.1 Gb VRAM, but my 6Gb GPU is giving
          me out of memory error. Any suggestions : (?'
        updatedAt: '2022-08-13T17:10:03.998Z'
      numEdits: 0
      reactions: []
    id: 62f7daeb81f596cfe3c7265c
    type: comment
  author: basujindal
  content: 'https://twitter.com/EMostaque/status/1557862289394515973  This tweet says
    that the current model can run on 5.1 Gb VRAM, but my 6Gb GPU is giving me out
    of memory error. Any suggestions : (?'
  created_at: 2022-08-13 16:10:03+00:00
  edited: false
  hidden: false
  id: 62f7daeb81f596cfe3c7265c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2b5ce930c66ffc1158f4e89deac5c147.svg
      fullname: David Witzling
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bottymcbotface
      type: user
    createdAt: '2022-08-13T20:22:52.000Z'
    data:
      edited: false
      editors:
      - bottymcbotface
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2b5ce930c66ffc1158f4e89deac5c147.svg
          fullname: David Witzling
          isHf: false
          isPro: false
          name: bottymcbotface
          type: user
        html: '<blockquote>

          <p><a rel="nofollow" href="https://twitter.com/EMostaque/status/1557862289394515973">https://twitter.com/EMostaque/status/1557862289394515973</a>  This
          tweet says that the current model can run on 5.1 Gb VRAM, but my 6Gb GPU
          is giving me out of memory error. Any suggestions : (?</p>

          </blockquote>

          <p>By default the code processes 3 images at a time.  To reduce the memory
          footprint, reduce "--_samples" to 1.  You can also use --W and --H to reduce
          the size of the generated image (512x512 default) and thereboy reduce the
          memory footprint.  </p>

          <p>On a 3060 12GB I can generate images up to  708x512 using n_samples=1</p>

          '
        raw: "> https://twitter.com/EMostaque/status/1557862289394515973  This tweet\
          \ says that the current model can run on 5.1 Gb VRAM, but my 6Gb GPU is\
          \ giving me out of memory error. Any suggestions : (?\n\nBy default the\
          \ code processes 3 images at a time.  To reduce the memory footprint, reduce\
          \ \"--_samples\" to 1.  You can also use --W and --H to reduce the size\
          \ of the generated image (512x512 default) and thereboy reduce the memory\
          \ footprint.  \n\nOn a 3060 12GB I can generate images up to  708x512 using\
          \ n_samples=1"
        updatedAt: '2022-08-13T20:22:52.588Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - basujindal
    id: 62f8081ce7c1c9bf10c8a321
    type: comment
  author: bottymcbotface
  content: "> https://twitter.com/EMostaque/status/1557862289394515973  This tweet\
    \ says that the current model can run on 5.1 Gb VRAM, but my 6Gb GPU is giving\
    \ me out of memory error. Any suggestions : (?\n\nBy default the code processes\
    \ 3 images at a time.  To reduce the memory footprint, reduce \"--_samples\" to\
    \ 1.  You can also use --W and --H to reduce the size of the generated image (512x512\
    \ default) and thereboy reduce the memory footprint.  \n\nOn a 3060 12GB I can\
    \ generate images up to  708x512 using n_samples=1"
  created_at: 2022-08-13 19:22:52+00:00
  edited: false
  hidden: false
  id: 62f8081ce7c1c9bf10c8a321
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0d82bb96f4b16673c491b7f56d4c9b8b.svg
      fullname: Dimitri Livitz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dlivitz
      type: user
    createdAt: '2022-08-15T22:13:53.000Z'
    data:
      edited: false
      editors:
      - dlivitz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0d82bb96f4b16673c491b7f56d4c9b8b.svg
          fullname: Dimitri Livitz
          isHf: false
          isPro: false
          name: dlivitz
          type: user
        html: '<p>With this version of the weights, on an 8GB card I can generate
          1 image if I go down to 448x448, and the quality seems fine. If I try much
          smaller, the images look broken.<br>512x512 does not work with this set
          up as is for me, but I suspect it can be squeezed further in the implementation
          somewhere. </p>

          <p>With the diffusers code I can generate the full 512x512 on the same card.</p>

          '
        raw: "With this version of the weights, on an 8GB card I can generate 1 image\
          \ if I go down to 448x448, and the quality seems fine. If I try much smaller,\
          \ the images look broken. \n512x512 does not work with this set up as is\
          \ for me, but I suspect it can be squeezed further in the implementation\
          \ somewhere. \n\nWith the diffusers code I can generate the full 512x512\
          \ on the same card."
        updatedAt: '2022-08-15T22:13:53.470Z'
      numEdits: 0
      reactions: []
    id: 62fac521d8a067d3b36d6278
    type: comment
  author: dlivitz
  content: "With this version of the weights, on an 8GB card I can generate 1 image\
    \ if I go down to 448x448, and the quality seems fine. If I try much smaller,\
    \ the images look broken. \n512x512 does not work with this set up as is for me,\
    \ but I suspect it can be squeezed further in the implementation somewhere. \n\
    \nWith the diffusers code I can generate the full 512x512 on the same card."
  created_at: 2022-08-15 21:13:53+00:00
  edited: false
  hidden: false
  id: 62fac521d8a067d3b36d6278
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
      fullname: Basu Jindal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: basujindal
      type: user
    createdAt: '2022-08-16T09:25:08.000Z'
    data:
      edited: true
      editors:
      - basujindal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
          fullname: Basu Jindal
          isHf: false
          isPro: false
          name: basujindal
          type: user
        html: '<p>Edit: Have added support for batched inference. It can now generate
          images in batches which reduces inference time to 40 seconds per image on
          6Gb RTX 2060 when using a batch size of 6 : )</p>

          <p>I have created a modified version of the repo that can run on lower VRAM
          but requires a slightly longer inference time. It can generate a 512x512
          image on a 6Gb GPU (RTX 2060 in my case) in 75 seconds. Please feel free
          to check it out and give suggestions. <a rel="nofollow" href="https://github.com/basujindal/stable-diffusion">https://github.com/basujindal/stable-diffusion</a></p>

          '
        raw: "Edit: Have added support for batched inference. It can now generate\
          \ images in batches which reduces inference time to 40 seconds per image\
          \ on 6Gb RTX 2060 when using a batch size of 6 : )\n \nI have created a\
          \ modified version of the repo that can run on lower VRAM but requires a\
          \ slightly longer inference time. It can generate a 512x512 image on a 6Gb\
          \ GPU (RTX 2060 in my case) in 75 seconds. Please feel free to check it\
          \ out and give suggestions. https://github.com/basujindal/stable-diffusion"
        updatedAt: '2022-08-17T17:01:24.815Z'
      numEdits: 2
      reactions: []
    id: 62fb6274e44837de54437a01
    type: comment
  author: basujindal
  content: "Edit: Have added support for batched inference. It can now generate images\
    \ in batches which reduces inference time to 40 seconds per image on 6Gb RTX 2060\
    \ when using a batch size of 6 : )\n \nI have created a modified version of the\
    \ repo that can run on lower VRAM but requires a slightly longer inference time.\
    \ It can generate a 512x512 image on a 6Gb GPU (RTX 2060 in my case) in 75 seconds.\
    \ Please feel free to check it out and give suggestions. https://github.com/basujindal/stable-diffusion"
  created_at: 2022-08-16 08:25:08+00:00
  edited: true
  hidden: false
  id: 62fb6274e44837de54437a01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c7d7fae78efaac69aefcdf8caa4623a0.svg
      fullname: Ruben Villarreal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cachirulo001
      type: user
    createdAt: '2022-08-17T07:59:42.000Z'
    data:
      edited: false
      editors:
      - cachirulo001
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c7d7fae78efaac69aefcdf8caa4623a0.svg
          fullname: Ruben Villarreal
          isHf: false
          isPro: false
          name: cachirulo001
          type: user
        html: '<p>hi basujindal, any setting I can change in the optimizedSD files
          that would lower the bar to 4Gb GPU? Using your files I can run a 128x128
          image but not higher. I do not mind long inference time. I just want to
          run a higher resolution with my poor gtx 970</p>

          '
        raw: hi basujindal, any setting I can change in the optimizedSD files that
          would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image
          but not higher. I do not mind long inference time. I just want to run a
          higher resolution with my poor gtx 970
        updatedAt: '2022-08-17T07:59:42.811Z'
      numEdits: 0
      reactions: []
    id: 62fc9fee65ba08da9ccf6fcf
    type: comment
  author: cachirulo001
  content: hi basujindal, any setting I can change in the optimizedSD files that would
    lower the bar to 4Gb GPU? Using your files I can run a 128x128 image but not higher.
    I do not mind long inference time. I just want to run a higher resolution with
    my poor gtx 970
  created_at: 2022-08-17 06:59:42+00:00
  edited: false
  hidden: false
  id: 62fc9fee65ba08da9ccf6fcf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
      fullname: Basu Jindal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: basujindal
      type: user
    createdAt: '2022-08-17T11:32:16.000Z'
    data:
      edited: false
      editors:
      - basujindal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
          fullname: Basu Jindal
          isHf: false
          isPro: false
          name: basujindal
          type: user
        html: '<blockquote>

          <p>hi basujindal, any setting I can change in the optimizedSD files that
          would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image
          but not higher. I do not mind long inference time. I just want to run a
          higher resolution with my poor gtx 970</p>

          </blockquote>

          <p>Hi, I am trying to modify the code to make it run on a lower VRAM but
          it seems complicated. Will let you know if it works.</p>

          '
        raw: '> hi basujindal, any setting I can change in the optimizedSD files that
          would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image
          but not higher. I do not mind long inference time. I just want to run a
          higher resolution with my poor gtx 970


          Hi, I am trying to modify the code to make it run on a lower VRAM but it
          seems complicated. Will let you know if it works.'
        updatedAt: '2022-08-17T11:32:16.848Z'
      numEdits: 0
      reactions: []
    id: 62fcd1c03724b86da9244d64
    type: comment
  author: basujindal
  content: '> hi basujindal, any setting I can change in the optimizedSD files that
    would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image but
    not higher. I do not mind long inference time. I just want to run a higher resolution
    with my poor gtx 970


    Hi, I am trying to modify the code to make it run on a lower VRAM but it seems
    complicated. Will let you know if it works.'
  created_at: 2022-08-17 10:32:16+00:00
  edited: false
  hidden: false
  id: 62fcd1c03724b86da9244d64
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
      fullname: Basu Jindal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: basujindal
      type: user
    createdAt: '2022-08-18T11:20:07.000Z'
    data:
      edited: true
      editors:
      - basujindal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba23aecece4c4af434210496c2d4ad52.svg
          fullname: Basu Jindal
          isHf: false
          isPro: false
          name: basujindal
          type: user
        html: '<blockquote>

          <p>hi basujindal, any setting I can change in the optimizedSD files that
          would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image
          but not higher. I do not mind long inference time. I just want to run a
          higher resolution with my poor gtx 970</p>

          </blockquote>

          <p>Hi, I have updated the repo. Now, you can generate 512x512 images in
          under 4Gb. Cheers!</p>

          '
        raw: '> hi basujindal, any setting I can change in the optimizedSD files that
          would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image
          but not higher. I do not mind long inference time. I just want to run a
          higher resolution with my poor gtx 970


          Hi, I have updated the repo. Now, you can generate 512x512 images in under
          4Gb. Cheers!'
        updatedAt: '2022-08-19T08:47:32.252Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - cachirulo001
    id: 62fe2067e9061c0170cfe5f5
    type: comment
  author: basujindal
  content: '> hi basujindal, any setting I can change in the optimizedSD files that
    would lower the bar to 4Gb GPU? Using your files I can run a 128x128 image but
    not higher. I do not mind long inference time. I just want to run a higher resolution
    with my poor gtx 970


    Hi, I have updated the repo. Now, you can generate 512x512 images in under 4Gb.
    Cheers!'
  created_at: 2022-08-18 10:20:07+00:00
  edited: true
  hidden: false
  id: 62fe2067e9061c0170cfe5f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4947471b579b770487fdacbaf65f4d4c.svg
      fullname: David B
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: db88
      type: user
    createdAt: '2022-08-18T19:58:02.000Z'
    data:
      edited: true
      editors:
      - db88
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4947471b579b770487fdacbaf65f4d4c.svg
          fullname: David B
          isHf: false
          isPro: false
          name: db88
          type: user
        html: '<p>try also tinkering with downsampling. Integrated the option into
          my colab if you want to play with it (via TXT2IMG). If you''re running it
          local, just use the arg<br><a rel="nofollow" href="https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17#scrollTo=9QnhfmAM0t-X">https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17#scrollTo=9QnhfmAM0t-X</a></p>

          <p>Haven''t played with the option, but thought it could be a good approach
          to try and reduce VRAM usage</p>

          '
        raw: 'try also tinkering with downsampling. Integrated the option into my
          colab if you want to play with it (via TXT2IMG). If you''re running it local,
          just use the arg

          https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17#scrollTo=9QnhfmAM0t-X


          Haven''t played with the option, but thought it could be a good approach
          to try and reduce VRAM usage'
        updatedAt: '2022-08-18T20:11:09.563Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - patrickvonplaten
    id: 62fe99cacc803924ee093cc6
    type: comment
  author: db88
  content: 'try also tinkering with downsampling. Integrated the option into my colab
    if you want to play with it (via TXT2IMG). If you''re running it local, just use
    the arg

    https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17#scrollTo=9QnhfmAM0t-X


    Haven''t played with the option, but thought it could be a good approach to try
    and reduce VRAM usage'
  created_at: 2022-08-18 18:58:02+00:00
  edited: true
  hidden: false
  id: 62fe99cacc803924ee093cc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-08-23T20:37:31.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: '<p>You can also remove the vae encoder part to save some more RAM before
          moving the models on GPU as it''s not needed for the pipe line. E.g.</p>

          <pre><code class="language-py"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span>
          StableDiffusionPipeline


          pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">"CompVis/stabe-diffusion-v1-4"</span>,
          use_auth_token=<span class="hljs-literal">True</span>, torch_dtype=torch.float16)

          <span class="hljs-keyword">del</span> pipe.vae.encoder

          </code></pre>

          '
        raw: 'You can also remove the vae encoder part to save some more RAM before
          moving the models on GPU as it''s not needed for the pipe line. E.g.


          ```py

          import torch

          from diffusers import StableDiffusionPipeline


          pipe = StableDiffusionPipeline.from_pretrained("CompVis/stabe-diffusion-v1-4",
          use_auth_token=True, torch_dtype=torch.float16)

          del pipe.vae.encoder

          ```'
        updatedAt: '2022-08-23T20:37:31.318Z'
      numEdits: 0
      reactions: []
    id: 63053a8bacc17ce4ad35964b
    type: comment
  author: patrickvonplaten
  content: 'You can also remove the vae encoder part to save some more RAM before
    moving the models on GPU as it''s not needed for the pipe line. E.g.


    ```py

    import torch

    from diffusers import StableDiffusionPipeline


    pipe = StableDiffusionPipeline.from_pretrained("CompVis/stabe-diffusion-v1-4",
    use_auth_token=True, torch_dtype=torch.float16)

    del pipe.vae.encoder

    ```'
  created_at: 2022-08-23 19:37:31+00:00
  edited: false
  hidden: false
  id: 63053a8bacc17ce4ad35964b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: CompVis/stable-diffusion-v-1-3-original
repo_type: model
status: open
target_branch: null
title: 'Running on lower Vram '
