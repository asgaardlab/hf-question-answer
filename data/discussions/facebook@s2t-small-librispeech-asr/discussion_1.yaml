!!python/object:huggingface_hub.community.DiscussionWithDetails
author: msis
conflicting_files: null
created_at: 2022-07-29 22:06:27+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655261196766-627ad5c1e75d606b399587d5.jpeg?w=200&h=200&f=face
      fullname: Mohamed Saad Ibn Seddik
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: msis
      type: user
    createdAt: '2022-07-29T23:06:27.000Z'
    data:
      edited: false
      editors:
      - msis
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1655261196766-627ad5c1e75d606b399587d5.jpeg?w=200&h=200&f=face
          fullname: Mohamed Saad Ibn Seddik
          isHf: false
          isPro: false
          name: msis
          type: user
        html: '<p>Can you please share scripts used to train the model ?</p>

          '
        raw: Can you please share scripts used to train the model ?
        updatedAt: '2022-07-29T23:06:27.837Z'
      numEdits: 0
      reactions: []
    id: 62e467f3b1ce3c713f1c7edb
    type: comment
  author: msis
  content: Can you please share scripts used to train the model ?
  created_at: 2022-07-29 22:06:27+00:00
  edited: false
  hidden: false
  id: 62e467f3b1ce3c713f1c7edb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2022-08-01T06:35:07.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;sanchit-gandhi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sanchit-gandhi\"\
          >@<span class=\"underline\">sanchit-gandhi</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;anton-l&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/anton-l\">@<span class=\"\
          underline\">anton-l</span></a></span>\n\n\t</span></span></p>\n"
        raw: cc @sanchit-gandhi @anton-l
        updatedAt: '2022-08-01T06:35:07.928Z'
      numEdits: 0
      reactions: []
    id: 62e7741bf97481d0a13f6185
    type: comment
  author: lysandre
  content: cc @sanchit-gandhi @anton-l
  created_at: 2022-08-01 05:35:07+00:00
  edited: false
  hidden: false
  id: 62e7741bf97481d0a13f6185
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
      fullname: Sanchit Gandhi
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sanchit-gandhi
      type: user
    createdAt: '2022-08-01T09:32:21.000Z'
    data:
      edited: false
      editors:
      - sanchit-gandhi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1653243468328-61f91cf54a8e5a275b2b3e7c.jpeg?w=200&h=200&f=face
          fullname: Sanchit Gandhi
          isHf: true
          isPro: false
          name: sanchit-gandhi
          type: user
        html: "<p>The scripts used to train the original model can be found in the\
          \ fairseq repository: <a rel=\"nofollow\" href=\"https://github.com/facebookresearch/fairseq/tree/main/examples/speech_to_text\"\
          >https://github.com/facebookresearch/fairseq/tree/main/examples/speech_to_text</a></p>\n\
          <p>In order to fine-tune the checkpoint using Transformers \U0001F917, you\
          \ can adapt the example seq2seq training script provided here: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py\"\
          >https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py</a><br>In\
          \ this case, there won't be a need to create an encoder-decoder model from\
          \ scratch. Instead, you can jump right into training and run the script\
          \ directly using the template command: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition#single-gpu-seq2seq\"\
          >https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition#single-gpu-seq2seq</a><br>The\
          \ only change you'll need to make is replacing the <code>model_name_or_path</code>\
          \ arg with the Hub id of the model (<code>facebook/s2t-small-librispeech-asr</code>):</p>\n\
          <pre><code class=\"language-bash\">python run_speech_recognition_seq2seq.py\
          \ \\\n     --nproc_per_node 8 run_speech_recognition_seq2seq.py \\\n   \
          \ --dataset_name=<span class=\"hljs-string\">\"librispeech_asr\"</span>\
          \ \\\n    --model_name_or_path=<span class=\"hljs-string\">\"facebook/s2t-small-librispeech-asr\"\
          </span> \\\n    --dataset_config_name=<span class=\"hljs-string\">\"clean\"\
          </span> \\\n    --train_split_name=<span class=\"hljs-string\">\"train.100\"\
          </span> \\\n    --eval_split_name=<span class=\"hljs-string\">\"validation\"\
          </span> \\\n    --output_dir=<span class=\"hljs-string\">\"./\"</span> \\\
          \n    --preprocessing_num_workers=<span class=\"hljs-string\">\"16\"</span>\
          \ \\\n    --length_column_name=<span class=\"hljs-string\">\"input_length\"\
          </span> \\\n    --overwrite_output_dir \\\n    --num_train_epochs=<span\
          \ class=\"hljs-string\">\"5\"</span> \\\n    --per_device_train_batch_size=<span\
          \ class=\"hljs-string\">\"8\"</span> \\\n    --per_device_eval_batch_size=<span\
          \ class=\"hljs-string\">\"8\"</span> \\\n    --gradient_accumulation_steps=<span\
          \ class=\"hljs-string\">\"8\"</span> \\\n    --learning_rate=<span class=\"\
          hljs-string\">\"3e-4\"</span> \\\n    --warmup_steps=<span class=\"hljs-string\"\
          >\"400\"</span> \\\n    --evaluation_strategy=<span class=\"hljs-string\"\
          >\"steps\"</span> \\\n    --text_column_name=<span class=\"hljs-string\"\
          >\"text\"</span> \\\n    --save_steps=<span class=\"hljs-string\">\"400\"\
          </span> \\\n    --eval_steps=<span class=\"hljs-string\">\"400\"</span>\
          \ \\\n    --logging_steps=<span class=\"hljs-string\">\"10\"</span> \\\n\
          \    --save_total_limit=<span class=\"hljs-string\">\"1\"</span> \\\n  \
          \  --freeze_feature_encoder \\\n    --gradient_checkpointing \\\n    --fp16\
          \ \\\n    --group_by_length \\\n    --predict_with_generate \\\n    --generation_max_length=<span\
          \ class=\"hljs-string\">\"40\"</span> \\\n    --generation_num_beams=<span\
          \ class=\"hljs-string\">\"1\"</span> \\\n    --do_train --do_eval \\\n \
          \   --do_lower_case\n</code></pre>\n"
        raw: "The scripts used to train the original model can be found in the fairseq\
          \ repository: https://github.com/facebookresearch/fairseq/tree/main/examples/speech_to_text\n\
          \nIn order to fine-tune the checkpoint using Transformers \U0001F917, you\
          \ can adapt the example seq2seq training script provided here: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py\n\
          In this case, there won't be a need to create an encoder-decoder model from\
          \ scratch. Instead, you can jump right into training and run the script\
          \ directly using the template command: https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition#single-gpu-seq2seq\n\
          The only change you'll need to make is replacing the `model_name_or_path`\
          \ arg with the Hub id of the model (`facebook/s2t-small-librispeech-asr`):\n\
          ```bash\npython run_speech_recognition_seq2seq.py \\\n \t--nproc_per_node\
          \ 8 run_speech_recognition_seq2seq.py \\\n\t--dataset_name=\"librispeech_asr\"\
          \ \\\n\t--model_name_or_path=\"facebook/s2t-small-librispeech-asr\" \\\n\
          \t--dataset_config_name=\"clean\" \\\n\t--train_split_name=\"train.100\"\
          \ \\\n\t--eval_split_name=\"validation\" \\\n\t--output_dir=\"./\" \\\n\t\
          --preprocessing_num_workers=\"16\" \\\n\t--length_column_name=\"input_length\"\
          \ \\\n\t--overwrite_output_dir \\\n\t--num_train_epochs=\"5\" \\\n\t--per_device_train_batch_size=\"\
          8\" \\\n\t--per_device_eval_batch_size=\"8\" \\\n\t--gradient_accumulation_steps=\"\
          8\" \\\n\t--learning_rate=\"3e-4\" \\\n\t--warmup_steps=\"400\" \\\n\t--evaluation_strategy=\"\
          steps\" \\\n\t--text_column_name=\"text\" \\\n\t--save_steps=\"400\" \\\n\
          \t--eval_steps=\"400\" \\\n\t--logging_steps=\"10\" \\\n\t--save_total_limit=\"\
          1\" \\\n\t--freeze_feature_encoder \\\n\t--gradient_checkpointing \\\n\t\
          --fp16 \\\n\t--group_by_length \\\n\t--predict_with_generate \\\n\t--generation_max_length=\"\
          40\" \\\n\t--generation_num_beams=\"1\" \\\n\t--do_train --do_eval \\\n\t\
          --do_lower_case\n```"
        updatedAt: '2022-08-01T09:32:21.258Z'
      numEdits: 0
      reactions: []
    id: 62e79da536a8e8a827fdd3c6
    type: comment
  author: sanchit-gandhi
  content: "The scripts used to train the original model can be found in the fairseq\
    \ repository: https://github.com/facebookresearch/fairseq/tree/main/examples/speech_to_text\n\
    \nIn order to fine-tune the checkpoint using Transformers \U0001F917, you can\
    \ adapt the example seq2seq training script provided here: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py\n\
    In this case, there won't be a need to create an encoder-decoder model from scratch.\
    \ Instead, you can jump right into training and run the script directly using\
    \ the template command: https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition#single-gpu-seq2seq\n\
    The only change you'll need to make is replacing the `model_name_or_path` arg\
    \ with the Hub id of the model (`facebook/s2t-small-librispeech-asr`):\n```bash\n\
    python run_speech_recognition_seq2seq.py \\\n \t--nproc_per_node 8 run_speech_recognition_seq2seq.py\
    \ \\\n\t--dataset_name=\"librispeech_asr\" \\\n\t--model_name_or_path=\"facebook/s2t-small-librispeech-asr\"\
    \ \\\n\t--dataset_config_name=\"clean\" \\\n\t--train_split_name=\"train.100\"\
    \ \\\n\t--eval_split_name=\"validation\" \\\n\t--output_dir=\"./\" \\\n\t--preprocessing_num_workers=\"\
    16\" \\\n\t--length_column_name=\"input_length\" \\\n\t--overwrite_output_dir\
    \ \\\n\t--num_train_epochs=\"5\" \\\n\t--per_device_train_batch_size=\"8\" \\\n\
    \t--per_device_eval_batch_size=\"8\" \\\n\t--gradient_accumulation_steps=\"8\"\
    \ \\\n\t--learning_rate=\"3e-4\" \\\n\t--warmup_steps=\"400\" \\\n\t--evaluation_strategy=\"\
    steps\" \\\n\t--text_column_name=\"text\" \\\n\t--save_steps=\"400\" \\\n\t--eval_steps=\"\
    400\" \\\n\t--logging_steps=\"10\" \\\n\t--save_total_limit=\"1\" \\\n\t--freeze_feature_encoder\
    \ \\\n\t--gradient_checkpointing \\\n\t--fp16 \\\n\t--group_by_length \\\n\t--predict_with_generate\
    \ \\\n\t--generation_max_length=\"40\" \\\n\t--generation_num_beams=\"1\" \\\n\
    \t--do_train --do_eval \\\n\t--do_lower_case\n```"
  created_at: 2022-08-01 08:32:21+00:00
  edited: false
  hidden: false
  id: 62e79da536a8e8a827fdd3c6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: facebook/s2t-small-librispeech-asr
repo_type: model
status: open
target_branch: null
title: Training steps
