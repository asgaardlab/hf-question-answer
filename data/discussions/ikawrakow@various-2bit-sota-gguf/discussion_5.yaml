!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Perpetuity7
conflicting_files: null
created_at: 2024-01-17 11:05:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1fbb6a84d2274b50227a4d28b8482155.svg
      fullname: SC.L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Perpetuity7
      type: user
    createdAt: '2024-01-17T11:05:54.000Z'
    data:
      edited: true
      editors:
      - Perpetuity7
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.901462972164154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1fbb6a84d2274b50227a4d28b8482155.svg
          fullname: SC.L
          isHf: false
          isPro: false
          name: Perpetuity7
          type: user
        html: "<p>The size of saily-100b-Q2_K.gguf is 49.6 GB. I expect that I can\
          \ load the full layer on rtx 3090, 4090 dual graphic card or Google Colab\
          \ Pro if you quantize this model with IQ2_XXS (bpw2.06).</p>\n<p>And the\
          \ size of saily_220b.Q2_K.gguf is also 87.80 GB, so I think it will be possible\
          \ to load the full layer on runpod a100 80gb if you quantize it with IQ2_XXS\
          \ (bpw2.06).</p>\n<p>Besides that, my personal wish is that you would make\
          \ mixtral-7b\xD78-instruct-LimaRP-zloss and Openbuddy-mixtral-7b\xD78-v16.3-32k\
          \ models into IQ2_XSS GGUF as well. This seems to be useful for 16GB graphics\
          \ card users. Thank you.</p>\n"
        raw: "The size of saily-100b-Q2_K.gguf is 49.6 GB. I expect that I can load\
          \ the full layer on rtx 3090, 4090 dual graphic card or Google Colab Pro\
          \ if you quantize this model with IQ2_XXS (bpw2.06).\n\nAnd the size of\
          \ saily_220b.Q2_K.gguf is also 87.80 GB, so I think it will be possible\
          \ to load the full layer on runpod a100 80gb if you quantize it with IQ2_XXS\
          \ (bpw2.06).\n\nBesides that, my personal wish is that you would make mixtral-7b\xD7\
          8-instruct-LimaRP-zloss and Openbuddy-mixtral-7b\xD78-v16.3-32k models into\
          \ IQ2_XSS GGUF as well. This seems to be useful for 16GB graphics card users.\
          \ Thank you."
        updatedAt: '2024-01-17T11:10:15.331Z'
      numEdits: 2
      reactions: []
    id: 65a7b4925e3029d4d5422329
    type: comment
  author: Perpetuity7
  content: "The size of saily-100b-Q2_K.gguf is 49.6 GB. I expect that I can load\
    \ the full layer on rtx 3090, 4090 dual graphic card or Google Colab Pro if you\
    \ quantize this model with IQ2_XXS (bpw2.06).\n\nAnd the size of saily_220b.Q2_K.gguf\
    \ is also 87.80 GB, so I think it will be possible to load the full layer on runpod\
    \ a100 80gb if you quantize it with IQ2_XXS (bpw2.06).\n\nBesides that, my personal\
    \ wish is that you would make mixtral-7b\xD78-instruct-LimaRP-zloss and Openbuddy-mixtral-7b\xD7\
    8-v16.3-32k models into IQ2_XSS GGUF as well. This seems to be useful for 16GB\
    \ graphics card users. Thank you."
  created_at: 2024-01-17 11:05:54+00:00
  edited: true
  hidden: false
  id: 65a7b4925e3029d4d5422329
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-20T08:14:05.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9420431852340698
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<p>100B+ models go beyond the computational resources I have available.
          But I have now contributed everything that is required<br>to prepare such
          models to the <code>llama.cpp</code> project, so hopefully someone who has
          access to computers with more RAM/VRAM/compute can do this.</p>

          '
        raw: '100B+ models go beyond the computational resources I have available.
          But I have now contributed everything that is required

          to prepare such models to the `llama.cpp` project, so hopefully someone
          who has access to computers with more RAM/VRAM/compute can do this.'
        updatedAt: '2024-01-20T08:14:05.019Z'
      numEdits: 0
      reactions: []
    id: 65ab80cd3e7c8dcbbef5a8a4
    type: comment
  author: ikawrakow
  content: '100B+ models go beyond the computational resources I have available. But
    I have now contributed everything that is required

    to prepare such models to the `llama.cpp` project, so hopefully someone who has
    access to computers with more RAM/VRAM/compute can do this.'
  created_at: 2024-01-20 08:14:05+00:00
  edited: false
  hidden: false
  id: 65ab80cd3e7c8dcbbef5a8a4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: ikawrakow/various-2bit-sota-gguf
repo_type: model
status: open
target_branch: null
title: '[Model request] Saily 100b, Saily 220b'
