!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Shqmil
conflicting_files: null
created_at: 2024-01-05 13:25:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e54b5c1642b6965e49d585d923e8ff40.svg
      fullname: 'Ibrahim '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shqmil
      type: user
    createdAt: '2024-01-05T13:25:07.000Z'
    data:
      edited: true
      editors:
      - Shqmil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7801902890205383
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e54b5c1642b6965e49d585d923e8ff40.svg
          fullname: 'Ibrahim '
          isHf: false
          isPro: false
          name: Shqmil
          type: user
        html: '<p>Can you quantize a 3b model rocket? For Chat </p>

          '
        raw: 'Can you quantize a 3b model rocket? For Chat '
        updatedAt: '2024-01-05T13:26:45.406Z'
      numEdits: 1
      reactions: []
    id: 65980333ce92304a717aca3e
    type: comment
  author: Shqmil
  content: 'Can you quantize a 3b model rocket? For Chat '
  created_at: 2024-01-05 13:25:07+00:00
  edited: true
  hidden: false
  id: 65980333ce92304a717aca3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-05T14:05:58.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9506418704986572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<p>So far I have only looked into improving quantization of general
          purpose models. The method I use utilizes an "importance matrix" that helps
          ensuring more accurate quantized values for more important model weights.
          This matrix is derived from a calibration run on a training dataset. For
          base models one can basically use any sufficiently broad text dataset. My
          best guess is that for chat/instruct tuned models I need to find a good
          training dataset that is geared towards chat/instruct tuning.</p>

          <p>So, in short, yes, I could do, but I need some time to get into quantizing
          this type of model. </p>

          '
        raw: 'So far I have only looked into improving quantization of general purpose
          models. The method I use utilizes an "importance matrix" that helps ensuring
          more accurate quantized values for more important model weights. This matrix
          is derived from a calibration run on a training dataset. For base models
          one can basically use any sufficiently broad text dataset. My best guess
          is that for chat/instruct tuned models I need to find a good training dataset
          that is geared towards chat/instruct tuning.


          So, in short, yes, I could do, but I need some time to get into quantizing
          this type of model. '
        updatedAt: '2024-01-05T14:05:58.234Z'
      numEdits: 0
      reactions: []
    id: 65980cc61b663b1e53aa8bb5
    type: comment
  author: ikawrakow
  content: 'So far I have only looked into improving quantization of general purpose
    models. The method I use utilizes an "importance matrix" that helps ensuring more
    accurate quantized values for more important model weights. This matrix is derived
    from a calibration run on a training dataset. For base models one can basically
    use any sufficiently broad text dataset. My best guess is that for chat/instruct
    tuned models I need to find a good training dataset that is geared towards chat/instruct
    tuning.


    So, in short, yes, I could do, but I need some time to get into quantizing this
    type of model. '
  created_at: 2024-01-05 14:05:58+00:00
  edited: false
  hidden: false
  id: 65980cc61b663b1e53aa8bb5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-05T15:42:58.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592844247817993
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Can you go for a 2 bit version of Mixtral Instruct? We don''t really
          use Mixtral base usually</p>

          '
        raw: Can you go for a 2 bit version of Mixtral Instruct? We don't really use
          Mixtral base usually
        updatedAt: '2024-01-05T15:42:58.683Z'
      numEdits: 0
      reactions: []
    id: 659823821207310bb8ba2874
    type: comment
  author: TheYuriLover
  content: Can you go for a 2 bit version of Mixtral Instruct? We don't really use
    Mixtral base usually
  created_at: 2024-01-05 15:42:58+00:00
  edited: false
  hidden: false
  id: 659823821207310bb8ba2874
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-05T17:29:52.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9727404713630676
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<p>Same answer as above: give me some time to get into quantizing instruct
          models before I start posting. </p>

          '
        raw: 'Same answer as above: give me some time to get into quantizing instruct
          models before I start posting. '
        updatedAt: '2024-01-05T17:29:52.894Z'
      numEdits: 0
      reactions: []
    id: 65983c90b01e6e0c71c73911
    type: comment
  author: ikawrakow
  content: 'Same answer as above: give me some time to get into quantizing instruct
    models before I start posting. '
  created_at: 2024-01-05 17:29:52+00:00
  edited: false
  hidden: false
  id: 65983c90b01e6e0c71c73911
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a451288e780acc42a7d5709ec7d3370.svg
      fullname: "D\xE4mpfchen"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dampfinchen
      type: user
    createdAt: '2024-01-05T19:41:04.000Z'
    data:
      edited: false
      editors:
      - Dampfinchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9658644795417786
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a451288e780acc42a7d5709ec7d3370.svg
          fullname: "D\xE4mpfchen"
          isHf: false
          isPro: false
          name: Dampfinchen
          type: user
        html: '<blockquote>

          <p>Can you go for a 2 bit version of Mixtral Instruct? We don''t really
          use Mixtral base usually</p>

          </blockquote>

          <p>I''d advise patience. When Ika''s PR is merged in LLama.cpp, every new
          model released after that will benefit from the recent changes. TheBloke
          might requant some older models too, if requested.</p>

          '
        raw: '> Can you go for a 2 bit version of Mixtral Instruct? We don''t really
          use Mixtral base usually


          I''d advise patience. When Ika''s PR is merged in LLama.cpp, every new model
          released after that will benefit from the recent changes. TheBloke might
          requant some older models too, if requested.

          '
        updatedAt: '2024-01-05T19:41:04.270Z'
      numEdits: 0
      reactions: []
    id: 65985b5058608c40442a0105
    type: comment
  author: Dampfinchen
  content: '> Can you go for a 2 bit version of Mixtral Instruct? We don''t really
    use Mixtral base usually


    I''d advise patience. When Ika''s PR is merged in LLama.cpp, every new model released
    after that will benefit from the recent changes. TheBloke might requant some older
    models too, if requested.

    '
  created_at: 2024-01-05 19:41:04+00:00
  edited: false
  hidden: false
  id: 65985b5058608c40442a0105
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-08T16:49:56.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9025400876998901
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: "<p>The Mixtral-instruct-8x7b quantizations are now posted.</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;Shqmil&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/Shqmil\">@<span class=\"underline\"\
          >Shqmil</span></a></span>\n\n\t</span></span> The interest for 2-bit quants\
          \ for rocket-3b is in the very small 2-bit version (2.06 bits per weight),\
          \ or in the better quality but larger (2.56 bits per weight) 2-bit version?</p>\n"
        raw: 'The Mixtral-instruct-8x7b quantizations are now posted.


          @Shqmil The interest for 2-bit quants for rocket-3b is in the very small
          2-bit version (2.06 bits per weight), or in the better quality but larger
          (2.56 bits per weight) 2-bit version?'
        updatedAt: '2024-01-08T16:49:56.041Z'
      numEdits: 0
      reactions: []
    id: 659c27b4349f26f2713b402f
    type: comment
  author: ikawrakow
  content: 'The Mixtral-instruct-8x7b quantizations are now posted.


    @Shqmil The interest for 2-bit quants for rocket-3b is in the very small 2-bit
    version (2.06 bits per weight), or in the better quality but larger (2.56 bits
    per weight) 2-bit version?'
  created_at: 2024-01-08 16:49:56+00:00
  edited: false
  hidden: false
  id: 659c27b4349f26f2713b402f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e54b5c1642b6965e49d585d923e8ff40.svg
      fullname: 'Ibrahim '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shqmil
      type: user
    createdAt: '2024-01-08T18:19:54.000Z'
    data:
      edited: true
      editors:
      - Shqmil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9937471151351929
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e54b5c1642b6965e49d585d923e8ff40.svg
          fullname: 'Ibrahim '
          isHf: false
          isPro: false
          name: Shqmil
          type: user
        html: '<p>If it''s not hard for you.?A smaller version would have been better.
          Thank you very much. </p>

          '
        raw: 'If it''s not hard for you.?A smaller version would have been better.
          Thank you very much. '
        updatedAt: '2024-01-08T18:20:34.295Z'
      numEdits: 1
      reactions: []
    id: 659c3ccaac5155f74d90d906
    type: comment
  author: Shqmil
  content: 'If it''s not hard for you.?A smaller version would have been better. Thank
    you very much. '
  created_at: 2024-01-08 18:19:54+00:00
  edited: true
  hidden: false
  id: 659c3ccaac5155f74d90d906
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/298bb4ee30144181bf12af21f846c4dd.svg
      fullname: kas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shing3232
      type: user
    createdAt: '2024-01-08T19:28:10.000Z'
    data:
      edited: false
      editors:
      - shing3232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.839338481426239
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/298bb4ee30144181bf12af21f846c4dd.svg
          fullname: kas
          isHf: false
          isPro: false
          name: shing3232
          type: user
        html: "<blockquote>\n<p>The Mixtral-instruct-8x7b quantizations are now posted.</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;Shqmil&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Shqmil\">@<span class=\"\
          underline\">Shqmil</span></a></span>\n\n\t</span></span> The interest for\
          \ 2-bit quants for rocket-3b is in the very small 2-bit version (2.06 bits\
          \ per weight), or in the better quality but larger (2.56 bits per weight)\
          \ 2-bit version?</p>\n</blockquote>\n<p>based on ranking from this post\
          \ <a rel=\"nofollow\" href=\"https://www.reddit.com/r/LocalLLaMA/comments/1916896/llm_comparisontest_confirm_leaderboard_big_news/\"\
          >https://www.reddit.com/r/LocalLLaMA/comments/1916896/llm_comparisontest_confirm_leaderboard_big_news/</a></p>\n\
          <p><a href=\"https://huggingface.co/cloudyu/Mixtral_34Bx2_MoE_60B\">https://huggingface.co/cloudyu/Mixtral_34Bx2_MoE_60B</a><br>Would\
          \ be interesting if it get a 2bit quant as well, but \"<a href=\"https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF&quot;\"\
          >https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF\"</a> Q2K  isnt\
          \ all that interesting.<br>but Q3MK is too big for my 24GB GPU.</p>\n"
        raw: "> The Mixtral-instruct-8x7b quantizations are now posted.\n> \n> @Shqmil\
          \ The interest for 2-bit quants for rocket-3b is in the very small 2-bit\
          \ version (2.06 bits per weight), or in the better quality but larger (2.56\
          \ bits per weight) 2-bit version?\n\nbased on ranking from this post https://www.reddit.com/r/LocalLLaMA/comments/1916896/llm_comparisontest_confirm_leaderboard_big_news/\n\
          \nhttps://huggingface.co/cloudyu/Mixtral_34Bx2_MoE_60B \nWould be interesting\
          \ if it get a 2bit quant as well, but \"https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF\"\
          \ Q2K  isnt all that interesting.\nbut Q3MK is too big for my 24GB GPU."
        updatedAt: '2024-01-08T19:28:10.332Z'
      numEdits: 0
      reactions: []
    id: 659c4cca1d398a23814947fa
    type: comment
  author: shing3232
  content: "> The Mixtral-instruct-8x7b quantizations are now posted.\n> \n> @Shqmil\
    \ The interest for 2-bit quants for rocket-3b is in the very small 2-bit version\
    \ (2.06 bits per weight), or in the better quality but larger (2.56 bits per weight)\
    \ 2-bit version?\n\nbased on ranking from this post https://www.reddit.com/r/LocalLLaMA/comments/1916896/llm_comparisontest_confirm_leaderboard_big_news/\n\
    \nhttps://huggingface.co/cloudyu/Mixtral_34Bx2_MoE_60B \nWould be interesting\
    \ if it get a 2bit quant as well, but \"https://huggingface.co/TheBloke/Mixtral_34Bx2_MoE_60B-GGUF\"\
    \ Q2K  isnt all that interesting.\nbut Q3MK is too big for my 24GB GPU."
  created_at: 2024-01-08 19:28:10+00:00
  edited: false
  hidden: false
  id: 659c4cca1d398a23814947fa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-08T21:17:28.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.898531436920166
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>can you go for a Q5_K_M version for mixtral instruct too? </p>

          '
        raw: 'can you go for a Q5_K_M version for mixtral instruct too? '
        updatedAt: '2024-01-08T21:17:28.987Z'
      numEdits: 0
      reactions: []
    id: 659c6668bc65f1e59de5976a
    type: comment
  author: TheYuriLover
  content: 'can you go for a Q5_K_M version for mixtral instruct too? '
  created_at: 2024-01-08 21:17:28+00:00
  edited: false
  hidden: false
  id: 659c6668bc65f1e59de5976a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/298bb4ee30144181bf12af21f846c4dd.svg
      fullname: kas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shing3232
      type: user
    createdAt: '2024-01-09T04:07:01.000Z'
    data:
      edited: false
      editors:
      - shing3232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8512281179428101
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/298bb4ee30144181bf12af21f846c4dd.svg
          fullname: kas
          isHf: false
          isPro: false
          name: shing3232
          type: user
        html: '<blockquote>

          <p>can you go for a Q5_K_M version for mixtral instruct too?</p>

          </blockquote>

          <p><a href="https://huggingface.co/ikawrakow/mixtral-instruct-8x7b-quantized-gguf">https://huggingface.co/ikawrakow/mixtral-instruct-8x7b-quantized-gguf</a><br>I
          through it already existed</p>

          '
        raw: '> can you go for a Q5_K_M version for mixtral instruct too?


          https://huggingface.co/ikawrakow/mixtral-instruct-8x7b-quantized-gguf

          I through it already existed'
        updatedAt: '2024-01-09T04:07:01.831Z'
      numEdits: 0
      reactions: []
    id: 659cc665ad9aec41859d60c1
    type: comment
  author: shing3232
  content: '> can you go for a Q5_K_M version for mixtral instruct too?


    https://huggingface.co/ikawrakow/mixtral-instruct-8x7b-quantized-gguf

    I through it already existed'
  created_at: 2024-01-09 04:07:01+00:00
  edited: false
  hidden: false
  id: 659cc665ad9aec41859d60c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-09T04:09:51.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9293892979621887
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>No there is Q5_K_S but not Q5_K_M</p>

          '
        raw: No there is Q5_K_S but not Q5_K_M
        updatedAt: '2024-01-09T04:09:51.093Z'
      numEdits: 0
      reactions: []
    id: 659cc70f7451fd38786266b6
    type: comment
  author: TheYuriLover
  content: No there is Q5_K_S but not Q5_K_M
  created_at: 2024-01-09 04:09:51+00:00
  edited: false
  hidden: false
  id: 659cc70f7451fd38786266b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-09T06:00:04.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9395161271095276
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<p>For Mixtral-instruct-8x7b Q5_K_M has about the same performance
          as Q5_K_S, so I did not publish. Where I live 100 Mb/s is the best one can
          get. The way HF is setup for these models (using git-lfs), a 35 GB file
          requires ~70 GB to be uploaded, so that takes nearly 2 hours with my Internet
          speed.  </p>

          '
        raw: 'For Mixtral-instruct-8x7b Q5_K_M has about the same performance as Q5_K_S,
          so I did not publish. Where I live 100 Mb/s is the best one can get. The
          way HF is setup for these models (using git-lfs), a 35 GB file requires
          ~70 GB to be uploaded, so that takes nearly 2 hours with my Internet speed.  '
        updatedAt: '2024-01-09T06:00:04.543Z'
      numEdits: 0
      reactions: []
    id: 659ce0e4e942a8f717c112d6
    type: comment
  author: ikawrakow
  content: 'For Mixtral-instruct-8x7b Q5_K_M has about the same performance as Q5_K_S,
    so I did not publish. Where I live 100 Mb/s is the best one can get. The way HF
    is setup for these models (using git-lfs), a 35 GB file requires ~70 GB to be
    uploaded, so that takes nearly 2 hours with my Internet speed.  '
  created_at: 2024-01-09 06:00:04+00:00
  edited: false
  hidden: false
  id: 659ce0e4e942a8f717c112d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-09T06:03:36.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9174530506134033
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>Thank God TheBloke exists, he''s the man to do this really tedious
          work for us lmao</p>

          '
        raw: Thank God TheBloke exists, he's the man to do this really tedious work
          for us lmao
        updatedAt: '2024-01-09T06:03:36.780Z'
      numEdits: 0
      reactions: []
    id: 659ce1b877ac6f1bf5002b29
    type: comment
  author: TheYuriLover
  content: Thank God TheBloke exists, he's the man to do this really tedious work
    for us lmao
  created_at: 2024-01-09 06:03:36+00:00
  edited: false
  hidden: false
  id: 659ce1b877ac6f1bf5002b29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-09T06:28:11.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9274623990058899
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<blockquote>

          <p>Can you quantize a 3b model rocket? For Chat</p>

          </blockquote>

          <p>Have added two 2-bit models (rocket-3b-2.31bpw.gguf and rocket-3b-2.76bpw.gguf).
          But their perplexity is so high, especially for the 2.31 bpw model, that
          I have doubts they will be useful for anything. </p>

          '
        raw: '> Can you quantize a 3b model rocket? For Chat


          Have added two 2-bit models (rocket-3b-2.31bpw.gguf and rocket-3b-2.76bpw.gguf).
          But their perplexity is so high, especially for the 2.31 bpw model, that
          I have doubts they will be useful for anything. '
        updatedAt: '2024-01-09T06:28:11.675Z'
      numEdits: 0
      reactions: []
    id: 659ce77b0626675b9bec2e44
    type: comment
  author: ikawrakow
  content: '> Can you quantize a 3b model rocket? For Chat


    Have added two 2-bit models (rocket-3b-2.31bpw.gguf and rocket-3b-2.76bpw.gguf).
    But their perplexity is so high, especially for the 2.31 bpw model, that I have
    doubts they will be useful for anything. '
  created_at: 2024-01-09 06:28:11+00:00
  edited: false
  hidden: false
  id: 659ce77b0626675b9bec2e44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/298bb4ee30144181bf12af21f846c4dd.svg
      fullname: kas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shing3232
      type: user
    createdAt: '2024-01-09T09:23:36.000Z'
    data:
      edited: false
      editors:
      - shing3232
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.944745659828186
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/298bb4ee30144181bf12af21f846c4dd.svg
          fullname: kas
          isHf: false
          isPro: false
          name: shing3232
          type: user
        html: '<blockquote>

          <p>For Mixtral-instruct-8x7b Q5_K_M has about the same performance as Q5_K_S,
          so I did not publish. Where I live 100 Mb/s is the best one can get. The
          way HF is setup for these models (using git-lfs), a 35 GB file requires
          ~70 GB to be uploaded, so that takes nearly 2 hours with my Internet speed.</p>

          </blockquote>

          <p>Wow, 100M is horrible slow by today''s standard. I guess you live somewhere
          in US.</p>

          '
        raw: '> For Mixtral-instruct-8x7b Q5_K_M has about the same performance as
          Q5_K_S, so I did not publish. Where I live 100 Mb/s is the best one can
          get. The way HF is setup for these models (using git-lfs), a 35 GB file
          requires ~70 GB to be uploaded, so that takes nearly 2 hours with my Internet
          speed.


          Wow, 100M is horrible slow by today''s standard. I guess you live somewhere
          in US.'
        updatedAt: '2024-01-09T09:23:36.661Z'
      numEdits: 0
      reactions: []
    id: 659d1098c33ebf86bb8091cd
    type: comment
  author: shing3232
  content: '> For Mixtral-instruct-8x7b Q5_K_M has about the same performance as Q5_K_S,
    so I did not publish. Where I live 100 Mb/s is the best one can get. The way HF
    is setup for these models (using git-lfs), a 35 GB file requires ~70 GB to be
    uploaded, so that takes nearly 2 hours with my Internet speed.


    Wow, 100M is horrible slow by today''s standard. I guess you live somewhere in
    US.'
  created_at: 2024-01-09 09:23:36+00:00
  edited: false
  hidden: false
  id: 659d1098c33ebf86bb8091cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
      fullname: Iwan Kawrakow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: ikawrakow
      type: user
    createdAt: '2024-01-09T10:02:44.000Z'
    data:
      edited: false
      editors:
      - ikawrakow
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9780839085578918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98d7cbc7bf4cbf4f2810cbc0a1a34d64.svg
          fullname: Iwan Kawrakow
          isHf: false
          isPro: false
          name: ikawrakow
          type: user
        html: '<blockquote>

          <p>Wow, 100M is horrible slow by today''s standard. I guess you live somewhere
          in US.</p>

          </blockquote>

          <p>Haha. It is Italy, actually. They claim my house is too far from the
          next switch station. While I can clearly see how they do network shaping.
          It starts way better than 100 Mb/s, then drops well below 100 Mb/s, and
          then, on a larger data transfer, stabilizes around 96 Mb/s. Why they wouldn''t
          sell me better than 100 Mb/s is beyond me. But my Italian is not quite there
          yet, so me trying to have a conversation with their representatives in my
          broken Italian or their broken English doesn''t really help.  </p>

          '
        raw: '> Wow, 100M is horrible slow by today''s standard. I guess you live
          somewhere in US.


          Haha. It is Italy, actually. They claim my house is too far from the next
          switch station. While I can clearly see how they do network shaping. It
          starts way better than 100 Mb/s, then drops well below 100 Mb/s, and then,
          on a larger data transfer, stabilizes around 96 Mb/s. Why they wouldn''t
          sell me better than 100 Mb/s is beyond me. But my Italian is not quite there
          yet, so me trying to have a conversation with their representatives in my
          broken Italian or their broken English doesn''t really help.  '
        updatedAt: '2024-01-09T10:02:44.287Z'
      numEdits: 0
      reactions: []
    id: 659d19c40e4e5472eea20308
    type: comment
  author: ikawrakow
  content: '> Wow, 100M is horrible slow by today''s standard. I guess you live somewhere
    in US.


    Haha. It is Italy, actually. They claim my house is too far from the next switch
    station. While I can clearly see how they do network shaping. It starts way better
    than 100 Mb/s, then drops well below 100 Mb/s, and then, on a larger data transfer,
    stabilizes around 96 Mb/s. Why they wouldn''t sell me better than 100 Mb/s is
    beyond me. But my Italian is not quite there yet, so me trying to have a conversation
    with their representatives in my broken Italian or their broken English doesn''t
    really help.  '
  created_at: 2024-01-09 10:02:44+00:00
  edited: false
  hidden: false
  id: 659d19c40e4e5472eea20308
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65dac40d7559de334db74c1eefbddf67.svg
      fullname: Hilda Cran May
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cran-May
      type: user
    createdAt: '2024-01-10T15:50:52.000Z'
    data:
      edited: false
      editors:
      - Cran-May
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.764091968536377
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65dac40d7559de334db74c1eefbddf67.svg
          fullname: Hilda Cran May
          isHf: false
          isPro: false
          name: Cran-May
          type: user
        html: '<p><a href="https://huggingface.co/OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k">https://huggingface.co/OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k</a><br>Can
          do a try? It offers muti-language supports.</p>

          '
        raw: 'https://huggingface.co/OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k

          Can do a try? It offers muti-language supports.'
        updatedAt: '2024-01-10T15:50:52.257Z'
      numEdits: 0
      reactions: []
    id: 659ebcdc9630ed72c82869d6
    type: comment
  author: Cran-May
  content: 'https://huggingface.co/OpenBuddy/openbuddy-mixtral-7bx8-v16.3-32k

    Can do a try? It offers muti-language supports.'
  created_at: 2024-01-10 15:50:52+00:00
  edited: false
  hidden: false
  id: 659ebcdc9630ed72c82869d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/104c4cbcef28930bee2655f564057a20.svg
      fullname: Abdullah Abdelrhim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: abdullah
      type: user
    createdAt: '2024-01-11T18:59:01.000Z'
    data:
      edited: false
      editors:
      - abdullah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8320308327674866
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/104c4cbcef28930bee2655f564057a20.svg
          fullname: Abdullah Abdelrhim
          isHf: false
          isPro: false
          name: abdullah
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ikawrakow&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ikawrakow\">@<span class=\"\
          underline\">ikawrakow</span></a></span>\n\n\t</span></span><br>Did you tried</p>\n\
          <pre><code class=\"language-python\">os.environ[<span class=\"hljs-string\"\
          >\"HF_HUB_ENABLE_HF_TRANSFER\"</span>] = <span class=\"hljs-string\">\"\
          1\"</span>\n</code></pre>\n<p>from <a rel=\"nofollow\" href=\"https://github.com/huggingface/hf_transfer\"\
          >https://github.com/huggingface/hf_transfer</a></p>\n"
        raw: "@ikawrakow \nDid you tried\n```python\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"\
          ] = \"1\"\n```\n\nfrom https://github.com/huggingface/hf_transfer"
        updatedAt: '2024-01-11T18:59:01.316Z'
      numEdits: 0
      reactions: []
    id: 65a03a7564520347aa40cc5a
    type: comment
  author: abdullah
  content: "@ikawrakow \nDid you tried\n```python\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"\
    ] = \"1\"\n```\n\nfrom https://github.com/huggingface/hf_transfer"
  created_at: 2024-01-11 18:59:01+00:00
  edited: false
  hidden: false
  id: 65a03a7564520347aa40cc5a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: ikawrakow/various-2bit-sota-gguf
repo_type: model
status: open
target_branch: null
title: Please 3b model rocket 2bit?
