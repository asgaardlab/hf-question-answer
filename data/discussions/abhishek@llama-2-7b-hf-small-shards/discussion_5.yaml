!!python/object:huggingface_hub.community.DiscussionWithDetails
author: EviIgenius
conflicting_files: null
created_at: 2023-11-05 16:37:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e36af286cce2ea9f68a5f25e67c5ffb3.svg
      fullname: Dev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: EviIgenius
      type: user
    createdAt: '2023-11-05T16:37:23.000Z'
    data:
      edited: false
      editors:
      - EviIgenius
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9226967692375183
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e36af286cce2ea9f68a5f25e67c5ffb3.svg
          fullname: Dev
          isHf: false
          isPro: false
          name: EviIgenius
          type: user
        html: '<p>Hey Abhishek , firstly thank you so much for your tutorial.<br>Noob
          Alert!!</p>

          <p>I have fine tuned the LlaMA and mistral sharded model for fine tuning
          on google colab and saved the same to hf. Now I am totally clueless about
          how can I run my fine tune model in google colab, also how can I conver
          the same into ggml/gguf format and quantize it into 4 bits.</p>

          <p>I don''t expect a full tutorial or answer , but please do give me some
          resource or reference :)</p>

          '
        raw: "Hey Abhishek , firstly thank you so much for your tutorial.\r\nNoob\
          \ Alert!!\r\n\r\nI have fine tuned the LlaMA and mistral sharded model for\
          \ fine tuning on google colab and saved the same to hf. Now I am totally\
          \ clueless about how can I run my fine tune model in google colab, also\
          \ how can I conver the same into ggml/gguf format and quantize it into 4\
          \ bits.\r\n\r\nI don't expect a full tutorial or answer , but please do\
          \ give me some resource or reference :)"
        updatedAt: '2023-11-05T16:37:23.879Z'
      numEdits: 0
      reactions: []
    id: 6547c4c3bd25cef7d17503b0
    type: comment
  author: EviIgenius
  content: "Hey Abhishek , firstly thank you so much for your tutorial.\r\nNoob Alert!!\r\
    \n\r\nI have fine tuned the LlaMA and mistral sharded model for fine tuning on\
    \ google colab and saved the same to hf. Now I am totally clueless about how can\
    \ I run my fine tune model in google colab, also how can I conver the same into\
    \ ggml/gguf format and quantize it into 4 bits.\r\n\r\nI don't expect a full tutorial\
    \ or answer , but please do give me some resource or reference :)"
  created_at: 2023-11-05 16:37:23+00:00
  edited: false
  hidden: false
  id: 6547c4c3bd25cef7d17503b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8a17611cc0fa3225f4cf62a64f8e6005.svg
      fullname: Brian Donatiello
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tiwanaca
      type: user
    createdAt: '2023-11-15T21:46:25.000Z'
    data:
      edited: true
      editors:
      - tiwanaca
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3428947925567627
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8a17611cc0fa3225f4cf62a64f8e6005.svg
          fullname: Brian Donatiello
          isHf: false
          isPro: false
          name: tiwanaca
          type: user
        html: '<p>First run this:<br>from peft import PeftConfig, PeftModel<br>from
          transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>hf_token = "your_hf_token"</p>

          <p>base_model_name = "abhishek/llama-2-7b-hf-small-shards" #path/to/your/model/or/name/on/hub"<br>adapter_model_name
          = "your repo"</p>

          <p>model = AutoModelForCausalLM.from_pretrained(base_model_name)<br>model
          = PeftModel.from_pretrained(model, adapter_model_name, use_auth_token=hf_token)</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(base_model_name)</p>

          <p>Then run this:</p>

          <h1 id="example-text-input">Example text input</h1>

          <p>input_text = "your message"<br>input_ids = tokenizer.encode(input_text,
          return_tensors=''pt'')</p>

          <h1 id="generate-text-using-keyword-arguments">Generate text using keyword
          arguments</h1>

          <p>outputs = model.generate(<br>    input_ids=input_ids,<br>    max_length=200,  #
          You can increase this value<br>    no_repeat_ngram_size=2,<br>    early_stopping=True,<br>    num_return_sequences=1<br>)</p>

          <p>generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)</p>

          <p>print(generated_text)</p>

          '
        raw: "First run this:\nfrom peft import PeftConfig, PeftModel\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\nhf_token = \"your_hf_token\"\
          \n\nbase_model_name = \"abhishek/llama-2-7b-hf-small-shards\" #path/to/your/model/or/name/on/hub\"\
          \nadapter_model_name = \"your repo\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\n\
          model = PeftModel.from_pretrained(model, adapter_model_name, use_auth_token=hf_token)\n\
          \ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nThen run\
          \ this:\n# Example text input\ninput_text = \"your message\"\ninput_ids\
          \ = tokenizer.encode(input_text, return_tensors='pt')\n\n# Generate text\
          \ using keyword arguments\noutputs = model.generate(\n    input_ids=input_ids,\n\
          \    max_length=200,  # You can increase this value\n    no_repeat_ngram_size=2,\n\
          \    early_stopping=True,\n    num_return_sequences=1\n)\n\ngenerated_text\
          \ = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(generated_text)"
        updatedAt: '2023-11-15T21:46:43.700Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - abrahamn
    id: 65553c319dc61e22c50499ff
    type: comment
  author: tiwanaca
  content: "First run this:\nfrom peft import PeftConfig, PeftModel\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\n\nhf_token = \"your_hf_token\"\n\
    \nbase_model_name = \"abhishek/llama-2-7b-hf-small-shards\" #path/to/your/model/or/name/on/hub\"\
    \nadapter_model_name = \"your repo\"\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_name)\n\
    model = PeftModel.from_pretrained(model, adapter_model_name, use_auth_token=hf_token)\n\
    \ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\nThen run this:\n\
    # Example text input\ninput_text = \"your message\"\ninput_ids = tokenizer.encode(input_text,\
    \ return_tensors='pt')\n\n# Generate text using keyword arguments\noutputs = model.generate(\n\
    \    input_ids=input_ids,\n    max_length=200,  # You can increase this value\n\
    \    no_repeat_ngram_size=2,\n    early_stopping=True,\n    num_return_sequences=1\n\
    )\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\
    \nprint(generated_text)"
  created_at: 2023-11-15 21:46:25+00:00
  edited: true
  hidden: false
  id: 65553c319dc61e22c50499ff
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: abhishek/llama-2-7b-hf-small-shards
repo_type: model
status: open
target_branch: null
title: How to load and quantize the fine tuned model in google colab or kaggle?
