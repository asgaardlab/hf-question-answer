!!python/object:huggingface_hub.community.DiscussionWithDetails
author: egriffiths
conflicting_files: null
created_at: 2023-10-25 10:06:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b581e33337552e40066662c11883fc.svg
      fullname: E G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: egriffiths
      type: user
    createdAt: '2023-10-25T11:06:45.000Z'
    data:
      edited: true
      editors:
      - egriffiths
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9748649001121521
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b581e33337552e40066662c11883fc.svg
          fullname: E G
          isHf: false
          isPro: false
          name: egriffiths
          type: user
        html: '<p>Previously I''ve seen the term ''sharding'' used for model parallelism,
          where your model weights won''t fit on a single GPU, so you split (or shard)
          it over multiple GPUs.</p>

          <p>But in this context it looks like the point of sharding is to enable
          you to run a model whose weights won''t fit into GPU memory on the single
          GPU. How does that work? Are each shard of model weights offloaded and reloaded
          for every forward/backward pass? I would have thought that would make execution
          time prohibitively slow.</p>

          <p>This docs page only vaguely mentions this use case: <a href="https://huggingface.co/transformers/v4.10.1/parallelism.html">https://huggingface.co/transformers/v4.10.1/parallelism.html</a></p>

          <blockquote>

          <p>In the modern machine learning the various approaches to parallelism
          are used to:</p>

          <ol>

          <li>fit very large models onto limited hardware - e.g. t5-11b is 45GB in
          just model params</li>

          </ol>

          </blockquote>

          <p>but doesn''t go into any detail on this.</p>

          '
        raw: 'Previously I''ve seen the term ''sharding'' used for model parallelism,
          where your model weights won''t fit on a single GPU, so you split (or shard)
          it over multiple GPUs.


          But in this context it looks like the point of sharding is to enable you
          to run a model whose weights won''t fit into GPU memory on the single GPU.
          How does that work? Are each shard of model weights offloaded and reloaded
          for every forward/backward pass? I would have thought that would make execution
          time prohibitively slow.


          This docs page only vaguely mentions this use case: https://huggingface.co/transformers/v4.10.1/parallelism.html


          > In the modern machine learning the various approaches to parallelism are
          used to:

          >

          >    1. fit very large models onto limited hardware - e.g. t5-11b is 45GB
          in just model params


          but doesn''t go into any detail on this.'
        updatedAt: '2023-10-25T11:10:23.284Z'
      numEdits: 1
      reactions: []
    id: 6538f6c556c9b35961e27414
    type: comment
  author: egriffiths
  content: 'Previously I''ve seen the term ''sharding'' used for model parallelism,
    where your model weights won''t fit on a single GPU, so you split (or shard) it
    over multiple GPUs.


    But in this context it looks like the point of sharding is to enable you to run
    a model whose weights won''t fit into GPU memory on the single GPU. How does that
    work? Are each shard of model weights offloaded and reloaded for every forward/backward
    pass? I would have thought that would make execution time prohibitively slow.


    This docs page only vaguely mentions this use case: https://huggingface.co/transformers/v4.10.1/parallelism.html


    > In the modern machine learning the various approaches to parallelism are used
    to:

    >

    >    1. fit very large models onto limited hardware - e.g. t5-11b is 45GB in just
    model params


    but doesn''t go into any detail on this.'
  created_at: 2023-10-25 10:06:45+00:00
  edited: true
  hidden: false
  id: 6538f6c556c9b35961e27414
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b581e33337552e40066662c11883fc.svg
      fullname: E G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: egriffiths
      type: user
    createdAt: '2023-10-25T11:17:37.000Z'
    data:
      edited: false
      editors:
      - egriffiths
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9459314346313477
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b581e33337552e40066662c11883fc.svg
          fullname: E G
          isHf: false
          isPro: false
          name: egriffiths
          type: user
        html: '<p>From the docs here: <a href="https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint">https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint</a></p>

          <p>it looks like I''ve got confused between sharding of checkpoints (a host
          memory optimization/feature) and sharding of a model over GPUs.</p>

          <blockquote>

          <p>each checkpoint shard is loaded one by one in RAM and deleted after being
          loaded in the model.</p>

          </blockquote>

          <p>The point of checkpoint sharding is that you may have less host memory
          than GPU memory. This doesn''t help if the total memory requirements of
          all shards are greater than that of GPU memory.</p>

          '
        raw: 'From the docs here: https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint


          it looks like I''ve got confused between sharding of checkpoints (a host
          memory optimization/feature) and sharding of a model over GPUs.


          > each checkpoint shard is loaded one by one in RAM and deleted after being
          loaded in the model.


          The point of checkpoint sharding is that you may have less host memory than
          GPU memory. This doesn''t help if the total memory requirements of all shards
          are greater than that of GPU memory.'
        updatedAt: '2023-10-25T11:17:37.828Z'
      numEdits: 0
      reactions: []
    id: 6538f951a318a98bf0b119db
    type: comment
  author: egriffiths
  content: 'From the docs here: https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/model#transformers.modeling_utils.load_sharded_checkpoint


    it looks like I''ve got confused between sharding of checkpoints (a host memory
    optimization/feature) and sharding of a model over GPUs.


    > each checkpoint shard is loaded one by one in RAM and deleted after being loaded
    in the model.


    The point of checkpoint sharding is that you may have less host memory than GPU
    memory. This doesn''t help if the total memory requirements of all shards are
    greater than that of GPU memory.'
  created_at: 2023-10-25 10:17:37+00:00
  edited: false
  hidden: false
  id: 6538f951a318a98bf0b119db
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: abhishek/llama-2-7b-hf-small-shards
repo_type: model
status: open
target_branch: null
title: How does sharding with 1 GPU work?
