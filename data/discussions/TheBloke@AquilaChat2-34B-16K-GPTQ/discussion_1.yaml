!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-11-24 16:59:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-24T16:59:49.000Z'
    data:
      edited: true
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.873222827911377
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>Just tried out this model, turns out not so successful.</p>

          <p>It lack tokenizer.model so exllamv2 cannot load it, due to exllamav2
          relies on  setecepiece which requires tokenizer.model.</p>

          <p>Next option is to load by autogptq, after resolving model config to be
          llama, the model spit out gerrbish. Seem there is till problem with tokenizer.</p>

          <p>Forgive my ignorance, how is some model contains tokenizer.model, and
          others not? Does the existence of tokenizer.model affect GPTQ process?</p>

          <p>Thanks!</p>

          '
        raw: 'Hi,


          Just tried out this model, turns out not so successful.


          It lack tokenizer.model so exllamv2 cannot load it, due to exllamav2 relies
          on  setecepiece which requires tokenizer.model.


          Next option is to load by autogptq, after resolving model config to be llama,
          the model spit out gerrbish. Seem there is till problem with tokenizer.


          Forgive my ignorance, how is some model contains tokenizer.model, and others
          not? Does the existence of tokenizer.model affect GPTQ process?


          Thanks!'
        updatedAt: '2023-11-24T16:59:59.805Z'
      numEdits: 1
      reactions: []
    id: 6560d68579911cb9fa6aa83d
    type: comment
  author: Yhyu13
  content: 'Hi,


    Just tried out this model, turns out not so successful.


    It lack tokenizer.model so exllamv2 cannot load it, due to exllamav2 relies on  setecepiece
    which requires tokenizer.model.


    Next option is to load by autogptq, after resolving model config to be llama,
    the model spit out gerrbish. Seem there is till problem with tokenizer.


    Forgive my ignorance, how is some model contains tokenizer.model, and others not?
    Does the existence of tokenizer.model affect GPTQ process?


    Thanks!'
  created_at: 2023-11-24 16:59:49+00:00
  edited: true
  hidden: false
  id: 6560d68579911cb9fa6aa83d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/AquilaChat2-34B-16K-GPTQ
repo_type: model
status: open
target_branch: null
title: Missing tokenizer.model and autogptq spit out gerrbish
