!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flake9
conflicting_files: null
created_at: 2023-08-22 06:40:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d49386f0635fd04b1a8138985a72000.svg
      fullname: Bhargav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flake9
      type: user
    createdAt: '2023-08-22T07:40:20.000Z'
    data:
      edited: false
      editors:
      - flake9
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6195928454399109
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d49386f0635fd04b1a8138985a72000.svg
          fullname: Bhargav
          isHf: false
          isPro: false
          name: flake9
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n<p>I am\
          \ trying to load TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ using autogptq<br>My\
          \ code looks like the screenshot.</p>\n<p>I have the model downloaded in\
          \ the same directoty.</p>\n<p>// drwxrwxrwx  3 root root 3000842 Aug 22\
          \ 07:13 Wizard-Vicuna-7B-Uncensored-GPTQ/</p>\n<p>In this Wizard-Vicuna-7B-Uncensored-GPTQ/\
          \ i have all the files. Basically i have git cloned it.<br>But while running\
          \ my script i am seeing below error. Can anyone help.</p>\n<p>root@a57c161a0e1c:/workspace#\
          \ python3 abc.py<br>Traceback (most recent call last):<br>  File \"/workspace/abc.py\"\
          , line 12, in <br>    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
          \ model_basename=model_basename, use_safetensors=True, trust_remote_code=True,\
          \ device=\"cuda:0\", use_triton=use_triton, quantize_config=None)<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py\"\
          , line 94, in from_quantized<br>    return quant_func(<br>  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
          , line 714, in from_quantized<br>    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")<br>FileNotFoundError: Could not\
          \ find model in Wizard-Vicuna-7B-Uncensored-GPTQ</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/6493d07f83f6d9c7085c3bd0/dSXhy7F34jwxGe8_tkCRq.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6493d07f83f6d9c7085c3bd0/dSXhy7F34jwxGe8_tkCRq.png\"\
          ></a><br>Please help!</p>\n"
        raw: "@TheBloke \r\n\r\nI am trying to load TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\
          \ using autogptq\r\nMy code looks like the screenshot.\r\n\r\nI have the\
          \ model downloaded in the same directoty.\r\n\r\n// drwxrwxrwx  3 root root\
          \ 3000842 Aug 22 07:13 Wizard-Vicuna-7B-Uncensored-GPTQ/\r\n\r\nIn this\
          \ Wizard-Vicuna-7B-Uncensored-GPTQ/ i have all the files. Basically i have\
          \ git cloned it.\r\nBut while running my script i am seeing below error.\
          \ Can anyone help.\r\n\r\nroot@a57c161a0e1c:/workspace# python3 abc.py\r\
          \nTraceback (most recent call last):\r\n  File \"/workspace/abc.py\", line\
          \ 12, in <module>\r\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
          \ model_basename=model_basename, use_safetensors=True, trust_remote_code=True,\
          \ device=\"cuda:0\", use_triton=use_triton, quantize_config=None)\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py\"\
          , line 94, in from_quantized\r\n    return quant_func(\r\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
          , line 714, in from_quantized\r\n    raise FileNotFoundError(f\"Could not\
          \ find model in {model_name_or_path}\")\r\nFileNotFoundError: Could not\
          \ find model in Wizard-Vicuna-7B-Uncensored-GPTQ\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6493d07f83f6d9c7085c3bd0/dSXhy7F34jwxGe8_tkCRq.png)\r\
          \nPlease help!"
        updatedAt: '2023-08-22T07:40:20.869Z'
      numEdits: 0
      reactions: []
    id: 64e46664297f3cc53c386e57
    type: comment
  author: flake9
  content: "@TheBloke \r\n\r\nI am trying to load TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\
    \ using autogptq\r\nMy code looks like the screenshot.\r\n\r\nI have the model\
    \ downloaded in the same directoty.\r\n\r\n// drwxrwxrwx  3 root root 3000842\
    \ Aug 22 07:13 Wizard-Vicuna-7B-Uncensored-GPTQ/\r\n\r\nIn this Wizard-Vicuna-7B-Uncensored-GPTQ/\
    \ i have all the files. Basically i have git cloned it.\r\nBut while running my\
    \ script i am seeing below error. Can anyone help.\r\n\r\nroot@a57c161a0e1c:/workspace#\
    \ python3 abc.py\r\nTraceback (most recent call last):\r\n  File \"/workspace/abc.py\"\
    , line 12, in <module>\r\n    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
    \ model_basename=model_basename, use_safetensors=True, trust_remote_code=True,\
    \ device=\"cuda:0\", use_triton=use_triton, quantize_config=None)\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/auto.py\", line 94,\
    \ in from_quantized\r\n    return quant_func(\r\n  File \"/usr/local/lib/python3.10/dist-packages/auto_gptq/modeling/_base.py\"\
    , line 714, in from_quantized\r\n    raise FileNotFoundError(f\"Could not find\
    \ model in {model_name_or_path}\")\r\nFileNotFoundError: Could not find model\
    \ in Wizard-Vicuna-7B-Uncensored-GPTQ\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/6493d07f83f6d9c7085c3bd0/dSXhy7F34jwxGe8_tkCRq.png)\r\
    \nPlease help!"
  created_at: 2023-08-22 06:40:20+00:00
  edited: false
  hidden: false
  id: 64e46664297f3cc53c386e57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/sQN1FNTRMwrVDGs2irgZt.png?w=200&h=200&f=face
      fullname: Atabekov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Temirbek
      type: user
    createdAt: '2023-08-22T11:56:53.000Z'
    data:
      edited: false
      editors:
      - Temirbek
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5151653289794922
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/sQN1FNTRMwrVDGs2irgZt.png?w=200&h=200&f=face
          fullname: Atabekov
          isHf: false
          isPro: false
          name: Temirbek
          type: user
        html: '<p>Hello.</p>

          <p>Try changing <code>model_basename = "Wizard-Vicuna-7B-Uncencored-GPTQ-4bit-128g.no-act.order"
          </code> to <code>model_basename = "model" </code></p>

          '
        raw: 'Hello.


          Try changing ```model_basename = "Wizard-Vicuna-7B-Uncencored-GPTQ-4bit-128g.no-act.order"
          ``` to ```model_basename = "model" ```'
        updatedAt: '2023-08-22T11:56:53.040Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - flake9
    id: 64e4a28582110e1785659a52
    type: comment
  author: Temirbek
  content: 'Hello.


    Try changing ```model_basename = "Wizard-Vicuna-7B-Uncencored-GPTQ-4bit-128g.no-act.order"
    ``` to ```model_basename = "model" ```'
  created_at: 2023-08-22 10:56:53+00:00
  edited: false
  hidden: false
  id: 64e4a28582110e1785659a52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-22T11:57:25.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8384943604469299
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yes I recently updated all my GPTQ models for Transformers compatibility
          (coming very soon)</p>

          <p>Please check the README again and you''ll see that the <code>model_basename</code>
          line is now: <code>model_basename = "model"</code>.  This is true for all
          branches in all GPTQ models.</p>

          <p>Or in fact you can simply leave out <code>model_basename</code> now:</p>

          <pre><code>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)

          </code></pre>

          <p>Because the model_basename is now also configured in <code>quantize_config.json</code>.</p>

          <p>In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain
          this in more detail, and provide example code for loading GPTQ models directly
          from Transformers.  I am waiting for the new Transformers release to happen
          before I do this, which will be today or tomorrow.</p>

          '
        raw: 'Yes I recently updated all my GPTQ models for Transformers compatibility
          (coming very soon)


          Please check the README again and you''ll see that the `model_basename`
          line is now: `model_basename = "model"`.  This is true for all branches
          in all GPTQ models.


          Or in fact you can simply leave out `model_basename` now:

          ```

          model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

          use_safetensors=True,

          trust_remote_code=True,

          device="cuda:0",

          use_triton=use_triton,

          quantize_config=None)

          ```


          Because the model_basename is now also configured in `quantize_config.json`.


          In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain
          this in more detail, and provide example code for loading GPTQ models directly
          from Transformers.  I am waiting for the new Transformers release to happen
          before I do this, which will be today or tomorrow.'
        updatedAt: '2023-08-22T11:57:25.327Z'
      numEdits: 0
      reactions: []
    id: 64e4a2a542028a0616ca16eb
    type: comment
  author: TheBloke
  content: 'Yes I recently updated all my GPTQ models for Transformers compatibility
    (coming very soon)


    Please check the README again and you''ll see that the `model_basename` line is
    now: `model_basename = "model"`.  This is true for all branches in all GPTQ models.


    Or in fact you can simply leave out `model_basename` now:

    ```

    model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,

    use_safetensors=True,

    trust_remote_code=True,

    device="cuda:0",

    use_triton=use_triton,

    quantize_config=None)

    ```


    Because the model_basename is now also configured in `quantize_config.json`.


    In the next 24 - 48 hours I will be updating all my GPTQ READMEs to explain this
    in more detail, and provide example code for loading GPTQ models directly from
    Transformers.  I am waiting for the new Transformers release to happen before
    I do this, which will be today or tomorrow.'
  created_at: 2023-08-22 10:57:25+00:00
  edited: false
  hidden: false
  id: 64e4a2a542028a0616ca16eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d49386f0635fd04b1a8138985a72000.svg
      fullname: Bhargav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flake9
      type: user
    createdAt: '2023-08-22T13:44:16.000Z'
    data:
      edited: false
      editors:
      - flake9
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8530462980270386
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d49386f0635fd04b1a8138985a72000.svg
          fullname: Bhargav
          isHf: false
          isPro: false
          name: flake9
          type: user
        html: "<p>Great Thanks <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Appreciate your work in this space.</p>\n"
        raw: "Great Thanks @TheBloke \n\nAppreciate your work in this space."
        updatedAt: '2023-08-22T13:44:16.521Z'
      numEdits: 0
      reactions: []
    id: 64e4bbb0708bb3855eb651ee
    type: comment
  author: flake9
  content: "Great Thanks @TheBloke \n\nAppreciate your work in this space."
  created_at: 2023-08-22 12:44:16+00:00
  edited: false
  hidden: false
  id: 64e4bbb0708bb3855eb651ee
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9d49386f0635fd04b1a8138985a72000.svg
      fullname: Bhargav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flake9
      type: user
    createdAt: '2023-08-22T13:44:19.000Z'
    data:
      status: closed
    id: 64e4bbb30195913c7faea9dd
    type: status-change
  author: flake9
  created_at: 2023-08-22 12:44:19+00:00
  id: 64e4bbb30195913c7faea9dd
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ
repo_type: model
status: closed
target_branch: null
title: Could not find model in Wizard-Vicuna-7B-Uncensored-GPTQ
