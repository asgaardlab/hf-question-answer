!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nancy10195
conflicting_files: null
created_at: 2022-07-08 20:03:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475624f7c4b6efeaa6bb08e6a88229b1.svg
      fullname: Nan Liu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nancy10195
      type: user
    createdAt: '2022-07-08T21:03:03.000Z'
    data:
      edited: false
      editors:
      - nancy10195
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475624f7c4b6efeaa6bb08e6a88229b1.svg
          fullname: Nan Liu
          isHf: false
          isPro: false
          name: nancy10195
          type: user
        html: '<p>Hello,</p>

          <p>I tried to fine tune clip model in bi_encoder model and cross_encoder
          modes.<br>I prepared the training data (image, text) pair as shown below:<br>train_samples
          = [<br>  InputExample(texts=[Image.open(''175105641.jpeg''), ''grey basket''],
          label=0.3),<br>  InputExample(texts=[Image.open(''175105641.jpeg''), ''vegetable
          washer''], label=0.2)<br>]. Q1. Not sure whether this is correct. </p>

          <p>and when I trained in bi_encoder mode, there is no error. But when I
          pass the evaluator, there is no evaluation results coming out. Q2. Wonder
          why there is no evaluation results showing.</p>

          <p> Q3: Then I trained a cross_encoder, there is an error. I posted the
          code and error as below. It seems that cross_encoder mode for clip model
          is not ready in sentence_transformer. </p>

          <p>code:<br>model_type = ''openai/clip-vit-base-patch32''<br>savepath_location
          = "clip-ViT-B-32_cross_encoder"<br>model_cross = CrossEncoder(model_type,
          num_labels=1)</p>

          <p>train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)<br>train_loss
          = losses.CosineSimilarityLoss(model_cross)<br>#, checkpoint_path=checkpoint_path_location
          remove checkpoint if it causes issue for now<br>model_cross.fit(train_dataloader=train_dataloader,<br>                epochs=3,<br>                warmup_steps=100,<br>                evaluation_steps=-1,<br>                output_path=savepath_location)<br>model_cross.save(savepath_location)</p>

          <p>error:<br>ValueError: Unrecognized configuration class &lt;class ''transformers.models.clip.configuration_clip.CLIPConfig''&gt;
          for this kind of AutoModel: AutoModelForSequenceClassification.<br>Model
          type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig,
          BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig,
          CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig,
          ElectraConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPTNeoConfig,
          GPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config,
          LEDConfig, LongformerConfig, MBartConfig, MegatronBertConfig, MobileBertConfig,
          MPNetConfig, NystromformerConfig, OpenAIGPTConfig, PerceiverConfig, PLBartConfig,
          QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoFormerConfig,
          SqueezeBertConfig, TapasConfig, TransfoXLConfig, XLMConfig, XLMRobertaConfig,
          XLMRobertaXLConfig, XLNetConfig, YosoConfig.</p>

          <p>Thanks for your reply. Best,</p>

          <p>Nan</p>

          '
        raw: "Hello,\r\n\r\nI tried to fine tune clip model in bi_encoder model and\
          \ cross_encoder modes.\r\nI prepared the training data (image, text) pair\
          \ as shown below: \r\ntrain_samples = [\r\n  InputExample(texts=[Image.open('175105641.jpeg'),\
          \ 'grey basket'], label=0.3),\r\n  InputExample(texts=[Image.open('175105641.jpeg'),\
          \ 'vegetable washer'], label=0.2)\r\n]. Q1. Not sure whether this is correct.\
          \ \r\n\r\nand when I trained in bi_encoder mode, there is no error. But\
          \ when I pass the evaluator, there is no evaluation results coming out.\
          \ Q2. Wonder why there is no evaluation results showing.\r\n\r\n Q3: Then\
          \ I trained a cross_encoder, there is an error. I posted the code and error\
          \ as below. It seems that cross_encoder mode for clip model is not ready\
          \ in sentence_transformer. \r\n\r\ncode: \r\nmodel_type = 'openai/clip-vit-base-patch32'\r\
          \nsavepath_location = \"clip-ViT-B-32_cross_encoder\"\r\nmodel_cross = CrossEncoder(model_type,\
          \ num_labels=1)\r\n\r\ntrain_dataloader = DataLoader(train_examples, shuffle=True,\
          \ batch_size=32)\r\ntrain_loss = losses.CosineSimilarityLoss(model_cross)\r\
          \n#, checkpoint_path=checkpoint_path_location remove checkpoint if it causes\
          \ issue for now\r\nmodel_cross.fit(train_dataloader=train_dataloader, \r\
          \n                epochs=3, \r\n                warmup_steps=100, \r\n \
          \               evaluation_steps=-1, \r\n                output_path=savepath_location)\r\
          \nmodel_cross.save(savepath_location)\r\n\r\nerror:\r\nValueError: Unrecognized\
          \ configuration class <class 'transformers.models.clip.configuration_clip.CLIPConfig'>\
          \ for this kind of AutoModel: AutoModelForSequenceClassification.\r\nModel\
          \ type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig,\
          \ BigBirdPegasusConfig, BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig,\
          \ CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig,\
          \ ElectraConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPTNeoConfig,\
          \ GPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config,\
          \ LEDConfig, LongformerConfig, MBartConfig, MegatronBertConfig, MobileBertConfig,\
          \ MPNetConfig, NystromformerConfig, OpenAIGPTConfig, PerceiverConfig, PLBartConfig,\
          \ QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RoFormerConfig,\
          \ SqueezeBertConfig, TapasConfig, TransfoXLConfig, XLMConfig, XLMRobertaConfig,\
          \ XLMRobertaXLConfig, XLNetConfig, YosoConfig.\r\n\r\nThanks for your reply.\
          \ Best,\r\n\r\nNan"
        updatedAt: '2022-07-08T21:03:03.954Z'
      numEdits: 0
      reactions: []
    id: 62c89b875a87d76d88ffa8c6
    type: comment
  author: nancy10195
  content: "Hello,\r\n\r\nI tried to fine tune clip model in bi_encoder model and\
    \ cross_encoder modes.\r\nI prepared the training data (image, text) pair as shown\
    \ below: \r\ntrain_samples = [\r\n  InputExample(texts=[Image.open('175105641.jpeg'),\
    \ 'grey basket'], label=0.3),\r\n  InputExample(texts=[Image.open('175105641.jpeg'),\
    \ 'vegetable washer'], label=0.2)\r\n]. Q1. Not sure whether this is correct.\
    \ \r\n\r\nand when I trained in bi_encoder mode, there is no error. But when I\
    \ pass the evaluator, there is no evaluation results coming out. Q2. Wonder why\
    \ there is no evaluation results showing.\r\n\r\n Q3: Then I trained a cross_encoder,\
    \ there is an error. I posted the code and error as below. It seems that cross_encoder\
    \ mode for clip model is not ready in sentence_transformer. \r\n\r\ncode: \r\n\
    model_type = 'openai/clip-vit-base-patch32'\r\nsavepath_location = \"clip-ViT-B-32_cross_encoder\"\
    \r\nmodel_cross = CrossEncoder(model_type, num_labels=1)\r\n\r\ntrain_dataloader\
    \ = DataLoader(train_examples, shuffle=True, batch_size=32)\r\ntrain_loss = losses.CosineSimilarityLoss(model_cross)\r\
    \n#, checkpoint_path=checkpoint_path_location remove checkpoint if it causes issue\
    \ for now\r\nmodel_cross.fit(train_dataloader=train_dataloader, \r\n         \
    \       epochs=3, \r\n                warmup_steps=100, \r\n                evaluation_steps=-1,\
    \ \r\n                output_path=savepath_location)\r\nmodel_cross.save(savepath_location)\r\
    \n\r\nerror:\r\nValueError: Unrecognized configuration class <class 'transformers.models.clip.configuration_clip.CLIPConfig'>\
    \ for this kind of AutoModel: AutoModelForSequenceClassification.\r\nModel type\
    \ should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig,\
    \ BloomConfig, CamembertConfig, CanineConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig,\
    \ DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, FlaubertConfig,\
    \ FNetConfig, FunnelConfig, GPT2Config, GPTNeoConfig, GPTJConfig, IBertConfig,\
    \ LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LongformerConfig,\
    \ MBartConfig, MegatronBertConfig, MobileBertConfig, MPNetConfig, NystromformerConfig,\
    \ OpenAIGPTConfig, PerceiverConfig, PLBartConfig, QDQBertConfig, ReformerConfig,\
    \ RemBertConfig, RobertaConfig, RoFormerConfig, SqueezeBertConfig, TapasConfig,\
    \ TransfoXLConfig, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig,\
    \ YosoConfig.\r\n\r\nThanks for your reply. Best,\r\n\r\nNan"
  created_at: 2022-07-08 20:03:03+00:00
  edited: false
  hidden: false
  id: 62c89b875a87d76d88ffa8c6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: sentence-transformers/clip-ViT-B-32
repo_type: model
status: open
target_branch: null
title: cross_encoder and bi_encoder for clip fine tuning
