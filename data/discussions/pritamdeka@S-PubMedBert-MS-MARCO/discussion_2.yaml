!!python/object:huggingface_hub.community.DiscussionWithDetails
author: johncrtz
conflicting_files: null
created_at: 2023-12-17 00:40:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b6aada47ded2a873e00508b9c3d56a1f.svg
      fullname: john hildenbrand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johncrtz
      type: user
    createdAt: '2023-12-17T00:40:28.000Z'
    data:
      edited: false
      editors:
      - johncrtz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9361673593521118
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b6aada47ded2a873e00508b9c3d56a1f.svg
          fullname: john hildenbrand
          isHf: false
          isPro: false
          name: johncrtz
          type: user
        html: '<p>Hello, i would like to know what how many words i can input into
          the pritamdeka/S-PubMedBert-MS-MARCO embedding model before it gets truncated.
          The documentation says something about max-sequence-length of 384 but does
          that refer to tokens or words? Generally i know that BERT allows for as
          much as 512 tokens, but since sentence level embedding doesnt require tokenization
          and lets you just input the sentence as it is, how can i make sure that
          i dont exceed such a token limit?<br>Please tell me if i mixed something
          up, im quite new to this topic and apreciate any feedback!</p>

          '
        raw: "Hello, i would like to know what how many words i can input into the\
          \ pritamdeka/S-PubMedBert-MS-MARCO embedding model before it gets truncated.\
          \ The documentation says something about max-sequence-length of 384 but\
          \ does that refer to tokens or words? Generally i know that BERT allows\
          \ for as much as 512 tokens, but since sentence level embedding doesnt require\
          \ tokenization and lets you just input the sentence as it is, how can i\
          \ make sure that i dont exceed such a token limit?\r\nPlease tell me if\
          \ i mixed something up, im quite new to this topic and apreciate any feedback!"
        updatedAt: '2023-12-17T00:40:28.275Z'
      numEdits: 0
      reactions: []
    id: 657e437c83543a061b53ba5a
    type: comment
  author: johncrtz
  content: "Hello, i would like to know what how many words i can input into the pritamdeka/S-PubMedBert-MS-MARCO\
    \ embedding model before it gets truncated. The documentation says something about\
    \ max-sequence-length of 384 but does that refer to tokens or words? Generally\
    \ i know that BERT allows for as much as 512 tokens, but since sentence level\
    \ embedding doesnt require tokenization and lets you just input the sentence as\
    \ it is, how can i make sure that i dont exceed such a token limit?\r\nPlease\
    \ tell me if i mixed something up, im quite new to this topic and apreciate any\
    \ feedback!"
  created_at: 2023-12-17 00:40:28+00:00
  edited: false
  hidden: false
  id: 657e437c83543a061b53ba5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654033392647-60c7e4f1672c96a317b85500.jpeg?w=200&h=200&f=face
      fullname: Pritam Deka
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pritamdeka
      type: user
    createdAt: '2023-12-17T01:23:58.000Z'
    data:
      edited: false
      editors:
      - pritamdeka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9381544589996338
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654033392647-60c7e4f1672c96a317b85500.jpeg?w=200&h=200&f=face
          fullname: Pritam Deka
          isHf: false
          isPro: false
          name: pritamdeka
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;johncrtz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/johncrtz\"\
          >@<span class=\"underline\">johncrtz</span></a></span>\n\n\t</span></span>\
          \ The model will truncate longer sentences to 350 tokens. Tokens in BERT\
          \ is not exactly words since the tokenizer uses special symbols for tokenization.\
          \ \"A common value for BERT &amp; Co. are 512 word pieces, which correspond\
          \ to about 300-400 words (for English). Longer texts than this are truncated\
          \ to the first x word pieces.\" This is from SBERT documentation. So as\
          \ an approximation, if your sentence has around more than 128 words, it's\
          \ better to shorten the sentences. You can do one thing, split the sentences,\
          \ calculate the embeddings for the individual split sentences and then add\
          \ the embeddings. Although I am not sure without experimenting how the resulting\
          \ embedding quality would be. But you can experiment both approaches, without\
          \ splitting and with splitting. Do lemme know how your experiments work\
          \ out. Cheers!!!!</p>\n"
        raw: Hi @johncrtz The model will truncate longer sentences to 350 tokens.
          Tokens in BERT is not exactly words since the tokenizer uses special symbols
          for tokenization. "A common value for BERT & Co. are 512 word pieces, which
          correspond to about 300-400 words (for English). Longer texts than this
          are truncated to the first x word pieces." This is from SBERT documentation.
          So as an approximation, if your sentence has around more than 128 words,
          it's better to shorten the sentences. You can do one thing, split the sentences,
          calculate the embeddings for the individual split sentences and then add
          the embeddings. Although I am not sure without experimenting how the resulting
          embedding quality would be. But you can experiment both approaches, without
          splitting and with splitting. Do lemme know how your experiments work out.
          Cheers!!!!
        updatedAt: '2023-12-17T01:23:58.228Z'
      numEdits: 0
      reactions: []
    id: 657e4dae2bffc5568af084e5
    type: comment
  author: pritamdeka
  content: Hi @johncrtz The model will truncate longer sentences to 350 tokens. Tokens
    in BERT is not exactly words since the tokenizer uses special symbols for tokenization.
    "A common value for BERT & Co. are 512 word pieces, which correspond to about
    300-400 words (for English). Longer texts than this are truncated to the first
    x word pieces." This is from SBERT documentation. So as an approximation, if your
    sentence has around more than 128 words, it's better to shorten the sentences.
    You can do one thing, split the sentences, calculate the embeddings for the individual
    split sentences and then add the embeddings. Although I am not sure without experimenting
    how the resulting embedding quality would be. But you can experiment both approaches,
    without splitting and with splitting. Do lemme know how your experiments work
    out. Cheers!!!!
  created_at: 2023-12-17 01:23:58+00:00
  edited: false
  hidden: false
  id: 657e4dae2bffc5568af084e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b6aada47ded2a873e00508b9c3d56a1f.svg
      fullname: john hildenbrand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: johncrtz
      type: user
    createdAt: '2024-01-01T16:33:06.000Z'
    data:
      edited: false
      editors:
      - johncrtz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9541949033737183
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b6aada47ded2a873e00508b9c3d56a1f.svg
          fullname: john hildenbrand
          isHf: false
          isPro: false
          name: johncrtz
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;pritamdeka&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pritamdeka\"\
          >@<span class=\"underline\">pritamdeka</span></a></span>\n\n\t</span></span>\
          \ thank you very much the clarification and the suggestions, i really appreciate\
          \ them.<br>What i tried so far was first of all analyzing how big my paragraphs\
          \ are that i want to embed by running them through the same tokenizer model\
          \ that pritamdeka/S-PubMedBert-MS-MARCO is using internally. It turns out\
          \ that most of them are below 350 tokens,  however there are still some\
          \ above that which means i ultimately have to split them up in some way.\
          \ The approach that you suggested seems reasonable to me, split every paragraph\
          \ after n words, create embeddings for each sub-paragraph and then combine\
          \ them in some way e.g. pooling.<br>I was just wondering if there is any\
          \ way to exactly control how many tokens are fed into the sentence transformer\
          \ without having to rely on an approximation. 128 words might work but i\
          \ think there will be many cases where a paragraph would be divided allthough\
          \ it would actually fit in which would create unnecessary distortion.</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/6570c3eef3853f99bc2e6819/oxKdPDHZWbgnL7t1uNB87.png\"\
          ><img alt=\"distribution of size per token.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/6570c3eef3853f99bc2e6819/oxKdPDHZWbgnL7t1uNB87.png\"\
          ></a></p>\n<p>I though about solving this by running each word of a paragraph\
          \ through the  pritamdeka/S-PubMedBert-MS-MARCO tokenizer(via AutoTokenizer.from_pretrained('pritamdeka/S-PubMedBert-MS-MARCO'))\
          \ and at the same time counting the resulting tokens. Whenever we reached\
          \ 350 tokens we cut the sentence where we took the last word from and this\
          \ would result in one chunk. We continue for the rest of the paragraph,\
          \ embed each chunk and then later on combine them by a pooling strategy.<br>This\
          \ way we make optimal use of the token limit, have control over how many\
          \ tokens we are feeding in and avoid unnecessary pooling.</p>\n<p>What do\
          \ you think of this approach? Is it unnecessarily complicated or are you\
          \ aware of better approaches on how to tackle this? </p>\n<p>Best regards\
          \ and a happy new year,<br>John</p>\n"
        raw: "Hi @pritamdeka thank you very much the clarification and the suggestions,\
          \ i really appreciate them. \nWhat i tried so far was first of all analyzing\
          \ how big my paragraphs are that i want to embed by running them through\
          \ the same tokenizer model that pritamdeka/S-PubMedBert-MS-MARCO is using\
          \ internally. It turns out that most of them are below 350 tokens,  however\
          \ there are still some above that which means i ultimately have to split\
          \ them up in some way. The approach that you suggested seems reasonable\
          \ to me, split every paragraph after n words, create embeddings for each\
          \ sub-paragraph and then combine them in some way e.g. pooling. \nI was\
          \ just wondering if there is any way to exactly control how many tokens\
          \ are fed into the sentence transformer without having to rely on an approximation.\
          \ 128 words might work but i think there will be many cases where a paragraph\
          \ would be divided allthough it would actually fit in which would create\
          \ unnecessary distortion.\n\n![distribution of size per token.png](https://cdn-uploads.huggingface.co/production/uploads/6570c3eef3853f99bc2e6819/oxKdPDHZWbgnL7t1uNB87.png)\n\
          \nI though about solving this by running each word of a paragraph through\
          \ the  pritamdeka/S-PubMedBert-MS-MARCO tokenizer(via AutoTokenizer.from_pretrained('pritamdeka/S-PubMedBert-MS-MARCO'))\
          \ and at the same time counting the resulting tokens. Whenever we reached\
          \ 350 tokens we cut the sentence where we took the last word from and this\
          \ would result in one chunk. We continue for the rest of the paragraph,\
          \ embed each chunk and then later on combine them by a pooling strategy.\n\
          This way we make optimal use of the token limit, have control over how many\
          \ tokens we are feeding in and avoid unnecessary pooling.\n\nWhat do you\
          \ think of this approach? Is it unnecessarily complicated or are you aware\
          \ of better approaches on how to tackle this? \n\nBest regards and a happy\
          \ new year,\nJohn\n\n"
        updatedAt: '2024-01-01T16:33:06.692Z'
      numEdits: 0
      reactions: []
    id: 6592e94268d0b7633121a6b1
    type: comment
  author: johncrtz
  content: "Hi @pritamdeka thank you very much the clarification and the suggestions,\
    \ i really appreciate them. \nWhat i tried so far was first of all analyzing how\
    \ big my paragraphs are that i want to embed by running them through the same\
    \ tokenizer model that pritamdeka/S-PubMedBert-MS-MARCO is using internally. It\
    \ turns out that most of them are below 350 tokens,  however there are still some\
    \ above that which means i ultimately have to split them up in some way. The approach\
    \ that you suggested seems reasonable to me, split every paragraph after n words,\
    \ create embeddings for each sub-paragraph and then combine them in some way e.g.\
    \ pooling. \nI was just wondering if there is any way to exactly control how many\
    \ tokens are fed into the sentence transformer without having to rely on an approximation.\
    \ 128 words might work but i think there will be many cases where a paragraph\
    \ would be divided allthough it would actually fit in which would create unnecessary\
    \ distortion.\n\n![distribution of size per token.png](https://cdn-uploads.huggingface.co/production/uploads/6570c3eef3853f99bc2e6819/oxKdPDHZWbgnL7t1uNB87.png)\n\
    \nI though about solving this by running each word of a paragraph through the\
    \  pritamdeka/S-PubMedBert-MS-MARCO tokenizer(via AutoTokenizer.from_pretrained('pritamdeka/S-PubMedBert-MS-MARCO'))\
    \ and at the same time counting the resulting tokens. Whenever we reached 350\
    \ tokens we cut the sentence where we took the last word from and this would result\
    \ in one chunk. We continue for the rest of the paragraph, embed each chunk and\
    \ then later on combine them by a pooling strategy.\nThis way we make optimal\
    \ use of the token limit, have control over how many tokens we are feeding in\
    \ and avoid unnecessary pooling.\n\nWhat do you think of this approach? Is it\
    \ unnecessarily complicated or are you aware of better approaches on how to tackle\
    \ this? \n\nBest regards and a happy new year,\nJohn\n\n"
  created_at: 2024-01-01 16:33:06+00:00
  edited: false
  hidden: false
  id: 6592e94268d0b7633121a6b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654033392647-60c7e4f1672c96a317b85500.jpeg?w=200&h=200&f=face
      fullname: Pritam Deka
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: pritamdeka
      type: user
    createdAt: '2024-01-06T21:27:33.000Z'
    data:
      edited: false
      editors:
      - pritamdeka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9558355212211609
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654033392647-60c7e4f1672c96a317b85500.jpeg?w=200&h=200&f=face
          fullname: Pritam Deka
          isHf: false
          isPro: false
          name: pritamdeka
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;johncrtz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/johncrtz\"\
          >@<span class=\"underline\">johncrtz</span></a></span>\n\n\t</span></span>..\
          \ Happy new year to you too. I guess your approach is suitable, however,\
          \ instead of counting each word and tokenizing them, I would suggest you\
          \ can use max_seq_length. What you can do is if the paragraph maximum seq\
          \ length exceeds 350 tokens, you can split it up at the last word. If it\
          \ doesn't exceed that value you don't need to split anything. So basically\
          \ it will involve an if-else loop for the code. That would be less complicated.\
          \ You can try both approaches, the one you suggested and the one I suggested\
          \ and use the better one. Do lemme know how it works out.. Cheers...</p>\n"
        raw: Hi @johncrtz.. Happy new year to you too. I guess your approach is suitable,
          however, instead of counting each word and tokenizing them, I would suggest
          you can use max_seq_length. What you can do is if the paragraph maximum
          seq length exceeds 350 tokens, you can split it up at the last word. If
          it doesn't exceed that value you don't need to split anything. So basically
          it will involve an if-else loop for the code. That would be less complicated.
          You can try both approaches, the one you suggested and the one I suggested
          and use the better one. Do lemme know how it works out.. Cheers...
        updatedAt: '2024-01-06T21:27:33.667Z'
      numEdits: 0
      reactions: []
    id: 6599c5c53b0b56c5e0da96ce
    type: comment
  author: pritamdeka
  content: Hi @johncrtz.. Happy new year to you too. I guess your approach is suitable,
    however, instead of counting each word and tokenizing them, I would suggest you
    can use max_seq_length. What you can do is if the paragraph maximum seq length
    exceeds 350 tokens, you can split it up at the last word. If it doesn't exceed
    that value you don't need to split anything. So basically it will involve an if-else
    loop for the code. That would be less complicated. You can try both approaches,
    the one you suggested and the one I suggested and use the better one. Do lemme
    know how it works out.. Cheers...
  created_at: 2024-01-06 21:27:33+00:00
  edited: false
  hidden: false
  id: 6599c5c53b0b56c5e0da96ce
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: pritamdeka/S-PubMedBert-MS-MARCO
repo_type: model
status: open
target_branch: null
title: pritamdeka/S-PubMedBert-MS-MARCO Maximum input length
