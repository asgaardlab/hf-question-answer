!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-07-20 03:28:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-20T04:28:50.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7577394247055054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>I bet if you trained this model on one, multiple or even all of
          these coding datasets, it would be better than wizarscoder hands down. Not
          sure if it would be better to train llama-2-13b on these datasets first
          then train it on the guanaco qlora, or the other way around</p>

          <p><a href="https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1">https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1</a></p>

          <p><a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k">https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k</a></p>

          <p><a href="https://huggingface.co/datasets/sahil2801/code_instructions_120k">https://huggingface.co/datasets/sahil2801/code_instructions_120k</a></p>

          <p><a href="https://huggingface.co/datasets/codeparrot/github-code-clean">https://huggingface.co/datasets/codeparrot/github-code-clean</a></p>

          <p><a href="https://huggingface.co/datasets/razent/wizardlm-code-evol-32k">https://huggingface.co/datasets/razent/wizardlm-code-evol-32k</a></p>

          '
        raw: "I bet if you trained this model on one, multiple or even all of these\
          \ coding datasets, it would be better than wizarscoder hands down. Not sure\
          \ if it would be better to train llama-2-13b on these datasets first then\
          \ train it on the guanaco qlora, or the other way around\r\n\r\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\r\
          \n\r\nhttps://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\r\n\r\nhttps://huggingface.co/datasets/sahil2801/code_instructions_120k\r\
          \n\r\nhttps://huggingface.co/datasets/codeparrot/github-code-clean\r\n\r\
          \nhttps://huggingface.co/datasets/razent/wizardlm-code-evol-32k"
        updatedAt: '2023-07-20T04:28:50.096Z'
      numEdits: 0
      reactions: []
    id: 64b8b802047fa3db942049e2
    type: comment
  author: rombodawg
  content: "I bet if you trained this model on one, multiple or even all of these\
    \ coding datasets, it would be better than wizarscoder hands down. Not sure if\
    \ it would be better to train llama-2-13b on these datasets first then train it\
    \ on the guanaco qlora, or the other way around\r\n\r\nhttps://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1\r\
    \n\r\nhttps://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\r\n\r\nhttps://huggingface.co/datasets/sahil2801/code_instructions_120k\r\
    \n\r\nhttps://huggingface.co/datasets/codeparrot/github-code-clean\r\n\r\nhttps://huggingface.co/datasets/razent/wizardlm-code-evol-32k"
  created_at: 2023-07-20 03:28:50+00:00
  edited: false
  hidden: false
  id: 64b8b802047fa3db942049e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-21T00:21:23.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9339062571525574
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: "<p>Actually forget those datasets, i made my own if you want to use\
          \ it. It would be much easier. I tried training the model myself but i kept\
          \ getting errors and im not exprerienced enough to fix them \U0001F61E<br>Heres\
          \ my dataset<br><a href=\"https://huggingface.co/datasets/rombodawg/MegaCodeTraining112k\"\
          >https://huggingface.co/datasets/rombodawg/MegaCodeTraining112k</a></p>\n"
        raw: "Actually forget those datasets, i made my own if you want to use it.\
          \ It would be much easier. I tried training the model myself but i kept\
          \ getting errors and im not exprerienced enough to fix them \U0001F61E\n\
          Heres my dataset\nhttps://huggingface.co/datasets/rombodawg/MegaCodeTraining112k"
        updatedAt: '2023-07-21T00:21:49.823Z'
      numEdits: 1
      reactions: []
    id: 64b9cf8310430817357733e5
    type: comment
  author: rombodawg
  content: "Actually forget those datasets, i made my own if you want to use it. It\
    \ would be much easier. I tried training the model myself but i kept getting errors\
    \ and im not exprerienced enough to fix them \U0001F61E\nHeres my dataset\nhttps://huggingface.co/datasets/rombodawg/MegaCodeTraining112k"
  created_at: 2023-07-20 23:21:23+00:00
  edited: true
  hidden: false
  id: 64b9cf8310430817357733e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
      fullname: Mikael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Mikael110
      type: user
    createdAt: '2023-07-21T01:12:47.000Z'
    data:
      edited: false
      editors:
      - Mikael110
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9619858860969543
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/229e60726cd92951c31d7303e40be2cd.svg
          fullname: Mikael
          isHf: false
          isPro: false
          name: Mikael110
          type: user
        html: '<p>Training such a large dataset is outside my budget. To put things
          into perspective the Guanaco dataset is only 20.9MB versus your 433MB dataset.</p>

          <p>I can try to help you debug the training issue you are having though.
          What script are you using to train and what are the errors?</p>

          '
        raw: 'Training such a large dataset is outside my budget. To put things into
          perspective the Guanaco dataset is only 20.9MB versus your 433MB dataset.


          I can try to help you debug the training issue you are having though. What
          script are you using to train and what are the errors?'
        updatedAt: '2023-07-21T01:12:47.934Z'
      numEdits: 0
      reactions: []
    id: 64b9db8f84ddd52599c4ecd3
    type: comment
  author: Mikael110
  content: 'Training such a large dataset is outside my budget. To put things into
    perspective the Guanaco dataset is only 20.9MB versus your 433MB dataset.


    I can try to help you debug the training issue you are having though. What script
    are you using to train and what are the errors?'
  created_at: 2023-07-21 00:12:47+00:00
  edited: false
  hidden: false
  id: 64b9db8f84ddd52599c4ecd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d782569b2e4c0805079f566b64c4b95b.svg
      fullname: Vijay Murari Tiyyala
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vtiyyal1
      type: user
    createdAt: '2023-07-24T05:26:47.000Z'
    data:
      edited: false
      editors:
      - vtiyyal1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9223416447639465
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d782569b2e4c0805079f566b64c4b95b.svg
          fullname: Vijay Murari Tiyyala
          isHf: false
          isPro: false
          name: vtiyyal1
          type: user
        html: "<blockquote>\n<p>Actually forget those datasets, i made my own if you\
          \ want to use it. It would be much easier. I tried training the model myself\
          \ but i kept getting errors and im not exprerienced enough to fix them \U0001F61E\
          <br>Heres my dataset<br><a href=\"https://huggingface.co/datasets/rombodawg/MegaCodeTraining112k\"\
          >https://huggingface.co/datasets/rombodawg/MegaCodeTraining112k</a></p>\n\
          </blockquote>\n<p>Why not the above datasets?</p>\n"
        raw: "> Actually forget those datasets, i made my own if you want to use it.\
          \ It would be much easier. I tried training the model myself but i kept\
          \ getting errors and im not exprerienced enough to fix them \U0001F61E\n\
          > Heres my dataset\n> https://huggingface.co/datasets/rombodawg/MegaCodeTraining112k\n\
          \nWhy not the above datasets?\n"
        updatedAt: '2023-07-24T05:26:47.697Z'
      numEdits: 0
      reactions: []
    id: 64be0b97afd1e46c552471fb
    type: comment
  author: vtiyyal1
  content: "> Actually forget those datasets, i made my own if you want to use it.\
    \ It would be much easier. I tried training the model myself but i kept getting\
    \ errors and im not exprerienced enough to fix them \U0001F61E\n> Heres my dataset\n\
    > https://huggingface.co/datasets/rombodawg/MegaCodeTraining112k\n\nWhy not the\
    \ above datasets?\n"
  created_at: 2023-07-24 04:26:47+00:00
  edited: false
  hidden: false
  id: 64be0b97afd1e46c552471fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-24T05:30:58.000Z'
    data:
      edited: true
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9705751538276672
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Because the one i made is much cleaner, and consise, in inclides
          both the 80k and 32k combined. Doesnt include code instruct 120k so you
          can use that to train seperatly because its formatted diffrently </p>

          '
        raw: 'Because the one i made is much cleaner, and consise, in inclides both
          the 80k and 32k combined. Doesnt include code instruct 120k so you can use
          that to train seperatly because its formatted diffrently '
        updatedAt: '2023-07-24T05:31:34.819Z'
      numEdits: 1
      reactions: []
    id: 64be0c92565b827f7ec16f7f
    type: comment
  author: rombodawg
  content: 'Because the one i made is much cleaner, and consise, in inclides both
    the 80k and 32k combined. Doesnt include code instruct 120k so you can use that
    to train seperatly because its formatted diffrently '
  created_at: 2023-07-24 04:30:58+00:00
  edited: true
  hidden: false
  id: 64be0c92565b827f7ec16f7f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Mikael110/llama-2-13b-guanaco-fp16
repo_type: model
status: open
target_branch: null
title: Train this model on coding datasets to make a BEAST coding model
