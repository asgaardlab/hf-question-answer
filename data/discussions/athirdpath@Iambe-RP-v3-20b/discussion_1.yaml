!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Alastar-Smith
conflicting_files: null
created_at: 2023-12-12 20:37:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9ef7d30f6beee01e617490be2f17bbb.svg
      fullname: Alastar Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alastar-Smith
      type: user
    createdAt: '2023-12-12T20:37:29.000Z'
    data:
      edited: true
      editors:
      - Alastar-Smith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9131532311439514
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9ef7d30f6beee01e617490be2f17bbb.svg
          fullname: Alastar Smith
          isHf: false
          isPro: false
          name: Alastar-Smith
          type: user
        html: '<p>Hello Good Sir!</p>

          <p>I''m a fan of your model! It gives me a really unique answers and follows
          instructions pretty well!<br>My problem is that I have 3060 12gb and GGUF
          version gives me only 3t\s, can you please make an EXL2-3bpw version?</p>

          <p>Thank you in advance!<br>Cheers!</p>

          '
        raw: 'Hello Good Sir!


          I''m a fan of your model! It gives me a really unique answers and follows
          instructions pretty well!

          My problem is that I have 3060 12gb and GGUF version gives me only 3t\s,
          can you please make an EXL2-3bpw version?


          Thank you in advance!

          Cheers!'
        updatedAt: '2023-12-13T06:59:38.278Z'
      numEdits: 4
      reactions: []
    id: 6578c48976b6de79781b1673
    type: comment
  author: Alastar-Smith
  content: 'Hello Good Sir!


    I''m a fan of your model! It gives me a really unique answers and follows instructions
    pretty well!

    My problem is that I have 3060 12gb and GGUF version gives me only 3t\s, can you
    please make an EXL2-3bpw version?


    Thank you in advance!

    Cheers!'
  created_at: 2023-12-12 20:37:29+00:00
  edited: true
  hidden: false
  id: 6578c48976b6de79781b1673
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-12T22:40:25.000Z'
    data:
      edited: false
      editors:
      - athirdpath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9703887701034546
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
          fullname: Raven
          isHf: false
          isPro: false
          name: athirdpath
          type: user
        html: '<p>Yeah! EXL quants take forever and need a GPU, so it''s too expensive
          to do remotely. I''ll have to do it overnight locally, so it won;t be until
          morning.</p>

          <p>We''ve got the same GPU so if it works well for me I might start doing
          it for all stable releases.</p>

          '
        raw: 'Yeah! EXL quants take forever and need a GPU, so it''s too expensive
          to do remotely. I''ll have to do it overnight locally, so it won;t be until
          morning.


          We''ve got the same GPU so if it works well for me I might start doing it
          for all stable releases.'
        updatedAt: '2023-12-12T22:40:25.738Z'
      numEdits: 0
      reactions: []
    id: 6578e159dea7e2122d2435c7
    type: comment
  author: athirdpath
  content: 'Yeah! EXL quants take forever and need a GPU, so it''s too expensive to
    do remotely. I''ll have to do it overnight locally, so it won;t be until morning.


    We''ve got the same GPU so if it works well for me I might start doing it for
    all stable releases.'
  created_at: 2023-12-12 22:40:25+00:00
  edited: false
  hidden: false
  id: 6578e159dea7e2122d2435c7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9ef7d30f6beee01e617490be2f17bbb.svg
      fullname: Alastar Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alastar-Smith
      type: user
    createdAt: '2023-12-13T06:57:52.000Z'
    data:
      edited: true
      editors:
      - Alastar-Smith
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9802098870277405
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9ef7d30f6beee01e617490be2f17bbb.svg
          fullname: Alastar Smith
          isHf: false
          isPro: false
          name: Alastar-Smith
          type: user
        html: '<p>Got it! Thank you! It works pretty good and fast at 3bpw, faster
          then 13b models at GGUF.<br>Precise as also pretty good since it is a RP
          model, people say that new EXL2-2 quants is even better.</p>

          '
        raw: 'Got it! Thank you! It works pretty good and fast at 3bpw, faster then
          13b models at GGUF.

          Precise as also pretty good since it is a RP model, people say that new
          EXL2-2 quants is even better.'
        updatedAt: '2023-12-13T06:58:55.920Z'
      numEdits: 1
      reactions: []
    id: 657955f0a87010c9f8b25584
    type: comment
  author: Alastar-Smith
  content: 'Got it! Thank you! It works pretty good and fast at 3bpw, faster then
    13b models at GGUF.

    Precise as also pretty good since it is a RP model, people say that new EXL2-2
    quants is even better.'
  created_at: 2023-12-13 06:57:52+00:00
  edited: true
  hidden: false
  id: 657955f0a87010c9f8b25584
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-13T19:37:56.000Z'
    data:
      edited: false
      editors:
      - athirdpath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9949405193328857
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
          fullname: Raven
          isHf: false
          isPro: false
          name: athirdpath
          type: user
        html: '<p>Ooof, 3bpw took ~14GB vram! I uploaded the 3.0bpw, I''ve got a 2.6bpw
          cooking.</p>

          '
        raw: Ooof, 3bpw took ~14GB vram! I uploaded the 3.0bpw, I've got a 2.6bpw
          cooking.
        updatedAt: '2023-12-13T19:37:56.943Z'
      numEdits: 0
      reactions: []
    id: 657a0814479c85a20f82111f
    type: comment
  author: athirdpath
  content: Ooof, 3bpw took ~14GB vram! I uploaded the 3.0bpw, I've got a 2.6bpw cooking.
  created_at: 2023-12-13 19:37:56+00:00
  edited: false
  hidden: false
  id: 657a0814479c85a20f82111f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-13T21:12:28.000Z'
    data:
      edited: false
      editors:
      - athirdpath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9927117824554443
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
          fullname: Raven
          isHf: false
          isPro: false
          name: athirdpath
          type: user
        html: '<p>D''oh, I didn''t know about 8-bit cache. Regardless, there is also
          a 2.6bpw uploading now.</p>

          '
        raw: D'oh, I didn't know about 8-bit cache. Regardless, there is also a 2.6bpw
          uploading now.
        updatedAt: '2023-12-13T21:12:28.773Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Alastar-Smith
    id: 657a1e3ce0cb28fb47014425
    type: comment
  author: athirdpath
  content: D'oh, I didn't know about 8-bit cache. Regardless, there is also a 2.6bpw
    uploading now.
  created_at: 2023-12-13 21:12:28+00:00
  edited: false
  hidden: false
  id: 657a1e3ce0cb28fb47014425
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-13T21:12:33.000Z'
    data:
      status: closed
    id: 657a1e41bbb9afe5fb50c1dc
    type: status-change
  author: athirdpath
  created_at: 2023-12-13 21:12:33+00:00
  id: 657a1e41bbb9afe5fb50c1dc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: athirdpath/Iambe-RP-v3-20b
repo_type: model
status: closed
target_branch: null
title: Iambe-RP-v3-20b-EXL2-3bpw
