!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grimpeaper23
conflicting_files: null
created_at: 2023-11-14 15:02:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
      fullname: Ajay Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimpeaper23
      type: user
    createdAt: '2023-11-14T15:02:00.000Z'
    data:
      edited: false
      editors:
      - grimpeaper23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9568191170692444
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
          fullname: Ajay Krishnan
          isHf: false
          isPro: false
          name: grimpeaper23
          type: user
        html: '<p>Hi Is it possible to get the image embeddings of the model? I don''t
          need the decoder outputs but I''d love to take the encoder representations
          alone. Apologies if this is already mentioned in the documentation and I
          had just missed out!</p>

          '
        raw: Hi Is it possible to get the image embeddings of the model? I don't need
          the decoder outputs but I'd love to take the encoder representations alone.
          Apologies if this is already mentioned in the documentation and I had just
          missed out!
        updatedAt: '2023-11-14T15:02:00.213Z'
      numEdits: 0
      reactions: []
    id: 65538be82e42bd6d1d4d43b1
    type: comment
  author: grimpeaper23
  content: Hi Is it possible to get the image embeddings of the model? I don't need
    the decoder outputs but I'd love to take the encoder representations alone. Apologies
    if this is already mentioned in the documentation and I had just missed out!
  created_at: 2023-11-14 15:02:00+00:00
  edited: false
  hidden: false
  id: 65538be82e42bd6d1d4d43b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668065185273-noauth.png?w=200&h=200&f=face
      fullname: "M\xE9lodie Boillet"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mboillet
      type: user
    createdAt: '2023-11-20T09:39:51.000Z'
    data:
      edited: false
      editors:
      - mboillet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6158036589622498
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668065185273-noauth.png?w=200&h=200&f=face
          fullname: "M\xE9lodie Boillet"
          isHf: false
          isPro: false
          name: mboillet
          type: user
        html: "<p>Hello! There is no dedicated function to do this directly. However,\
          \ you can retrieve the  encoder representations very easily:</p>\n<ul>\n\
          <li>I suggest you clone the repository directly: <a rel=\"nofollow\" href=\"\
          https://gitlab.teklia.com/dla/doc-ufcn\">https://gitlab.teklia.com/dla/doc-ufcn</a></li>\n\
          <li>Install the environment using <code>pip install -e .</code></li>\n<li>To\
          \ keep only the encoder part, you can simply comment/remove the last part\
          \ of the forward method in the <code>model.py</code> file:</li>\n</ul>\n\
          <pre><code class=\"language-python\">    <span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">forward</span>(<span class=\"hljs-params\"\
          >self, input_tensor</span>):\n        <span class=\"hljs-string\">\"\"\"\
          </span>\n<span class=\"hljs-string\">        Define the forward step of\
          \ the network.</span>\n<span class=\"hljs-string\">        It consists in\
          \ 4 successive dilated blocks followed by 3</span>\n<span class=\"hljs-string\"\
          >        convolutional blocks, a final convolution and a softmax layer.</span>\n\
          <span class=\"hljs-string\">        :param input_tensor: The input tensor.</span>\n\
          <span class=\"hljs-string\">        :return: The output tensor.</span>\n\
          <span class=\"hljs-string\">        \"\"\"</span>\n        <span class=\"\
          hljs-keyword\">with</span> autocast(enabled=self.amp):\n            tensor\
          \ = self.dilated_block1(input_tensor)\n            out_block1 = tensor\n\
          \            tensor = self.dilated_block2(self.pool(tensor))\n         \
          \   out_block2 = tensor\n            tensor = self.dilated_block3(self.pool(tensor))\n\
          \            out_block3 = tensor\n            tensor = self.dilated_block4(self.pool(tensor))\n\
          \            <span class=\"hljs-keyword\">return</span> tensor\n</code></pre>\n\
          <p>You can then follow the example given in the documentation and apply\
          \ the model as follows:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> cv2\n<span class=\"hljs-keyword\"\
          >from</span> doc_ufcn <span class=\"hljs-keyword\">import</span> models\n\
          <span class=\"hljs-keyword\">from</span> doc_ufcn.main <span class=\"hljs-keyword\"\
          >import</span> DocUFCN\n\nimage = cv2.cvtColor(cv2.imread(IMAGE_PATH), cv2.COLOR_BGR2RGB)\n\
          \nmodel_path, parameters = models.download_model(<span class=\"hljs-string\"\
          >'generic-historical-line'</span>)\nmodel = DocUFCN(<span class=\"hljs-built_in\"\
          >len</span>(parameters[<span class=\"hljs-string\">'classes'</span>]), parameters[<span\
          \ class=\"hljs-string\">'input_size'</span>], <span class=\"hljs-string\"\
          >'cpu'</span>)\nmodel.load(model_path, parameters[<span class=\"hljs-string\"\
          >'mean'</span>], parameters[<span class=\"hljs-string\">'std'</span>], mode=<span\
          \ class=\"hljs-string\">\"eval\"</span>)\n\n_, raw_output, _, _ = model.predict(image,\
          \ raw_output=<span class=\"hljs-literal\">True</span>)\n<span class=\"hljs-built_in\"\
          >print</span>(raw_output.shape)\n</code></pre>\n"
        raw: "Hello! There is no dedicated function to do this directly. However,\
          \ you can retrieve the  encoder representations very easily:\n* I suggest\
          \ you clone the repository directly: https://gitlab.teklia.com/dla/doc-ufcn\n\
          * Install the environment using `pip install -e .`\n* To keep only the encoder\
          \ part, you can simply comment/remove the last part of the forward method\
          \ in the `model.py` file:\n\n```python\n    def forward(self, input_tensor):\n\
          \        \"\"\"\n        Define the forward step of the network.\n     \
          \   It consists in 4 successive dilated blocks followed by 3\n        convolutional\
          \ blocks, a final convolution and a softmax layer.\n        :param input_tensor:\
          \ The input tensor.\n        :return: The output tensor.\n        \"\"\"\
          \n        with autocast(enabled=self.amp):\n            tensor = self.dilated_block1(input_tensor)\n\
          \            out_block1 = tensor\n            tensor = self.dilated_block2(self.pool(tensor))\n\
          \            out_block2 = tensor\n            tensor = self.dilated_block3(self.pool(tensor))\n\
          \            out_block3 = tensor\n            tensor = self.dilated_block4(self.pool(tensor))\n\
          \            return tensor\n```\n\nYou can then follow the example given\
          \ in the documentation and apply the model as follows:\n```python\nimport\
          \ cv2\nfrom doc_ufcn import models\nfrom doc_ufcn.main import DocUFCN\n\n\
          image = cv2.cvtColor(cv2.imread(IMAGE_PATH), cv2.COLOR_BGR2RGB)\n\nmodel_path,\
          \ parameters = models.download_model('generic-historical-line')\nmodel =\
          \ DocUFCN(len(parameters['classes']), parameters['input_size'], 'cpu')\n\
          model.load(model_path, parameters['mean'], parameters['std'], mode=\"eval\"\
          )\n\n_, raw_output, _, _ = model.predict(image, raw_output=True)\nprint(raw_output.shape)\n\
          ```\n\n"
        updatedAt: '2023-11-20T09:39:51.511Z'
      numEdits: 0
      reactions: []
    id: 655b2967deee83130a568621
    type: comment
  author: mboillet
  content: "Hello! There is no dedicated function to do this directly. However, you\
    \ can retrieve the  encoder representations very easily:\n* I suggest you clone\
    \ the repository directly: https://gitlab.teklia.com/dla/doc-ufcn\n* Install the\
    \ environment using `pip install -e .`\n* To keep only the encoder part, you can\
    \ simply comment/remove the last part of the forward method in the `model.py`\
    \ file:\n\n```python\n    def forward(self, input_tensor):\n        \"\"\"\n \
    \       Define the forward step of the network.\n        It consists in 4 successive\
    \ dilated blocks followed by 3\n        convolutional blocks, a final convolution\
    \ and a softmax layer.\n        :param input_tensor: The input tensor.\n     \
    \   :return: The output tensor.\n        \"\"\"\n        with autocast(enabled=self.amp):\n\
    \            tensor = self.dilated_block1(input_tensor)\n            out_block1\
    \ = tensor\n            tensor = self.dilated_block2(self.pool(tensor))\n    \
    \        out_block2 = tensor\n            tensor = self.dilated_block3(self.pool(tensor))\n\
    \            out_block3 = tensor\n            tensor = self.dilated_block4(self.pool(tensor))\n\
    \            return tensor\n```\n\nYou can then follow the example given in the\
    \ documentation and apply the model as follows:\n```python\nimport cv2\nfrom doc_ufcn\
    \ import models\nfrom doc_ufcn.main import DocUFCN\n\nimage = cv2.cvtColor(cv2.imread(IMAGE_PATH),\
    \ cv2.COLOR_BGR2RGB)\n\nmodel_path, parameters = models.download_model('generic-historical-line')\n\
    model = DocUFCN(len(parameters['classes']), parameters['input_size'], 'cpu')\n\
    model.load(model_path, parameters['mean'], parameters['std'], mode=\"eval\")\n\
    \n_, raw_output, _, _ = model.predict(image, raw_output=True)\nprint(raw_output.shape)\n\
    ```\n\n"
  created_at: 2023-11-20 09:39:51+00:00
  edited: false
  hidden: false
  id: 655b2967deee83130a568621
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
      fullname: Ajay Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimpeaper23
      type: user
    createdAt: '2023-11-20T19:20:22.000Z'
    data:
      edited: false
      editors:
      - grimpeaper23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9169359803199768
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
          fullname: Ajay Krishnan
          isHf: false
          isPro: false
          name: grimpeaper23
          type: user
        html: '<p>Got it. I implemented a similar workaround. Thank you!</p>

          '
        raw: Got it. I implemented a similar workaround. Thank you!
        updatedAt: '2023-11-20T19:20:22.147Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655bb176ec3e20cf00ed045e
    id: 655bb176ec3e20cf00ed045c
    type: comment
  author: grimpeaper23
  content: Got it. I implemented a similar workaround. Thank you!
  created_at: 2023-11-20 19:20:22+00:00
  edited: false
  hidden: false
  id: 655bb176ec3e20cf00ed045c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
      fullname: Ajay Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimpeaper23
      type: user
    createdAt: '2023-11-20T19:20:22.000Z'
    data:
      status: closed
    id: 655bb176ec3e20cf00ed045e
    type: status-change
  author: grimpeaper23
  created_at: 2023-11-20 19:20:22+00:00
  id: 655bb176ec3e20cf00ed045e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Teklia/doc-ufcn-generic-historical-line
repo_type: model
status: closed
target_branch: null
title: Get Image Embeddings
