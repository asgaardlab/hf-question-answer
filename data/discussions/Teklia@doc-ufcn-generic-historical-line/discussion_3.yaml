!!python/object:huggingface_hub.community.DiscussionWithDetails
author: grimpeaper23
conflicting_files: null
created_at: 2023-11-21 14:37:44+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
      fullname: Ajay Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimpeaper23
      type: user
    createdAt: '2023-11-21T14:37:44.000Z'
    data:
      edited: false
      editors:
      - grimpeaper23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9883504509925842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
          fullname: Ajay Krishnan
          isHf: false
          isPro: false
          name: grimpeaper23
          type: user
        html: '<p>Hi!</p>

          <p>Rookie doubt, I was going through the code and noticed there was no means
          to perform a batched inference so I have been writing my own code, did I
          miss out on anything or is there no batched support available exactly? And
          if there isn''t, do you have any pointers on how I could be going about
          it? Currently I''ve been modifying the predict function to support batches.</p>

          '
        raw: "Hi!\r\n\r\nRookie doubt, I was going through the code and noticed there\
          \ was no means to perform a batched inference so I have been writing my\
          \ own code, did I miss out on anything or is there no batched support available\
          \ exactly? And if there isn't, do you have any pointers on how I could be\
          \ going about it? Currently I've been modifying the predict function to\
          \ support batches."
        updatedAt: '2023-11-21T14:37:44.639Z'
      numEdits: 0
      reactions: []
    id: 655cc0b848320febfc51cb9e
    type: comment
  author: grimpeaper23
  content: "Hi!\r\n\r\nRookie doubt, I was going through the code and noticed there\
    \ was no means to perform a batched inference so I have been writing my own code,\
    \ did I miss out on anything or is there no batched support available exactly?\
    \ And if there isn't, do you have any pointers on how I could be going about it?\
    \ Currently I've been modifying the predict function to support batches."
  created_at: 2023-11-21 14:37:44+00:00
  edited: false
  hidden: false
  id: 655cc0b848320febfc51cb9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668065185273-noauth.png?w=200&h=200&f=face
      fullname: "M\xE9lodie Boillet"
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mboillet
      type: user
    createdAt: '2023-11-22T07:16:24.000Z'
    data:
      edited: false
      editors:
      - mboillet
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9233927130699158
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668065185273-noauth.png?w=200&h=200&f=face
          fullname: "M\xE9lodie Boillet"
          isHf: false
          isPro: false
          name: mboillet
          type: user
        html: '<p>Hello! There is currently no support for batch processing. We plan
          to add it in the next version.</p>

          <p>In the meantime, the best way to process in batches is to modify the
          <code>predict</code> function. Please note that you need to pad your images
          before processing them with the model. To pad them, you can take inspiration
          from the <code>pad_images_masks</code> method used during training and available
          in the <code>train/utils.__init__.py</code> file.</p>

          '
        raw: 'Hello! There is currently no support for batch processing. We plan to
          add it in the next version.


          In the meantime, the best way to process in batches is to modify the `predict`
          function. Please note that you need to pad your images before processing
          them with the model. To pad them, you can take inspiration from the `pad_images_masks`
          method used during training and available in the `train/utils.__init__.py`
          file.'
        updatedAt: '2023-11-22T07:16:24.046Z'
      numEdits: 0
      reactions: []
    id: 655daac87dbbdc709ecd5df8
    type: comment
  author: mboillet
  content: 'Hello! There is currently no support for batch processing. We plan to
    add it in the next version.


    In the meantime, the best way to process in batches is to modify the `predict`
    function. Please note that you need to pad your images before processing them
    with the model. To pad them, you can take inspiration from the `pad_images_masks`
    method used during training and available in the `train/utils.__init__.py` file.'
  created_at: 2023-11-22 07:16:24+00:00
  edited: false
  hidden: false
  id: 655daac87dbbdc709ecd5df8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
      fullname: Ajay Krishnan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: grimpeaper23
      type: user
    createdAt: '2023-11-22T18:47:50.000Z'
    data:
      edited: false
      editors:
      - grimpeaper23
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.838439404964447
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/da75c2ca1300c95d9c815298e9bcc7be.svg
          fullname: Ajay Krishnan
          isHf: false
          isPro: false
          name: grimpeaper23
          type: user
        html: '<p>When will the next version be out? Is there a specific date in mind?
          </p>

          '
        raw: 'When will the next version be out? Is there a specific date in mind? '
        updatedAt: '2023-11-22T18:47:50.546Z'
      numEdits: 0
      reactions: []
    id: 655e4cd6f4be8787c8c16c44
    type: comment
  author: grimpeaper23
  content: 'When will the next version be out? Is there a specific date in mind? '
  created_at: 2023-11-22 18:47:50+00:00
  edited: false
  hidden: false
  id: 655e4cd6f4be8787c8c16c44
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Teklia/doc-ufcn-generic-historical-line
repo_type: model
status: open
target_branch: null
title: Batched Inference
