!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Romanserk
conflicting_files: null
created_at: 2023-05-20 15:09:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66b3ccac3013aad9a361e643f53bbcd9.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Romanserk
      type: user
    createdAt: '2023-05-20T16:09:39.000Z'
    data:
      edited: false
      editors:
      - Romanserk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66b3ccac3013aad9a361e643f53bbcd9.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: Romanserk
          type: user
        html: '<p>First of all i want to say thank you for all your efforts, in addition
          could you provide some advice on the recommended hardware specifications
          required to effectively run models of this size ?</p>

          '
        raw: First of all i want to say thank you for all your efforts, in addition
          could you provide some advice on the recommended hardware specifications
          required to effectively run models of this size ?
        updatedAt: '2023-05-20T16:09:39.114Z'
      numEdits: 0
      reactions: []
    id: 6468f0c35d701566394ea8c1
    type: comment
  author: Romanserk
  content: First of all i want to say thank you for all your efforts, in addition
    could you provide some advice on the recommended hardware specifications required
    to effectively run models of this size ?
  created_at: 2023-05-20 15:09:39+00:00
  edited: false
  hidden: false
  id: 6468f0c35d701566394ea8c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T16:48:09.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>According to the model card, 4_0 should fit all layers into a 3090.</p>

          <p>I was going to download 5_1 and try that offloading some layers to RAM.
          Though haven''t tried it yet so maybe I''m wrong in my thinking.</p>

          '
        raw: 'According to the model card, 4_0 should fit all layers into a 3090.


          I was going to download 5_1 and try that offloading some layers to RAM.
          Though haven''t tried it yet so maybe I''m wrong in my thinking.'
        updatedAt: '2023-05-20T16:48:09.950Z'
      numEdits: 0
      reactions: []
    id: 6468f9c997ffc33d43c91901
    type: comment
  author: mancub
  content: 'According to the model card, 4_0 should fit all layers into a 3090.


    I was going to download 5_1 and try that offloading some layers to RAM. Though
    haven''t tried it yet so maybe I''m wrong in my thinking.'
  created_at: 2023-05-20 15:48:09+00:00
  edited: false
  hidden: false
  id: 6468f9c997ffc33d43c91901
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66b3ccac3013aad9a361e643f53bbcd9.svg
      fullname: Roman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Romanserk
      type: user
    createdAt: '2023-05-20T17:50:07.000Z'
    data:
      edited: false
      editors:
      - Romanserk
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66b3ccac3013aad9a361e643f53bbcd9.svg
          fullname: Roman
          isHf: false
          isPro: false
          name: Romanserk
          type: user
        html: '<p>Thank you for your reply, when you say should fit all layers what
          does it mean, can i expect response time faster then 100 tokens per second
          ? And is it possible to run on multiple graphics cards ?</p>

          '
        raw: Thank you for your reply, when you say should fit all layers what does
          it mean, can i expect response time faster then 100 tokens per second ?
          And is it possible to run on multiple graphics cards ?
        updatedAt: '2023-05-20T17:50:07.227Z'
      numEdits: 0
      reactions: []
    id: 6469084f99182de1784c6f9c
    type: comment
  author: Romanserk
  content: Thank you for your reply, when you say should fit all layers what does
    it mean, can i expect response time faster then 100 tokens per second ? And is
    it possible to run on multiple graphics cards ?
  created_at: 2023-05-20 16:50:07+00:00
  edited: false
  hidden: false
  id: 6469084f99182de1784c6f9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
      fullname: Man Cub
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mancub
      type: user
    createdAt: '2023-05-20T18:12:51.000Z'
    data:
      edited: false
      editors:
      - mancub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6da5ac096770743875fe1ee7eb1897f.svg
          fullname: Man Cub
          isHf: false
          isPro: false
          name: mancub
          type: user
        html: '<p>Most of these models we use have 40 layers in them and so loading
          the entire model into the VRAM greatly speeds up inference. When you load
          less than 40 layers (the entire model), then portion of the model is loaded
          into RAM and inevitably a slowdown happens. Here''s a random link that will
          explain layers, contexts, inference, etc: <a rel="nofollow" href="https://kipp.ly/blog/transformer-inference-arithmetic/">https://kipp.ly/blog/transformer-inference-arithmetic/</a></p>

          <p>I do not know of any consumer setup (graphics cards) that can produce
          100t/s.</p>

          <p>Here I''ve noticed a nice improvement when combining CPU+GPU to get ~10t/s.
          Prior to this on pure GPU I was getting 5-6t/s.</p>

          <p>You can load the model across multiple graphics cards, but inference
          will only happen on one as far as I understand it.</p>

          '
        raw: 'Most of these models we use have 40 layers in them and so loading the
          entire model into the VRAM greatly speeds up inference. When you load less
          than 40 layers (the entire model), then portion of the model is loaded into
          RAM and inevitably a slowdown happens. Here''s a random link that will explain
          layers, contexts, inference, etc: https://kipp.ly/blog/transformer-inference-arithmetic/


          I do not know of any consumer setup (graphics cards) that can produce 100t/s.


          Here I''ve noticed a nice improvement when combining CPU+GPU to get ~10t/s.
          Prior to this on pure GPU I was getting 5-6t/s.


          You can load the model across multiple graphics cards, but inference will
          only happen on one as far as I understand it.'
        updatedAt: '2023-05-20T18:12:51.147Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - Romanserk
        - PrimeD
        - mirek190
        - dzupin
    id: 64690da37407ab1cff3814f6
    type: comment
  author: mancub
  content: 'Most of these models we use have 40 layers in them and so loading the
    entire model into the VRAM greatly speeds up inference. When you load less than
    40 layers (the entire model), then portion of the model is loaded into RAM and
    inevitably a slowdown happens. Here''s a random link that will explain layers,
    contexts, inference, etc: https://kipp.ly/blog/transformer-inference-arithmetic/


    I do not know of any consumer setup (graphics cards) that can produce 100t/s.


    Here I''ve noticed a nice improvement when combining CPU+GPU to get ~10t/s. Prior
    to this on pure GPU I was getting 5-6t/s.


    You can load the model across multiple graphics cards, but inference will only
    happen on one as far as I understand it.'
  created_at: 2023-05-20 17:12:51+00:00
  edited: false
  hidden: false
  id: 64690da37407ab1cff3814f6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/VicUnlocked-30B-LoRA-GGML
repo_type: model
status: open
target_branch: null
title: Hardware recommendations
