!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Sven00
conflicting_files: null
created_at: 2023-05-24 05:37:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
      fullname: Sven Heyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sven00
      type: user
    createdAt: '2023-05-24T06:37:40.000Z'
    data:
      edited: true
      editors:
      - Sven00
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
          fullname: Sven Heyer
          isHf: false
          isPro: false
          name: Sven00
          type: user
        html: '<p>Hello</p>

          <p>I''m facing a very odd issue while running the following code:</p>

          <p>#################<br>tokenizer = AutoTokenizer.from_pretrained(afs_path+"nomic-ai/gpt4all-j")<br>model
          = AutoModelForCausalLM.from_pretrained(afs_path+"nomic-ai/gpt4all-j", torch_dtype=torch.float16,
          revision="v1.2-jazzy")<br>model = model.to(''cuda:0'')</p>

          <p>prompt = f"""The task is to write a summary of the follwing text: {input}"""<br>inputs
          = tokenizer(prompt, return_tensors=''pt'').to(model.device)<br>input_length
          = inputs.input_ids.shape[1]<br>outputs = model.generate(<br>    **inputs,
          max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50,
          return_dict_in_generate=True<br>)<br>token = outputs.sequences[0, input_length:]<br>output_str
          = tokenizer.decode(token)<br>print(output_str)<br>#################</p>

          <p>Specifically, the cell is executed successfully but the response is empty
          ("Setting <code>pad_token_id</code> to <code>eos_token_id</code>:50256 for
          open-end generation.<br>&lt;|endoftext|&gt;"). </p>

          <p>The prompt statement generates 714 tokens which is much less than the
          max token of 2048 for this model. In addition, the cell sometimes provides
          a reasonable output while executing multiple times (but totally on a random
          basis). This issue does not appear with a relatively low amount of input
          tokens (approx 250 or less). I have played with the parameters of "model.generate"
          but the issue remains same. I have also checked that GPU is NOT out of memory.</p>

          <p>Do you have an idea what the root cause is? Thank you</p>

          '
        raw: "Hello\n\nI'm facing a very odd issue while running the following code:\n\
          \n#################\ntokenizer = AutoTokenizer.from_pretrained(afs_path+\"\
          nomic-ai/gpt4all-j\")\nmodel = AutoModelForCausalLM.from_pretrained(afs_path+\"\
          nomic-ai/gpt4all-j\", torch_dtype=torch.float16, revision=\"v1.2-jazzy\"\
          ) \nmodel = model.to('cuda:0')\n\nprompt = f\"\"\"The task is to write a\
          \ summary of the follwing text: {input}\"\"\"\ninputs = tokenizer(prompt,\
          \ return_tensors='pt').to(model.device)\ninput_length = inputs.input_ids.shape[1]\n\
          outputs = model.generate(\n    **inputs, max_new_tokens=128, do_sample=True,\
          \ temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n)\n\
          token = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\n\
          print(output_str)\n#################\n\nSpecifically, the cell is executed\
          \ successfully but the response is empty (\"Setting `pad_token_id` to `eos_token_id`:50256\
          \ for open-end generation.\n<|endoftext|>\"). \n\nThe prompt statement generates\
          \ 714 tokens which is much less than the max token of 2048 for this model.\
          \ In addition, the cell sometimes provides a reasonable output while executing\
          \ multiple times (but totally on a random basis). This issue does not appear\
          \ with a relatively low amount of input tokens (approx 250 or less). I have\
          \ played with the parameters of \"model.generate\" but the issue remains\
          \ same. I have also checked that GPU is NOT out of memory.\n\nDo you have\
          \ an idea what the root cause is? Thank you"
        updatedAt: '2023-05-24T10:18:50.071Z'
      numEdits: 2
      reactions: []
    id: 646db0b4deb963805b30b905
    type: comment
  author: Sven00
  content: "Hello\n\nI'm facing a very odd issue while running the following code:\n\
    \n#################\ntokenizer = AutoTokenizer.from_pretrained(afs_path+\"nomic-ai/gpt4all-j\"\
    )\nmodel = AutoModelForCausalLM.from_pretrained(afs_path+\"nomic-ai/gpt4all-j\"\
    , torch_dtype=torch.float16, revision=\"v1.2-jazzy\") \nmodel = model.to('cuda:0')\n\
    \nprompt = f\"\"\"The task is to write a summary of the follwing text: {input}\"\
    \"\"\ninputs = tokenizer(prompt, return_tensors='pt').to(model.device)\ninput_length\
    \ = inputs.input_ids.shape[1]\noutputs = model.generate(\n    **inputs, max_new_tokens=128,\
    \ do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True\n\
    )\ntoken = outputs.sequences[0, input_length:]\noutput_str = tokenizer.decode(token)\n\
    print(output_str)\n#################\n\nSpecifically, the cell is executed successfully\
    \ but the response is empty (\"Setting `pad_token_id` to `eos_token_id`:50256\
    \ for open-end generation.\n<|endoftext|>\"). \n\nThe prompt statement generates\
    \ 714 tokens which is much less than the max token of 2048 for this model. In\
    \ addition, the cell sometimes provides a reasonable output while executing multiple\
    \ times (but totally on a random basis). This issue does not appear with a relatively\
    \ low amount of input tokens (approx 250 or less). I have played with the parameters\
    \ of \"model.generate\" but the issue remains same. I have also checked that GPU\
    \ is NOT out of memory.\n\nDo you have an idea what the root cause is? Thank you"
  created_at: 2023-05-24 05:37:40+00:00
  edited: true
  hidden: false
  id: 646db0b4deb963805b30b905
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/71d6b53ba2eea94e2bd3f0459c54601d.svg
      fullname: Sven Heyer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sven00
      type: user
    createdAt: '2023-05-24T06:38:12.000Z'
    data:
      from: Empty response with a undetermined amount of input tokens
      to: Empty response
    id: 646db0d4e34b2ec2d2cd83b3
    type: title-change
  author: Sven00
  created_at: 2023-05-24 05:38:12+00:00
  id: 646db0d4e34b2ec2d2cd83b3
  new_title: Empty response
  old_title: Empty response with a undetermined amount of input tokens
  type: title-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: nomic-ai/gpt4all-j
repo_type: model
status: open
target_branch: null
title: Empty response
