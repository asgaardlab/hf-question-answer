!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Alex18d
conflicting_files: null
created_at: 2023-07-21 19:58:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ba846423bf7a0fd53bb754e3cb9b660.svg
      fullname: Alexandr
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alex18d
      type: user
    createdAt: '2023-07-21T20:58:26.000Z'
    data:
      edited: false
      editors:
      - Alex18d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5266125202178955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ba846423bf7a0fd53bb754e3cb9b660.svg
          fullname: Alexandr
          isHf: false
          isPro: false
          name: Alex18d
          type: user
        html: '<p>Can I use this model for my own purposes if my Core i7-6700 computer
          has 16 GB RAM and integrated video. Maybe I''m not making the right settings
          or I don''t have the right hardware, so I get the error "RuntimeError: [enforce
          failed at ..\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator:
          not enough memory: you tried to allocate 67108864 bytes."<br>Can you tell
          me if this model or another one will work for me to generate text by promts?</p>

          <p>my main.py:</p>

          <p>import torch<br>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "GPT/LLongMA-2-7B-GPTQ"<br>model_basename = "gptq_model-4bit-32g"</p>

          <p>use_triton = False<br>device = "cuda:0" if torch.cuda.is_available()
          else "cpu"  # use CPU if CUDA is not available</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>                                           device=device,<br>                                           use_triton=False,<br>                                           use_safetensors=True,<br>                                           torch_dtype=torch.float32,<br>                                           trust_remote_code=True)</p>

          <p>prompt = "Tell me about AI"<br>prompt_template=f''''''{prompt}<br>''''''</p>

          <p>print("\n\n*** Generate:")</p>

          <p>input_ids = tokenizer(prompt_template, return_tensors=''pt'').input_ids.cuda()<br>output
          = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>

          <h1 id="inference-can-also-be-done-using-transformers-pipeline">Inference
          can also be done using transformers'' pipeline</h1>

          <h1 id="prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq">Prevent
          printing spurious transformers error when using pipeline with AutoGPTQ</h1>

          <p>logging.set_verbosity(logging.CRITICAL)</p>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])</p>

          '
        raw: "Can I use this model for my own purposes if my Core i7-6700 computer\
          \ has 16 GB RAM and integrated video. Maybe I'm not making the right settings\
          \ or I don't have the right hardware, so I get the error \"RuntimeError:\
          \ [enforce failed at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator:\
          \ not enough memory: you tried to allocate 67108864 bytes.\"\r\nCan you\
          \ tell me if this model or another one will work for me to generate text\
          \ by promts?\r\n\r\nmy main.py:\r\n\r\nimport torch\r\nfrom transformers\
          \ import AutoTokenizer, pipeline, logging\r\nfrom auto_gptq import AutoGPTQForCausalLM,\
          \ BaseQuantizeConfig\r\n\r\nmodel_name_or_path = \"GPT/LLongMA-2-7B-GPTQ\"\
          \r\nmodel_basename = \"gptq_model-4bit-32g\"\r\n\r\nuse_triton = False\r\
          \ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # use CPU\
          \ if CUDA is not available\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
          \ \r\n                                           device=device, \r\n   \
          \                                        use_triton=False, \r\n        \
          \                                   use_safetensors=True, \r\n         \
          \                                  torch_dtype=torch.float32, \r\n     \
          \                                      trust_remote_code=True)\r\n\r\n\r\
          \nprompt = \"Tell me about AI\"\r\nprompt_template=f'''{prompt}\r\n'''\r\
          \n\r\nprint(\"\\n\\n*** Generate:\")\r\n\r\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids.cuda()\r\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\r\nprint(tokenizer.decode(output[0]))\r\
          \n\r\n# Inference can also be done using transformers' pipeline\r\n\r\n\
          # Prevent printing spurious transformers error when using pipeline with\
          \ AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\n\r\nprint(\"***\
          \ Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\",\r\n    model=model,\r\
          \n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\n    temperature=0.7,\r\
          \n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\
          \n"
        updatedAt: '2023-07-21T20:58:26.888Z'
      numEdits: 0
      reactions: []
    id: 64baf17236eb058cd9bd9671
    type: comment
  author: Alex18d
  content: "Can I use this model for my own purposes if my Core i7-6700 computer has\
    \ 16 GB RAM and integrated video. Maybe I'm not making the right settings or I\
    \ don't have the right hardware, so I get the error \"RuntimeError: [enforce failed\
    \ at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough\
    \ memory: you tried to allocate 67108864 bytes.\"\r\nCan you tell me if this model\
    \ or another one will work for me to generate text by promts?\r\n\r\nmy main.py:\r\
    \n\r\nimport torch\r\nfrom transformers import AutoTokenizer, pipeline, logging\r\
    \nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\r\n\r\nmodel_name_or_path\
    \ = \"GPT/LLongMA-2-7B-GPTQ\"\r\nmodel_basename = \"gptq_model-4bit-32g\"\r\n\r\
    \nuse_triton = False\r\ndevice = \"cuda:0\" if torch.cuda.is_available() else\
    \ \"cpu\"  # use CPU if CUDA is not available\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True)\r\n\r\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\
    \ \r\n                                           device=device, \r\n         \
    \                                  use_triton=False, \r\n                    \
    \                       use_safetensors=True, \r\n                           \
    \                torch_dtype=torch.float32, \r\n                             \
    \              trust_remote_code=True)\r\n\r\n\r\nprompt = \"Tell me about AI\"\
    \r\nprompt_template=f'''{prompt}\r\n'''\r\n\r\nprint(\"\\n\\n*** Generate:\")\r\
    \n\r\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\r\
    \noutput = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\r\
    \nprint(tokenizer.decode(output[0]))\r\n\r\n# Inference can also be done using\
    \ transformers' pipeline\r\n\r\n# Prevent printing spurious transformers error\
    \ when using pipeline with AutoGPTQ\r\nlogging.set_verbosity(logging.CRITICAL)\r\
    \n\r\nprint(\"*** Pipeline:\")\r\npipe = pipeline(\r\n    \"text-generation\"\
    ,\r\n    model=model,\r\n    tokenizer=tokenizer,\r\n    max_new_tokens=512,\r\
    \n    temperature=0.7,\r\n    top_p=0.95,\r\n    repetition_penalty=1.15\r\n)\r\
    \n\r\nprint(pipe(prompt_template)[0]['generated_text'])\r\n"
  created_at: 2023-07-21 19:58:26+00:00
  edited: false
  hidden: false
  id: 64baf17236eb058cd9bd9671
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-07-22T08:25:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.986505389213562
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I don''t think an integrated GPU is going to provide enough VRAM</p>

          <p>I suggest to try the GGML models instead.</p>

          '
        raw: 'I don''t think an integrated GPU is going to provide enough VRAM


          I suggest to try the GGML models instead.'
        updatedAt: '2023-07-22T08:25:20.564Z'
      numEdits: 0
      reactions: []
    id: 64bb9270976343e90a33e235
    type: comment
  author: TheBloke
  content: 'I don''t think an integrated GPU is going to provide enough VRAM


    I suggest to try the GGML models instead.'
  created_at: 2023-07-22 07:25:20+00:00
  edited: false
  hidden: false
  id: 64bb9270976343e90a33e235
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/LLongMA-2-7B-GPTQ
repo_type: model
status: open
target_branch: null
title: Can I use this model for my own purposes with the hardware?
