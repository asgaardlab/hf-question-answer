!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sbakhtyar
conflicting_files: null
created_at: 2024-01-25 09:59:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cdedebf3492482456f6e26464af27b22.svg
      fullname: khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sbakhtyar
      type: user
    createdAt: '2024-01-25T09:59:54.000Z'
    data:
      edited: false
      editors:
      - sbakhtyar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48802533745765686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cdedebf3492482456f6e26464af27b22.svg
          fullname: khan
          isHf: false
          isPro: false
          name: sbakhtyar
          type: user
        html: '<p>phi-2 Model :<br>PhiForCausalLM(<br>  (model): PhiModel(<br>    (embed_tokens):
          Embedding(51200, 2560)<br>    (embed_dropout): Dropout(p=0.0, inplace=False)<br>    (layers):
          ModuleList(<br>      (0-31): 32 x PhiDecoderLayer(<br>        (self_attn):
          PhiAttention(<br>          (q_proj): Linear4bit(in_features=2560, out_features=2560,
          bias=True)<br>          (k_proj): Linear4bit(in_features=2560, out_features=2560,
          bias=True)<br>          (v_proj): Linear4bit(in_features=2560, out_features=2560,
          bias=True)<br>          (dense): Linear4bit(in_features=2560, out_features=2560,
          bias=True)<br>          (rotary_emb): PhiRotaryEmbedding()<br>        )<br>        (mlp):
          PhiMLP(<br>          (activation_fn): NewGELUActivation()<br>          (fc1):
          Linear4bit(in_features=2560, out_features=10240, bias=True)<br>          (fc2):
          Linear4bit(in_features=10240, out_features=2560, bias=True)<br>        )<br>        (input_layernorm):
          LayerNorm((2560,), eps=1e-05, elementwise_affine=True)<br>        (resid_dropout):
          Dropout(p=0.1, inplace=False)<br>      )<br>    )<br>    (final_layernorm):
          LayerNorm((2560,), eps=1e-05, elementwise_affine=True)<br>  )<br>  (lm_head):
          Linear(in_features=2560, out_features=51200, bias=True)<br>)</p>

          <p>Cannot see ["Wqkv", "out_proj"] modules that is to be used in Universal-NER.
          How is it working.</p>

          '
        raw: "phi-2 Model : \r\nPhiForCausalLM(\r\n  (model): PhiModel(\r\n    (embed_tokens):\
          \ Embedding(51200, 2560)\r\n    (embed_dropout): Dropout(p=0.0, inplace=False)\r\
          \n    (layers): ModuleList(\r\n      (0-31): 32 x PhiDecoderLayer(\r\n \
          \       (self_attn): PhiAttention(\r\n          (q_proj): Linear4bit(in_features=2560,\
          \ out_features=2560, bias=True)\r\n          (k_proj): Linear4bit(in_features=2560,\
          \ out_features=2560, bias=True)\r\n          (v_proj): Linear4bit(in_features=2560,\
          \ out_features=2560, bias=True)\r\n          (dense): Linear4bit(in_features=2560,\
          \ out_features=2560, bias=True)\r\n          (rotary_emb): PhiRotaryEmbedding()\r\
          \n        )\r\n        (mlp): PhiMLP(\r\n          (activation_fn): NewGELUActivation()\r\
          \n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\r\
          \n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\r\
          \n        )\r\n        (input_layernorm): LayerNorm((2560,), eps=1e-05,\
          \ elementwise_affine=True)\r\n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\
          \n      )\r\n    )\r\n    (final_layernorm): LayerNorm((2560,), eps=1e-05,\
          \ elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=2560,\
          \ out_features=51200, bias=True)\r\n)\r\n\r\nCannot see [\"Wqkv\", \"out_proj\"\
          ] modules that is to be used in Universal-NER. How is it working."
        updatedAt: '2024-01-25T09:59:54.950Z'
      numEdits: 0
      reactions: []
    id: 65b2311acd9b73cf84659884
    type: comment
  author: sbakhtyar
  content: "phi-2 Model : \r\nPhiForCausalLM(\r\n  (model): PhiModel(\r\n    (embed_tokens):\
    \ Embedding(51200, 2560)\r\n    (embed_dropout): Dropout(p=0.0, inplace=False)\r\
    \n    (layers): ModuleList(\r\n      (0-31): 32 x PhiDecoderLayer(\r\n       \
    \ (self_attn): PhiAttention(\r\n          (q_proj): Linear4bit(in_features=2560,\
    \ out_features=2560, bias=True)\r\n          (k_proj): Linear4bit(in_features=2560,\
    \ out_features=2560, bias=True)\r\n          (v_proj): Linear4bit(in_features=2560,\
    \ out_features=2560, bias=True)\r\n          (dense): Linear4bit(in_features=2560,\
    \ out_features=2560, bias=True)\r\n          (rotary_emb): PhiRotaryEmbedding()\r\
    \n        )\r\n        (mlp): PhiMLP(\r\n          (activation_fn): NewGELUActivation()\r\
    \n          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\r\
    \n          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\r\
    \n        )\r\n        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\r\
    \n        (resid_dropout): Dropout(p=0.1, inplace=False)\r\n      )\r\n    )\r\
    \n    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\r\
    \n  )\r\n  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\r\
    \n)\r\n\r\nCannot see [\"Wqkv\", \"out_proj\"] modules that is to be used in Universal-NER.\
    \ How is it working."
  created_at: 2024-01-25 09:59:54+00:00
  edited: false
  hidden: false
  id: 65b2311acd9b73cf84659884
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Mit1208/phi-2-universal-NER
repo_type: model
status: open
target_branch: null
title: Cannot see ["Wqkv", "out_proj"] modules in PHI-2 that is to be used in Universal-NER.
  How is it working.
