!!python/object:huggingface_hub.community.DiscussionWithDetails
author: igroboy
conflicting_files: null
created_at: 2023-08-15 15:18:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/544d0f88e344c0d3d3ed2be9711413b7.svg
      fullname: Evgeny
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: igroboy
      type: user
    createdAt: '2023-08-15T16:18:57.000Z'
    data:
      edited: false
      editors:
      - igroboy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9763617515563965
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/544d0f88e344c0d3d3ed2be9711413b7.svg
          fullname: Evgeny
          isHf: false
          isPro: false
          name: igroboy
          type: user
        html: '<p>Why results of Saiga 2 are much worse on V100? I tried inference
          model on V100, and indeed results are not satisfying. What''s the reason?
          And can I somehow fix it?</p>

          '
        raw: Why results of Saiga 2 are much worse on V100? I tried inference model
          on V100, and indeed results are not satisfying. What's the reason? And can
          I somehow fix it?
        updatedAt: '2023-08-15T16:18:57.660Z'
      numEdits: 0
      reactions: []
    id: 64dba571e7bc8544f9a38191
    type: comment
  author: igroboy
  content: Why results of Saiga 2 are much worse on V100? I tried inference model
    on V100, and indeed results are not satisfying. What's the reason? And can I somehow
    fix it?
  created_at: 2023-08-15 15:18:57+00:00
  edited: false
  hidden: false
  id: 64dba571e7bc8544f9a38191
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-08-18T09:55:12.000Z'
    data:
      edited: true
      editors:
      - IlyaGusev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7161337733268738
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
          fullname: Ilya Gusev
          isHf: false
          isPro: true
          name: IlyaGusev
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues/18">https://github.com/TimDettmers/bitsandbytes/issues/18</a><br><a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues/529">https://github.com/TimDettmers/bitsandbytes/issues/529</a><br><a
          rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/issues/379">https://github.com/oobabooga/text-generation-webui/issues/379</a></p>

          <p>8-bit quantization requires features that are not a part of V100. Differences
          in <a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/autograd/_functions.py#L225C1-L225C1">igemmlt</a>
          seem to be the main problem.</p>

          <p>To fix it: don''t use 8-bit quantization. Or don''t use V100.</p>

          '
        raw: 'https://github.com/TimDettmers/bitsandbytes/issues/18

          https://github.com/TimDettmers/bitsandbytes/issues/529

          https://github.com/oobabooga/text-generation-webui/issues/379


          8-bit quantization requires features that are not a part of V100. Differences
          in [igemmlt](https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/autograd/_functions.py#L225C1-L225C1)
          seem to be the main problem.


          To fix it: don''t use 8-bit quantization. Or don''t use V100.'
        updatedAt: '2023-08-18T09:56:04.165Z'
      numEdits: 1
      reactions: []
    id: 64df4000411804d5a0654fb7
    type: comment
  author: IlyaGusev
  content: 'https://github.com/TimDettmers/bitsandbytes/issues/18

    https://github.com/TimDettmers/bitsandbytes/issues/529

    https://github.com/oobabooga/text-generation-webui/issues/379


    8-bit quantization requires features that are not a part of V100. Differences
    in [igemmlt](https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/autograd/_functions.py#L225C1-L225C1)
    seem to be the main problem.


    To fix it: don''t use 8-bit quantization. Or don''t use V100.'
  created_at: 2023-08-18 08:55:12+00:00
  edited: true
  hidden: false
  id: 64df4000411804d5a0654fb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1612371927570-5fc2346dea82dd667bb0ffbc.jpeg?w=200&h=200&f=face
      fullname: Ilya Gusev
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: IlyaGusev
      type: user
    createdAt: '2023-08-24T09:58:29.000Z'
    data:
      status: closed
    id: 64e729c50c47bf287c9083ac
    type: status-change
  author: IlyaGusev
  created_at: 2023-08-24 08:58:29+00:00
  id: 64e729c50c47bf287c9083ac
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: IlyaGusev/saiga2_7b_lora
repo_type: model
status: closed
target_branch: null
title: Why results are much worse on V100?
