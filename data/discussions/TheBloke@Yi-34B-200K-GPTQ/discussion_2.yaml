!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zuzuou
conflicting_files: null
created_at: 2023-11-14 03:47:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e04f161fbf3efdb6c533b205743c9a0.svg
      fullname: bo.zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zuzuou
      type: user
    createdAt: '2023-11-14T03:47:18.000Z'
    data:
      edited: false
      editors:
      - zuzuou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3369790017604828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e04f161fbf3efdb6c533b205743c9a0.svg
          fullname: bo.zhou
          isHf: false
          isPro: false
          name: zuzuou
          type: user
        html: "<p>I encounter this error when loading  Yi-34B-200K-GPTQ model. How\
          \ can I deal with it ?</p>\n<pre><code>(yi) [root@node4 yi]# python demo/text_generation.py\
          \  --model /data/models/Yi-34B-200K-GPTQ\nNamespace(model='/data/models/Yi-34B-200K-GPTQ',\
          \ tokenizer='', max_tokens=512, streaming=False, prompt='Let me tell you\
          \ an interesting story about cat Tom and mouse Jerry,', eos_token='&lt;|endoftext|&gt;')\n\
          Traceback (most recent call last):\n  File \"/data/app/yi/demo/text_generation.py\"\
          , line 78, in &lt;module&gt;\n    main(args)\n  File \"/data/app/yi/demo/text_generation.py\"\
          , line 49, in main\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 560, in from_pretrained\n    return model_class.from_pretrained(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3019, in from_pretrained\n    state_dict = load_state_dict(resolved_archive_file)\n\
          \                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 463, in load_state_dict\n    with safe_open(checkpoint_file, framework=\"\
          pt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nsafetensors_rust.SafetensorError:\
          \ Error while deserializing header: HeaderTooLarge\n</code></pre>\n"
        raw: "I encounter this error when loading  Yi-34B-200K-GPTQ model. How can\
          \ I deal with it ?\r\n```\r\n(yi) [root@node4 yi]# python demo/text_generation.py\
          \  --model /data/models/Yi-34B-200K-GPTQ\r\nNamespace(model='/data/models/Yi-34B-200K-GPTQ',\
          \ tokenizer='', max_tokens=512, streaming=False, prompt='Let me tell you\
          \ an interesting story about cat Tom and mouse Jerry,', eos_token='<|endoftext|>')\r\
          \nTraceback (most recent call last):\r\n  File \"/data/app/yi/demo/text_generation.py\"\
          , line 78, in <module>\r\n    main(args)\r\n  File \"/data/app/yi/demo/text_generation.py\"\
          , line 49, in main\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 560, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3019, in from_pretrained\r\n    state_dict = load_state_dict(resolved_archive_file)\r\
          \n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 463, in load_state_dict\r\n    with safe_open(checkpoint_file, framework=\"\
          pt\") as f:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nsafetensors_rust.SafetensorError:\
          \ Error while deserializing header: HeaderTooLarge\r\n```"
        updatedAt: '2023-11-14T03:47:18.690Z'
      numEdits: 0
      reactions: []
    id: 6552edc665818134ec6dbc6c
    type: comment
  author: zuzuou
  content: "I encounter this error when loading  Yi-34B-200K-GPTQ model. How can I\
    \ deal with it ?\r\n```\r\n(yi) [root@node4 yi]# python demo/text_generation.py\
    \  --model /data/models/Yi-34B-200K-GPTQ\r\nNamespace(model='/data/models/Yi-34B-200K-GPTQ',\
    \ tokenizer='', max_tokens=512, streaming=False, prompt='Let me tell you an interesting\
    \ story about cat Tom and mouse Jerry,', eos_token='<|endoftext|>')\r\nTraceback\
    \ (most recent call last):\r\n  File \"/data/app/yi/demo/text_generation.py\"\
    , line 78, in <module>\r\n    main(args)\r\n  File \"/data/app/yi/demo/text_generation.py\"\
    , line 49, in main\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n  \
    \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 560, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3019, in from_pretrained\r\n    state_dict = load_state_dict(resolved_archive_file)\r\
    \n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/anaconda3/envs/yi/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 463, in load_state_dict\r\n    with safe_open(checkpoint_file, framework=\"\
    pt\") as f:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nsafetensors_rust.SafetensorError:\
    \ Error while deserializing header: HeaderTooLarge\r\n```"
  created_at: 2023-11-14 03:47:18+00:00
  edited: false
  hidden: false
  id: 6552edc665818134ec6dbc6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-14T21:30:14.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7258186340332031
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''m not sure what <code>demo/text_generation.py</code> is? Is that
          launching TGI?  If so, try adding <code>--quantize gptq</code> to the arguments</p>

          <p>You''ll also need <code>--trust-remote-code</code>.</p>

          <p>Or is this your custom code?  If it is, then errors about deserializing
          headers usually mean a bad download. Try downloading the model weights again
          and make sure the checksums match.</p>

          '
        raw: 'I''m not sure what `demo/text_generation.py` is? Is that launching TGI?  If
          so, try adding `--quantize gptq` to the arguments


          You''ll also need `--trust-remote-code`.


          Or is this your custom code?  If it is, then errors about deserializing
          headers usually mean a bad download. Try downloading the model weights again
          and make sure the checksums match.'
        updatedAt: '2023-11-14T21:31:35.812Z'
      numEdits: 1
      reactions: []
    id: 6553e6e6eeb42b373ff35d25
    type: comment
  author: TheBloke
  content: 'I''m not sure what `demo/text_generation.py` is? Is that launching TGI?  If
    so, try adding `--quantize gptq` to the arguments


    You''ll also need `--trust-remote-code`.


    Or is this your custom code?  If it is, then errors about deserializing headers
    usually mean a bad download. Try downloading the model weights again and make
    sure the checksums match.'
  created_at: 2023-11-14 21:30:14+00:00
  edited: true
  hidden: false
  id: 6553e6e6eeb42b373ff35d25
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Yi-34B-200K-GPTQ
repo_type: model
status: open
target_branch: null
title: 'safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge'
