!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dareposte
conflicting_files: null
created_at: 2023-11-11 12:01:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
      fullname: Derek Yates
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dareposte
      type: user
    createdAt: '2023-11-11T12:01:03.000Z'
    data:
      edited: false
      editors:
      - dareposte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.766221821308136
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
          fullname: Derek Yates
          isHf: false
          isPro: true
          name: dareposte
          type: user
        html: '<p>Could not get this to load in transformers or exllama, AutoGPTQ,
          or exllamav2.   Model card states otherwise, so curious if I''m doing it
          wrong or if the card has an error in it.   Thanks for the conversion.   I
          know it''s a new model, just curious what other people are doing and if
          this has the llama adaptations applied to it or if it''s just a conversion
          of the Yi naming convention.</p>

          <p>ExLlama:  KeyError: ''model.layers.0.input_layernorm.weight'' --&gt;.
          Looks due to the Yi model''s naming on the layers.<br>ExLlamav2: ValueError:
          ## Could not find model.layers.0.input_layernorm.* in model --&gt; Same
          problem?<br>Transformers: is_auto_gptq_available ---&gt; AutoGPTQ does not
          support Yi model yet?<br>raise ImportError(<br>AutoGPTQ: raise TypeError(f"{config.model_type}
          isn''t supported yet.") --&gt; Same?</p>

          '
        raw: "Could not get this to load in transformers or exllama, AutoGPTQ, or\
          \ exllamav2.   Model card states otherwise, so curious if I'm doing it wrong\
          \ or if the card has an error in it.   Thanks for the conversion.   I know\
          \ it's a new model, just curious what other people are doing and if this\
          \ has the llama adaptations applied to it or if it's just a conversion of\
          \ the Yi naming convention.\r\n\r\nExLlama:  KeyError: 'model.layers.0.input_layernorm.weight'\
          \ -->. Looks due to the Yi model's naming on the layers.\r\nExLlamav2: ValueError:\
          \ ## Could not find model.layers.0.input_layernorm.* in model --> Same problem?\r\
          \nTransformers: is_auto_gptq_available ---> AutoGPTQ does not support Yi\
          \ model yet?\r\nraise ImportError(\r\nAutoGPTQ: raise TypeError(f\"{config.model_type}\
          \ isn't supported yet.\") --> Same?\r\n\r\n"
        updatedAt: '2023-11-11T12:01:03.821Z'
      numEdits: 0
      reactions: []
    id: 654f6cff0c11ee1eb9b89fc5
    type: comment
  author: dareposte
  content: "Could not get this to load in transformers or exllama, AutoGPTQ, or exllamav2.\
    \   Model card states otherwise, so curious if I'm doing it wrong or if the card\
    \ has an error in it.   Thanks for the conversion.   I know it's a new model,\
    \ just curious what other people are doing and if this has the llama adaptations\
    \ applied to it or if it's just a conversion of the Yi naming convention.\r\n\r\
    \nExLlama:  KeyError: 'model.layers.0.input_layernorm.weight' -->. Looks due to\
    \ the Yi model's naming on the layers.\r\nExLlamav2: ValueError: ## Could not\
    \ find model.layers.0.input_layernorm.* in model --> Same problem?\r\nTransformers:\
    \ is_auto_gptq_available ---> AutoGPTQ does not support Yi model yet?\r\nraise\
    \ ImportError(\r\nAutoGPTQ: raise TypeError(f\"{config.model_type} isn't supported\
    \ yet.\") --> Same?\r\n\r\n"
  created_at: 2023-11-11 12:01:03+00:00
  edited: false
  hidden: false
  id: 654f6cff0c11ee1eb9b89fc5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-11-11T15:01:46.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9929863810539246
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>I believe exllamav2 has support at least, you just have to update
          it</p>

          '
        raw: I believe exllamav2 has support at least, you just have to update it
        updatedAt: '2023-11-11T15:01:46.296Z'
      numEdits: 0
      reactions: []
    id: 654f975ac0e106160e4ea743
    type: comment
  author: YaTharThShaRma999
  content: I believe exllamav2 has support at least, you just have to update it
  created_at: 2023-11-11 15:01:46+00:00
  edited: false
  hidden: false
  id: 654f975ac0e106160e4ea743
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
      fullname: Derek Yates
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dareposte
      type: user
    createdAt: '2023-11-11T15:34:40.000Z'
    data:
      edited: false
      editors:
      - dareposte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9478169679641724
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
          fullname: Derek Yates
          isHf: false
          isPro: true
          name: dareposte
          type: user
        html: '<p>Tried this with the latest exllamav2 (git pull, pip install -e .)
          but still getting the issue.   Was able to load the other AWQ model using
          AutoAWQ, and able to load the GGUF using llama.cpp.   Also was able to get
          exl2 to load an EXL2 file quant from another user, just not able to use
          this particular one.   I''m sure the issue is on my end, thanks for the
          advice.</p>

          '
        raw: Tried this with the latest exllamav2 (git pull, pip install -e .) but
          still getting the issue.   Was able to load the other AWQ model using AutoAWQ,
          and able to load the GGUF using llama.cpp.   Also was able to get exl2 to
          load an EXL2 file quant from another user, just not able to use this particular
          one.   I'm sure the issue is on my end, thanks for the advice.
        updatedAt: '2023-11-11T15:34:40.243Z'
      numEdits: 0
      reactions: []
    id: 654f9f10c0e106160e4fe6e2
    type: comment
  author: dareposte
  content: Tried this with the latest exllamav2 (git pull, pip install -e .) but still
    getting the issue.   Was able to load the other AWQ model using AutoAWQ, and able
    to load the GGUF using llama.cpp.   Also was able to get exl2 to load an EXL2
    file quant from another user, just not able to use this particular one.   I'm
    sure the issue is on my end, thanks for the advice.
  created_at: 2023-11-11 15:34:40+00:00
  edited: false
  hidden: false
  id: 654f9f10c0e106160e4fe6e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
      fullname: Vladimir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vdruts
      type: user
    createdAt: '2023-11-11T15:55:51.000Z'
    data:
      edited: false
      editors:
      - vdruts
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.206534743309021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661628312160-noauth.png?w=200&h=200&f=face
          fullname: Vladimir
          isHf: false
          isPro: false
          name: vdruts
          type: user
        html: '<p>Same, have tried everything.</p>

          <p>2023-11-11 10:54:10 ERROR:Failed to load the model.<br>Traceback (most
          recent call last):<br>  File "C:\Users\xxxx\Deep\text-generation-webui\modules\ui_model_menu.py",
          line 210, in load_model_wrapper<br>    shared.model, shared.tokenizer =
          load_model(shared.model_name, loader)<br>                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\modules\models.py", line 85, in
          load_model<br>    output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\modules\models.py", line 350,
          in ExLlama_HF_loader<br>    return ExllamaHF.from_pretrained(model_name)<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\modules\exllama_hf.py", line 174,
          in from_pretrained<br>    return ExllamaHF(config)<br>           ^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\modules\exllama_hf.py", line 31,
          in <strong>init</strong><br>    self.ex_model = ExLlama(self.ex_config)<br>                    ^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\installer_files\env\Lib\site-packages\exllama\model.py",
          line 889, in <strong>init</strong><br>    layer = ExLlamaDecoderLayer(self.config,
          tensors, f"model.layers.{i}", i, sin, cos)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\installer_files\env\Lib\site-packages\exllama\model.py",
          line 520, in <strong>init</strong><br>    self.input_layernorm = ExLlamaRMSNorm(self.config,
          tensors, key + ".input_layernorm.weight")<br>                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "C:\Users\xxxx\Deep\text-generation-webui\installer_files\env\Lib\site-packages\exllama\model.py",
          line 284, in <strong>init</strong><br>    self.weight = tensors[key]<br>                  ~~~~~~~^^^^^<br>KeyError:
          ''model.layers.0.input_layernorm.weight''</p>

          '
        raw: "Same, have tried everything.\n\n2023-11-11 10:54:10 ERROR:Failed to\
          \ load the model.\nTraceback (most recent call last):\n  File \"C:\\Users\\\
          xxxx\\Deep\\text-generation-webui\\modules\\ui_model_menu.py\", line 210,\
          \ in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\modules\\models.py\"\
          , line 85, in load_model\n    output = load_func_map[loader](model_name)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\xxxx\\\
          Deep\\text-generation-webui\\modules\\models.py\", line 350, in ExLlama_HF_loader\n\
          \    return ExllamaHF.from_pretrained(model_name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\modules\\exllama_hf.py\"\
          , line 174, in from_pretrained\n    return ExllamaHF(config)\n         \
          \  ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\\
          modules\\exllama_hf.py\", line 31, in __init__\n    self.ex_model = ExLlama(self.ex_config)\n\
          \                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\xxxx\\\
          Deep\\text-generation-webui\\installer_files\\env\\Lib\\site-packages\\\
          exllama\\model.py\", line 889, in __init__\n    layer = ExLlamaDecoderLayer(self.config,\
          \ tensors, f\"model.layers.{i}\", i, sin, cos)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\exllama\\model.py\", line 520, in __init__\n  \
          \  self.input_layernorm = ExLlamaRMSNorm(self.config, tensors, key + \"\
          .input_layernorm.weight\")\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\installer_files\\\
          env\\Lib\\site-packages\\exllama\\model.py\", line 284, in __init__\n  \
          \  self.weight = tensors[key]\n                  ~~~~~~~^^^^^\nKeyError:\
          \ 'model.layers.0.input_layernorm.weight'"
        updatedAt: '2023-11-11T15:55:51.236Z'
      numEdits: 0
      reactions: []
    id: 654fa4075491c57ee9ab59a4
    type: comment
  author: vdruts
  content: "Same, have tried everything.\n\n2023-11-11 10:54:10 ERROR:Failed to load\
    \ the model.\nTraceback (most recent call last):\n  File \"C:\\Users\\xxxx\\Deep\\\
    text-generation-webui\\modules\\ui_model_menu.py\", line 210, in load_model_wrapper\n\
    \    shared.model, shared.tokenizer = load_model(shared.model_name, loader)\n\
    \                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\modules\\models.py\",\
    \ line 85, in load_model\n    output = load_func_map[loader](model_name)\n   \
    \          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\xxxx\\Deep\\\
    text-generation-webui\\modules\\models.py\", line 350, in ExLlama_HF_loader\n\
    \    return ExllamaHF.from_pretrained(model_name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\modules\\exllama_hf.py\"\
    , line 174, in from_pretrained\n    return ExllamaHF(config)\n           ^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\modules\\exllama_hf.py\"\
    , line 31, in __init__\n    self.ex_model = ExLlama(self.ex_config)\n        \
    \            ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\exllama\\model.py\", line 889, in __init__\n\
    \    layer = ExLlamaDecoderLayer(self.config, tensors, f\"model.layers.{i}\",\
    \ i, sin, cos)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\installer_files\\env\\\
    Lib\\site-packages\\exllama\\model.py\", line 520, in __init__\n    self.input_layernorm\
    \ = ExLlamaRMSNorm(self.config, tensors, key + \".input_layernorm.weight\")\n\
    \                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"C:\\Users\\xxxx\\Deep\\text-generation-webui\\installer_files\\env\\\
    Lib\\site-packages\\exllama\\model.py\", line 284, in __init__\n    self.weight\
    \ = tensors[key]\n                  ~~~~~~~^^^^^\nKeyError: 'model.layers.0.input_layernorm.weight'"
  created_at: 2023-11-11 15:55:51+00:00
  edited: false
  hidden: false
  id: 654fa4075491c57ee9ab59a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-11T15:57:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9782230257987976
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dareposte&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dareposte\">@<span class=\"\
          underline\">dareposte</span></a></span>\n\n\t</span></span> are you able\
          \ to check if you can load my Yi-34B-GPTQ, ie the non 200K version, using\
          \ ExLlama?</p>\n<p>It's expected that AutoGPTQ won't be able to load it.\
          \  I didn't realise that Transformers wouldn't be able to load it, but now\
          \ I see the error message, then I guess that's expected too, though disappointing.\
          \  Transformers can make GPTQs that AutoGPTQ can't make, so I thought it\
          \ could load them too. I guess not.</p>\n<p>But I've been told that ExLlama\
          \ added specific support for Yi, so I'm surprised that ExLlama can't load\
          \ it.  I actually made this GPTQ differently to how I made the non-200K\
          \ version; I used AutoGPTQ to make it, using an AutoGPTQ PR which adds Yi\
          \ support to make it.  The non-200K was made with Transformers, and I was\
          \ told that was working with ExLlama after turboderp added support.</p>\n\
          <p>Let me know if the non-200K works and if necessary I'll re-make these\
          \ 200K GPTQs with Transformers, like I made the non-200K ones.  And/or get\
          \ in touch with turboderp about it.</p>\n"
        raw: '@dareposte are you able to check if you can load my Yi-34B-GPTQ, ie
          the non 200K version, using ExLlama?


          It''s expected that AutoGPTQ won''t be able to load it.  I didn''t realise
          that Transformers wouldn''t be able to load it, but now I see the error
          message, then I guess that''s expected too, though disappointing.  Transformers
          can make GPTQs that AutoGPTQ can''t make, so I thought it could load them
          too. I guess not.


          But I''ve been told that ExLlama added specific support for Yi, so I''m
          surprised that ExLlama can''t load it.  I actually made this GPTQ differently
          to how I made the non-200K version; I used AutoGPTQ to make it, using an
          AutoGPTQ PR which adds Yi support to make it.  The non-200K was made with
          Transformers, and I was told that was working with ExLlama after turboderp
          added support.


          Let me know if the non-200K works and if necessary I''ll re-make these 200K
          GPTQs with Transformers, like I made the non-200K ones.  And/or get in touch
          with turboderp about it.'
        updatedAt: '2023-11-11T15:57:29.255Z'
      numEdits: 0
      reactions: []
    id: 654fa4695da3196a7864ad0f
    type: comment
  author: TheBloke
  content: '@dareposte are you able to check if you can load my Yi-34B-GPTQ, ie the
    non 200K version, using ExLlama?


    It''s expected that AutoGPTQ won''t be able to load it.  I didn''t realise that
    Transformers wouldn''t be able to load it, but now I see the error message, then
    I guess that''s expected too, though disappointing.  Transformers can make GPTQs
    that AutoGPTQ can''t make, so I thought it could load them too. I guess not.


    But I''ve been told that ExLlama added specific support for Yi, so I''m surprised
    that ExLlama can''t load it.  I actually made this GPTQ differently to how I made
    the non-200K version; I used AutoGPTQ to make it, using an AutoGPTQ PR which adds
    Yi support to make it.  The non-200K was made with Transformers, and I was told
    that was working with ExLlama after turboderp added support.


    Let me know if the non-200K works and if necessary I''ll re-make these 200K GPTQs
    with Transformers, like I made the non-200K ones.  And/or get in touch with turboderp
    about it.'
  created_at: 2023-11-11 15:57:29+00:00
  edited: false
  hidden: false
  id: 654fa4695da3196a7864ad0f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
      fullname: Derek Yates
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dareposte
      type: user
    createdAt: '2023-11-11T15:57:41.000Z'
    data:
      edited: true
      editors:
      - dareposte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9718418717384338
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
          fullname: Derek Yates
          isHf: false
          isPro: true
          name: dareposte
          type: user
        html: '<p>Yep, still no luck on my end for this.   I cloned the ExLlamaV2
          repo directly in a fresh venv, and also did not work there to load the model.   (Edit
          - this actually does work, it was an issue with the venv - confirmed below).</p>

          <p>AutoAWQ was able to load the AWQ quant model, but outputs garbage.   So
          dead-end there as well. (Note:  This does works fine in AutoAWQ directly,
          but outputs garbage in the AutoAWQ loader in ooba.   Problem is in Ooba,
          not the model.)</p>

          '
        raw: 'Yep, still no luck on my end for this.   I cloned the ExLlamaV2 repo
          directly in a fresh venv, and also did not work there to load the model.   (Edit
          - this actually does work, it was an issue with the venv - confirmed below).


          AutoAWQ was able to load the AWQ quant model, but outputs garbage.   So
          dead-end there as well. (Note:  This does works fine in AutoAWQ directly,
          but outputs garbage in the AutoAWQ loader in ooba.   Problem is in Ooba,
          not the model.)'
        updatedAt: '2023-11-11T16:38:44.900Z'
      numEdits: 1
      reactions: []
    id: 654fa4750f44a05d81a64c9d
    type: comment
  author: dareposte
  content: 'Yep, still no luck on my end for this.   I cloned the ExLlamaV2 repo directly
    in a fresh venv, and also did not work there to load the model.   (Edit - this
    actually does work, it was an issue with the venv - confirmed below).


    AutoAWQ was able to load the AWQ quant model, but outputs garbage.   So dead-end
    there as well. (Note:  This does works fine in AutoAWQ directly, but outputs garbage
    in the AutoAWQ loader in ooba.   Problem is in Ooba, not the model.)'
  created_at: 2023-11-11 15:57:41+00:00
  edited: true
  hidden: false
  id: 654fa4750f44a05d81a64c9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
      fullname: Derek Yates
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dareposte
      type: user
    createdAt: '2023-11-11T16:04:45.000Z'
    data:
      edited: false
      editors:
      - dareposte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9746543169021606
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
          fullname: Derek Yates
          isHf: false
          isPro: true
          name: dareposte
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;dareposte&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/dareposte\"\
          >@<span class=\"underline\">dareposte</span></a></span>\n\n\t</span></span>\
          \ are you able to check if you can load my Yi-34B-GPTQ, ie the non 200K\
          \ version, using ExLlama?</p>\n<p>It's expected that AutoGPTQ won't be able\
          \ to load it.  I didn't realise that Transformers wouldn't be able to load\
          \ it, but now I see the error message, then I guess that's expected too,\
          \ though disappointing.  Transformers can make GPTQs that AutoGPTQ can't\
          \ make, so I thought it could load them too. I guess not.</p>\n<p>But I've\
          \ been told that ExLlama added specific support for Yi, so I'm surprised\
          \ that ExLlama can't load it.  I actually made this GPTQ differently to\
          \ how I made the non-200K version; I used AutoGPTQ to make it, using an\
          \ AutoGPTQ PR which adds Yi support to make it.  The non-200K was made with\
          \ Transformers, and I was told that was working with ExLlama after turboderp\
          \ added support.</p>\n<p>Let me know if the non-200K works and if necessary\
          \ I'll re-make these 200K GPTQs with Transformers, like I made the non-200K\
          \ ones.  And/or get in touch with turboderp about it.</p>\n</blockquote>\n\
          <p>I'm pulling it down now, will report back shortly.   I've been successfully\
          \ using the 4.65bpw exl2 quant uploaded by LoneStriker in v2 successfully,\
          \ but I'm finding Exllamav2 is just extremely buggy right now.  </p>\n<p>Did\
          \ you successfully load and test this quant on any software?  Or just script\
          \ it in?   I'm not tied to any particular implementation but am looking\
          \ for something less buggy than exlv2 currently is.   It runs great for\
          \ about 10-15 minutes then slows to a crawl even with an empty context for\
          \ me.   Don't usually use Ooba much, but it's a bit quicker to iterate prompts\
          \ and model types in there when it's working.</p>\n"
        raw: "> @dareposte are you able to check if you can load my Yi-34B-GPTQ, ie\
          \ the non 200K version, using ExLlama?\n> \n> It's expected that AutoGPTQ\
          \ won't be able to load it.  I didn't realise that Transformers wouldn't\
          \ be able to load it, but now I see the error message, then I guess that's\
          \ expected too, though disappointing.  Transformers can make GPTQs that\
          \ AutoGPTQ can't make, so I thought it could load them too. I guess not.\n\
          > \n> But I've been told that ExLlama added specific support for Yi, so\
          \ I'm surprised that ExLlama can't load it.  I actually made this GPTQ differently\
          \ to how I made the non-200K version; I used AutoGPTQ to make it, using\
          \ an AutoGPTQ PR which adds Yi support to make it.  The non-200K was made\
          \ with Transformers, and I was told that was working with ExLlama after\
          \ turboderp added support.\n> \n> Let me know if the non-200K works and\
          \ if necessary I'll re-make these 200K GPTQs with Transformers, like I made\
          \ the non-200K ones.  And/or get in touch with turboderp about it.\n\nI'm\
          \ pulling it down now, will report back shortly.   I've been successfully\
          \ using the 4.65bpw exl2 quant uploaded by LoneStriker in v2 successfully,\
          \ but I'm finding Exllamav2 is just extremely buggy right now.  \n\nDid\
          \ you successfully load and test this quant on any software?  Or just script\
          \ it in?   I'm not tied to any particular implementation but am looking\
          \ for something less buggy than exlv2 currently is.   It runs great for\
          \ about 10-15 minutes then slows to a crawl even with an empty context for\
          \ me.   Don't usually use Ooba much, but it's a bit quicker to iterate prompts\
          \ and model types in there when it's working.\n"
        updatedAt: '2023-11-11T16:04:45.125Z'
      numEdits: 0
      reactions: []
    id: 654fa61d0aa8eba4c22381e0
    type: comment
  author: dareposte
  content: "> @dareposte are you able to check if you can load my Yi-34B-GPTQ, ie\
    \ the non 200K version, using ExLlama?\n> \n> It's expected that AutoGPTQ won't\
    \ be able to load it.  I didn't realise that Transformers wouldn't be able to\
    \ load it, but now I see the error message, then I guess that's expected too,\
    \ though disappointing.  Transformers can make GPTQs that AutoGPTQ can't make,\
    \ so I thought it could load them too. I guess not.\n> \n> But I've been told\
    \ that ExLlama added specific support for Yi, so I'm surprised that ExLlama can't\
    \ load it.  I actually made this GPTQ differently to how I made the non-200K version;\
    \ I used AutoGPTQ to make it, using an AutoGPTQ PR which adds Yi support to make\
    \ it.  The non-200K was made with Transformers, and I was told that was working\
    \ with ExLlama after turboderp added support.\n> \n> Let me know if the non-200K\
    \ works and if necessary I'll re-make these 200K GPTQs with Transformers, like\
    \ I made the non-200K ones.  And/or get in touch with turboderp about it.\n\n\
    I'm pulling it down now, will report back shortly.   I've been successfully using\
    \ the 4.65bpw exl2 quant uploaded by LoneStriker in v2 successfully, but I'm finding\
    \ Exllamav2 is just extremely buggy right now.  \n\nDid you successfully load\
    \ and test this quant on any software?  Or just script it in?   I'm not tied to\
    \ any particular implementation but am looking for something less buggy than exlv2\
    \ currently is.   It runs great for about 10-15 minutes then slows to a crawl\
    \ even with an empty context for me.   Don't usually use Ooba much, but it's a\
    \ bit quicker to iterate prompts and model types in there when it's working.\n"
  created_at: 2023-11-11 16:04:45+00:00
  edited: false
  hidden: false
  id: 654fa61d0aa8eba4c22381e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-11T16:23:15.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8976554274559021
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>AWQ version works fine for me via Python code:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer,\
          \ TextStreamer\n\nmodel_name_or_path = <span class=\"hljs-string\">\"/workspace/process/01-ai_yi-34b-200k/awq/main/\"\
          </span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    model_name_or_path,\n    low_cpu_mem_usage=<span class=\"hljs-literal\"\
          >True</span>, trust_remote_code=<span class=\"hljs-literal\">True</span>,\n\
          \    device_map=<span class=\"hljs-string\">\"cuda:0\"</span>\n)\n\n<span\
          \ class=\"hljs-comment\"># Using the text streamer to stream output one\
          \ token at a time</span>\nstreamer = TextStreamer(tokenizer, skip_prompt=<span\
          \ class=\"hljs-literal\">True</span>, skip_special_tokens=<span class=\"\
          hljs-literal\">True</span>)\n\nprompt = <span class=\"hljs-string\">\"The\
          \ meaning of life is\"</span>\nprompt_template=<span class=\"hljs-string\"\
          >f'''<span class=\"hljs-subst\">{prompt}</span>'''</span>\n\n<span class=\"\
          hljs-comment\"># Convert prompt to tokens</span>\ntokens = tokenizer(\n\
          \    prompt_template,\n    return_tensors=<span class=\"hljs-string\">'pt'</span>\n\
          ).input_ids.cuda()\n\ngeneration_params = {\n    <span class=\"hljs-string\"\
          >\"do_sample\"</span>: <span class=\"hljs-literal\">True</span>,\n    <span\
          \ class=\"hljs-string\">\"temperature\"</span>: <span class=\"hljs-number\"\
          >0.7</span>,\n    <span class=\"hljs-string\">\"top_p\"</span>: <span class=\"\
          hljs-number\">0.95</span>,\n    <span class=\"hljs-string\">\"top_k\"</span>:\
          \ <span class=\"hljs-number\">40</span>,\n    <span class=\"hljs-string\"\
          >\"max_new_tokens\"</span>: <span class=\"hljs-number\">512</span>,\n  \
          \  <span class=\"hljs-string\">\"repetition_penalty\"</span>: <span class=\"\
          hljs-number\">1.1</span>\n}\n\n<span class=\"hljs-comment\"># Generate streamed\
          \ output, visible one token at a time</span>\ngeneration_output = model.generate(\n\
          \    tokens,\n    streamer=streamer,\n    **generation_params\n)\n</code></pre>\n\
          <p>Output:</p>\n<pre><code> to find your gift. The purpose of life is to\
          \ give it away.\u201D ~Pablo Picasso\nI\u2019m really starting to understand\
          \ what this quote means. For the past few months, I have been working with\
          \ a young man who has a form of Autism and suffers from many learning disabilities.\
          \ He has been my student since last year, but he really hasn\u2019t opened\
          \ up until recently. I was frustrated at first because I couldn\u2019t get\
          \ him to talk about his feelings or anything else. Now that he trusts me\
          \ enough, he tells me when he doesn\u2019t like something and why. It helps\
          \ us communicate so much better. We\u2019ve also bonded on another level:\
          \ one where we can joke around and be silly together. I think that\u2019\
          s the best part of all! \U0001F642\nNow you may ask how this relates to\
          \ Picasso\u2019s quote? Well, I feel like I found my calling in life as\
          \ a teacher. I know that teaching isn\u2019t for everyone, but I absolutely\
          \ love it! I love being able to help students learn new things and challenge\
          \ them everyday. I love the fact that each day is different and there are\
          \ no two days alike. I love making a difference in the lives of others.\
          \ I could go on forever!\nMy passion in life is to make sure that my kids\
          \ succeed and reach their full potential. I want them to realize that they\
          \ can do anything they put their minds to. I want them to believe in themselves\
          \ and never let anyone tell them otherwise. Most importantly, I want them\
          \ to find happiness within themselves.\nThis is why I chose to become an\
          \ educator: because I wanted to change lives and inspire greatness. My goal\
          \ every single day is to teach with passion and compassion; two qualities\
          \ which are not always easy to come by these days\u2026but definitely worth\
          \ fighting for!\nIn conclusion, here are some tips for finding your true\
          \ passion in life:\nWhat is Your Life Purpose\n\u201CI am convinced that\
          \ life is 10% what happens to me and 90% how I react to it.\u201D \u2013\
          \ Charles R. Swindoll\nHave you ever felt lost or confused about what you\
          \ should do next in life? Do you feel like you don\u2019t know what your\
          \ purpose is? If so, then you are not alone. Many people struggle with these\
          \ same questions. However, if we look closely at our own experiences, we\
          \ will see that there are certain things that happen over and over again.\
          \ These patterns are called \u201Crecurring\n</code></pre>\n<p>GPTQ version\
          \ also works fine for me from Transformers, using the released version of\
          \ AutoGPTQ 0.5.1, which does not have the PR.  So I was right the first\
          \ time, Transformers can load GPTQs without AutoGPTQ having specific support.\
          \  I don't know why it's not working for you, but it works fine for me:</p>\n\
          <p>Test code:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> argparse\nparser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Process and upload quantisations'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'model_dir'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'model dir'</span>)\nargs = parser.parse_args()\n\
          \n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path\
          \ = args.model_dir\n<span class=\"hljs-comment\"># To use a different branch,\
          \ change revision</span>\n<span class=\"hljs-comment\"># For example: revision=\"\
          gptq-4bit-32g-actorder_True\"</span>\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=<span class=\"\
          hljs-string\">\"auto\"</span>,\n                                       \
          \      trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=<span class=\"\
          hljs-literal\">True</span>, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>)\n\nprompt = <span class=\"hljs-string\">\"Tell me about AI\"\
          </span>\nprompt_template=<span class=\"hljs-string\">f'''<span class=\"\
          hljs-subst\">{prompt}</span>'''</span>\n\n<span class=\"hljs-built_in\"\
          >print</span>(<span class=\"hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\
          \ninput_ids = tokenizer(prompt_template, return_tensors=<span class=\"hljs-string\"\
          >'pt'</span>).input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
          \ temperature=<span class=\"hljs-number\">0.7</span>, do_sample=<span class=\"\
          hljs-literal\">True</span>, top_p=<span class=\"hljs-number\">0.95</span>,\
          \ top_k=<span class=\"hljs-number\">40</span>, max_new_tokens=<span class=\"\
          hljs-number\">512</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>]))\n\n<span class=\"hljs-comment\"># Inference\
          \ can also be done using transformers' pipeline</span>\n\n<span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">\"*** Pipeline:\"\
          </span>)\npipe = pipeline(\n    <span class=\"hljs-string\">\"text-generation\"\
          </span>,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=<span\
          \ class=\"hljs-number\">512</span>,\n    do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n    temperature=<span class=\"hljs-number\">0.7</span>,\n\
          \    top_p=<span class=\"hljs-number\">0.95</span>,\n    top_k=<span class=\"\
          hljs-number\">40</span>,\n    repetition_penalty=<span class=\"hljs-number\"\
          >1.1</span>\n)\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n<p>Output:</p>\n<pre><code>*** Generate:\nTell me about AI\
          \ and Machine Learning in the context of security?\nMachine learning is\
          \ a subset of Artificial Intelligence. It is an algorithm that learns from\
          \ the data provided to it and makes decisions. For example, if I give you\
          \ a data set of 1000 images of cats and 1000 images of dogs, and tell you\
          \ to classify them as cats or dogs, you would look at the images, understand\
          \ what a cat looks like and what a dog looks like and then classify them.\
          \ If I give you 10,000 images of cats and dogs, it would be difficult for\
          \ you to classify them. But, a machine learning algorithm would be able\
          \ to look at the images and tell you which one is a cat and which one is\
          \ a dog. That\u2019s the power of machine learning.\nIn the context of security,\
          \ machine learning can be used for anomaly detection, intrusion detection,\
          \ and malware detection. Machine learning algorithms are trained on a large\
          \ amount of data and then used to detect anomalies or intrusions in the\
          \ system.\nCan you elaborate on anomaly detection?\nAnomaly detection is\
          \ the process of identifying outliers or anomalies in data. In the context\
          \ of security, anomaly detection can be used to identify unusual or unexpected\
          \ activity on a network or system. For example, if there is a sudden spike\
          \ in network traffic or a large number of failed login attempts, this could\
          \ be an indication of an attack. Machine learning algorithms can be used\
          \ to analyze network traffic and identify patterns of anomalous behavior.\n\
          What are the other use cases of machine learning in the security context?\n\
          Machine learning can also be used for intrusion detection and malware detection.\
          \ In intrusion detection, machine learning algorithms can be used to analyze\
          \ network traffic and identify patterns of malicious activity. For example,\
          \ if there is a sudden spike in network traffic or a large number of failed\
          \ login attempts, this could be an indication of an attack. Machine learning\
          \ algorithms can be used to analyze network traffic and identify patterns\
          \ of anomalous behavior.\nIn malware detection, machine learning algorithms\
          \ can be used to identify unknown or previously unseen malware. Machine\
          \ learning algorithms can be trained on a large number of known malware\
          \ samples and then used to identify new or unknown malware samples.\nWhat\
          \ are the different types of machine learning models?\nThere are three main\
          \ types of machine learning models: supervised learning, unsupervised learning,\
          \ and reinforcement learning.\nSupervised learning is a type of machine\
          \ learning in which the algorithm is trained on a set of labeled data. In\
          \ supervised learning, the algorithm is given\n</code></pre>\n<p>So as far\
          \ as I can tell, this GPTQ and AWQ are completely fine via Transformers,\
          \ confirming they're structureally OK.   I've not tested ExLlama yet; I'll\
          \ try to, later.</p>\n<p>If you continue to have problems, wait a few hours\
          \ for me to complete the \"Llamafied\" version of Yi 34B 200K, which should\
          \ work automatically everywhere.</p>\n"
        raw: "AWQ version works fine for me via Python code:\n```python\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\nmodel_name_or_path\
          \ = \"/workspace/process/01-ai_yi-34b-200k/awq/main/\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n\
          \    model_name_or_path,\n    low_cpu_mem_usage=True, trust_remote_code=True,\n\
          \    device_map=\"cuda:0\"\n)\n\n# Using the text streamer to stream output\
          \ one token at a time\nstreamer = TextStreamer(tokenizer, skip_prompt=True,\
          \ skip_special_tokens=True)\n\nprompt = \"The meaning of life is\"\nprompt_template=f'''{prompt}'''\n\
          \n# Convert prompt to tokens\ntokens = tokenizer(\n    prompt_template,\n\
          \    return_tensors='pt'\n).input_ids.cuda()\n\ngeneration_params = {\n\
          \    \"do_sample\": True,\n    \"temperature\": 0.7,\n    \"top_p\": 0.95,\n\
          \    \"top_k\": 40,\n    \"max_new_tokens\": 512,\n    \"repetition_penalty\"\
          : 1.1\n}\n\n# Generate streamed output, visible one token at a time\ngeneration_output\
          \ = model.generate(\n    tokens,\n    streamer=streamer,\n    **generation_params\n\
          )\n```\n\nOutput:\n```\n to find your gift. The purpose of life is to give\
          \ it away.\u201D ~Pablo Picasso\nI\u2019m really starting to understand\
          \ what this quote means. For the past few months, I have been working with\
          \ a young man who has a form of Autism and suffers from many learning disabilities.\
          \ He has been my student since last year, but he really hasn\u2019t opened\
          \ up until recently. I was frustrated at first because I couldn\u2019t get\
          \ him to talk about his feelings or anything else. Now that he trusts me\
          \ enough, he tells me when he doesn\u2019t like something and why. It helps\
          \ us communicate so much better. We\u2019ve also bonded on another level:\
          \ one where we can joke around and be silly together. I think that\u2019\
          s the best part of all! \U0001F642\nNow you may ask how this relates to\
          \ Picasso\u2019s quote? Well, I feel like I found my calling in life as\
          \ a teacher. I know that teaching isn\u2019t for everyone, but I absolutely\
          \ love it! I love being able to help students learn new things and challenge\
          \ them everyday. I love the fact that each day is different and there are\
          \ no two days alike. I love making a difference in the lives of others.\
          \ I could go on forever!\nMy passion in life is to make sure that my kids\
          \ succeed and reach their full potential. I want them to realize that they\
          \ can do anything they put their minds to. I want them to believe in themselves\
          \ and never let anyone tell them otherwise. Most importantly, I want them\
          \ to find happiness within themselves.\nThis is why I chose to become an\
          \ educator: because I wanted to change lives and inspire greatness. My goal\
          \ every single day is to teach with passion and compassion; two qualities\
          \ which are not always easy to come by these days\u2026but definitely worth\
          \ fighting for!\nIn conclusion, here are some tips for finding your true\
          \ passion in life:\nWhat is Your Life Purpose\n\u201CI am convinced that\
          \ life is 10% what happens to me and 90% how I react to it.\u201D \u2013\
          \ Charles R. Swindoll\nHave you ever felt lost or confused about what you\
          \ should do next in life? Do you feel like you don\u2019t know what your\
          \ purpose is? If so, then you are not alone. Many people struggle with these\
          \ same questions. However, if we look closely at our own experiences, we\
          \ will see that there are certain things that happen over and over again.\
          \ These patterns are called \u201Crecurring\n```\n\nGPTQ version also works\
          \ fine for me from Transformers, using the released version of AutoGPTQ\
          \ 0.5.1, which does not have the PR.  So I was right the first time, Transformers\
          \ can load GPTQs without AutoGPTQ having specific support.  I don't know\
          \ why it's not working for you, but it works fine for me:\n\nTest code:\n\
          ```python\nimport argparse\nparser = argparse.ArgumentParser(description='Process\
          \ and upload quantisations')\nparser.add_argument('model_dir', type=str,\
          \ help='model dir')\nargs = parser.parse_args()\n\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path =\
          \ args.model_dir\n# To use a different branch, change revision\n# For example:\
          \ revision=\"gptq-4bit-32g-actorder_True\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=True)\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n\
          \nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}'''\n\nprint(\"\
          \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          \n# Inference can also be done using transformers' pipeline\n\nprint(\"\
          *** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\nOutput:\n\n\
          ```\n*** Generate:\nTell me about AI and Machine Learning in the context\
          \ of security?\nMachine learning is a subset of Artificial Intelligence.\
          \ It is an algorithm that learns from the data provided to it and makes\
          \ decisions. For example, if I give you a data set of 1000 images of cats\
          \ and 1000 images of dogs, and tell you to classify them as cats or dogs,\
          \ you would look at the images, understand what a cat looks like and what\
          \ a dog looks like and then classify them. If I give you 10,000 images of\
          \ cats and dogs, it would be difficult for you to classify them. But, a\
          \ machine learning algorithm would be able to look at the images and tell\
          \ you which one is a cat and which one is a dog. That\u2019s the power of\
          \ machine learning.\nIn the context of security, machine learning can be\
          \ used for anomaly detection, intrusion detection, and malware detection.\
          \ Machine learning algorithms are trained on a large amount of data and\
          \ then used to detect anomalies or intrusions in the system.\nCan you elaborate\
          \ on anomaly detection?\nAnomaly detection is the process of identifying\
          \ outliers or anomalies in data. In the context of security, anomaly detection\
          \ can be used to identify unusual or unexpected activity on a network or\
          \ system. For example, if there is a sudden spike in network traffic or\
          \ a large number of failed login attempts, this could be an indication of\
          \ an attack. Machine learning algorithms can be used to analyze network\
          \ traffic and identify patterns of anomalous behavior.\nWhat are the other\
          \ use cases of machine learning in the security context?\nMachine learning\
          \ can also be used for intrusion detection and malware detection. In intrusion\
          \ detection, machine learning algorithms can be used to analyze network\
          \ traffic and identify patterns of malicious activity. For example, if there\
          \ is a sudden spike in network traffic or a large number of failed login\
          \ attempts, this could be an indication of an attack. Machine learning algorithms\
          \ can be used to analyze network traffic and identify patterns of anomalous\
          \ behavior.\nIn malware detection, machine learning algorithms can be used\
          \ to identify unknown or previously unseen malware. Machine learning algorithms\
          \ can be trained on a large number of known malware samples and then used\
          \ to identify new or unknown malware samples.\nWhat are the different types\
          \ of machine learning models?\nThere are three main types of machine learning\
          \ models: supervised learning, unsupervised learning, and reinforcement\
          \ learning.\nSupervised learning is a type of machine learning in which\
          \ the algorithm is trained on a set of labeled data. In supervised learning,\
          \ the algorithm is given\n```\n\nSo as far as I can tell, this GPTQ and\
          \ AWQ are completely fine via Transformers, confirming they're structureally\
          \ OK.   I've not tested ExLlama yet; I'll try to, later.\n\nIf you continue\
          \ to have problems, wait a few hours for me to complete the \"Llamafied\"\
          \ version of Yi 34B 200K, which should work automatically everywhere."
        updatedAt: '2023-11-11T16:23:15.744Z'
      numEdits: 0
      reactions: []
    id: 654faa73c0e106160e51f777
    type: comment
  author: TheBloke
  content: "AWQ version works fine for me via Python code:\n```python\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n\nmodel_name_or_path\
    \ = \"/workspace/process/01-ai_yi-34b-200k/awq/main/\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n   \
    \ model_name_or_path,\n    low_cpu_mem_usage=True, trust_remote_code=True,\n \
    \   device_map=\"cuda:0\"\n)\n\n# Using the text streamer to stream output one\
    \ token at a time\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\
    \nprompt = \"The meaning of life is\"\nprompt_template=f'''{prompt}'''\n\n# Convert\
    \ prompt to tokens\ntokens = tokenizer(\n    prompt_template,\n    return_tensors='pt'\n\
    ).input_ids.cuda()\n\ngeneration_params = {\n    \"do_sample\": True,\n    \"\
    temperature\": 0.7,\n    \"top_p\": 0.95,\n    \"top_k\": 40,\n    \"max_new_tokens\"\
    : 512,\n    \"repetition_penalty\": 1.1\n}\n\n# Generate streamed output, visible\
    \ one token at a time\ngeneration_output = model.generate(\n    tokens,\n    streamer=streamer,\n\
    \    **generation_params\n)\n```\n\nOutput:\n```\n to find your gift. The purpose\
    \ of life is to give it away.\u201D ~Pablo Picasso\nI\u2019m really starting to\
    \ understand what this quote means. For the past few months, I have been working\
    \ with a young man who has a form of Autism and suffers from many learning disabilities.\
    \ He has been my student since last year, but he really hasn\u2019t opened up\
    \ until recently. I was frustrated at first because I couldn\u2019t get him to\
    \ talk about his feelings or anything else. Now that he trusts me enough, he tells\
    \ me when he doesn\u2019t like something and why. It helps us communicate so much\
    \ better. We\u2019ve also bonded on another level: one where we can joke around\
    \ and be silly together. I think that\u2019s the best part of all! \U0001F642\n\
    Now you may ask how this relates to Picasso\u2019s quote? Well, I feel like I\
    \ found my calling in life as a teacher. I know that teaching isn\u2019t for everyone,\
    \ but I absolutely love it! I love being able to help students learn new things\
    \ and challenge them everyday. I love the fact that each day is different and\
    \ there are no two days alike. I love making a difference in the lives of others.\
    \ I could go on forever!\nMy passion in life is to make sure that my kids succeed\
    \ and reach their full potential. I want them to realize that they can do anything\
    \ they put their minds to. I want them to believe in themselves and never let\
    \ anyone tell them otherwise. Most importantly, I want them to find happiness\
    \ within themselves.\nThis is why I chose to become an educator: because I wanted\
    \ to change lives and inspire greatness. My goal every single day is to teach\
    \ with passion and compassion; two qualities which are not always easy to come\
    \ by these days\u2026but definitely worth fighting for!\nIn conclusion, here are\
    \ some tips for finding your true passion in life:\nWhat is Your Life Purpose\n\
    \u201CI am convinced that life is 10% what happens to me and 90% how I react to\
    \ it.\u201D \u2013 Charles R. Swindoll\nHave you ever felt lost or confused about\
    \ what you should do next in life? Do you feel like you don\u2019t know what your\
    \ purpose is? If so, then you are not alone. Many people struggle with these same\
    \ questions. However, if we look closely at our own experiences, we will see that\
    \ there are certain things that happen over and over again. These patterns are\
    \ called \u201Crecurring\n```\n\nGPTQ version also works fine for me from Transformers,\
    \ using the released version of AutoGPTQ 0.5.1, which does not have the PR.  So\
    \ I was right the first time, Transformers can load GPTQs without AutoGPTQ having\
    \ specific support.  I don't know why it's not working for you, but it works fine\
    \ for me:\n\nTest code:\n```python\nimport argparse\nparser = argparse.ArgumentParser(description='Process\
    \ and upload quantisations')\nparser.add_argument('model_dir', type=str, help='model\
    \ dir')\nargs = parser.parse_args()\n\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer, pipeline\n\nmodel_name_or_path = args.model_dir\n# To use a different\
    \ branch, change revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n         \
    \                                    device_map=\"auto\",\n                  \
    \                           trust_remote_code=True)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True, trust_remote_code=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}'''\n\
    \nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
    output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95,\
    \ top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\n# Inference\
    \ can also be done using transformers' pipeline\n\nprint(\"*** Pipeline:\")\n\
    pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=512,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n\
    \    top_k=40,\n    repetition_penalty=1.1\n)\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\
    ```\nOutput:\n\n```\n*** Generate:\nTell me about AI and Machine Learning in the\
    \ context of security?\nMachine learning is a subset of Artificial Intelligence.\
    \ It is an algorithm that learns from the data provided to it and makes decisions.\
    \ For example, if I give you a data set of 1000 images of cats and 1000 images\
    \ of dogs, and tell you to classify them as cats or dogs, you would look at the\
    \ images, understand what a cat looks like and what a dog looks like and then\
    \ classify them. If I give you 10,000 images of cats and dogs, it would be difficult\
    \ for you to classify them. But, a machine learning algorithm would be able to\
    \ look at the images and tell you which one is a cat and which one is a dog. That\u2019\
    s the power of machine learning.\nIn the context of security, machine learning\
    \ can be used for anomaly detection, intrusion detection, and malware detection.\
    \ Machine learning algorithms are trained on a large amount of data and then used\
    \ to detect anomalies or intrusions in the system.\nCan you elaborate on anomaly\
    \ detection?\nAnomaly detection is the process of identifying outliers or anomalies\
    \ in data. In the context of security, anomaly detection can be used to identify\
    \ unusual or unexpected activity on a network or system. For example, if there\
    \ is a sudden spike in network traffic or a large number of failed login attempts,\
    \ this could be an indication of an attack. Machine learning algorithms can be\
    \ used to analyze network traffic and identify patterns of anomalous behavior.\n\
    What are the other use cases of machine learning in the security context?\nMachine\
    \ learning can also be used for intrusion detection and malware detection. In\
    \ intrusion detection, machine learning algorithms can be used to analyze network\
    \ traffic and identify patterns of malicious activity. For example, if there is\
    \ a sudden spike in network traffic or a large number of failed login attempts,\
    \ this could be an indication of an attack. Machine learning algorithms can be\
    \ used to analyze network traffic and identify patterns of anomalous behavior.\n\
    In malware detection, machine learning algorithms can be used to identify unknown\
    \ or previously unseen malware. Machine learning algorithms can be trained on\
    \ a large number of known malware samples and then used to identify new or unknown\
    \ malware samples.\nWhat are the different types of machine learning models?\n\
    There are three main types of machine learning models: supervised learning, unsupervised\
    \ learning, and reinforcement learning.\nSupervised learning is a type of machine\
    \ learning in which the algorithm is trained on a set of labeled data. In supervised\
    \ learning, the algorithm is given\n```\n\nSo as far as I can tell, this GPTQ\
    \ and AWQ are completely fine via Transformers, confirming they're structureally\
    \ OK.   I've not tested ExLlama yet; I'll try to, later.\n\nIf you continue to\
    \ have problems, wait a few hours for me to complete the \"Llamafied\" version\
    \ of Yi 34B 200K, which should work automatically everywhere."
  created_at: 2023-11-11 16:23:15+00:00
  edited: false
  hidden: false
  id: 654faa73c0e106160e51f777
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
      fullname: Derek Yates
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dareposte
      type: user
    createdAt: '2023-11-11T16:27:00.000Z'
    data:
      edited: false
      editors:
      - dareposte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9405255913734436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
          fullname: Derek Yates
          isHf: false
          isPro: true
          name: dareposte
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> - False alarm,\
          \ they're both working for me now in base exllamav2 with a fresh venv. \
          \  Thanks again for the conversion, it must have been something in my venv\
          \ being mis-matched from all the trials.</p>\n<p>For others -- Still not\
          \ able to get them working in Ooba due to unknown reasons, but loading in\
          \ latest exllamav2 does in fact work with this quant.   Below is the proof\
          \ on a pair of A6000's where it loaded full context.</p>\n<p>(venv) xxx@cn1:~/AI/api_llm/exllamav2$\
          \ python3 test_inference.py -m /home/xxx/AI/models/Yi-34B-200K-GPTQ -p \"\
          Once upon a time \" -gs auto<br> -- Model: /home/xxx/AI/models/Yi-34B-200K-GPTQ<br>\
          \ -- Options: ['gpu_split: auto', 'rope_scale 1.0', 'rope_alpha 1.0']<br>\
          \ -- Loading tokenizer...<br> -- Loading model...<br> -- Warmup...<br> --\
          \ Generating...</p>\n<p>Once upon a time there was a king. He had 130 sons\
          \ and one daughter. The princess loved her father very much, but hated her\
          \ brothers for they were cruel to their poor sister. They used to make fun\
          \ of the girl and say: \u201CYou are so ugly that you look like an old witch!\u201D\
          <br>One day the princess asked her father to let her leave his palace because\
          \ she wanted to marry. And he consented. Soon after this the young lady\
          \ came back home again. She told everybody how badly her husband treated\
          \ her in fact he didn\u2019t love her at all. The king got angry with the\
          \ son-in-</p>\n<p> -- Response generated in 4.12 seconds, 128 tokens, 31.06\
          \ tokens/second (includes prompt eval.)</p>\n"
        raw: "@TheBloke - False alarm, they're both working for me now in base exllamav2\
          \ with a fresh venv.   Thanks again for the conversion, it must have been\
          \ something in my venv being mis-matched from all the trials.\n\nFor others\
          \ -- Still not able to get them working in Ooba due to unknown reasons,\
          \ but loading in latest exllamav2 does in fact work with this quant.   Below\
          \ is the proof on a pair of A6000's where it loaded full context.\n\n(venv)\
          \ xxx@cn1:~/AI/api_llm/exllamav2$ python3 test_inference.py -m /home/xxx/AI/models/Yi-34B-200K-GPTQ\
          \ -p \"Once upon a time \" -gs auto\n -- Model: /home/xxx/AI/models/Yi-34B-200K-GPTQ\n\
          \ -- Options: ['gpu_split: auto', 'rope_scale 1.0', 'rope_alpha 1.0']\n\
          \ -- Loading tokenizer...\n -- Loading model...\n -- Warmup...\n -- Generating...\n\
          \nOnce upon a time there was a king. He had 130 sons and one daughter. The\
          \ princess loved her father very much, but hated her brothers for they were\
          \ cruel to their poor sister. They used to make fun of the girl and say:\
          \ \u201CYou are so ugly that you look like an old witch!\u201D\nOne day\
          \ the princess asked her father to let her leave his palace because she\
          \ wanted to marry. And he consented. Soon after this the young lady came\
          \ back home again. She told everybody how badly her husband treated her\
          \ in fact he didn\u2019t love her at all. The king got angry with the son-in-\n\
          \n -- Response generated in 4.12 seconds, 128 tokens, 31.06 tokens/second\
          \ (includes prompt eval.)"
        updatedAt: '2023-11-11T16:27:00.323Z'
      numEdits: 0
      reactions: []
    id: 654fab5445c0dccd5704b761
    type: comment
  author: dareposte
  content: "@TheBloke - False alarm, they're both working for me now in base exllamav2\
    \ with a fresh venv.   Thanks again for the conversion, it must have been something\
    \ in my venv being mis-matched from all the trials.\n\nFor others -- Still not\
    \ able to get them working in Ooba due to unknown reasons, but loading in latest\
    \ exllamav2 does in fact work with this quant.   Below is the proof on a pair\
    \ of A6000's where it loaded full context.\n\n(venv) xxx@cn1:~/AI/api_llm/exllamav2$\
    \ python3 test_inference.py -m /home/xxx/AI/models/Yi-34B-200K-GPTQ -p \"Once\
    \ upon a time \" -gs auto\n -- Model: /home/xxx/AI/models/Yi-34B-200K-GPTQ\n --\
    \ Options: ['gpu_split: auto', 'rope_scale 1.0', 'rope_alpha 1.0']\n -- Loading\
    \ tokenizer...\n -- Loading model...\n -- Warmup...\n -- Generating...\n\nOnce\
    \ upon a time there was a king. He had 130 sons and one daughter. The princess\
    \ loved her father very much, but hated her brothers for they were cruel to their\
    \ poor sister. They used to make fun of the girl and say: \u201CYou are so ugly\
    \ that you look like an old witch!\u201D\nOne day the princess asked her father\
    \ to let her leave his palace because she wanted to marry. And he consented. Soon\
    \ after this the young lady came back home again. She told everybody how badly\
    \ her husband treated her in fact he didn\u2019t love her at all. The king got\
    \ angry with the son-in-\n\n -- Response generated in 4.12 seconds, 128 tokens,\
    \ 31.06 tokens/second (includes prompt eval.)"
  created_at: 2023-11-11 16:27:00+00:00
  edited: false
  hidden: false
  id: 654fab5445c0dccd5704b761
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
      fullname: Derek Yates
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dareposte
      type: user
    createdAt: '2023-11-11T16:29:34.000Z'
    data:
      edited: false
      editors:
      - dareposte
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9566894173622131
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e3ac906458a92f58aa83b67a56750574.svg
          fullname: Derek Yates
          isHf: false
          isPro: true
          name: dareposte
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> - Thanks for\
          \ the confirmation, it looks like the problems are all related to attempting\
          \ to use ooba - now I remember why I don't use it much.</p>\n<p>I'm switching\
          \ back to python / transformers for the moment.</p>\n<p>Appreciate your\
          \ help and responses, and your ongoing contribution to the community is\
          \ much appreciated by all.</p>\n"
        raw: '@TheBloke - Thanks for the confirmation, it looks like the problems
          are all related to attempting to use ooba - now I remember why I don''t
          use it much.


          I''m switching back to python / transformers for the moment.


          Appreciate your help and responses, and your ongoing contribution to the
          community is much appreciated by all.

          '
        updatedAt: '2023-11-11T16:29:34.838Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 654fabee81c52527f4fe53d1
    type: comment
  author: dareposte
  content: '@TheBloke - Thanks for the confirmation, it looks like the problems are
    all related to attempting to use ooba - now I remember why I don''t use it much.


    I''m switching back to python / transformers for the moment.


    Appreciate your help and responses, and your ongoing contribution to the community
    is much appreciated by all.

    '
  created_at: 2023-11-11 16:29:34+00:00
  edited: false
  hidden: false
  id: 654fabee81c52527f4fe53d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-12T04:29:41.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7451802492141724
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p>I made Yi-34B model working with both latest textgen webui and latest\
          \ exllamav2 <a href=\"https://huggingface.co/01-ai/Yi-34B/discussions/22\"\
          >https://huggingface.co/01-ai/Yi-34B/discussions/22</a></p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;dareposte&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/dareposte\">@<span class=\"underline\">dareposte</span></a></span>\n\
          \n\t</span></span> </p>\n<p>You should use the instruction here for insatlling\
          \ exllamav2 <a rel=\"nofollow\" href=\"https://github.com/turboderp/exllamav2#installation\"\
          >https://github.com/turboderp/exllamav2#installation</a></p>\n<p>python\
          \ setup.py install --user</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;vdruts&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vdruts\"\
          >@<span class=\"underline\">vdruts</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>You were using exllama, but Yi requires exllamav2 I believe</p>\n"
        raw: "I made Yi-34B model working with both latest textgen webui and latest\
          \ exllamav2 https://huggingface.co/01-ai/Yi-34B/discussions/22\n\n@dareposte\
          \ \n\nYou should use the instruction here for insatlling exllamav2 https://github.com/turboderp/exllamav2#installation\n\
          \npython setup.py install --user\n\n@vdruts \n\nYou were using exllama,\
          \ but Yi requires exllamav2 I believe\n\n"
        updatedAt: '2023-11-12T04:29:41.521Z'
      numEdits: 0
      reactions: []
    id: 655054b5e5f380aca8f55187
    type: comment
  author: Yhyu13
  content: "I made Yi-34B model working with both latest textgen webui and latest\
    \ exllamav2 https://huggingface.co/01-ai/Yi-34B/discussions/22\n\n@dareposte \n\
    \nYou should use the instruction here for insatlling exllamav2 https://github.com/turboderp/exllamav2#installation\n\
    \npython setup.py install --user\n\n@vdruts \n\nYou were using exllama, but Yi\
    \ requires exllamav2 I believe\n\n"
  created_at: 2023-11-12 04:29:41+00:00
  edited: false
  hidden: false
  id: 655054b5e5f380aca8f55187
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Yi-34B-200K-GPTQ
repo_type: model
status: open
target_branch: null
title: Unable to load in ooba
