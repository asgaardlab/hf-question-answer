!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ohwi
conflicting_files: null
created_at: 2023-12-26 13:14:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/66236075bcb8e6d2b7f978706e0db6ba.svg
      fullname: HG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ohwi
      type: user
    createdAt: '2023-12-26T13:14:50.000Z'
    data:
      edited: true
      editors:
      - ohwi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9063171744346619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/66236075bcb8e6d2b7f978706e0db6ba.svg
          fullname: HG
          isHf: false
          isPro: false
          name: ohwi
          type: user
        html: "<p>Hello, and thanks for open-sourcing these great models.</p>\n<p>I\
          \ have a question regarding the hyperparameters used for instruction tuning.</p>\n\
          <p>Did you employ the hyperparameters outlined in the Qwen paper for instruction\
          \ tuning?</p>\n<blockquote>\n<p>The model\u2019s training process utilizes\
          \ the AdamW optimizer, with the following hyperparameters: \u03B21<br>set\
          \ to 0.9, \u03B22 set to 0.95, and \u03F5 set to 10\u22128. The sequence\
          \ length is limited to 2048, and the batch<br>size is 128. The model undergoes\
          \ a total of 4000 steps, with the learning rate gradually increased<br>over\
          \ the first 1430 steps, reaching a peak of 2 \xD7 10\u22126. To prevent\
          \ overfitting, weight decay is applied<br>with a value of 0.1, dropout is\
          \ set to 0.1, and gradient clipping is enforced with a limit of 1.0.</p>\n\
          </blockquote>\n<p>Thanks!</p>\n"
        raw: "Hello, and thanks for open-sourcing these great models.\n\nI have a\
          \ question regarding the hyperparameters used for instruction tuning.\n\n\
          Did you employ the hyperparameters outlined in the Qwen paper for instruction\
          \ tuning?\n\n> The model\u2019s training process utilizes the AdamW optimizer,\
          \ with the following hyperparameters: \u03B21\nset to 0.9, \u03B22 set to\
          \ 0.95, and \u03F5 set to 10\u22128. The sequence length is limited to 2048,\
          \ and the batch\nsize is 128. The model undergoes a total of 4000 steps,\
          \ with the learning rate gradually increased\nover the first 1430 steps,\
          \ reaching a peak of 2 \xD7 10\u22126. To prevent overfitting, weight decay\
          \ is applied\nwith a value of 0.1, dropout is set to 0.1, and gradient clipping\
          \ is enforced with a limit of 1.0.\n\nThanks!"
        updatedAt: '2023-12-26T13:18:03.160Z'
      numEdits: 1
      reactions: []
    id: 658ad1ca438d7b1ccf7ea4b0
    type: comment
  author: ohwi
  content: "Hello, and thanks for open-sourcing these great models.\n\nI have a question\
    \ regarding the hyperparameters used for instruction tuning.\n\nDid you employ\
    \ the hyperparameters outlined in the Qwen paper for instruction tuning?\n\n>\
    \ The model\u2019s training process utilizes the AdamW optimizer, with the following\
    \ hyperparameters: \u03B21\nset to 0.9, \u03B22 set to 0.95, and \u03F5 set to\
    \ 10\u22128. The sequence length is limited to 2048, and the batch\nsize is 128.\
    \ The model undergoes a total of 4000 steps, with the learning rate gradually\
    \ increased\nover the first 1430 steps, reaching a peak of 2 \xD7 10\u22126. To\
    \ prevent overfitting, weight decay is applied\nwith a value of 0.1, dropout is\
    \ set to 0.1, and gradient clipping is enforced with a limit of 1.0.\n\nThanks!"
  created_at: 2023-12-26 13:14:50+00:00
  edited: true
  hidden: false
  id: 658ad1ca438d7b1ccf7ea4b0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: rinna/nekomata-7b-instruction
repo_type: model
status: open
target_branch: null
title: About hyper-parameters
