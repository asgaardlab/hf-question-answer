!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NickyNicky
conflicting_files: null
created_at: 2023-12-09 04:42:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
      fullname: Nicky
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NickyNicky
      type: user
    createdAt: '2023-12-09T04:42:36.000Z'
    data:
      edited: false
      editors:
      - NickyNicky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5027062892913818
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ab1fba948e86cce23e8ab573f12ff04.svg
          fullname: Nicky
          isHf: false
          isPro: false
          name: NickyNicky
          type: user
        html: "<pre><code class=\"language-Python\">!pip install intel-extension-<span\
          \ class=\"hljs-keyword\">for</span>-transformers\n</code></pre>\n<pre><code\
          \ class=\"language-Python\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoTokenizer, TextStreamer\n\
          <span class=\"hljs-keyword\">from</span> intel_extension_for_transformers.transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM\nmodel_name\
          \ = <span class=\"hljs-string\">\"Intel/neural-chat-7b-v1-1\"</span>   \
          \  <span class=\"hljs-comment\"># Hugging Face model_id or local model</span>\n\
          \nprompt = <span class=\"hljs-string\">\"Once upon a time, there existed\
          \ a little girl,\"</span>\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,\
          \ \n                                          trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\ninputs = tokenizer(prompt, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>).input_ids\nstreamer = TextStreamer(tokenizer)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name, \n          \
          \                                   load_in_8bit=<span class=\"hljs-literal\"\
          >True</span>,\n                                             trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\noutputs = model.generate(inputs,\
          \ streamer=streamer, max_new_tokens=<span class=\"hljs-number\">300</span>)\n\
          </code></pre>\n<p>Error:</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/W7qmRzP4KT3I87AyiIG1T.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/W7qmRzP4KT3I87AyiIG1T.png\"\
          ></a></p>\n<pre><code class=\"language-Python\">!pip install git+<span class=\"\
          hljs-string\">\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/layer_norm\"\
          </span>\n</code></pre>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/3FiaGODF55PkOCMmoWp1J.png\"\
          ><img alt=\"image.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/3FiaGODF55PkOCMmoWp1J.png\"\
          ></a></p>\n"
        raw: "```Python\r\n!pip install intel-extension-for-transformers\r\n```\r\n\
          \r\n```Python\r\nfrom transformers import AutoTokenizer, TextStreamer\r\n\
          from intel_extension_for_transformers.transformers import AutoModelForCausalLM\r\
          \nmodel_name = \"Intel/neural-chat-7b-v1-1\"     # Hugging Face model_id\
          \ or local model\r\n\r\nprompt = \"Once upon a time, there existed a little\
          \ girl,\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name, \r\
          \n                                          trust_remote_code=True)\r\n\
          inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\nstreamer\
          \ = TextStreamer(tokenizer)\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\
          \ \r\n                                             load_in_8bit=True,\r\n\
          \                                             trust_remote_code=True)\r\n\
          outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\r\
          \n```\r\n\r\nError:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/W7qmRzP4KT3I87AyiIG1T.png)\r\
          \n\r\n\r\n```Python\r\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/layer_norm\"\
          \r\n```\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/3FiaGODF55PkOCMmoWp1J.png)\r\
          \n\r\n"
        updatedAt: '2023-12-09T04:42:36.508Z'
      numEdits: 0
      reactions: []
    id: 6573f03c3e0cb21bc7cdd696
    type: comment
  author: NickyNicky
  content: "```Python\r\n!pip install intel-extension-for-transformers\r\n```\r\n\r\
    \n```Python\r\nfrom transformers import AutoTokenizer, TextStreamer\r\nfrom intel_extension_for_transformers.transformers\
    \ import AutoModelForCausalLM\r\nmodel_name = \"Intel/neural-chat-7b-v1-1\"  \
    \   # Hugging Face model_id or local model\r\n\r\nprompt = \"Once upon a time,\
    \ there existed a little girl,\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_name,\
    \ \r\n                                          trust_remote_code=True)\r\ninputs\
    \ = tokenizer(prompt, return_tensors=\"pt\").input_ids\r\nstreamer = TextStreamer(tokenizer)\r\
    \n\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name, \r\n          \
    \                                   load_in_8bit=True,\r\n                   \
    \                          trust_remote_code=True)\r\noutputs = model.generate(inputs,\
    \ streamer=streamer, max_new_tokens=300)\r\n```\r\n\r\nError:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/W7qmRzP4KT3I87AyiIG1T.png)\r\
    \n\r\n\r\n```Python\r\n!pip install git+\"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/layer_norm\"\
    \r\n```\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/641b435ba5f876fe30c5ae0a/3FiaGODF55PkOCMmoWp1J.png)\r\
    \n\r\n"
  created_at: 2023-12-09 04:42:36+00:00
  edited: false
  hidden: false
  id: 6573f03c3e0cb21bc7cdd696
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2a8dece24cbc1779eb906d5582573c49.svg
      fullname: Dong, Bo
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: BoDong
      type: user
    createdAt: '2023-12-11T04:17:32.000Z'
    data:
      edited: true
      editors:
      - BoDong
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8741648197174072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2a8dece24cbc1779eb906d5582573c49.svg
          fullname: Dong, Bo
          isHf: false
          isPro: false
          name: BoDong
          type: user
        html: "<p>Thanks for your usage, but from the log you provided, we found that\
          \ the error happened in this project <a rel=\"nofollow\" href=\"https://github.com/togethercomputer/stripedhyena\"\
          >https://github.com/togethercomputer/stripedhyena</a>, not related to ours\
          \ project.</p>\n<p>I will recommend you to go through following suggestions\
          \ to fix the error for \"togethercomputer/stripedhyena\"<br>There are two\
          \ solutions for second pip install error<br>1.\tMake sure the env is installed\
          \ Microsoft Visual C++ Build Tools or MinGW-w64 for windows, GCC and cmake\
          \ for Linux . Therefore the build process for flash-attention may successfully.<br>2.\t\
          Install flash-attention through pre build binary rather than build in time</p>\n\
          <p>After solve second error and successfully install flash-attention, the\
          \ first import module error should be solved at same time.</p>\n"
        raw: "Thanks for your usage, but from the log you provided, we found that\
          \ the error happened in this project https://github.com/togethercomputer/stripedhyena,\
          \ not related to ours project.\n\nI will recommend you to go through following\
          \ suggestions to fix the error for \"togethercomputer/stripedhyena\"\nThere\
          \ are two solutions for second pip install error\n1.\tMake sure the env\
          \ is installed Microsoft Visual C++ Build Tools or MinGW-w64 for windows,\
          \ GCC and cmake for Linux . Therefore the build process for flash-attention\
          \ may successfully.\n2.\tInstall flash-attention through pre build binary\
          \ rather than build in time\n                         \nAfter solve second\
          \ error and successfully install flash-attention, the first import module\
          \ error should be solved at same time.\n"
        updatedAt: '2023-12-11T04:23:46.485Z'
      numEdits: 2
      reactions: []
    id: 65768d5c8b44ef012b8f5351
    type: comment
  author: BoDong
  content: "Thanks for your usage, but from the log you provided, we found that the\
    \ error happened in this project https://github.com/togethercomputer/stripedhyena,\
    \ not related to ours project.\n\nI will recommend you to go through following\
    \ suggestions to fix the error for \"togethercomputer/stripedhyena\"\nThere are\
    \ two solutions for second pip install error\n1.\tMake sure the env is installed\
    \ Microsoft Visual C++ Build Tools or MinGW-w64 for windows, GCC and cmake for\
    \ Linux . Therefore the build process for flash-attention may successfully.\n\
    2.\tInstall flash-attention through pre build binary rather than build in time\n\
    \                         \nAfter solve second error and successfully install\
    \ flash-attention, the first import module error should be solved at same time.\n"
  created_at: 2023-12-11 04:17:32+00:00
  edited: true
  hidden: false
  id: 65768d5c8b44ef012b8f5351
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Intel/neural-chat-7b-v1-1
repo_type: model
status: open
target_branch: null
title: Error load Model
