!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Joemgu
conflicting_files: null
created_at: 2023-07-24 15:54:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
      fullname: Jonas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joemgu
      type: user
    createdAt: '2023-07-24T16:54:49.000Z'
    data:
      edited: false
      editors:
      - Joemgu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8887677192687988
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
          fullname: Jonas
          isHf: false
          isPro: false
          name: Joemgu
          type: user
        html: '<p>I have used both the 7b and 13b chat-hf models in different computation
          types (int8, int8_float16, int8_bfloat16), however I always run into the
          same issue.<br>Inference is run with 3.17.1 and transformers 4.31.0.</p>

          <p>When using the correct prompt format (I triple-checked) ex. for summarization
          and the whole input prompt is longer than 2048 tokens the generation fails
          and produces weird hallucinations, returning a long chain of special characters
          like #, ", etc. With input prompts shorter than 2000 tokens, but output
          can be 1024, the model correctly performs tasks. However if I input &gt;
          3000 tokens, no matter the set generation size, the output will always return
          gibberish.</p>

          <p>It seems like as long both input sequence length and generated tokens
          combined are within 2048 tokens, everything works fine, but as soon as we
          start exceeding these 2048 tokens, the model starts to break down. I suspect
          (or at least it feels like) the model has only an effective context length
          of 2048. I can also notice the model starting to forget the initial instruction
          while generating tokens and exceeding this invisible 2048 token wall.</p>

          <p>Is there a way to fix this behavior and make use of the full 4096 context
          length?</p>

          '
        raw: "I have used both the 7b and 13b chat-hf models in different computation\
          \ types (int8, int8_float16, int8_bfloat16), however I always run into the\
          \ same issue.\r\nInference is run with 3.17.1 and transformers 4.31.0.\r\
          \n\r\nWhen using the correct prompt format (I triple-checked) ex. for summarization\
          \ and the whole input prompt is longer than 2048 tokens the generation fails\
          \ and produces weird hallucinations, returning a long chain of special characters\
          \ like #, \", etc. With input prompts shorter than 2000 tokens, but output\
          \ can be 1024, the model correctly performs tasks. However if I input >\
          \ 3000 tokens, no matter the set generation size, the output will always\
          \ return gibberish.\r\n\r\nIt seems like as long both input sequence length\
          \ and generated tokens combined are within 2048 tokens, everything works\
          \ fine, but as soon as we start exceeding these 2048 tokens, the model starts\
          \ to break down. I suspect (or at least it feels like) the model has only\
          \ an effective context length of 2048. I can also notice the model starting\
          \ to forget the initial instruction while generating tokens and exceeding\
          \ this invisible 2048 token wall.\r\n\r\nIs there a way to fix this behavior\
          \ and make use of the full 4096 context length?"
        updatedAt: '2023-07-24T16:54:49.684Z'
      numEdits: 0
      reactions: []
    id: 64beacd91a62149c5e988519
    type: comment
  author: Joemgu
  content: "I have used both the 7b and 13b chat-hf models in different computation\
    \ types (int8, int8_float16, int8_bfloat16), however I always run into the same\
    \ issue.\r\nInference is run with 3.17.1 and transformers 4.31.0.\r\n\r\nWhen\
    \ using the correct prompt format (I triple-checked) ex. for summarization and\
    \ the whole input prompt is longer than 2048 tokens the generation fails and produces\
    \ weird hallucinations, returning a long chain of special characters like #, \"\
    , etc. With input prompts shorter than 2000 tokens, but output can be 1024, the\
    \ model correctly performs tasks. However if I input > 3000 tokens, no matter\
    \ the set generation size, the output will always return gibberish.\r\n\r\nIt\
    \ seems like as long both input sequence length and generated tokens combined\
    \ are within 2048 tokens, everything works fine, but as soon as we start exceeding\
    \ these 2048 tokens, the model starts to break down. I suspect (or at least it\
    \ feels like) the model has only an effective context length of 2048. I can also\
    \ notice the model starting to forget the initial instruction while generating\
    \ tokens and exceeding this invisible 2048 token wall.\r\n\r\nIs there a way to\
    \ fix this behavior and make use of the full 4096 context length?"
  created_at: 2023-07-24 15:54:49+00:00
  edited: false
  hidden: false
  id: 64beacd91a62149c5e988519
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: michaelfeil
      type: user
    createdAt: '2023-07-24T17:43:34.000Z'
    data:
      edited: false
      editors:
      - michaelfeil
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9113658666610718
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644fac0ce1d7a97f3b653ab1/fottSAPFrJdKeMW2UJv_l.jpeg?w=200&h=200&f=face
          fullname: Michael
          isHf: false
          isPro: false
          name: michaelfeil
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;Joemgu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Joemgu\">@<span class=\"\
          underline\">Joemgu</span></a></span>\n\n\t</span></span>,<br>Thanks for\
          \ your issue post. I think, I may also have seen some issues for token inputs\
          \ &gt; 2048 tokens for ctranslate2 models like falcon - where i suspected\
          \ that is because of the model length.<br>I just converted the model with\
          \ <a rel=\"nofollow\" href=\"https://github.com/michaelfeil/hf-hub-ctranslate2/blob/main/conversion_utils/convert.py\"\
          >https://github.com/michaelfeil/hf-hub-ctranslate2/blob/main/conversion_utils/convert.py</a>\
          \ and uploaded it here.</p>\n<p>Would you be so kind an post your issue\
          \ here? <a rel=\"nofollow\" href=\"https://github.com/OpenNMT/CTranslate2/issues\"\
          >https://github.com/OpenNMT/CTranslate2/issues</a></p>\n"
        raw: 'Hi @Joemgu,

          Thanks for your issue post. I think, I may also have seen some issues for
          token inputs > 2048 tokens for ctranslate2 models like falcon - where i
          suspected that is because of the model length.

          I just converted the model with https://github.com/michaelfeil/hf-hub-ctranslate2/blob/main/conversion_utils/convert.py
          and uploaded it here.


          Would you be so kind an post your issue here? https://github.com/OpenNMT/CTranslate2/issues'
        updatedAt: '2023-07-24T17:43:34.352Z'
      numEdits: 0
      reactions: []
    id: 64beb8461a62149c5e99efb7
    type: comment
  author: michaelfeil
  content: 'Hi @Joemgu,

    Thanks for your issue post. I think, I may also have seen some issues for token
    inputs > 2048 tokens for ctranslate2 models like falcon - where i suspected that
    is because of the model length.

    I just converted the model with https://github.com/michaelfeil/hf-hub-ctranslate2/blob/main/conversion_utils/convert.py
    and uploaded it here.


    Would you be so kind an post your issue here? https://github.com/OpenNMT/CTranslate2/issues'
  created_at: 2023-07-24 16:43:34+00:00
  edited: false
  hidden: false
  id: 64beb8461a62149c5e99efb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
      fullname: Jonas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joemgu
      type: user
    createdAt: '2023-07-24T17:49:50.000Z'
    data:
      edited: false
      editors:
      - Joemgu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9736691117286682
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
          fullname: Jonas
          isHf: false
          isPro: false
          name: Joemgu
          type: user
        html: '<p>Thanks for the quick answer, just posted the issue in case you are
          interesting in following it.</p>

          <p>Edit: Just noticed you commented on it, thank you!</p>

          '
        raw: 'Thanks for the quick answer, just posted the issue in case you are interesting
          in following it.


          Edit: Just noticed you commented on it, thank you!'
        updatedAt: '2023-07-24T17:49:50.734Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - michaelfeil
    id: 64beb9be4d2052b1aa45f456
    type: comment
  author: Joemgu
  content: 'Thanks for the quick answer, just posted the issue in case you are interesting
    in following it.


    Edit: Just noticed you commented on it, thank you!'
  created_at: 2023-07-24 16:49:50+00:00
  edited: false
  hidden: false
  id: 64beb9be4d2052b1aa45f456
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
      fullname: Jonas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joemgu
      type: user
    createdAt: '2023-07-25T09:25:02.000Z'
    data:
      edited: false
      editors:
      - Joemgu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.978032112121582
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
          fullname: Jonas
          isHf: false
          isPro: false
          name: Joemgu
          type: user
        html: '<p>For anyone wondering, the issue was due to having the wrong cuda
          version installed, as the current supported version is 11. Than Michael
          for your help!</p>

          '
        raw: For anyone wondering, the issue was due to having the wrong cuda version
          installed, as the current supported version is 11. Than Michael for your
          help!
        updatedAt: '2023-07-25T09:25:02.199Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - michaelfeil
      relatedEventId: 64bf94ee12d00c4589391d2c
    id: 64bf94ee12d00c4589391d2b
    type: comment
  author: Joemgu
  content: For anyone wondering, the issue was due to having the wrong cuda version
    installed, as the current supported version is 11. Than Michael for your help!
  created_at: 2023-07-25 08:25:02+00:00
  edited: false
  hidden: false
  id: 64bf94ee12d00c4589391d2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62f8b731ebd15ad7b5af6cee/cg0VkLeobAl1hbfeSpVIH.jpeg?w=200&h=200&f=face
      fullname: Jonas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Joemgu
      type: user
    createdAt: '2023-07-25T09:25:02.000Z'
    data:
      status: closed
    id: 64bf94ee12d00c4589391d2c
    type: status-change
  author: Joemgu
  created_at: 2023-07-25 08:25:02+00:00
  id: 64bf94ee12d00c4589391d2c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: michaelfeil/ct2fast-Llama-2-7b-chat-hf
repo_type: model
status: closed
target_branch: null
title: Llama 2 fails with context length >> 2000
