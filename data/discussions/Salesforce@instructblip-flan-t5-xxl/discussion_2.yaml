!!python/object:huggingface_hub.community.DiscussionWithDetails
author: richardbasile
conflicting_files: null
created_at: 2023-09-27 17:19:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b04e1fe7a2f4133f95b26c04dc70d42.svg
      fullname: Richard Basile
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: richardbasile
      type: user
    createdAt: '2023-09-27T18:19:08.000Z'
    data:
      edited: false
      editors:
      - richardbasile
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6006391048431396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b04e1fe7a2f4133f95b26c04dc70d42.svg
          fullname: Richard Basile
          isHf: false
          isPro: false
          name: richardbasile
          type: user
        html: "<p>I am testing several image captioning models in a SageMaker image\
          \ terminal running python on a g5 instance. For instance, I was able to\
          \ test Salesforce/instructblip-flan-t5-<em>xl</em> on a ml.g5.4xlarge using\
          \ the <em>PyTorch 2.0.0 Python 3.10 GPU Optimized</em> image. However, when\
          \ I run the Salesforce/instructblip-flan-t5-<em>xxl</em>  model, I get the\
          \ below error specific to GPU memory regardless of the instance type used.\
          \ I did try setting PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024\u200B\
          \ which made no difference. Any insights on how to mitigate this error would\
          \ be appreciated.</p>\n<pre><code>Traceback (most recent call last):\n \
          \ File \"/root/image-caption-main/sample_smartsheet_images.py\", line 13,\
          \ in &lt;module&gt;\n    from model.salesforce.instructblip_flan_t5_xxl\
          \ import caption_image\n  File \"/root/image-caption-main/model/salesforce/instructblip_flan_t5_xxl.py\"\
          , line 12, in &lt;module&gt;\n    model.to(device)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2065, in to\n    return super().to(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1145, in to\n    return self._apply(convert)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 797, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 797, in _apply\n    module._apply(fn)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 797, in _apply\n    module._apply(fn)\n  [Previous line repeated\
          \ 5 more times]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 820, in _apply\n    param_applied = fn(param)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1143, in convert\n    return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None, non_blocking)\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 22.20 GiB total\
          \ capacity; 21.33 GiB already allocated; 73.12 MiB free; 21.43 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\n</code></pre>\n"
        raw: "I am testing several image captioning models in a SageMaker image terminal\
          \ running python on a g5 instance. For instance, I was able to test Salesforce/instructblip-flan-t5-*xl*\
          \ on a ml.g5.4xlarge using the *PyTorch 2.0.0 Python 3.10 GPU Optimized*\
          \ image. However, when I run the Salesforce/instructblip-flan-t5-*xxl* \
          \ model, I get the below error specific to GPU memory regardless of the\
          \ instance type used. I did try setting PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024\u200B\
          \ which made no difference. Any insights on how to mitigate this error would\
          \ be appreciated.\r\n\r\n```\r\nTraceback (most recent call last):\r\n \
          \ File \"/root/image-caption-main/sample_smartsheet_images.py\", line 13,\
          \ in <module>\r\n    from model.salesforce.instructblip_flan_t5_xxl import\
          \ caption_image\r\n  File \"/root/image-caption-main/model/salesforce/instructblip_flan_t5_xxl.py\"\
          , line 12, in <module>\r\n    model.to(device)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
          , line 2065, in to\r\n    return super().to(*args, **kwargs)\r\n  File \"\
          /opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line\
          \ 1145, in to\r\n    return self._apply(convert)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 797, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated\
          \ 5 more times]\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 820, in _apply\r\n    param_applied = fn(param)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1143, in convert\r\n    return t.to(device, dtype if t.is_floating_point()\
          \ or t.is_complex() else None, non_blocking)\r\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 160.00 MiB (GPU 0; 22.20 GiB total\
          \ capacity; 21.33 GiB already allocated; 73.12 MiB free; 21.43 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\n```"
        updatedAt: '2023-09-27T18:19:08.140Z'
      numEdits: 0
      reactions: []
    id: 6514721c4a40f143595a6062
    type: comment
  author: richardbasile
  content: "I am testing several image captioning models in a SageMaker image terminal\
    \ running python on a g5 instance. For instance, I was able to test Salesforce/instructblip-flan-t5-*xl*\
    \ on a ml.g5.4xlarge using the *PyTorch 2.0.0 Python 3.10 GPU Optimized* image.\
    \ However, when I run the Salesforce/instructblip-flan-t5-*xxl*  model, I get\
    \ the below error specific to GPU memory regardless of the instance type used.\
    \ I did try setting PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024\u200B which\
    \ made no difference. Any insights on how to mitigate this error would be appreciated.\r\
    \n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/root/image-caption-main/sample_smartsheet_images.py\"\
    , line 13, in <module>\r\n    from model.salesforce.instructblip_flan_t5_xxl import\
    \ caption_image\r\n  File \"/root/image-caption-main/model/salesforce/instructblip_flan_t5_xxl.py\"\
    , line 12, in <module>\r\n    model.to(device)\r\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\"\
    , line 2065, in to\r\n    return super().to(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1145, in to\r\n    return self._apply(convert)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 797, in _apply\r\n    module._apply(fn)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 797, in _apply\r\n    module._apply(fn)\r\n  [Previous line repeated 5\
    \ more times]\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 820, in _apply\r\n    param_applied = fn(param)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1143, in convert\r\n    return t.to(device, dtype if t.is_floating_point()\
    \ or t.is_complex() else None, non_blocking)\r\ntorch.cuda.OutOfMemoryError: CUDA\
    \ out of memory. Tried to allocate 160.00 MiB (GPU 0; 22.20 GiB total capacity;\
    \ 21.33 GiB already allocated; 73.12 MiB free; 21.43 GiB reserved in total by\
    \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n```"
  created_at: 2023-09-27 17:19:08+00:00
  edited: false
  hidden: false
  id: 6514721c4a40f143595a6062
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-09-28T16:38:01.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579931497573853
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>did you use 8 or 4-bit inference?</p>

          '
        raw: 'Hi,


          did you use 8 or 4-bit inference?'
        updatedAt: '2023-09-28T16:38:01.555Z'
      numEdits: 0
      reactions: []
    id: 6515abe9cc0f7d552b6c26f9
    type: comment
  author: nielsr
  content: 'Hi,


    did you use 8 or 4-bit inference?'
  created_at: 2023-09-28 15:38:01+00:00
  edited: false
  hidden: false
  id: 6515abe9cc0f7d552b6c26f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0b04e1fe7a2f4133f95b26c04dc70d42.svg
      fullname: Richard Basile
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: richardbasile
      type: user
    createdAt: '2023-10-02T12:41:48.000Z'
    data:
      edited: false
      editors:
      - richardbasile
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.918749988079071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0b04e1fe7a2f4133f95b26c04dc70d42.svg
          fullname: Richard Basile
          isHf: false
          isPro: false
          name: richardbasile
          type: user
        html: '<p>I used the code found on the model card: <a href="https://huggingface.co/Salesforce/instructblip-flan-t5-xxl">https://huggingface.co/Salesforce/instructblip-flan-t5-xxl</a></p>

          <p>The model card makes no reference to 8-bit or 4-bit inference. </p>

          <p>From what we have observed, the model is too large to fit on a single
          GPU. When executing, a single GPU is maxed out resulting in the above error
          while the remaining GPUs are not used. I am currently looking at the accelerate
          library as a possible solution.</p>

          '
        raw: "I used the code found on the model card: https://huggingface.co/Salesforce/instructblip-flan-t5-xxl\n\
          \nThe model card makes no reference to 8-bit or 4-bit inference. \n\nFrom\
          \ what we have observed, the model is too large to fit on a single GPU.\
          \ When executing, a single GPU is maxed out resulting in the above error\
          \ while the remaining GPUs are not used. I am currently looking at the accelerate\
          \ library as a possible solution."
        updatedAt: '2023-10-02T12:41:48.128Z'
      numEdits: 0
      reactions: []
    id: 651aba8c8e62b015b8417334
    type: comment
  author: richardbasile
  content: "I used the code found on the model card: https://huggingface.co/Salesforce/instructblip-flan-t5-xxl\n\
    \nThe model card makes no reference to 8-bit or 4-bit inference. \n\nFrom what\
    \ we have observed, the model is too large to fit on a single GPU. When executing,\
    \ a single GPU is maxed out resulting in the above error while the remaining GPUs\
    \ are not used. I am currently looking at the accelerate library as a possible\
    \ solution."
  created_at: 2023-10-02 11:41:48+00:00
  edited: false
  hidden: false
  id: 651aba8c8e62b015b8417334
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
      fullname: Niels Rogge
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nielsr
      type: user
    createdAt: '2023-10-04T20:43:49.000Z'
    data:
      edited: false
      editors:
      - nielsr
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7334703207015991
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg?w=200&h=200&f=face
          fullname: Niels Rogge
          isHf: true
          isPro: false
          name: nielsr
          type: user
        html: '<p>Hi,</p>

          <p>Refer to the code snippets of BLIP-2 regarding 4- and 8-bit inference:
          <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b#running-the-model-on-gpu">https://huggingface.co/Salesforce/blip2-opt-2.7b#running-the-model-on-gpu</a>.</p>

          <p>These greatly reduce the amount of memory (by a factor of 4 to 8).</p>

          '
        raw: 'Hi,


          Refer to the code snippets of BLIP-2 regarding 4- and 8-bit inference: https://huggingface.co/Salesforce/blip2-opt-2.7b#running-the-model-on-gpu.


          These greatly reduce the amount of memory (by a factor of 4 to 8).'
        updatedAt: '2023-10-04T20:43:49.300Z'
      numEdits: 0
      reactions: []
    id: 651dce85d7152e29cf808fe5
    type: comment
  author: nielsr
  content: 'Hi,


    Refer to the code snippets of BLIP-2 regarding 4- and 8-bit inference: https://huggingface.co/Salesforce/blip2-opt-2.7b#running-the-model-on-gpu.


    These greatly reduce the amount of memory (by a factor of 4 to 8).'
  created_at: 2023-10-04 19:43:49+00:00
  edited: false
  hidden: false
  id: 651dce85d7152e29cf808fe5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Salesforce/instructblip-flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: 'torch.cuda.OutOfMemoryError: CUDA out of memory'
