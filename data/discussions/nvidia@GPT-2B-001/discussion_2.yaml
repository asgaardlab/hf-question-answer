!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dataplayer12
conflicting_files: null
created_at: 2023-05-02 09:17:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2fa0944081ed996c31f648eae065f32.svg
      fullname: Jaiyam Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dataplayer12
      type: user
    createdAt: '2023-05-02T10:17:26.000Z'
    data:
      edited: false
      editors:
      - dataplayer12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2fa0944081ed996c31f648eae065f32.svg
          fullname: Jaiyam Sharma
          isHf: false
          isPro: false
          name: dataplayer12
          type: user
        html: "<p>I am using the latest official Nemo docker container (tag 23.02)\
          \ and installed <code>apex</code> and <code>NeMo</code> as recommended.\
          \ I still get this error. What am I doing wrong? I'm using an A5000 GPU.</p>\n\
          <pre><code class=\"language-Shell\">python megatron_gpt_eval.py gpt_model_file=/gpt/GPT-2B-001/GPT-2B-001_bf16_tp1.nemo\
          \ server=True tensor_model_parallel_size=1 trainer.devices=1\n[NeMo W 2023-05-02\
          \ 09:58:51 experimental:27] Module &lt;class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'&gt;\
          \ is experimental, not ready for production and is not fully supported.\
          \ Use at your own risk.\n[NeMo W 2023-05-02 09:58:51 experimental:27] Module\
          \ &lt;class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'&gt;\
          \ is experimental, not ready for production and is not fully supported.\
          \ Use at your own risk.\n[NeMo W 2023-05-02 09:58:52 experimental:27] Module\
          \ &lt;class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'&gt;\
          \ is experimental, not ready for production and is not fully supported.\
          \ Use at your own risk.\n[NeMo W 2023-05-02 09:58:53 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119:\
          \ UserWarning: Future Hydra versions will no longer change working directory\
          \ at job runtime by default.\n    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/\
          \ for more information.\n      ret = run_job(\n\nUsing 16bit None Automatic\
          \ Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available:\
          \ False, using: 0 TPU cores\nIPU available: False, using: 0 IPUs\nHPU available:\
          \ False, using: 0 HPUs\n[NeMo I 2023-05-02 09:59:11 megatron_init:225] Rank\
          \ 0 has data parallel group: [0, 1]\n[NeMo I 2023-05-02 09:59:11 megatron_init:228]\
          \ All data parallel group ranks: [[0, 1]]\n[NeMo I 2023-05-02 09:59:11 megatron_init:229]\
          \ Ranks 0 has data parallel rank: 0\n[NeMo I 2023-05-02 09:59:11 megatron_init:237]\
          \ Rank 0 has model parallel group: [0]\n[NeMo I 2023-05-02 09:59:11 megatron_init:238]\
          \ All model parallel group ranks: [[0], [1]]\n[NeMo I 2023-05-02 09:59:11\
          \ megatron_init:248] Rank 0 has tensor model parallel group: [0]\n[NeMo\
          \ I 2023-05-02 09:59:11 megatron_init:252] All tensor model parallel group\
          \ ranks: [[0], [1]]\n[NeMo I 2023-05-02 09:59:11 megatron_init:253] Rank\
          \ 0 has tensor model parallel rank: 0\n[NeMo I 2023-05-02 09:59:11 megatron_init:267]\
          \ Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2023-05-02 09:59:11\
          \ megatron_init:279] Rank 0 has embedding group: [0]\n[NeMo I 2023-05-02\
          \ 09:59:11 megatron_init:285] All pipeline model parallel group ranks: [[0],\
          \ [1]]\n[NeMo I 2023-05-02 09:59:11 megatron_init:286] Rank 0 has pipeline\
          \ model parallel rank 0\n[NeMo I 2023-05-02 09:59:11 megatron_init:287]\
          \ All embedding group ranks: [[0], [1]]\n[NeMo I 2023-05-02 09:59:11 megatron_init:288]\
          \ Rank 0 has embedding rank: 0\n23-05-02 09:59:11 - PID:673 - rank:(0, 0,\
          \ 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to\
          \ constant 1\n[NeMo I 2023-05-02 09:59:11 tokenizer_utils:191] Getting SentencePiece\
          \ with model: /tmp/tmpm0_nvln6/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n\
          [NeMo I 2023-05-02 09:59:11 megatron_base_model:205] Padded vocab_size:\
          \ 256000, original vocab_size: 256000, dummy tokens: 0.\n[NeMo I 2023-05-02\
          \ 09:59:12 megatron_init:225] Rank 0 has data parallel group: [0, 1]\n[NeMo\
          \ I 2023-05-02 09:59:12 megatron_init:228] All data parallel group ranks:\
          \ [[0, 1]]\n[NeMo I 2023-05-02 09:59:12 megatron_init:229] Ranks 0 has data\
          \ parallel rank: 0\n[NeMo I 2023-05-02 09:59:12 megatron_init:237] Rank\
          \ 0 has model parallel group: [0]\n[NeMo I 2023-05-02 09:59:12 megatron_init:238]\
          \ All model parallel group ranks: [[0], [1]]\n[NeMo I 2023-05-02 09:59:12\
          \ megatron_init:248] Rank 0 has tensor model parallel group: [0]\n[NeMo\
          \ I 2023-05-02 09:59:12 megatron_init:252] All tensor model parallel group\
          \ ranks: [[0], [1]]\n[NeMo I 2023-05-02 09:59:12 megatron_init:253] Rank\
          \ 0 has tensor model parallel rank: 0\n[NeMo I 2023-05-02 09:59:12 megatron_init:267]\
          \ Rank 0 has pipeline model parallel group: [0]\n[NeMo I 2023-05-02 09:59:12\
          \ megatron_init:279] Rank 0 has embedding group: [0]\n[NeMo I 2023-05-02\
          \ 09:59:12 megatron_init:285] All pipeline model parallel group ranks: [[0],\
          \ [1]]\n[NeMo I 2023-05-02 09:59:12 megatron_init:286] Rank 0 has pipeline\
          \ model parallel rank 0\n[NeMo I 2023-05-02 09:59:12 megatron_init:287]\
          \ All embedding group ranks: [[0], [1]]\n[NeMo I 2023-05-02 09:59:12 megatron_init:288]\
          \ Rank 0 has embedding rank: 0\n[NeMo I 2023-05-02 09:59:12 tokenizer_utils:191]\
          \ Getting SentencePiece with model: /tmp/tmpm0_nvln6/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\n\
          [NeMo I 2023-05-02 09:59:12 megatron_base_model:205] Padded vocab_size:\
          \ 256000, original vocab_size: 256000, dummy tokens: 0.\n[NeMo E 2023-05-02\
          \ 09:59:12 common:506] Model instantiation failed!\n    Target class:  \
          \     nemo.collections.nlp.models.language_modeling.megatron_gpt_model.MegatronGPTModel\n\
          \    Error(s):   precision 16 is not supported. Float16Module (megatron_amp_O2)\
          \ supports only fp16 and bf16.\n    Traceback (most recent call last):\n\
          \      File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
          , line 485, in from_config_dict\n        instance = imported_cls(cfg=config,\
          \ trainer=trainer)\n      File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\"\
          , line 128, in __init__\n        self.model = Float16Module(module=self.model,\
          \ precision=cfg.precision)\n      File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/modules/common/megatron/module.py\"\
          , line 278, in __init__\n        raise Exception(\n    Exception: precision\
          \ 16 is not supported. Float16Module (megatron_amp_O2) supports only fp16\
          \ and bf16.\n\nError executing job with overrides: ['gpt_model_file=/jaiyam/gpt/GPT-2B-001/GPT-2B-001_bf16_tp1.nemo',\
          \ 'server=True', 'tensor_model_parallel_size=2', 'trainer.devices=2']\n\
          Traceback (most recent call last):\n  File \"megatron_gpt_eval.py\", line\
          \ 279, in &lt;module&gt;\n    main()  # noqa pylint: disable=no-value-for-parameter\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/config/hydra_runner.py\"\
          , line 105, in wrapper\n    _run_hydra(\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 389, in _run_hydra\n    _run_app(\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 452, in _run_app\n    run_and_report(\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 216, in run_and_report\n    raise ex\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 213, in run_and_report\n    return func()\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 453, in &lt;lambda&gt;\n    lambda: hydra.run(\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py\"\
          , line 132, in run\n    _ = ret.return_value\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\"\
          , line 260, in return_value\n    raise self._return_value\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\"\
          , line 186, in run_job\n    ret.return_value = task_function(task_cfg)\n\
          \  File \"megatron_gpt_eval.py\", line 182, in main\n    model = MegatronGPTModel.restore_from(\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/modelPT.py\"\
          , line 436, in restore_from\n    instance = cls._save_restore_connector.restore_from(\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/parts/nlp_overrides.py\"\
          , line 366, in restore_from\n    loaded_params = super().load_config_and_state_dict(\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/connectors/save_restore_connector.py\"\
          , line 162, in load_config_and_state_dict\n    instance = calling_cls.from_config_dict(config=conf,\
          \ trainer=trainer)\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
          , line 507, in from_config_dict\n    raise e\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
          , line 499, in from_config_dict\n    instance = cls(cfg=config, trainer=trainer)\n\
          \  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\"\
          , line 128, in __init__\n    self.model = Float16Module(module=self.model,\
          \ precision=cfg.precision)\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/modules/common/megatron/module.py\"\
          , line 278, in __init__\n    raise Exception(\nException: precision 16 is\
          \ not supported. Float16Module (megatron_amp_O2) supports only fp16 and\
          \ bf16.\n</code></pre>\n"
        raw: "I am using the latest official Nemo docker container (tag 23.02) and\
          \ installed `apex` and `NeMo` as recommended. I still get this error. What\
          \ am I doing wrong? I'm using an A5000 GPU.\r\n\r\n```Shell\r\npython megatron_gpt_eval.py\
          \ gpt_model_file=/gpt/GPT-2B-001/GPT-2B-001_bf16_tp1.nemo server=True tensor_model_parallel_size=1\
          \ trainer.devices=1\r\n[NeMo W 2023-05-02 09:58:51 experimental:27] Module\
          \ <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'>\
          \ is experimental, not ready for production and is not fully supported.\
          \ Use at your own risk.\r\n[NeMo W 2023-05-02 09:58:51 experimental:27]\
          \ Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'>\
          \ is experimental, not ready for production and is not fully supported.\
          \ Use at your own risk.\r\n[NeMo W 2023-05-02 09:58:52 experimental:27]\
          \ Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'>\
          \ is experimental, not ready for production and is not fully supported.\
          \ Use at your own risk.\r\n[NeMo W 2023-05-02 09:58:53 nemo_logging:349]\
          \ /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning:\
          \ Future Hydra versions will no longer change working directory at job runtime\
          \ by default.\r\n    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/\
          \ for more information.\r\n      ret = run_job(\r\n\r\nUsing 16bit None\
          \ Automatic Mixed Precision (AMP)\r\nGPU available: True (cuda), used: True\r\
          \nTPU available: False, using: 0 TPU cores\r\nIPU available: False, using:\
          \ 0 IPUs\r\nHPU available: False, using: 0 HPUs\r\n[NeMo I 2023-05-02 09:59:11\
          \ megatron_init:225] Rank 0 has data parallel group: [0, 1]\r\n[NeMo I 2023-05-02\
          \ 09:59:11 megatron_init:228] All data parallel group ranks: [[0, 1]]\r\n\
          [NeMo I 2023-05-02 09:59:11 megatron_init:229] Ranks 0 has data parallel\
          \ rank: 0\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:237] Rank 0 has model\
          \ parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:238]\
          \ All model parallel group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11\
          \ megatron_init:248] Rank 0 has tensor model parallel group: [0]\r\n[NeMo\
          \ I 2023-05-02 09:59:11 megatron_init:252] All tensor model parallel group\
          \ ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:253] Rank\
          \ 0 has tensor model parallel rank: 0\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:267]\
          \ Rank 0 has pipeline model parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:11\
          \ megatron_init:279] Rank 0 has embedding group: [0]\r\n[NeMo I 2023-05-02\
          \ 09:59:11 megatron_init:285] All pipeline model parallel group ranks: [[0],\
          \ [1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:286] Rank 0 has pipeline\
          \ model parallel rank 0\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:287]\
          \ All embedding group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:288]\
          \ Rank 0 has embedding rank: 0\r\n23-05-02 09:59:11 - PID:673 - rank:(0,\
          \ 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches\
          \ to constant 1\r\n[NeMo I 2023-05-02 09:59:11 tokenizer_utils:191] Getting\
          \ SentencePiece with model: /tmp/tmpm0_nvln6/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\r\
          \n[NeMo I 2023-05-02 09:59:11 megatron_base_model:205] Padded vocab_size:\
          \ 256000, original vocab_size: 256000, dummy tokens: 0.\r\n[NeMo I 2023-05-02\
          \ 09:59:12 megatron_init:225] Rank 0 has data parallel group: [0, 1]\r\n\
          [NeMo I 2023-05-02 09:59:12 megatron_init:228] All data parallel group ranks:\
          \ [[0, 1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:229] Ranks 0 has\
          \ data parallel rank: 0\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:237]\
          \ Rank 0 has model parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:238]\
          \ All model parallel group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12\
          \ megatron_init:248] Rank 0 has tensor model parallel group: [0]\r\n[NeMo\
          \ I 2023-05-02 09:59:12 megatron_init:252] All tensor model parallel group\
          \ ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:253] Rank\
          \ 0 has tensor model parallel rank: 0\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:267]\
          \ Rank 0 has pipeline model parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:12\
          \ megatron_init:279] Rank 0 has embedding group: [0]\r\n[NeMo I 2023-05-02\
          \ 09:59:12 megatron_init:285] All pipeline model parallel group ranks: [[0],\
          \ [1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:286] Rank 0 has pipeline\
          \ model parallel rank 0\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:287]\
          \ All embedding group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:288]\
          \ Rank 0 has embedding rank: 0\r\n[NeMo I 2023-05-02 09:59:12 tokenizer_utils:191]\
          \ Getting SentencePiece with model: /tmp/tmpm0_nvln6/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\r\
          \n[NeMo I 2023-05-02 09:59:12 megatron_base_model:205] Padded vocab_size:\
          \ 256000, original vocab_size: 256000, dummy tokens: 0.\r\n[NeMo E 2023-05-02\
          \ 09:59:12 common:506] Model instantiation failed!\r\n    Target class:\
          \       nemo.collections.nlp.models.language_modeling.megatron_gpt_model.MegatronGPTModel\r\
          \n    Error(s):   precision 16 is not supported. Float16Module (megatron_amp_O2)\
          \ supports only fp16 and bf16.\r\n    Traceback (most recent call last):\r\
          \n      File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
          , line 485, in from_config_dict\r\n        instance = imported_cls(cfg=config,\
          \ trainer=trainer)\r\n      File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\"\
          , line 128, in __init__\r\n        self.model = Float16Module(module=self.model,\
          \ precision=cfg.precision)\r\n      File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/modules/common/megatron/module.py\"\
          , line 278, in __init__\r\n        raise Exception(\r\n    Exception: precision\
          \ 16 is not supported. Float16Module (megatron_amp_O2) supports only fp16\
          \ and bf16.\r\n\r\nError executing job with overrides: ['gpt_model_file=/jaiyam/gpt/GPT-2B-001/GPT-2B-001_bf16_tp1.nemo',\
          \ 'server=True', 'tensor_model_parallel_size=2', 'trainer.devices=2']\r\n\
          Traceback (most recent call last):\r\n  File \"megatron_gpt_eval.py\", line\
          \ 279, in <module>\r\n    main()  # noqa pylint: disable=no-value-for-parameter\r\
          \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/config/hydra_runner.py\"\
          , line 105, in wrapper\r\n    _run_hydra(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 389, in _run_hydra\r\n    _run_app(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 452, in _run_app\r\n    run_and_report(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 216, in run_and_report\r\n    raise ex\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 213, in run_and_report\r\n    return func()\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
          , line 453, in <lambda>\r\n    lambda: hydra.run(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py\"\
          , line 132, in run\r\n    _ = ret.return_value\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\"\
          , line 260, in return_value\r\n    raise self._return_value\r\n  File \"\
          /usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\", line 186,\
          \ in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n  File\
          \ \"megatron_gpt_eval.py\", line 182, in main\r\n    model = MegatronGPTModel.restore_from(\r\
          \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/modelPT.py\"\
          , line 436, in restore_from\r\n    instance = cls._save_restore_connector.restore_from(\r\
          \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/parts/nlp_overrides.py\"\
          , line 366, in restore_from\r\n    loaded_params = super().load_config_and_state_dict(\r\
          \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/connectors/save_restore_connector.py\"\
          , line 162, in load_config_and_state_dict\r\n    instance = calling_cls.from_config_dict(config=conf,\
          \ trainer=trainer)\r\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
          , line 507, in from_config_dict\r\n    raise e\r\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
          , line 499, in from_config_dict\r\n    instance = cls(cfg=config, trainer=trainer)\r\
          \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\"\
          , line 128, in __init__\r\n    self.model = Float16Module(module=self.model,\
          \ precision=cfg.precision)\r\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/modules/common/megatron/module.py\"\
          , line 278, in __init__\r\n    raise Exception(\r\nException: precision\
          \ 16 is not supported. Float16Module (megatron_amp_O2) supports only fp16\
          \ and bf16.\r\n```"
        updatedAt: '2023-05-02T10:17:26.054Z'
      numEdits: 0
      reactions: []
    id: 6450e3361b1c19bdb8d2ec77
    type: comment
  author: dataplayer12
  content: "I am using the latest official Nemo docker container (tag 23.02) and installed\
    \ `apex` and `NeMo` as recommended. I still get this error. What am I doing wrong?\
    \ I'm using an A5000 GPU.\r\n\r\n```Shell\r\npython megatron_gpt_eval.py gpt_model_file=/gpt/GPT-2B-001/GPT-2B-001_bf16_tp1.nemo\
    \ server=True tensor_model_parallel_size=1 trainer.devices=1\r\n[NeMo W 2023-05-02\
    \ 09:58:51 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'>\
    \ is experimental, not ready for production and is not fully supported. Use at\
    \ your own risk.\r\n[NeMo W 2023-05-02 09:58:51 experimental:27] Module <class\
    \ 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'>\
    \ is experimental, not ready for production and is not fully supported. Use at\
    \ your own risk.\r\n[NeMo W 2023-05-02 09:58:52 experimental:27] Module <class\
    \ 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'>\
    \ is experimental, not ready for production and is not fully supported. Use at\
    \ your own risk.\r\n[NeMo W 2023-05-02 09:58:53 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119:\
    \ UserWarning: Future Hydra versions will no longer change working directory at\
    \ job runtime by default.\r\n    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/\
    \ for more information.\r\n      ret = run_job(\r\n\r\nUsing 16bit None Automatic\
    \ Mixed Precision (AMP)\r\nGPU available: True (cuda), used: True\r\nTPU available:\
    \ False, using: 0 TPU cores\r\nIPU available: False, using: 0 IPUs\r\nHPU available:\
    \ False, using: 0 HPUs\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:225] Rank\
    \ 0 has data parallel group: [0, 1]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:228]\
    \ All data parallel group ranks: [[0, 1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:229]\
    \ Ranks 0 has data parallel rank: 0\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:237]\
    \ Rank 0 has model parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:238]\
    \ All model parallel group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:248]\
    \ Rank 0 has tensor model parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:252]\
    \ All tensor model parallel group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11\
    \ megatron_init:253] Rank 0 has tensor model parallel rank: 0\r\n[NeMo I 2023-05-02\
    \ 09:59:11 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\r\n\
    [NeMo I 2023-05-02 09:59:11 megatron_init:279] Rank 0 has embedding group: [0]\r\
    \n[NeMo I 2023-05-02 09:59:11 megatron_init:285] All pipeline model parallel group\
    \ ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:286] Rank 0 has\
    \ pipeline model parallel rank 0\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:287]\
    \ All embedding group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:11 megatron_init:288]\
    \ Rank 0 has embedding rank: 0\r\n23-05-02 09:59:11 - PID:673 - rank:(0, 0, 0,\
    \ 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant\
    \ 1\r\n[NeMo I 2023-05-02 09:59:11 tokenizer_utils:191] Getting SentencePiece\
    \ with model: /tmp/tmpm0_nvln6/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\r\
    \n[NeMo I 2023-05-02 09:59:11 megatron_base_model:205] Padded vocab_size: 256000,\
    \ original vocab_size: 256000, dummy tokens: 0.\r\n[NeMo I 2023-05-02 09:59:12\
    \ megatron_init:225] Rank 0 has data parallel group: [0, 1]\r\n[NeMo I 2023-05-02\
    \ 09:59:12 megatron_init:228] All data parallel group ranks: [[0, 1]]\r\n[NeMo\
    \ I 2023-05-02 09:59:12 megatron_init:229] Ranks 0 has data parallel rank: 0\r\
    \n[NeMo I 2023-05-02 09:59:12 megatron_init:237] Rank 0 has model parallel group:\
    \ [0]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:238] All model parallel group\
    \ ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:248] Rank 0 has\
    \ tensor model parallel group: [0]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:252]\
    \ All tensor model parallel group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12\
    \ megatron_init:253] Rank 0 has tensor model parallel rank: 0\r\n[NeMo I 2023-05-02\
    \ 09:59:12 megatron_init:267] Rank 0 has pipeline model parallel group: [0]\r\n\
    [NeMo I 2023-05-02 09:59:12 megatron_init:279] Rank 0 has embedding group: [0]\r\
    \n[NeMo I 2023-05-02 09:59:12 megatron_init:285] All pipeline model parallel group\
    \ ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:286] Rank 0 has\
    \ pipeline model parallel rank 0\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:287]\
    \ All embedding group ranks: [[0], [1]]\r\n[NeMo I 2023-05-02 09:59:12 megatron_init:288]\
    \ Rank 0 has embedding rank: 0\r\n[NeMo I 2023-05-02 09:59:12 tokenizer_utils:191]\
    \ Getting SentencePiece with model: /tmp/tmpm0_nvln6/2053796188904e679f7e2754a2a1f280_mt_nlg_plus_multilingual_ja_zh_the_stack_frac_015_256k.model\r\
    \n[NeMo I 2023-05-02 09:59:12 megatron_base_model:205] Padded vocab_size: 256000,\
    \ original vocab_size: 256000, dummy tokens: 0.\r\n[NeMo E 2023-05-02 09:59:12\
    \ common:506] Model instantiation failed!\r\n    Target class:       nemo.collections.nlp.models.language_modeling.megatron_gpt_model.MegatronGPTModel\r\
    \n    Error(s):   precision 16 is not supported. Float16Module (megatron_amp_O2)\
    \ supports only fp16 and bf16.\r\n    Traceback (most recent call last):\r\n \
    \     File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
    , line 485, in from_config_dict\r\n        instance = imported_cls(cfg=config,\
    \ trainer=trainer)\r\n      File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\"\
    , line 128, in __init__\r\n        self.model = Float16Module(module=self.model,\
    \ precision=cfg.precision)\r\n      File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/modules/common/megatron/module.py\"\
    , line 278, in __init__\r\n        raise Exception(\r\n    Exception: precision\
    \ 16 is not supported. Float16Module (megatron_amp_O2) supports only fp16 and\
    \ bf16.\r\n\r\nError executing job with overrides: ['gpt_model_file=/jaiyam/gpt/GPT-2B-001/GPT-2B-001_bf16_tp1.nemo',\
    \ 'server=True', 'tensor_model_parallel_size=2', 'trainer.devices=2']\r\nTraceback\
    \ (most recent call last):\r\n  File \"megatron_gpt_eval.py\", line 279, in <module>\r\
    \n    main()  # noqa pylint: disable=no-value-for-parameter\r\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/config/hydra_runner.py\"\
    , line 105, in wrapper\r\n    _run_hydra(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
    , line 389, in _run_hydra\r\n    _run_app(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
    , line 452, in _run_app\r\n    run_and_report(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
    , line 216, in run_and_report\r\n    raise ex\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
    , line 213, in run_and_report\r\n    return func()\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/utils.py\"\
    , line 453, in <lambda>\r\n    lambda: hydra.run(\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py\"\
    , line 132, in run\r\n    _ = ret.return_value\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\"\
    , line 260, in return_value\r\n    raise self._return_value\r\n  File \"/usr/local/lib/python3.8/dist-packages/hydra/core/utils.py\"\
    , line 186, in run_job\r\n    ret.return_value = task_function(task_cfg)\r\n \
    \ File \"megatron_gpt_eval.py\", line 182, in main\r\n    model = MegatronGPTModel.restore_from(\r\
    \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/modelPT.py\"\
    , line 436, in restore_from\r\n    instance = cls._save_restore_connector.restore_from(\r\
    \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/parts/nlp_overrides.py\"\
    , line 366, in restore_from\r\n    loaded_params = super().load_config_and_state_dict(\r\
    \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/connectors/save_restore_connector.py\"\
    , line 162, in load_config_and_state_dict\r\n    instance = calling_cls.from_config_dict(config=conf,\
    \ trainer=trainer)\r\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
    , line 507, in from_config_dict\r\n    raise e\r\n  File \"/usr/local/lib/python3.8/dist-packages/nemo/core/classes/common.py\"\
    , line 499, in from_config_dict\r\n    instance = cls(cfg=config, trainer=trainer)\r\
    \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py\"\
    , line 128, in __init__\r\n    self.model = Float16Module(module=self.model, precision=cfg.precision)\r\
    \n  File \"/usr/local/lib/python3.8/dist-packages/nemo/collections/nlp/modules/common/megatron/module.py\"\
    , line 278, in __init__\r\n    raise Exception(\r\nException: precision 16 is\
    \ not supported. Float16Module (megatron_amp_O2) supports only fp16 and bf16.\r\
    \n```"
  created_at: 2023-05-02 09:17:26+00:00
  edited: false
  hidden: false
  id: 6450e3361b1c19bdb8d2ec77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3eebe6e2d5c8da64a6006b79222c5622.svg
      fullname: "\xD6zg\xFCr An\u0131l \xD6zl\xFC"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anilozlu
      type: user
    createdAt: '2023-05-02T10:34:36.000Z'
    data:
      edited: true
      editors:
      - anilozlu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3eebe6e2d5c8da64a6006b79222c5622.svg
          fullname: "\xD6zg\xFCr An\u0131l \xD6zl\xFC"
          isHf: false
          isPro: false
          name: anilozlu
          type: user
        html: '<p>I have the same problem</p>

          <p>EDIT: I was able to circumvent this problem by editing line 278 in python3.8/site-packages/nemo/collections/nlp/modules/common/megatron
          from</p>

          <pre><code>if precision == 16:

          </code></pre>

          <p>to</p>

          <pre><code>if precision == "16":

          </code></pre>

          <p>although I am having a different problem now. I hope this helps you!</p>

          '
        raw: 'I have the same problem


          EDIT: I was able to circumvent this problem by editing line 278 in python3.8/site-packages/nemo/collections/nlp/modules/common/megatron
          from

          ```

          if precision == 16:

          ```

          to

          ```

          if precision == "16":

          ```


          although I am having a different problem now. I hope this helps you!'
        updatedAt: '2023-05-02T11:16:08.797Z'
      numEdits: 2
      reactions: []
    id: 6450e73c8c5830e111da84c6
    type: comment
  author: anilozlu
  content: 'I have the same problem


    EDIT: I was able to circumvent this problem by editing line 278 in python3.8/site-packages/nemo/collections/nlp/modules/common/megatron
    from

    ```

    if precision == 16:

    ```

    to

    ```

    if precision == "16":

    ```


    although I am having a different problem now. I hope this helps you!'
  created_at: 2023-05-02 09:34:36+00:00
  edited: true
  hidden: false
  id: 6450e73c8c5830e111da84c6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b8ca0b4e841858c1d234671187234f56.svg
      fullname: Sandeep Subramanian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaximumEntropy
      type: user
    createdAt: '2023-05-03T03:26:15.000Z'
    data:
      edited: true
      editors:
      - MaximumEntropy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b8ca0b4e841858c1d234671187234f56.svg
          fullname: Sandeep Subramanian
          isHf: false
          isPro: false
          name: MaximumEntropy
          type: user
        html: "<p>Hi, since you are running on an A5000 GPU, can you please try to\
          \ specify trainer.precision=bf16 when running megatron_gpt_eval.py? We have\
          \ a fix for trainer.precision=16 in NeMo 1.18 here <a rel=\"nofollow\" href=\"\
          https://github.com/NVIDIA/NeMo/pull/6543\">https://github.com/NVIDIA/NeMo/pull/6543</a>.</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;anilozlu&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/anilozlu\">@<span class=\"\
          underline\">anilozlu</span></a></span>\n\n\t</span></span> Changing the\
          \ check from an int to a string is not recommended. The error is correct\
          \ in pointing out that <code>megatron_amp_O2</code> will not work with <code>bf16</code>.</p>\n"
        raw: 'Hi, since you are running on an A5000 GPU, can you please try to specify
          trainer.precision=bf16 when running megatron_gpt_eval.py? We have a fix
          for trainer.precision=16 in NeMo 1.18 here https://github.com/NVIDIA/NeMo/pull/6543.


          @anilozlu Changing the check from an int to a string is not recommended.
          The error is correct in pointing out that `megatron_amp_O2` will not work
          with `bf16`.'
        updatedAt: '2023-05-03T03:27:34.598Z'
      numEdits: 1
      reactions: []
    id: 6451d4575fb40b9f50bed409
    type: comment
  author: MaximumEntropy
  content: 'Hi, since you are running on an A5000 GPU, can you please try to specify
    trainer.precision=bf16 when running megatron_gpt_eval.py? We have a fix for trainer.precision=16
    in NeMo 1.18 here https://github.com/NVIDIA/NeMo/pull/6543.


    @anilozlu Changing the check from an int to a string is not recommended. The error
    is correct in pointing out that `megatron_amp_O2` will not work with `bf16`.'
  created_at: 2023-05-03 02:26:15+00:00
  edited: true
  hidden: false
  id: 6451d4575fb40b9f50bed409
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f2fa0944081ed996c31f648eae065f32.svg
      fullname: Jaiyam Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dataplayer12
      type: user
    createdAt: '2023-05-08T16:22:14.000Z'
    data:
      edited: false
      editors:
      - dataplayer12
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f2fa0944081ed996c31f648eae065f32.svg
          fullname: Jaiyam Sharma
          isHf: false
          isPro: false
          name: dataplayer12
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;MaximumEntropy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MaximumEntropy\"\
          >@<span class=\"underline\">MaximumEntropy</span></a></span>\n\n\t</span></span>\
          \ this solves the problem.</p>\n"
        raw: Thanks @MaximumEntropy this solves the problem.
        updatedAt: '2023-05-08T16:22:14.198Z'
      numEdits: 0
      reactions: []
      relatedEventId: 645921b6f92601affa33686d
    id: 645921b6f92601affa33686c
    type: comment
  author: dataplayer12
  content: Thanks @MaximumEntropy this solves the problem.
  created_at: 2023-05-08 15:22:14+00:00
  edited: false
  hidden: false
  id: 645921b6f92601affa33686c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f2fa0944081ed996c31f648eae065f32.svg
      fullname: Jaiyam Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dataplayer12
      type: user
    createdAt: '2023-05-08T16:22:14.000Z'
    data:
      status: closed
    id: 645921b6f92601affa33686d
    type: status-change
  author: dataplayer12
  created_at: 2023-05-08 15:22:14+00:00
  id: 645921b6f92601affa33686d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: nvidia/GPT-2B-001
repo_type: model
status: closed
target_branch: null
title: Does not work with NeMo container
