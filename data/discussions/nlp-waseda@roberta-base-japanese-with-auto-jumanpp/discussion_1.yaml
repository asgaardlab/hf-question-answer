!!python/object:huggingface_hub.community.DiscussionWithDetails
author: KoichiYasuoka
conflicting_files: null
created_at: 2022-10-17 14:12:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
      fullname: Koichi Yasuoka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KoichiYasuoka
      type: user
    createdAt: '2022-10-17T15:12:07.000Z'
    data:
      edited: false
      editors:
      - KoichiYasuoka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
          fullname: Koichi Yasuoka
          isHf: false
          isPro: false
          name: KoichiYasuoka
          type: user
        html: "<p>Duplicated lines are observed in vocab.txt, and no \"##\" included.\
          \ I suspect vocab.txt broken, and it can be fixed with:</p>\n<pre><code>from\
          \ transformers import AlbertTokenizer\nfrom transformers.utils import cached_file\n\
          tkz=AlbertTokenizer(cached_file(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
          ,\"spiece.model\"))\nwith open(\"vocab.txt\",\"w\",encoding=\"utf-8\") as\
          \ w:\n  print(\"\\n\".join((\"##\"+x).replace(\"##\\u2581\",\"\").replace(\"\
          ##[\",\"[\").replace(\"##&lt;\",\"&lt;\") for x in tkz.convert_ids_to_tokens(list(range(len(tkz))))),file=w)\n\
          </code></pre>\n"
        raw: "Duplicated lines are observed in vocab.txt, and no \"##\" included.\
          \ I suspect vocab.txt broken, and it can be fixed with:\r\n\r\n```\r\nfrom\
          \ transformers import AlbertTokenizer\r\nfrom transformers.utils import\
          \ cached_file\r\ntkz=AlbertTokenizer(cached_file(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
          ,\"spiece.model\"))\r\nwith open(\"vocab.txt\",\"w\",encoding=\"utf-8\"\
          ) as w:\r\n  print(\"\\n\".join((\"##\"+x).replace(\"##\\u2581\",\"\").replace(\"\
          ##[\",\"[\").replace(\"##<\",\"<\") for x in tkz.convert_ids_to_tokens(list(range(len(tkz))))),file=w)\r\
          \n```"
        updatedAt: '2022-10-17T15:12:07.798Z'
      numEdits: 0
      reactions: []
    id: 634d70c7fc4873ed0d032fce
    type: comment
  author: KoichiYasuoka
  content: "Duplicated lines are observed in vocab.txt, and no \"##\" included. I\
    \ suspect vocab.txt broken, and it can be fixed with:\r\n\r\n```\r\nfrom transformers\
    \ import AlbertTokenizer\r\nfrom transformers.utils import cached_file\r\ntkz=AlbertTokenizer(cached_file(\"\
    nlp-waseda/roberta-base-japanese-with-auto-jumanpp\",\"spiece.model\"))\r\nwith\
    \ open(\"vocab.txt\",\"w\",encoding=\"utf-8\") as w:\r\n  print(\"\\n\".join((\"\
    ##\"+x).replace(\"##\\u2581\",\"\").replace(\"##[\",\"[\").replace(\"##<\",\"\
    <\") for x in tkz.convert_ids_to_tokens(list(range(len(tkz))))),file=w)\r\n```"
  created_at: 2022-10-17 14:12:07+00:00
  edited: false
  hidden: false
  id: 634d70c7fc4873ed0d032fce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650261345491-noauth.jpeg?w=200&h=200&f=face
      fullname: Hao Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: conan1024hao
      type: user
    createdAt: '2022-10-18T00:29:27.000Z'
    data:
      edited: false
      editors:
      - conan1024hao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650261345491-noauth.jpeg?w=200&h=200&f=face
          fullname: Hao Wang
          isHf: false
          isPro: false
          name: conan1024hao
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KoichiYasuoka&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KoichiYasuoka\"\
          >@<span class=\"underline\">KoichiYasuoka</span></a></span>\n\n\t</span></span>\
          \ Thank you Prof.Yasuoka, it's very helpful to us. We also found that with\
          \ WordPiece, this model with auto jumanpp can not perform well as the old\
          \ model in JGLUE tasks, so we will add sentencepiece for BertJapaneseTokenizer\
          \ as soon as possible.</p>\n"
        raw: '@KoichiYasuoka Thank you Prof.Yasuoka, it''s very helpful to us. We
          also found that with WordPiece, this model with auto jumanpp can not perform
          well as the old model in JGLUE tasks, so we will add sentencepiece for BertJapaneseTokenizer
          as soon as possible.'
        updatedAt: '2022-10-18T00:29:27.041Z'
      numEdits: 0
      reactions: []
    id: 634df367be5a827d48746929
    type: comment
  author: conan1024hao
  content: '@KoichiYasuoka Thank you Prof.Yasuoka, it''s very helpful to us. We also
    found that with WordPiece, this model with auto jumanpp can not perform well as
    the old model in JGLUE tasks, so we will add sentencepiece for BertJapaneseTokenizer
    as soon as possible.'
  created_at: 2022-10-17 23:29:27+00:00
  edited: false
  hidden: false
  id: 634df367be5a827d48746929
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
      fullname: Koichi Yasuoka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KoichiYasuoka
      type: user
    createdAt: '2022-10-18T03:56:58.000Z'
    data:
      edited: true
      editors:
      - KoichiYasuoka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
          fullname: Koichi Yasuoka
          isHf: false
          isPro: false
          name: KoichiYasuoka
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;conan1024hao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/conan1024hao\"\
          >@<span class=\"underline\">conan1024hao</span></a></span>\n\n\t</span></span>\
          \ for fixing vocab.txt. I've just confirmed that the script below work well\
          \ on Google Colaboratory:</p>\n<pre><code>!test -d jumanpp-2.0.0-rc3 ||\
          \ curl -L https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\
          \ | tar xJf -\n!test -x /usr/local/bin/jumanpp || ( mkdir jumanpp-2.0.0-rc3/build\
          \ &amp;&amp; cd jumanpp-2.0.0-rc3/build &amp;&amp; cmake .. -DCMAKE_BUILD_TYPE=Release\
          \ &amp;&amp; make install )\n!pip install transformers pyknp\nfrom transformers\
          \ import pipeline\nfmp=pipeline(\"fill-mask\",\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
          )\nprint(fmp(\"\u56FD\u5883\u306E[MASK]\u30C8\u30F3\u30CD\u30EB\u3092\u629C\
          \u3051\u308B\u3068\u96EA\u56FD\u3067\u3042\u3063\u305F\u3002\"))\n</code></pre>\n\
          <p>Thank you again and I'm looking forward to subword_tokenizer with sentencepiece.</p>\n"
        raw: "Thank you @conan1024hao for fixing vocab.txt. I've just confirmed that\
          \ the script below work well on Google Colaboratory:\n\n```\n!test -d jumanpp-2.0.0-rc3\
          \ || curl -L https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\
          \ | tar xJf -\n!test -x /usr/local/bin/jumanpp || ( mkdir jumanpp-2.0.0-rc3/build\
          \ && cd jumanpp-2.0.0-rc3/build && cmake .. -DCMAKE_BUILD_TYPE=Release &&\
          \ make install )\n!pip install transformers pyknp\nfrom transformers import\
          \ pipeline\nfmp=pipeline(\"fill-mask\",\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
          )\nprint(fmp(\"\u56FD\u5883\u306E[MASK]\u30C8\u30F3\u30CD\u30EB\u3092\u629C\
          \u3051\u308B\u3068\u96EA\u56FD\u3067\u3042\u3063\u305F\u3002\"))\n```\n\n\
          Thank you again and I'm looking forward to subword_tokenizer with sentencepiece."
        updatedAt: '2022-10-18T04:20:16.773Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - conan1024hao
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dkawahara
      relatedEventId: 634e240ad00bb5d92c3f3d1e
    id: 634e240ad00bb5d92c3f3d1d
    type: comment
  author: KoichiYasuoka
  content: "Thank you @conan1024hao for fixing vocab.txt. I've just confirmed that\
    \ the script below work well on Google Colaboratory:\n\n```\n!test -d jumanpp-2.0.0-rc3\
    \ || curl -L https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\
    \ | tar xJf -\n!test -x /usr/local/bin/jumanpp || ( mkdir jumanpp-2.0.0-rc3/build\
    \ && cd jumanpp-2.0.0-rc3/build && cmake .. -DCMAKE_BUILD_TYPE=Release && make\
    \ install )\n!pip install transformers pyknp\nfrom transformers import pipeline\n\
    fmp=pipeline(\"fill-mask\",\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
    )\nprint(fmp(\"\u56FD\u5883\u306E[MASK]\u30C8\u30F3\u30CD\u30EB\u3092\u629C\u3051\
    \u308B\u3068\u96EA\u56FD\u3067\u3042\u3063\u305F\u3002\"))\n```\n\nThank you again\
    \ and I'm looking forward to subword_tokenizer with sentencepiece."
  created_at: 2022-10-18 02:56:58+00:00
  edited: true
  hidden: false
  id: 634e240ad00bb5d92c3f3d1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
      fullname: Koichi Yasuoka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KoichiYasuoka
      type: user
    createdAt: '2022-10-18T03:56:58.000Z'
    data:
      status: closed
    id: 634e240ad00bb5d92c3f3d1e
    type: status-change
  author: KoichiYasuoka
  created_at: 2022-10-18 02:56:58+00:00
  id: 634e240ad00bb5d92c3f3d1e
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
      fullname: Koichi Yasuoka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KoichiYasuoka
      type: user
    createdAt: '2022-10-18T11:28:55.000Z'
    data:
      edited: true
      editors:
      - KoichiYasuoka
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617763868834-605090bead163858f01accfe.jpeg?w=200&h=200&f=face
          fullname: Koichi Yasuoka
          isHf: false
          isPro: false
          name: KoichiYasuoka
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;dkawahara&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/dkawahara\">@<span class=\"\
          underline\">dkawahara</span></a></span>\n\n\t</span></span> san, I've also\
          \ confirmed that the tentative script below work well:</p>\n<pre><code>!test\
          \ -d jumanpp-2.0.0-rc3 || curl -L https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\
          \ | tar xJf -\n!test -x /usr/local/bin/jumanpp || ( mkdir jumanpp-2.0.0-rc3/build\
          \ &amp;&amp; cd jumanpp-2.0.0-rc3/build &amp;&amp; cmake .. -DCMAKE_BUILD_TYPE=Release\
          \ &amp;&amp; make install )\n!pip install transformers pyknp sentencepiece\n\
          from transformers import pipeline,AlbertTokenizer\nfrom transformers.utils\
          \ import cached_file\nspm=AlbertTokenizer(cached_file(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
          ,\"spiece.model\"),keep_accents=True,do_lower_case=False)\nfmp=pipeline(\"\
          fill-mask\",\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\")\nfmp.tokenizer.subword_tokenizer.tokenize=lambda\
          \ x:[(\"##\"+t).replace(\"##[\",\"[\").replace(\"##&lt;\",\"&lt;\").replace(\"\
          ##\\u2581\",\"\") for t in spm.tokenize(x)]\nprint(fmp(\"\u56FD\u5883\u306E\
          [MASK]\u30C8\u30F3\u30CD\u30EB\u3092\u629C\u3051\u308B\u3068\u96EA\u56FD\
          \u3067\u3042\u3063\u305F\u3002\"))\n</code></pre>\n<p>to use sentencepiece\
          \ as <code>subword_tokenizer</code>. But it seems too tentative and needs\
          \ to be brushed up...</p>\n"
        raw: "@dkawahara san, I've also confirmed that the tentative script below\
          \ work well:\n\n```\n!test -d jumanpp-2.0.0-rc3 || curl -L https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\
          \ | tar xJf -\n!test -x /usr/local/bin/jumanpp || ( mkdir jumanpp-2.0.0-rc3/build\
          \ && cd jumanpp-2.0.0-rc3/build && cmake .. -DCMAKE_BUILD_TYPE=Release &&\
          \ make install )\n!pip install transformers pyknp sentencepiece\nfrom transformers\
          \ import pipeline,AlbertTokenizer\nfrom transformers.utils import cached_file\n\
          spm=AlbertTokenizer(cached_file(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
          ,\"spiece.model\"),keep_accents=True,do_lower_case=False)\nfmp=pipeline(\"\
          fill-mask\",\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\")\nfmp.tokenizer.subword_tokenizer.tokenize=lambda\
          \ x:[(\"##\"+t).replace(\"##[\",\"[\").replace(\"##<\",\"<\").replace(\"\
          ##\\u2581\",\"\") for t in spm.tokenize(x)]\nprint(fmp(\"\u56FD\u5883\u306E\
          [MASK]\u30C8\u30F3\u30CD\u30EB\u3092\u629C\u3051\u308B\u3068\u96EA\u56FD\
          \u3067\u3042\u3063\u305F\u3002\"))\n```\n\nto use sentencepiece as `subword_tokenizer`.\
          \ But it seems too tentative and needs to be brushed up..."
        updatedAt: '2022-10-18T11:52:42.957Z'
      numEdits: 3
      reactions: []
    id: 634e8df7d049354d7ee1ff2a
    type: comment
  author: KoichiYasuoka
  content: "@dkawahara san, I've also confirmed that the tentative script below work\
    \ well:\n\n```\n!test -d jumanpp-2.0.0-rc3 || curl -L https://github.com/ku-nlp/jumanpp/releases/download/v2.0.0-rc3/jumanpp-2.0.0-rc3.tar.xz\
    \ | tar xJf -\n!test -x /usr/local/bin/jumanpp || ( mkdir jumanpp-2.0.0-rc3/build\
    \ && cd jumanpp-2.0.0-rc3/build && cmake .. -DCMAKE_BUILD_TYPE=Release && make\
    \ install )\n!pip install transformers pyknp sentencepiece\nfrom transformers\
    \ import pipeline,AlbertTokenizer\nfrom transformers.utils import cached_file\n\
    spm=AlbertTokenizer(cached_file(\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\"\
    ,\"spiece.model\"),keep_accents=True,do_lower_case=False)\nfmp=pipeline(\"fill-mask\"\
    ,\"nlp-waseda/roberta-base-japanese-with-auto-jumanpp\")\nfmp.tokenizer.subword_tokenizer.tokenize=lambda\
    \ x:[(\"##\"+t).replace(\"##[\",\"[\").replace(\"##<\",\"<\").replace(\"##\\u2581\"\
    ,\"\") for t in spm.tokenize(x)]\nprint(fmp(\"\u56FD\u5883\u306E[MASK]\u30C8\u30F3\
    \u30CD\u30EB\u3092\u629C\u3051\u308B\u3068\u96EA\u56FD\u3067\u3042\u3063\u305F\
    \u3002\"))\n```\n\nto use sentencepiece as `subword_tokenizer`. But it seems too\
    \ tentative and needs to be brushed up..."
  created_at: 2022-10-18 10:28:55+00:00
  edited: true
  hidden: false
  id: 634e8df7d049354d7ee1ff2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650261345491-noauth.jpeg?w=200&h=200&f=face
      fullname: Hao Wang
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: conan1024hao
      type: user
    createdAt: '2022-10-21T14:39:24.000Z'
    data:
      edited: false
      editors:
      - conan1024hao
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1650261345491-noauth.jpeg?w=200&h=200&f=face
          fullname: Hao Wang
          isHf: false
          isPro: false
          name: conan1024hao
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;KoichiYasuoka&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/KoichiYasuoka\"\
          >@<span class=\"underline\">KoichiYasuoka</span></a></span>\n\n\t</span></span>\
          \ Hi Prof. Yasuoka, we have merged this PR: <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/transformers/pull/19769\">https://github.com/huggingface/transformers/pull/19769</a>.\
          \ Now sentencepiece can be used in BertJapaneseTokenizer, please have a\
          \ try.</p>\n"
        raw: '@KoichiYasuoka Hi Prof. Yasuoka, we have merged this PR: https://github.com/huggingface/transformers/pull/19769.
          Now sentencepiece can be used in BertJapaneseTokenizer, please have a try.'
        updatedAt: '2022-10-21T14:39:24.932Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KoichiYasuoka
    id: 6352af1cb3678a43742ef188
    type: comment
  author: conan1024hao
  content: '@KoichiYasuoka Hi Prof. Yasuoka, we have merged this PR: https://github.com/huggingface/transformers/pull/19769.
    Now sentencepiece can be used in BertJapaneseTokenizer, please have a try.'
  created_at: 2022-10-21 13:39:24+00:00
  edited: false
  hidden: false
  id: 6352af1cb3678a43742ef188
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: nlp-waseda/roberta-base-japanese-with-auto-jumanpp
repo_type: model
status: closed
target_branch: null
title: vocab.txt may be broken
