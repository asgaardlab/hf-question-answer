!!python/object:huggingface_hub.community.DiscussionWithDetails
author: renyanda
conflicting_files: null
created_at: 2023-05-31 10:27:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30a4d59261f83589e0f2a9126a7f3eee.svg
      fullname: renyanda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: renyanda
      type: user
    createdAt: '2023-05-31T11:27:43.000Z'
    data:
      edited: true
      editors:
      - renyanda
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30a4d59261f83589e0f2a9126a7f3eee.svg
          fullname: renyanda
          isHf: false
          isPro: false
          name: renyanda
          type: user
        html: "<p>There must be something wrong with my code as the loss is 0.0 at\
          \ epoch 0. I think there might be an issue with my dataset or the loss calculation\
          \ logic. I am entirely new to the LLM field. Is there anyone who could point\
          \ out the error</p>\n<p>my dataset</p>\n<pre><code class=\"language-python\"\
          >max_length = <span class=\"hljs-number\">256</span>\n    dataset = load_dataset(<span\
          \ class=\"hljs-string\">'tatsu-lab/alpaca'</span>).<span class=\"hljs-built_in\"\
          >map</span>(\n        <span class=\"hljs-keyword\">lambda</span> elem: {\n\
          \            <span class=\"hljs-string\">\"input_ids\"</span>: tokeniser.encode(\n\
          \                elem[<span class=\"hljs-string\">\"instruction\"</span>],\n\
          \                padding=<span class=\"hljs-string\">\"max_length\"</span>,\n\
          \                truncation=<span class=\"hljs-literal\">True</span>,\n\
          \                max_length=max_length\n            ),\n            <span\
          \ class=\"hljs-string\">\"label_ids\"</span>: tokeniser.encode(\n      \
          \          elem[<span class=\"hljs-string\">\"text\"</span>],\n        \
          \        padding=<span class=\"hljs-string\">\"max_length\"</span>,\n  \
          \              truncation=<span class=\"hljs-literal\">True</span>,\n  \
          \              max_length=max_length\n            ),\n            <span\
          \ class=\"hljs-comment\"># \"label\": elem[\"output\"],</span>\n       \
          \ }\n    )\n</code></pre>\n<p>The training code</p>\n<pre><code class=\"\
          language-python\">    trainer = Seq2SeqTrainer(\n        model=model,\n\
          \        train_dataset=dataset[<span class=\"hljs-string\">'train'</span>],\n\
          \        <span class=\"hljs-comment\"># eval_dataset=dataset['test'],</span>\n\
          \        args=training_args,\n        <span class=\"hljs-comment\"># data_collator=data_collator,</span>\n\
          \    )\n    trainer.train()\n</code></pre>\n<p>I also tried a modified Trainer\
          \ but the loss is still 0.0</p>\n<p>My trainer:</p>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">class</span> <span class=\"\
          hljs-title class_\">ModifiedTrainer</span>(<span class=\"hljs-title class_\
          \ inherited__\">Seq2SeqTrainer</span>):\n    <span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">compute_loss</span>(<span\
          \ class=\"hljs-params\">self, model, inputs, return_outputs=<span class=\"\
          hljs-literal\">False</span></span>):\n        <span class=\"hljs-keyword\"\
          >return</span> model(\n            input_ids=inputs[<span class=\"hljs-string\"\
          >\"input_ids\"</span>],\n            attention_mask=torch.ones_like(inputs[<span\
          \ class=\"hljs-string\">\"input_ids\"</span>]).<span class=\"hljs-built_in\"\
          >bool</span>(),\n            labels=inputs[<span class=\"hljs-string\">\"\
          labels\"</span>],\n        ).loss\n</code></pre>\n"
        raw: "There must be something wrong with my code as the loss is 0.0 at epoch\
          \ 0. I think there might be an issue with my dataset or the loss calculation\
          \ logic. I am entirely new to the LLM field. Is there anyone who could point\
          \ out the error\n\nmy dataset\n```python\nmax_length = 256\n    dataset\
          \ = load_dataset('tatsu-lab/alpaca').map(\n        lambda elem: {\n    \
          \        \"input_ids\": tokeniser.encode(\n                elem[\"instruction\"\
          ],\n                padding=\"max_length\",\n                truncation=True,\n\
          \                max_length=max_length\n            ),\n            \"label_ids\"\
          : tokeniser.encode(\n                elem[\"text\"],\n                padding=\"\
          max_length\",\n                truncation=True,\n                max_length=max_length\n\
          \            ),\n            # \"label\": elem[\"output\"],\n        }\n\
          \    )\n```\n\nThe training code\n```python\n    trainer = Seq2SeqTrainer(\n\
          \        model=model,\n        train_dataset=dataset['train'],\n       \
          \ # eval_dataset=dataset['test'],\n        args=training_args,\n       \
          \ # data_collator=data_collator,\n    )\n    trainer.train()\n```\n\nI also\
          \ tried a modified Trainer but the loss is still 0.0\n\nMy trainer:\n```python\n\
          class ModifiedTrainer(Seq2SeqTrainer):\n    def compute_loss(self, model,\
          \ inputs, return_outputs=False):\n        return model(\n            input_ids=inputs[\"\
          input_ids\"],\n            attention_mask=torch.ones_like(inputs[\"input_ids\"\
          ]).bool(),\n            labels=inputs[\"labels\"],\n        ).loss\n```"
        updatedAt: '2023-05-31T11:31:56.854Z'
      numEdits: 1
      reactions: []
    id: 64772f2f2573014e954496e4
    type: comment
  author: renyanda
  content: "There must be something wrong with my code as the loss is 0.0 at epoch\
    \ 0. I think there might be an issue with my dataset or the loss calculation logic.\
    \ I am entirely new to the LLM field. Is there anyone who could point out the\
    \ error\n\nmy dataset\n```python\nmax_length = 256\n    dataset = load_dataset('tatsu-lab/alpaca').map(\n\
    \        lambda elem: {\n            \"input_ids\": tokeniser.encode(\n      \
    \          elem[\"instruction\"],\n                padding=\"max_length\",\n \
    \               truncation=True,\n                max_length=max_length\n    \
    \        ),\n            \"label_ids\": tokeniser.encode(\n                elem[\"\
    text\"],\n                padding=\"max_length\",\n                truncation=True,\n\
    \                max_length=max_length\n            ),\n            # \"label\"\
    : elem[\"output\"],\n        }\n    )\n```\n\nThe training code\n```python\n \
    \   trainer = Seq2SeqTrainer(\n        model=model,\n        train_dataset=dataset['train'],\n\
    \        # eval_dataset=dataset['test'],\n        args=training_args,\n      \
    \  # data_collator=data_collator,\n    )\n    trainer.train()\n```\n\nI also tried\
    \ a modified Trainer but the loss is still 0.0\n\nMy trainer:\n```python\nclass\
    \ ModifiedTrainer(Seq2SeqTrainer):\n    def compute_loss(self, model, inputs,\
    \ return_outputs=False):\n        return model(\n            input_ids=inputs[\"\
    input_ids\"],\n            attention_mask=torch.ones_like(inputs[\"input_ids\"\
    ]).bool(),\n            labels=inputs[\"labels\"],\n        ).loss\n```"
  created_at: 2023-05-31 10:27:43+00:00
  edited: true
  hidden: false
  id: 64772f2f2573014e954496e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: bigscience/mt0-small
repo_type: model
status: open
target_branch: null
title: I am getting 0.0 loss value at the very first epoch of training this model
