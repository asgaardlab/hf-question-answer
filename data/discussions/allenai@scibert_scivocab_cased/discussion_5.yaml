!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FocacciaX
conflicting_files: null
created_at: 2023-11-02 14:54:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BmWxckBNBEdYKDEgB3YMp.jpeg?w=200&h=200&f=face
      fullname: FX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FocacciaX
      type: user
    createdAt: '2023-11-02T15:54:59.000Z'
    data:
      edited: true
      editors:
      - FocacciaX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47638729214668274
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BmWxckBNBEdYKDEgB3YMp.jpeg?w=200&h=200&f=face
          fullname: FX
          isHf: false
          isPro: false
          name: FocacciaX
          type: user
        html: "<p>Hi, I am using the tokenizer. but it behaves in a way that I don't\
          \ understand.</p>\n<p>In the example below, the last few words are about\
          \ an URL, which should not be put into different subwords. However, the\
          \ tokenizer chopped the URL.<br>I wonder if this is because I am not handlling\
          \ it correctly, or scibert behaves incorrectly, or it's unevitable problem\
          \ in bert tokenization.</p>\n<p>Thanks!</p>\n<pre><code class=\"language-md\"\
          >Sentence:\nMethods PSY expression correlation analysis An expression correlation\
          \ analysis was performed for PSY using the freely available Arabidopsis\
          \ co-expression tool (ACT) (<span class=\"hljs-strong\">__http://www.arabidopsis.leeds.ac.uk/__</span>)[31].\n\
          \nTokens:\n['[CLS]', 'methods', 'ps', '##y', 'expression', 'correlation',\
          \ 'analysis', 'an', 'expression', 'correlation', 'analysis', 'was', 'performed',\
          \ 'for', 'ps', '##y', 'using', 'the', 'freely', 'available', 'ar', '##abi',\
          \ '##do', '##ps', '##is', 'co', '-', 'expression', 'tool', '(', 'act', ')',\
          \ '(', <span class=\"hljs-strong\">__'http', ':', '/', '/', 'www', '.',\
          \ 'ar', '##abi', '##do', '##ps', '##is', '.', 'le', '##eds', '.', 'ac',\
          \ '.', 'uk', '/'__</span>, ')', '[', '31', ']', '.', '[SEP]']\n\nword<span\
          \ class=\"hljs-emphasis\">_ids:</span>\n<span class=\"hljs-emphasis\"> [None,\
          \ 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 15, 16, 17, 17,\
          \ 17, 17, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,\
          \ 32, 32, 32, 32, 33, 34, <span class=\"hljs-strong\">__34, 35, 36, 37,\
          \ 38, 39, 40, 41, 42, 43, 44__</span>, None}</span>\n</code></pre>\n<p>Corresponding\
          \ code:</p>\n<pre><code class=\"language-py\">tokenized_inputs = tokenizer(\n\
          \            sent,\n            padding=<span class=\"hljs-string\">\"max_length\"\
          </span>,\n            truncation=<span class=\"hljs-literal\">True</span>,\n\
          \            max_length=data_args_max_seq_length,\n            is_split_into_words=<span\
          \ class=\"hljs-literal\">False</span>,\n            return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>  <span class=\"hljs-comment\"># Return\
          \ PyTorch tensors</span>\n        )\n</code></pre>\n"
        raw: "Hi, I am using the tokenizer. but it behaves in a way that I don't understand.\n\
          \nIn the example below, the last few words are about an URL, which should\
          \ not be put into different subwords. However, the tokenizer chopped the\
          \ URL. \nI wonder if this is because I am not handlling it correctly, or\
          \ scibert behaves incorrectly, or it's unevitable problem in bert tokenization.\n\
          \nThanks!\n\n```md\nSentence:\nMethods PSY expression correlation analysis\
          \ An expression correlation analysis was performed for PSY using the freely\
          \ available Arabidopsis co-expression tool (ACT) (__http://www.arabidopsis.leeds.ac.uk/__)[31].\n\
          \nTokens:\n['[CLS]', 'methods', 'ps', '##y', 'expression', 'correlation',\
          \ 'analysis', 'an', 'expression', 'correlation', 'analysis', 'was', 'performed',\
          \ 'for', 'ps', '##y', 'using', 'the', 'freely', 'available', 'ar', '##abi',\
          \ '##do', '##ps', '##is', 'co', '-', 'expression', 'tool', '(', 'act', ')',\
          \ '(', __'http', ':', '/', '/', 'www', '.', 'ar', '##abi', '##do', '##ps',\
          \ '##is', '.', 'le', '##eds', '.', 'ac', '.', 'uk', '/'__, ')', '[', '31',\
          \ ']', '.', '[SEP]']\n\nword_ids:\n [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8,\
          \ 9, 10, 11, 12, 12, 13, 14, 15, 16, 17, 17, 17, 17, 17, 18, 19, 20, 21,\
          \ 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 32, 32, 32, 32, 33, 34, __34,\
          \ 35, 36, 37, 38, 39, 40, 41, 42, 43, 44__, None}\n``` \n\nCorresponding\
          \ code:\n```py\ntokenized_inputs = tokenizer(\n            sent,\n     \
          \       padding=\"max_length\",\n            truncation=True,\n        \
          \    max_length=data_args_max_seq_length,\n            is_split_into_words=False,\n\
          \            return_tensors=\"pt\"  # Return PyTorch tensors\n        )\n\
          ```\n"
        updatedAt: '2023-11-02T16:11:10.813Z'
      numEdits: 1
      reactions: []
    id: 6543c6537a23df0e63602df7
    type: comment
  author: FocacciaX
  content: "Hi, I am using the tokenizer. but it behaves in a way that I don't understand.\n\
    \nIn the example below, the last few words are about an URL, which should not\
    \ be put into different subwords. However, the tokenizer chopped the URL. \nI\
    \ wonder if this is because I am not handlling it correctly, or scibert behaves\
    \ incorrectly, or it's unevitable problem in bert tokenization.\n\nThanks!\n\n\
    ```md\nSentence:\nMethods PSY expression correlation analysis An expression correlation\
    \ analysis was performed for PSY using the freely available Arabidopsis co-expression\
    \ tool (ACT) (__http://www.arabidopsis.leeds.ac.uk/__)[31].\n\nTokens:\n['[CLS]',\
    \ 'methods', 'ps', '##y', 'expression', 'correlation', 'analysis', 'an', 'expression',\
    \ 'correlation', 'analysis', 'was', 'performed', 'for', 'ps', '##y', 'using',\
    \ 'the', 'freely', 'available', 'ar', '##abi', '##do', '##ps', '##is', 'co', '-',\
    \ 'expression', 'tool', '(', 'act', ')', '(', __'http', ':', '/', '/', 'www',\
    \ '.', 'ar', '##abi', '##do', '##ps', '##is', '.', 'le', '##eds', '.', 'ac', '.',\
    \ 'uk', '/'__, ')', '[', '31', ']', '.', '[SEP]']\n\nword_ids:\n [None, 0, 1,\
    \ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 15, 16, 17, 17, 17, 17, 17,\
    \ 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 32, 32, 32, 32,\
    \ 33, 34, __34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44__, None}\n``` \n\nCorresponding\
    \ code:\n```py\ntokenized_inputs = tokenizer(\n            sent,\n           \
    \ padding=\"max_length\",\n            truncation=True,\n            max_length=data_args_max_seq_length,\n\
    \            is_split_into_words=False,\n            return_tensors=\"pt\"  #\
    \ Return PyTorch tensors\n        )\n```\n"
  created_at: 2023-11-02 14:54:59+00:00
  edited: true
  hidden: false
  id: 6543c6537a23df0e63602df7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/BmWxckBNBEdYKDEgB3YMp.jpeg?w=200&h=200&f=face
      fullname: FX
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FocacciaX
      type: user
    createdAt: '2023-11-16T11:44:53.000Z'
    data:
      status: closed
    id: 655600b57f8195872ea6cb57
    type: status-change
  author: FocacciaX
  created_at: 2023-11-16 11:44:53+00:00
  id: 655600b57f8195872ea6cb57
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: allenai/scibert_scivocab_cased
repo_type: model
status: closed
target_branch: null
title: URL incorrectly handled in tokenization
