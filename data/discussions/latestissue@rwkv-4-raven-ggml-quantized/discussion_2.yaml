!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-07-06 19:11:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-06T20:11:19.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9182049036026001
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Can you make gptq versions? Even if you only make the 14b model
          i would really appreciate it! So i can use it with oobagooba text gen web
          ui</p>

          '
        raw: Can you make gptq versions? Even if you only make the 14b model i would
          really appreciate it! So i can use it with oobagooba text gen web ui
        updatedAt: '2023-07-06T20:11:19.694Z'
      numEdits: 0
      reactions: []
    id: 64a71fe7e34f5658789f7d10
    type: comment
  author: rombodawg
  content: Can you make gptq versions? Even if you only make the 14b model i would
    really appreciate it! So i can use it with oobagooba text gen web ui
  created_at: 2023-07-06 19:11:19+00:00
  edited: false
  hidden: false
  id: 64a71fe7e34f5658789f7d10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632c49cd25a261e762d08579/OU85fqBWknTuzQE3ODWMF.jpeg?w=200&h=200&f=face
      fullname: "Krystian W\u0119grzyniak"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: latestissue
      type: user
    createdAt: '2023-07-07T13:38:37.000Z'
    data:
      edited: false
      editors:
      - latestissue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9716792106628418
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/632c49cd25a261e762d08579/OU85fqBWknTuzQE3ODWMF.jpeg?w=200&h=200&f=face
          fullname: "Krystian W\u0119grzyniak"
          isHf: false
          isPro: false
          name: latestissue
          type: user
        html: '<p>As far as I know, there is no GPTQ implementation for RWKV as of
          yet, is there? If so, you can redirect me to the link, then sure, I''ll
          do it for the 14B and other sizes.</p>

          '
        raw: As far as I know, there is no GPTQ implementation for RWKV as of yet,
          is there? If so, you can redirect me to the link, then sure, I'll do it
          for the 14B and other sizes.
        updatedAt: '2023-07-07T13:38:37.784Z'
      numEdits: 0
      reactions: []
    id: 64a8155d5971e111e3f88bb0
    type: comment
  author: latestissue
  content: As far as I know, there is no GPTQ implementation for RWKV as of yet, is
    there? If so, you can redirect me to the link, then sure, I'll do it for the 14B
    and other sizes.
  created_at: 2023-07-07 12:38:37+00:00
  edited: false
  hidden: false
  id: 64a8155d5971e111e3f88bb0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: latestissue/rwkv-4-raven-ggml-quantized
repo_type: model
status: open
target_branch: null
title: GPTQ version?
