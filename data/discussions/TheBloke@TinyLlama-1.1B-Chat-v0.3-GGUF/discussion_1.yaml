!!python/object:huggingface_hub.community.DiscussionWithDetails
author: appvoid
conflicting_files: null
created_at: 2023-10-11 02:04:18+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
      fullname: appvoid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appvoid
      type: user
    createdAt: '2023-10-11T03:04:18.000Z'
    data:
      edited: true
      editors:
      - appvoid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.967268705368042
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
          fullname: appvoid
          isHf: false
          isPro: false
          name: appvoid
          type: user
        html: '<p>For the ones looking for the sweet spot, after some prompt engineering,
          I was able to get decent responses using <strong>q6_k</strong>.  Lowering
          the quality more than this results in poor performance. It seems like intelligence
          or the so-called internal world representation emerges on ~835MB.</p>

          <pre><code>What is the difference between a cow and a dog?


          A cow is a farm animal, while a dog is a member of the canine family.

          </code></pre>

          <p>I''m definitely using it for my future projects.</p>

          '
        raw: 'For the ones looking for the sweet spot, after some prompt engineering,
          I was able to get decent responses using **q6_k**.  Lowering the quality
          more than this results in poor performance. It seems like intelligence or
          the so-called internal world representation emerges on ~835MB.


          ```

          What is the difference between a cow and a dog?


          A cow is a farm animal, while a dog is a member of the canine family.

          ```


          I''m definitely using it for my future projects.'
        updatedAt: '2023-10-11T03:42:01.589Z'
      numEdits: 2
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - catblue44
      - count: 1
        reaction: "\U0001F92F"
        users:
        - LinkuStarto
    id: 652610b2774dcb910372405d
    type: comment
  author: appvoid
  content: 'For the ones looking for the sweet spot, after some prompt engineering,
    I was able to get decent responses using **q6_k**.  Lowering the quality more
    than this results in poor performance. It seems like intelligence or the so-called
    internal world representation emerges on ~835MB.


    ```

    What is the difference between a cow and a dog?


    A cow is a farm animal, while a dog is a member of the canine family.

    ```


    I''m definitely using it for my future projects.'
  created_at: 2023-10-11 02:04:18+00:00
  edited: true
  hidden: false
  id: 652610b2774dcb910372405d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d581a78a256b2a52e7fcef7e8272cb9.svg
      fullname: Peter Kotsch
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LinkuStarto
      type: user
    createdAt: '2023-12-19T22:47:11.000Z'
    data:
      edited: false
      editors:
      - LinkuStarto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7966111302375793
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d581a78a256b2a52e7fcef7e8272cb9.svg
          fullname: Peter Kotsch
          isHf: false
          isPro: false
          name: LinkuStarto
          type: user
        html: '<p>would you mind sharing your {system} prompt? I''m getting very inconsistent
          responses from the model and formatting seems correct here? ... Its always
          hit or miss for me.</p>

          <p>from llama_cpp import Llama<br>llm = Llama(model_path="models/tinyllama-1.1b-chat-v0.3.Q6_K.gguf",
          verbose=False)<br>prompt = "What is the difference between a cow and a dog?"<br>s_system
          = "The clear answer for this question would be:"<br>formatted_prompt = f"&lt;|im_start|&gt;user\n{prompt}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{s_system}"<br>stream
          = llm(formatted_prompt,<br>             top_k=50, repeat_penalty=1.1, top_p=0.9,
          max_tokens=64,<br>             stop=["", ". ",  "&lt;|im_end|&gt;"], echo=True,
          stream=True)</p>

          <p>for s in stream:<br>    print(s[''choices''][0][''text''], end='''')</p>

          '
        raw: "would you mind sharing your {system} prompt? I'm getting very inconsistent\
          \ responses from the model and formatting seems correct here? ... Its always\
          \ hit or miss for me.\n\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"\
          models/tinyllama-1.1b-chat-v0.3.Q6_K.gguf\", verbose=False)\nprompt = \"\
          What is the difference between a cow and a dog?\"\ns_system = \"The clear\
          \ answer for this question would be:\"\nformatted_prompt = f\"<|im_start|>user\\\
          n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{s_system}\"\nstream = llm(formatted_prompt,\n\
          \             top_k=50, repeat_penalty=1.1, top_p=0.9, max_tokens=64,\n\
          \             stop=[\"</s>\", \". \",  \"<|im_end|>\"], echo=True, stream=True)\n\
          \nfor s in stream:\n    print(s['choices'][0]['text'], end='')"
        updatedAt: '2023-12-19T22:47:11.126Z'
      numEdits: 0
      reactions: []
    id: 65821d6f60e64a15efedcc3d
    type: comment
  author: LinkuStarto
  content: "would you mind sharing your {system} prompt? I'm getting very inconsistent\
    \ responses from the model and formatting seems correct here? ... Its always hit\
    \ or miss for me.\n\nfrom llama_cpp import Llama\nllm = Llama(model_path=\"models/tinyllama-1.1b-chat-v0.3.Q6_K.gguf\"\
    , verbose=False)\nprompt = \"What is the difference between a cow and a dog?\"\
    \ns_system = \"The clear answer for this question would be:\"\nformatted_prompt\
    \ = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{s_system}\"\
    \nstream = llm(formatted_prompt,\n             top_k=50, repeat_penalty=1.1, top_p=0.9,\
    \ max_tokens=64,\n             stop=[\"</s>\", \". \",  \"<|im_end|>\"], echo=True,\
    \ stream=True)\n\nfor s in stream:\n    print(s['choices'][0]['text'], end='')"
  created_at: 2023-12-19 22:47:11+00:00
  edited: false
  hidden: false
  id: 65821d6f60e64a15efedcc3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
      fullname: appvoid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appvoid
      type: user
    createdAt: '2023-12-20T00:30:52.000Z'
    data:
      edited: false
      editors:
      - appvoid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9632790684700012
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a813dedbb9e28866a91b27/i2pGYzY1htiY-L3WFPSgl.jpeg?w=200&h=200&f=face
          fullname: appvoid
          isHf: false
          isPro: false
          name: appvoid
          type: user
        html: '<p>i cannot recall the prompt (probably didn''t use any anyway) but
          always use low temperature and play with top_p until you get something similar
          to what you expect</p>

          '
        raw: i cannot recall the prompt (probably didn't use any anyway) but always
          use low temperature and play with top_p until you get something similar
          to what you expect
        updatedAt: '2023-12-20T00:30:52.107Z'
      numEdits: 0
      reactions: []
    id: 658235bc75754a803e34571c
    type: comment
  author: appvoid
  content: i cannot recall the prompt (probably didn't use any anyway) but always
    use low temperature and play with top_p until you get something similar to what
    you expect
  created_at: 2023-12-20 00:30:52+00:00
  edited: false
  hidden: false
  id: 658235bc75754a803e34571c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF
repo_type: model
status: open
target_branch: null
title: Better than expected!
