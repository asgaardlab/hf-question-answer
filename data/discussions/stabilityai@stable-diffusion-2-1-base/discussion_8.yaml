!!python/object:huggingface_hub.community.DiscussionWithDetails
author: danaarad
conflicting_files: null
created_at: 2023-01-12 12:08:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3acd382b0d989cc3306c3865388307f.svg
      fullname: Dana Arad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danaarad
      type: user
    createdAt: '2023-01-12T12:08:10.000Z'
    data:
      edited: false
      editors:
      - danaarad
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3acd382b0d989cc3306c3865388307f.svg
          fullname: Dana Arad
          isHf: false
          isPro: false
          name: danaarad
          type: user
        html: '<p>Hi, Im having trouble finding a CLIPModel checkpoint (or just the
          CLIPVisionModel) that matches the CLIPTextModel used in this version. The
          open-clip library provides a different interface (not using the CLIPVisionModel
          class). Other CLIPModel  checkpoint do not match the projection dim of this
          version (1024, while other checkpoints here are 768 or 512).  Does anyone
          has a solution or can refer me to the correct checkpoint?<br>Thanks!</p>

          '
        raw: "Hi, Im having trouble finding a CLIPModel checkpoint (or just the CLIPVisionModel)\
          \ that matches the CLIPTextModel used in this version. The open-clip library\
          \ provides a different interface (not using the CLIPVisionModel class).\
          \ Other CLIPModel  checkpoint do not match the projection dim of this version\
          \ (1024, while other checkpoints here are 768 or 512).  Does anyone has\
          \ a solution or can refer me to the correct checkpoint?\r\nThanks!\r\n"
        updatedAt: '2023-01-12T12:08:10.436Z'
      numEdits: 0
      reactions: []
    id: 63bff82a177bc4d05e483017
    type: comment
  author: danaarad
  content: "Hi, Im having trouble finding a CLIPModel checkpoint (or just the CLIPVisionModel)\
    \ that matches the CLIPTextModel used in this version. The open-clip library provides\
    \ a different interface (not using the CLIPVisionModel class). Other CLIPModel\
    \  checkpoint do not match the projection dim of this version (1024, while other\
    \ checkpoints here are 768 or 512).  Does anyone has a solution or can refer me\
    \ to the correct checkpoint?\r\nThanks!\r\n"
  created_at: 2023-01-12 12:08:10+00:00
  edited: false
  hidden: false
  id: 63bff82a177bc4d05e483017
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0dfc57082335f8a0bb08464e6a1e5917.svg
      fullname: yongxing he
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lioo
      type: user
    createdAt: '2023-03-08T08:54:11.000Z'
    data:
      edited: false
      editors:
      - lioo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0dfc57082335f8a0bb08464e6a1e5917.svg
          fullname: yongxing he
          isHf: false
          isPro: false
          name: lioo
          type: user
        html: '<p>hello, do you get the clip vision model finally? I tested the laion/CLIP-ViT-H-14-laion2B-s32B-b79K
          model, but the result and parameters are not the same. I''m confused, which
          clip model is used for SD2/2-1?</p>

          '
        raw: hello, do you get the clip vision model finally? I tested the laion/CLIP-ViT-H-14-laion2B-s32B-b79K
          model, but the result and parameters are not the same. I'm confused, which
          clip model is used for SD2/2-1?
        updatedAt: '2023-03-08T08:54:11.314Z'
      numEdits: 0
      reactions: []
    id: 64084d33e6713eae2bde7083
    type: comment
  author: lioo
  content: hello, do you get the clip vision model finally? I tested the laion/CLIP-ViT-H-14-laion2B-s32B-b79K
    model, but the result and parameters are not the same. I'm confused, which clip
    model is used for SD2/2-1?
  created_at: 2023-03-08 08:54:11+00:00
  edited: false
  hidden: false
  id: 64084d33e6713eae2bde7083
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9e2ea9c034c39fa6de59789e90919a21.svg
      fullname: AWEI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WuYW
      type: user
    createdAt: '2023-09-23T05:29:19.000Z'
    data:
      edited: false
      editors:
      - WuYW
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.926364541053772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9e2ea9c034c39fa6de59789e90919a21.svg
          fullname: AWEI
          isHf: false
          isPro: false
          name: WuYW
          type: user
        html: '<p>Hi, do you figure it out?</p>

          '
        raw: Hi, do you figure it out?
        updatedAt: '2023-09-23T05:29:19.750Z'
      numEdits: 0
      reactions: []
    id: 650e77af544d7edd811afc80
    type: comment
  author: WuYW
  content: Hi, do you figure it out?
  created_at: 2023-09-23 04:29:19+00:00
  edited: false
  hidden: false
  id: 650e77af544d7edd811afc80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d3acd382b0d989cc3306c3865388307f.svg
      fullname: Dana Arad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: danaarad
      type: user
    createdAt: '2023-09-26T07:06:55.000Z'
    data:
      edited: false
      editors:
      - danaarad
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9724936485290527
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d3acd382b0d989cc3306c3865388307f.svg
          fullname: Dana Arad
          isHf: false
          isPro: false
          name: danaarad
          type: user
        html: '<p>Hi, didn''t figure it out, ended up using a previous SD version.
          If anyone has any input please share!</p>

          '
        raw: Hi, didn't figure it out, ended up using a previous SD version. If anyone
          has any input please share!
        updatedAt: '2023-09-26T07:06:55.471Z'
      numEdits: 0
      reactions: []
    id: 6512830f311a12ef0c2b5217
    type: comment
  author: danaarad
  content: Hi, didn't figure it out, ended up using a previous SD version. If anyone
    has any input please share!
  created_at: 2023-09-26 06:06:55+00:00
  edited: false
  hidden: false
  id: 6512830f311a12ef0c2b5217
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: stabilityai/stable-diffusion-2-1-base
repo_type: model
status: open
target_branch: null
title: Loading the CLIPModel (or CLIPVisionModel) that matches this checkpoint
