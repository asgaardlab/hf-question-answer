!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Pi3141
conflicting_files: null
created_at: 2023-03-22 15:00:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-03-22T16:00:53.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>You''re just git cloning Llama and then quantizing it with Alpaca.cpp.
          the quantization script is basically the same for both alpaca and llama.
          And I don''t see any part where you merge the Lora weights with the llama
          model. </p>

          '
        raw: 'You''re just git cloning Llama and then quantizing it with Alpaca.cpp.
          the quantization script is basically the same for both alpaca and llama.
          And I don''t see any part where you merge the Lora weights with the llama
          model. '
        updatedAt: '2023-03-22T16:00:53.903Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wassname
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - shafiqalibhai
    id: 641b26351911d3be6743e384
    type: comment
  author: Pi3141
  content: 'You''re just git cloning Llama and then quantizing it with Alpaca.cpp.
    the quantization script is basically the same for both alpaca and llama. And I
    don''t see any part where you merge the Lora weights with the llama model. '
  created_at: 2023-03-22 15:00:53+00:00
  edited: false
  hidden: false
  id: 641b26351911d3be6743e384
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668859453057-63188acc57a5f5d7ce205a39.png?w=200&h=200&f=face
      fullname: fanghui
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: xfh
      type: user
    createdAt: '2023-03-23T00:00:59.000Z'
    data:
      edited: false
      editors:
      - xfh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668859453057-63188acc57a5f5d7ce205a39.png?w=200&h=200&f=face
          fullname: fanghui
          isHf: false
          isPro: false
          name: xfh
          type: user
        html: '<p>Yes, I did not know that where is alpaca.cpp 65B model weight, that
          is llama 65B q_4 weight .<br>You are right.<br>it run using alpaca.cpp</p>

          '
        raw: 'Yes, I did not know that where is alpaca.cpp 65B model weight, that
          is llama 65B q_4 weight .

          You are right.

          it run using alpaca.cpp'
        updatedAt: '2023-03-23T00:00:59.215Z'
      numEdits: 0
      reactions: []
    id: 641b96bbd42926275da8f013
    type: comment
  author: xfh
  content: 'Yes, I did not know that where is alpaca.cpp 65B model weight, that is
    llama 65B q_4 weight .

    You are right.

    it run using alpaca.cpp'
  created_at: 2023-03-22 23:00:59+00:00
  edited: false
  hidden: false
  id: 641b96bbd42926275da8f013
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-03-23T02:50:05.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<blockquote>

          <p>Yes, I did not know that where is alpaca.cpp 65B model weight, that is
          llama 65B q_4 weight<br>it run using alpaca.cpp</p>

          </blockquote>

          <p>Then what''s the difference between this and the original LLaMa 65B?</p>

          '
        raw: "> Yes, I did not know that where is alpaca.cpp 65B model weight, that\
          \ is llama 65B q_4 weight \n> it run using alpaca.cpp\n\nThen what's the\
          \ difference between this and the original LLaMa 65B?"
        updatedAt: '2023-03-23T02:50:05.709Z'
      numEdits: 0
      reactions: []
    id: 641bbe5d5d0c7772c60fe35f
    type: comment
  author: Pi3141
  content: "> Yes, I did not know that where is alpaca.cpp 65B model weight, that\
    \ is llama 65B q_4 weight \n> it run using alpaca.cpp\n\nThen what's the difference\
    \ between this and the original LLaMa 65B?"
  created_at: 2023-03-23 01:50:05+00:00
  edited: false
  hidden: false
  id: 641bbe5d5d0c7772c60fe35f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-31T03:44:46.000Z'
    data:
      edited: true
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>So here is my attempt to explain Alpaca vs LLaMA, and why it''s
          hard to find Alpaca.</p>

          <p>So LLaMA predicts the next word, it is not great at following instructions
          or chatting. If you chat with it, you will probably be disappointed.</p>

          <p> Once we teach LLaMA to follow instructions and chat it becomes Alpaca,
          which is more comparable to ChatGPT. Fun!</p>

          <p>There are two common ways to get to Alpaca:</p>

          <ul>

          <li>fine tuning LLaMA which results is a new model</li>

          <li><a rel="nofollow" href="https://arxiv.org/abs/2106.09685">LoRA</a> which
          results in a small add-on to the base LLaMA model (they can be merged if
          needed) <a rel="nofollow" href="https://github.com/huggingface/peft">usually
          people use the huggingface/peft library </a>. It needs to be combined with
          the LLaMA base model to work. On one hand the lora addon is small to upload
          and is good for training on smaller computers. But on the other it can be
          less compatible.</li>

          </ul>

          <p>Most people are using LoRA so that''s why it can be hard to find a full
          and compatible Alpaca model.</p>

          <p>Don''t worry this will all be clearer in the next few weeks as people
          make more formats available. And the Alpaca dataset is being cleaned which
          is actually leading to differen''t version of Alpaca, depending on if they
          use the original dataset, the old cleaned dataset, or the latest cleaned
          alpaca dataset.</p>

          '
        raw: "So here is my attempt to explain Alpaca vs LLaMA, and why it's hard\
          \ to find Alpaca.\n\nSo LLaMA predicts the next word, it is not great at\
          \ following instructions or chatting. If you chat with it, you will probably\
          \ be disappointed.\n\n Once we teach LLaMA to follow instructions and chat\
          \ it becomes Alpaca, which is more comparable to ChatGPT. Fun!\n\nThere\
          \ are two common ways to get to Alpaca:\n- fine tuning LLaMA which results\
          \ is a new model\n- [LoRA](https://arxiv.org/abs/2106.09685) which results\
          \ in a small add-on to the base LLaMA model (they can be merged if needed)\
          \ [usually people use the huggingface/peft library ](https://github.com/huggingface/peft).\
          \ It needs to be combined with the LLaMA base model to work. On one hand\
          \ the lora addon is small to upload and is good for training on smaller\
          \ computers. But on the other it can be less compatible.\n\nMost people\
          \ are using LoRA so that's why it can be hard to find a full and compatible\
          \ Alpaca model.\n\nDon't worry this will all be clearer in the next few\
          \ weeks as people make more formats available. And the Alpaca dataset is\
          \ being cleaned which is actually leading to differen't version of Alpaca,\
          \ depending on if they use the original dataset, the old cleaned dataset,\
          \ or the latest cleaned alpaca dataset."
        updatedAt: '2023-03-31T03:49:35.587Z'
      numEdits: 6
      reactions: []
    id: 6426572e03e27cca35e693f7
    type: comment
  author: wassname
  content: "So here is my attempt to explain Alpaca vs LLaMA, and why it's hard to\
    \ find Alpaca.\n\nSo LLaMA predicts the next word, it is not great at following\
    \ instructions or chatting. If you chat with it, you will probably be disappointed.\n\
    \n Once we teach LLaMA to follow instructions and chat it becomes Alpaca, which\
    \ is more comparable to ChatGPT. Fun!\n\nThere are two common ways to get to Alpaca:\n\
    - fine tuning LLaMA which results is a new model\n- [LoRA](https://arxiv.org/abs/2106.09685)\
    \ which results in a small add-on to the base LLaMA model (they can be merged\
    \ if needed) [usually people use the huggingface/peft library ](https://github.com/huggingface/peft).\
    \ It needs to be combined with the LLaMA base model to work. On one hand the lora\
    \ addon is small to upload and is good for training on smaller computers. But\
    \ on the other it can be less compatible.\n\nMost people are using LoRA so that's\
    \ why it can be hard to find a full and compatible Alpaca model.\n\nDon't worry\
    \ this will all be clearer in the next few weeks as people make more formats available.\
    \ And the Alpaca dataset is being cleaned which is actually leading to differen't\
    \ version of Alpaca, depending on if they use the original dataset, the old cleaned\
    \ dataset, or the latest cleaned alpaca dataset."
  created_at: 2023-03-31 02:44:46+00:00
  edited: true
  hidden: false
  id: 6426572e03e27cca35e693f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-03-31T03:57:00.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<blockquote>

          <p>So here is my attempt to explain Alpaca vs LLaMA, and why it''s hard
          to find Alpaca.</p>

          </blockquote>

          <p>I know what the difference between Alpaca and LLaMA is. But this one
          is the exact same model as LLaMA. By "this" I mean the model in this repo,
          and by "the model in this repo", I mean LLaMA. So what I''m really asking
          is "what''s the difference between LLaMA and LLaMA? And the answer is nothing.
          The model in <em>this</em> repo is LLaMA 65B. It isn''t Alpaca 65B at all.
          </p>

          <p>Also my question was a half-rhetorical question.</p>

          '
        raw: "> So here is my attempt to explain Alpaca vs LLaMA, and why it's hard\
          \ to find Alpaca.\n\nI know what the difference between Alpaca and LLaMA\
          \ is. But this one is the exact same model as LLaMA. By \"this\" I mean\
          \ the model in this repo, and by \"the model in this repo\", I mean LLaMA.\
          \ So what I'm really asking is \"what's the difference between LLaMA and\
          \ LLaMA? And the answer is nothing. The model in _this_ repo is LLaMA 65B.\
          \ It isn't Alpaca 65B at all. \n\nAlso my question was a half-rhetorical\
          \ question."
        updatedAt: '2023-03-31T03:57:00.425Z'
      numEdits: 0
      reactions: []
    id: 64265a0ceed6b41a5ab44af7
    type: comment
  author: Pi3141
  content: "> So here is my attempt to explain Alpaca vs LLaMA, and why it's hard\
    \ to find Alpaca.\n\nI know what the difference between Alpaca and LLaMA is. But\
    \ this one is the exact same model as LLaMA. By \"this\" I mean the model in this\
    \ repo, and by \"the model in this repo\", I mean LLaMA. So what I'm really asking\
    \ is \"what's the difference between LLaMA and LLaMA? And the answer is nothing.\
    \ The model in _this_ repo is LLaMA 65B. It isn't Alpaca 65B at all. \n\nAlso\
    \ my question was a half-rhetorical question."
  created_at: 2023-03-31 02:57:00+00:00
  edited: false
  hidden: false
  id: 64265a0ceed6b41a5ab44af7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-31T06:20:25.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>Oh right, I got you. </p>

          <p>Yeah xfh might want to change the name of the repo.</p>

          '
        raw: "Oh right, I got you. \n\nYeah xfh might want to change the name of the\
          \ repo."
        updatedAt: '2023-03-31T06:20:25.059Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Pi3141
        - shafiqalibhai
    id: 64267ba9b249517f9e84452c
    type: comment
  author: wassname
  content: "Oh right, I got you. \n\nYeah xfh might want to change the name of the\
    \ repo."
  created_at: 2023-03-31 05:20:25+00:00
  edited: false
  hidden: false
  id: 64267ba9b249517f9e84452c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: xfh/alpaca.cpp_65b_ggml
repo_type: model
status: open
target_branch: null
title: This isn't Alpaca 65B
