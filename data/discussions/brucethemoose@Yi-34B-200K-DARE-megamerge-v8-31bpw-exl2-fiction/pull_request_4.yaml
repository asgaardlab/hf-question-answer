!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: []
created_at: 2024-01-16 02:10:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T02:10:59.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9972096085548401
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Required once again, as it was in the v7.<br>And it works.</p>

          '
        raw: 'Required once again, as it was in the v7.

          And it works.'
        updatedAt: '2024-01-16T02:10:59.924Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 65a5e5b3ff5276e77b4b9624
    type: comment
  author: Nexesenex
  content: 'Required once again, as it was in the v7.

    And it works.'
  created_at: 2024-01-16 02:10:59+00:00
  edited: false
  hidden: false
  id: 65a5e5b3ff5276e77b4b9624
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T02:11:00.000Z'
    data:
      oid: f69d60db0b5e0ffa32b2a5111f3f7350aa935bf7
      parents:
      - 4e5986baa3a23a8664e6c9b3ead2d608da6b8111
      subject: Tokenizer of Yi Dare v5
    id: 65a5e5b40000000000000000
    type: commit
  author: Nexesenex
  created_at: 2024-01-16 02:11:00+00:00
  id: 65a5e5b40000000000000000
  oid: f69d60db0b5e0ffa32b2a5111f3f7350aa935bf7
  summary: Tokenizer of Yi Dare v5
  type: commit
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T02:15:45.000Z'
    data:
      status: merged
    id: 65a5e6d18fb86f6dd798dd15
    type: status-change
  author: brucethemoose
  created_at: 2024-01-16 02:15:45+00:00
  id: 65a5e6d18fb86f6dd798dd15
  new_status: merged
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T02:23:31.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9856767058372498
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Thanks.</p>

          <p>Note that the tokenizer from the v5 merge is just a copy of Yi''s tokenizer,
          IIRC. This was before I was even aware of mergekit''s union tokenizer merge.</p>

          <p>It might not work quite right, as some tokens (like the ChatML tokens)
          are missing.</p>

          '
        raw: 'Thanks.


          Note that the tokenizer from the v5 merge is just a copy of Yi''s tokenizer,
          IIRC. This was before I was even aware of mergekit''s union tokenizer merge.


          It might not work quite right, as some tokens (like the ChatML tokens) are
          missing.'
        updatedAt: '2024-01-16T02:25:50.932Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - Nexesenex
    id: 65a5e8a3d55515532bec17d1
    type: comment
  author: brucethemoose
  content: 'Thanks.


    Note that the tokenizer from the v5 merge is just a copy of Yi''s tokenizer, IIRC.
    This was before I was even aware of mergekit''s union tokenizer merge.


    It might not work quite right, as some tokens (like the ChatML tokens) are missing.'
  created_at: 2024-01-16 02:23:31+00:00
  edited: true
  hidden: false
  id: 65a5e8a3d55515532bec17d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T02:34:34.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8856500387191772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Well, you integrated Bagel, and it works on the same tokenizer despite
          all the prompt formats it offers.</p>

          <p>Here''s Jon''s views on ChatML :</p>

          <p>"ChatML (sort of)</p>

          <p>I don''t really understand the point of having special tokens for &lt;|im_start|&gt;
          and &lt;|im_end|&gt;, because in practice they just act as BOS and EOS tokens
          (but, please correct me if I''m wrong).</p>

          <p>So, instead of:</p>

          <p>{bos}&lt;|im_start|&gt;{role}<br>{text}<br>&lt;|im_end|&gt;{eos}</p>

          <p>I just changed it to:</p>

          <p>{bos}{role}<br>{text}<br>{eos}</p>

          <p>If you really want to use &lt;|im_start|&gt; and &lt;|im_end|&gt;, just
          update your tokenizer_config.json to use &lt;|im_start|&gt; instead of <s>
          and &lt;|im_end|&gt; instead of </s> and when tokenizing. And if you still
          don''t like what I''ve done to this chat-ml-ish format, feel free to cry
          into your pillow or fork the code and do a new fine-tune."</p>

          <p><a href="https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2#chatml-sort-of">https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2#chatml-sort-of</a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/PT9yQhsV9fJyDbukzpSVP.png"><img
          alt="Screenshot 2024-01-16 at 03-57-23 Text generation web UI.png" src="https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/PT9yQhsV9fJyDbukzpSVP.png"></a></p>

          <p>Also, a few tests I made. Is the calibration that you make in your quant
          possibly affecting so much the wikitext and ptb perplexity?</p>

          '
        raw: 'Well, you integrated Bagel, and it works on the same tokenizer despite
          all the prompt formats it offers.


          Here''s Jon''s views on ChatML :


          "ChatML (sort of)


          I don''t really understand the point of having special tokens for <|im_start|>
          and <|im_end|>, because in practice they just act as BOS and EOS tokens
          (but, please correct me if I''m wrong).


          So, instead of:


          {bos}<|im_start|>{role}

          {text}

          <|im_end|>{eos}


          I just changed it to:


          {bos}{role}

          {text}

          {eos}


          If you really want to use <|im_start|> and <|im_end|>, just update your
          tokenizer_config.json to use <|im_start|> instead of <s> and <|im_end|>
          instead of </s> and when tokenizing. And if you still don''t like what I''ve
          done to this chat-ml-ish format, feel free to cry into your pillow or fork
          the code and do a new fine-tune."


          https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2#chatml-sort-of



          ![Screenshot 2024-01-16 at 03-57-23 Text generation web UI.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/PT9yQhsV9fJyDbukzpSVP.png)



          Also, a few tests I made. Is the calibration that you make in your quant
          possibly affecting so much the wikitext and ptb perplexity?'
        updatedAt: '2024-01-16T02:59:20.531Z'
      numEdits: 2
      reactions: []
    id: 65a5eb3aee220af178d28541
    type: comment
  author: Nexesenex
  content: 'Well, you integrated Bagel, and it works on the same tokenizer despite
    all the prompt formats it offers.


    Here''s Jon''s views on ChatML :


    "ChatML (sort of)


    I don''t really understand the point of having special tokens for <|im_start|>
    and <|im_end|>, because in practice they just act as BOS and EOS tokens (but,
    please correct me if I''m wrong).


    So, instead of:


    {bos}<|im_start|>{role}

    {text}

    <|im_end|>{eos}


    I just changed it to:


    {bos}{role}

    {text}

    {eos}


    If you really want to use <|im_start|> and <|im_end|>, just update your tokenizer_config.json
    to use <|im_start|> instead of <s> and <|im_end|> instead of </s> and when tokenizing.
    And if you still don''t like what I''ve done to this chat-ml-ish format, feel
    free to cry into your pillow or fork the code and do a new fine-tune."


    https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2#chatml-sort-of



    ![Screenshot 2024-01-16 at 03-57-23 Text generation web UI.png](https://cdn-uploads.huggingface.co/production/uploads/6451b24dc5d273f95482bfa4/PT9yQhsV9fJyDbukzpSVP.png)



    Also, a few tests I made. Is the calibration that you make in your quant possibly
    affecting so much the wikitext and ptb perplexity?'
  created_at: 2024-01-16 02:34:34+00:00
  edited: true
  hidden: false
  id: 65a5eb3aee220af178d28541
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T03:40:21.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8566543459892273
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Hmmm, you are not the first person to report something off with
          the quantization. I test all the merges myself in ooba with 4-bit bitsandbytes,
          and the perplexity of the raw weights is good.</p>

          <p>I ran exllamav2''s own perplexity test in wikitext just to rule out ooba:
          </p>

          <blockquote>

          <p>v8-exl2-4bpw-fiction: 6.2723</p>

          </blockquote>

          <blockquote>

          <p>v8-exl2-31pw-fiction: 8203.8706</p>

          </blockquote>

          <blockquote>

          <p>v8-exl2-26pw-fiction: 77592.0066</p>

          </blockquote>

          <blockquote>

          <p>v7-exl2-31bpw-fiction: 9097.3480</p>

          </blockquote>

          <p>Oof. Yeah something is wrong with the lower quants, possibly all of them.</p>

          '
        raw: "Hmmm, you are not the first person to report something off with the\
          \ quantization. I test all the merges myself in ooba with 4-bit bitsandbytes,\
          \ and the perplexity of the raw weights is good.\n\nI ran exllamav2's own\
          \ perplexity test in wikitext just to rule out ooba: \n\n> v8-exl2-4bpw-fiction:\
          \ 6.2723\n\n> v8-exl2-31pw-fiction: 8203.8706\n\n> v8-exl2-26pw-fiction:\
          \ 77592.0066\n\n> v7-exl2-31bpw-fiction: 9097.3480\n\nOof. Yeah something\
          \ is wrong with the lower quants, possibly all of them."
        updatedAt: '2024-01-16T03:45:37.947Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a5faa547b88de066d08213
    type: comment
  author: brucethemoose
  content: "Hmmm, you are not the first person to report something off with the quantization.\
    \ I test all the merges myself in ooba with 4-bit bitsandbytes, and the perplexity\
    \ of the raw weights is good.\n\nI ran exllamav2's own perplexity test in wikitext\
    \ just to rule out ooba: \n\n> v8-exl2-4bpw-fiction: 6.2723\n\n> v8-exl2-31pw-fiction:\
    \ 8203.8706\n\n> v8-exl2-26pw-fiction: 77592.0066\n\n> v7-exl2-31bpw-fiction:\
    \ 9097.3480\n\nOof. Yeah something is wrong with the lower quants, possibly all\
    \ of them."
  created_at: 2024-01-16 03:40:21+00:00
  edited: true
  hidden: false
  id: 65a5faa547b88de066d08213
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T03:51:29.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6536601185798645
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Here''s perplexity on the actual .parquet file I quantized with:
          </p>

          <blockquote>

          <p>v8-exl2-31pw-fiction: 14.1167</p>

          </blockquote>

          <blockquote>

          <p>v8-exl2-26pw-fiction: 21.5868</p>

          </blockquote>

          <p>Still catastrophic, albeit not hilariously catastrophic like wikitext.</p>

          <p>I guess I will take the 3.1bpw and lower quants down? TBH I didn''t really
          notice they were broken because I have only been testing 4bpw locally.</p>

          '
        raw: "Here's perplexity on the actual .parquet file I quantized with: \n\n\
          > v8-exl2-31pw-fiction: 14.1167\n\n> v8-exl2-26pw-fiction: 21.5868\n\nStill\
          \ catastrophic, albeit not hilariously catastrophic like wikitext.\n\nI\
          \ guess I will take the 3.1bpw and lower quants down? TBH I didn't really\
          \ notice they were broken because I have only been testing 4bpw locally."
        updatedAt: '2024-01-16T03:53:15.117Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65a5fd412ca35d9cd8a8bf90
    type: comment
  author: brucethemoose
  content: "Here's perplexity on the actual .parquet file I quantized with: \n\n>\
    \ v8-exl2-31pw-fiction: 14.1167\n\n> v8-exl2-26pw-fiction: 21.5868\n\nStill catastrophic,\
    \ albeit not hilariously catastrophic like wikitext.\n\nI guess I will take the\
    \ 3.1bpw and lower quants down? TBH I didn't really notice they were broken because\
    \ I have only been testing 4bpw locally."
  created_at: 2024-01-16 03:51:29+00:00
  edited: true
  hidden: false
  id: 65a5fd412ca35d9cd8a8bf90
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T03:57:28.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6185662150382996
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;turboderp&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/turboderp\">@<span class=\"\
          underline\">turboderp</span></a></span>\n\n\t</span></span> Do you have\
          \ any idea what's going on here? The quantization commands I used are:</p>\n\
          <pre><code>python /home/alpha/AI/exllamav2/convert.py --in_dir /home/alpha/FastModels/v8/v8\
          \ -o /home/alpha/FastModels/scratch -om /home/alpha/FastModels/v8meas.json\
          \ --cal_dataset /home/alpha/Documents/stories.parquet -ml 32768 -mr 8 -ss\
          \ 4096 -b 4.0 -hb 6 -nr\n</code></pre>\n<pre><code>python /home/alpha/AI/exllamav2/convert.py\
          \ --in_dir /home/alpha/FastModels/v8/v8 -o /home/alpha/FastModels/scratch\
          \ -m /home/alpha/FastModels/v8meas.json --cal_dataset /home/alpha/Documents/stories.parquet\
          \ -l 12288 -r 26 -ml 32768 -mr 8 -ss 4096 -b 4.0 -hb 6 -cf /home/alpha/FastModels/v8-exl2-4bpw-fiction\
          \ -nr\n</code></pre>\n<p>The measurements file is here: <a href=\"https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8/blob/main/v8meas.json\"\
          >https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8/blob/main/v8meas.json</a></p>\n\
          <p>Anecdotally I noticed exllama seemed to be allocating 2.2bpw to most\
          \ everything except a few layers in the middle. </p>\n"
        raw: "@turboderp Do you have any idea what's going on here? The quantization\
          \ commands I used are:\n\n```\npython /home/alpha/AI/exllamav2/convert.py\
          \ --in_dir /home/alpha/FastModels/v8/v8 -o /home/alpha/FastModels/scratch\
          \ -om /home/alpha/FastModels/v8meas.json --cal_dataset /home/alpha/Documents/stories.parquet\
          \ -ml 32768 -mr 8 -ss 4096 -b 4.0 -hb 6 -nr\n```\n\n```\npython /home/alpha/AI/exllamav2/convert.py\
          \ --in_dir /home/alpha/FastModels/v8/v8 -o /home/alpha/FastModels/scratch\
          \ -m /home/alpha/FastModels/v8meas.json --cal_dataset /home/alpha/Documents/stories.parquet\
          \ -l 12288 -r 26 -ml 32768 -mr 8 -ss 4096 -b 4.0 -hb 6 -cf /home/alpha/FastModels/v8-exl2-4bpw-fiction\
          \ -nr\n```\n\nThe measurements file is here: https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8/blob/main/v8meas.json\n\
          \nAnecdotally I noticed exllama seemed to be allocating 2.2bpw to most everything\
          \ except a few layers in the middle. \n"
        updatedAt: '2024-01-16T03:57:28.889Z'
      numEdits: 0
      reactions: []
    id: 65a5fea85c86b8ac307d519c
    type: comment
  author: brucethemoose
  content: "@turboderp Do you have any idea what's going on here? The quantization\
    \ commands I used are:\n\n```\npython /home/alpha/AI/exllamav2/convert.py --in_dir\
    \ /home/alpha/FastModels/v8/v8 -o /home/alpha/FastModels/scratch -om /home/alpha/FastModels/v8meas.json\
    \ --cal_dataset /home/alpha/Documents/stories.parquet -ml 32768 -mr 8 -ss 4096\
    \ -b 4.0 -hb 6 -nr\n```\n\n```\npython /home/alpha/AI/exllamav2/convert.py --in_dir\
    \ /home/alpha/FastModels/v8/v8 -o /home/alpha/FastModels/scratch -m /home/alpha/FastModels/v8meas.json\
    \ --cal_dataset /home/alpha/Documents/stories.parquet -l 12288 -r 26 -ml 32768\
    \ -mr 8 -ss 4096 -b 4.0 -hb 6 -cf /home/alpha/FastModels/v8-exl2-4bpw-fiction\
    \ -nr\n```\n\nThe measurements file is here: https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8/blob/main/v8meas.json\n\
    \nAnecdotally I noticed exllama seemed to be allocating 2.2bpw to most everything\
    \ except a few layers in the middle. \n"
  created_at: 2024-01-16 03:57:28+00:00
  edited: false
  hidden: false
  id: 65a5fea85c86b8ac307d519c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2024-01-16T04:01:35.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9757485389709473
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>Glad my little tests could help. I wish Ooba included Hellaswag
          (I guess it''s not just about including the text file lol), as LlamaCPP
          does.</p>

          <p>For your defective quants, check LoneStriker''s, his 3bpw Exl2 quant
          works as intended.</p>

          <p>And otherwise, great jobs on your merges!</p>

          '
        raw: 'Glad my little tests could help. I wish Ooba included Hellaswag (I guess
          it''s not just about including the text file lol), as LlamaCPP does.


          For your defective quants, check LoneStriker''s, his 3bpw Exl2 quant works
          as intended.


          And otherwise, great jobs on your merges!'
        updatedAt: '2024-01-16T04:01:35.200Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 65a5ff9f0797583a68522df3
    type: comment
  author: Nexesenex
  content: 'Glad my little tests could help. I wish Ooba included Hellaswag (I guess
    it''s not just about including the text file lol), as LlamaCPP does.


    For your defective quants, check LoneStriker''s, his 3bpw Exl2 quant works as
    intended.


    And otherwise, great jobs on your merges!'
  created_at: 2024-01-16 04:01:35+00:00
  edited: false
  hidden: false
  id: 65a5ff9f0797583a68522df3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2024-01-18T06:41:46.000Z'
    data:
      edited: true
      editors:
      - TheYuriLover
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9431067705154419
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>\
          \ </p>\n<blockquote>\n<p>Oof. Yeah something is wrong with the lower quants,\
          \ possibly all of them.</p>\n</blockquote>\n<p>You should try to use a calibration\
          \ dataset composed of random tokens, I'm not joking it's probably the best\
          \ solution to this<br><a rel=\"nofollow\" href=\"https://github.com/ggerganov/llama.cpp/discussions/5006\"\
          >https://github.com/ggerganov/llama.cpp/discussions/5006</a></p>\n"
        raw: "@brucethemoose \n> Oof. Yeah something is wrong with the lower quants,\
          \ possibly all of them.\n\nYou should try to use a calibration dataset composed\
          \ of random tokens, I'm not joking it's probably the best solution to this\n\
          https://github.com/ggerganov/llama.cpp/discussions/5006\n"
        updatedAt: '2024-01-18T06:42:08.206Z'
      numEdits: 2
      reactions: []
    id: 65a8c82a55b4aa2cf430dee3
    type: comment
  author: TheYuriLover
  content: "@brucethemoose \n> Oof. Yeah something is wrong with the lower quants,\
    \ possibly all of them.\n\nYou should try to use a calibration dataset composed\
    \ of random tokens, I'm not joking it's probably the best solution to this\nhttps://github.com/ggerganov/llama.cpp/discussions/5006\n"
  created_at: 2024-01-18 06:41:46+00:00
  edited: true
  hidden: false
  id: 65a8c82a55b4aa2cf430dee3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-18T16:51:41.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9864406585693359
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah I am in that thread already, the one who tested with exl2.
          </p>

          <p>The jury is still out, but its very interesting. I will do more testing
          later.</p>

          '
        raw: "Yeah I am in that thread already, the one who tested with exl2. \n\n\
          The jury is still out, but its very interesting. I will do more testing\
          \ later."
        updatedAt: '2024-01-18T16:51:41.100Z'
      numEdits: 0
      reactions: []
    id: 65a9571d57f263e3d0d53227
    type: comment
  author: brucethemoose
  content: "Yeah I am in that thread already, the one who tested with exl2. \n\nThe\
    \ jury is still out, but its very interesting. I will do more testing later."
  created_at: 2024-01-18 16:51:41+00:00
  edited: false
  hidden: false
  id: 65a9571d57f263e3d0d53227
  type: comment
is_pull_request: true
merge_commit_oid: d9f5b5ca9a5f60655dcc9d6f7a0020158d221b82
num: 4
repo_id: brucethemoose/Yi-34B-200K-DARE-megamerge-v8-31bpw-exl2-fiction
repo_type: model
status: merged
target_branch: refs/heads/main
title: Tokenizer of Yi Dare v5
