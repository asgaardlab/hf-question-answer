!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zappa2005
conflicting_files: null
created_at: 2024-01-15 17:04:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-15T17:04:15.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659150242805481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>,\
          \ great stuff! Thanks for the models, I really like your Yi merges a lot.\
          \ As i only own a 4080, do you think it would be possible to do a smaller\
          \ quant also so that I can run with a bigger context?</p>\n"
        raw: Hi @brucethemoose, great stuff! Thanks for the models, I really like
          your Yi merges a lot. As i only own a 4080, do you think it would be possible
          to do a smaller quant also so that I can run with a bigger context?
        updatedAt: '2024-01-15T17:04:15.404Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 65a5658f01ed2b702d1d5122
    type: comment
  author: zappa2005
  content: Hi @brucethemoose, great stuff! Thanks for the models, I really like your
    Yi merges a lot. As i only own a 4080, do you think it would be possible to do
    a smaller quant also so that I can run with a bigger context?
  created_at: 2024-01-15 17:04:15+00:00
  edited: false
  hidden: false
  id: 65a5658f01ed2b702d1d5122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-15T17:54:42.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9002457857131958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Yeah, what''s a good size for 16GB? 2.63bpw, maybe?</p>

          '
        raw: Yeah, what's a good size for 16GB? 2.63bpw, maybe?
        updatedAt: '2024-01-15T17:54:42.256Z'
      numEdits: 0
      reactions: []
    id: 65a571627b020810944a9ed1
    type: comment
  author: brucethemoose
  content: Yeah, what's a good size for 16GB? 2.63bpw, maybe?
  created_at: 2024-01-15 17:54:42+00:00
  edited: false
  hidden: false
  id: 65a571627b020810944a9ed1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-15T18:55:23.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9787670373916626
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>I''m still trying to find the sweet spot between 2.4bpw and 2.63bpw.
          I am not sure. What do you think?</p>

          '
        raw: I'm still trying to find the sweet spot between 2.4bpw and 2.63bpw. I
          am not sure. What do you think?
        updatedAt: '2024-01-15T18:55:23.199Z'
      numEdits: 0
      reactions: []
    id: 65a57f9bf5142717792489f9
    type: comment
  author: zappa2005
  content: I'm still trying to find the sweet spot between 2.4bpw and 2.63bpw. I am
    not sure. What do you think?
  created_at: 2024-01-15 18:55:23+00:00
  edited: false
  hidden: false
  id: 65a57f9bf5142717792489f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-15T19:47:51.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9833869934082031
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Depends on how much context you want. I can upload 2.6 and 2.4,
          I will post the measurements file as well in case anyone wants to quant
          it themself.</p>

          '
        raw: 'Depends on how much context you want. I can upload 2.6 and 2.4, I will
          post the measurements file as well in case anyone wants to quant it themself.

          '
        updatedAt: '2024-01-15T19:47:51.522Z'
      numEdits: 0
      reactions: []
    id: 65a58be7b3dd88367f010140
    type: comment
  author: brucethemoose
  content: 'Depends on how much context you want. I can upload 2.6 and 2.4, I will
    post the measurements file as well in case anyone wants to quant it themself.

    '
  created_at: 2024-01-15 19:47:51+00:00
  edited: false
  hidden: false
  id: 65a58be7b3dd88367f010140
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-15T20:33:30.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46368739008903503
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Thank you!</p>

          '
        raw: Thank you!
        updatedAt: '2024-01-15T20:33:30.909Z'
      numEdits: 0
      reactions: []
    id: 65a5969aee220af178a8e208
    type: comment
  author: zappa2005
  content: Thank you!
  created_at: 2024-01-15 20:33:30+00:00
  edited: false
  hidden: false
  id: 65a5969aee220af178a8e208
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-15T23:17:34.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7233414053916931
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>2.6 and 2.4 are linked here: <a href="https://huggingface.co/collections/brucethemoose/most-recent-merge-65742644ca03b6c514afa204">https://huggingface.co/collections/brucethemoose/most-recent-merge-65742644ca03b6c514afa204</a></p>

          <p>Different sizes are not hard to make, though you probably don''t want
          to go smaller than 2.4bpw. See:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/5mEUaCTY6R4oRK65Q8E3M.png"><img
          alt="exl2-1.png" src="https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/5mEUaCTY6R4oRK65Q8E3M.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/Z1VSwN53nwgXlL952_7gy.png"><img
          alt="exl2-2.png" src="https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/Z1VSwN53nwgXlL952_7gy.png"></a></p>

          <p>Also TheBloke has just posted experimentals quantizations with a new
          GGUF format that uses profiling data like exllamav2: </p>

          <p><a href="https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF">https://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF</a></p>

          <p>This... might be very interesting, actually. llama.cpp supposedly has
          an 8-bit cache like exllama as well. I might post fiction oriented GGUFs
          depending on how well all that works.</p>

          '
        raw: "2.6 and 2.4 are linked here: https://huggingface.co/collections/brucethemoose/most-recent-merge-65742644ca03b6c514afa204\n\
          \nDifferent sizes are not hard to make, though you probably don't want to\
          \ go smaller than 2.4bpw. See:\n\n![exl2-1.png](https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/5mEUaCTY6R4oRK65Q8E3M.png)\n\
          \n![exl2-2.png](https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/Z1VSwN53nwgXlL952_7gy.png)\n\
          \nAlso TheBloke has just posted experimentals quantizations with a new GGUF\
          \ format that uses profiling data like exllamav2: \n\nhttps://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF\n\
          \nThis... might be very interesting, actually. llama.cpp supposedly has\
          \ an 8-bit cache like exllama as well. I might post fiction oriented GGUFs\
          \ depending on how well all that works."
        updatedAt: '2024-01-15T23:22:29.536Z'
      numEdits: 5
      reactions: []
    id: 65a5bd0ebcc380eddda06261
    type: comment
  author: brucethemoose
  content: "2.6 and 2.4 are linked here: https://huggingface.co/collections/brucethemoose/most-recent-merge-65742644ca03b6c514afa204\n\
    \nDifferent sizes are not hard to make, though you probably don't want to go smaller\
    \ than 2.4bpw. See:\n\n![exl2-1.png](https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/5mEUaCTY6R4oRK65Q8E3M.png)\n\
    \n![exl2-2.png](https://cdn-uploads.huggingface.co/production/uploads/638a3a2efe3185ae7324f1f6/Z1VSwN53nwgXlL952_7gy.png)\n\
    \nAlso TheBloke has just posted experimentals quantizations with a new GGUF format\
    \ that uses profiling data like exllamav2: \n\nhttps://huggingface.co/TheBloke/Yi-34B-200K-DARE-megamerge-v8-GGUF\n\
    \nThis... might be very interesting, actually. llama.cpp supposedly has an 8-bit\
    \ cache like exllama as well. I might post fiction oriented GGUFs depending on\
    \ how well all that works."
  created_at: 2024-01-15 23:17:34+00:00
  edited: true
  hidden: false
  id: 65a5bd0ebcc380eddda06261
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-15T23:27:10.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761551022529602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Thanks for your time, work and explanations. It really helps! I''ll
          also check the GGUF and maybe run the same story to compare.</p>

          <p>3.5 to 4.0 bpw seems to be a sweet spot fro Mixtral 8x7, but is it the
          same for Yi? I mean, can this be generalized?</p>

          '
        raw: 'Thanks for your time, work and explanations. It really helps! I''ll
          also check the GGUF and maybe run the same story to compare.


          3.5 to 4.0 bpw seems to be a sweet spot fro Mixtral 8x7, but is it the same
          for Yi? I mean, can this be generalized?'
        updatedAt: '2024-01-15T23:27:10.468Z'
      numEdits: 0
      reactions: []
    id: 65a5bf4ed81b6fa6c890d404
    type: comment
  author: zappa2005
  content: 'Thanks for your time, work and explanations. It really helps! I''ll also
    check the GGUF and maybe run the same story to compare.


    3.5 to 4.0 bpw seems to be a sweet spot fro Mixtral 8x7, but is it the same for
    Yi? I mean, can this be generalized?'
  created_at: 2024-01-15 23:27:10+00:00
  edited: false
  hidden: false
  id: 65a5bf4ed81b6fa6c890d404
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-15T23:41:46.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9725897908210754
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,
          I could not get it to work. I produced gibberish in any case, even directly
          in oogabooga chat. I''ll try the 2.4bpw tomorrow.</p>

          <p>The new 2bit quants are really interesting, but as these require new
          llama from Jan 4th, I guess I have to wait until oogabooga updates its repo
          with the new requirement. I have no idea how to update it myself :-)</p>

          '
        raw: 'I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,
          I could not get it to work. I produced gibberish in any case, even directly
          in oogabooga chat. I''ll try the 2.4bpw tomorrow.


          The new 2bit quants are really interesting, but as these require new llama
          from Jan 4th, I guess I have to wait until oogabooga updates its repo with
          the new requirement. I have no idea how to update it myself :-)'
        updatedAt: '2024-01-15T23:41:46.799Z'
      numEdits: 0
      reactions: []
    id: 65a5c2ba93d165b6e5de29cf
    type: comment
  author: zappa2005
  content: 'I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,
    I could not get it to work. I produced gibberish in any case, even directly in
    oogabooga chat. I''ll try the 2.4bpw tomorrow.


    The new 2bit quants are really interesting, but as these require new llama from
    Jan 4th, I guess I have to wait until oogabooga updates its repo with the new
    requirement. I have no idea how to update it myself :-)'
  created_at: 2024-01-15 23:41:46+00:00
  edited: false
  hidden: false
  id: 65a5c2ba93d165b6e5de29cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-15T23:43:54.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9821557998657227
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p><em>shrug</em></p>

          <p>The impact of quantization also varies on the task, more than that graph
          would suggest. I''ve seen llama.cpp''s Q6_K be unusable compared to Q8_0
          for some business uses.</p>

          <p>However I wouldn''t be surprised if 3.5bpw-4bpw is a sweetspot for Yi
          as well.</p>

          '
        raw: '*shrug*


          The impact of quantization also varies on the task, more than that graph
          would suggest. I''ve seen llama.cpp''s Q6_K be unusable compared to Q8_0
          for some business uses.


          However I wouldn''t be surprised if 3.5bpw-4bpw is a sweetspot for Yi as
          well.


          '
        updatedAt: '2024-01-15T23:43:54.026Z'
      numEdits: 0
      reactions: []
    id: 65a5c33a47b88de066bc7c0e
    type: comment
  author: brucethemoose
  content: '*shrug*


    The impact of quantization also varies on the task, more than that graph would
    suggest. I''ve seen llama.cpp''s Q6_K be unusable compared to Q8_0 for some business
    uses.


    However I wouldn''t be surprised if 3.5bpw-4bpw is a sweetspot for Yi as well.


    '
  created_at: 2024-01-15 23:43:54+00:00
  edited: false
  hidden: false
  id: 65a5c33a47b88de066bc7c0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-15T23:45:23.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9576596021652222
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,
          I could not get it to work. I produced gibberish in any case, even directly
          in oogabooga chat. I''ll try the 2.4bpw tomorrow.</p>

          <p>The new 2bit quants are really interesting, but as these require new
          llama from Jan 4th, I guess I have to wait until oogabooga updates its repo
          with the new requirement. I have no idea how to update it myself :-)</p>

          </blockquote>

          <p>It should work if you install llama-cpp-python from github (which is
          what text-generation-ui uses internally).</p>

          <p>Alternatively, I would just recommend kobold.cpp. Its faster than ooba,
          with better prompt caching and support for things like dynatemp as well.
          I''d be using it if I wasn''t stuck on exllama (for the moment).</p>

          <p>IDK why its busted, lemme check if it loads.</p>

          '
        raw: "> I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,\
          \ I could not get it to work. I produced gibberish in any case, even directly\
          \ in oogabooga chat. I'll try the 2.4bpw tomorrow.\n> \n> The new 2bit quants\
          \ are really interesting, but as these require new llama from Jan 4th, I\
          \ guess I have to wait until oogabooga updates its repo with the new requirement.\
          \ I have no idea how to update it myself :-)\n\nIt should work if you install\
          \ llama-cpp-python from github (which is what text-generation-ui uses internally).\n\
          \nAlternatively, I would just recommend kobold.cpp. Its faster than ooba,\
          \ with better prompt caching and support for things like dynatemp as well.\
          \ I'd be using it if I wasn't stuck on exllama (for the moment).\n\nIDK\
          \ why its busted, lemme check if it loads."
        updatedAt: '2024-01-15T23:45:49.414Z'
      numEdits: 1
      reactions: []
    id: 65a5c393f2e68a4148304e8c
    type: comment
  author: brucethemoose
  content: "> I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,\
    \ I could not get it to work. I produced gibberish in any case, even directly\
    \ in oogabooga chat. I'll try the 2.4bpw tomorrow.\n> \n> The new 2bit quants\
    \ are really interesting, but as these require new llama from Jan 4th, I guess\
    \ I have to wait until oogabooga updates its repo with the new requirement. I\
    \ have no idea how to update it myself :-)\n\nIt should work if you install llama-cpp-python\
    \ from github (which is what text-generation-ui uses internally).\n\nAlternatively,\
    \ I would just recommend kobold.cpp. Its faster than ooba, with better prompt\
    \ caching and support for things like dynatemp as well. I'd be using it if I wasn't\
    \ stuck on exllama (for the moment).\n\nIDK why its busted, lemme check if it\
    \ loads."
  created_at: 2024-01-15 23:45:23+00:00
  edited: true
  hidden: false
  id: 65a5c393f2e68a4148304e8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-15T23:54:02.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9667581915855408
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,
          I could not get it to work. I produced gibberish in any case, even directly
          in oogabooga chat. I''ll try the 2.4bpw tomorrow.</p>

          <p>The new 2bit quants are really interesting, but as these require new
          llama from Jan 4th, I guess I have to wait until oogabooga updates its repo
          with the new requirement. I have no idea how to update it myself :-)</p>

          </blockquote>

          <p>So the 2.6bpw works for me. I dunno how "smart" it is, but it referenced
          some details from a pretty huge context with coherent english, so I think
          it''s working.</p>

          <p>I used the absolute newest verison of exllamav2 to quantize, so maybe
          ooba is not up to date enough?</p>

          '
        raw: "> I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,\
          \ I could not get it to work. I produced gibberish in any case, even directly\
          \ in oogabooga chat. I'll try the 2.4bpw tomorrow.\n> \n> The new 2bit quants\
          \ are really interesting, but as these require new llama from Jan 4th, I\
          \ guess I have to wait until oogabooga updates its repo with the new requirement.\
          \ I have no idea how to update it myself :-)\n\nSo the 2.6bpw works for\
          \ me. I dunno how \"smart\" it is, but it referenced some details from a\
          \ pretty huge context with coherent english, so I think it's working.\n\n\
          I used the absolute newest verison of exllamav2 to quantize, so maybe ooba\
          \ is not up to date enough?"
        updatedAt: '2024-01-15T23:54:02.475Z'
      numEdits: 0
      reactions: []
    id: 65a5c59af2e68a414831126e
    type: comment
  author: brucethemoose
  content: "> I tried the 2.6bpw, it loads with 22k (8bit cache) in my 4080. However,\
    \ I could not get it to work. I produced gibberish in any case, even directly\
    \ in oogabooga chat. I'll try the 2.4bpw tomorrow.\n> \n> The new 2bit quants\
    \ are really interesting, but as these require new llama from Jan 4th, I guess\
    \ I have to wait until oogabooga updates its repo with the new requirement. I\
    \ have no idea how to update it myself :-)\n\nSo the 2.6bpw works for me. I dunno\
    \ how \"smart\" it is, but it referenced some details from a pretty huge context\
    \ with coherent english, so I think it's working.\n\nI used the absolute newest\
    \ verison of exllamav2 to quantize, so maybe ooba is not up to date enough?"
  created_at: 2024-01-15 23:54:02+00:00
  edited: false
  hidden: false
  id: 65a5c59af2e68a414831126e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-16T04:12:21.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8650783896446228
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>So on second thought. you were right, something is busted with these
          quantizations, see: </p>

          <p><a href="https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8-31bpw-exl2-fiction/discussions/4#65a5ff9f0797583a68522df3">https://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8-31bpw-exl2-fiction/discussions/4#65a5ff9f0797583a68522df3</a></p>

          <p>I am taking them down or adding a warning for now.</p>

          '
        raw: "So on second thought. you were right, something is busted with these\
          \ quantizations, see: \n\nhttps://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8-31bpw-exl2-fiction/discussions/4#65a5ff9f0797583a68522df3\n\
          \nI am taking them down or adding a warning for now."
        updatedAt: '2024-01-16T04:25:16.818Z'
      numEdits: 2
      reactions: []
    id: 65a60225ff5276e77b53a910
    type: comment
  author: brucethemoose
  content: "So on second thought. you were right, something is busted with these quantizations,\
    \ see: \n\nhttps://huggingface.co/brucethemoose/Yi-34B-200K-DARE-megamerge-v8-31bpw-exl2-fiction/discussions/4#65a5ff9f0797583a68522df3\n\
    \nI am taking them down or adding a warning for now."
  created_at: 2024-01-16 04:12:21+00:00
  edited: true
  hidden: false
  id: 65a60225ff5276e77b53a910
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
      fullname: Zappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zappa2005
      type: user
    createdAt: '2024-01-16T09:17:23.000Z'
    data:
      edited: false
      editors:
      - zappa2005
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9091157913208008
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/302fb28d27b64624c37c6755526f9d6f.svg
          fullname: Zappa
          isHf: false
          isPro: false
          name: zappa2005
          type: user
        html: '<p>Thanks for your insights! I''ll compare the TheBloke quants (probably
          the IQ2_XS to check ooga for the new method and Q2_K_S).</p>

          <p>I also checked the requirements.txt of ooga to see if I could easily
          use a newer llama-cpp-python myself, but it is quite detailed with precompiled
          wheels for all kinds of hardware accelerations.</p>

          <p>I''ll check kobold.cpp, thanks for the hint.</p>

          '
        raw: 'Thanks for your insights! I''ll compare the TheBloke quants (probably
          the IQ2_XS to check ooga for the new method and Q2_K_S).


          I also checked the requirements.txt of ooga to see if I could easily use
          a newer llama-cpp-python myself, but it is quite detailed with precompiled
          wheels for all kinds of hardware accelerations.


          I''ll check kobold.cpp, thanks for the hint.'
        updatedAt: '2024-01-16T09:17:23.252Z'
      numEdits: 0
      reactions: []
    id: 65a649a38a0485c12d09276a
    type: comment
  author: zappa2005
  content: 'Thanks for your insights! I''ll compare the TheBloke quants (probably
    the IQ2_XS to check ooga for the new method and Q2_K_S).


    I also checked the requirements.txt of ooga to see if I could easily use a newer
    llama-cpp-python myself, but it is quite detailed with precompiled wheels for
    all kinds of hardware accelerations.


    I''ll check kobold.cpp, thanks for the hint.'
  created_at: 2024-01-16 09:17:23+00:00
  edited: false
  hidden: false
  id: 65a649a38a0485c12d09276a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: brucethemoose/Yi-34B-200K-DARE-megamerge-v8-31bpw-exl2-fiction
repo_type: model
status: open
target_branch: null
title: Smaller quant
