!!python/object:huggingface_hub.community.DiscussionWithDetails
author: milyiyo
conflicting_files: null
created_at: 2023-03-13 02:14:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623782442046-noauth.png?w=200&h=200&f=face
      fullname: Alberto Carmona Barthelemy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: milyiyo
      type: user
    createdAt: '2023-03-13T03:14:58.000Z'
    data:
      edited: false
      editors:
      - milyiyo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1623782442046-noauth.png?w=200&h=200&f=face
          fullname: Alberto Carmona Barthelemy
          isHf: false
          isPro: false
          name: milyiyo
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;maderix&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/maderix\">@<span class=\"\
          underline\">maderix</span></a></span>\n\n\t</span></span> :)</p>\n<p>Do\
          \ you have any Colab showing how to use these models or a Space in HuggingFace?</p>\n"
        raw: "Hi @maderix :)\r\n\r\nDo you have any Colab showing how to use these\
          \ models or a Space in HuggingFace?\r\n\r\n"
        updatedAt: '2023-03-13T03:14:58.337Z'
      numEdits: 0
      reactions: []
    id: 640e9532474aa6f895595b9f
    type: comment
  author: milyiyo
  content: "Hi @maderix :)\r\n\r\nDo you have any Colab showing how to use these models\
    \ or a Space in HuggingFace?\r\n\r\n"
  created_at: 2023-03-13 02:14:58+00:00
  edited: false
  hidden: false
  id: 640e9532474aa6f895595b9f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651644128941-noauth.jpeg?w=200&h=200&f=face
      fullname: Manjeet Singh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: maderix
      type: user
    createdAt: '2023-03-14T16:44:38.000Z'
    data:
      edited: false
      editors:
      - maderix
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651644128941-noauth.jpeg?w=200&h=200&f=face
          fullname: Manjeet Singh
          isHf: false
          isPro: false
          name: maderix
          type: user
        html: '<p>You can follow the steps here: <a rel="nofollow" href="https://github.com/qwopqwop200/GPTQ-for-LLaMa">https://github.com/qwopqwop200/GPTQ-for-LLaMa</a></p>

          '
        raw: 'You can follow the steps here: https://github.com/qwopqwop200/GPTQ-for-LLaMa'
        updatedAt: '2023-03-14T16:44:38.589Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - milyiyo
        - johnrobinsn
    id: 6410a4767669d1869a33c05f
    type: comment
  author: maderix
  content: 'You can follow the steps here: https://github.com/qwopqwop200/GPTQ-for-LLaMa'
  created_at: 2023-03-14 15:44:38+00:00
  edited: false
  hidden: false
  id: 6410a4767669d1869a33c05f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a3b7ad4ba8aecbe5e0f3f870b984c5c.svg
      fullname: Sirenfal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sirenfal
      type: user
    createdAt: '2023-03-14T17:35:59.000Z'
    data:
      edited: true
      editors:
      - Sirenfal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a3b7ad4ba8aecbe5e0f3f870b984c5c.svg
          fullname: Sirenfal
          isHf: false
          isPro: false
          name: Sirenfal
          type: user
        html: '<p>Can you please also include <code>config.json</code> for the checkpoints?</p>

          '
        raw: Can you please also include `config.json` for the checkpoints?
        updatedAt: '2023-03-14T17:36:13.047Z'
      numEdits: 1
      reactions: []
    id: 6410b07f60a49973afaf35aa
    type: comment
  author: Sirenfal
  content: Can you please also include `config.json` for the checkpoints?
  created_at: 2023-03-14 16:35:59+00:00
  edited: true
  hidden: false
  id: 6410b07f60a49973afaf35aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-03-15T10:50:40.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p><a rel="nofollow" href="https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb">https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb</a></p>

          '
        raw: https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb
        updatedAt: '2023-03-15T10:50:40.296Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - maderix
        - milyiyo
      - count: 1
        reaction: "\U0001F44D"
        users:
        - milyiyo
    id: 6411a3007723da6be57f799f
    type: comment
  author: wassname
  content: https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb
  created_at: 2023-03-15 09:50:40+00:00
  edited: false
  hidden: false
  id: 6411a3007723da6be57f799f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657812125796-noauth.png?w=200&h=200&f=face
      fullname: Austin S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: titan087
      type: user
    createdAt: '2023-03-15T21:52:08.000Z'
    data:
      edited: false
      editors:
      - titan087
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1657812125796-noauth.png?w=200&h=200&f=face
          fullname: Austin S
          isHf: false
          isPro: false
          name: titan087
          type: user
        html: '<blockquote>

          <p>Can you please also include <code>config.json</code> for the checkpoints?</p>

          </blockquote>

          <p>You can use the config from the already converted models. I am using
          the 30b config with the 4bit model and its working fine</p>

          '
        raw: '> Can you please also include `config.json` for the checkpoints?


          You can use the config from the already converted models. I am using the
          30b config with the 4bit model and its working fine'
        updatedAt: '2023-03-15T21:52:08.300Z'
      numEdits: 0
      reactions: []
    id: 64123e08eef867ce8f4dd353
    type: comment
  author: titan087
  content: '> Can you please also include `config.json` for the checkpoints?


    You can use the config from the already converted models. I am using the 30b config
    with the 4bit model and its working fine'
  created_at: 2023-03-15 20:52:08+00:00
  edited: false
  hidden: false
  id: 64123e08eef867ce8f4dd353
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54a1513a84c612c464076537924bfdf7.svg
      fullname: Satvik Pendem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cercatrova
      type: user
    createdAt: '2023-03-18T02:35:44.000Z'
    data:
      edited: false
      editors:
      - cercatrova
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54a1513a84c612c464076537924bfdf7.svg
          fullname: Satvik Pendem
          isHf: false
          isPro: false
          name: cercatrova
          type: user
        html: '<p>Does this work with <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>?</p>

          '
        raw: Does this work with [llama.cpp](https://github.com/ggerganov/llama.cpp)?
        updatedAt: '2023-03-18T02:35:44.459Z'
      numEdits: 0
      reactions: []
    id: 641523806cab3118d30eb82e
    type: comment
  author: cercatrova
  content: Does this work with [llama.cpp](https://github.com/ggerganov/llama.cpp)?
  created_at: 2023-03-18 01:35:44+00:00
  edited: false
  hidden: false
  id: 641523806cab3118d30eb82e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651644128941-noauth.jpeg?w=200&h=200&f=face
      fullname: Manjeet Singh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: maderix
      type: user
    createdAt: '2023-03-18T03:13:51.000Z'
    data:
      edited: false
      editors:
      - maderix
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651644128941-noauth.jpeg?w=200&h=200&f=face
          fullname: Manjeet Singh
          isHf: false
          isPro: false
          name: maderix
          type: user
        html: '<p>llama.cpp uses a different format, it has a script for converting
          the original pth checkpoints to ggml format. I doubt these models will work
          as they are already quantized.</p>

          '
        raw: llama.cpp uses a different format, it has a script for converting the
          original pth checkpoints to ggml format. I doubt these models will work
          as they are already quantized.
        updatedAt: '2023-03-18T03:13:51.237Z'
      numEdits: 0
      reactions: []
    id: 64152c6f2a163d630047d429
    type: comment
  author: maderix
  content: llama.cpp uses a different format, it has a script for converting the original
    pth checkpoints to ggml format. I doubt these models will work as they are already
    quantized.
  created_at: 2023-03-18 02:13:51+00:00
  edited: false
  hidden: false
  id: 64152c6f2a163d630047d429
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54a1513a84c612c464076537924bfdf7.svg
      fullname: Satvik Pendem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cercatrova
      type: user
    createdAt: '2023-03-18T03:18:10.000Z'
    data:
      edited: false
      editors:
      - cercatrova
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54a1513a84c612c464076537924bfdf7.svg
          fullname: Satvik Pendem
          isHf: false
          isPro: false
          name: cercatrova
          type: user
        html: '<p>Makes sense, do you know how much RAM it takes to quantize the 65B
          model to 4 bits, for use in llama.cpp using their quantization script?</p>

          '
        raw: Makes sense, do you know how much RAM it takes to quantize the 65B model
          to 4 bits, for use in llama.cpp using their quantization script?
        updatedAt: '2023-03-18T03:18:10.123Z'
      numEdits: 0
      reactions: []
    id: 64152d7253d2aee4d452e7e2
    type: comment
  author: cercatrova
  content: Makes sense, do you know how much RAM it takes to quantize the 65B model
    to 4 bits, for use in llama.cpp using their quantization script?
  created_at: 2023-03-18 02:18:10+00:00
  edited: false
  hidden: false
  id: 64152d7253d2aee4d452e7e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ffa5c70e33f1ecdd2c22fe5f8915ef54.svg
      fullname: Rod Z
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rodzite
      type: user
    createdAt: '2023-03-20T21:46:58.000Z'
    data:
      edited: false
      editors:
      - Rodzite
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ffa5c70e33f1ecdd2c22fe5f8915ef54.svg
          fullname: Rod Z
          isHf: false
          isPro: false
          name: Rodzite
          type: user
        html: '<p>I''ve heard it takes over 100 GB of RAM/swap to quantize the 65B
          model.</p>

          '
        raw: I've heard it takes over 100 GB of RAM/swap to quantize the 65B model.
        updatedAt: '2023-03-20T21:46:58.238Z'
      numEdits: 0
      reactions: []
    id: 6418d45208588020c7ad28a6
    type: comment
  author: Rodzite
  content: I've heard it takes over 100 GB of RAM/swap to quantize the 65B model.
  created_at: 2023-03-20 20:46:58+00:00
  edited: false
  hidden: false
  id: 6418d45208588020c7ad28a6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651644128941-noauth.jpeg?w=200&h=200&f=face
      fullname: Manjeet Singh
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: maderix
      type: user
    createdAt: '2023-03-21T02:21:44.000Z'
    data:
      edited: false
      editors:
      - maderix
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1651644128941-noauth.jpeg?w=200&h=200&f=face
          fullname: Manjeet Singh
          isHf: false
          isPro: false
          name: maderix
          type: user
        html: '<p>Yeah it took around 126GB in my case, not sure if that''s a bug
          in the GPTQ conversion script.</p>

          '
        raw: Yeah it took around 126GB in my case, not sure if that's a bug in the
          GPTQ conversion script.
        updatedAt: '2023-03-21T02:21:44.962Z'
      numEdits: 0
      reactions: []
    id: 641914b87818018e3e4e3f88
    type: comment
  author: maderix
  content: Yeah it took around 126GB in my case, not sure if that's a bug in the GPTQ
    conversion script.
  created_at: 2023-03-21 01:21:44+00:00
  edited: false
  hidden: false
  id: 641914b87818018e3e4e3f88
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d130e7051d81303130883709aacb2618.svg
      fullname: Logesh Kumar umapathi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: infinitylogesh
      type: user
    createdAt: '2023-03-22T07:34:10.000Z'
    data:
      edited: false
      editors:
      - infinitylogesh
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d130e7051d81303130883709aacb2618.svg
          fullname: Logesh Kumar umapathi
          isHf: false
          isPro: false
          name: infinitylogesh
          type: user
        html: "<blockquote>\n<p><a rel=\"nofollow\" href=\"https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb\"\
          >https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb</a></p>\n\
          </blockquote>\n<p>Following this notebook and using the converted weights\
          \ from here is giving me this error ? Any one else facing this issue, any\
          \ help on this would be great ! Thanks</p>\n<pre><code>size mismatch for\
          \ model.layers.0.self_attn.q_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          \    size mismatch for model.layers.0.self_attn.k_proj.scales: copying a\
          \ param with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 4096]).\n    size mismatch for model.layers.0.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n    size mismatch for\
          \ model.layers.0.self_attn.o_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          \    size mismatch for model.layers.0.mlp.gate_proj.scales: copying a param\
          \ with shape torch.Size([11008, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 11008]).\n    size mismatch for model.layers.0.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n    size mismatch for\
          \ model.layers.0.mlp.up_proj.scales: copying a param with shape torch.Size([11008,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 11008]).\n\
          \    size mismatch for model.layers.1.self_attn.q_proj.scales: copying a\
          \ param with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 4096]).\n    size mismatch for model.layers.1.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n    size mismatch for\
          \ model.layers.1.self_attn.v_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          \    size mismatch for model.layers.1.self_attn.o_proj.scales: copying a\
          \ param with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 4096]).\n    size mismatch for model.layers.1.mlp.gate_proj.scales:\
          \ copying a param with shape torch.Size([11008, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 11008]).\n    size mismatch for\
          \ model.layers.1.mlp.down_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          ....\n</code></pre>\n"
        raw: "> https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb\n\
          \nFollowing this notebook and using the converted weights from here is giving\
          \ me this error ? Any one else facing this issue, any help on this would\
          \ be great ! Thanks\n```\nsize mismatch for model.layers.0.self_attn.q_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n\tsize mismatch for\
          \ model.layers.0.self_attn.k_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          \tsize mismatch for model.layers.0.self_attn.v_proj.scales: copying a param\
          \ with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n\tsize mismatch for\
          \ model.layers.0.mlp.gate_proj.scales: copying a param with shape torch.Size([11008,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 11008]).\n\
          \tsize mismatch for model.layers.0.mlp.down_proj.scales: copying a param\
          \ with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.mlp.up_proj.scales:\
          \ copying a param with shape torch.Size([11008, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 11008]).\n\tsize mismatch for\
          \ model.layers.1.self_attn.q_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          \tsize mismatch for model.layers.1.self_attn.k_proj.scales: copying a param\
          \ with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n\tsize mismatch for\
          \ model.layers.1.self_attn.o_proj.scales: copying a param with shape torch.Size([4096,\
          \ 1]) from checkpoint, the shape in current model is torch.Size([1, 4096]).\n\
          \tsize mismatch for model.layers.1.mlp.gate_proj.scales: copying a param\
          \ with shape torch.Size([11008, 1]) from checkpoint, the shape in current\
          \ model is torch.Size([1, 11008]).\n\tsize mismatch for model.layers.1.mlp.down_proj.scales:\
          \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the\
          \ shape in current model is torch.Size([1, 4096]).\n....\n```"
        updatedAt: '2023-03-22T07:34:10.187Z'
      numEdits: 0
      reactions: []
    id: 641aaf725d107c5c5f3408ae
    type: comment
  author: infinitylogesh
  content: "> https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb\n\
    \nFollowing this notebook and using the converted weights from here is giving\
    \ me this error ? Any one else facing this issue, any help on this would be great\
    \ ! Thanks\n```\nsize mismatch for model.layers.0.self_attn.q_proj.scales: copying\
    \ a param with shape torch.Size([4096, 1]) from checkpoint, the shape in current\
    \ model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([11008, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\n\tsize mismatch for model.layers.0.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.0.mlp.up_proj.scales:\
    \ copying a param with shape torch.Size([11008, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\n\tsize mismatch for model.layers.1.self_attn.q_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.1.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.1.self_attn.v_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.1.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.scales:\
    \ copying a param with shape torch.Size([11008, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 11008]).\n\tsize mismatch for model.layers.1.mlp.down_proj.scales:\
    \ copying a param with shape torch.Size([4096, 1]) from checkpoint, the shape\
    \ in current model is torch.Size([1, 4096]).\n....\n```"
  created_at: 2023-03-22 06:34:10+00:00
  edited: false
  hidden: false
  id: 641aaf725d107c5c5f3408ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
      fullname: Baffo 32
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baffo32
      type: user
    createdAt: '2023-03-24T09:26:05.000Z'
    data:
      edited: true
      editors:
      - baffo32
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9dd3236477291c9a41ae9b98213ab651.svg
          fullname: Baffo 32
          isHf: false
          isPro: false
          name: baffo32
          type: user
        html: '<blockquote>

          <p>llama.cpp uses a different format, it has a script for converting the
          original pth checkpoints to ggml format. I doubt these models will work
          as they are already quantized.</p>

          </blockquote>

          <p>there''s little reason for quantization to affect the format [edit: i
          see this may not be true; it is notable that the pytorch format is loaded
          similarly]</p>

          <blockquote>

          <blockquote>

          <p><a rel="nofollow" href="https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb">https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb</a></p>

          </blockquote>

          <p>Following this notebook and using the converted weights from here is
          giving me this error ? Any one else facing this issue, any help on this
          would be great ! Thanks</p>

          </blockquote>

          <p>it looks like one of the two projects you are combining is expecting
          the weights transposed. you may need to load them manually and replace each
          one with the result of calling <code>.T</code>to get them to load. The maintainer
          of the colab can likely help more, as it is a much simpler change in code
          than data.</p>

          '
        raw: "> llama.cpp uses a different format, it has a script for converting\
          \ the original pth checkpoints to ggml format. I doubt these models will\
          \ work as they are already quantized.\n\nthere's little reason for quantization\
          \ to affect the format [edit: i see this may not be true; it is notable\
          \ that the pytorch format is loaded similarly]\n\n> > https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb\n\
          > \n> Following this notebook and using the converted weights from here\
          \ is giving me this error ? Any one else facing this issue, any help on\
          \ this would be great ! Thanks\n\nit looks like one of the two projects\
          \ you are combining is expecting the weights transposed. you may need to\
          \ load them manually and replace each one with the result of calling `.T`to\
          \ get them to load. The maintainer of the colab can likely help more, as\
          \ it is a much simpler change in code than data."
        updatedAt: '2023-03-24T09:48:15.638Z'
      numEdits: 2
      reactions: []
    id: 641d6cad74f8dcbc525a8fd7
    type: comment
  author: baffo32
  content: "> llama.cpp uses a different format, it has a script for converting the\
    \ original pth checkpoints to ggml format. I doubt these models will work as they\
    \ are already quantized.\n\nthere's little reason for quantization to affect the\
    \ format [edit: i see this may not be true; it is notable that the pytorch format\
    \ is loaded similarly]\n\n> > https://github.com/amrrs/llama-4bit-colab/blob/main/LLaMA_4_bit_on_Google_Colab.ipynb\n\
    > \n> Following this notebook and using the converted weights from here is giving\
    \ me this error ? Any one else facing this issue, any help on this would be great\
    \ ! Thanks\n\nit looks like one of the two projects you are combining is expecting\
    \ the weights transposed. you may need to load them manually and replace each\
    \ one with the result of calling `.T`to get them to load. The maintainer of the\
    \ colab can likely help more, as it is a much simpler change in code than data."
  created_at: 2023-03-24 08:26:05+00:00
  edited: true
  hidden: false
  id: 641d6cad74f8dcbc525a8fd7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: maderix/llama-65b-4bit
repo_type: model
status: open
target_branch: null
title: Example of how to use the models
