!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lukaemon
conflicting_files: null
created_at: 2023-01-09 06:23:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
      fullname: Lucas Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lukaemon
      type: user
    createdAt: '2023-01-09T06:23:01.000Z'
    data:
      edited: false
      editors:
      - lukaemon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
          fullname: Lucas Shen
          isHf: false
          isPro: false
          name: lukaemon
          type: user
        html: '<p>Running this code, taken from the doc with no change, on single
          a6000 works as expected, <code>&lt;pad&gt; Wie alt sind Sie?&lt;/s&gt;</code>.
          </p>

          <pre><code class="language-#">from transformers import T5Tokenizer, T5ForConditionalGeneration


          tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xxl")

          model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xxl",
          device_map="auto")


          input_text = "translate English to German: How old are you?"

          input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")


          outputs = model.generate(input_ids)

          print(tokenizer.decode(outputs[0]))

          </code></pre>

          <p>But it generates garbage on  2*3090: <code>&lt;pad&gt;&lt;pad&gt;crawl
          himemme person tsch center  Preis    Tau the residency now</code></p>

          <p>Can someone point me a direction how to get this working on 2*3090? What''s
          possible source of problem?</p>

          '
        raw: "Running this code, taken from the doc with no change, on single a6000\
          \ works as expected, `<pad> Wie alt sind Sie?</s>`. \r\n```# pip install\
          \ accelerate\r\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\r\
          \n\r\ntokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\r\n\
          model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\"\
          , device_map=\"auto\")\r\n\r\ninput_text = \"translate English to German:\
          \ How old are you?\"\r\ninput_ids = tokenizer(input_text, return_tensors=\"\
          pt\").input_ids.to(\"cuda\")\r\n\r\noutputs = model.generate(input_ids)\r\
          \nprint(tokenizer.decode(outputs[0]))\r\n```\r\n\r\nBut it generates garbage\
          \ on  2*3090: `<pad><pad>crawl himemme person tsch center  Preis    Tau\
          \ the residency now`\r\n\r\nCan someone point me a direction how to get\
          \ this working on 2*3090? What's possible source of problem?"
        updatedAt: '2023-01-09T06:23:01.467Z'
      numEdits: 0
      reactions: []
    id: 63bbb2c57fd5e883e1468d24
    type: comment
  author: lukaemon
  content: "Running this code, taken from the doc with no change, on single a6000\
    \ works as expected, `<pad> Wie alt sind Sie?</s>`. \r\n```# pip install accelerate\r\
    \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\r\n\r\ntokenizer\
    \ = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\")\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
    google/flan-t5-xxl\", device_map=\"auto\")\r\n\r\ninput_text = \"translate English\
    \ to German: How old are you?\"\r\ninput_ids = tokenizer(input_text, return_tensors=\"\
    pt\").input_ids.to(\"cuda\")\r\n\r\noutputs = model.generate(input_ids)\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n```\r\n\r\nBut it generates garbage on  2*3090: `<pad><pad>crawl himemme person\
    \ tsch center  Preis    Tau the residency now`\r\n\r\nCan someone point me a direction\
    \ how to get this working on 2*3090? What's possible source of problem?"
  created_at: 2023-01-09 06:23:01+00:00
  edited: false
  hidden: false
  id: 63bbb2c57fd5e883e1468d24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
      fullname: Lucas Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lukaemon
      type: user
    createdAt: '2023-01-10T10:23:32.000Z'
    data:
      edited: false
      editors:
      - lukaemon
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
          fullname: Lucas Shen
          isHf: false
          isPro: false
          name: lukaemon
          type: user
        html: "<p>I got it to work. device_map='auto' won't work on xxl modle for\
          \ my 2*3090 pc. </p>\n<p>This works fine. </p>\n<pre><code class=\"language-python\"\
          >model = T5ForConditionalGeneration.from_pretrained(\n            checkpoint,\n\
          \            low_cpu_mem_usage=<span class=\"hljs-literal\">True</span>,\n\
          \            torch_dtype=torch.bfloat16)\nmodel.parallelize()\n</code></pre>\n"
        raw: "I got it to work. device_map='auto' won't work on xxl modle for my 2*3090\
          \ pc. \n\nThis works fine. \n```python\nmodel = T5ForConditionalGeneration.from_pretrained(\n\
          \            checkpoint,\n            low_cpu_mem_usage=True,\n        \
          \    torch_dtype=torch.bfloat16)\nmodel.parallelize()\n```"
        updatedAt: '2023-01-10T10:23:32.779Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63bd3ca4065f9bd92cdc6fdb
    id: 63bd3ca4065f9bd92cdc6fda
    type: comment
  author: lukaemon
  content: "I got it to work. device_map='auto' won't work on xxl modle for my 2*3090\
    \ pc. \n\nThis works fine. \n```python\nmodel = T5ForConditionalGeneration.from_pretrained(\n\
    \            checkpoint,\n            low_cpu_mem_usage=True,\n            torch_dtype=torch.bfloat16)\n\
    model.parallelize()\n```"
  created_at: 2023-01-10 10:23:32+00:00
  edited: false
  hidden: false
  id: 63bd3ca4065f9bd92cdc6fda
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6099e69332c054a2e98479ce/c_I1SJ9eZUZr_sY0JM93b.jpeg?w=200&h=200&f=face
      fullname: Lucas Shen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lukaemon
      type: user
    createdAt: '2023-01-10T10:23:32.000Z'
    data:
      status: closed
    id: 63bd3ca4065f9bd92cdc6fdb
    type: status-change
  author: lukaemon
  created_at: 2023-01-10 10:23:32+00:00
  id: 63bd3ca4065f9bd92cdc6fdb
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 19
repo_id: google/flan-t5-xxl
repo_type: model
status: closed
target_branch: null
title: xxl model running on single a6000 vs 2*3090
