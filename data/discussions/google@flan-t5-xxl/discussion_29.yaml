!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ArthurConmy
conflicting_files: null
created_at: 2023-02-12 12:42:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4f808fae966e808105e89712c97d90d2.svg
      fullname: VConm
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurConmy
      type: user
    createdAt: '2023-02-12T12:42:13.000Z'
    data:
      edited: false
      editors:
      - ArthurConmy
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4f808fae966e808105e89712c97d90d2.svg
          fullname: VConm
          isHf: false
          isPro: false
          name: ArthurConmy
          type: user
        html: '<p>I added some breakpoints to the following code:</p>

          <pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration

          tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")

          model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small",
          device_map="auto")


          input_ids = tokenizer("summarize: studies have shown that owning a dog is
          good for you ", return_tensors="pt").input_ids # Batch size 1

          outputs = model.generate(input_ids)

          print(tokenizer.decode(outputs[0]))

          </code></pre>

          <p>I saw that this activates greedy generation mode in <a rel="nofollow"
          href="https://github.com/huggingface/transformers/blob/23c146c38b42d1193849fbd6f2943bf754b7c428/src/transformers/generation/utils.py#L1090">this
          function</a>. However, the paper <a rel="nofollow" href="https://openreview.net/pdf?id=gEZrGCozdqR">https://openreview.net/pdf?id=gEZrGCozdqR</a>
          has many more usages of top-k sampling than greedy sampling. Shouldn''t
          the default be top-k sampling?</p>

          <p>(more generally, I''m curious about what the best practices for sampling
          from SOTA LLMs is. It seems that top-p nucleus sampling is the most common,
          but beam search and greedy seem also used, and the articles online don''t
          mention <strong>which</strong> models use each of them)</p>

          '
        raw: "I added some breakpoints to the following code:\r\n\r\n```\r\nfrom transformers\
          \ import T5Tokenizer, T5ForConditionalGeneration\r\ntokenizer = T5Tokenizer.from_pretrained(\"\
          google/flan-t5-small\")\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
          google/flan-t5-small\", device_map=\"auto\")\r\n\r\ninput_ids = tokenizer(\"\
          summarize: studies have shown that owning a dog is good for you \", return_tensors=\"\
          pt\").input_ids # Batch size 1\r\noutputs = model.generate(input_ids)\r\n\
          print(tokenizer.decode(outputs[0]))\r\n```\r\n\r\nI saw that this activates\
          \ greedy generation mode in <a href=\"https://github.com/huggingface/transformers/blob/23c146c38b42d1193849fbd6f2943bf754b7c428/src/transformers/generation/utils.py#L1090\"\
          >this function</a>. However, the paper https://openreview.net/pdf?id=gEZrGCozdqR\
          \ has many more usages of top-k sampling than greedy sampling. Shouldn't\
          \ the default be top-k sampling?\r\n\r\n(more generally, I'm curious about\
          \ what the best practices for sampling from SOTA LLMs is. It seems that\
          \ top-p nucleus sampling is the most common, but beam search and greedy\
          \ seem also used, and the articles online don't mention **which** models\
          \ use each of them)"
        updatedAt: '2023-02-12T12:42:13.646Z'
      numEdits: 0
      reactions: []
    id: 63e8dea5ca4fc7d30de82cd1
    type: comment
  author: ArthurConmy
  content: "I added some breakpoints to the following code:\r\n\r\n```\r\nfrom transformers\
    \ import T5Tokenizer, T5ForConditionalGeneration\r\ntokenizer = T5Tokenizer.from_pretrained(\"\
    google/flan-t5-small\")\r\nmodel = T5ForConditionalGeneration.from_pretrained(\"\
    google/flan-t5-small\", device_map=\"auto\")\r\n\r\ninput_ids = tokenizer(\"summarize:\
    \ studies have shown that owning a dog is good for you \", return_tensors=\"pt\"\
    ).input_ids # Batch size 1\r\noutputs = model.generate(input_ids)\r\nprint(tokenizer.decode(outputs[0]))\r\
    \n```\r\n\r\nI saw that this activates greedy generation mode in <a href=\"https://github.com/huggingface/transformers/blob/23c146c38b42d1193849fbd6f2943bf754b7c428/src/transformers/generation/utils.py#L1090\"\
    >this function</a>. However, the paper https://openreview.net/pdf?id=gEZrGCozdqR\
    \ has many more usages of top-k sampling than greedy sampling. Shouldn't the default\
    \ be top-k sampling?\r\n\r\n(more generally, I'm curious about what the best practices\
    \ for sampling from SOTA LLMs is. It seems that top-p nucleus sampling is the\
    \ most common, but beam search and greedy seem also used, and the articles online\
    \ don't mention **which** models use each of them)"
  created_at: 2023-02-12 12:42:13+00:00
  edited: false
  hidden: false
  id: 63e8dea5ca4fc7d30de82cd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae0ebefe67d2cdd3c1f26bf3da5f5094.svg
      fullname: Alexandru Coca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deathcrush
      type: user
    createdAt: '2023-03-03T10:14:16.000Z'
    data:
      edited: false
      editors:
      - deathcrush
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae0ebefe67d2cdd3c1f26bf3da5f5094.svg
          fullname: Alexandru Coca
          isHf: false
          isPro: false
          name: deathcrush
          type: user
        html: '<p>I think greedy sampling is the simplest strategy that works on the
          widest variety of tasks. It covers summarisation and open-ended generation,
          but also things like generative classification or question answering. For
          the latter class of tasks, you may not want to use stochastic sampling,
          especially if you finetuned the model on a specific task, as then you need
          to somehow choose one of the hypotheses. So the defaults the library authors
          set cover all tasks. People interested in summarisation or other tasks where
          diversity is important, can consult the literature and set the parameters
          accordingly. It would be great to have a table listing the major works and
          the parameters they use, that would be a great addition to the docs!</p>

          '
        raw: I think greedy sampling is the simplest strategy that works on the widest
          variety of tasks. It covers summarisation and open-ended generation, but
          also things like generative classification or question answering. For the
          latter class of tasks, you may not want to use stochastic sampling, especially
          if you finetuned the model on a specific task, as then you need to somehow
          choose one of the hypotheses. So the defaults the library authors set cover
          all tasks. People interested in summarisation or other tasks where diversity
          is important, can consult the literature and set the parameters accordingly.
          It would be great to have a table listing the major works and the parameters
          they use, that would be a great addition to the docs!
        updatedAt: '2023-03-03T10:14:16.272Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rachith
    id: 6401c878c17fbf7a4f40544e
    type: comment
  author: deathcrush
  content: I think greedy sampling is the simplest strategy that works on the widest
    variety of tasks. It covers summarisation and open-ended generation, but also
    things like generative classification or question answering. For the latter class
    of tasks, you may not want to use stochastic sampling, especially if you finetuned
    the model on a specific task, as then you need to somehow choose one of the hypotheses.
    So the defaults the library authors set cover all tasks. People interested in
    summarisation or other tasks where diversity is important, can consult the literature
    and set the parameters accordingly. It would be great to have a table listing
    the major works and the parameters they use, that would be a great addition to
    the docs!
  created_at: 2023-03-03 10:14:16+00:00
  edited: false
  hidden: false
  id: 6401c878c17fbf7a4f40544e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c693e0e93b8a893a2c25757bc8b7b45d.svg
      fullname: Rachith Aiyappa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rachith
      type: user
    createdAt: '2023-03-10T22:36:18.000Z'
    data:
      edited: false
      editors:
      - rachith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c693e0e93b8a893a2c25757bc8b7b45d.svg
          fullname: Rachith Aiyappa
          isHf: false
          isPro: false
          name: rachith
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ArthurConmy&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ArthurConmy\"\
          >@<span class=\"underline\">ArthurConmy</span></a></span>\n\n\t</span></span>,<br>Not\
          \ related to your question but since it may be useful for you to know nevertheless,<br>could\
          \ you check if model.generate(input_ids,min_new_tokens=30) actually gives\
          \ you 30 new tokens? This is related to the issue <a rel=\"nofollow\" href=\"\
          https://discuss.huggingface.co/t/minimum-number-of-tokens-in-generate/33481\"\
          >here</a> or <a href=\"/google/flan-t5-xxl/discussions/38\">#38</a></p>\n"
        raw: "@ArthurConmy, \nNot related to your question but since it may be useful\
          \ for you to know nevertheless, \ncould you check if model.generate(input_ids,min_new_tokens=30)\
          \ actually gives you 30 new tokens? This is related to the issue [here](https://discuss.huggingface.co/t/minimum-number-of-tokens-in-generate/33481)\
          \ or #38"
        updatedAt: '2023-03-10T22:36:18.892Z'
      numEdits: 0
      reactions: []
    id: 640bb0e2c1a2a0d21fc57210
    type: comment
  author: rachith
  content: "@ArthurConmy, \nNot related to your question but since it may be useful\
    \ for you to know nevertheless, \ncould you check if model.generate(input_ids,min_new_tokens=30)\
    \ actually gives you 30 new tokens? This is related to the issue [here](https://discuss.huggingface.co/t/minimum-number-of-tokens-in-generate/33481)\
    \ or #38"
  created_at: 2023-03-10 22:36:18+00:00
  edited: false
  hidden: false
  id: 640bb0e2c1a2a0d21fc57210
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: google/flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: Confusion about sampling for `flan-t5` models
