!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ivoschaper
conflicting_files: null
created_at: 2022-12-21 14:01:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c3ba2cca8ceeb6195ada29727b923dcc.svg
      fullname: Ivo Schaper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivoschaper
      type: user
    createdAt: '2022-12-21T14:01:45.000Z'
    data:
      edited: false
      editors:
      - ivoschaper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c3ba2cca8ceeb6195ada29727b923dcc.svg
          fullname: Ivo Schaper
          isHf: false
          isPro: false
          name: ivoschaper
          type: user
        html: '<p>I''m trying to run this model with Amazon SageMaker and am able
          to successfully deploy it on a ml.m5.xlarge instance.</p>

          <p>Unfortunately, when invoking the endpoint I get a <code>PredictionException</code>
          which says: <code>Could not load model /.sagemaker/mms/models/google__flan-t5-xxl
          with any of the following classes: (transformers.models.auto.modeling_auto.AutoModelForSeqZSeqLMu0027,
          transformers.models.t5.modeling_t5.T5ForConditionalGeneration).</code></p>

          <p>Has anyone had this issue or, even better, been able to deploy and use
          this model via SageMaker?</p>

          <p>Interestingly, when I try to deploy and invoke the endpoint for the flan-t5-large
          model (with the same ml.m5.xlarge instance) I don''t face any issues. I
          strongly suspect that this is related to the model size so I tried using
          the biggest instance SageMaker offers (and that is available to me) which
          is a ml.p3.8xlarge instance, yet I still face the same issue.</p>

          '
        raw: "I'm trying to run this model with Amazon SageMaker and am able to successfully\
          \ deploy it on a ml.m5.xlarge instance.\r\n\r\nUnfortunately, when invoking\
          \ the endpoint I get a `PredictionException` which says: `Could not load\
          \ model /.sagemaker/mms/models/google__flan-t5-xxl with any of the following\
          \ classes: (transformers.models.auto.modeling_auto.AutoModelForSeqZSeqLMu0027,\
          \ transformers.models.t5.modeling_t5.T5ForConditionalGeneration).`\r\n\r\
          \nHas anyone had this issue or, even better, been able to deploy and use\
          \ this model via SageMaker?\r\n\r\nInterestingly, when I try to deploy and\
          \ invoke the endpoint for the flan-t5-large model (with the same ml.m5.xlarge\
          \ instance) I don't face any issues. I strongly suspect that this is related\
          \ to the model size so I tried using the biggest instance SageMaker offers\
          \ (and that is available to me) which is a ml.p3.8xlarge instance, yet I\
          \ still face the same issue."
        updatedAt: '2022-12-21T14:01:45.857Z'
      numEdits: 0
      reactions: []
    id: 63a311c9a0ed04b823be87d3
    type: comment
  author: ivoschaper
  content: "I'm trying to run this model with Amazon SageMaker and am able to successfully\
    \ deploy it on a ml.m5.xlarge instance.\r\n\r\nUnfortunately, when invoking the\
    \ endpoint I get a `PredictionException` which says: `Could not load model /.sagemaker/mms/models/google__flan-t5-xxl\
    \ with any of the following classes: (transformers.models.auto.modeling_auto.AutoModelForSeqZSeqLMu0027,\
    \ transformers.models.t5.modeling_t5.T5ForConditionalGeneration).`\r\n\r\nHas\
    \ anyone had this issue or, even better, been able to deploy and use this model\
    \ via SageMaker?\r\n\r\nInterestingly, when I try to deploy and invoke the endpoint\
    \ for the flan-t5-large model (with the same ml.m5.xlarge instance) I don't face\
    \ any issues. I strongly suspect that this is related to the model size so I tried\
    \ using the biggest instance SageMaker offers (and that is available to me) which\
    \ is a ml.p3.8xlarge instance, yet I still face the same issue."
  created_at: 2022-12-21 14:01:45+00:00
  edited: false
  hidden: false
  id: 63a311c9a0ed04b823be87d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c3ba2cca8ceeb6195ada29727b923dcc.svg
      fullname: Ivo Schaper
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ivoschaper
      type: user
    createdAt: '2022-12-21T14:04:24.000Z'
    data:
      edited: false
      editors:
      - ivoschaper
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c3ba2cca8ceeb6195ada29727b923dcc.svg
          fullname: Ivo Schaper
          isHf: false
          isPro: false
          name: ivoschaper
          type: user
        html: "<p>My code for deployment looks like this (very similar to what HuggingFace\
          \ provides, but not exactly the same):</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">from</span> sagemaker.huggingface <span class=\"\
          hljs-keyword\">import</span> HuggingFaceModel\n<span class=\"hljs-keyword\"\
          >import</span> boto3\n<span class=\"hljs-keyword\">import</span> os\n\n\
          AWS_ACCESS_KEY_ID = os.environ[<span class=\"hljs-string\">\"AWS_ACCESS_KEY_ID\"\
          </span>]\nAWS_SECRET_ACCESS_KEY = os.environ[<span class=\"hljs-string\"\
          >\"AWS_SECRET_ACCESS_KEY\"</span>]\n\n\n<span class=\"hljs-comment\"># Hub\
          \ Model configuration. https://huggingface.co/models</span>\nhub = {\n \
          \   <span class=\"hljs-string\">'HF_MODEL_ID'</span>:<span class=\"hljs-string\"\
          >'google/flan-t5-xxl'</span>,\n    <span class=\"hljs-string\">'HF_TASK'</span>:<span\
          \ class=\"hljs-string\">'text2text-generation'</span>\n}\n\niam_client =\
          \ boto3.client(<span class=\"hljs-string\">'iam'</span>)\n\n<span class=\"\
          hljs-comment\"># IAM role</span>\nrole = iam_client.get_role(RoleName=<span\
          \ class=\"hljs-string\">'my-role-with-sagemaker-access'</span>)[<span class=\"\
          hljs-string\">'Role'</span>][<span class=\"hljs-string\">'Arn'</span>]\n\
          \n<span class=\"hljs-comment\"># create Hugging Face Model Class</span>\n\
          huggingface_model = HuggingFaceModel(\n    transformers_version=<span class=\"\
          hljs-string\">'4.17.0'</span>, \n    pytorch_version=<span class=\"hljs-string\"\
          >'1.10.2'</span>,\n    py_version=<span class=\"hljs-string\">'py38'</span>,\n\
          \    env=hub,\n    role=role,\n)\n\n<span class=\"hljs-comment\"># deploy\
          \ model to SageMaker Inference</span>\npredictor = huggingface_model.deploy(\n\
          \    initial_instance_count=<span class=\"hljs-number\">1</span>, <span\
          \ class=\"hljs-comment\"># number of instances</span>\n    instance_type=<span\
          \ class=\"hljs-string\">'ml.m5.xlarge'</span> <span class=\"hljs-comment\"\
          ># ec2 instance type</span>\n)\n</code></pre>\n<p>And my invocation looks\
          \ like this:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> boto3\n<span class=\"hljs-keyword\">from</span> sagemaker.serializers\
          \ <span class=\"hljs-keyword\">import</span> JSONSerializer\n<span class=\"\
          hljs-keyword\">import</span> json\n\nclient = boto3.client(<span class=\"\
          hljs-string\">'sagemaker-runtime'</span>)\n\nendpoint_name = <span class=\"\
          hljs-string\">\"huggingface-pytorch-inference-XXXXXXXXX\"</span>\n\n<span\
          \ class=\"hljs-comment\"># The MIME type of the input data in the request\
          \ body.</span>\ncontent_type = <span class=\"hljs-string\">\"application/json\"\
          </span>\n\n<span class=\"hljs-comment\"># The desired MIME type of the inference\
          \ in the response.</span>\naccept = <span class=\"hljs-string\">\"application/json\"\
          </span>\n\n<span class=\"hljs-comment\"># Payload for inference.</span>\n\
          payload = {\n    <span class=\"hljs-string\">\"inputs\"</span>: <span class=\"\
          hljs-string\">\"The capital of Germany is\"</span>,\n    <span class=\"\
          hljs-string\">\"parameters\"</span>: {\n        <span class=\"hljs-string\"\
          >\"temperature\"</span>: <span class=\"hljs-number\">0.7</span>,\n    },\n\
          \    <span class=\"hljs-string\">\"options\"</span>: {\n        <span class=\"\
          hljs-string\">\"use_cache\"</span>: <span class=\"hljs-literal\">False</span>,\n\
          \    },\n} \n\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n\
          \    ContentType=content_type,\n    Accept=accept,\n    Body=JSONSerializer().serialize(payload)\n\
          \    )\n\npred = json.loads(response[<span class=\"hljs-string\">'Body'</span>].read())\n\
          <span class=\"hljs-built_in\">print</span>(pred)\n\nprediction = pred[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>]\n\
          <span class=\"hljs-built_in\">print</span>(prediction)\n</code></pre>\n"
        raw: "My code for deployment looks like this (very similar to what HuggingFace\
          \ provides, but not exactly the same):\n\n```python\nfrom sagemaker.huggingface\
          \ import HuggingFaceModel\nimport boto3\nimport os\n\nAWS_ACCESS_KEY_ID\
          \ = os.environ[\"AWS_ACCESS_KEY_ID\"]\nAWS_SECRET_ACCESS_KEY = os.environ[\"\
          AWS_SECRET_ACCESS_KEY\"]\n\n\n# Hub Model configuration. https://huggingface.co/models\n\
          hub = {\n\t'HF_MODEL_ID':'google/flan-t5-xxl',\n\t'HF_TASK':'text2text-generation'\n\
          }\n\niam_client = boto3.client('iam')\n\n# IAM role\nrole = iam_client.get_role(RoleName='my-role-with-sagemaker-access')['Role']['Arn']\n\
          \n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\
          \ttransformers_version='4.17.0', \n\tpytorch_version='1.10.2',\n\tpy_version='py38',\n\
          \tenv=hub,\n\trole=role,\n)\n\n# deploy model to SageMaker Inference\npredictor\
          \ = huggingface_model.deploy(\n\tinitial_instance_count=1, # number of instances\n\
          \tinstance_type='ml.m5.xlarge' # ec2 instance type\n)\n```\n\nAnd my invocation\
          \ looks like this:\n```python\nimport boto3\nfrom sagemaker.serializers\
          \ import JSONSerializer\nimport json\n\nclient = boto3.client('sagemaker-runtime')\n\
          \nendpoint_name = \"huggingface-pytorch-inference-XXXXXXXXX\"\n\n# The MIME\
          \ type of the input data in the request body.\ncontent_type = \"application/json\"\
          \n\n# The desired MIME type of the inference in the response.\naccept =\
          \ \"application/json\"\n\n# Payload for inference.\npayload = {\n    \"\
          inputs\": \"The capital of Germany is\",\n    \"parameters\": {\n      \
          \  \"temperature\": 0.7,\n    },\n    \"options\": {\n        \"use_cache\"\
          : False,\n    },\n} \n\nresponse = client.invoke_endpoint(\n    EndpointName=endpoint_name,\n\
          \    ContentType=content_type,\n    Accept=accept,\n    Body=JSONSerializer().serialize(payload)\n\
          \    )\n\npred = json.loads(response['Body'].read())\nprint(pred)\n\nprediction\
          \ = pred[0]['generated_text']\nprint(prediction)\n```"
        updatedAt: '2022-12-21T14:04:24.211Z'
      numEdits: 0
      reactions: []
    id: 63a312681a19cbf69e7c233e
    type: comment
  author: ivoschaper
  content: "My code for deployment looks like this (very similar to what HuggingFace\
    \ provides, but not exactly the same):\n\n```python\nfrom sagemaker.huggingface\
    \ import HuggingFaceModel\nimport boto3\nimport os\n\nAWS_ACCESS_KEY_ID = os.environ[\"\
    AWS_ACCESS_KEY_ID\"]\nAWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"\
    ]\n\n\n# Hub Model configuration. https://huggingface.co/models\nhub = {\n\t'HF_MODEL_ID':'google/flan-t5-xxl',\n\
    \t'HF_TASK':'text2text-generation'\n}\n\niam_client = boto3.client('iam')\n\n\
    # IAM role\nrole = iam_client.get_role(RoleName='my-role-with-sagemaker-access')['Role']['Arn']\n\
    \n# create Hugging Face Model Class\nhuggingface_model = HuggingFaceModel(\n\t\
    transformers_version='4.17.0', \n\tpytorch_version='1.10.2',\n\tpy_version='py38',\n\
    \tenv=hub,\n\trole=role,\n)\n\n# deploy model to SageMaker Inference\npredictor\
    \ = huggingface_model.deploy(\n\tinitial_instance_count=1, # number of instances\n\
    \tinstance_type='ml.m5.xlarge' # ec2 instance type\n)\n```\n\nAnd my invocation\
    \ looks like this:\n```python\nimport boto3\nfrom sagemaker.serializers import\
    \ JSONSerializer\nimport json\n\nclient = boto3.client('sagemaker-runtime')\n\n\
    endpoint_name = \"huggingface-pytorch-inference-XXXXXXXXX\"\n\n# The MIME type\
    \ of the input data in the request body.\ncontent_type = \"application/json\"\n\
    \n# The desired MIME type of the inference in the response.\naccept = \"application/json\"\
    \n\n# Payload for inference.\npayload = {\n    \"inputs\": \"The capital of Germany\
    \ is\",\n    \"parameters\": {\n        \"temperature\": 0.7,\n    },\n    \"\
    options\": {\n        \"use_cache\": False,\n    },\n} \n\nresponse = client.invoke_endpoint(\n\
    \    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Accept=accept,\n\
    \    Body=JSONSerializer().serialize(payload)\n    )\n\npred = json.loads(response['Body'].read())\n\
    print(pred)\n\nprediction = pred[0]['generated_text']\nprint(prediction)\n```"
  created_at: 2022-12-21 14:04:24+00:00
  edited: false
  hidden: false
  id: 63a312681a19cbf69e7c233e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ffa0b99a9752a897dcbfabf85f70ff3.svg
      fullname: Xiang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xiangdal
      type: user
    createdAt: '2023-01-24T23:43:41.000Z'
    data:
      edited: false
      editors:
      - xiangdal
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ffa0b99a9752a897dcbfabf85f70ff3.svg
          fullname: Xiang
          isHf: false
          isPro: false
          name: xiangdal
          type: user
        html: '<p>I''m encountering the same issue. Did you find a workaround?</p>

          '
        raw: I'm encountering the same issue. Did you find a workaround?
        updatedAt: '2023-01-24T23:43:41.744Z'
      numEdits: 0
      reactions: []
    id: 63d06d2d9f1643d247199132
    type: comment
  author: xiangdal
  content: I'm encountering the same issue. Did you find a workaround?
  created_at: 2023-01-24 23:43:41+00:00
  edited: false
  hidden: false
  id: 63d06d2d9f1643d247199132
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/43dc294c7e1ab7fc6000b05cd58b89e2.svg
      fullname: Rohan Lekhwani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rlekhwani-umass
      type: user
    createdAt: '2023-02-08T11:32:14.000Z'
    data:
      edited: false
      editors:
      - rlekhwani-umass
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/43dc294c7e1ab7fc6000b05cd58b89e2.svg
          fullname: Rohan Lekhwani
          isHf: false
          isPro: false
          name: rlekhwani-umass
          type: user
        html: '<p>Facing the same issue. Have raised a GitHub issue here<br><a rel="nofollow"
          href="https://github.com/huggingface/transformers/issues/21402">https://github.com/huggingface/transformers/issues/21402</a></p>

          '
        raw: 'Facing the same issue. Have raised a GitHub issue here

          https://github.com/huggingface/transformers/issues/21402'
        updatedAt: '2023-02-08T11:32:14.741Z'
      numEdits: 0
      reactions: []
    id: 63e3883e8b8c518479883f5e
    type: comment
  author: rlekhwani-umass
  content: 'Facing the same issue. Have raised a GitHub issue here

    https://github.com/huggingface/transformers/issues/21402'
  created_at: 2023-02-08 11:32:14+00:00
  edited: false
  hidden: false
  id: 63e3883e8b8c518479883f5e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: google/flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: Deploying on Amazon SageMaker
