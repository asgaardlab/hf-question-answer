!!python/object:huggingface_hub.community.DiscussionWithDetails
author: phdykd
conflicting_files: null
created_at: 2023-05-03 18:43:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-05-03T19:43:02.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>I have 4K rows x 15 columns of txt data that has categorical text
          information in it. I wanted to use Langchain to do information retrieval
          using the following model repo. I did receive this error while implementing
          the below given code snippet "ValueError: Error raised by inference API:
          Input validation error: <code>inputs</code> tokens + <code>max_new_tokens</code>
          must be &lt;= 1512. Given: 190761 <code>inputs</code> tokens and 20 <code>max_new_tokens</code>"
          .</p>

          <p>Does 1512 chunk size is the limit for this repo? What can I use max number
          for the chunk size?<br>If this is not convenient repo, what would you recommend
          me to use as model while trying to retrieve information for the entire dataset
          I have (4K rows x 15 columns)?</p>

          <p>from langchain.text_splitter import CharacterTextSplitter<br>text_splitter
          = CharacterTextSplitter(chunk_size=200000, chunk_overlap=0)<br>docs = text_splitter.split_documents(documents)<br>llm=HuggingFaceHub(repo_id="google/flan-t5-xxl",
          model_kwargs={"temperature":0.7, "max_length":512})</p>

          '
        raw: "I have 4K rows x 15 columns of txt data that has categorical text information\
          \ in it. I wanted to use Langchain to do information retrieval using the\
          \ following model repo. I did receive this error while implementing the\
          \ below given code snippet \"ValueError: Error raised by inference API:\
          \ Input validation error: `inputs` tokens + `max_new_tokens` must be <=\
          \ 1512. Given: 190761 `inputs` tokens and 20 `max_new_tokens`\" .\r\n\r\n\
          Does 1512 chunk size is the limit for this repo? What can I use max number\
          \ for the chunk size?\r\nIf this is not convenient repo, what would you\
          \ recommend me to use as model while trying to retrieve information for\
          \ the entire dataset I have (4K rows x 15 columns)?\r\n\r\nfrom langchain.text_splitter\
          \ import CharacterTextSplitter\r\ntext_splitter = CharacterTextSplitter(chunk_size=200000,\
          \ chunk_overlap=0)\r\ndocs = text_splitter.split_documents(documents)\r\n\
          llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\"\
          :0.7, \"max_length\":512})"
        updatedAt: '2023-05-03T19:43:02.844Z'
      numEdits: 0
      reactions: []
    id: 6452b946515bd8f741792611
    type: comment
  author: phdykd
  content: "I have 4K rows x 15 columns of txt data that has categorical text information\
    \ in it. I wanted to use Langchain to do information retrieval using the following\
    \ model repo. I did receive this error while implementing the below given code\
    \ snippet \"ValueError: Error raised by inference API: Input validation error:\
    \ `inputs` tokens + `max_new_tokens` must be <= 1512. Given: 190761 `inputs` tokens\
    \ and 20 `max_new_tokens`\" .\r\n\r\nDoes 1512 chunk size is the limit for this\
    \ repo? What can I use max number for the chunk size?\r\nIf this is not convenient\
    \ repo, what would you recommend me to use as model while trying to retrieve information\
    \ for the entire dataset I have (4K rows x 15 columns)?\r\n\r\nfrom langchain.text_splitter\
    \ import CharacterTextSplitter\r\ntext_splitter = CharacterTextSplitter(chunk_size=200000,\
    \ chunk_overlap=0)\r\ndocs = text_splitter.split_documents(documents)\r\nllm=HuggingFaceHub(repo_id=\"\
    google/flan-t5-xxl\", model_kwargs={\"temperature\":0.7, \"max_length\":512})"
  created_at: 2023-05-03 18:43:02+00:00
  edited: false
  hidden: false
  id: 6452b946515bd8f741792611
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c22d043b1970f1b8ca7222f7b6e6d1e3.svg
      fullname: Elham Khabiri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khabiri
      type: user
    createdAt: '2023-05-05T13:37:47.000Z'
    data:
      edited: false
      editors:
      - khabiri
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c22d043b1970f1b8ca7222f7b6e6d1e3.svg
          fullname: Elham Khabiri
          isHf: false
          isPro: false
          name: khabiri
          type: user
        html: '<p>I have the same issue</p>

          '
        raw: I have the same issue
        updatedAt: '2023-05-05T13:37:47.675Z'
      numEdits: 0
      reactions: []
    id: 645506abd55525a4fee54476
    type: comment
  author: khabiri
  content: I have the same issue
  created_at: 2023-05-05 12:37:47+00:00
  edited: false
  hidden: false
  id: 645506abd55525a4fee54476
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
      fullname: Yusuf Kemal Demir
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: phdykd
      type: user
    createdAt: '2023-05-09T17:01:08.000Z'
    data:
      edited: false
      editors:
      - phdykd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a98b9269635f8d2554b839d4da7f5d4.svg
          fullname: Yusuf Kemal Demir
          isHf: false
          isPro: false
          name: phdykd
          type: user
        html: '<p>Who is gonna fix this?</p>

          '
        raw: Who is gonna fix this?
        updatedAt: '2023-05-09T17:01:08.918Z'
      numEdits: 0
      reactions: []
    id: 645a7c54dbf60d37335f1cc4
    type: comment
  author: phdykd
  content: Who is gonna fix this?
  created_at: 2023-05-09 16:01:08+00:00
  edited: false
  hidden: false
  id: 645a7c54dbf60d37335f1cc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
      fullname: Kamalraj M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Kamaljp
      type: user
    createdAt: '2023-05-10T02:51:01.000Z'
    data:
      edited: false
      editors:
      - Kamaljp
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf57e3c6d0a9fa281c4a982b0c93558a.svg
          fullname: Kamalraj M M
          isHf: false
          isPro: false
          name: Kamaljp
          type: user
        html: '<p>The issue is the approach you have taken. Lets try and fix the approach<br>Approach
          1:<br>Since the text data is really tabular data, either csv / tsv it is
          better to use CSVLoader / Dataframe loader to load the file. Then you can
          try using the flan-t5-xxl as the LLM.<br>Approach 2:<br>If you insist that
          the data has to be loaded as text, then convert the tabular data into json
          data. The load the json data as text. Split the text using recursivecharactertext
          splitter. After that use the flan-t5-xxl model. </p>

          <p>The model token limit is hard limit. So don''t expect that to be increased.
          Solution is either work with the given token limit, or look for alternate
          models.</p>

          '
        raw: "The issue is the approach you have taken. Lets try and fix the approach\n\
          Approach 1:\nSince the text data is really tabular data, either csv / tsv\
          \ it is better to use CSVLoader / Dataframe loader to load the file. Then\
          \ you can try using the flan-t5-xxl as the LLM. \nApproach 2:\nIf you insist\
          \ that the data has to be loaded as text, then convert the tabular data\
          \ into json data. The load the json data as text. Split the text using recursivecharactertext\
          \ splitter. After that use the flan-t5-xxl model. \n\nThe model token limit\
          \ is hard limit. So don't expect that to be increased. Solution is either\
          \ work with the given token limit, or look for alternate models."
        updatedAt: '2023-05-10T02:51:01.516Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - Elfsong
    id: 645b0695dbf60d373367c782
    type: comment
  author: Kamaljp
  content: "The issue is the approach you have taken. Lets try and fix the approach\n\
    Approach 1:\nSince the text data is really tabular data, either csv / tsv it is\
    \ better to use CSVLoader / Dataframe loader to load the file. Then you can try\
    \ using the flan-t5-xxl as the LLM. \nApproach 2:\nIf you insist that the data\
    \ has to be loaded as text, then convert the tabular data into json data. The\
    \ load the json data as text. Split the text using recursivecharactertext splitter.\
    \ After that use the flan-t5-xxl model. \n\nThe model token limit is hard limit.\
    \ So don't expect that to be increased. Solution is either work with the given\
    \ token limit, or look for alternate models."
  created_at: 2023-05-10 01:51:01+00:00
  edited: false
  hidden: false
  id: 645b0695dbf60d373367c782
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b68a8d18c616a44b4255ac7eeefeb90.svg
      fullname: Sara.Amd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaraAmd
      type: user
    createdAt: '2023-05-28T23:06:55.000Z'
    data:
      edited: false
      editors:
      - SaraAmd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6b68a8d18c616a44b4255ac7eeefeb90.svg
          fullname: Sara.Amd
          isHf: false
          isPro: false
          name: SaraAmd
          type: user
        html: '<p>can anyone tell me what is the maximum number of tokens Flan-T5
          XXL can handle?</p>

          '
        raw: can anyone tell me what is the maximum number of tokens Flan-T5 XXL can
          handle?
        updatedAt: '2023-05-28T23:06:55.108Z'
      numEdits: 0
      reactions: []
    id: 6473de8f63001a0002d2ca7e
    type: comment
  author: SaraAmd
  content: can anyone tell me what is the maximum number of tokens Flan-T5 XXL can
    handle?
  created_at: 2023-05-28 22:06:55+00:00
  edited: false
  hidden: false
  id: 6473de8f63001a0002d2ca7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c025fb90f20318e31aa7e270da19881.svg
      fullname: Doron Bartov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: doronbt
      type: user
    createdAt: '2023-06-02T07:43:25.000Z'
    data:
      edited: false
      editors:
      - doronbt
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c025fb90f20318e31aa7e270da19881.svg
          fullname: Doron Bartov
          isHf: false
          isPro: false
          name: doronbt
          type: user
        html: '<p>I am getting this error when running this message:<br>caption =
          agent.run("hi there")</p>

          '
        raw: 'I am getting this error when running this message:

          caption = agent.run("hi there")'
        updatedAt: '2023-06-02T07:43:25.931Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - dirtbkr
        - nakul1986
        - Mindjacker
        - adrn-mm
        - M-drh
    id: 64799d9db74892dfb51d78ba
    type: comment
  author: doronbt
  content: 'I am getting this error when running this message:

    caption = agent.run("hi there")'
  created_at: 2023-06-02 06:43:25+00:00
  edited: false
  hidden: false
  id: 64799d9db74892dfb51d78ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6336764d78045d0ef06f7a6e7b10c4c9.svg
      fullname: Aishik Bandyopadhyay
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mindjacker
      type: user
    createdAt: '2023-06-10T10:27:29.000Z'
    data:
      edited: false
      editors:
      - Mindjacker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4848862588405609
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6336764d78045d0ef06f7a6e7b10c4c9.svg
          fullname: Aishik Bandyopadhyay
          isHf: false
          isPro: false
          name: Mindjacker
          type: user
        html: "<blockquote>\n<p>I am getting this error when running this message:<br>caption\
          \ = agent.run(\"hi there\")</p>\n</blockquote>\n<p>same here. This is my\
          \ code : </p>\n<p>```<br>from transformers.tools import HfAgent<br>from\
          \ huggingface_hub import login</p>\n<p>login(\"my api key\")</p>\n<p>agent\
          \ = HfAgent(url_endpoint=\"<a rel=\"nofollow\" href=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5&quot;\"\
          >https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          </a>)<br>print(\"OpenAssistant is initialized \U0001F4AA\")</p>\n<p>resp\
          \ = agent.chat(\"&lt;|prompter|&gt;hello!&lt;|endoftext|&gt;&lt;|assistant|&gt;\"\
          )</p>\n<h1 id=\"resp--agentchathello\">resp = agent.chat(\"hello!\")</h1>\n\
          <p>print(resp) ```</p>\n"
        raw: "> I am getting this error when running this message:\n> caption = agent.run(\"\
          hi there\")\n\nsame here. This is my code : \n```\nfrom transformers.tools\
          \ import HfAgent\nfrom huggingface_hub import login\n\nlogin(\"my api key\"\
          )\n\nagent = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
          )\nprint(\"OpenAssistant is initialized \U0001F4AA\")\n\nresp = agent.chat(\"\
          <|prompter|>hello!<|endoftext|><|assistant|>\")\n# resp = agent.chat(\"\
          hello!\")\n\nprint(resp) ```"
        updatedAt: '2023-06-10T10:27:29.023Z'
      numEdits: 0
      reactions: []
    id: 648450116a4aa6c7e49e87d0
    type: comment
  author: Mindjacker
  content: "> I am getting this error when running this message:\n> caption = agent.run(\"\
    hi there\")\n\nsame here. This is my code : \n```\nfrom transformers.tools import\
    \ HfAgent\nfrom huggingface_hub import login\n\nlogin(\"my api key\")\n\nagent\
    \ = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\"\
    )\nprint(\"OpenAssistant is initialized \U0001F4AA\")\n\nresp = agent.chat(\"\
    <|prompter|>hello!<|endoftext|><|assistant|>\")\n# resp = agent.chat(\"hello!\"\
    )\n\nprint(resp) ```"
  created_at: 2023-06-10 09:27:29+00:00
  edited: false
  hidden: false
  id: 648450116a4aa6c7e49e87d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9216ff9656d7d7cffc3727f4563419b.svg
      fullname: Pranit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PM234
      type: user
    createdAt: '2023-06-10T11:41:19.000Z'
    data:
      edited: false
      editors:
      - PM234
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7625572681427002
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9216ff9656d7d7cffc3727f4563419b.svg
          fullname: Pranit
          isHf: false
          isPro: false
          name: PM234
          type: user
        html: '<p>I tried<br>llm=HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.7,
          "max_length":1024})<br>agent = create_csv_agent(llm,<br>                         ''/content/drive/MyDrive/Dataset.csv'',<br>                         verbose=True)<br>and
          I performed agent.run("how many columns are there ?")<br>but I get the ValueError:
          Error raised by inference API: Input validation error: <code>inputs</code>
          must have less than 1000 tokens. Given: 1396 what could be the solution
          to perform on csv dataset with lang chain agent. Please anyone help. Thanks!</p>

          '
        raw: "I tried \nllm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"\
          temperature\":0.7, \"max_length\":1024})\nagent = create_csv_agent(llm,\
          \ \n                         '/content/drive/MyDrive/Dataset.csv', \n  \
          \                       verbose=True)\nand I performed agent.run(\"how many\
          \ columns are there ?\")\nbut I get the ValueError: Error raised by inference\
          \ API: Input validation error: `inputs` must have less than 1000 tokens.\
          \ Given: 1396 what could be the solution to perform on csv dataset with\
          \ lang chain agent. Please anyone help. Thanks!"
        updatedAt: '2023-06-10T11:41:19.840Z'
      numEdits: 0
      reactions: []
    id: 6484615f4bb88d273c55446d
    type: comment
  author: PM234
  content: "I tried \nllm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"\
    temperature\":0.7, \"max_length\":1024})\nagent = create_csv_agent(llm, \n   \
    \                      '/content/drive/MyDrive/Dataset.csv', \n              \
    \           verbose=True)\nand I performed agent.run(\"how many columns are there\
    \ ?\")\nbut I get the ValueError: Error raised by inference API: Input validation\
    \ error: `inputs` must have less than 1000 tokens. Given: 1396 what could be the\
    \ solution to perform on csv dataset with lang chain agent. Please anyone help.\
    \ Thanks!"
  created_at: 2023-06-10 10:41:19+00:00
  edited: false
  hidden: false
  id: 6484615f4bb88d273c55446d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f9216ff9656d7d7cffc3727f4563419b.svg
      fullname: Pranit
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PM234
      type: user
    createdAt: '2023-06-13T23:15:17.000Z'
    data:
      edited: false
      editors:
      - PM234
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7868777513504028
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f9216ff9656d7d7cffc3727f4563419b.svg
          fullname: Pranit
          isHf: false
          isPro: false
          name: PM234
          type: user
        html: '<blockquote>

          <p>I tried<br>llm=HuggingFaceHub(repo_id="google/flan-t5-xxl", model_kwargs={"temperature":0.7,
          "max_length":1024})<br>agent = create_csv_agent(llm,<br>                         ''/content/drive/MyDrive/Dataset.csv'',<br>                         verbose=True)<br>and
          I performed agent.run("how many columns are there ?")<br>but I get the ValueError:
          Error raised by inference API: Input validation error: <code>inputs</code>
          must have less than 1000 tokens. Given: 1396 what could be the solution
          to perform on csv dataset with lang chain agent. Please anyone help. Thanks!</p>

          </blockquote>

          <p>Actually there''s issue with using other models passing for create_csv_agent()
          function so while I tried reducing columns and rows then it gives parsing
          output error for csv. So I reckon that there''s issue with other models
          used.</p>

          '
        raw: "> I tried \n> llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"\
          temperature\":0.7, \"max_length\":1024})\n> agent = create_csv_agent(llm,\
          \ \n>                          '/content/drive/MyDrive/Dataset.csv', \n\
          >                          verbose=True)\n> and I performed agent.run(\"\
          how many columns are there ?\")\n> but I get the ValueError: Error raised\
          \ by inference API: Input validation error: `inputs` must have less than\
          \ 1000 tokens. Given: 1396 what could be the solution to perform on csv\
          \ dataset with lang chain agent. Please anyone help. Thanks!\n\nActually\
          \ there's issue with using other models passing for create_csv_agent() function\
          \ so while I tried reducing columns and rows then it gives parsing output\
          \ error for csv. So I reckon that there's issue with other models used."
        updatedAt: '2023-06-13T23:15:17.532Z'
      numEdits: 0
      reactions: []
    id: 6488f8858007c2c7ece146c1
    type: comment
  author: PM234
  content: "> I tried \n> llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"\
    temperature\":0.7, \"max_length\":1024})\n> agent = create_csv_agent(llm, \n>\
    \                          '/content/drive/MyDrive/Dataset.csv', \n>         \
    \                 verbose=True)\n> and I performed agent.run(\"how many columns\
    \ are there ?\")\n> but I get the ValueError: Error raised by inference API: Input\
    \ validation error: `inputs` must have less than 1000 tokens. Given: 1396 what\
    \ could be the solution to perform on csv dataset with lang chain agent. Please\
    \ anyone help. Thanks!\n\nActually there's issue with using other models passing\
    \ for create_csv_agent() function so while I tried reducing columns and rows then\
    \ it gives parsing output error for csv. So I reckon that there's issue with other\
    \ models used."
  created_at: 2023-06-13 22:15:17+00:00
  edited: false
  hidden: false
  id: 6488f8858007c2c7ece146c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/325622efe7d5b90cd3ee08de9e7c6683.svg
      fullname: Muhammed Fawas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FAWAS97
      type: user
    createdAt: '2023-06-22T08:42:13.000Z'
    data:
      edited: false
      editors:
      - FAWAS97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9945204257965088
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/325622efe7d5b90cd3ee08de9e7c6683.svg
          fullname: Muhammed Fawas
          isHf: false
          isPro: false
          name: FAWAS97
          type: user
        html: '<p>is this issue solved?</p>

          '
        raw: 'is this issue solved?

          '
        updatedAt: '2023-06-22T08:42:13.864Z'
      numEdits: 0
      reactions: []
    id: 6494096574db6cc30f48e432
    type: comment
  author: FAWAS97
  content: 'is this issue solved?

    '
  created_at: 2023-06-22 07:42:13+00:00
  edited: false
  hidden: false
  id: 6494096574db6cc30f48e432
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/acbf1a997e5d81313310593af91e88f8.svg
      fullname: Ouafae KARMOUDA
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ouafae
      type: user
    createdAt: '2023-06-23T16:03:40.000Z'
    data:
      edited: false
      editors:
      - Ouafae
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9191655516624451
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/acbf1a997e5d81313310593af91e88f8.svg
          fullname: Ouafae KARMOUDA
          isHf: false
          isPro: false
          name: Ouafae
          type: user
        html: '<p>I''m currently using the Hugging Face Inference API to generate
          text using the flan-t5-xxl model. My request involves the generation of
          250 tokens.</p>

          <p>Is there a built-in limit to the number of tokens that can be generated
          in a single request or could it be due to some other reasons like the model
          itself stopping earlier? Any guidance on this would be greatly appreciated.</p>

          '
        raw: 'I''m currently using the Hugging Face Inference API to generate text
          using the flan-t5-xxl model. My request involves the generation of 250 tokens.


          Is there a built-in limit to the number of tokens that can be generated
          in a single request or could it be due to some other reasons like the model
          itself stopping earlier? Any guidance on this would be greatly appreciated.'
        updatedAt: '2023-06-23T16:03:40.018Z'
      numEdits: 0
      reactions: []
    id: 6495c25cd1b80976be6da67a
    type: comment
  author: Ouafae
  content: 'I''m currently using the Hugging Face Inference API to generate text using
    the flan-t5-xxl model. My request involves the generation of 250 tokens.


    Is there a built-in limit to the number of tokens that can be generated in a single
    request or could it be due to some other reasons like the model itself stopping
    earlier? Any guidance on this would be greatly appreciated.'
  created_at: 2023-06-23 15:03:40+00:00
  edited: false
  hidden: false
  id: 6495c25cd1b80976be6da67a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98c48a73bd7ff644bcbd36beac082a88.svg
      fullname: a
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: itunscode
      type: user
    createdAt: '2023-07-09T04:57:16.000Z'
    data:
      edited: true
      editors:
      - itunscode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5852922201156616
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98c48a73bd7ff644bcbd36beac082a88.svg
          fullname: a
          isHf: false
          isPro: false
          name: itunscode
          type: user
        html: '<p><code>pip install torch</code><br><code>pip install sentence-transformers</code></p>

          '
        raw: '`pip install torch`

          `pip install sentence-transformers`'
        updatedAt: '2023-07-09T04:57:43.495Z'
      numEdits: 1
      reactions: []
    id: 64aa3e2ccc73827ce691511f
    type: comment
  author: itunscode
  content: '`pip install torch`

    `pip install sentence-transformers`'
  created_at: 2023-07-09 03:57:16+00:00
  edited: true
  hidden: false
  id: 64aa3e2ccc73827ce691511f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: google/flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: 'ValueError: Error raised by inference API: Input validation error: `inputs`
  tokens + `max_new_tokens` must be <= 1512. Given: 190761 `inputs` tokens and 20
  `max_new_tokens`'
