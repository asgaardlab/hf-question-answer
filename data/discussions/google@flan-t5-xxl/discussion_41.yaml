!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lovodkin93
conflicting_files: null
created_at: 2023-04-04 07:00:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/242e344dca08057bdf1eef09f69b41b2.svg
      fullname: Aviv Slobodkin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lovodkin93
      type: user
    createdAt: '2023-04-04T08:00:29.000Z'
    data:
      edited: false
      editors:
      - lovodkin93
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/242e344dca08057bdf1eef09f69b41b2.svg
          fullname: Aviv Slobodkin
          isHf: false
          isPro: false
          name: lovodkin93
          type: user
        html: '<p>Hello,<br>I saw that the model''s model_max_length is 512, which
          makes it difficult to use the model in in-context-learning setting for tasks
          involving longer than a sentence inputs (such as reading-comprehension tasks).<br>I
          also saw that the flan-ul2 has been updated to support longer inputs (from
          512 tokens to 2048).<br>Therefore, I was wondering if there are any plans
          to update the flan-f5-xxl model as well?<br>Thanks!</p>

          '
        raw: "Hello,\r\nI saw that the model's model_max_length is 512, which makes\
          \ it difficult to use the model in in-context-learning setting for tasks\
          \ involving longer than a sentence inputs (such as reading-comprehension\
          \ tasks).\r\nI also saw that the flan-ul2 has been updated to support longer\
          \ inputs (from 512 tokens to 2048).\r\nTherefore, I was wondering if there\
          \ are any plans to update the flan-f5-xxl model as well?\r\nThanks!"
        updatedAt: '2023-04-04T08:00:29.244Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - rachith
        - slopezsantamaria
    id: 642bd91d5edcc5760cb1b14a
    type: comment
  author: lovodkin93
  content: "Hello,\r\nI saw that the model's model_max_length is 512, which makes\
    \ it difficult to use the model in in-context-learning setting for tasks involving\
    \ longer than a sentence inputs (such as reading-comprehension tasks).\r\nI also\
    \ saw that the flan-ul2 has been updated to support longer inputs (from 512 tokens\
    \ to 2048).\r\nTherefore, I was wondering if there are any plans to update the\
    \ flan-f5-xxl model as well?\r\nThanks!"
  created_at: 2023-04-04 07:00:29+00:00
  edited: false
  hidden: false
  id: 642bd91d5edcc5760cb1b14a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg
      fullname: Shayne Longpre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shayne
      type: user
    createdAt: '2023-04-05T20:37:40.000Z'
    data:
      edited: false
      editors:
      - Shayne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg
          fullname: Shayne Longpre
          isHf: false
          isPro: false
          name: Shayne
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lovodkin93&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lovodkin93\">@<span class=\"\
          underline\">lovodkin93</span></a></span>\n\n\t</span></span> The Flan-T5\
          \ models were trained on 2k token input windows and 512 output windows so\
          \ should be able to manage pretty long in-context sequences. I'm not sure\
          \ if there is a sequence length window imposed on this API?</p>\n"
        raw: '@lovodkin93 The Flan-T5 models were trained on 2k token input windows
          and 512 output windows so should be able to manage pretty long in-context
          sequences. I''m not sure if there is a sequence length window imposed on
          this API?'
        updatedAt: '2023-04-05T20:37:40.290Z'
      numEdits: 0
      reactions: []
    id: 642ddc14baf943d5db4678d2
    type: comment
  author: Shayne
  content: '@lovodkin93 The Flan-T5 models were trained on 2k token input windows
    and 512 output windows so should be able to manage pretty long in-context sequences.
    I''m not sure if there is a sequence length window imposed on this API?'
  created_at: 2023-04-05 19:37:40+00:00
  edited: false
  hidden: false
  id: 642ddc14baf943d5db4678d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/242e344dca08057bdf1eef09f69b41b2.svg
      fullname: Aviv Slobodkin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lovodkin93
      type: user
    createdAt: '2023-04-06T08:58:12.000Z'
    data:
      edited: false
      editors:
      - lovodkin93
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/242e344dca08057bdf1eef09f69b41b2.svg
          fullname: Aviv Slobodkin
          isHf: false
          isPro: false
          name: lovodkin93
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Shayne&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Shayne\">@<span class=\"\
          underline\">Shayne</span></a></span>\n\n\t</span></span> There is.<br>The\
          \ tokenizer's <code>model_max_length</code> parameter is set by default\
          \ to 512.<br>So I wanted to make sure whether updating it to 2048 would\
          \ be something the model can handle.</p>\n"
        raw: '@Shayne There is.

          The tokenizer''s `model_max_length` parameter is set by default to 512.

          So I wanted to make sure whether updating it to 2048 would be something
          the model can handle.'
        updatedAt: '2023-04-06T08:58:12.872Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - samarthagarwal23
        - karthik2909
        - AxAI
        - RedOneAI
    id: 642e89a45e76acc792c884bb
    type: comment
  author: lovodkin93
  content: '@Shayne There is.

    The tokenizer''s `model_max_length` parameter is set by default to 512.

    So I wanted to make sure whether updating it to 2048 would be something the model
    can handle.'
  created_at: 2023-04-06 07:58:12+00:00
  edited: false
  hidden: false
  id: 642e89a45e76acc792c884bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6b68a8d18c616a44b4255ac7eeefeb90.svg
      fullname: Sara.Amd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SaraAmd
      type: user
    createdAt: '2023-06-13T19:58:28.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/6b68a8d18c616a44b4255ac7eeefeb90.svg
          fullname: Sara.Amd
          isHf: false
          isPro: false
          name: SaraAmd
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-22T13:43:34.287Z'
      numEdits: 0
      reactions: []
    id: 6488ca647eb771e1644357b4
    type: comment
  author: SaraAmd
  content: This comment has been hidden
  created_at: 2023-06-13 18:58:28+00:00
  edited: true
  hidden: true
  id: 6488ca647eb771e1644357b4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/339cfd6a4c8b272f284c7f547de39739.svg
      fullname: Tushar Bokade
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Suspense1441
      type: user
    createdAt: '2023-06-29T12:59:38.000Z'
    data:
      edited: false
      editors:
      - Suspense1441
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5123412013053894
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/339cfd6a4c8b272f284c7f547de39739.svg
          fullname: Tushar Bokade
          isHf: false
          isPro: false
          name: Suspense1441
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;SaraAmd&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SaraAmd\"\
          >@<span class=\"underline\">SaraAmd</span></a></span>\n\n\t</span></span>\
          \  <span data-props=\"{&quot;user&quot;:&quot;lovodkin93&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lovodkin93\">@<span class=\"\
          underline\">lovodkin93</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;Shayne&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Shayne\">@<span class=\"underline\">Shayne</span></a></span>\n\
          \n\t</span></span> , any updates regarding the maximum sequence length flan\
          \ t5 XL (3 billion) can handle?</p>\n"
        raw: 'Hey @SaraAmd  @lovodkin93 @Shayne , any updates regarding the maximum
          sequence length flan t5 XL (3 billion) can handle?

          '
        updatedAt: '2023-06-29T12:59:38.642Z'
      numEdits: 0
      reactions: []
    id: 649d803a4e08d59d20048bca
    type: comment
  author: Suspense1441
  content: 'Hey @SaraAmd  @lovodkin93 @Shayne , any updates regarding the maximum
    sequence length flan t5 XL (3 billion) can handle?

    '
  created_at: 2023-06-29 11:59:38+00:00
  edited: false
  hidden: false
  id: 649d803a4e08d59d20048bca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/27422da0ba6ceb82e4908756c1cdc0db.svg
      fullname: brianna bromell
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Briannab3d
      type: user
    createdAt: '2023-07-04T13:18:54.000Z'
    data:
      edited: false
      editors:
      - Briannab3d
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9676607847213745
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/27422da0ba6ceb82e4908756c1cdc0db.svg
          fullname: brianna bromell
          isHf: false
          isPro: false
          name: Briannab3d
          type: user
        html: '<p>Please correct me if I''m wrong as I struggle with this as well,
          the maximum sequence length for the t5 tokenizer is 512 but the maximum
          sequence link for the model itself is theoretically unlimited with an ideal
          comprehension at 2048 tokens and lower.</p>

          <p>I can''t seem to figure out how to do chunks but so far this is what
          I''ve gathered</p>

          '
        raw: 'Please correct me if I''m wrong as I struggle with this as well, the
          maximum sequence length for the t5 tokenizer is 512 but the maximum sequence
          link for the model itself is theoretically unlimited with an ideal comprehension
          at 2048 tokens and lower.


          I can''t seem to figure out how to do chunks but so far this is what I''ve
          gathered'
        updatedAt: '2023-07-04T13:18:54.309Z'
      numEdits: 0
      reactions: []
    id: 64a41c3e33ded98516bd2dba
    type: comment
  author: Briannab3d
  content: 'Please correct me if I''m wrong as I struggle with this as well, the maximum
    sequence length for the t5 tokenizer is 512 but the maximum sequence link for
    the model itself is theoretically unlimited with an ideal comprehension at 2048
    tokens and lower.


    I can''t seem to figure out how to do chunks but so far this is what I''ve gathered'
  created_at: 2023-07-04 12:18:54+00:00
  edited: false
  hidden: false
  id: 64a41c3e33ded98516bd2dba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e858635ce58ca6623648fc2b8a62d82a.svg
      fullname: Suman Ghosh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Sumantechie123
      type: user
    createdAt: '2023-08-18T05:57:20.000Z'
    data:
      edited: false
      editors:
      - Sumantechie123
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7851248979568481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e858635ce58ca6623648fc2b8a62d82a.svg
          fullname: Suman Ghosh
          isHf: false
          isPro: false
          name: Sumantechie123
          type: user
        html: '<p>Is there any update on the token size?<br>Anyone able to use more
          than 512 tokens?</p>

          '
        raw: 'Is there any update on the token size?

          Anyone able to use more than 512 tokens?'
        updatedAt: '2023-08-18T05:57:20.770Z'
      numEdits: 0
      reactions: []
    id: 64df08403d3a7519f19401cf
    type: comment
  author: Sumantechie123
  content: 'Is there any update on the token size?

    Anyone able to use more than 512 tokens?'
  created_at: 2023-08-18 04:57:20+00:00
  edited: false
  hidden: false
  id: 64df08403d3a7519f19401cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642358216e61cda1b3a137c7/jnd78Je2IgLB8gKEFL_N2.jpeg?w=200&h=200&f=face
      fullname: yuanzhedong
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yzdyzdyzd
      type: user
    createdAt: '2023-08-22T03:14:38.000Z'
    data:
      edited: false
      editors:
      - yzdyzdyzd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9101163148880005
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642358216e61cda1b3a137c7/jnd78Je2IgLB8gKEFL_N2.jpeg?w=200&h=200&f=face
          fullname: yuanzhedong
          isHf: false
          isPro: false
          name: yzdyzdyzd
          type: user
        html: '<p>same question here, thanks!</p>

          '
        raw: same question here, thanks!
        updatedAt: '2023-08-22T03:14:38.050Z'
      numEdits: 0
      reactions: []
    id: 64e4281e2fbff6ed9c17260e
    type: comment
  author: yzdyzdyzd
  content: same question here, thanks!
  created_at: 2023-08-22 02:14:38+00:00
  edited: false
  hidden: false
  id: 64e4281e2fbff6ed9c17260e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7af7529042ea7e023a46e315ecd13559.svg
      fullname: Umesh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: umesh-c
      type: user
    createdAt: '2023-08-23T09:39:56.000Z'
    data:
      edited: false
      editors:
      - umesh-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723020792007446
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7af7529042ea7e023a46e315ecd13559.svg
          fullname: Umesh
          isHf: false
          isPro: false
          name: umesh-c
          type: user
        html: '<p>Guys this is very important requirement. Can we get some traction
          on this please? </p>

          '
        raw: 'Guys this is very important requirement. Can we get some traction on
          this please? '
        updatedAt: '2023-08-23T09:39:56.877Z'
      numEdits: 0
      reactions:
      - count: 7
        reaction: "\U0001F44D"
        users:
        - slopezsantamaria
        - cristian-rodriguez
        - ptrdvn
        - Pkoosha
        - rjtmehta99
        - xhluca
        - niksharm
    id: 64e5d3ecaab459f43466e44e
    type: comment
  author: umesh-c
  content: 'Guys this is very important requirement. Can we get some traction on this
    please? '
  created_at: 2023-08-23 08:39:56+00:00
  edited: false
  hidden: false
  id: 64e5d3ecaab459f43466e44e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/370ec63c5ab2226fd5f749533543fdc6.svg
      fullname: Pouya Kousha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Pkoosha
      type: user
    createdAt: '2023-09-24T02:45:45.000Z'
    data:
      edited: false
      editors:
      - Pkoosha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9939785599708557
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/370ec63c5ab2226fd5f749533543fdc6.svg
          fullname: Pouya Kousha
          isHf: false
          isPro: false
          name: Pkoosha
          type: user
        html: '<p>I have the same question here. </p>

          '
        raw: 'I have the same question here. '
        updatedAt: '2023-09-24T02:45:45.513Z'
      numEdits: 0
      reactions: []
    id: 650fa2d98d01590937d7ae01
    type: comment
  author: Pkoosha
  content: 'I have the same question here. '
  created_at: 2023-09-24 01:45:45+00:00
  edited: false
  hidden: false
  id: 650fa2d98d01590937d7ae01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-09-25T12:29:01.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9350206851959229
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>hi everyone,<br>thanks a lot for your interest and your questions.\
          \ I think that you can use flan-t5 for much longer sequences out of the\
          \ box - in the T5 modeling script nothing depends on <code>tokenizer.model_max_length</code>.\
          \  I think that <code>tokenizer.model_max_length</code> was a code legacy\
          \ as the tokenizer has been copied over from previous t5 models.<br>As a\
          \ rule of thumb it is recommended to not run with a large sequence length\
          \ than the one authors used for training (which I cannot find on their original\
          \ paper). I have run a summarization example on a long sequence (&gt; 512)\
          \ and it seems to work fine:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n\
          \nmodel_id = <span class=\"hljs-string\">\"google/flan-t5-xxl\"</span>\n\
          \nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=<span class=\"\
          hljs-literal\">True</span>,\n    bnb_4bit_use_double_quant=<span class=\"\
          hljs-literal\">False</span>\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id,\
          \ quantization_config=quantization_config)\ntok = AutoTokenizer.from_pretrained(model_id)\n\
          \n\ntext = <span class=\"hljs-string\">\"\"\"Summarize the following news\
          \ article in detail:\\n</span>\n<span class=\"hljs-string\">A capsule containing\
          \ precious samples from an asteroid landed safely on Earth on Sunday, the\
          \ culmination of a roughly 4-billion-mile journey over the past seven years.</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">The asteroid\
          \ samples were collected by NASAs OSIRIS-REx spacecraft, which flew by Earth\
          \ early Sunday morning and jettisoned the capsule over a designated landing\
          \ zone in the Utah desert. The unofficial touchdown time was 8:52 a.m. MT,\
          \ 3 minutes ahead of the predicted landing time.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">The dramatic event \u2014\
          \ which the NASA livestream narrator described as \u201Copening a time capsule\
          \ to our ancient solar system\u201D \u2014 marked a major milestone for\
          \ the United States: The collected rocks and soil were NASAs first samples\
          \ brought back to Earth from an asteroid. Experts have said the bounty could\
          \ help scientists unlock secrets about the solar system and how it came\
          \ to be, including how life emerged on this planet.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">Bruce Betts, chief scientist\
          \ at The Planetary Society, a nonprofit organization that conducts research,\
          \ advocacy and outreach to promote space exploration, congratulated the\
          \ NASA team on what he called an \u201Cimpressive and very complicated mission,\u201D\
          \ adding that the asteroid samples are the start of a thrilling new chapter\
          \ in space history.</span>\n<span class=\"hljs-string\"></span>\n<span class=\"\
          hljs-string\">\u201CIt's exciting because this mission launched in 2016\
          \ and so there's a feeling of, Wow, this day has finally come,'\u201D he\
          \ said. \u201CBut scientifically, it's exciting because this is an amazing\
          \ opportunity to study a very complex story that goes way back to the dawn\
          \ of the solar system.\u201D</span>\n<span class=\"hljs-string\"></span>\n\
          <span class=\"hljs-string\">The samples were gathered from the surface of\
          \ a near-Earth asteroid known as Bennu. The space rock, which is roughly\
          \ as tall as the Empire State Building, is located more than 200 million\
          \ miles away from Earth but orbits in such a way that it occasionally swings\
          \ within 4.6 million miles of the planet.</span>\n<span class=\"hljs-string\"\
          ></span>\n<span class=\"hljs-string\">Bennu's main draw owes to its age.\
          \ The asteroid is estimated to have formed in the first 10 million years\
          \ of the solar system's existence, making it a pristine remnant from a chaotic\
          \ time more than 4.5 billion years ago. As such, studying an asteroid's\
          \ chemical and physical properties is thought to be one of the best ways\
          \ to understand the earliest days of the solar system.</span>\n<span class=\"\
          hljs-string\"></span>\n<span class=\"hljs-string\">\u201CThey're pretty\
          \ well untouched from right around 4.5 billion years ago,\u201D Betts said.\
          \ \u201CTo get insights into these rocks gives real power to not just the\
          \ science of asteroids but to everything in our solar system.\u201D</span>\n\
          <span class=\"hljs-string\"></span>\n<span class=\"hljs-string\">Researchers\
          \ are keen to understand what role \u2014 if any \u2014 asteroids played\
          \ in the emergence of life on Earth. There are theories, for instance, that\
          \ asteroids and comets may have delivered water and other building blocks\
          \ of life to the planet.</span>\n<span class=\"hljs-string\"></span>\n<span\
          \ class=\"hljs-string\">Bennu has also been of interest to scientists because,\
          \ like other near-Earth asteroids, it is classified as a potentially hazardous\
          \ object. NASA's Planetary Defense Coordination Office previously estimated\
          \ that there is a 1 in 2,700 chance of Bennu slamming into Earth sometime\
          \ between the years 2175 and 2199.</span>\n<span class=\"hljs-string\">\"\
          \"\"</span>\n\ninput_ids = tok(text, return_tensors=<span class=\"hljs-string\"\
          >\"pt\"</span>, padding=<span class=\"hljs-literal\">True</span>).to(<span\
          \ class=\"hljs-number\">0</span>)\n\nout = model.generate(**input_ids, max_new_tokens=<span\
          \ class=\"hljs-number\">100</span>, do_sample=<span class=\"hljs-literal\"\
          >False</span>)\n<span class=\"hljs-built_in\">print</span>(tok.batch_decode(out,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>))\n&gt;&gt;&gt;[<span\
          \ class=\"hljs-string\">\"The samples were collected by NASA's OSIRIS-REx\
          \ spacecraft, which flew by Earth early Sunday morning and jettisoned the\
          \ capsule over a designated landing zone in the Utah desert.\"</span>]\n\
          </code></pre>\n<p>If you want to disable the warning you can just set <code>tokenizer.model_max_length=4096</code></p>\n"
        raw: "hi everyone, \nthanks a lot for your interest and your questions. I\
          \ think that you can use flan-t5 for much longer sequences out of the box\
          \ - in the T5 modeling script nothing depends on `tokenizer.model_max_length`.\
          \  I think that `tokenizer.model_max_length` was a code legacy as the tokenizer\
          \ has been copied over from previous t5 models. \nAs a rule of thumb it\
          \ is recommended to not run with a large sequence length than the one authors\
          \ used for training (which I cannot find on their original paper). I have\
          \ run a summarization example on a long sequence (> 512) and it seems to\
          \ work fine:\n\n```python\nfrom transformers import AutoTokenizer, BitsAndBytesConfig,\
          \ AutoModelForSeq2SeqLM\n\nmodel_id = \"google/flan-t5-xxl\"\n\nquantization_config\
          \ = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False\n\
          )\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, quantization_config=quantization_config)\n\
          tok = AutoTokenizer.from_pretrained(model_id)\n\n\ntext = \"\"\"Summarize\
          \ the following news article in detail:\\n\nA capsule containing precious\
          \ samples from an asteroid landed safely on Earth on Sunday, the culmination\
          \ of a roughly 4-billion-mile journey over the past seven years.\n\nThe\
          \ asteroid samples were collected by NASAs OSIRIS-REx spacecraft, which\
          \ flew by Earth early Sunday morning and jettisoned the capsule over a designated\
          \ landing zone in the Utah desert. The unofficial touchdown time was 8:52\
          \ a.m. MT, 3 minutes ahead of the predicted landing time.\n\nThe dramatic\
          \ event \u2014 which the NASA livestream narrator described as \u201Copening\
          \ a time capsule to our ancient solar system\u201D \u2014 marked a major\
          \ milestone for the United States: The collected rocks and soil were NASAs\
          \ first samples brought back to Earth from an asteroid. Experts have said\
          \ the bounty could help scientists unlock secrets about the solar system\
          \ and how it came to be, including how life emerged on this planet.\n\n\
          Bruce Betts, chief scientist at The Planetary Society, a nonprofit organization\
          \ that conducts research, advocacy and outreach to promote space exploration,\
          \ congratulated the NASA team on what he called an \u201Cimpressive and\
          \ very complicated mission,\u201D adding that the asteroid samples are the\
          \ start of a thrilling new chapter in space history.\n\n\u201CIt's exciting\
          \ because this mission launched in 2016 and so there's a feeling of, Wow,\
          \ this day has finally come,'\u201D he said. \u201CBut scientifically, it's\
          \ exciting because this is an amazing opportunity to study a very complex\
          \ story that goes way back to the dawn of the solar system.\u201D\n\nThe\
          \ samples were gathered from the surface of a near-Earth asteroid known\
          \ as Bennu. The space rock, which is roughly as tall as the Empire State\
          \ Building, is located more than 200 million miles away from Earth but orbits\
          \ in such a way that it occasionally swings within 4.6 million miles of\
          \ the planet.\n\nBennu's main draw owes to its age. The asteroid is estimated\
          \ to have formed in the first 10 million years of the solar system's existence,\
          \ making it a pristine remnant from a chaotic time more than 4.5 billion\
          \ years ago. As such, studying an asteroid's chemical and physical properties\
          \ is thought to be one of the best ways to understand the earliest days\
          \ of the solar system.\n\n\u201CThey're pretty well untouched from right\
          \ around 4.5 billion years ago,\u201D Betts said. \u201CTo get insights\
          \ into these rocks gives real power to not just the science of asteroids\
          \ but to everything in our solar system.\u201D\n\nResearchers are keen to\
          \ understand what role \u2014 if any \u2014 asteroids played in the emergence\
          \ of life on Earth. There are theories, for instance, that asteroids and\
          \ comets may have delivered water and other building blocks of life to the\
          \ planet.\n\nBennu has also been of interest to scientists because, like\
          \ other near-Earth asteroids, it is classified as a potentially hazardous\
          \ object. NASA's Planetary Defense Coordination Office previously estimated\
          \ that there is a 1 in 2,700 chance of Bennu slamming into Earth sometime\
          \ between the years 2175 and 2199.\n\"\"\"\n\ninput_ids = tok(text, return_tensors=\"\
          pt\", padding=True).to(0)\n\nout = model.generate(**input_ids, max_new_tokens=100,\
          \ do_sample=False)\nprint(tok.batch_decode(out, skip_special_tokens=True))\n\
          >>>[\"The samples were collected by NASA's OSIRIS-REx spacecraft, which\
          \ flew by Earth early Sunday morning and jettisoned the capsule over a designated\
          \ landing zone in the Utah desert.\"]\n```\n\nIf you want to disable the\
          \ warning you can just set `tokenizer.model_max_length=4096`"
        updatedAt: '2023-09-25T12:29:01.713Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - xhluca
        - niksharm
        - lysandre
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - constantinSch
    id: 65117d0d33ddefa58fee136f
    type: comment
  author: ybelkada
  content: "hi everyone, \nthanks a lot for your interest and your questions. I think\
    \ that you can use flan-t5 for much longer sequences out of the box - in the T5\
    \ modeling script nothing depends on `tokenizer.model_max_length`.  I think that\
    \ `tokenizer.model_max_length` was a code legacy as the tokenizer has been copied\
    \ over from previous t5 models. \nAs a rule of thumb it is recommended to not\
    \ run with a large sequence length than the one authors used for training (which\
    \ I cannot find on their original paper). I have run a summarization example on\
    \ a long sequence (> 512) and it seems to work fine:\n\n```python\nfrom transformers\
    \ import AutoTokenizer, BitsAndBytesConfig, AutoModelForSeq2SeqLM\n\nmodel_id\
    \ = \"google/flan-t5-xxl\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n\
    \    bnb_4bit_use_double_quant=False\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id,\
    \ quantization_config=quantization_config)\ntok = AutoTokenizer.from_pretrained(model_id)\n\
    \n\ntext = \"\"\"Summarize the following news article in detail:\\n\nA capsule\
    \ containing precious samples from an asteroid landed safely on Earth on Sunday,\
    \ the culmination of a roughly 4-billion-mile journey over the past seven years.\n\
    \nThe asteroid samples were collected by NASAs OSIRIS-REx spacecraft, which flew\
    \ by Earth early Sunday morning and jettisoned the capsule over a designated landing\
    \ zone in the Utah desert. The unofficial touchdown time was 8:52 a.m. MT, 3 minutes\
    \ ahead of the predicted landing time.\n\nThe dramatic event \u2014 which the\
    \ NASA livestream narrator described as \u201Copening a time capsule to our ancient\
    \ solar system\u201D \u2014 marked a major milestone for the United States: The\
    \ collected rocks and soil were NASAs first samples brought back to Earth from\
    \ an asteroid. Experts have said the bounty could help scientists unlock secrets\
    \ about the solar system and how it came to be, including how life emerged on\
    \ this planet.\n\nBruce Betts, chief scientist at The Planetary Society, a nonprofit\
    \ organization that conducts research, advocacy and outreach to promote space\
    \ exploration, congratulated the NASA team on what he called an \u201Cimpressive\
    \ and very complicated mission,\u201D adding that the asteroid samples are the\
    \ start of a thrilling new chapter in space history.\n\n\u201CIt's exciting because\
    \ this mission launched in 2016 and so there's a feeling of, Wow, this day has\
    \ finally come,'\u201D he said. \u201CBut scientifically, it's exciting because\
    \ this is an amazing opportunity to study a very complex story that goes way back\
    \ to the dawn of the solar system.\u201D\n\nThe samples were gathered from the\
    \ surface of a near-Earth asteroid known as Bennu. The space rock, which is roughly\
    \ as tall as the Empire State Building, is located more than 200 million miles\
    \ away from Earth but orbits in such a way that it occasionally swings within\
    \ 4.6 million miles of the planet.\n\nBennu's main draw owes to its age. The asteroid\
    \ is estimated to have formed in the first 10 million years of the solar system's\
    \ existence, making it a pristine remnant from a chaotic time more than 4.5 billion\
    \ years ago. As such, studying an asteroid's chemical and physical properties\
    \ is thought to be one of the best ways to understand the earliest days of the\
    \ solar system.\n\n\u201CThey're pretty well untouched from right around 4.5 billion\
    \ years ago,\u201D Betts said. \u201CTo get insights into these rocks gives real\
    \ power to not just the science of asteroids but to everything in our solar system.\u201D\
    \n\nResearchers are keen to understand what role \u2014 if any \u2014 asteroids\
    \ played in the emergence of life on Earth. There are theories, for instance,\
    \ that asteroids and comets may have delivered water and other building blocks\
    \ of life to the planet.\n\nBennu has also been of interest to scientists because,\
    \ like other near-Earth asteroids, it is classified as a potentially hazardous\
    \ object. NASA's Planetary Defense Coordination Office previously estimated that\
    \ there is a 1 in 2,700 chance of Bennu slamming into Earth sometime between the\
    \ years 2175 and 2199.\n\"\"\"\n\ninput_ids = tok(text, return_tensors=\"pt\"\
    , padding=True).to(0)\n\nout = model.generate(**input_ids, max_new_tokens=100,\
    \ do_sample=False)\nprint(tok.batch_decode(out, skip_special_tokens=True))\n>>>[\"\
    The samples were collected by NASA's OSIRIS-REx spacecraft, which flew by Earth\
    \ early Sunday morning and jettisoned the capsule over a designated landing zone\
    \ in the Utah desert.\"]\n```\n\nIf you want to disable the warning you can just\
    \ set `tokenizer.model_max_length=4096`"
  created_at: 2023-09-25 11:29:01+00:00
  edited: false
  hidden: false
  id: 65117d0d33ddefa58fee136f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7af7529042ea7e023a46e315ecd13559.svg
      fullname: Umesh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: umesh-c
      type: user
    createdAt: '2023-10-05T06:35:28.000Z'
    data:
      edited: false
      editors:
      - umesh-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9277500510215759
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7af7529042ea7e023a46e315ecd13559.svg
          fullname: Umesh
          isHf: false
          isPro: false
          name: umesh-c
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> Thanks for the\
          \ pointers. Sorry I was not clear in my comment above, the concern was not\
          \ for inout sequence length, but more of an output sequence length which\
          \ is more important IMO. Currently if we try to generate the output longer\
          \ than 512, this model truncates the output after reaching 512 tokens and\
          \ left the output untraceable. I can see other models out there which support\
          \ more than 512 tokens output out of the box. Thanks!</p>\n"
        raw: '@ybelkada Thanks for the pointers. Sorry I was not clear in my comment
          above, the concern was not for inout sequence length, but more of an output
          sequence length which is more important IMO. Currently if we try to generate
          the output longer than 512, this model truncates the output after reaching
          512 tokens and left the output untraceable. I can see other models out there
          which support more than 512 tokens output out of the box. Thanks!'
        updatedAt: '2023-10-05T06:35:28.691Z'
      numEdits: 0
      reactions: []
    id: 651e5930a4f3020868713c24
    type: comment
  author: umesh-c
  content: '@ybelkada Thanks for the pointers. Sorry I was not clear in my comment
    above, the concern was not for inout sequence length, but more of an output sequence
    length which is more important IMO. Currently if we try to generate the output
    longer than 512, this model truncates the output after reaching 512 tokens and
    left the output untraceable. I can see other models out there which support more
    than 512 tokens output out of the box. Thanks!'
  created_at: 2023-10-05 05:35:28+00:00
  edited: false
  hidden: false
  id: 651e5930a4f3020868713c24
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-05T07:32:37.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7377291321754456
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>I see <span data-props=\"{&quot;user&quot;:&quot;umesh-c&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/umesh-c\"\
          >@<span class=\"underline\">umesh-c</span></a></span>\n\n\t</span></span>\
          \ , if you run my script above with <code>max_new_tokens=1000</code> and\
          \ <code>eos_token_id=-1</code> (to generate unlimited sequences) do you\
          \ still see this behaviour? </p>\n"
        raw: 'I see @umesh-c , if you run my script above with `max_new_tokens=1000`
          and `eos_token_id=-1` (to generate unlimited sequences) do you still see
          this behaviour? '
        updatedAt: '2023-10-05T07:32:37.357Z'
      numEdits: 0
      reactions: []
    id: 651e66950b6949da8c4e345c
    type: comment
  author: ybelkada
  content: 'I see @umesh-c , if you run my script above with `max_new_tokens=1000`
    and `eos_token_id=-1` (to generate unlimited sequences) do you still see this
    behaviour? '
  created_at: 2023-10-05 06:32:37+00:00
  edited: false
  hidden: false
  id: 651e66950b6949da8c4e345c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0306a5b25a25605f6118568a25d5416c.svg
      fullname: BuckWinston
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: librehash
      type: user
    createdAt: '2023-10-15T10:09:03.000Z'
    data:
      edited: false
      editors:
      - librehash
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9686390161514282
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0306a5b25a25605f6118568a25d5416c.svg
          fullname: BuckWinston
          isHf: false
          isPro: false
          name: librehash
          type: user
        html: "<p>Chiming in on this thread. Was Googling about this same issue discussed\
          \ here and stumbled upon this thread. I too am very curious to hear about\
          \ whether anyone has had any issues with generating outputs &gt;512 tokens\
          \ using the inference settings proposed by <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>\
          \ . Also, <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ybelkada\">@<span class=\"\
          underline\">ybelkada</span></a></span>\n\n\t</span></span> many thanks for\
          \ you taking the time to answer this thread and put this question to rest\
          \ because this was sort of a make or break for me for the project I'm working\
          \ on currently. </p>\n"
        raw: 'Chiming in on this thread. Was Googling about this same issue discussed
          here and stumbled upon this thread. I too am very curious to hear about
          whether anyone has had any issues with generating outputs >512 tokens using
          the inference settings proposed by @ybelkada . Also, @ybelkada many thanks
          for you taking the time to answer this thread and put this question to rest
          because this was sort of a make or break for me for the project I''m working
          on currently. '
        updatedAt: '2023-10-15T10:09:03.862Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ybelkada
    id: 652bba3f30355beba6af1c8a
    type: comment
  author: librehash
  content: 'Chiming in on this thread. Was Googling about this same issue discussed
    here and stumbled upon this thread. I too am very curious to hear about whether
    anyone has had any issues with generating outputs >512 tokens using the inference
    settings proposed by @ybelkada . Also, @ybelkada many thanks for you taking the
    time to answer this thread and put this question to rest because this was sort
    of a make or break for me for the project I''m working on currently. '
  created_at: 2023-10-15 09:09:03+00:00
  edited: false
  hidden: false
  id: 652bba3f30355beba6af1c8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-16T19:26:33.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.918606698513031
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;librehash&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/librehash\"\
          >@<span class=\"underline\">librehash</span></a></span>\n\n\t</span></span>\
          \ !<br>Have you managed to run generation with outputs &gt; 512 tokens?\
          \ Can you share one of the failure case you are facing?</p>\n"
        raw: "Thanks @librehash ! \nHave you managed to run generation with outputs\
          \ > 512 tokens? Can you share one of the failure case you are facing?"
        updatedAt: '2023-10-16T19:26:33.887Z'
      numEdits: 0
      reactions: []
    id: 652d8e694dfc91e4b1944d49
    type: comment
  author: ybelkada
  content: "Thanks @librehash ! \nHave you managed to run generation with outputs\
    \ > 512 tokens? Can you share one of the failure case you are facing?"
  created_at: 2023-10-16 18:26:33+00:00
  edited: false
  hidden: false
  id: 652d8e694dfc91e4b1944d49
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: google/flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: any plans on increasing model's model_max_length?
