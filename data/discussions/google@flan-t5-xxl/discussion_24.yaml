!!python/object:huggingface_hub.community.DiscussionWithDetails
author: silicon-dopamine
conflicting_files: null
created_at: 2023-01-27 13:38:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84e68d948b60fd3a0e27e04bfa623d07.svg
      fullname: Silicon Dopamine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: silicon-dopamine
      type: user
    createdAt: '2023-01-27T13:38:54.000Z'
    data:
      edited: true
      editors:
      - silicon-dopamine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84e68d948b60fd3a0e27e04bfa623d07.svg
          fullname: Silicon Dopamine
          isHf: false
          isPro: false
          name: silicon-dopamine
          type: user
        html: "<p>I downloaded <code>google/flan-5-xxl</code> using</p>\n<pre><code>from\
          \ huggingface_hub import snapshot_download\nsnapshot_download(repo_id=\"\
          google/flan-t5-xxl\")\n</code></pre>\n<p>I can't use the normal <code>from_pretrained</code>\
          \ automatic downloading because of firewall settings. I then tried to load\
          \ the model</p>\n<pre><code>from transformers import T5ForConditionalGeneration,\
          \ T5Tokenizer\n\npath = \"&lt;my_home_path&gt;/.cache/huggingface/hub/models--google--flan-t5-xxl/snapshots/cfce5d9d92822886d7ffea8204d3691fc97295c4\"\
          \n\ntokenizer = T5Tokenizer.from_pretrained(path,\n    from_tf=False,\n\
          \    from_flax=False,\n)\nmodel = T5ForConditionalGeneration.from_pretrained(\n\
          \    path,\n    from_tf=False,\n    from_flax=False,\n)\n</code></pre>\n\
          <p>And received the error</p>\n<pre><code>OSError: Error no file named ['pytorch_model.bin',\
          \ 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory\
          \ &lt;my_home_path&gt;/.cache/huggingface/hub/models--google--flan-t5-xxl/snapshots/cfce5d9d92822886d7ffea8204d3691fc97295c4\
          \ or `from_tf` and `from_flax` set to False.\n</code></pre>\n<p>I've verified\
          \ the path is correct. Is this a bug or is there a different way to load\
          \ the snapshot? <a href=\"https://huggingface.co/google/flan-t5-xxl/tree/main\"\
          >https://huggingface.co/google/flan-t5-xxl/tree/main</a> has no 'pytorch_model.bin'\
          \ since the model is sharded. Note that the tokenizer does load properly.</p>\n"
        raw: "I downloaded `google/flan-5-xxl` using\n```\nfrom huggingface_hub import\
          \ snapshot_download\nsnapshot_download(repo_id=\"google/flan-t5-xxl\")\n\
          ```\n\nI can't use the normal `from_pretrained` automatic downloading because\
          \ of firewall settings. I then tried to load the model\n```\nfrom transformers\
          \ import T5ForConditionalGeneration, T5Tokenizer\n\npath = \"<my_home_path>/.cache/huggingface/hub/models--google--flan-t5-xxl/snapshots/cfce5d9d92822886d7ffea8204d3691fc97295c4\"\
          \n\ntokenizer = T5Tokenizer.from_pretrained(path,\n    from_tf=False,\n\
          \    from_flax=False,\n)\nmodel = T5ForConditionalGeneration.from_pretrained(\n\
          \    path,\n    from_tf=False,\n    from_flax=False,\n)\n```\nAnd received\
          \ the error\n```\nOSError: Error no file named ['pytorch_model.bin', 'tf_model.h5',\
          \ 'model.ckpt.index', 'flax_model.msgpack'] found in directory <my_home_path>/.cache/huggingface/hub/models--google--flan-t5-xxl/snapshots/cfce5d9d92822886d7ffea8204d3691fc97295c4\
          \ or `from_tf` and `from_flax` set to False.\n```\nI've verified the path\
          \ is correct. Is this a bug or is there a different way to load the snapshot?\
          \ https://huggingface.co/google/flan-t5-xxl/tree/main has no 'pytorch_model.bin'\
          \ since the model is sharded. Note that the tokenizer does load properly."
        updatedAt: '2023-01-27T13:48:23.167Z'
      numEdits: 3
      reactions: []
    id: 63d3d3eeff1384ce6c5d0231
    type: comment
  author: silicon-dopamine
  content: "I downloaded `google/flan-5-xxl` using\n```\nfrom huggingface_hub import\
    \ snapshot_download\nsnapshot_download(repo_id=\"google/flan-t5-xxl\")\n```\n\n\
    I can't use the normal `from_pretrained` automatic downloading because of firewall\
    \ settings. I then tried to load the model\n```\nfrom transformers import T5ForConditionalGeneration,\
    \ T5Tokenizer\n\npath = \"<my_home_path>/.cache/huggingface/hub/models--google--flan-t5-xxl/snapshots/cfce5d9d92822886d7ffea8204d3691fc97295c4\"\
    \n\ntokenizer = T5Tokenizer.from_pretrained(path,\n    from_tf=False,\n    from_flax=False,\n\
    )\nmodel = T5ForConditionalGeneration.from_pretrained(\n    path,\n    from_tf=False,\n\
    \    from_flax=False,\n)\n```\nAnd received the error\n```\nOSError: Error no\
    \ file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack']\
    \ found in directory <my_home_path>/.cache/huggingface/hub/models--google--flan-t5-xxl/snapshots/cfce5d9d92822886d7ffea8204d3691fc97295c4\
    \ or `from_tf` and `from_flax` set to False.\n```\nI've verified the path is correct.\
    \ Is this a bug or is there a different way to load the snapshot? https://huggingface.co/google/flan-t5-xxl/tree/main\
    \ has no 'pytorch_model.bin' since the model is sharded. Note that the tokenizer\
    \ does load properly."
  created_at: 2023-01-27 13:38:54+00:00
  edited: true
  hidden: false
  id: 63d3d3eeff1384ce6c5d0231
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84e68d948b60fd3a0e27e04bfa623d07.svg
      fullname: Silicon Dopamine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: silicon-dopamine
      type: user
    createdAt: '2023-01-27T19:07:26.000Z'
    data:
      edited: false
      editors:
      - silicon-dopamine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84e68d948b60fd3a0e27e04bfa623d07.svg
          fullname: Silicon Dopamine
          isHf: false
          isPro: false
          name: silicon-dopamine
          type: user
        html: '<p>Disregard. This was a result of running on an old version of transformers
          that didn''t support sharded models.</p>

          '
        raw: Disregard. This was a result of running on an old version of transformers
          that didn't support sharded models.
        updatedAt: '2023-01-27T19:07:26.350Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63d420ee2424c652f6f8bf34
    id: 63d420ee2424c652f6f8bf33
    type: comment
  author: silicon-dopamine
  content: Disregard. This was a result of running on an old version of transformers
    that didn't support sharded models.
  created_at: 2023-01-27 19:07:26+00:00
  edited: false
  hidden: false
  id: 63d420ee2424c652f6f8bf33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/84e68d948b60fd3a0e27e04bfa623d07.svg
      fullname: Silicon Dopamine
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: silicon-dopamine
      type: user
    createdAt: '2023-01-27T19:07:26.000Z'
    data:
      status: closed
    id: 63d420ee2424c652f6f8bf34
    type: status-change
  author: silicon-dopamine
  created_at: 2023-01-27 19:07:26+00:00
  id: 63d420ee2424c652f6f8bf34
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: google/flan-t5-xxl
repo_type: model
status: closed
target_branch: null
title: How do I load this from a snapshot?
