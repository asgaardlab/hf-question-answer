!!python/object:huggingface_hub.community.DiscussionWithDetails
author: joekr552
conflicting_files: null
created_at: 2023-03-16 18:53:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f1af4968aa78846a822e0d5ee6a168d.svg
      fullname: Joel Kronander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: joekr552
      type: user
    createdAt: '2023-03-16T19:53:47.000Z'
    data:
      edited: false
      editors:
      - joekr552
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f1af4968aa78846a822e0d5ee6a168d.svg
          fullname: Joel Kronander
          isHf: false
          isPro: true
          name: joekr552
          type: user
        html: '<p>For the inference API, and the homepage preview, the model output
          seems overly truncated. </p>

          <p>E.g. trying any Chain of Thought on e.g. GSM 8K, gives very weird short,
          (truncated), responses. </p>

          <p>Seems like a bug to me?</p>

          '
        raw: "For the inference API, and the homepage preview, the model output seems\
          \ overly truncated. \r\n\r\nE.g. trying any Chain of Thought on e.g. GSM\
          \ 8K, gives very weird short, (truncated), responses. \r\n\r\nSeems like\
          \ a bug to me?"
        updatedAt: '2023-03-16T19:53:47.393Z'
      numEdits: 0
      reactions: []
    id: 641373cbb0c7659b8fd44d10
    type: comment
  author: joekr552
  content: "For the inference API, and the homepage preview, the model output seems\
    \ overly truncated. \r\n\r\nE.g. trying any Chain of Thought on e.g. GSM 8K, gives\
    \ very weird short, (truncated), responses. \r\n\r\nSeems like a bug to me?"
  created_at: 2023-03-16 18:53:47+00:00
  edited: false
  hidden: false
  id: 641373cbb0c7659b8fd44d10
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bea5eb4cfd4559ef65be516db26b3e7.svg
      fullname: Dhruv Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bfgdhrubo
      type: user
    createdAt: '2023-04-03T04:04:51.000Z'
    data:
      edited: false
      editors:
      - bfgdhrubo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bea5eb4cfd4559ef65be516db26b3e7.svg
          fullname: Dhruv Sharma
          isHf: false
          isPro: false
          name: bfgdhrubo
          type: user
        html: '<p>I agree.  I tried it using the inference api as well. And the output
          was sometimes only a single word.</p>

          '
        raw: I agree.  I tried it using the inference api as well. And the output
          was sometimes only a single word.
        updatedAt: '2023-04-03T04:04:51.443Z'
      numEdits: 0
      reactions: []
    id: 642a5063b20cdada1200f200
    type: comment
  author: bfgdhrubo
  content: I agree.  I tried it using the inference api as well. And the output was
    sometimes only a single word.
  created_at: 2023-04-03 03:04:51+00:00
  edited: false
  hidden: false
  id: 642a5063b20cdada1200f200
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg
      fullname: Shayne Longpre
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Shayne
      type: user
    createdAt: '2023-04-05T20:36:14.000Z'
    data:
      edited: false
      editors:
      - Shayne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a1cf1ef1fd442c36ed65c68e51919fed.svg
          fullname: Shayne Longpre
          isHf: false
          isPro: false
          name: Shayne
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;joekr552&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/joekr552\">@<span class=\"\
          underline\">joekr552</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;bfgdhrubo&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/bfgdhrubo\">@<span class=\"underline\">bfgdhrubo</span></a></span>\n\
          \n\t</span></span> The Flan Collection is more oriented to short-answer\
          \ academic-style tasks, rather than longer conversational dialog responses\
          \ like ChatGPT type models. Also chain of thought style questions are still\
          \ quite hard for models of this size, though they demonstrate some capabilities\
          \ as we trained on a few of those tasks. </p>\n<p>If you're looking for\
          \ longer, conversational responses or CoT specifically, I'd recommend either\
          \ (a) finetuning additionally on dialog-style data (e.g. like Alpaca, distilled\
          \ from ChatGPT), or (b) using Flan-UL2 which should have slightly stronger\
          \ CoT capabilities. Flan-T5 really excels for short-answer NLP tasks and\
          \ for finetuning, where it beats out most competitors, even of slightly\
          \ larger size.</p>\n<p>Hope this was helpful!</p>\n"
        raw: "@joekr552 @bfgdhrubo The Flan Collection is more oriented to short-answer\
          \ academic-style tasks, rather than longer conversational dialog responses\
          \ like ChatGPT type models. Also chain of thought style questions are still\
          \ quite hard for models of this size, though they demonstrate some capabilities\
          \ as we trained on a few of those tasks. \n\nIf you're looking for longer,\
          \ conversational responses or CoT specifically, I'd recommend either (a)\
          \ finetuning additionally on dialog-style data (e.g. like Alpaca, distilled\
          \ from ChatGPT), or (b) using Flan-UL2 which should have slightly stronger\
          \ CoT capabilities. Flan-T5 really excels for short-answer NLP tasks and\
          \ for finetuning, where it beats out most competitors, even of slightly\
          \ larger size.\n\nHope this was helpful!"
        updatedAt: '2023-04-05T20:36:14.386Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - lucecpkn
        - joffreyv
        - ntwalibas
        - syeminPark
        - abarkova
        - mk-20
    id: 642ddbbe32bdf5af73edfc28
    type: comment
  author: Shayne
  content: "@joekr552 @bfgdhrubo The Flan Collection is more oriented to short-answer\
    \ academic-style tasks, rather than longer conversational dialog responses like\
    \ ChatGPT type models. Also chain of thought style questions are still quite hard\
    \ for models of this size, though they demonstrate some capabilities as we trained\
    \ on a few of those tasks. \n\nIf you're looking for longer, conversational responses\
    \ or CoT specifically, I'd recommend either (a) finetuning additionally on dialog-style\
    \ data (e.g. like Alpaca, distilled from ChatGPT), or (b) using Flan-UL2 which\
    \ should have slightly stronger CoT capabilities. Flan-T5 really excels for short-answer\
    \ NLP tasks and for finetuning, where it beats out most competitors, even of slightly\
    \ larger size.\n\nHope this was helpful!"
  created_at: 2023-04-05 19:36:14+00:00
  edited: false
  hidden: false
  id: 642ddbbe32bdf5af73edfc28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c40cef65c16508dafe9fdbf1146c8153.svg
      fullname: Tristan Chambers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: tristanchambers-bids
      type: user
    createdAt: '2023-06-16T01:56:15.000Z'
    data:
      edited: false
      editors:
      - tristanchambers-bids
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6443504691123962
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c40cef65c16508dafe9fdbf1146c8153.svg
          fullname: Tristan Chambers
          isHf: false
          isPro: true
          name: tristanchambers-bids
          type: user
        html: "<p>Understood that this model isn't trained for verbose conversation\
          \ style response. However, even with simple tasks the output is strangely\
          \ truncated. See below for an example:</p>\n<p>PROMPT<br>Insert spaces between\
          \ the words in the following text: Thisishowtothrowbackafishyoudon\u2019\
          tlike,andthatwaysomethingbadwon\u2019tfallonyou;thisishowtobullyaman;thisishowamanbulliesyou;thisishowtoloveaman;andifthisdoesn\u2019\
          tworkthereareotherways,andiftheydon\u2019tworkdon\u2019tfeeltoobadaboutgivingup;thisishowtospitupintheairifyoufeellikeit,andthisishowtomovequicksothatitdoesn\u2019\
          tfallonyou;thisishowtomakeendsmeet;alwayssqueezebreadtomakesureit\u2019\
          sfresh;butwhatifthebakerwon\u2019tletmefeelthebread?;youmeantosaythatafterallyouarereallygoingtobethekindofwomanwhothebakerwon\u2019\
          tletnearthebread?</p>\n<p>RESPONSE<br>This is how to throw back a fish you\
          \ don\u2019t like, and that way something bad</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/TFCnbUtjVsAcsC_YKUZVl.png\"\
          ><img alt=\"Screenshot from 2023-06-15 18-53-09.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/TFCnbUtjVsAcsC_YKUZVl.png\"\
          ></a></p>\n"
        raw: "Understood that this model isn't trained for verbose conversation style\
          \ response. However, even with simple tasks the output is strangely truncated.\
          \ See below for an example:\n\nPROMPT\nInsert spaces between the words in\
          \ the following text: Thisishowtothrowbackafishyoudon\u2019tlike,andthatwaysomethingbadwon\u2019\
          tfallonyou;thisishowtobullyaman;thisishowamanbulliesyou;thisishowtoloveaman;andifthisdoesn\u2019\
          tworkthereareotherways,andiftheydon\u2019tworkdon\u2019tfeeltoobadaboutgivingup;thisishowtospitupintheairifyoufeellikeit,andthisishowtomovequicksothatitdoesn\u2019\
          tfallonyou;thisishowtomakeendsmeet;alwayssqueezebreadtomakesureit\u2019\
          sfresh;butwhatifthebakerwon\u2019tletmefeelthebread?;youmeantosaythatafterallyouarereallygoingtobethekindofwomanwhothebakerwon\u2019\
          tletnearthebread?\n\nRESPONSE\nThis is how to throw back a fish you don\u2019\
          t like, and that way something bad\n\n\n![Screenshot from 2023-06-15 18-53-09.png](https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/TFCnbUtjVsAcsC_YKUZVl.png)\n"
        updatedAt: '2023-06-16T01:56:15.463Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tdwivedi717
    id: 648bc13f9f4df9495b858957
    type: comment
  author: tristanchambers-bids
  content: "Understood that this model isn't trained for verbose conversation style\
    \ response. However, even with simple tasks the output is strangely truncated.\
    \ See below for an example:\n\nPROMPT\nInsert spaces between the words in the\
    \ following text: Thisishowtothrowbackafishyoudon\u2019tlike,andthatwaysomethingbadwon\u2019\
    tfallonyou;thisishowtobullyaman;thisishowamanbulliesyou;thisishowtoloveaman;andifthisdoesn\u2019\
    tworkthereareotherways,andiftheydon\u2019tworkdon\u2019tfeeltoobadaboutgivingup;thisishowtospitupintheairifyoufeellikeit,andthisishowtomovequicksothatitdoesn\u2019\
    tfallonyou;thisishowtomakeendsmeet;alwayssqueezebreadtomakesureit\u2019sfresh;butwhatifthebakerwon\u2019\
    tletmefeelthebread?;youmeantosaythatafterallyouarereallygoingtobethekindofwomanwhothebakerwon\u2019\
    tletnearthebread?\n\nRESPONSE\nThis is how to throw back a fish you don\u2019\
    t like, and that way something bad\n\n\n![Screenshot from 2023-06-15 18-53-09.png](https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/TFCnbUtjVsAcsC_YKUZVl.png)\n"
  created_at: 2023-06-16 00:56:15+00:00
  edited: false
  hidden: false
  id: 648bc13f9f4df9495b858957
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
      fullname: Vatsa Pandey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VatsaDev
      type: user
    createdAt: '2023-06-30T17:07:22.000Z'
    data:
      edited: false
      editors:
      - VatsaDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5889763236045837
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
          fullname: Vatsa Pandey
          isHf: false
          isPro: false
          name: VatsaDev
          type: user
        html: '<p>I have the exact same issue, with<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/ShPxlE-Xn5X44FHdMjIHF.png"><img
          alt="Screen Shot 2023-06-30 at 12.06.43 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/ShPxlE-Xn5X44FHdMjIHF.png"></a></p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/Y0i4x9_Q5BqHLve7eZO3Q.png"><img
          alt="Screen Shot 2023-06-30 at 12.06.58 PM.png" src="https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/Y0i4x9_Q5BqHLve7eZO3Q.png"></a></p>

          '
        raw: "I have the exact same issue, with \n![Screen Shot 2023-06-30 at 12.06.43\
          \ PM.png](https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/ShPxlE-Xn5X44FHdMjIHF.png)\n\
          \n![Screen Shot 2023-06-30 at 12.06.58 PM.png](https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/Y0i4x9_Q5BqHLve7eZO3Q.png)\n"
        updatedAt: '2023-06-30T17:07:22.176Z'
      numEdits: 0
      reactions: []
    id: 649f0bca19a15b1fc976a01f
    type: comment
  author: VatsaDev
  content: "I have the exact same issue, with \n![Screen Shot 2023-06-30 at 12.06.43\
    \ PM.png](https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/ShPxlE-Xn5X44FHdMjIHF.png)\n\
    \n![Screen Shot 2023-06-30 at 12.06.58 PM.png](https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/Y0i4x9_Q5BqHLve7eZO3Q.png)\n"
  created_at: 2023-06-30 16:07:22+00:00
  edited: false
  hidden: false
  id: 649f0bca19a15b1fc976a01f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0c0368357b486a435843a5fde1e8bc1.svg
      fullname: Jyotirman Adarsh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jyotirman
      type: user
    createdAt: '2023-07-15T07:15:44.000Z'
    data:
      edited: false
      editors:
      - jyotirman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9939101934432983
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0c0368357b486a435843a5fde1e8bc1.svg
          fullname: Jyotirman Adarsh
          isHf: false
          isPro: false
          name: jyotirman
          type: user
        html: '<p>I am also having same issue, response length is very short and truncated.
          How to solve the issue?</p>

          '
        raw: 'I am also having same issue, response length is very short and truncated.
          How to solve the issue?

          '
        updatedAt: '2023-07-15T07:15:44.731Z'
      numEdits: 0
      reactions: []
    id: 64b247a0ffed52962a9e0bae
    type: comment
  author: jyotirman
  content: 'I am also having same issue, response length is very short and truncated.
    How to solve the issue?

    '
  created_at: 2023-07-15 06:15:44+00:00
  edited: false
  hidden: false
  id: 64b247a0ffed52962a9e0bae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
      fullname: Vatsa Pandey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VatsaDev
      type: user
    createdAt: '2023-07-16T01:31:33.000Z'
    data:
      edited: false
      editors:
      - VatsaDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9560325741767883
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/648a66c628e95179adc68d49/WXTipJgemlrxXouBOGl96.png?w=200&h=200&f=face
          fullname: Vatsa Pandey
          isHf: false
          isPro: false
          name: VatsaDev
          type: user
        html: '<p>From what the model states, its meant to give short anwsers, beyond
          that though, The model utters absolute nonsense.</p>

          <p>I''ve noticed that having a provided context can reduce this alot. Though
          it will then from statement very similar to the context</p>

          '
        raw: 'From what the model states, its meant to give short anwsers, beyond
          that though, The model utters absolute nonsense.


          I''ve noticed that having a provided context can reduce this alot. Though
          it will then from statement very similar to the context'
        updatedAt: '2023-07-16T01:31:33.027Z'
      numEdits: 0
      reactions: []
    id: 64b34875ee7a5f1825f5e5bc
    type: comment
  author: VatsaDev
  content: 'From what the model states, its meant to give short anwsers, beyond that
    though, The model utters absolute nonsense.


    I''ve noticed that having a provided context can reduce this alot. Though it will
    then from statement very similar to the context'
  created_at: 2023-07-16 00:31:33+00:00
  edited: false
  hidden: false
  id: 64b34875ee7a5f1825f5e5bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c40cef65c16508dafe9fdbf1146c8153.svg
      fullname: Tristan Chambers
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: tristanchambers-bids
      type: user
    createdAt: '2023-11-17T15:19:41.000Z'
    data:
      edited: false
      editors:
      - tristanchambers-bids
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6793158054351807
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c40cef65c16508dafe9fdbf1146c8153.svg
          fullname: Tristan Chambers
          isHf: false
          isPro: true
          name: tristanchambers-bids
          type: user
        html: '<p>This issue seems to be fixed now!</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/mWSymxXcvqJjXzFR68DV4.png"><img
          alt="Screen Shot 2023-11-17 at 7.18.59 AM.png" src="https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/mWSymxXcvqJjXzFR68DV4.png"></a></p>

          '
        raw: 'This issue seems to be fixed now!



          ![Screen Shot 2023-11-17 at 7.18.59 AM.png](https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/mWSymxXcvqJjXzFR68DV4.png)

          '
        updatedAt: '2023-11-17T15:19:41.897Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - VatsaDev
    id: 6557848d66423b57b2036c3d
    type: comment
  author: tristanchambers-bids
  content: 'This issue seems to be fixed now!



    ![Screen Shot 2023-11-17 at 7.18.59 AM.png](https://cdn-uploads.huggingface.co/production/uploads/641a136cf5e9c66105ff1f8e/mWSymxXcvqJjXzFR68DV4.png)

    '
  created_at: 2023-11-17 15:19:41+00:00
  edited: false
  hidden: false
  id: 6557848d66423b57b2036c3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bde5e434549ad0b6a4991b465e6630ce.svg
      fullname: Milind
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flysaurus
      type: user
    createdAt: '2024-01-09T18:39:52.000Z'
    data:
      edited: false
      editors:
      - flysaurus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9897023439407349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bde5e434549ad0b6a4991b465e6630ce.svg
          fullname: Milind
          isHf: false
          isPro: false
          name: flysaurus
          type: user
        html: '<p>I believe I am having a similar issue when I asked a COT question.<br>Question:
          Answer the following question by step-by-step reasoning. The cafeteria had
          23 apples if 20 were used and 6 new were bought. How many apples are left
          at the end?<br>Model response: [''The cafeteria used 20 apples and bought
          6 apples so there are 23 - 20 ='']</p>

          '
        raw: 'I believe I am having a similar issue when I asked a COT question.

          Question: Answer the following question by step-by-step reasoning. The cafeteria
          had 23 apples if 20 were used and 6 new were bought. How many apples are
          left at the end?

          Model response: [''The cafeteria used 20 apples and bought 6 apples so there
          are 23 - 20 ='']

          '
        updatedAt: '2024-01-09T18:39:52.155Z'
      numEdits: 0
      reactions: []
    id: 659d92f85bff2252a98d7618
    type: comment
  author: flysaurus
  content: 'I believe I am having a similar issue when I asked a COT question.

    Question: Answer the following question by step-by-step reasoning. The cafeteria
    had 23 apples if 20 were used and 6 new were bought. How many apples are left
    at the end?

    Model response: [''The cafeteria used 20 apples and bought 6 apples so there are
    23 - 20 ='']

    '
  created_at: 2024-01-09 18:39:52+00:00
  edited: false
  hidden: false
  id: 659d92f85bff2252a98d7618
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 39
repo_id: google/flan-t5-xxl
repo_type: model
status: open
target_branch: null
title: Output seems overly truncated, and max_length don't seem to matter
