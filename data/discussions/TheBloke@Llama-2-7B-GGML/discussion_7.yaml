!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ashubi
conflicting_files: null
created_at: 2023-12-30 07:30:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4284a64b68ca2079e14db8e949888270.svg
      fullname: ashutosh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ashubi
      type: user
    createdAt: '2023-12-30T07:30:26.000Z'
    data:
      edited: true
      editors:
      - ashubi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7569793462753296
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4284a64b68ca2079e14db8e949888270.svg
          fullname: ashutosh
          isHf: false
          isPro: false
          name: ashubi
          type: user
        html: "<p>I'm chatting with documents using TheBloke/Llama-2-7B-GGML, but\
          \ when I ask a question, it says, \"Number of tokens (525) exceeded maximum\
          \ context length (512).\" and this number keeps going up\u2014526, 527,\
          \ etc.\u2014and eventually it will respond in an unstructured manner. I\
          \ am running this model in CPU<br>Note: The response is good if the warning\
          \ \"Number of tokens (525) exceeded maximum context length (512)\" is not\
          \ generated for any query.<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/655b3111930a0e1b0ac1a9cd/SN9efuQwb940WClA8tzjL.png\"\
          ><img alt=\"Screenshot from 2023-12-30 12-59-49.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/655b3111930a0e1b0ac1a9cd/SN9efuQwb940WClA8tzjL.png\"\
          ></a></p>\n"
        raw: "I'm chatting with documents using TheBloke/Llama-2-7B-GGML, but when\
          \ I ask a question, it says, \"Number of tokens (525) exceeded maximum context\
          \ length (512).\" and this number keeps going up\u2014526, 527, etc.\u2014\
          and eventually it will respond in an unstructured manner. I am running this\
          \ model in CPU\nNote: The response is good if the warning \"Number of tokens\
          \ (525) exceeded maximum context length (512)\" is not generated for any\
          \ query.\n![Screenshot from 2023-12-30 12-59-49.png](https://cdn-uploads.huggingface.co/production/uploads/655b3111930a0e1b0ac1a9cd/SN9efuQwb940WClA8tzjL.png)\n"
        updatedAt: '2023-12-30T07:33:35.216Z'
      numEdits: 1
      reactions: []
    id: 658fc712353318837522f86f
    type: comment
  author: ashubi
  content: "I'm chatting with documents using TheBloke/Llama-2-7B-GGML, but when I\
    \ ask a question, it says, \"Number of tokens (525) exceeded maximum context length\
    \ (512).\" and this number keeps going up\u2014526, 527, etc.\u2014and eventually\
    \ it will respond in an unstructured manner. I am running this model in CPU\n\
    Note: The response is good if the warning \"Number of tokens (525) exceeded maximum\
    \ context length (512)\" is not generated for any query.\n![Screenshot from 2023-12-30\
    \ 12-59-49.png](https://cdn-uploads.huggingface.co/production/uploads/655b3111930a0e1b0ac1a9cd/SN9efuQwb940WClA8tzjL.png)\n"
  created_at: 2023-12-30 07:30:26+00:00
  edited: true
  hidden: false
  id: 658fc712353318837522f86f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: TheBloke/Llama-2-7B-GGML
repo_type: model
status: open
target_branch: null
title: Number of tokens (525) exceeded maximum context length (512).
