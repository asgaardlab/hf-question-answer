!!python/object:huggingface_hub.community.DiscussionWithDetails
author: deepakkaura26
conflicting_files: null
created_at: 2023-07-27 16:00:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
      fullname: Deepak  Kaura
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepakkaura26
      type: user
    createdAt: '2023-07-27T17:00:00.000Z'
    data:
      edited: false
      editors:
      - deepakkaura26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9307751059532166
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63c8ef6c00104ea998d92645/8zVt_tzR2fPgk7s6dA03k.jpeg?w=200&h=200&f=face
          fullname: Deepak  Kaura
          isHf: false
          isPro: false
          name: deepakkaura26
          type: user
        html: '<p>Can someone show me example of this model running on colab through  CPU
          ?</p>

          '
        raw: Can someone show me example of this model running on colab through  CPU
          ?
        updatedAt: '2023-07-27T17:00:00.931Z'
      numEdits: 0
      reactions: []
    id: 64c2a2908f50849b102b09dd
    type: comment
  author: deepakkaura26
  content: Can someone show me example of this model running on colab through  CPU
    ?
  created_at: 2023-07-27 16:00:00+00:00
  edited: false
  hidden: false
  id: 64c2a2908f50849b102b09dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
      fullname: balu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: balu548411
      type: user
    createdAt: '2023-07-28T00:14:26.000Z'
    data:
      edited: false
      editors:
      - balu548411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4592084288597107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/84a05ca2b27c59b32bc63e6838871242.svg
          fullname: balu
          isHf: false
          isPro: false
          name: balu548411
          type: user
        html: '<p>from transformers import AutoTokenizer, pipeline, logging<br>from
          auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig</p>

          <p>model_name_or_path = "TheBloke/Llama-2-7B-GPTQ"<br>model_basename = "gptq_model-4bit-128g"</p>

          <p>use_triton = False</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        device="cuda:0",<br>        use_triton=use_triton,<br>        quantize_config=None)</p>

          <p>"""<br>To download from a specific branch, use the revision parameter,
          as in this example:</p>

          <p>model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,<br>        revision="gptq-4bit-32g-actorder_True",<br>        model_basename=model_basename,<br>        use_safetensors=True,<br>        trust_remote_code=True,<br>        quantize_config=None)<br>"""</p>

          <p>prompt = "Tell me about AI"<br>prompt_template=f''''''{prompt}<br>''''''</p>

          <p>print("\n\n*** Generate:")</p>

          <p>input_ids = tokenizer(prompt_template, return_tensors=''pt'').input_ids<br>output
          = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)<br>print(tokenizer.decode(output[0]))</p>

          <h1 id="inference-can-also-be-done-using-transformers-pipeline">Inference
          can also be done using transformers'' pipeline</h1>

          <h1 id="prevent-printing-spurious-transformers-error-when-using-pipeline-with-autogptq">Prevent
          printing spurious transformers error when using pipeline with AutoGPTQ</h1>

          <p>logging.set_verbosity(logging.CRITICAL)</p>

          <p>print("*** Pipeline:")<br>pipe = pipeline(<br>    "text-generation",<br>    model=model,<br>    tokenizer=tokenizer,<br>    max_new_tokens=512,<br>    temperature=0.7,<br>    top_p=0.95,<br>    repetition_penalty=1.15<br>)</p>

          <p>print(pipe(prompt_template)[0][''generated_text''])</p>

          <p>You can find this code in readme file</p>

          '
        raw: "from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
          \ import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path =\
          \ \"TheBloke/Llama-2-7B-GPTQ\"\nmodel_basename = \"gptq_model-4bit-128g\"\
          \n\nuse_triton = False\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=True)\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        model_basename=model_basename,\n        use_safetensors=True,\n\
          \        trust_remote_code=True,\n        device=\"cuda:0\",\n        use_triton=use_triton,\n\
          \        quantize_config=None)\n\n\"\"\"\nTo download from a specific branch,\
          \ use the revision parameter, as in this example:\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
          \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
          \        use_safetensors=True,\n        trust_remote_code=True,\n      \
          \  quantize_config=None)\n\"\"\"\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n\
          '''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors='pt').input_ids\noutput = model.generate(inputs=input_ids,\
          \ temperature=0.7, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          \n# Inference can also be done using transformers' pipeline\n\n# Prevent\
          \ printing spurious transformers error when using pipeline with AutoGPTQ\n\
          logging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\")\npipe\
          \ = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
          \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\n\n\n\nYou can\
          \ find this code in readme file"
        updatedAt: '2023-07-28T00:14:26.416Z'
      numEdits: 0
      reactions: []
    id: 64c308621920bd982083fa53
    type: comment
  author: balu548411
  content: "from transformers import AutoTokenizer, pipeline, logging\nfrom auto_gptq\
    \ import AutoGPTQForCausalLM, BaseQuantizeConfig\n\nmodel_name_or_path = \"TheBloke/Llama-2-7B-GPTQ\"\
    \nmodel_basename = \"gptq_model-4bit-128g\"\n\nuse_triton = False\n\ntokenizer\
    \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n\nmodel\
    \ = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        device=\"\
    cuda:0\",\n        use_triton=use_triton,\n        quantize_config=None)\n\n\"\
    \"\"\nTo download from a specific branch, use the revision parameter, as in this\
    \ example:\n\nmodel = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n\
    \        revision=\"gptq-4bit-32g-actorder_True\",\n        model_basename=model_basename,\n\
    \        use_safetensors=True,\n        trust_remote_code=True,\n        quantize_config=None)\n\
    \"\"\"\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n'''\n\n\
    print(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids\n\
    output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n\n# Prevent printing spurious transformers error when using pipeline\
    \ with AutoGPTQ\nlogging.set_verbosity(logging.CRITICAL)\n\nprint(\"*** Pipeline:\"\
    )\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n\
    \    max_new_tokens=512,\n    temperature=0.7,\n    top_p=0.95,\n    repetition_penalty=1.15\n\
    )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n\n\n\n\nYou can find this\
    \ code in readme file"
  created_at: 2023-07-27 23:14:26+00:00
  edited: false
  hidden: false
  id: 64c308621920bd982083fa53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
      fullname: prakash bhardwaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prakash1524
      type: user
    createdAt: '2023-08-01T08:22:01.000Z'
    data:
      edited: false
      editors:
      - prakash1524
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8808377385139465
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
          fullname: prakash bhardwaj
          isHf: false
          isPro: false
          name: prakash1524
          type: user
        html: '<p>what you provided is for gptq model correct?<br>i am a beginner
          only so forgive me if its a basic doubt</p>

          '
        raw: 'what you provided is for gptq model correct?

          i am a beginner only so forgive me if its a basic doubt'
        updatedAt: '2023-08-01T08:22:01.397Z'
      numEdits: 0
      reactions: []
    id: 64c8c0a9166b735583275099
    type: comment
  author: prakash1524
  content: 'what you provided is for gptq model correct?

    i am a beginner only so forgive me if its a basic doubt'
  created_at: 2023-08-01 07:22:01+00:00
  edited: false
  hidden: false
  id: 64c8c0a9166b735583275099
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-08-01T16:03:27.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9744637608528137
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>yes its for gptq, not ggml</p>

          '
        raw: yes its for gptq, not ggml
        updatedAt: '2023-08-01T16:03:27.443Z'
      numEdits: 0
      reactions: []
    id: 64c92ccf3137cc529df92cfc
    type: comment
  author: YaTharThShaRma999
  content: yes its for gptq, not ggml
  created_at: 2023-08-01 15:03:27+00:00
  edited: false
  hidden: false
  id: 64c92ccf3137cc529df92cfc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Llama-2-7B-GGML
repo_type: model
status: open
target_branch: null
title: 'How to run on colab ? '
