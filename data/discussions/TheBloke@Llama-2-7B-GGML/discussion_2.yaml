!!python/object:huggingface_hub.community.DiscussionWithDetails
author: prakash1524
conflicting_files: null
created_at: 2023-08-01 08:09:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
      fullname: prakash bhardwaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prakash1524
      type: user
    createdAt: '2023-08-01T09:09:01.000Z'
    data:
      edited: false
      editors:
      - prakash1524
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6374924182891846
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
          fullname: prakash bhardwaj
          isHf: false
          isPro: false
          name: prakash1524
          type: user
        html: '<p>OSError: TheBloke/Llama-2-7B-GGML does not appear to have a file
          named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.</p>

          <p>getting this error while loading the model using the same code mentioned
          </p>

          <h1 id="load-model-directly">Load model directly</h1>

          <p>from transformers import AutoModel<br>model = AutoModel.from_pretrained("TheBloke/Llama-2-7B-GGML")</p>

          '
        raw: "OSError: TheBloke/Llama-2-7B-GGML does not appear to have a file named\
          \ pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\r\n\r\
          \ngetting this error while loading the model using the same code mentioned\
          \ \r\n# Load model directly\r\nfrom transformers import AutoModel\r\nmodel\
          \ = AutoModel.from_pretrained(\"TheBloke/Llama-2-7B-GGML\")"
        updatedAt: '2023-08-01T09:09:01.414Z'
      numEdits: 0
      reactions: []
    id: 64c8cbadc864d962edfb2521
    type: comment
  author: prakash1524
  content: "OSError: TheBloke/Llama-2-7B-GGML does not appear to have a file named\
    \ pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\r\n\r\ngetting\
    \ this error while loading the model using the same code mentioned \r\n# Load\
    \ model directly\r\nfrom transformers import AutoModel\r\nmodel = AutoModel.from_pretrained(\"\
    TheBloke/Llama-2-7B-GGML\")"
  created_at: 2023-08-01 08:09:01+00:00
  edited: false
  hidden: false
  id: 64c8cbadc864d962edfb2521
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
      fullname: Yatharth  Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaTharThShaRma999
      type: user
    createdAt: '2023-08-01T16:02:54.000Z'
    data:
      edited: false
      editors:
      - YaTharThShaRma999
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5608909130096436
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c82779fdf94f80cdb5020504f83c818b.svg
          fullname: Yatharth  Sharma
          isHf: false
          isPro: false
          name: YaTharThShaRma999
          type: user
        html: '<p>this is not for transformers, this is for llama cpp, ctransformers,
          llama cpp python, text generation webui.</p>

          '
        raw: 'this is not for transformers, this is for llama cpp, ctransformers,
          llama cpp python, text generation webui.


          '
        updatedAt: '2023-08-01T16:02:54.839Z'
      numEdits: 0
      reactions: []
    id: 64c92cae203170e13c2f63a5
    type: comment
  author: YaTharThShaRma999
  content: 'this is not for transformers, this is for llama cpp, ctransformers, llama
    cpp python, text generation webui.


    '
  created_at: 2023-08-01 15:02:54+00:00
  edited: false
  hidden: false
  id: 64c92cae203170e13c2f63a5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
      fullname: prakash bhardwaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prakash1524
      type: user
    createdAt: '2023-08-02T07:28:09.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
          fullname: prakash bhardwaj
          isHf: false
          isPro: false
          name: prakash1524
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-08-02T07:29:40.267Z'
      numEdits: 0
      reactions: []
    id: 64ca05897fe12ecd0aaedc60
    type: comment
  author: prakash1524
  content: This comment has been hidden
  created_at: 2023-08-02 06:28:09+00:00
  edited: true
  hidden: true
  id: 64ca05897fe12ecd0aaedc60
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
      fullname: prakash bhardwaj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: prakash1524
      type: user
    createdAt: '2023-08-02T11:09:56.000Z'
    data:
      edited: false
      editors:
      - prakash1524
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8516771197319031
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/54323eeec98061a9ec8f6670e62d6cbf.svg
          fullname: prakash bhardwaj
          isHf: false
          isPro: false
          name: prakash1524
          type: user
        html: '<p>can we fine tune this model (ggml model) or we will have to fine
          tune original model then convert it to ggml format as mentioned on some
          github repo,just a beginner doubt?<br>i finetuned a sharded model but not
          able to fine tune this model getting error TheBloke/Llama-2-7B-GGML does
          not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt
          or flax_model.msgpack.</p>

          '
        raw: 'can we fine tune this model (ggml model) or we will have to fine tune
          original model then convert it to ggml format as mentioned on some github
          repo,just a beginner doubt?

          i finetuned a sharded model but not able to fine tune this model getting
          error TheBloke/Llama-2-7B-GGML does not appear to have a file named pytorch_model.bin,
          tf_model.h5, model.ckpt or flax_model.msgpack.

          '
        updatedAt: '2023-08-02T11:09:56.093Z'
      numEdits: 0
      reactions: []
    id: 64ca39845aa1ab065cee3556
    type: comment
  author: prakash1524
  content: 'can we fine tune this model (ggml model) or we will have to fine tune
    original model then convert it to ggml format as mentioned on some github repo,just
    a beginner doubt?

    i finetuned a sharded model but not able to fine tune this model getting error
    TheBloke/Llama-2-7B-GGML does not appear to have a file named pytorch_model.bin,
    tf_model.h5, model.ckpt or flax_model.msgpack.

    '
  created_at: 2023-08-02 10:09:56+00:00
  edited: false
  hidden: false
  id: 64ca39845aa1ab065cee3556
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-08-02T11:13:22.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9419882297515869
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You can''t fine tune a GGML model using Python/transformers code.  There
          is some training support in llama.cpp that you might be able to use, but
          I don''t have any experience of it.</p>

          <p>The general procedure is:</p>

          <ul>

          <li>Fine tune the original unquantised model. This can be done as:<ul>

          <li>a full training in float16 - very expensive </li>

          <li>a LoRA adapter training in float16 - less expensive</li>

          <li>or a LoRA training in 4-bit, known as QLoRA - much cheaper.</li>

          </ul>

          </li>

          <li>Whichever is chosen, the result will be a new unquantised float16 model</li>

          <li>That can then be quantised in GGML, and that GGML then used for inference.</li>

          </ul>

          '
        raw: "You can't fine tune a GGML model using Python/transformers code.  There\
          \ is some training support in llama.cpp that you might be able to use, but\
          \ I don't have any experience of it.\n\nThe general procedure is:\n- Fine\
          \ tune the original unquantised model. This can be done as:\n  -  a full\
          \ training in float16 - very expensive \n  - a LoRA adapter training in\
          \ float16 - less expensive\n   - or a LoRA training in 4-bit, known as QLoRA\
          \ - much cheaper.  \n- Whichever is chosen, the result will be a new unquantised\
          \ float16 model\n- That can then be quantised in GGML, and that GGML then\
          \ used for inference."
        updatedAt: '2023-08-02T11:13:22.478Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\u2764\uFE0F"
        users:
        - prakash1524
        - chyavanshenoy
        - mikeee
        - sujantkumarkv
    id: 64ca3a529172674336f76e9f
    type: comment
  author: TheBloke
  content: "You can't fine tune a GGML model using Python/transformers code.  There\
    \ is some training support in llama.cpp that you might be able to use, but I don't\
    \ have any experience of it.\n\nThe general procedure is:\n- Fine tune the original\
    \ unquantised model. This can be done as:\n  -  a full training in float16 - very\
    \ expensive \n  - a LoRA adapter training in float16 - less expensive\n   - or\
    \ a LoRA training in 4-bit, known as QLoRA - much cheaper.  \n- Whichever is chosen,\
    \ the result will be a new unquantised float16 model\n- That can then be quantised\
    \ in GGML, and that GGML then used for inference."
  created_at: 2023-08-02 10:13:22+00:00
  edited: false
  hidden: false
  id: 64ca3a529172674336f76e9f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Llama-2-7B-GGML
repo_type: model
status: open
target_branch: null
title: error in loading the model using colab
