!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ProphetOfBostrom
conflicting_files: null
created_at: 2024-01-14 00:03:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-14T00:03:04.000Z'
    data:
      edited: true
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9657157063484192
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p><a href="https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-GGUF/discussions/1">Turns
          out if you claim you''re not going to do something, you''ll do it.</a><br><a
          href="https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ">https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ</a>
          ta-da!<br>I''ve barely tested this and not at all out of text-generation-webui,
          but nothing''s obviously wrong with it. It''s 18 gigabytes. It should Just
          Work on TGWUI.<br>The (shared between all experts) attention weights are
          4 bit instead. This adds almost nothing to the file size and nothing beyond
          that to the total memory usage. I think</p>

          <p>This isn''t really one-upmanship at all though, zaq likely has the upper
          hand. Exl2 is fast and their quant has, yknow, more of the model left in
          it.<br>This is not so fast. I get 6 tokens per second with torch.compile(),
          half that with straight torch and ATEN is between but it seems to encode
          very quickly so it feels quite snappy. 6 is just about fast enough that
          I''m producing tokens faster than I can really  ''enjoy'' so I''m happy.<br>***and
          so I speculate that this should have a stronger better understanding of
          long contexts, even if you''re less likely to generate one in the first
          place.</p>

          <p>the real boon ought to be contrastive search which i''ve kept forgetting
          to do. I''ll go try it now.</p>

          '
        raw: "[Turns out if you claim you're not going to do something, you'll do\
          \ it.](https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-GGUF/discussions/1)\n\
          https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ\
          \ ta-da!\nI've barely tested this and not at all out of text-generation-webui,\
          \ but nothing's obviously wrong with it. It's 18 gigabytes. It should Just\
          \ Work on TGWUI.\nThe (shared between all experts) attention weights are\
          \ 4 bit instead. This adds almost nothing to the file size and nothing beyond\
          \ that to the total memory usage. I think\n\nThis isn't really one-upmanship\
          \ at all though, zaq likely has the upper hand. Exl2 is fast and their quant\
          \ has, yknow, more of the model left in it.\nThis is not so fast. I get\
          \ 6 tokens per second with torch.compile(), half that with straight torch\
          \ and ATEN is between but it seems to encode very quickly so it feels quite\
          \ snappy. 6 is just about fast enough that I'm producing tokens faster than\
          \ I can really  'enjoy' so I'm happy. \n***and so I speculate that this\
          \ should have a stronger better understanding of long contexts, even if\
          \ you're less likely to generate one in the first place.\n\nthe real boon\
          \ ought to be contrastive search which i've kept forgetting to do. I'll\
          \ go try it now."
        updatedAt: '2024-01-14T00:13:36.449Z'
      numEdits: 2
      reactions: []
    id: 65a324b8f95d78ab7664e254
    type: comment
  author: ProphetOfBostrom
  content: "[Turns out if you claim you're not going to do something, you'll do it.](https://huggingface.co/NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss-GGUF/discussions/1)\n\
    https://huggingface.co/ProphetOfBostrom/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss_attn-4bit-moe-2bit-HQQ\
    \ ta-da!\nI've barely tested this and not at all out of text-generation-webui,\
    \ but nothing's obviously wrong with it. It's 18 gigabytes. It should Just Work\
    \ on TGWUI.\nThe (shared between all experts) attention weights are 4 bit instead.\
    \ This adds almost nothing to the file size and nothing beyond that to the total\
    \ memory usage. I think\n\nThis isn't really one-upmanship at all though, zaq\
    \ likely has the upper hand. Exl2 is fast and their quant has, yknow, more of\
    \ the model left in it.\nThis is not so fast. I get 6 tokens per second with torch.compile(),\
    \ half that with straight torch and ATEN is between but it seems to encode very\
    \ quickly so it feels quite snappy. 6 is just about fast enough that I'm producing\
    \ tokens faster than I can really  'enjoy' so I'm happy. \n***and so I speculate\
    \ that this should have a stronger better understanding of long contexts, even\
    \ if you're less likely to generate one in the first place.\n\nthe real boon ought\
    \ to be contrastive search which i've kept forgetting to do. I'll go try it now."
  created_at: 2024-01-14 00:03:04+00:00
  edited: true
  hidden: false
  id: 65a324b8f95d78ab7664e254
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
      fullname: Undi
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Undi95
      type: user
    createdAt: '2024-01-14T03:11:53.000Z'
    data:
      edited: false
      editors:
      - Undi95
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9211029410362244
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ab1241ad514ca8d1430003/shjRx3yOP2eQvIyOWsQYV.png?w=200&h=200&f=face
          fullname: Undi
          isHf: false
          isPro: false
          name: Undi95
          type: user
        html: '<p>Thanks for this!</p>

          '
        raw: Thanks for this!
        updatedAt: '2024-01-14T03:11:53.495Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ProphetOfBostrom
    id: 65a350f93522df7a27c3defa
    type: comment
  author: Undi95
  content: Thanks for this!
  created_at: 2024-01-14 03:11:53+00:00
  edited: false
  hidden: false
  id: 65a350f93522df7a27c3defa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/SskUDcMVnMSZrCGcplXwv.png?w=200&h=200&f=face
      fullname: hyeonse o
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hyeonse
      type: user
    createdAt: '2024-01-14T05:51:11.000Z'
    data:
      edited: false
      editors:
      - hyeonse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950311541557312
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/SskUDcMVnMSZrCGcplXwv.png?w=200&h=200&f=face
          fullname: hyeonse o
          isHf: false
          isPro: false
          name: hyeonse
          type: user
        html: '<p>Amazing! I was hoping to see a HQQ of Noromix ever since I stumbled
          across this - <a rel="nofollow" href="https://github.com/dvmazur/mixtral-offloading">https://github.com/dvmazur/mixtral-offloading</a></p>

          <p>Would the expert offloading strategy work with this model? I am told
          HQQ + MoE offloading can make Mixtral 8x7b usable with 12gb, which means
          I could run this on my personal desktop.</p>

          '
        raw: 'Amazing! I was hoping to see a HQQ of Noromix ever since I stumbled
          across this - https://github.com/dvmazur/mixtral-offloading


          Would the expert offloading strategy work with this model? I am told HQQ
          + MoE offloading can make Mixtral 8x7b usable with 12gb, which means I could
          run this on my personal desktop.'
        updatedAt: '2024-01-14T05:51:11.585Z'
      numEdits: 0
      reactions: []
    id: 65a3764f212d6aca9aba5c65
    type: comment
  author: hyeonse
  content: 'Amazing! I was hoping to see a HQQ of Noromix ever since I stumbled across
    this - https://github.com/dvmazur/mixtral-offloading


    Would the expert offloading strategy work with this model? I am told HQQ + MoE
    offloading can make Mixtral 8x7b usable with 12gb, which means I could run this
    on my personal desktop.'
  created_at: 2024-01-14 05:51:11+00:00
  edited: false
  hidden: false
  id: 65a3764f212d6aca9aba5c65
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
      fullname: Albion Perfidia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ProphetOfBostrom
      type: user
    createdAt: '2024-01-14T08:08:53.000Z'
    data:
      edited: true
      editors:
      - ProphetOfBostrom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9621975421905518
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/644dbb7d53ad80c6593bdee1/YulTWMLWulSy-DvpnxwZF.jpeg?w=200&h=200&f=face
          fullname: Albion Perfidia
          isHf: false
          isPro: false
          name: ProphetOfBostrom
          type: user
        html: '<p>"Amazing! I was hoping to see a HQQ of Noromix"<br>Genuinely thought
          I''d never see a single download lol. Glad to be wrong. I''ve never seen
          this project before - but my understanding of what HQQ does suggests that
          my model should for 3 reasons.</p>

          <ol>

          <li>I followed the method mobius used for their reference mixtral quant.
          I think I''m being reasonable to assume that the HQQ Mixtral from the HQQ
          people will work with your HQQ Mixtral only loader - this can''t be far
          off.<br>And then I deleted the other reasons because</li>

          </ol>

          <h2 id="tldr-yes-itd-better-work">tl;dr. yes, it''d better work.</h2>

          <p>In fact I''m not very clear on why they''ve decided to be so specific
          about the model it loads. HQQ can do any transformer. </p>

          <p>I mean any transformer. Not "any llama2 language model". <a href="https://huggingface.co/mobiuslabsgmbh/CLIP-ViT-H-14-laion2B-2bit_g16_s128-HQQ">I
          mean here''s their reference ViT-H</a>. Which is part of <strong>stable
          diffusion</strong> 2, for instance. It''s an image processor. It doesn''t
          have any text input. so if my quant of a mixtral finetune isn''t compatible,
          I''m not inclined to pin it on anything HQQ-y. </p>

          <p>However, I was wondering why I hadn''t seen any discussion of CPU inference
          for HQQ, and I didn''t even have to resort to google, you just found one
          for me. Very cool! <strong>Please let me know if it works</strong>, <del>but
          I may well test it now.</del></p>

          <p>UPDATE: "<a rel="nofollow" href="https://www.youtube.com/watch?v=gvdf5n-zI14">For
          now, there is no command-line script available for running the model locally.
          However, you can create one using the demo notebook as a reference.</a>"
          </p>

          <p>Given how they say it works, you''re going to find that you''re pressed
          to squeeze every last MOE layer you can in to the GPU. I suggest, if there''s
          any options to adjust, you prioritise that when it comes to allocating VRAM.  Attention
          layers are much less, if not the least important thing to get to the GPU.
          <strong>and I expect that''s why they''re using more bits for them (as am
          I, <a href="https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ">as
          did the OGs</a>)- because they can get away with being slow to move.</strong>  *
          Caches go next. It''s the big dumb fully connected feedforward weights that
          really are best moved as little as possible. For mixtral, that''s the expert/MOE
          layers.</p>

          '
        raw: "\"Amazing! I was hoping to see a HQQ of Noromix\"\nGenuinely thought\
          \ I'd never see a single download lol. Glad to be wrong. I've never seen\
          \ this project before - but my understanding of what HQQ does suggests that\
          \ my model should for 3 reasons.\n1. I followed the method mobius used for\
          \ their reference mixtral quant. I think I'm being reasonable to assume\
          \ that the HQQ Mixtral from the HQQ people will work with your HQQ Mixtral\
          \ only loader - this can't be far off. \nAnd then I deleted the other reasons\
          \ because \n## tl;dr. yes, it'd better work. \nIn fact I'm not very clear\
          \ on why they've decided to be so specific about the model it loads. HQQ\
          \ can do any transformer. \n\nI mean any transformer. Not \"any llama2 language\
          \ model\". [I mean here's their reference ViT-H](https://huggingface.co/mobiuslabsgmbh/CLIP-ViT-H-14-laion2B-2bit_g16_s128-HQQ).\
          \ Which is part of **stable diffusion** 2, for instance. It's an image processor.\
          \ It doesn't have any text input. so if my quant of a mixtral finetune isn't\
          \ compatible, I'm not inclined to pin it on anything HQQ-y. \n\nHowever,\
          \ I was wondering why I hadn't seen any discussion of CPU inference for\
          \ HQQ, and I didn't even have to resort to google, you just found one for\
          \ me. Very cool! **Please let me know if it works**, ~but I may well test\
          \ it now.~\n\nUPDATE: \"[For now, there is no command-line script available\
          \ for running the model locally. However, you can create one using the demo\
          \ notebook as a reference.](https://www.youtube.com/watch?v=gvdf5n-zI14)\"\
          \ \n\nGiven how they say it works, you're going to find that you're pressed\
          \ to squeeze every last MOE layer you can in to the GPU. I suggest, if there's\
          \ any options to adjust, you prioritise that when it comes to allocating\
          \ VRAM.  Attention layers are much less, if not the least important thing\
          \ to get to the GPU. **and I expect that's why they're using more bits for\
          \ them (as am I, [as did the OGs](https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ))-\
          \ because they can get away with being slow to move.**  * Caches go next.\
          \ It's the big dumb fully connected feedforward weights that really are\
          \ best moved as little as possible. For mixtral, that's the expert/MOE layers.\n"
        updatedAt: '2024-01-16T03:49:18.050Z'
      numEdits: 7
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - hyeonse
    id: 65a39695d0e350dbc9e6ad44
    type: comment
  author: ProphetOfBostrom
  content: "\"Amazing! I was hoping to see a HQQ of Noromix\"\nGenuinely thought I'd\
    \ never see a single download lol. Glad to be wrong. I've never seen this project\
    \ before - but my understanding of what HQQ does suggests that my model should\
    \ for 3 reasons.\n1. I followed the method mobius used for their reference mixtral\
    \ quant. I think I'm being reasonable to assume that the HQQ Mixtral from the\
    \ HQQ people will work with your HQQ Mixtral only loader - this can't be far off.\
    \ \nAnd then I deleted the other reasons because \n## tl;dr. yes, it'd better\
    \ work. \nIn fact I'm not very clear on why they've decided to be so specific\
    \ about the model it loads. HQQ can do any transformer. \n\nI mean any transformer.\
    \ Not \"any llama2 language model\". [I mean here's their reference ViT-H](https://huggingface.co/mobiuslabsgmbh/CLIP-ViT-H-14-laion2B-2bit_g16_s128-HQQ).\
    \ Which is part of **stable diffusion** 2, for instance. It's an image processor.\
    \ It doesn't have any text input. so if my quant of a mixtral finetune isn't compatible,\
    \ I'm not inclined to pin it on anything HQQ-y. \n\nHowever, I was wondering why\
    \ I hadn't seen any discussion of CPU inference for HQQ, and I didn't even have\
    \ to resort to google, you just found one for me. Very cool! **Please let me know\
    \ if it works**, ~but I may well test it now.~\n\nUPDATE: \"[For now, there is\
    \ no command-line script available for running the model locally. However, you\
    \ can create one using the demo notebook as a reference.](https://www.youtube.com/watch?v=gvdf5n-zI14)\"\
    \ \n\nGiven how they say it works, you're going to find that you're pressed to\
    \ squeeze every last MOE layer you can in to the GPU. I suggest, if there's any\
    \ options to adjust, you prioritise that when it comes to allocating VRAM.  Attention\
    \ layers are much less, if not the least important thing to get to the GPU. **and\
    \ I expect that's why they're using more bits for them (as am I, [as did the OGs](https://huggingface.co/mobiuslabsgmbh/Mixtral-8x7B-v0.1-hf-attn-4bit-moe-2bit-HQQ))-\
    \ because they can get away with being slow to move.**  * Caches go next. It's\
    \ the big dumb fully connected feedforward weights that really are best moved\
    \ as little as possible. For mixtral, that's the expert/MOE layers.\n"
  created_at: 2024-01-14 08:08:53+00:00
  edited: true
  hidden: false
  id: 65a39695d0e350dbc9e6ad44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
      fullname: Zack Stone
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zaq-hack
      type: user
    createdAt: '2024-01-14T08:42:31.000Z'
    data:
      edited: false
      editors:
      - zaq-hack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9527799487113953
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63b534842bd3611e666bbdc3/TLhYkNTb7xKiKkyuBJAc1.png?w=200&h=200&f=face
          fullname: Zack Stone
          isHf: false
          isPro: false
          name: zaq-hack
          type: user
        html: '<p>Sweet ... might give it a try to compare with the EXL2. I''ve had
          mixed luck with this flavor of Noromaid vs. previous ones. Sometimes, it
          spits some hot RP. Sometimes, it spits strangely summarized crap-ola. (Although
          this version is able to have witty banter about The Princess Bride and Die
          Hard, which I have found hilarious.)</p>

          '
        raw: Sweet ... might give it a try to compare with the EXL2. I've had mixed
          luck with this flavor of Noromaid vs. previous ones. Sometimes, it spits
          some hot RP. Sometimes, it spits strangely summarized crap-ola. (Although
          this version is able to have witty banter about The Princess Bride and Die
          Hard, which I have found hilarious.)
        updatedAt: '2024-01-14T08:42:31.932Z'
      numEdits: 0
      reactions: []
    id: 65a39e7741b6ef119cce9a50
    type: comment
  author: zaq-hack
  content: Sweet ... might give it a try to compare with the EXL2. I've had mixed
    luck with this flavor of Noromaid vs. previous ones. Sometimes, it spits some
    hot RP. Sometimes, it spits strangely summarized crap-ola. (Although this version
    is able to have witty banter about The Princess Bride and Die Hard, which I have
    found hilarious.)
  created_at: 2024-01-14 08:42:31+00:00
  edited: false
  hidden: false
  id: 65a39e7741b6ef119cce9a50
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: NeverSleep/Noromaid-v0.4-Mixtral-Instruct-8x7b-Zloss
repo_type: model
status: open
target_branch: null
title: 1.64-upmanship (2 bit HQQ quantize)
