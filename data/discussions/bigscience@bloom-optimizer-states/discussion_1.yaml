!!python/object:huggingface_hub.community.DiscussionWithDetails
author: litchi
conflicting_files: null
created_at: 2023-03-29 08:59:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c359831a370e5442f30fabb98dc9095.svg
      fullname: yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: litchi
      type: user
    createdAt: '2023-03-29T09:59:55.000Z'
    data:
      edited: true
      editors:
      - litchi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c359831a370e5442f30fabb98dc9095.svg
          fullname: yang
          isHf: false
          isPro: false
          name: litchi
          type: user
        html: "<p>there is no  weight of layer0,layer1?<br>I didn't find model file\
          \ as follow:<br>1.input_layernorm.bias<br>1.input_layernorm.weight<br>1.mlp.dense_4h_to_h.bias<br>1.mlp.dense_4h_to_h.weight<br>1.mlp.dense_h_to_4h.bias<br>1.mlp.dense_h_to_4h.weight<br>1.post_attention_layernorm.bias<br>1.post_attention_layernorm.weight<br>1.self_attention.dense.bias<br>1.self_attention.dense.weight<br>1.self_attention.query_key_value.bias<br>1.self_attention.query_key_value.weight<br><span\
          \ data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\">@<span\
          \ class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span>\
          \  <span data-props=\"{&quot;user&quot;:&quot;stas&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/stas\">@<span class=\"\
          underline\">stas</span></a></span>\n\n\t</span></span></p>\n"
        raw: "there is no  weight of layer0,layer1? \nI didn't find model file as\
          \ follow:\n1.input_layernorm.bias\n1.input_layernorm.weight\n1.mlp.dense_4h_to_h.bias\n\
          1.mlp.dense_4h_to_h.weight\n1.mlp.dense_h_to_4h.bias\n1.mlp.dense_h_to_4h.weight\n\
          1.post_attention_layernorm.bias\n1.post_attention_layernorm.weight\n1.self_attention.dense.bias\n\
          1.self_attention.dense.weight\n1.self_attention.query_key_value.bias\n1.self_attention.query_key_value.weight\n\
          @Muennighoff  @stas"
        updatedAt: '2023-03-29T10:07:37.930Z'
      numEdits: 1
      reactions: []
    id: 64240c1b3fa01ecba6fd6bf1
    type: comment
  author: litchi
  content: "there is no  weight of layer0,layer1? \nI didn't find model file as follow:\n\
    1.input_layernorm.bias\n1.input_layernorm.weight\n1.mlp.dense_4h_to_h.bias\n1.mlp.dense_4h_to_h.weight\n\
    1.mlp.dense_h_to_4h.bias\n1.mlp.dense_h_to_4h.weight\n1.post_attention_layernorm.bias\n\
    1.post_attention_layernorm.weight\n1.self_attention.dense.bias\n1.self_attention.dense.weight\n\
    1.self_attention.query_key_value.bias\n1.self_attention.query_key_value.weight\n\
    @Muennighoff  @stas"
  created_at: 2023-03-29 08:59:55+00:00
  edited: true
  hidden: false
  id: 64240c1b3fa01ecba6fd6bf1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7c359831a370e5442f30fabb98dc9095.svg
      fullname: yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: litchi
      type: user
    createdAt: '2023-03-29T10:07:40.000Z'
    data:
      status: closed
    id: 64240dec42eda9692269bc5c
    type: status-change
  author: litchi
  created_at: 2023-03-29 09:07:40+00:00
  id: 64240dec42eda9692269bc5c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/7c359831a370e5442f30fabb98dc9095.svg
      fullname: yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: litchi
      type: user
    createdAt: '2023-03-29T10:07:47.000Z'
    data:
      status: open
    id: 64240df380b5c89bfa786965
    type: status-change
  author: litchi
  created_at: 2023-03-29 09:07:47+00:00
  id: 64240df380b5c89bfa786965
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-03-29T10:46:30.000Z'
    data:
      edited: true
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>Not 100% sure, but I think it''s because 1 is the tied embeddings
          which are not numbered but in <code>tied_modules.</code></p>

          '
        raw: Not 100% sure, but I think it's because 1 is the tied embeddings which
          are not numbered but in `tied_modules.`
        updatedAt: '2023-03-29T10:46:41.832Z'
      numEdits: 1
      reactions: []
    id: 6424170673bbae7a6d844e5b
    type: comment
  author: Muennighoff
  content: Not 100% sure, but I think it's because 1 is the tied embeddings which
    are not numbered but in `tied_modules.`
  created_at: 2023-03-29 09:46:30+00:00
  edited: true
  hidden: false
  id: 6424170673bbae7a6d844e5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c359831a370e5442f30fabb98dc9095.svg
      fullname: yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: litchi
      type: user
    createdAt: '2023-03-29T11:12:50.000Z'
    data:
      edited: true
      editors:
      - litchi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c359831a370e5442f30fabb98dc9095.svg
          fullname: yang
          isHf: false
          isPro: false
          name: litchi
          type: user
        html: "<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63f317280a16587ea97171b7/vba7aWgocvAE4GSa_4ROm.jpeg\"\
          ><img alt=\"20230329-191114.jpg\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63f317280a16587ea97171b7/vba7aWgocvAE4GSa_4ROm.jpeg\"\
          ></a><br> I find  model file of layer.10 is about 27GB\uFF08the same as\
          \ layer.11\uFF09, but tied_modules only 42GB.<br>layer.0   + layer.1  =\
          \ 27GB x2 =54GB.  </p>\n<p>I don't think the tied_modules includes  the\
          \ layer0 and layer1\uFF0C Is that correct?<br><span data-props=\"{&quot;user&quot;:&quot;Muennighoff&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Muennighoff\"\
          >@<span class=\"underline\">Muennighoff</span></a></span>\n\n\t</span></span></p>\n"
        raw: "![20230329-191114.jpg](https://cdn-uploads.huggingface.co/production/uploads/63f317280a16587ea97171b7/vba7aWgocvAE4GSa_4ROm.jpeg)\n\
          \ I find  model file of layer.10 is about 27GB\uFF08the same as layer.11\uFF09\
          , but tied_modules only 42GB.\nlayer.0   + layer.1  = 27GB x2 =54GB.  \n\
          \nI don't think the tied_modules includes  the layer0 and layer1\uFF0C Is\
          \ that correct?\n@Muennighoff"
        updatedAt: '2023-03-29T11:18:09.889Z'
      numEdits: 2
      reactions: []
    id: 64241d32a3d22b240e0f0567
    type: comment
  author: litchi
  content: "![20230329-191114.jpg](https://cdn-uploads.huggingface.co/production/uploads/63f317280a16587ea97171b7/vba7aWgocvAE4GSa_4ROm.jpeg)\n\
    \ I find  model file of layer.10 is about 27GB\uFF08the same as layer.11\uFF09\
    , but tied_modules only 42GB.\nlayer.0   + layer.1  = 27GB x2 =54GB.  \n\nI don't\
    \ think the tied_modules includes  the layer0 and layer1\uFF0C Is that correct?\n\
    @Muennighoff"
  created_at: 2023-03-29 10:12:50+00:00
  edited: true
  hidden: false
  id: 64241d32a3d22b240e0f0567
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
      fullname: Niklas Muennighoff
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Muennighoff
      type: user
    createdAt: '2023-03-29T11:31:56.000Z'
    data:
      edited: false
      editors:
      - Muennighoff
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1659799717718-5f1eb362eec0ad2a071ad6e2.jpeg?w=200&h=200&f=face
          fullname: Niklas Muennighoff
          isHf: false
          isPro: false
          name: Muennighoff
          type: user
        html: '<p>I don''t think there is a layer.0 (see <a href="https://huggingface.co/bigscience/bloom-optimizer-states/tree/main/global_step95000">https://huggingface.co/bigscience/bloom-optimizer-states/tree/main/global_step95000</a>);
          The numbering is a bit weird because it includes layers that do not have
          parameters hence some numbers are missing; Just try loading it &amp; you
          will see if sth is missing - I think it should work</p>

          '
        raw: I don't think there is a layer.0 (see https://huggingface.co/bigscience/bloom-optimizer-states/tree/main/global_step95000);
          The numbering is a bit weird because it includes layers that do not have
          parameters hence some numbers are missing; Just try loading it & you will
          see if sth is missing - I think it should work
        updatedAt: '2023-03-29T11:31:56.113Z'
      numEdits: 0
      reactions: []
    id: 642421ac73bbae7a6d84a273
    type: comment
  author: Muennighoff
  content: I don't think there is a layer.0 (see https://huggingface.co/bigscience/bloom-optimizer-states/tree/main/global_step95000);
    The numbering is a bit weird because it includes layers that do not have parameters
    hence some numbers are missing; Just try loading it & you will see if sth is missing
    - I think it should work
  created_at: 2023-03-29 10:31:56+00:00
  edited: false
  hidden: false
  id: 642421ac73bbae7a6d84a273
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bigscience/bloom-optimizer-states
repo_type: model
status: open
target_branch: null
title: "branch\uFF1Aglobal_step95000_universal \uFF0Cthere is no  weight file of layer0,layer1? "
