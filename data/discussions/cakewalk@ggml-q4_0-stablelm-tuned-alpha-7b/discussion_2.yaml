!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LaferriereJC
conflicting_files: null
created_at: 2023-04-20 03:23:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-04-20T04:23:29.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>I tried text-generation-webui, and it detects llama but fails to
          run yet I can run</p>

          <p>python server.py --model ggml-alpaca-7b-q4 --listen</p>

          '
        raw: "I tried text-generation-webui, and it detects llama but fails to run\
          \ yet I can run\r\n\r\npython server.py --model ggml-alpaca-7b-q4 --listen"
        updatedAt: '2023-04-20T04:23:29.310Z'
      numEdits: 0
      reactions: []
    id: 6440be417841867cd5b76a1f
    type: comment
  author: LaferriereJC
  content: "I tried text-generation-webui, and it detects llama but fails to run yet\
    \ I can run\r\n\r\npython server.py --model ggml-alpaca-7b-q4 --listen"
  created_at: 2023-04-20 03:23:29+00:00
  edited: false
  hidden: false
  id: 6440be417841867cd5b76a1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/641aceb7e3d2eebea49f397c38048d0b.svg
      fullname: Dmitry Chestnykh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dchest
      type: user
    createdAt: '2023-04-20T09:02:18.000Z'
    data:
      edited: false
      editors:
      - dchest
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/641aceb7e3d2eebea49f397c38048d0b.svg
          fullname: Dmitry Chestnykh
          isHf: false
          isPro: false
          name: dchest
          type: user
        html: '<p>It runs via this cformers fork <a rel="nofollow" href="https://github.com/antimatter15/cformers">https://github.com/antimatter15/cformers</a></p>

          '
        raw: It runs via this cformers fork https://github.com/antimatter15/cformers
        updatedAt: '2023-04-20T09:02:18.796Z'
      numEdits: 0
      reactions: []
    id: 6440ff9ae46e14ed55838d11
    type: comment
  author: dchest
  content: It runs via this cformers fork https://github.com/antimatter15/cformers
  created_at: 2023-04-20 08:02:18+00:00
  edited: false
  hidden: false
  id: 6440ff9ae46e14ed55838d11
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676331053953-noauth.jpeg?w=200&h=200&f=face
      fullname: Kim, Jinwhan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mozzipa
      type: user
    createdAt: '2023-04-20T13:45:47.000Z'
    data:
      edited: false
      editors:
      - Mozzipa
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1676331053953-noauth.jpeg?w=200&h=200&f=face
          fullname: Kim, Jinwhan
          isHf: false
          isPro: false
          name: Mozzipa
          type: user
        html: '<p>I tried cformer with M1 mac. But its response is only blank.<br>If
          I input "hi" on "&gt;" , nothing appears . And "&gt;" again.</p>

          <pre><code>&gt; Hi



          &gt;

          </code></pre>

          '
        raw: 'I tried cformer with M1 mac. But its response is only blank.

          If I input "hi" on ">" , nothing appears . And ">" again.


          ```

          > Hi



          >

          ```'
        updatedAt: '2023-04-20T13:45:47.784Z'
      numEdits: 0
      reactions: []
    id: 6441420be46e14ed558ced80
    type: comment
  author: Mozzipa
  content: 'I tried cformer with M1 mac. But its response is only blank.

    If I input "hi" on ">" , nothing appears . And ">" again.


    ```

    > Hi



    >

    ```'
  created_at: 2023-04-20 12:45:47+00:00
  edited: false
  hidden: false
  id: 6441420be46e14ed558ced80
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-04-20T13:54:45.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>I tried to modify interface as such<br>    # stablelm<br>    ''cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b'':
          ModelUrlMap(<br>        cpp_model_name="gptneox",<br>        int4_fixed_zero="<a
          href="https://huggingface.co/cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b/resolve/main/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin&quot;">https://huggingface.co/cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b/resolve/main/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin"</a>),</p>

          <p>and chat.py<br>model_map = {''stablelm'': ''cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b'',
          ''pythia'': ''OpenAssistant/oasst-sft-1-pythia-12b'', ''bloom'': ''bigscience/bloom-7b1'',
          ''gptj'': ''Eleuther$</p>

          <p>but when I attempt to run<br>python chat.py -m stablelm</p>

          <p>I get an error<br>cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b does not
          appear to have a file named config.json</p>

          <p>but none of the other models have config.json</p>

          <p>Do you have instructions for how to set this up?</p>

          <p>I assumed gptneox from looking at the config.json''s for the stablelm''s
          models (i.e. <a href="https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/blob/main/config.json">https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/blob/main/config.json</a>)</p>

          '
        raw: "I tried to modify interface as such\n    # stablelm\n    'cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b':\
          \ ModelUrlMap(\n        cpp_model_name=\"gptneox\",\n        int4_fixed_zero=\"\
          https://huggingface.co/cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b/resolve/main/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin\"\
          ),\n\nand chat.py\nmodel_map = {'stablelm': 'cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b',\
          \ 'pythia': 'OpenAssistant/oasst-sft-1-pythia-12b', 'bloom': 'bigscience/bloom-7b1',\
          \ 'gptj': 'Eleuther$\n\nbut when I attempt to run\npython chat.py -m stablelm\n\
          \nI get an error\ncakewalk/ggml-q4_0-stablelm-tuned-alpha-7b does not appear\
          \ to have a file named config.json\n\nbut none of the other models have\
          \ config.json\n\nDo you have instructions for how to set this up?\n\nI assumed\
          \ gptneox from looking at the config.json's for the stablelm's models (i.e.\
          \ https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/blob/main/config.json)"
        updatedAt: '2023-04-20T13:54:45.626Z'
      numEdits: 0
      reactions: []
    id: 644144257f13a7b5a2623e25
    type: comment
  author: LaferriereJC
  content: "I tried to modify interface as such\n    # stablelm\n    'cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b':\
    \ ModelUrlMap(\n        cpp_model_name=\"gptneox\",\n        int4_fixed_zero=\"\
    https://huggingface.co/cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b/resolve/main/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin\"\
    ),\n\nand chat.py\nmodel_map = {'stablelm': 'cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b',\
    \ 'pythia': 'OpenAssistant/oasst-sft-1-pythia-12b', 'bloom': 'bigscience/bloom-7b1',\
    \ 'gptj': 'Eleuther$\n\nbut when I attempt to run\npython chat.py -m stablelm\n\
    \nI get an error\ncakewalk/ggml-q4_0-stablelm-tuned-alpha-7b does not appear to\
    \ have a file named config.json\n\nbut none of the other models have config.json\n\
    \nDo you have instructions for how to set this up?\n\nI assumed gptneox from looking\
    \ at the config.json's for the stablelm's models (i.e. https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b/blob/main/config.json)"
  created_at: 2023-04-20 12:54:45+00:00
  edited: false
  hidden: false
  id: 644144257f13a7b5a2623e25
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a731b50c22aaeeffb56a929295011362.svg
      fullname: Den Millman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: donkilluminatti
      type: user
    createdAt: '2023-04-20T15:03:45.000Z'
    data:
      edited: true
      editors:
      - donkilluminatti
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a731b50c22aaeeffb56a929295011362.svg
          fullname: Den Millman
          isHf: false
          isPro: false
          name: donkilluminatti
          type: user
        html: '<p>There is a Windows fork which, when started, will ask which file
          to select <a rel="nofollow" href="https://github.com/LostRuins/koboldcpp">https://github.com/LostRuins/koboldcpp</a><br>Support
          GGML models<br>!!!Pardon - this model does not start</p>

          '
        raw: 'There is a Windows fork which, when started, will ask which file to
          select https://github.com/LostRuins/koboldcpp

          Support GGML models

          !!!Pardon - this model does not start'
        updatedAt: '2023-04-20T15:36:15.214Z'
      numEdits: 3
      reactions: []
    id: 64415451e46e14ed558f6c44
    type: comment
  author: donkilluminatti
  content: 'There is a Windows fork which, when started, will ask which file to select
    https://github.com/LostRuins/koboldcpp

    Support GGML models

    !!!Pardon - this model does not start'
  created_at: 2023-04-20 14:03:45+00:00
  edited: true
  hidden: false
  id: 64415451e46e14ed558f6c44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-04-20T22:59:57.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>I see the cformers link you provided had been updated to include
          an option for this model.</p>

          '
        raw: I see the cformers link you provided had been updated to include an option
          for this model.
        updatedAt: '2023-04-20T22:59:57.030Z'
      numEdits: 0
      reactions: []
    id: 6441c3ed0771bec9d26884f2
    type: comment
  author: LaferriereJC
  content: I see the cformers link you provided had been updated to include an option
    for this model.
  created_at: 2023-04-20 21:59:57+00:00
  edited: false
  hidden: false
  id: 6441c3ed0771bec9d26884f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c84200d82b2a5c0729007e5a991fe16.svg
      fullname: zatochu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zatochu
      type: user
    createdAt: '2023-04-20T23:08:23.000Z'
    data:
      edited: true
      editors:
      - zatochu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c84200d82b2a5c0729007e5a991fe16.svg
          fullname: zatochu
          isHf: false
          isPro: false
          name: zatochu
          type: user
        html: '<p>It should run with llama.cpp now as of this commit. <a rel="nofollow"
          href="https://github.com/ggerganov/llama.cpp/commit/12b5900dbc9743dee3ce83513cf5c3a44523a1b6">https://github.com/ggerganov/llama.cpp/commit/12b5900dbc9743dee3ce83513cf5c3a44523a1b6</a><br>Edit:
          Maybe not?</p>

          '
        raw: 'It should run with llama.cpp now as of this commit. https://github.com/ggerganov/llama.cpp/commit/12b5900dbc9743dee3ce83513cf5c3a44523a1b6

          Edit: Maybe not?'
        updatedAt: '2023-04-20T23:09:43.410Z'
      numEdits: 1
      reactions: []
    id: 6441c5e7a839ee80331e94f0
    type: comment
  author: zatochu
  content: 'It should run with llama.cpp now as of this commit. https://github.com/ggerganov/llama.cpp/commit/12b5900dbc9743dee3ce83513cf5c3a44523a1b6

    Edit: Maybe not?'
  created_at: 2023-04-20 22:08:23+00:00
  edited: true
  hidden: false
  id: 6441c5e7a839ee80331e94f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
      fullname: Joshua Laferriere
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LaferriereJC
      type: user
    createdAt: '2023-04-20T23:50:49.000Z'
    data:
      edited: false
      editors:
      - LaferriereJC
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e8da869c47dbc3f9052f1cbd4cc5ae6.svg
          fullname: Joshua Laferriere
          isHf: false
          isPro: false
          name: LaferriereJC
          type: user
        html: '<p>My concern is it''s not going to respect the stop tokens identified
          in<br><a href="https://huggingface.co/vvsotnikov/stablelm-tuned-alpha-3b-16bit">https://huggingface.co/vvsotnikov/stablelm-tuned-alpha-3b-16bit</a></p>

          <p>as a result, I''m looking at the above model with deepspeed and hard
          coded the class into text-generation_webui''s load_preset_values</p>

          <p>The code worked with cformers, and it was fast, but it was generating
          a lot of run-on sentences.</p>

          '
        raw: "My concern is it's not going to respect the stop tokens identified in\
          \ \nhttps://huggingface.co/vvsotnikov/stablelm-tuned-alpha-3b-16bit\n\n\
          as a result, I'm looking at the above model with deepspeed and hard coded\
          \ the class into text-generation_webui's load_preset_values\n\nThe code\
          \ worked with cformers, and it was fast, but it was generating a lot of\
          \ run-on sentences."
        updatedAt: '2023-04-20T23:50:49.670Z'
      numEdits: 0
      reactions: []
    id: 6441cfd9a839ee80331f6272
    type: comment
  author: LaferriereJC
  content: "My concern is it's not going to respect the stop tokens identified in\
    \ \nhttps://huggingface.co/vvsotnikov/stablelm-tuned-alpha-3b-16bit\n\nas a result,\
    \ I'm looking at the above model with deepspeed and hard coded the class into\
    \ text-generation_webui's load_preset_values\n\nThe code worked with cformers,\
    \ and it was fast, but it was generating a lot of run-on sentences."
  created_at: 2023-04-20 22:50:49+00:00
  edited: false
  hidden: false
  id: 6441cfd9a839ee80331f6272
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-04-24T01:13:12.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: '<p>will this work with <a rel="nofollow" href="https://github.com/ggerganov/ggml/tree/master/examples/stablelm">https://github.com/ggerganov/ggml/tree/master/examples/stablelm</a>
          ? Seems like this model has the incorrect n_ctx and n_embd sizes?</p>

          <p>main: seed = 1682298523<br>stablelm_model_load: loading model from ''../../models/cakewalk__ggml-q4_0-stablelm-tuned-alpha-7b/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin''
          - please wait ...<br>stablelm_model_load: n_vocab = 50432<br>stablelm_model_load:
          n_ctx   = 6144<br>stablelm_model_load: n_embd  = 48<br>stablelm_model_load:
          n_head  = 16<br>stablelm_model_load: n_layer = 32<br>stablelm_model_load:
          n_rot   = 1<br>stablelm_model_load: ftype   = 2<br>stablelm_model_load:
          ggml ctx size =  75.89 MB<br>stablelm_model_load: memory_size =    36.00
          MB, n_mem = 196608<br>stablelm_model_load: tensor ''gpt_neox.embed_in.weight''
          has wrong size in model file<br>main: failed to load model from ''../../models/cakewalk__ggml-q4_0-stablelm-tuned-alpha-7b/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin''<br>stablelm_model_load:
          %</p>

          '
        raw: 'will this work with https://github.com/ggerganov/ggml/tree/master/examples/stablelm
          ? Seems like this model has the incorrect n_ctx and n_embd sizes?


          main: seed = 1682298523

          stablelm_model_load: loading model from ''../../models/cakewalk__ggml-q4_0-stablelm-tuned-alpha-7b/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin''
          - please wait ...

          stablelm_model_load: n_vocab = 50432

          stablelm_model_load: n_ctx   = 6144

          stablelm_model_load: n_embd  = 48

          stablelm_model_load: n_head  = 16

          stablelm_model_load: n_layer = 32

          stablelm_model_load: n_rot   = 1

          stablelm_model_load: ftype   = 2

          stablelm_model_load: ggml ctx size =  75.89 MB

          stablelm_model_load: memory_size =    36.00 MB, n_mem = 196608

          stablelm_model_load: tensor ''gpt_neox.embed_in.weight'' has wrong size
          in model file

          main: failed to load model from ''../../models/cakewalk__ggml-q4_0-stablelm-tuned-alpha-7b/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin''

          stablelm_model_load: %'
        updatedAt: '2023-04-24T01:13:12.969Z'
      numEdits: 0
      reactions: []
    id: 6445d7a80f2fc80feb2f2f2b
    type: comment
  author: winglian
  content: 'will this work with https://github.com/ggerganov/ggml/tree/master/examples/stablelm
    ? Seems like this model has the incorrect n_ctx and n_embd sizes?


    main: seed = 1682298523

    stablelm_model_load: loading model from ''../../models/cakewalk__ggml-q4_0-stablelm-tuned-alpha-7b/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin''
    - please wait ...

    stablelm_model_load: n_vocab = 50432

    stablelm_model_load: n_ctx   = 6144

    stablelm_model_load: n_embd  = 48

    stablelm_model_load: n_head  = 16

    stablelm_model_load: n_layer = 32

    stablelm_model_load: n_rot   = 1

    stablelm_model_load: ftype   = 2

    stablelm_model_load: ggml ctx size =  75.89 MB

    stablelm_model_load: memory_size =    36.00 MB, n_mem = 196608

    stablelm_model_load: tensor ''gpt_neox.embed_in.weight'' has wrong size in model
    file

    main: failed to load model from ''../../models/cakewalk__ggml-q4_0-stablelm-tuned-alpha-7b/ggml-model-stablelm-tuned-alpha-7b-q4_0.bin''

    stablelm_model_load: %'
  created_at: 2023-04-24 00:13:12+00:00
  edited: false
  hidden: false
  id: 6445d7a80f2fc80feb2f2f2b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: cakewalk/ggml-q4_0-stablelm-tuned-alpha-7b
repo_type: model
status: open
target_branch: null
title: How do you run this?
