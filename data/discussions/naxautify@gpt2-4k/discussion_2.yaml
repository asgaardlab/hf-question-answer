!!python/object:huggingface_hub.community.DiscussionWithDetails
author: venketh
conflicting_files: null
created_at: 2023-11-09 13:06:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2cc5f6ee43e3648726fb3cf833e34dab.svg
      fullname: Venkatesh Srinivas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: venketh
      type: user
    createdAt: '2023-11-09T13:06:48.000Z'
    data:
      edited: false
      editors:
      - venketh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8854220509529114
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2cc5f6ee43e3648726fb3cf833e34dab.svg
          fullname: Venkatesh Srinivas
          isHf: false
          isPro: false
          name: venketh
          type: user
        html: '<ul>

          <li>Fine-tune GPT2 or train from scratch? (AFAICT it''s challenging to fine-tune
          w/ different n_ctx than a base model)</li>

          <li>What trainer?</li>

          <li>I''m seeing the weights tensor have a dim of [2048,768] rather than
          the expected [4096,768] for "gpt2-4k"; is this a -2k context gpt2?</li>

          <li>Should n_ctx/n_positions in config.json be updated?</li>

          </ul>

          '
        raw: "* Fine-tune GPT2 or train from scratch? (AFAICT it's challenging to\
          \ fine-tune w/ different n_ctx than a base model)\r\n* What trainer?\r\n\
          * I'm seeing the weights tensor have a dim of [2048,768] rather than the\
          \ expected [4096,768] for \"gpt2-4k\"; is this a -2k context gpt2?\r\n*\
          \ Should n_ctx/n_positions in config.json be updated?"
        updatedAt: '2023-11-09T13:06:48.104Z'
      numEdits: 0
      reactions: []
    id: 654cd968ee0ca47d70dc3c43
    type: comment
  author: venketh
  content: "* Fine-tune GPT2 or train from scratch? (AFAICT it's challenging to fine-tune\
    \ w/ different n_ctx than a base model)\r\n* What trainer?\r\n* I'm seeing the\
    \ weights tensor have a dim of [2048,768] rather than the expected [4096,768]\
    \ for \"gpt2-4k\"; is this a -2k context gpt2?\r\n* Should n_ctx/n_positions in\
    \ config.json be updated?"
  created_at: 2023-11-09 13:06:48+00:00
  edited: false
  hidden: false
  id: 654cd968ee0ca47d70dc3c43
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: naxautify/gpt2-4k
repo_type: model
status: open
target_branch: null
title: How did you train / FT this?
