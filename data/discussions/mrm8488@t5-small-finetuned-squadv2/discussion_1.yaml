!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ayazdan
conflicting_files: null
created_at: 2022-10-24 15:53:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/76e5e7b51515b58e207c7d2a4b360d30.svg
      fullname: Amir Yazdanbakhsh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ayazdan
      type: user
    createdAt: '2022-10-24T16:53:11.000Z'
    data:
      edited: false
      editors:
      - ayazdan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/76e5e7b51515b58e207c7d2a4b360d30.svg
          fullname: Amir Yazdanbakhsh
          isHf: false
          isPro: false
          name: ayazdan
          type: user
        html: "<p>I am trying to use the canonical <code>run_seq2seq_qa</code> to\
          \ finetune T5-small on SquadV2, but so far I was not able to reach to the\
          \ f1 accuracy that you have achieved (eval_f1 ~ 66). Do you have any suggestions\
          \ how to modify the script for a better finetuning?</p>\n<pre><code>python3\
          \ transformer-sparsity/examples/pytorch/question-answering/run_seq2seq_qa.py\
          \ \\\n            --model_name_or_path t5-small \\\n            --dataset_name\
          \ squad_v2 \\\n            --context_column context \\\n            --question_column\
          \ question \\\n            --answer_column answers \\\n            --do_train\
          \ \\\n            --do_eval \\\n            --per_device_train_batch_size\
          \ 16 \\\n            --per_device_eval_batch_size 8 \\\n            --learning_rate\
          \ 3e-5 \\\n            --num_train_epochs 10 \\\n            --max_seq_length\
          \ 384 \\\n            --doc_stride 128 \\\n            --load_best_model_at_end\
          \ \\\n            --eval_steps ${eval_steps} \\\n            --save_steps\
          \ ${eval_steps} \\\n            --evaluation_strategy steps \\\n       \
          \     --logging_steps ${eval_steps} \\\n            --version_2_with_negative\
          \ \\\n            --logging_strategy steps \\\n            --save_total_limit\
          \ 4 \\\n            --greater_is_better true \\\n            --metric_for_best_model\
          \ f1 \\\n            --label_names \"start_positions\", \"end_positions\"\
          \ \\\n            --predict_with_generate \\\n            --overwrite_output_dir\
          \ \\\n            --output_dir ${ckpt_path} 2&gt;&amp;1 | tee ~/${ckpt_path}/finetune_run_$(date\
          \ +\"%Y_%m_%d_%I_%M_%p\").log\n</code></pre>\n"
        raw: "I am trying to use the canonical `run_seq2seq_qa` to finetune T5-small\
          \ on SquadV2, but so far I was not able to reach to the f1 accuracy that\
          \ you have achieved (eval_f1 ~ 66). Do you have any suggestions how to modify\
          \ the script for a better finetuning?\r\n\r\n```\r\npython3 transformer-sparsity/examples/pytorch/question-answering/run_seq2seq_qa.py\
          \ \\\r\n\t\t\t--model_name_or_path t5-small \\\r\n\t\t\t--dataset_name squad_v2\
          \ \\\r\n\t\t\t--context_column context \\\r\n\t\t\t--question_column question\
          \ \\\r\n\t\t\t--answer_column answers \\\r\n\t\t\t--do_train \\\r\n\t\t\t\
          --do_eval \\\r\n\t\t\t--per_device_train_batch_size 16 \\\r\n\t\t\t--per_device_eval_batch_size\
          \ 8 \\\r\n\t\t\t--learning_rate 3e-5 \\\r\n\t\t\t--num_train_epochs 10 \\\
          \r\n\t\t\t--max_seq_length 384 \\\r\n\t\t\t--doc_stride 128 \\\r\n\t\t\t\
          --load_best_model_at_end \\\r\n\t\t\t--eval_steps ${eval_steps} \\\r\n\t\
          \t\t--save_steps ${eval_steps} \\\r\n\t\t\t--evaluation_strategy steps \\\
          \r\n\t\t\t--logging_steps ${eval_steps} \\\r\n\t\t\t--version_2_with_negative\
          \ \\\r\n\t\t\t--logging_strategy steps \\\r\n\t\t\t--save_total_limit 4\
          \ \\\r\n\t\t\t--greater_is_better true \\\r\n\t\t\t--metric_for_best_model\
          \ f1 \\\r\n\t\t\t--label_names \"start_positions\", \"end_positions\" \\\
          \r\n\t\t\t--predict_with_generate \\\r\n\t\t\t--overwrite_output_dir \\\r\
          \n\t\t\t--output_dir ${ckpt_path} 2>&1 | tee ~/${ckpt_path}/finetune_run_$(date\
          \ +\"%Y_%m_%d_%I_%M_%p\").log\r\n```"
        updatedAt: '2022-10-24T16:53:11.985Z'
      numEdits: 0
      reactions: []
    id: 6356c2f73c32f2c90f4dbede
    type: comment
  author: ayazdan
  content: "I am trying to use the canonical `run_seq2seq_qa` to finetune T5-small\
    \ on SquadV2, but so far I was not able to reach to the f1 accuracy that you have\
    \ achieved (eval_f1 ~ 66). Do you have any suggestions how to modify the script\
    \ for a better finetuning?\r\n\r\n```\r\npython3 transformer-sparsity/examples/pytorch/question-answering/run_seq2seq_qa.py\
    \ \\\r\n\t\t\t--model_name_or_path t5-small \\\r\n\t\t\t--dataset_name squad_v2\
    \ \\\r\n\t\t\t--context_column context \\\r\n\t\t\t--question_column question\
    \ \\\r\n\t\t\t--answer_column answers \\\r\n\t\t\t--do_train \\\r\n\t\t\t--do_eval\
    \ \\\r\n\t\t\t--per_device_train_batch_size 16 \\\r\n\t\t\t--per_device_eval_batch_size\
    \ 8 \\\r\n\t\t\t--learning_rate 3e-5 \\\r\n\t\t\t--num_train_epochs 10 \\\r\n\t\
    \t\t--max_seq_length 384 \\\r\n\t\t\t--doc_stride 128 \\\r\n\t\t\t--load_best_model_at_end\
    \ \\\r\n\t\t\t--eval_steps ${eval_steps} \\\r\n\t\t\t--save_steps ${eval_steps}\
    \ \\\r\n\t\t\t--evaluation_strategy steps \\\r\n\t\t\t--logging_steps ${eval_steps}\
    \ \\\r\n\t\t\t--version_2_with_negative \\\r\n\t\t\t--logging_strategy steps \\\
    \r\n\t\t\t--save_total_limit 4 \\\r\n\t\t\t--greater_is_better true \\\r\n\t\t\
    \t--metric_for_best_model f1 \\\r\n\t\t\t--label_names \"start_positions\", \"\
    end_positions\" \\\r\n\t\t\t--predict_with_generate \\\r\n\t\t\t--overwrite_output_dir\
    \ \\\r\n\t\t\t--output_dir ${ckpt_path} 2>&1 | tee ~/${ckpt_path}/finetune_run_$(date\
    \ +\"%Y_%m_%d_%I_%M_%p\").log\r\n```"
  created_at: 2022-10-24 15:53:11+00:00
  edited: false
  hidden: false
  id: 6356c2f73c32f2c90f4dbede
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mrm8488/t5-small-finetuned-squadv2
repo_type: model
status: open
target_branch: null
title: 'Using `run_seq2seq_qa` to finetune t5-small on SquadV2 '
