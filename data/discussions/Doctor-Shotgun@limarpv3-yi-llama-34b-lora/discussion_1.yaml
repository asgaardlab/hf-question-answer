!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lmg-anon
conflicting_files: null
created_at: 2023-11-08 02:59:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmg-anon
      type: user
    createdAt: '2023-11-08T02:59:29.000Z'
    data:
      edited: true
      editors:
      - lmg-anon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8624157309532166
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: lmg-anon
          type: user
        html: '<p>Did you fine-tune the model with or without BOS? Apparently this
          model works much better without BOS.<br><a rel="nofollow" href="https://github.com/01-ai/Yi/discussions/5#discussioncomment-7484547">https://github.com/01-ai/Yi/discussions/5#discussioncomment-7484547</a></p>

          '
        raw: 'Did you fine-tune the model with or without BOS? Apparently this model
          works much better without BOS.

          https://github.com/01-ai/Yi/discussions/5#discussioncomment-7484547'
        updatedAt: '2023-11-08T03:01:11.547Z'
      numEdits: 1
      reactions: []
    id: 654af991a2de0e36e4587109
    type: comment
  author: lmg-anon
  content: 'Did you fine-tune the model with or without BOS? Apparently this model
    works much better without BOS.

    https://github.com/01-ai/Yi/discussions/5#discussioncomment-7484547'
  created_at: 2023-11-08 02:59:29+00:00
  edited: true
  hidden: false
  id: 654af991a2de0e36e4587109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-11-08T03:20:02.000Z'
    data:
      edited: true
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6411710977554321
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: "<p>It was trained using the Yi Llama repo llama-tokenizer branch as\
          \ a base, using raw text completion format in axolotl with this config yaml:</p>\n\
          <pre><code class=\"language-yaml\"><span class=\"hljs-attr\">base_model:</span>\
          \ <span class=\"hljs-string\">./models/yi-llama-34b</span>\n<span class=\"\
          hljs-attr\">model_type:</span> <span class=\"hljs-string\">LlamaForCausalLM</span>\n\
          <span class=\"hljs-attr\">tokenizer_type:</span> <span class=\"hljs-string\"\
          >LlamaTokenizer</span>\n<span class=\"hljs-attr\">is_llama_derived_model:</span>\
          \ <span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-attr\"\
          >load_in_8bit:</span> <span class=\"hljs-literal\">true</span>\n<span class=\"\
          hljs-attr\">load_in_4bit:</span> <span class=\"hljs-literal\">false</span>\n\
          <span class=\"hljs-attr\">strict:</span> <span class=\"hljs-literal\">false</span>\n\
          \n<span class=\"hljs-attr\">datasets:</span>\n  <span class=\"hljs-bullet\"\
          >-</span> <span class=\"hljs-attr\">path:</span> <span class=\"hljs-string\"\
          >train-all-yi-4k.jsonl</span>\n    <span class=\"hljs-attr\">type:</span>\
          \ <span class=\"hljs-string\">completion</span>\n<span class=\"hljs-attr\"\
          >dataset_prepared_path:</span>\n<span class=\"hljs-attr\">val_set_size:</span>\
          \ <span class=\"hljs-number\">0.01</span>\n<span class=\"hljs-attr\">output_dir:</span>\
          \ <span class=\"hljs-string\">./limarp-lora-out</span>\n\n<span class=\"\
          hljs-attr\">sequence_len:</span> <span class=\"hljs-number\">4096</span>\n\
          <span class=\"hljs-attr\">sample_packing:</span> <span class=\"hljs-literal\"\
          >true</span>\n<span class=\"hljs-attr\">pad_to_sequence_len:</span> <span\
          \ class=\"hljs-literal\">false</span>\n\n<span class=\"hljs-attr\">adapter:</span>\
          \ <span class=\"hljs-string\">lora</span>\n<span class=\"hljs-attr\">lora_model_dir:</span>\n\
          <span class=\"hljs-attr\">lora_r:</span> <span class=\"hljs-number\">32</span>\n\
          <span class=\"hljs-attr\">lora_alpha:</span> <span class=\"hljs-number\"\
          >16</span>\n<span class=\"hljs-attr\">lora_dropout:</span> <span class=\"\
          hljs-number\">0.05</span>\n<span class=\"hljs-attr\">lora_target_linear:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">lora_fan_in_fan_out:</span>\n\
          \n<span class=\"hljs-attr\">wandb_project:</span> <span class=\"hljs-string\"\
          >34b-qlora</span>\n<span class=\"hljs-attr\">wandb_entity:</span>\n<span\
          \ class=\"hljs-attr\">wandb_watch:</span>\n<span class=\"hljs-attr\">wandb_run_id:</span>\n\
          <span class=\"hljs-attr\">wandb_log_model:</span>\n\n<span class=\"hljs-attr\"\
          >gradient_accumulation_steps:</span> <span class=\"hljs-number\">4</span>\n\
          <span class=\"hljs-attr\">micro_batch_size:</span> <span class=\"hljs-number\"\
          >2</span>\n<span class=\"hljs-attr\">num_epochs:</span> <span class=\"hljs-number\"\
          >2</span>\n<span class=\"hljs-attr\">optimizer:</span> <span class=\"hljs-string\"\
          >adamw_bnb_8bit</span>\n<span class=\"hljs-attr\">lr_scheduler:</span> <span\
          \ class=\"hljs-string\">cosine</span>\n<span class=\"hljs-attr\">learning_rate:</span>\
          \ <span class=\"hljs-number\">0.00015</span>\n\n<span class=\"hljs-attr\"\
          >train_on_inputs:</span> <span class=\"hljs-literal\">true</span>\n<span\
          \ class=\"hljs-attr\">group_by_length:</span> <span class=\"hljs-literal\"\
          >false</span>\n<span class=\"hljs-attr\">bf16:</span> <span class=\"hljs-literal\"\
          >true</span>\n<span class=\"hljs-attr\">fp16:</span> <span class=\"hljs-literal\"\
          >false</span>\n<span class=\"hljs-attr\">tf32:</span> <span class=\"hljs-literal\"\
          >true</span>\n\n<span class=\"hljs-attr\">gradient_checkpointing:</span>\
          \ <span class=\"hljs-literal\">true</span>\n<span class=\"hljs-attr\">early_stopping_patience:</span>\n\
          <span class=\"hljs-attr\">resume_from_checkpoint:</span>\n<span class=\"\
          hljs-attr\">local_rank:</span>\n<span class=\"hljs-attr\">logging_steps:</span>\
          \ <span class=\"hljs-number\">1</span>\n<span class=\"hljs-attr\">xformers_attention:</span>\n\
          <span class=\"hljs-attr\">flash_attention:</span> <span class=\"hljs-literal\"\
          >true</span>\n\n<span class=\"hljs-attr\">warmup_steps:</span> <span class=\"\
          hljs-number\">10</span>\n<span class=\"hljs-attr\">eval_steps:</span> <span\
          \ class=\"hljs-number\">20</span>\n<span class=\"hljs-attr\">eval_table_size:</span>\n\
          <span class=\"hljs-attr\">eval_table_max_new_tokens:</span> <span class=\"\
          hljs-number\">128</span>\n<span class=\"hljs-attr\">save_steps:</span>\n\
          <span class=\"hljs-attr\">debug:</span>\n<span class=\"hljs-attr\">deepspeed:</span>\n\
          <span class=\"hljs-attr\">weight_decay:</span> <span class=\"hljs-number\"\
          >0.0</span>\n<span class=\"hljs-attr\">fsdp:</span>\n<span class=\"hljs-attr\"\
          >fsdp_config:</span>\n<span class=\"hljs-attr\">special_tokens:</span>\n\
          \  <span class=\"hljs-attr\">bos_token:</span> <span class=\"hljs-string\"\
          >\"&lt;|startoftext|&gt;\"</span>\n  <span class=\"hljs-attr\">eos_token:</span>\
          \ <span class=\"hljs-string\">\"&lt;|endoftext|&gt;\"</span>\n  <span class=\"\
          hljs-attr\">unk_token:</span> <span class=\"hljs-string\">\"&lt;unk&gt;\"\
          </span>\n</code></pre>\n<p>I'd assume that means that it's using token ID\
          \ 1 as BOS and token ID 2 as EOS as specified in the config. Haven't had\
          \ a chance to test it out - still running the lora merge to base and upload\
          \ on another system, but hoping to see results when it's merged to other\
          \ finetunes when we have them.</p>\n<p>EDIT: Hmmm looking at the tokenizer_config.json\
          \ itself, it specifies <code>\"add_bos_token\": false</code>, but I'm not\
          \ sure in what context that's referring to.</p>\n"
        raw: "It was trained using the Yi Llama repo llama-tokenizer branch as a base,\
          \ using raw text completion format in axolotl with this config yaml:\n\n\
          ```yaml\nbase_model: ./models/yi-llama-34b\nmodel_type: LlamaForCausalLM\n\
          tokenizer_type: LlamaTokenizer\nis_llama_derived_model: true\n\nload_in_8bit:\
          \ true\nload_in_4bit: false\nstrict: false\n\ndatasets:\n  - path: train-all-yi-4k.jsonl\n\
          \    type: completion\ndataset_prepared_path:\nval_set_size: 0.01\noutput_dir:\
          \ ./limarp-lora-out\n\nsequence_len: 4096\nsample_packing: true\npad_to_sequence_len:\
          \ false\n\nadapter: lora\nlora_model_dir:\nlora_r: 32\nlora_alpha: 16\n\
          lora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project:\
          \ 34b-qlora\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\
          \ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 2\noptimizer:\
          \ adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.00015\n\ntrain_on_inputs:\
          \ true\ngroup_by_length: false\nbf16: true\nfp16: false\ntf32: true\n\n\
          gradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\n\
          local_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\
          \nwarmup_steps: 10\neval_steps: 20\neval_table_size:\neval_table_max_new_tokens:\
          \ 128\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\n\
          special_tokens:\n  bos_token: \"<|startoftext|>\"\n  eos_token: \"<|endoftext|>\"\
          \n  unk_token: \"<unk>\"\n```\n\nI'd assume that means that it's using token\
          \ ID 1 as BOS and token ID 2 as EOS as specified in the config. Haven't\
          \ had a chance to test it out - still running the lora merge to base and\
          \ upload on another system, but hoping to see results when it's merged to\
          \ other finetunes when we have them.\n\nEDIT: Hmmm looking at the tokenizer_config.json\
          \ itself, it specifies `\"add_bos_token\": false`, but I'm not sure in what\
          \ context that's referring to."
        updatedAt: '2023-11-08T03:24:19.602Z'
      numEdits: 1
      reactions: []
    id: 654afe62b3f4b6f747dbb8cc
    type: comment
  author: Doctor-Shotgun
  content: "It was trained using the Yi Llama repo llama-tokenizer branch as a base,\
    \ using raw text completion format in axolotl with this config yaml:\n\n```yaml\n\
    base_model: ./models/yi-llama-34b\nmodel_type: LlamaForCausalLM\ntokenizer_type:\
    \ LlamaTokenizer\nis_llama_derived_model: true\n\nload_in_8bit: true\nload_in_4bit:\
    \ false\nstrict: false\n\ndatasets:\n  - path: train-all-yi-4k.jsonl\n    type:\
    \ completion\ndataset_prepared_path:\nval_set_size: 0.01\noutput_dir: ./limarp-lora-out\n\
    \nsequence_len: 4096\nsample_packing: true\npad_to_sequence_len: false\n\nadapter:\
    \ lora\nlora_model_dir:\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear:\
    \ true\nlora_fan_in_fan_out:\n\nwandb_project: 34b-qlora\nwandb_entity:\nwandb_watch:\n\
    wandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size:\
    \ 2\nnum_epochs: 2\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate:\
    \ 0.00015\n\ntrain_on_inputs: true\ngroup_by_length: false\nbf16: true\nfp16:\
    \ false\ntf32: true\n\ngradient_checkpointing: true\nearly_stopping_patience:\n\
    resume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\n\
    flash_attention: true\n\nwarmup_steps: 10\neval_steps: 20\neval_table_size:\n\
    eval_table_max_new_tokens: 128\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay:\
    \ 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token: \"<|startoftext|>\"\n\
    \  eos_token: \"<|endoftext|>\"\n  unk_token: \"<unk>\"\n```\n\nI'd assume that\
    \ means that it's using token ID 1 as BOS and token ID 2 as EOS as specified in\
    \ the config. Haven't had a chance to test it out - still running the lora merge\
    \ to base and upload on another system, but hoping to see results when it's merged\
    \ to other finetunes when we have them.\n\nEDIT: Hmmm looking at the tokenizer_config.json\
    \ itself, it specifies `\"add_bos_token\": false`, but I'm not sure in what context\
    \ that's referring to."
  created_at: 2023-11-08 03:20:02+00:00
  edited: true
  hidden: false
  id: 654afe62b3f4b6f747dbb8cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmg-anon
      type: user
    createdAt: '2023-11-08T03:35:30.000Z'
    data:
      edited: false
      editors:
      - lmg-anon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9835583567619324
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: lmg-anon
          type: user
        html: '<blockquote>

          <p>but I''m not sure in what context that''s referring to</p>

          </blockquote>

          <p>it means that the inference shouldn''t have bos as the first token. so
          if the dataset contains BOS this could cause problems... well, this is something
          to be aware if the model doesn''t perform as expected with this fine-tuning.
          I''m still trying to merge it myself too.</p>

          '
        raw: '>but I''m not sure in what context that''s referring to


          it means that the inference shouldn''t have bos as the first token. so if
          the dataset contains BOS this could cause problems... well, this is something
          to be aware if the model doesn''t perform as expected with this fine-tuning.
          I''m still trying to merge it myself too.'
        updatedAt: '2023-11-08T03:35:30.901Z'
      numEdits: 0
      reactions: []
    id: 654b0202c70ba116b1361301
    type: comment
  author: lmg-anon
  content: '>but I''m not sure in what context that''s referring to


    it means that the inference shouldn''t have bos as the first token. so if the
    dataset contains BOS this could cause problems... well, this is something to be
    aware if the model doesn''t perform as expected with this fine-tuning. I''m still
    trying to merge it myself too.'
  created_at: 2023-11-08 03:35:30+00:00
  edited: false
  hidden: false
  id: 654b0202c70ba116b1361301
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-11-08T03:44:37.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8719755411148071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<p>I''m not sure if there''s an arg to specify whether or not to use
          a BOS token in training in axolotl. I suppose you could set <code>bos_token:
          ""</code> in the yaml?</p>

          '
        raw: 'I''m not sure if there''s an arg to specify whether or not to use a
          BOS token in training in axolotl. I suppose you could set `bos_token: ""`
          in the yaml?'
        updatedAt: '2023-11-08T03:44:37.044Z'
      numEdits: 0
      reactions: []
    id: 654b0425ea04f2d4d93761b9
    type: comment
  author: Doctor-Shotgun
  content: 'I''m not sure if there''s an arg to specify whether or not to use a BOS
    token in training in axolotl. I suppose you could set `bos_token: ""` in the yaml?'
  created_at: 2023-11-08 03:44:37+00:00
  edited: false
  hidden: false
  id: 654b0425ea04f2d4d93761b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmg-anon
      type: user
    createdAt: '2023-11-08T04:01:46.000Z'
    data:
      edited: false
      editors:
      - lmg-anon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9368489980697632
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: lmg-anon
          type: user
        html: '<p>I have absolutely no clue either, but if the BOS isn''t in the dataset
          then maybe it is already not using it since the <code>tokenizer_config.json</code>
          specifies <code>"add_bos_token": false</code> as you mentioned before.</p>

          '
        raw: 'I have absolutely no clue either, but if the BOS isn''t in the dataset
          then maybe it is already not using it since the `tokenizer_config.json`
          specifies `"add_bos_token": false` as you mentioned before.'
        updatedAt: '2023-11-08T04:01:46.992Z'
      numEdits: 0
      reactions: []
    id: 654b082ae4ce07c7d79dc6ad
    type: comment
  author: lmg-anon
  content: 'I have absolutely no clue either, but if the BOS isn''t in the dataset
    then maybe it is already not using it since the `tokenizer_config.json` specifies
    `"add_bos_token": false` as you mentioned before.'
  created_at: 2023-11-08 04:01:46+00:00
  edited: false
  hidden: false
  id: 654b082ae4ce07c7d79dc6ad
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmg-anon
      type: user
    createdAt: '2023-11-09T15:41:24.000Z'
    data:
      edited: false
      editors:
      - lmg-anon
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.984580397605896
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
          fullname: Anon
          isHf: false
          isPro: false
          name: lmg-anon
          type: user
        html: "<p>Ok, I found that using BOS with this lora causes the model to get\
          \ the format completely wrong, and without BOS everything seems to go smoothly.\
          \ So I guess everything was right after all \U0001F44D</p>\n"
        raw: "Ok, I found that using BOS with this lora causes the model to get the\
          \ format completely wrong, and without BOS everything seems to go smoothly.\
          \ So I guess everything was right after all \U0001F44D"
        updatedAt: '2023-11-09T15:41:24.716Z'
      numEdits: 0
      reactions: []
      relatedEventId: 654cfda4fe1bf411d2c48b3f
    id: 654cfda4fe1bf411d2c48b3e
    type: comment
  author: lmg-anon
  content: "Ok, I found that using BOS with this lora causes the model to get the\
    \ format completely wrong, and without BOS everything seems to go smoothly. So\
    \ I guess everything was right after all \U0001F44D"
  created_at: 2023-11-09 15:41:24+00:00
  edited: false
  hidden: false
  id: 654cfda4fe1bf411d2c48b3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9d8683646248f2e3856c4b396945d440.svg
      fullname: Anon
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmg-anon
      type: user
    createdAt: '2023-11-09T15:41:24.000Z'
    data:
      status: closed
    id: 654cfda4fe1bf411d2c48b3f
    type: status-change
  author: lmg-anon
  created_at: 2023-11-09 15:41:24+00:00
  id: 654cfda4fe1bf411d2c48b3f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Doctor-Shotgun/limarpv3-yi-llama-34b-lora
repo_type: model
status: closed
target_branch: null
title: Question
