!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brucethemoose
conflicting_files: null
created_at: 2023-11-10 00:19:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-10T00:19:23.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.977343738079071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>This lora seems to work on the 200K version of Yi, but if you ever
          revisit it, would you consider using that as a base model instead?</p>

          <p>The long context is hugely useful (and seems to work well).</p>

          '
        raw: "This lora seems to work on the 200K version of Yi, but if you ever revisit\
          \ it, would you consider using that as a base model instead?\r\n\r\nThe\
          \ long context is hugely useful (and seems to work well)."
        updatedAt: '2023-11-10T00:19:23.483Z'
      numEdits: 0
      reactions: []
    id: 654d770b921f4f1ff4b70000
    type: comment
  author: brucethemoose
  content: "This lora seems to work on the 200K version of Yi, but if you ever revisit\
    \ it, would you consider using that as a base model instead?\r\n\r\nThe long context\
    \ is hugely useful (and seems to work well)."
  created_at: 2023-11-10 00:19:23+00:00
  edited: false
  hidden: false
  id: 654d770b921f4f1ff4b70000
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-11-11T00:44:49.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9823194742202759
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<p>It would be possible, but I chose to train on the base version to
          maintain (hopefully) better compatibility for merging to other models, especially
          since the base 34b is already usable up to 32k ctx at inference. I would
          however like to try to train the dataset at full length rather than 4k,
          however that was a bit compute-prohibitive.</p>

          '
        raw: It would be possible, but I chose to train on the base version to maintain
          (hopefully) better compatibility for merging to other models, especially
          since the base 34b is already usable up to 32k ctx at inference. I would
          however like to try to train the dataset at full length rather than 4k,
          however that was a bit compute-prohibitive.
        updatedAt: '2023-11-11T00:44:49.799Z'
      numEdits: 0
      reactions: []
    id: 654ece812382f591eebdf2ac
    type: comment
  author: Doctor-Shotgun
  content: It would be possible, but I chose to train on the base version to maintain
    (hopefully) better compatibility for merging to other models, especially since
    the base 34b is already usable up to 32k ctx at inference. I would however like
    to try to train the dataset at full length rather than 4k, however that was a
    bit compute-prohibitive.
  created_at: 2023-11-11 00:44:49+00:00
  edited: false
  hidden: false
  id: 654ece812382f591eebdf2ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-11T01:21:26.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9267920851707458
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>maintain (hopefully) better compatibility for merging to other models</p>

          </blockquote>

          <p>You mean most other trainers will use the 34K model as well?</p>

          <p>IDK, long context with no RoPE stretching is super appealing to me. I
          figured everyone would default to the 200K model.</p>

          '
        raw: '> maintain (hopefully) better compatibility for merging to other models


          You mean most other trainers will use the 34K model as well?


          IDK, long context with no RoPE stretching is super appealing to me. I figured
          everyone would default to the 200K model.'
        updatedAt: '2023-11-11T01:21:26.838Z'
      numEdits: 0
      reactions: []
    id: 654ed716286b72eb7c1d3fa7
    type: comment
  author: brucethemoose
  content: '> maintain (hopefully) better compatibility for merging to other models


    You mean most other trainers will use the 34K model as well?


    IDK, long context with no RoPE stretching is super appealing to me. I figured
    everyone would default to the 200K model.'
  created_at: 2023-11-11 01:21:26+00:00
  edited: false
  hidden: false
  id: 654ed716286b72eb7c1d3fa7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-11T01:23:01.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7525793313980103
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>Also, I believe at least one other trainer is doing the 200K model:
          <a rel="nofollow" href="https://old.reddit.com/r/LocalLLaMA/comments/17rzed4/yi34b_vs_yi34b200k_on_sequences_32k_and_4k/">https://old.reddit.com/r/LocalLLaMA/comments/17rzed4/yi34b_vs_yi34b200k_on_sequences_32k_and_4k/</a></p>

          '
        raw: 'Also, I believe at least one other trainer is doing the 200K model:
          https://old.reddit.com/r/LocalLLaMA/comments/17rzed4/yi34b_vs_yi34b200k_on_sequences_32k_and_4k/'
        updatedAt: '2023-11-11T01:23:01.252Z'
      numEdits: 0
      reactions: []
    id: 654ed7750c8ecb1dae72dd91
    type: comment
  author: brucethemoose
  content: 'Also, I believe at least one other trainer is doing the 200K model: https://old.reddit.com/r/LocalLLaMA/comments/17rzed4/yi34b_vs_yi34b200k_on_sequences_32k_and_4k/'
  created_at: 2023-11-11 01:23:01+00:00
  edited: false
  hidden: false
  id: 654ed7750c8ecb1dae72dd91
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Doctor-Shotgun/limarpv3-yi-llama-34b-lora
repo_type: model
status: open
target_branch: null
title: 200K Version
