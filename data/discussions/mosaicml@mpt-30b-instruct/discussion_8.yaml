!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ukumar
conflicting_files: null
created_at: 2023-07-26 11:08:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2ac6609f20a3dd8c54a8d54dd69edb7f.svg
      fullname: Ujjwal Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ukumar
      type: user
    createdAt: '2023-07-26T12:08:10.000Z'
    data:
      edited: false
      editors:
      - ukumar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6743042469024658
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2ac6609f20a3dd8c54a8d54dd69edb7f.svg
          fullname: Ujjwal Kumar
          isHf: false
          isPro: false
          name: ukumar
          type: user
        html: '<p> I''m facing a performance problem while using the mpt 30b instruct
          model for inference, locally. Comparatively, the mpt 30b chat model from
          Hugging Face''s demo endpoints performs significantly better. Although there''s
          no demo available for the instruct model, the performance difference seems
          unexpected.<br>This is my code:<br>......<br>model = transformers.AutoModelForCausalLM.from_pretrained(<br>      name=
          mosaicml/mpt-30b-instruct,<br>      device_map= "auto",<br>      cache_dir=local_path,<br>      torch_dtype=torch.float32,<br>      local_files_only=True,<br>      trust_remote_code=True,<br>    )</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(''EleutherAI/gpt-neox-20b'',<br>                                             cache_dir=local_path,<br>                                             local_files_only=True)</p>

          <p>pipe = pipeline(task,<br>                model=model,<br>                tokenizer=tokenizer,<br>                #device=''cuda:0'',<br>                device_map=
          "auto",<br>                max_length= 512,<br>                do_sample=True,<br>                top_p=
          0.9,<br>                num_return_sequences=1,<br>                eos_token_id=tokenizer.eos_token_id,<br>                 temperature=0.8<br>                )<br>Then
          I use langchain for text generation.<br>Also, I couldn''t find the top_p
          and temperature values from the config files.</p>

          '
        raw: " I'm facing a performance problem while using the mpt 30b instruct model\
          \ for inference, locally. Comparatively, the mpt 30b chat model from Hugging\
          \ Face's demo endpoints performs significantly better. Although there's\
          \ no demo available for the instruct model, the performance difference seems\
          \ unexpected. \r\nThis is my code:\r\n......\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n      name= mosaicml/mpt-30b-instruct,\r\n      device_map= \"auto\",\r\
          \n      cache_dir=local_path,\r\n      torch_dtype=torch.float32,   \r\n\
          \      local_files_only=True,\r\n      trust_remote_code=True,\r\n    )\r\
          \n\r\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b',\r\
          \n                                             cache_dir=local_path,\r\n\
          \                                             local_files_only=True)\r\n\
          \r\npipe = pipeline(task, \r\n                model=model, \r\n        \
          \        tokenizer=tokenizer, \r\n                #device='cuda:0',\r\n\
          \                device_map= \"auto\",\r\n                max_length= 512,\
          \ \r\n                do_sample=True,\r\n                top_p= 0.9,\r\n\
          \                num_return_sequences=1,\r\n                eos_token_id=tokenizer.eos_token_id,\r\
          \n                 temperature=0.8       \r\n                )\r\nThen I\
          \ use langchain for text generation.\r\nAlso, I couldn't find the top_p\
          \ and temperature values from the config files.\r\n"
        updatedAt: '2023-07-26T12:08:10.580Z'
      numEdits: 0
      reactions: []
    id: 64c10caae56520a63d472071
    type: comment
  author: ukumar
  content: " I'm facing a performance problem while using the mpt 30b instruct model\
    \ for inference, locally. Comparatively, the mpt 30b chat model from Hugging Face's\
    \ demo endpoints performs significantly better. Although there's no demo available\
    \ for the instruct model, the performance difference seems unexpected. \r\nThis\
    \ is my code:\r\n......\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n      name= mosaicml/mpt-30b-instruct,\r\n      device_map= \"auto\",\r\n  \
    \    cache_dir=local_path,\r\n      torch_dtype=torch.float32,   \r\n      local_files_only=True,\r\
    \n      trust_remote_code=True,\r\n    )\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('EleutherAI/gpt-neox-20b',\r\
    \n                                             cache_dir=local_path,\r\n     \
    \                                        local_files_only=True)\r\n\r\npipe =\
    \ pipeline(task, \r\n                model=model, \r\n                tokenizer=tokenizer,\
    \ \r\n                #device='cuda:0',\r\n                device_map= \"auto\"\
    ,\r\n                max_length= 512, \r\n                do_sample=True,\r\n\
    \                top_p= 0.9,\r\n                num_return_sequences=1,\r\n  \
    \              eos_token_id=tokenizer.eos_token_id,\r\n                 temperature=0.8\
    \       \r\n                )\r\nThen I use langchain for text generation.\r\n\
    Also, I couldn't find the top_p and temperature values from the config files.\r\
    \n"
  created_at: 2023-07-26 11:08:10+00:00
  edited: false
  hidden: false
  id: 64c10caae56520a63d472071
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 8
repo_id: mosaicml/mpt-30b-instruct
repo_type: model
status: open
target_branch: null
title: 'Huge Performance Disparity: MPT 30b Instruct Model (on Local Inference) vs.
  MPT 30b Chat Model (using HF Demo)'
