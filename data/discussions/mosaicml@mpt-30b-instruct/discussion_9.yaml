!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JHolmes89
conflicting_files: null
created_at: 2023-08-04 20:21:42+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9bfb0ed253d7e5afa133ce71b331607d.svg
      fullname: Jonathan Holmes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JHolmes89
      type: user
    createdAt: '2023-08-04T21:21:42.000Z'
    data:
      edited: true
      editors:
      - JHolmes89
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7598671317100525
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9bfb0ed253d7e5afa133ce71b331607d.svg
          fullname: Jonathan Holmes
          isHf: false
          isPro: false
          name: JHolmes89
          type: user
        html: "<p>I'm running MPT-30B-Instruct on 4 A10s as follows: </p>\n<pre><code>import\
          \ transformers\nimport pickle\nname = 'mosaicml/mpt-30b-instruct'\ntokenizer\
          \ = transformers.AutoTokenizer.from_pretrained(name)\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n\
          \    name, trust_remote_code=True, load_in_8bit=True,  device_map='auto'\n\
          \                                                         )\npipe = transformers.pipeline('text-generation',\
          \ model=model, \n                             tokenizer=tokenizer, device_map='auto')\n\
          with open('my_2000_token_text.txt', 'rb') as f:\n     text = f.read()\n\
          output = pipe(text, max_new_tokens=600)[0]['generated_text']\n</code></pre>\n\
          <p>What happens here is that instead of the expected behavior of output\
          \ being an extension of input, it will be only part of input, with no additional\
          \ text at the end (i.e., <code>text.startswith(output)</code> will be <code>True</code>\
          \ and output will be shorter than text, while we would expect the reverse).</p>\n\
          <p>MPT-30B is supposed to have an 8k token context window, and <code>max_seq_len</code>\
          \ is 8192 by default, so I don't see any reason this should be happening.\
          \ How can I fix this problem?</p>\n"
        raw: "I'm running MPT-30B-Instruct on 4 A10s as follows: \n\n```\nimport transformers\n\
          import pickle\nname = 'mosaicml/mpt-30b-instruct'\ntokenizer = transformers.AutoTokenizer.from_pretrained(name)\n\
          model = transformers.AutoModelForCausalLM.from_pretrained(\n    name, trust_remote_code=True,\
          \ load_in_8bit=True,  device_map='auto'\n                              \
          \                           )\npipe = transformers.pipeline('text-generation',\
          \ model=model, \n                             tokenizer=tokenizer, device_map='auto')\n\
          with open('my_2000_token_text.txt', 'rb') as f:\n     text = f.read()\n\
          output = pipe(text, max_new_tokens=600)[0]['generated_text']\n```\n\nWhat\
          \ happens here is that instead of the expected behavior of output being\
          \ an extension of input, it will be only part of input, with no additional\
          \ text at the end (i.e., `text.startswith(output)` will be `True` and output\
          \ will be shorter than text, while we would expect the reverse).\n\nMPT-30B\
          \ is supposed to have an 8k token context window, and `max_seq_len` is 8192\
          \ by default, so I don't see any reason this should be happening. How can\
          \ I fix this problem?"
        updatedAt: '2023-08-04T21:38:03.361Z'
      numEdits: 1
      reactions: []
    id: 64cd6be61b981b2e0c8e8e45
    type: comment
  author: JHolmes89
  content: "I'm running MPT-30B-Instruct on 4 A10s as follows: \n\n```\nimport transformers\n\
    import pickle\nname = 'mosaicml/mpt-30b-instruct'\ntokenizer = transformers.AutoTokenizer.from_pretrained(name)\n\
    model = transformers.AutoModelForCausalLM.from_pretrained(\n    name, trust_remote_code=True,\
    \ load_in_8bit=True,  device_map='auto'\n                                    \
    \                     )\npipe = transformers.pipeline('text-generation', model=model,\
    \ \n                             tokenizer=tokenizer, device_map='auto')\nwith\
    \ open('my_2000_token_text.txt', 'rb') as f:\n     text = f.read()\noutput = pipe(text,\
    \ max_new_tokens=600)[0]['generated_text']\n```\n\nWhat happens here is that instead\
    \ of the expected behavior of output being an extension of input, it will be only\
    \ part of input, with no additional text at the end (i.e., `text.startswith(output)`\
    \ will be `True` and output will be shorter than text, while we would expect the\
    \ reverse).\n\nMPT-30B is supposed to have an 8k token context window, and `max_seq_len`\
    \ is 8192 by default, so I don't see any reason this should be happening. How\
    \ can I fix this problem?"
  created_at: 2023-08-04 20:21:42+00:00
  edited: true
  hidden: false
  id: 64cd6be61b981b2e0c8e8e45
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: mosaicml/mpt-30b-instruct
repo_type: model
status: open
target_branch: null
title: Truncated model input, no output.
