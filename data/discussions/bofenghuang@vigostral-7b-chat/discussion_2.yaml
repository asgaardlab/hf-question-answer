!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YorelNation
conflicting_files: null
created_at: 2023-10-18 07:19:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660638608323-noauth.png?w=200&h=200&f=face
      fullname: Antonin Leroy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YorelNation
      type: user
    createdAt: '2023-10-18T08:19:36.000Z'
    data:
      edited: false
      editors:
      - YorelNation
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5847191214561462
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660638608323-noauth.png?w=200&h=200&f=face
          fullname: Antonin Leroy
          isHf: false
          isPro: false
          name: YorelNation
          type: user
        html: '<p>Hi,</p>

          <p>Would it be possible to somehow deploy vigostral the same way we can
          deploy mistral via their recommended method: <a rel="nofollow" href="https://docs.mistral.ai/quickstart">https://docs.mistral.ai/quickstart</a><br>Can
          I simply run:</p>

          <p>docker run --gpus all <br>    -e HF_TOKEN=$HF_TOKEN -p 8000:8000 <br>     ghcr.io/mistralai/mistral-src/vllm:latest
          <br>    --host 0.0.0.0 <br>    --model bofenghuang/vigostral-7b-chat</p>

          <p>I don''t have the hardware to try this yet that''s why I''m asking :)</p>

          <p>Thanks</p>

          '
        raw: "Hi,\r\n\r\nWould it be possible to somehow deploy vigostral the same\
          \ way we can deploy mistral via their recommended method: https://docs.mistral.ai/quickstart\r\
          \nCan I simply run:\r\n\r\ndocker run --gpus all \\\r\n    -e HF_TOKEN=$HF_TOKEN\
          \ -p 8000:8000 \\\r\n     ghcr.io/mistralai/mistral-src/vllm:latest \\\r\
          \n    --host 0.0.0.0 \\\r\n    --model bofenghuang/vigostral-7b-chat\r\n\
          \r\nI don't have the hardware to try this yet that's why I'm asking :)\r\
          \n\r\nThanks"
        updatedAt: '2023-10-18T08:19:36.686Z'
      numEdits: 0
      reactions: []
    id: 652f951817096ceb6bf1949d
    type: comment
  author: YorelNation
  content: "Hi,\r\n\r\nWould it be possible to somehow deploy vigostral the same way\
    \ we can deploy mistral via their recommended method: https://docs.mistral.ai/quickstart\r\
    \nCan I simply run:\r\n\r\ndocker run --gpus all \\\r\n    -e HF_TOKEN=$HF_TOKEN\
    \ -p 8000:8000 \\\r\n     ghcr.io/mistralai/mistral-src/vllm:latest \\\r\n   \
    \ --host 0.0.0.0 \\\r\n    --model bofenghuang/vigostral-7b-chat\r\n\r\nI don't\
    \ have the hardware to try this yet that's why I'm asking :)\r\n\r\nThanks"
  created_at: 2023-10-18 07:19:36+00:00
  edited: false
  hidden: false
  id: 652f951817096ceb6bf1949d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea3c8a9f178dfc1df3f75d71ecbe39dd.svg
      fullname: bofeng huang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bofenghuang
      type: user
    createdAt: '2023-10-18T10:10:59.000Z'
    data:
      edited: false
      editors:
      - bofenghuang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463478326797485
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea3c8a9f178dfc1df3f75d71ecbe39dd.svg
          fullname: bofeng huang
          isHf: false
          isPro: false
          name: bofenghuang
          type: user
        html: '<p>Hi, </p>

          <p>Thanks for your message. I will look into it :)</p>

          '
        raw: "Hi, \n\nThanks for your message. I will look into it :)"
        updatedAt: '2023-10-18T10:10:59.998Z'
      numEdits: 0
      reactions: []
    id: 652faf33dffbfb2911b736be
    type: comment
  author: bofenghuang
  content: "Hi, \n\nThanks for your message. I will look into it :)"
  created_at: 2023-10-18 09:10:59+00:00
  edited: false
  hidden: false
  id: 652faf33dffbfb2911b736be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ea3c8a9f178dfc1df3f75d71ecbe39dd.svg
      fullname: bofeng huang
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bofenghuang
      type: user
    createdAt: '2023-10-25T12:48:33.000Z'
    data:
      edited: false
      editors:
      - bofenghuang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3762473464012146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ea3c8a9f178dfc1df3f75d71ecbe39dd.svg
          fullname: bofeng huang
          isHf: false
          isPro: false
          name: bofenghuang
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;YorelNation&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/YorelNation\"\
          >@<span class=\"underline\">YorelNation</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>The Mistral AI version has not yet been updated to support the\
          \ prompt format of the Vigostral model.</p>\n<p>However, I have managed\
          \ to create another Docker image that also leverages vLLM for inference.\
          \ You can use it as follows:</p>\n<pre><code class=\"language-bash\"><span\
          \ class=\"hljs-comment\"># Launch inference engine</span>\ndocker run --gpus\
          \ <span class=\"hljs-string\">'\"device=0\"'</span> \\\n    -e HF_TOKEN=<span\
          \ class=\"hljs-variable\">$HF_TOKEN</span> -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest\
          \ \\\n    --host 0.0.0.0 \\\n    --model bofenghuang/vigostral-7b-chat\n\
          \n<span class=\"hljs-comment\"># Launch inference engine on mutli-GPUs (4\
          \ here)</span>\ndocker run --gpus all \\\n    -e HF_TOKEN=<span class=\"\
          hljs-variable\">$HF_TOKEN</span> -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest\
          \ \\\n    --host 0.0.0.0 \\\n    --tensor-parallel-size 4 \\\n    --model\
          \ bofenghuang/vigostral-7b-chat\n\n<span class=\"hljs-comment\"># Launch\
          \ inference engine using the quantized AWQ version</span>\n<span class=\"\
          hljs-comment\"># Note only supports Ampere or newer GPUs</span>\ndocker\
          \ run --gpus <span class=\"hljs-string\">'\"device=0\"'</span> \\\n    -e\
          \ HF_TOKEN=<span class=\"hljs-variable\">$HF_TOKEN</span> -p 8000:8000 \\\
          \n    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host 0.0.0.0 \\\n\
          \    --quantization awq \\\n    --model TheBloke/Vigostral-7B-Chat-AWQ\n\
          \n<span class=\"hljs-comment\"># Launch inference engine using the downloaded\
          \ weights</span>\ndocker run --gpus <span class=\"hljs-string\">'\"device=0\"\
          '</span> \\\n    -p 8000:8000 \\\n    -v /path/to/model/:/mnt/model/ \\\n\
          \    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host 0.0.0.0 \\\n\
          \    --model=<span class=\"hljs-string\">\"/mnt/model/\"</span>\n</code></pre>\n"
        raw: "Hi @YorelNation ,\n\nThe Mistral AI version has not yet been updated\
          \ to support the prompt format of the Vigostral model.\n\nHowever, I have\
          \ managed to create another Docker image that also leverages vLLM for inference.\
          \ You can use it as follows:\n\n```bash\n# Launch inference engine\ndocker\
          \ run --gpus '\"device=0\"' \\\n    -e HF_TOKEN=$HF_TOKEN -p 8000:8000 \\\
          \n    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host 0.0.0.0 \\\n\
          \    --model bofenghuang/vigostral-7b-chat\n\n# Launch inference engine\
          \ on mutli-GPUs (4 here)\ndocker run --gpus all \\\n    -e HF_TOKEN=$HF_TOKEN\
          \ -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host\
          \ 0.0.0.0 \\\n    --tensor-parallel-size 4 \\\n    --model bofenghuang/vigostral-7b-chat\n\
          \n# Launch inference engine using the quantized AWQ version\n# Note only\
          \ supports Ampere or newer GPUs\ndocker run --gpus '\"device=0\"' \\\n \
          \   -e HF_TOKEN=$HF_TOKEN -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest\
          \ \\\n    --host 0.0.0.0 \\\n    --quantization awq \\\n    --model TheBloke/Vigostral-7B-Chat-AWQ\n\
          \n# Launch inference engine using the downloaded weights\ndocker run --gpus\
          \ '\"device=0\"' \\\n    -p 8000:8000 \\\n    -v /path/to/model/:/mnt/model/\
          \ \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host 0.0.0.0\
          \ \\\n    --model=\"/mnt/model/\"\n```"
        updatedAt: '2023-10-25T12:48:33.155Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - YorelNation
    id: 65390ea1e593ea431955b264
    type: comment
  author: bofenghuang
  content: "Hi @YorelNation ,\n\nThe Mistral AI version has not yet been updated to\
    \ support the prompt format of the Vigostral model.\n\nHowever, I have managed\
    \ to create another Docker image that also leverages vLLM for inference. You can\
    \ use it as follows:\n\n```bash\n# Launch inference engine\ndocker run --gpus\
    \ '\"device=0\"' \\\n    -e HF_TOKEN=$HF_TOKEN -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest\
    \ \\\n    --host 0.0.0.0 \\\n    --model bofenghuang/vigostral-7b-chat\n\n# Launch\
    \ inference engine on mutli-GPUs (4 here)\ndocker run --gpus all \\\n    -e HF_TOKEN=$HF_TOKEN\
    \ -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host\
    \ 0.0.0.0 \\\n    --tensor-parallel-size 4 \\\n    --model bofenghuang/vigostral-7b-chat\n\
    \n# Launch inference engine using the quantized AWQ version\n# Note only supports\
    \ Ampere or newer GPUs\ndocker run --gpus '\"device=0\"' \\\n    -e HF_TOKEN=$HF_TOKEN\
    \ -p 8000:8000 \\\n    ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host\
    \ 0.0.0.0 \\\n    --quantization awq \\\n    --model TheBloke/Vigostral-7B-Chat-AWQ\n\
    \n# Launch inference engine using the downloaded weights\ndocker run --gpus '\"\
    device=0\"' \\\n    -p 8000:8000 \\\n    -v /path/to/model/:/mnt/model/ \\\n \
    \   ghcr.io/bofenghuang/vigogne/vllm:latest \\\n    --host 0.0.0.0 \\\n    --model=\"\
    /mnt/model/\"\n```"
  created_at: 2023-10-25 11:48:33+00:00
  edited: false
  hidden: false
  id: 65390ea1e593ea431955b264
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660638608323-noauth.png?w=200&h=200&f=face
      fullname: Antonin Leroy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YorelNation
      type: user
    createdAt: '2023-10-25T13:50:35.000Z'
    data:
      edited: false
      editors:
      - YorelNation
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.873624324798584
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660638608323-noauth.png?w=200&h=200&f=face
          fullname: Antonin Leroy
          isHf: false
          isPro: false
          name: YorelNation
          type: user
        html: '<p>Thanks ! Will try this</p>

          '
        raw: Thanks ! Will try this
        updatedAt: '2023-10-25T13:50:35.906Z'
      numEdits: 0
      reactions: []
    id: 65391d2b56c9b35961e801b2
    type: comment
  author: YorelNation
  content: Thanks ! Will try this
  created_at: 2023-10-25 12:50:35+00:00
  edited: false
  hidden: false
  id: 65391d2b56c9b35961e801b2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bofenghuang/vigostral-7b-chat
repo_type: model
status: open
target_branch: null
title: Running an inference server using Docker + vLLM
