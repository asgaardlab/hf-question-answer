!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pratikkotian04
conflicting_files: null
created_at: 2022-05-31 04:16:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614938784535-noauth.jpeg?w=200&h=200&f=face
      fullname: Pratik Kotian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pratikkotian04
      type: user
    createdAt: '2022-05-31T05:16:03.000Z'
    data:
      edited: false
      editors:
      - pratikkotian04
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1614938784535-noauth.jpeg?w=200&h=200&f=face
          fullname: Pratik Kotian
          isHf: false
          isPro: false
          name: pratikkotian04
          type: user
        html: '<p>I want to generate 10 sentences using this paraphrase. How can I
          do so?</p>

          '
        raw: I want to generate 10 sentences using this paraphrase. How can I do so?
        updatedAt: '2022-05-31T05:16:03.000Z'
      numEdits: 0
      reactions: []
    id: 6295a4931e87ffbe5c099932
    type: comment
  author: pratikkotian04
  content: I want to generate 10 sentences using this paraphrase. How can I do so?
  created_at: 2022-05-31 04:16:03+00:00
  edited: false
  hidden: false
  id: 6295a4931e87ffbe5c099932
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1627293523680-607fdcaa7c746d01ecb1917e.png?w=200&h=200&f=face
      fullname: Eugene Siow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eugenesiow
      type: user
    createdAt: '2022-05-31T06:19:20.000Z'
    data:
      edited: false
      editors:
      - eugenesiow
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1627293523680-607fdcaa7c746d01ecb1917e.png?w=200&h=200&f=face
          fullname: Eugene Siow
          isHf: false
          isPro: false
          name: eugenesiow
          type: user
        html: "<p>You can specify the <code>num_return_sequences=10</code> parameter\
          \ in <code>generate()</code> and use beam search to decode.</p>\n<p>Load\
          \ the model as per normal:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> BartForConditionalGeneration,\
          \ BartTokenizer\n\nmodel = BartForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">'eugenesiow/bart-paraphrase'</span>)\ndevice = torch.device(<span\
          \ class=\"hljs-string\">\"cuda\"</span> <span class=\"hljs-keyword\">if</span>\
          \ torch.cuda.is_available() <span class=\"hljs-keyword\">else</span> <span\
          \ class=\"hljs-string\">\"cpu\"</span>)\nmodel = model.to(device)\ntokenizer\
          \ = BartTokenizer.from_pretrained(<span class=\"hljs-string\">'eugenesiow/bart-paraphrase'</span>)\n\
          </code></pre>\n<p>Use beam search, specifying the number of beams and number\
          \ of return sequences:</p>\n<pre><code class=\"language-python\">input_sentence\
          \ = <span class=\"hljs-string\">\"Never mind less slow, how about making\
          \ it work first?\"</span>\nbatch = tokenizer(input_sentence, return_tensors=<span\
          \ class=\"hljs-string\">'pt'</span>).to(device)\n\ntorch.manual_seed(<span\
          \ class=\"hljs-number\">0</span>)\ngenerated_ids_set = model.generate(batch[<span\
          \ class=\"hljs-string\">'input_ids'</span>], num_beams=<span class=\"hljs-number\"\
          >30</span>, num_return_sequences=<span class=\"hljs-number\">10</span>,\
          \ early_stopping=<span class=\"hljs-literal\">True</span>)\n<span class=\"\
          hljs-keyword\">for</span> generated_ids <span class=\"hljs-keyword\">in</span>\
          \ generated_ids_set: \n  generated_sentence = tokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n  <span\
          \ class=\"hljs-built_in\">print</span>(generated_sentence)\n</code></pre>\n\
          <p>The output looks something like that:</p>\n<pre><code>['&lt;/s&gt;',\
          \ 'How', ' can', ' I', ' make', ' it', ' work', ' first', '?', '&lt;/s&gt;',\
          \ '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;/s&gt;', 'How', ' do', ' I', ' make',\
          \ ' it', ' work', ' first', '?', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n\
          ['&lt;/s&gt;', 'How', ' should', ' I', ' make', ' it', ' work', ' first',\
          \ '?', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;/s&gt;', 'What',\
          \ ' should', ' I', ' do', ' to', ' make', ' it', ' work', ' first', '?',\
          \ '&lt;/s&gt;']\n['&lt;/s&gt;', 'How', ' can', ' I', ' make', ' it', ' work',\
          \ ' faster', '?', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;/s&gt;',\
          \ 'How', ' do', ' I', ' make', ' things', ' work', ' first', '?', '&lt;/s&gt;',\
          \ '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;/s&gt;', 'How', ' do', ' I', ' make',\
          \ ' it', ' work', '?', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n\
          ['&lt;/s&gt;', 'How', ' do', ' I', ' make', ' it', ' work', ' faster', '?',\
          \ '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;']\n['&lt;/s&gt;', 'How', ' can',\
          \ ' I', ' be', ' less', ' slow', '?', '&lt;/s&gt;', '&lt;pad&gt;', '&lt;pad&gt;',\
          \ '&lt;pad&gt;']\n['&lt;/s&gt;', 'What', ' is', ' the', ' best', ' way',\
          \ ' to', ' make', ' something', ' work', '?', '&lt;/s&gt;']\n</code></pre>\n"
        raw: "You can specify the `num_return_sequences=10` parameter in `generate()`\
          \ and use beam search to decode.\n\nLoad the model as per normal:\n```python\n\
          import torch\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\
          \nmodel = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n\
          device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"\
          )\nmodel = model.to(device)\ntokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n\
          ```\n\nUse beam search, specifying the number of beams and number of return\
          \ sequences:\n```python\ninput_sentence = \"Never mind less slow, how about\
          \ making it work first?\"\nbatch = tokenizer(input_sentence, return_tensors='pt').to(device)\n\
          \ntorch.manual_seed(0)\ngenerated_ids_set = model.generate(batch['input_ids'],\
          \ num_beams=30, num_return_sequences=10, early_stopping=True)\nfor generated_ids\
          \ in generated_ids_set: \n  generated_sentence = tokenizer.batch_decode(generated_ids,\
          \ skip_special_tokens=True)\n  print(generated_sentence)\n```\n\nThe output\
          \ looks something like that:\n```\n['</s>', 'How', ' can', ' I', ' make',\
          \ ' it', ' work', ' first', '?', '</s>', '<pad>', '<pad>']\n['</s>', 'How',\
          \ ' do', ' I', ' make', ' it', ' work', ' first', '?', '</s>', '<pad>',\
          \ '<pad>']\n['</s>', 'How', ' should', ' I', ' make', ' it', ' work', '\
          \ first', '?', '</s>', '<pad>', '<pad>']\n['</s>', 'What', ' should', '\
          \ I', ' do', ' to', ' make', ' it', ' work', ' first', '?', '</s>']\n['</s>',\
          \ 'How', ' can', ' I', ' make', ' it', ' work', ' faster', '?', '</s>',\
          \ '<pad>', '<pad>']\n['</s>', 'How', ' do', ' I', ' make', ' things', '\
          \ work', ' first', '?', '</s>', '<pad>', '<pad>']\n['</s>', 'How', ' do',\
          \ ' I', ' make', ' it', ' work', '?', '</s>', '<pad>', '<pad>', '<pad>']\n\
          ['</s>', 'How', ' do', ' I', ' make', ' it', ' work', ' faster', '?', '</s>',\
          \ '<pad>', '<pad>']\n['</s>', 'How', ' can', ' I', ' be', ' less', ' slow',\
          \ '?', '</s>', '<pad>', '<pad>', '<pad>']\n['</s>', 'What', ' is', ' the',\
          \ ' best', ' way', ' to', ' make', ' something', ' work', '?', '</s>']\n\
          ```"
        updatedAt: '2022-05-31T06:19:20.000Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - darian
    id: 6295b368d1e3d926188cf49e
    type: comment
  author: eugenesiow
  content: "You can specify the `num_return_sequences=10` parameter in `generate()`\
    \ and use beam search to decode.\n\nLoad the model as per normal:\n```python\n\
    import torch\nfrom transformers import BartForConditionalGeneration, BartTokenizer\n\
    \nmodel = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n\
    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel\
    \ = model.to(device)\ntokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n\
    ```\n\nUse beam search, specifying the number of beams and number of return sequences:\n\
    ```python\ninput_sentence = \"Never mind less slow, how about making it work first?\"\
    \nbatch = tokenizer(input_sentence, return_tensors='pt').to(device)\n\ntorch.manual_seed(0)\n\
    generated_ids_set = model.generate(batch['input_ids'], num_beams=30, num_return_sequences=10,\
    \ early_stopping=True)\nfor generated_ids in generated_ids_set: \n  generated_sentence\
    \ = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n  print(generated_sentence)\n\
    ```\n\nThe output looks something like that:\n```\n['</s>', 'How', ' can', ' I',\
    \ ' make', ' it', ' work', ' first', '?', '</s>', '<pad>', '<pad>']\n['</s>',\
    \ 'How', ' do', ' I', ' make', ' it', ' work', ' first', '?', '</s>', '<pad>',\
    \ '<pad>']\n['</s>', 'How', ' should', ' I', ' make', ' it', ' work', ' first',\
    \ '?', '</s>', '<pad>', '<pad>']\n['</s>', 'What', ' should', ' I', ' do', ' to',\
    \ ' make', ' it', ' work', ' first', '?', '</s>']\n['</s>', 'How', ' can', ' I',\
    \ ' make', ' it', ' work', ' faster', '?', '</s>', '<pad>', '<pad>']\n['</s>',\
    \ 'How', ' do', ' I', ' make', ' things', ' work', ' first', '?', '</s>', '<pad>',\
    \ '<pad>']\n['</s>', 'How', ' do', ' I', ' make', ' it', ' work', '?', '</s>',\
    \ '<pad>', '<pad>', '<pad>']\n['</s>', 'How', ' do', ' I', ' make', ' it', ' work',\
    \ ' faster', '?', '</s>', '<pad>', '<pad>']\n['</s>', 'How', ' can', ' I', ' be',\
    \ ' less', ' slow', '?', '</s>', '<pad>', '<pad>', '<pad>']\n['</s>', 'What',\
    \ ' is', ' the', ' best', ' way', ' to', ' make', ' something', ' work', '?',\
    \ '</s>']\n```"
  created_at: 2022-05-31 05:19:20+00:00
  edited: false
  hidden: false
  id: 6295b368d1e3d926188cf49e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1627293523680-607fdcaa7c746d01ecb1917e.png?w=200&h=200&f=face
      fullname: Eugene Siow
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: eugenesiow
      type: user
    createdAt: '2023-03-28T06:46:47.000Z'
    data:
      status: closed
    id: 64228d57fd58c53d7cd06061
    type: status-change
  author: eugenesiow
  created_at: 2023-03-28 05:46:47+00:00
  id: 64228d57fd58c53d7cd06061
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: eugenesiow/bart-paraphrase
repo_type: model
status: closed
target_branch: null
title: How to add the number of sequences in model.generate for Bart Paraphrase
