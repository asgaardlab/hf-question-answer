!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Haiart
conflicting_files: null
created_at: 2023-11-28 11:40:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/818d9cc7adf75b927d1f39051038f91a.svg
      fullname: Patrick Macedo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Haiart
      type: user
    createdAt: '2023-11-28T11:40:46.000Z'
    data:
      edited: false
      editors:
      - Haiart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8970159292221069
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/818d9cc7adf75b927d1f39051038f91a.svg
          fullname: Patrick Macedo
          isHf: false
          isPro: false
          name: Haiart
          type: user
        html: '<p>Interesting idea, have you considered other models like: Starling-LM-7B-alpha,
          Chupacabra-v3, Zephyr-7b-beta and OpenChat-3.5?<br>Maybe a Frankenmerge
          between these Six models (including these two over here OpenHermes-2.5 and
          Neural-Chat-3.1) resulting in 3 different 11B models, then merging all three
          with Dare_Ties.</p>

          <p>Also, could you upload the Q8_0 GGUF? Thank you in advance.</p>

          '
        raw: "Interesting idea, have you considered other models like: Starling-LM-7B-alpha,\
          \ Chupacabra-v3, Zephyr-7b-beta and OpenChat-3.5?\r\nMaybe a Frankenmerge\
          \ between these Six models (including these two over here OpenHermes-2.5\
          \ and Neural-Chat-3.1) resulting in 3 different 11B models, then merging\
          \ all three with Dare_Ties.\r\n\r\nAlso, could you upload the Q8_0 GGUF?\
          \ Thank you in advance."
        updatedAt: '2023-11-28T11:40:46.219Z'
      numEdits: 0
      reactions: []
    id: 6565d1be9bf6665f10fd8c94
    type: comment
  author: Haiart
  content: "Interesting idea, have you considered other models like: Starling-LM-7B-alpha,\
    \ Chupacabra-v3, Zephyr-7b-beta and OpenChat-3.5?\r\nMaybe a Frankenmerge between\
    \ these Six models (including these two over here OpenHermes-2.5 and Neural-Chat-3.1)\
    \ resulting in 3 different 11B models, then merging all three with Dare_Ties.\r\
    \n\r\nAlso, could you upload the Q8_0 GGUF? Thank you in advance."
  created_at: 2023-11-28 11:40:46+00:00
  edited: false
  hidden: false
  id: 6565d1be9bf6665f10fd8c94
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9506bfa1b3fa7411fa8f6d3a817403a1.svg
      fullname: "Sascha L\xFCscher"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: S4sch
      type: user
    createdAt: '2023-11-29T13:17:06.000Z'
    data:
      edited: false
      editors:
      - S4sch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9641274809837341
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9506bfa1b3fa7411fa8f6d3a817403a1.svg
          fullname: "Sascha L\xFCscher"
          isHf: false
          isPro: true
          name: S4sch
          type: user
        html: '<p>yes, i uploaded here <a href="https://huggingface.co/S4sch/Open-Hermes-2.5-neural-chat-3.1-frankenmerge-11b-gguf-q8">https://huggingface.co/S4sch/Open-Hermes-2.5-neural-chat-3.1-frankenmerge-11b-gguf-q8</a>
          a q8 and a q5. However, while talking to the model, it has sometimes weird
          repetitions issues, i think it might have to do with the eos tokens. I will
          try to do another merge where i unionize the special tokens of both and
          see whether it is better.</p>

          <p>And yes, i can definitely try that! I''m currently figuring out, which
          models are good to merge, as i realized that if they are different in the
          architecture or the special tokens and how they got trained, the result
          can be quite weird. In the next few days i will try out some different merges
          and see what is good and what not.</p>

          '
        raw: 'yes, i uploaded here https://huggingface.co/S4sch/Open-Hermes-2.5-neural-chat-3.1-frankenmerge-11b-gguf-q8
          a q8 and a q5. However, while talking to the model, it has sometimes weird
          repetitions issues, i think it might have to do with the eos tokens. I will
          try to do another merge where i unionize the special tokens of both and
          see whether it is better.


          And yes, i can definitely try that! I''m currently figuring out, which models
          are good to merge, as i realized that if they are different in the architecture
          or the special tokens and how they got trained, the result can be quite
          weird. In the next few days i will try out some different merges and see
          what is good and what not.

          '
        updatedAt: '2023-11-29T13:17:06.800Z'
      numEdits: 0
      reactions: []
    id: 656739d2063938c9b4fed6a0
    type: comment
  author: S4sch
  content: 'yes, i uploaded here https://huggingface.co/S4sch/Open-Hermes-2.5-neural-chat-3.1-frankenmerge-11b-gguf-q8
    a q8 and a q5. However, while talking to the model, it has sometimes weird repetitions
    issues, i think it might have to do with the eos tokens. I will try to do another
    merge where i unionize the special tokens of both and see whether it is better.


    And yes, i can definitely try that! I''m currently figuring out, which models
    are good to merge, as i realized that if they are different in the architecture
    or the special tokens and how they got trained, the result can be quite weird.
    In the next few days i will try out some different merges and see what is good
    and what not.

    '
  created_at: 2023-11-29 13:17:06+00:00
  edited: false
  hidden: false
  id: 656739d2063938c9b4fed6a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/818d9cc7adf75b927d1f39051038f91a.svg
      fullname: Patrick Macedo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Haiart
      type: user
    createdAt: '2023-11-29T13:36:14.000Z'
    data:
      edited: true
      editors:
      - Haiart
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9777068495750427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/818d9cc7adf75b927d1f39051038f91a.svg
          fullname: Patrick Macedo
          isHf: false
          isPro: false
          name: Haiart
          type: user
        html: '<p>Interestingly enough, all models that I recommended above are all
          based on Mistral-v0.1 but of course, they employ different training methods
          and whatnot.<br>Unionizing the tokens is sensible and welcomed, but I somehow
          didn''t face any repetition issues, but my settings are probably different
          than yours.</p>

          <p>The idea above was simple on paper, since it''s just picking two models
          from the Six available, "Frankenmerging" two of them, until you have three
          different 11B ones (without using a single model twice on the Frankenmerge)
          and then Merging all three together (preferably with Dare_Ties) for a definitive
          11B Mistral based model, but like you mentioned, it has issues, if you''re
          going to try anyway, I''ll be looking forward to it. I do think that they
          have good compatibility since they''re all based on a single model after
          all.</p>

          '
        raw: 'Interestingly enough, all models that I recommended above are all based
          on Mistral-v0.1 but of course, they employ different training methods and
          whatnot.

          Unionizing the tokens is sensible and welcomed, but I somehow didn''t face
          any repetition issues, but my settings are probably different than yours.


          The idea above was simple on paper, since it''s just picking two models
          from the Six available, "Frankenmerging" two of them, until you have three
          different 11B ones (without using a single model twice on the Frankenmerge)
          and then Merging all three together (preferably with Dare_Ties) for a definitive
          11B Mistral based model, but like you mentioned, it has issues, if you''re
          going to try anyway, I''ll be looking forward to it. I do think that they
          have good compatibility since they''re all based on a single model after
          all.'
        updatedAt: '2023-11-29T13:44:45.448Z'
      numEdits: 1
      reactions: []
    id: 65673e4e4f87f5f4aee8a157
    type: comment
  author: Haiart
  content: 'Interestingly enough, all models that I recommended above are all based
    on Mistral-v0.1 but of course, they employ different training methods and whatnot.

    Unionizing the tokens is sensible and welcomed, but I somehow didn''t face any
    repetition issues, but my settings are probably different than yours.


    The idea above was simple on paper, since it''s just picking two models from the
    Six available, "Frankenmerging" two of them, until you have three different 11B
    ones (without using a single model twice on the Frankenmerge) and then Merging
    all three together (preferably with Dare_Ties) for a definitive 11B Mistral based
    model, but like you mentioned, it has issues, if you''re going to try anyway,
    I''ll be looking forward to it. I do think that they have good compatibility since
    they''re all based on a single model after all.'
  created_at: 2023-11-29 13:36:14+00:00
  edited: true
  hidden: false
  id: 65673e4e4f87f5f4aee8a157
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-02T00:53:07.000Z'
    data:
      edited: false
      editors:
      - athirdpath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9741818904876709
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
          fullname: Raven
          isHf: false
          isPro: false
          name: athirdpath
          type: user
        html: '<p>Hello S4sch, this is very interesting. I''m testing DARE and DPO
          right now, but my 20b models are prohibitively large for DPO training. I''m
          going to do some iterations on your strategy.</p>

          '
        raw: Hello S4sch, this is very interesting. I'm testing DARE and DPO right
          now, but my 20b models are prohibitively large for DPO training. I'm going
          to do some iterations on your strategy.
        updatedAt: '2023-12-02T00:53:07.965Z'
      numEdits: 0
      reactions: []
    id: 656a7ff3903e16e62b8b8985
    type: comment
  author: athirdpath
  content: Hello S4sch, this is very interesting. I'm testing DARE and DPO right now,
    but my 20b models are prohibitively large for DPO training. I'm going to do some
    iterations on your strategy.
  created_at: 2023-12-02 00:53:07+00:00
  edited: false
  hidden: false
  id: 656a7ff3903e16e62b8b8985
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
      fullname: Raven
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: athirdpath
      type: user
    createdAt: '2023-12-02T03:25:36.000Z'
    data:
      edited: false
      editors:
      - athirdpath
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8722715973854065
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/633a809fa4a8f33508dce32c/m3tP8ibrZK0qOla00g2rq.png?w=200&h=200&f=face
          fullname: Raven
          isHf: false
          isPro: false
          name: athirdpath
          type: user
        html: '<p><a href="https://huggingface.co/athirdpath/BigMistral-11b-S4sch">A
          model following the same layout but made of only the base model</a> exhibits
          the same eos issues, unfortunately. It seems to be an effect of the layer
          layout.</p>

          '
        raw: '[A model following the same layout but made of only the base model](https://huggingface.co/athirdpath/BigMistral-11b-S4sch)
          exhibits the same eos issues, unfortunately. It seems to be an effect of
          the layer layout.'
        updatedAt: '2023-12-02T03:25:36.031Z'
      numEdits: 0
      reactions: []
    id: 656aa3b09dcedd16d5ddf3a9
    type: comment
  author: athirdpath
  content: '[A model following the same layout but made of only the base model](https://huggingface.co/athirdpath/BigMistral-11b-S4sch)
    exhibits the same eos issues, unfortunately. It seems to be an effect of the layer
    layout.'
  created_at: 2023-12-02 03:25:36+00:00
  edited: false
  hidden: false
  id: 656aa3b09dcedd16d5ddf3a9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9506bfa1b3fa7411fa8f6d3a817403a1.svg
      fullname: "Sascha L\xFCscher"
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: S4sch
      type: user
    createdAt: '2023-12-02T23:29:10.000Z'
    data:
      edited: false
      editors:
      - S4sch
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9901534914970398
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9506bfa1b3fa7411fa8f6d3a817403a1.svg
          fullname: "Sascha L\xFCscher"
          isHf: false
          isPro: true
          name: S4sch
          type: user
        html: '<p>Hi athirdpath! yes, that really seems to be case. It would make
          sense that if the models are trained in their own specific format, that
          if we merge them it can easily get confused. It is interesting though that
          its not always the case and in some tests it can still do quite well, its
          just not very reliable. </p>

          '
        raw: 'Hi athirdpath! yes, that really seems to be case. It would make sense
          that if the models are trained in their own specific format, that if we
          merge them it can easily get confused. It is interesting though that its
          not always the case and in some tests it can still do quite well, its just
          not very reliable. '
        updatedAt: '2023-12-02T23:29:10.712Z'
      numEdits: 0
      reactions: []
    id: 656bbdc69496f21be85cb866
    type: comment
  author: S4sch
  content: 'Hi athirdpath! yes, that really seems to be case. It would make sense
    that if the models are trained in their own specific format, that if we merge
    them it can easily get confused. It is interesting though that its not always
    the case and in some tests it can still do quite well, its just not very reliable. '
  created_at: 2023-12-02 23:29:10+00:00
  edited: false
  hidden: false
  id: 656bbdc69496f21be85cb866
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: S4sch/Open-Hermes-2.5-neural-chat-3.1-frankenmerge-11b
repo_type: model
status: open
target_branch: null
title: Very interesting.
