!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ShaneTian
conflicting_files: null
created_at: 2022-08-10 12:41:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660138927470-62e37ac7a0be7413eb879b0a.jpeg?w=200&h=200&f=face
      fullname: Shane Tian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ShaneTian
      type: user
    createdAt: '2022-08-10T13:41:38.000Z'
    data:
      edited: false
      editors:
      - ShaneTian
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660138927470-62e37ac7a0be7413eb879b0a.jpeg?w=200&h=200&f=face
          fullname: Shane Tian
          isHf: false
          isPro: false
          name: ShaneTian
          type: user
        html: '<p><code>transformers.CpmTokenizer</code> is based on <code>transformers.XLNetTokenizer</code>,
          but the <a rel="nofollow" href="https://github.com/TsinghuaAI/CPM-1-Finetune/blob/c0d892185912b28f8efeaeb55905f3f4fb227e46/data_utils/tokenization_gpt2.py#L32">original
          CPM-1 tokenizer</a> is not.</p>

          <p>I found in fine-tuning:</p>

          <ul>

          <li>the original tokenizer always add an <code>eod_token = &lt;eod&gt;</code>
          in the end of sentence , see <a rel="nofollow" href="https://github.com/TsinghuaAI/CPM-1-Finetune/blob/c0d892185912b28f8efeaeb55905f3f4fb227e46/finetune_lm.py#L62">here</a>.</li>

          <li>the <code>transformers.CpmTokenizer</code> always add <code>sep_token
          = &lt;sep&gt;</code> and <code>cls_token = &lt;cls&gt;</code> in the end
          of sentence, see <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/d7e2d7b40b1070cddfe878e13705725f49a2cf1f/src/transformers/models/xlnet/tokenization_xlnet.py#L275">here</a>.</li>

          </ul>

          <p>I am confused.<br>In LM fine-tuning, how to prepare the input data?</p>

          <ul>

          <li><code>[token_id_1, token_id_2, ..., eod_token_id]</code>, where <code>eod_token_id</code>
          is the id of <code>&lt;eod&gt;</code> token in <code>transformers.CpmTokenizer</code></li>

          <li><code>[token_id_1, token_id_2, ..., eos_token_id]</code>, where <code>eos_token_id</code>
          is the id of <code>&lt;/s&gt;</code> token in <code>transformers.CpmTokenizer</code></li>

          <li><code>[token_id_1, token_id_2, ..., eos_token_id]</code>, where <code>eos_token_id</code>
          is the id of <code>&lt;|endoftext|&gt;</code> token in <code>transformers.GPT2Tokenizer</code></li>

          <li><code>[token_id_1, token_id_2, ..., sep_token_id, cls_token_id]</code>,
          just call <code>CpmTokenizer</code></li>

          </ul>

          '
        raw: "`transformers.CpmTokenizer` is based on `transformers.XLNetTokenizer`,\
          \ but the [original CPM-1 tokenizer](https://github.com/TsinghuaAI/CPM-1-Finetune/blob/c0d892185912b28f8efeaeb55905f3f4fb227e46/data_utils/tokenization_gpt2.py#L32)\
          \ is not.\r\n\r\nI found in fine-tuning:\r\n- the original tokenizer always\
          \ add an `eod_token = <eod>` in the end of sentence , see [here](https://github.com/TsinghuaAI/CPM-1-Finetune/blob/c0d892185912b28f8efeaeb55905f3f4fb227e46/finetune_lm.py#L62).\r\
          \n- the `transformers.CpmTokenizer` always add `sep_token = <sep>` and `cls_token\
          \ = <cls>` in the end of sentence, see [here](https://github.com/huggingface/transformers/blob/d7e2d7b40b1070cddfe878e13705725f49a2cf1f/src/transformers/models/xlnet/tokenization_xlnet.py#L275).\r\
          \n\r\nI am confused.\r\nIn LM fine-tuning, how to prepare the input data?\r\
          \n- `[token_id_1, token_id_2, ..., eod_token_id]`, where `eod_token_id`\
          \ is the id of `<eod>` token in `transformers.CpmTokenizer`\r\n- `[token_id_1,\
          \ token_id_2, ..., eos_token_id]`, where `eos_token_id` is the id of `</s>`\
          \ token in `transformers.CpmTokenizer`\r\n- `[token_id_1, token_id_2, ...,\
          \ eos_token_id]`, where `eos_token_id` is the id of `<|endoftext|>` token\
          \ in `transformers.GPT2Tokenizer`\r\n- `[token_id_1, token_id_2, ..., sep_token_id,\
          \ cls_token_id]`, just call `CpmTokenizer`"
        updatedAt: '2022-08-10T13:41:38.063Z'
      numEdits: 0
      reactions: []
    id: 62f3b592a0bee757fda53cfa
    type: comment
  author: ShaneTian
  content: "`transformers.CpmTokenizer` is based on `transformers.XLNetTokenizer`,\
    \ but the [original CPM-1 tokenizer](https://github.com/TsinghuaAI/CPM-1-Finetune/blob/c0d892185912b28f8efeaeb55905f3f4fb227e46/data_utils/tokenization_gpt2.py#L32)\
    \ is not.\r\n\r\nI found in fine-tuning:\r\n- the original tokenizer always add\
    \ an `eod_token = <eod>` in the end of sentence , see [here](https://github.com/TsinghuaAI/CPM-1-Finetune/blob/c0d892185912b28f8efeaeb55905f3f4fb227e46/finetune_lm.py#L62).\r\
    \n- the `transformers.CpmTokenizer` always add `sep_token = <sep>` and `cls_token\
    \ = <cls>` in the end of sentence, see [here](https://github.com/huggingface/transformers/blob/d7e2d7b40b1070cddfe878e13705725f49a2cf1f/src/transformers/models/xlnet/tokenization_xlnet.py#L275).\r\
    \n\r\nI am confused.\r\nIn LM fine-tuning, how to prepare the input data?\r\n\
    - `[token_id_1, token_id_2, ..., eod_token_id]`, where `eod_token_id` is the id\
    \ of `<eod>` token in `transformers.CpmTokenizer`\r\n- `[token_id_1, token_id_2,\
    \ ..., eos_token_id]`, where `eos_token_id` is the id of `</s>` token in `transformers.CpmTokenizer`\r\
    \n- `[token_id_1, token_id_2, ..., eos_token_id]`, where `eos_token_id` is the\
    \ id of `<|endoftext|>` token in `transformers.GPT2Tokenizer`\r\n- `[token_id_1,\
    \ token_id_2, ..., sep_token_id, cls_token_id]`, just call `CpmTokenizer`"
  created_at: 2022-08-10 12:41:38+00:00
  edited: false
  hidden: false
  id: 62f3b592a0bee757fda53cfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-09-06T19:41:07.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8884994387626648
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Wow so sorry for the very much late reply! You are right, we should
          probably correct the <code>build_inputs_with_special_tokens</code> function,
          which is used when you set <code>add_special_tokens = True</code> (to format
          the inputs)</p>

          '
        raw: Wow so sorry for the very much late reply! You are right, we should probably
          correct the `build_inputs_with_special_tokens` function, which is used when
          you set `add_special_tokens = True` (to format the inputs)
        updatedAt: '2023-09-06T19:41:07.049Z'
      numEdits: 0
      reactions: []
    id: 64f8d5d3f6b80fae5abad274
    type: comment
  author: ArthurZ
  content: Wow so sorry for the very much late reply! You are right, we should probably
    correct the `build_inputs_with_special_tokens` function, which is used when you
    set `add_special_tokens = True` (to format the inputs)
  created_at: 2023-09-06 18:41:07+00:00
  edited: false
  hidden: false
  id: 64f8d5d3f6b80fae5abad274
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-09-06T19:43:19.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7730841636657715
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>You can also change the template processor if you are using a fast
          tokenizer.</p>

          '
        raw: You can also change the template processor if you are using a fast tokenizer.
        updatedAt: '2023-09-06T19:43:19.279Z'
      numEdits: 0
      reactions: []
    id: 64f8d6575515d7dcceb37831
    type: comment
  author: ArthurZ
  content: You can also change the template processor if you are using a fast tokenizer.
  created_at: 2023-09-06 18:43:19+00:00
  edited: false
  hidden: false
  id: 64f8d6575515d7dcceb37831
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TsinghuaAI/CPM-Generate
repo_type: model
status: open
target_branch: null
title: '`CpmTokenizer` is different from the original CPM-1 tokenizer in GitHub'
