!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-07-13 00:24:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-07-13T01:24:33.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9051006436347961
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>This was a conversation between me and some other people in the
          koboldcpp discord</p>

          <p>me: but do you know if this works the same in oobagooba? the no need
          for superhot for 8k deal?<br>discord guy: ya I saw it''s the same, change
          n_ctx to 4096 or 8192<br>some ppl say 8K ctx turns them dumb for some reason
          LOL<br>regardless of kcpp or other webui</p>

          <p>So is it true? Do you not even need super hot? Can you run any ggml model
          in 8k? And can you run GPTQ models, Fp16 models in 8k without superhot also?
          Let me know if you have any idea how any of this works because im clueless
          lol</p>

          '
        raw: "This was a conversation between me and some other people in the koboldcpp\
          \ discord\r\n\r\nme: but do you know if this works the same in oobagooba?\
          \ the no need for superhot for 8k deal?\r\ndiscord guy: ya I saw it's the\
          \ same, change n_ctx to 4096 or 8192\r\nsome ppl say 8K ctx turns them dumb\
          \ for some reason LOL\r\nregardless of kcpp or other webui\r\n\r\nSo is\
          \ it true? Do you not even need super hot? Can you run any ggml model in\
          \ 8k? And can you run GPTQ models, Fp16 models in 8k without superhot also?\
          \ Let me know if you have any idea how any of this works because im clueless\
          \ lol"
        updatedAt: '2023-07-13T01:24:33.178Z'
      numEdits: 0
      reactions: []
    id: 64af5251fcd8c4d4b5eea3aa
    type: comment
  author: rombodawg
  content: "This was a conversation between me and some other people in the koboldcpp\
    \ discord\r\n\r\nme: but do you know if this works the same in oobagooba? the\
    \ no need for superhot for 8k deal?\r\ndiscord guy: ya I saw it's the same, change\
    \ n_ctx to 4096 or 8192\r\nsome ppl say 8K ctx turns them dumb for some reason\
    \ LOL\r\nregardless of kcpp or other webui\r\n\r\nSo is it true? Do you not even\
    \ need super hot? Can you run any ggml model in 8k? And can you run GPTQ models,\
    \ Fp16 models in 8k without superhot also? Let me know if you have any idea how\
    \ any of this works because im clueless lol"
  created_at: 2023-07-13 00:24:33+00:00
  edited: false
  hidden: false
  id: 64af5251fcd8c4d4b5eea3aa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/WizardLM-13B-V1.1-GGML
repo_type: model
status: open
target_branch: null
title: Is it true that superhot is useless now and any ggml model can do 8k context?
