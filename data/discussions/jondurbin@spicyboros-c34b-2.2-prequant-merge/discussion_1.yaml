!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SabinStargem
conflicting_files: null
created_at: 2023-09-12 13:12:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
      fullname: Sabin Stargem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SabinStargem
      type: user
    createdAt: '2023-09-12T14:12:04.000Z'
    data:
      edited: false
      editors:
      - SabinStargem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9731370210647583
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
          fullname: Sabin Stargem
          isHf: false
          isPro: false
          name: SabinStargem
          type: user
        html: '<p>As mentioned in the title, I have an opinion.  I believe that people
          don''t see your 34b models during searches, because they have to attach
          an "c" to the parameter count in order for your CodeLlama variants to show
          up.</p>

          '
        raw: As mentioned in the title, I have an opinion.  I believe that people
          don't see your 34b models during searches, because they have to attach an
          "c" to the parameter count in order for your CodeLlama variants to show
          up.
        updatedAt: '2023-09-12T14:12:04.101Z'
      numEdits: 0
      reactions: []
    id: 650071b4d117b28d30fca3d6
    type: comment
  author: SabinStargem
  content: As mentioned in the title, I have an opinion.  I believe that people don't
    see your 34b models during searches, because they have to attach an "c" to the
    parameter count in order for your CodeLlama variants to show up.
  created_at: 2023-09-12 13:12:04+00:00
  edited: false
  hidden: false
  id: 650071b4d117b28d30fca3d6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
      fullname: Jon Durbin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: jondurbin
      type: user
    createdAt: '2023-09-12T14:13:02.000Z'
    data:
      edited: false
      editors:
      - jondurbin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.945635974407196
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6453dafca647b92069ac541a/QkUleoJtHHdTkqtW54QIG.jpeg?w=200&h=200&f=face
          fullname: Jon Durbin
          isHf: false
          isPro: true
          name: jondurbin
          type: user
        html: '<p>I was still trying to be optimistic and holding out for a base llama-34b.  But,
          alas, it may never happen.</p>

          '
        raw: I was still trying to be optimistic and holding out for a base llama-34b.  But,
          alas, it may never happen.
        updatedAt: '2023-09-12T14:13:02.337Z'
      numEdits: 0
      reactions: []
    id: 650071ee52aec889f4b05362
    type: comment
  author: jondurbin
  content: I was still trying to be optimistic and holding out for a base llama-34b.  But,
    alas, it may never happen.
  created_at: 2023-09-12 13:13:02+00:00
  edited: false
  hidden: false
  id: 650071ee52aec889f4b05362
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
      fullname: Sabin Stargem
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SabinStargem
      type: user
    createdAt: '2023-09-12T14:24:50.000Z'
    data:
      edited: false
      editors:
      - SabinStargem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9970258474349976
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/TKk0SYTJxvCYG3tZPpTQt.jpeg?w=200&h=200&f=face
          fullname: Sabin Stargem
          isHf: false
          isPro: false
          name: SabinStargem
          type: user
        html: '<p>Personally, I think that CodeLlama is basically Llama 2.1.   Don''t
          think I heard anyone say that 34b had the issues that were reported for
          other sizes.</p>

          '
        raw: Personally, I think that CodeLlama is basically Llama 2.1.   Don't think
          I heard anyone say that 34b had the issues that were reported for other
          sizes.
        updatedAt: '2023-09-12T14:24:50.854Z'
      numEdits: 0
      reactions: []
    id: 650074b2f322f915667069a4
    type: comment
  author: SabinStargem
  content: Personally, I think that CodeLlama is basically Llama 2.1.   Don't think
    I heard anyone say that 34b had the issues that were reported for other sizes.
  created_at: 2023-09-12 13:24:50+00:00
  edited: false
  hidden: false
  id: 650074b2f322f915667069a4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/431f1c79efb0ece9bd46a71e0bf2dda1.svg
      fullname: Dan J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gershwin69
      type: user
    createdAt: '2023-09-13T07:36:49.000Z'
    data:
      edited: false
      editors:
      - Gershwin69
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9579768180847168
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/431f1c79efb0ece9bd46a71e0bf2dda1.svg
          fullname: Dan J
          isHf: false
          isPro: false
          name: Gershwin69
          type: user
        html: '<p>I would assume "CodeLlama" has been trained on a majority code tasks
          and logic, and thus is less optimised for conversations and story prompts,
          hence adding the "c". It''d be misleading to name this in a way that doesn''t
          denote the fact that it''s not a "standard" llama2 34b model.</p>

          <p>However you could name it "spicyboros-cl2-34b-2.2" to avoid any mistaken
          interpretations of the "34b" value. I can imagine some programs/scripts
          could attempt parsing it, for example.</p>

          <p>Also, the model card doesn''t mention the context size, but the gguf
          suggests that its 16k, is that correct? What impact does using it at non-16k
          context sized have on its results?</p>

          '
        raw: 'I would assume "CodeLlama" has been trained on a majority code tasks
          and logic, and thus is less optimised for conversations and story prompts,
          hence adding the "c". It''d be misleading to name this in a way that doesn''t
          denote the fact that it''s not a "standard" llama2 34b model.


          However you could name it "spicyboros-cl2-34b-2.2" to avoid any mistaken
          interpretations of the "34b" value. I can imagine some programs/scripts
          could attempt parsing it, for example.


          Also, the model card doesn''t mention the context size, but the gguf suggests
          that its 16k, is that correct? What impact does using it at non-16k context
          sized have on its results?'
        updatedAt: '2023-09-13T07:36:49.599Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Nexesenex
    id: 65016691f355a888f960e091
    type: comment
  author: Gershwin69
  content: 'I would assume "CodeLlama" has been trained on a majority code tasks and
    logic, and thus is less optimised for conversations and story prompts, hence adding
    the "c". It''d be misleading to name this in a way that doesn''t denote the fact
    that it''s not a "standard" llama2 34b model.


    However you could name it "spicyboros-cl2-34b-2.2" to avoid any mistaken interpretations
    of the "34b" value. I can imagine some programs/scripts could attempt parsing
    it, for example.


    Also, the model card doesn''t mention the context size, but the gguf suggests
    that its 16k, is that correct? What impact does using it at non-16k context sized
    have on its results?'
  created_at: 2023-09-13 06:36:49+00:00
  edited: false
  hidden: false
  id: 65016691f355a888f960e091
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: jondurbin/spicyboros-c34b-2.2-prequant-merge
repo_type: model
status: open
target_branch: null
title: 'Request:  I think the name should be "34b" rather than "c34b".'
