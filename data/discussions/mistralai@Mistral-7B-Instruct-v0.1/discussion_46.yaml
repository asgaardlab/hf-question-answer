!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TwoCats17
conflicting_files: null
created_at: 2023-10-09 14:34:47+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Gw50mm3T8PGxmMiauRsI4.jpeg?w=200&h=200&f=face
      fullname: Cats
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TwoCats17
      type: user
    createdAt: '2023-10-09T15:34:47.000Z'
    data:
      edited: false
      editors:
      - TwoCats17
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5865211486816406
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Gw50mm3T8PGxmMiauRsI4.jpeg?w=200&h=200&f=face
          fullname: Cats
          isHf: false
          isPro: false
          name: TwoCats17
          type: user
        html: '<p>i use this code:</p>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer<br>import
          torch</p>

          <p>if torch.cuda.is_available():<br>    device = torch.device("cuda")<br>    print(f"Using
          GPU {torch.cuda.get_device_name(0)}")<br>    memory_allocated = torch.cuda.memory_allocated()<br>    print(f"GPU
          memory_allocated: {memory_allocated}")</p>

          <p>else:<br>    device = torch.device("cpu")<br>    print(f"Using CPU {torch.cuda.get_device_name(0)}")</p>

          <p>model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br>tokenizer
          = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")</p>

          <p>messages = [<br>    {"role": "user", "content": "What is your favourite
          condiment?"},<br>    {"role": "assistant", "content": "Well, I''m quite
          partial to a good squeeze of fresh lemon juice. It adds just the right amount
          of zesty flavour to whatever I''m cooking up in the kitchen!"},<br>    {"role":
          "user", "content": "Do you have mayonnaise recipes?"}<br>]</p>

          <p>encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")</p>

          <p>model_inputs = encodeds.to(device)<br>model.to(device)</p>

          <p>generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)</p>

          <p>decoded = tokenizer.batch_decode(generated_ids)<br>print(decoded[0])</p>

          <p>but when i start this i got:<br>Using GPU NVIDIA GeForce GTX 1050 Ti<br>GPU
          memory_allocated: 0<br>killed</p>

          <p>I checked the loading of the video card using<br>nvidia-smi<br>It''s
          loaded at no more than 500mb when I try to run the code<br>how much memory
          do I need to run Mistral-7B-Instruct-v0.1?</p>

          '
        raw: "i use this code:\r\n\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\nimport torch\r\n\r\nif torch.cuda.is_available():\r\n\
          \    device = torch.device(\"cuda\")\r\n    print(f\"Using GPU {torch.cuda.get_device_name(0)}\"\
          )\r\n    memory_allocated = torch.cuda.memory_allocated()\r\n    print(f\"\
          GPU memory_allocated: {memory_allocated}\")\r\n\r\nelse:\r\n    device =\
          \ torch.device(\"cpu\")\r\n    print(f\"Using CPU {torch.cuda.get_device_name(0)}\"\
          )\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\r\n\r\nmessages = [\r\n    {\"role\": \"user\", \"content\": \"What is\
          \ your favourite condiment?\"},\r\n    {\"role\": \"assistant\", \"content\"\
          : \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds\
          \ just the right amount of zesty flavour to whatever I'm cooking up in the\
          \ kitchen!\"},\r\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise\
          \ recipes?\"}\r\n]\r\n\r\nencodeds = tokenizer.apply_chat_template(messages,\
          \ return_tensors=\"pt\")\r\n\r\nmodel_inputs = encodeds.to(device)\r\nmodel.to(device)\r\
          \n\r\n\r\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000,\
          \ do_sample=True)\r\n\r\n\r\ndecoded = tokenizer.batch_decode(generated_ids)\r\
          \nprint(decoded[0])\r\n\r\nbut when i start this i got:\r\nUsing GPU NVIDIA\
          \ GeForce GTX 1050 Ti\r\nGPU memory_allocated: 0\r\nkilled\r\n\r\nI checked\
          \ the loading of the video card using\r\nnvidia-smi\r\nIt's loaded at no\
          \ more than 500mb when I try to run the code\r\nhow much memory do I need\
          \ to run Mistral-7B-Instruct-v0.1?\r\n"
        updatedAt: '2023-10-09T15:34:47.200Z'
      numEdits: 0
      reactions: []
    id: 65241d97e5d47a2f52b6de08
    type: comment
  author: TwoCats17
  content: "i use this code:\r\n\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\nimport torch\r\n\r\nif torch.cuda.is_available():\r\n    device\
    \ = torch.device(\"cuda\")\r\n    print(f\"Using GPU {torch.cuda.get_device_name(0)}\"\
    )\r\n    memory_allocated = torch.cuda.memory_allocated()\r\n    print(f\"GPU\
    \ memory_allocated: {memory_allocated}\")\r\n\r\nelse:\r\n    device = torch.device(\"\
    cpu\")\r\n    print(f\"Using CPU {torch.cuda.get_device_name(0)}\")\r\n\r\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
    )\r\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
    )\r\n\r\nmessages = [\r\n    {\"role\": \"user\", \"content\": \"What is your\
    \ favourite condiment?\"},\r\n    {\"role\": \"assistant\", \"content\": \"Well,\
    \ I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right\
    \ amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\r\n \
    \   {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\r\n\
    ]\r\n\r\nencodeds = tokenizer.apply_chat_template(messages, return_tensors=\"\
    pt\")\r\n\r\nmodel_inputs = encodeds.to(device)\r\nmodel.to(device)\r\n\r\n\r\n\
    generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\r\
    \n\r\n\r\ndecoded = tokenizer.batch_decode(generated_ids)\r\nprint(decoded[0])\r\
    \n\r\nbut when i start this i got:\r\nUsing GPU NVIDIA GeForce GTX 1050 Ti\r\n\
    GPU memory_allocated: 0\r\nkilled\r\n\r\nI checked the loading of the video card\
    \ using\r\nnvidia-smi\r\nIt's loaded at no more than 500mb when I try to run the\
    \ code\r\nhow much memory do I need to run Mistral-7B-Instruct-v0.1?\r\n"
  created_at: 2023-10-09 14:34:47+00:00
  edited: false
  hidden: false
  id: 65241d97e5d47a2f52b6de08
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/828d7a436f1a7a08466785e7ab078c3c.svg
      fullname: Ayush Agrawal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ayush8120
      type: user
    createdAt: '2023-10-10T14:22:43.000Z'
    data:
      edited: false
      editors:
      - Ayush8120
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9399418830871582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/828d7a436f1a7a08466785e7ab078c3c.svg
          fullname: Ayush Agrawal
          isHf: false
          isPro: false
          name: Ayush8120
          type: user
        html: '<p>For unquantized model, you would be needing 16 GB memory while loading
          the model shards. </p>

          '
        raw: 'For unquantized model, you would be needing 16 GB memory while loading
          the model shards. '
        updatedAt: '2023-10-10T14:22:43.518Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ArthurZ
    id: 65255e33e1c63be429c069b4
    type: comment
  author: Ayush8120
  content: 'For unquantized model, you would be needing 16 GB memory while loading
    the model shards. '
  created_at: 2023-10-10 13:22:43+00:00
  edited: false
  hidden: false
  id: 65255e33e1c63be429c069b4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 46
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: killed message
