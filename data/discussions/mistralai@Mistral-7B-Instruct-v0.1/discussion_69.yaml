!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SivaPrasad02
conflicting_files: null
created_at: 2023-11-06 06:11:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ae2eaf2a202108f5ae23b9a1c7090917.svg
      fullname: Siva prasad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SivaPrasad02
      type: user
    createdAt: '2023-11-06T06:11:38.000Z'
    data:
      edited: false
      editors:
      - SivaPrasad02
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8829845786094666
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ae2eaf2a202108f5ae23b9a1c7090917.svg
          fullname: Siva prasad
          isHf: false
          isPro: false
          name: SivaPrasad02
          type: user
        html: '<p>Hi all, I am facing one error and one issue. Did anyone encounter
          a similar issue? </p>

          <ol>

          <li><p>ValueError: past key much have a shape of (<code>batch_size, num_heads,
          self.config.sliding_window-1, head_dim</code>), got torch.Size([1, 8, 1868,
          128])<br>The above error came while making inferences with an input length
          of 6k. Config is the default one that was used.</p>

          </li>

          <li><p>Batch inference generation is not the same as a single sample as
          shown below(generating  less number of tokens while doing batch inference
          with the same setting)</p>

          </li>

          </ol>

          <p>While doing batch inference<br>"""<br>[INST] Determine whether the following
          pairs of sentences embody an entailment relation or not.</p>

          <p>Sentences: America is a monarchy. Therefore America has a king.<br>Relation:<br>Choices:
          entailment,no-entailment [/INST]</p>

          <p>The relation between the two sentences is an entailment.<br>"""<br>the
          same while doing inference with only single sample.<br>"""<br>[INST] Determine
          whether the following pairs of sentences embody an entailment relation or
          not.</p>

          <p>Sentences: No one ordered any beverages. So no one ordered orange juice.<br>Relation:<br>Choices:
          entailment,no-entailment [/INST]</p>

          <p>The pair of sentences embody an entailment relation.</p>

          <p>Explanation: The first sentence, "No one ordered any beverages," is the
          antecedent of the conditional statement "So no one ordered orange juice."
          The second sentence is the consequent of the conditional statement. The
          entailment relation holds because if the antecedent is true, then the consequent
          must also be true. In this case, if no one ordered any beverages, then it
          logically follows that no one ordered orange juice.<br>"""</p>

          '
        raw: "\r\nHi all, I am facing one error and one issue. Did anyone encounter\
          \ a similar issue? \r\n\r\n1. ValueError: past key much have a shape of\
          \ (`batch_size, num_heads, self.config.sliding_window-1, head_dim`), got\
          \ torch.Size([1, 8, 1868, 128])\r\nThe above error came while making inferences\
          \ with an input length of 6k. Config is the default one that was used.\r\
          \n\r\n2. Batch inference generation is not the same as a single sample as\
          \ shown below(generating  less number of tokens while doing batch inference\
          \ with the same setting)\r\n\r\nWhile doing batch inference\r\n\"\"\"\r\n\
          [INST] Determine whether the following pairs of sentences embody an entailment\
          \ relation or not.\r\n\r\nSentences: America is a monarchy. Therefore America\
          \ has a king.\r\nRelation:\r\nChoices: entailment,no-entailment [/INST]\r\
          \n\r\nThe relation between the two sentences is an entailment.\r\n\"\"\"\
          \r\nthe same while doing inference with only single sample.\r\n\"\"\"\r\n\
          [INST] Determine whether the following pairs of sentences embody an entailment\
          \ relation or not.\r\n\r\nSentences: No one ordered any beverages. So no\
          \ one ordered orange juice.\r\nRelation:\r\nChoices: entailment,no-entailment\
          \ [/INST]\r\n\r\nThe pair of sentences embody an entailment relation.\r\n\
          \r\nExplanation: The first sentence, \"No one ordered any beverages,\" is\
          \ the antecedent of the conditional statement \"So no one ordered orange\
          \ juice.\" The second sentence is the consequent of the conditional statement.\
          \ The entailment relation holds because if the antecedent is true, then\
          \ the consequent must also be true. In this case, if no one ordered any\
          \ beverages, then it logically follows that no one ordered orange juice.\r\
          \n\"\"\""
        updatedAt: '2023-11-06T06:11:38.365Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - SivaPrasad02
        - onaka-ga-pkpk
        - erik-akert
    id: 6548839a7f43165b3d2716e5
    type: comment
  author: SivaPrasad02
  content: "\r\nHi all, I am facing one error and one issue. Did anyone encounter\
    \ a similar issue? \r\n\r\n1. ValueError: past key much have a shape of (`batch_size,\
    \ num_heads, self.config.sliding_window-1, head_dim`), got torch.Size([1, 8, 1868,\
    \ 128])\r\nThe above error came while making inferences with an input length of\
    \ 6k. Config is the default one that was used.\r\n\r\n2. Batch inference generation\
    \ is not the same as a single sample as shown below(generating  less number of\
    \ tokens while doing batch inference with the same setting)\r\n\r\nWhile doing\
    \ batch inference\r\n\"\"\"\r\n[INST] Determine whether the following pairs of\
    \ sentences embody an entailment relation or not.\r\n\r\nSentences: America is\
    \ a monarchy. Therefore America has a king.\r\nRelation:\r\nChoices: entailment,no-entailment\
    \ [/INST]\r\n\r\nThe relation between the two sentences is an entailment.\r\n\"\
    \"\"\r\nthe same while doing inference with only single sample.\r\n\"\"\"\r\n\
    [INST] Determine whether the following pairs of sentences embody an entailment\
    \ relation or not.\r\n\r\nSentences: No one ordered any beverages. So no one ordered\
    \ orange juice.\r\nRelation:\r\nChoices: entailment,no-entailment [/INST]\r\n\r\
    \nThe pair of sentences embody an entailment relation.\r\n\r\nExplanation: The\
    \ first sentence, \"No one ordered any beverages,\" is the antecedent of the conditional\
    \ statement \"So no one ordered orange juice.\" The second sentence is the consequent\
    \ of the conditional statement. The entailment relation holds because if the antecedent\
    \ is true, then the consequent must also be true. In this case, if no one ordered\
    \ any beverages, then it logically follows that no one ordered orange juice.\r\
    \n\"\"\""
  created_at: 2023-11-06 06:11:38+00:00
  edited: false
  hidden: false
  id: 6548839a7f43165b3d2716e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-08T12:23:40.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9765501618385315
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>I also see the first issue when I try to inference with more than
          the sliding window length. I''m unsure why this is the case.</p>

          '
        raw: I also see the first issue when I try to inference with more than the
          sliding window length. I'm unsure why this is the case.
        updatedAt: '2023-12-08T12:23:40.447Z'
      numEdits: 0
      reactions: []
    id: 65730acc6966e9683d943885
    type: comment
  author: RonanMcGovern
  content: I also see the first issue when I try to inference with more than the sliding
    window length. I'm unsure why this is the case.
  created_at: 2023-12-08 12:23:40+00:00
  edited: false
  hidden: false
  id: 65730acc6966e9683d943885
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 69
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Error while doing inference
