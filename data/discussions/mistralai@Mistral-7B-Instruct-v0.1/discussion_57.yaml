!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nadav-Timor
conflicting_files: null
created_at: 2023-10-26 14:54:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674221847840-62be5fc0c990a0a515da5f34.jpeg?w=200&h=200&f=face
      fullname: Nadav Timor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nadav-Timor
      type: user
    createdAt: '2023-10-26T15:54:43.000Z'
    data:
      edited: false
      editors:
      - Nadav-Timor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5104539394378662
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674221847840-62be5fc0c990a0a515da5f34.jpeg?w=200&h=200&f=face
          fullname: Nadav Timor
          isHf: false
          isPro: false
          name: Nadav-Timor
          type: user
        html: "<p>Hi,<br>Can you please clarify how you use the <code>max_position_embeddings</code>\
          \ hyperparameter? The <code>config.json</code> file <a href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/7ad5799710574ba1c1d953eba3077af582f3a773/config.json#L11\"\
          >specifies</a> <code>max_position_embeddings=32768</code> while the paper\
          \ claims an attention span of 131K tokens (see <a rel=\"nofollow\" href=\"\
          https://arxiv.org/pdf/2310.06825.pdf\">Section 2 on \"Architectural details\"\
          \ \u2192 \"Sliding Window Attention\"</a>).<br>Thanks!</p>\n"
        raw: "Hi,\r\nCan you please clarify how you use the `max_position_embeddings`\
          \ hyperparameter? The `config.json` file [specifies](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/7ad5799710574ba1c1d953eba3077af582f3a773/config.json#L11)\
          \ `max_position_embeddings=32768` while the paper claims an attention span\
          \ of 131K tokens (see [Section 2 on \"Architectural details\" \u2192 \"\
          Sliding Window Attention\"](https://arxiv.org/pdf/2310.06825.pdf)).\r\n\
          Thanks!"
        updatedAt: '2023-10-26T15:54:43.350Z'
      numEdits: 0
      reactions: []
    id: 653a8bc34d105d9696b44d6b
    type: comment
  author: Nadav-Timor
  content: "Hi,\r\nCan you please clarify how you use the `max_position_embeddings`\
    \ hyperparameter? The `config.json` file [specifies](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/7ad5799710574ba1c1d953eba3077af582f3a773/config.json#L11)\
    \ `max_position_embeddings=32768` while the paper claims an attention span of\
    \ 131K tokens (see [Section 2 on \"Architectural details\" \u2192 \"Sliding Window\
    \ Attention\"](https://arxiv.org/pdf/2310.06825.pdf)).\r\nThanks!"
  created_at: 2023-10-26 14:54:43+00:00
  edited: false
  hidden: false
  id: 653a8bc34d105d9696b44d6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674221847840-62be5fc0c990a0a515da5f34.jpeg?w=200&h=200&f=face
      fullname: Nadav Timor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nadav-Timor
      type: user
    createdAt: '2023-10-27T21:27:21.000Z'
    data:
      edited: true
      editors:
      - Nadav-Timor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8258491158485413
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674221847840-62be5fc0c990a0a515da5f34.jpeg?w=200&h=200&f=face
          fullname: Nadav Timor
          isHf: false
          isPro: false
          name: Nadav-Timor
          type: user
        html: '<p>See <a rel="nofollow" href="https://github.com/mistralai/mistral-src/issues/53">this</a>
          GitHub Issue by @ParadoxZW from a few days ago</p>

          '
        raw: See [this](https://github.com/mistralai/mistral-src/issues/53) GitHub
          Issue by @ParadoxZW from a few days ago
        updatedAt: '2023-10-31T13:55:47.908Z'
      numEdits: 1
      reactions: []
    id: 653c2b399d020960618557a0
    type: comment
  author: Nadav-Timor
  content: See [this](https://github.com/mistralai/mistral-src/issues/53) GitHub Issue
    by @ParadoxZW from a few days ago
  created_at: 2023-10-27 20:27:21+00:00
  edited: true
  hidden: false
  id: 653c2b399d020960618557a0
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 57
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: '`max_position_embeddings=32768` with "attention span of 131K tokens"'
