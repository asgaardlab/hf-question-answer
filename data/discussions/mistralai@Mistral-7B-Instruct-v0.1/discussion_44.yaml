!!python/object:huggingface_hub.community.DiscussionWithDetails
author: codegood
conflicting_files: null
created_at: 2023-10-06 20:35:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-10-06T21:35:13.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6027231812477112
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: '<p>I cannot use the model with transformers==4.33.3, but when I upgrade
          transformers to the latest version I get the below error. I don''t get this
          error when using the previous version.</p>

          <p>ImportError: Using <code>load_in_8bit=True</code> requires Accelerate:
          <code>pip install accelerate</code> and the latest version of bitsandbytes
          <code>pip install -i https://test.pypi.org/simple/ bitsandbytes</code> or
          pip install bitsandbytes` </p>

          <p>My code:</p>

          <p>model =AutoModelForCausalLM.from_pretrained( # Initiating the model<br>    model_id,<br>    device_map="auto",<br>    offload_folder
          = "/content/drive/MyDrive/Mistral7B",<br>    trust_remote_code=True,<br>    quantization_config=bnb_config,<br>    torch_dtype="auto",<br>    use_flash_attention_2
          = True,<br>    # revision = "4a426d8015bef5a0cb3acff8d4474ee9ab4071d5"</p>

          <p>)</p>

          '
        raw: "I cannot use the model with transformers==4.33.3, but when I upgrade\
          \ transformers to the latest version I get the below error. I don't get\
          \ this error when using the previous version.\r\n\r\nImportError: Using\
          \ `load_in_8bit=True` requires Accelerate: `pip install accelerate` and\
          \ the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/\
          \ bitsandbytes` or pip install bitsandbytes` \r\n\r\nMy code:\r\n\r\nmodel\
          \ =AutoModelForCausalLM.from_pretrained( # Initiating the model\r\n    model_id,\r\
          \n    device_map=\"auto\",\r\n    offload_folder = \"/content/drive/MyDrive/Mistral7B\"\
          ,\r\n    trust_remote_code=True,\r\n    quantization_config=bnb_config,\r\
          \n    torch_dtype=\"auto\",\r\n    use_flash_attention_2 = True,\r\n   \
          \ # revision = \"4a426d8015bef5a0cb3acff8d4474ee9ab4071d5\"\r\n\r\n)"
        updatedAt: '2023-10-06T21:35:13.561Z'
      numEdits: 0
      reactions: []
    id: 65207d91f28ce435b4493b70
    type: comment
  author: codegood
  content: "I cannot use the model with transformers==4.33.3, but when I upgrade transformers\
    \ to the latest version I get the below error. I don't get this error when using\
    \ the previous version.\r\n\r\nImportError: Using `load_in_8bit=True` requires\
    \ Accelerate: `pip install accelerate` and the latest version of bitsandbytes\
    \ `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`\
    \ \r\n\r\nMy code:\r\n\r\nmodel =AutoModelForCausalLM.from_pretrained( # Initiating\
    \ the model\r\n    model_id,\r\n    device_map=\"auto\",\r\n    offload_folder\
    \ = \"/content/drive/MyDrive/Mistral7B\",\r\n    trust_remote_code=True,\r\n \
    \   quantization_config=bnb_config,\r\n    torch_dtype=\"auto\",\r\n    use_flash_attention_2\
    \ = True,\r\n    # revision = \"4a426d8015bef5a0cb3acff8d4474ee9ab4071d5\"\r\n\
    \r\n)"
  created_at: 2023-10-06 20:35:13+00:00
  edited: false
  hidden: false
  id: 65207d91f28ce435b4493b70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd006801e37e0139d955efbb9ca609a2.svg
      fullname: Kha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AyouKha
      type: user
    createdAt: '2023-10-13T10:14:55.000Z'
    data:
      edited: false
      editors:
      - AyouKha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9135463237762451
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd006801e37e0139d955efbb9ca609a2.svg
          fullname: Kha
          isHf: false
          isPro: false
          name: AyouKha
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;codegood&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/codegood\">@<span class=\"\
          underline\">codegood</span></a></span>\n\n\t</span></span> Have you found\
          \ a fix? I have been struggling with the same error on Azure ML Studio.\
          \ Thanks</p>\n"
        raw: '@codegood Have you found a fix? I have been struggling with the same
          error on Azure ML Studio. Thanks

          '
        updatedAt: '2023-10-13T10:14:55.400Z'
      numEdits: 0
      reactions: []
    id: 6529189f4d1ab06b9d885081
    type: comment
  author: AyouKha
  content: '@codegood Have you found a fix? I have been struggling with the same error
    on Azure ML Studio. Thanks

    '
  created_at: 2023-10-13 09:14:55+00:00
  edited: false
  hidden: false
  id: 6529189f4d1ab06b9d885081
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-10-13T20:31:46.000Z'
    data:
      edited: true
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9394963383674622
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: '<p>No, not yet. Looking to get some update from the developers.</p>

          '
        raw: No, not yet. Looking to get some update from the developers.
        updatedAt: '2023-10-13T20:32:08.518Z'
      numEdits: 1
      reactions: []
    id: 6529a9324e655e57a9fe857b
    type: comment
  author: codegood
  content: No, not yet. Looking to get some update from the developers.
  created_at: 2023-10-13 19:31:46+00:00
  edited: true
  hidden: false
  id: 6529a9324e655e57a9fe857b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b5808961105fdf3e2a75b8/jsMKTkf8ek5WAlTvlLuDt.jpeg?w=200&h=200&f=face
      fullname: Seyed Saeid Masoumzadeh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: omidai
      type: user
    createdAt: '2023-10-14T19:58:17.000Z'
    data:
      edited: false
      editors:
      - omidai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.864965558052063
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b5808961105fdf3e2a75b8/jsMKTkf8ek5WAlTvlLuDt.jpeg?w=200&h=200&f=face
          fullname: Seyed Saeid Masoumzadeh
          isHf: false
          isPro: false
          name: omidai
          type: user
        html: '<p>in your bnb_config remove  load_in_8bit=true. or whatever bit you
          are trying to use. </p>

          '
        raw: 'in your bnb_config remove  load_in_8bit=true. or whatever bit you are
          trying to use. '
        updatedAt: '2023-10-14T19:58:17.320Z'
      numEdits: 0
      reactions: []
    id: 652af2d930355beba69843e4
    type: comment
  author: omidai
  content: 'in your bnb_config remove  load_in_8bit=true. or whatever bit you are
    trying to use. '
  created_at: 2023-10-14 18:58:17+00:00
  edited: false
  hidden: false
  id: 652af2d930355beba69843e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-10-16T01:08:22.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9131954908370972
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;omidai&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/omidai\">@<span class=\"\
          underline\">omidai</span></a></span>\n\n\t</span></span> then the model\
          \ will be too big to load. That's the power of BnB. </p>\n"
        raw: "@omidai then the model will be too big to load. That's the power of\
          \ BnB. \n"
        updatedAt: '2023-10-16T01:08:22.307Z'
      numEdits: 0
      reactions: []
    id: 652c8d0666313ebb61c2008e
    type: comment
  author: codegood
  content: "@omidai then the model will be too big to load. That's the power of BnB.\
    \ \n"
  created_at: 2023-10-16 00:08:22+00:00
  edited: false
  hidden: false
  id: 652c8d0666313ebb61c2008e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b5808961105fdf3e2a75b8/jsMKTkf8ek5WAlTvlLuDt.jpeg?w=200&h=200&f=face
      fullname: Seyed Saeid Masoumzadeh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: omidai
      type: user
    createdAt: '2023-10-16T07:28:41.000Z'
    data:
      edited: false
      editors:
      - omidai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9888684749603271
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b5808961105fdf3e2a75b8/jsMKTkf8ek5WAlTvlLuDt.jpeg?w=200&h=200&f=face
          fullname: Seyed Saeid Masoumzadeh
          isHf: false
          isPro: false
          name: omidai
          type: user
        html: '<p>correct, in my case, I just started with a fresh environment and
          installed all packages again then It was resolved. It seems there is a hidden
          package dependency. </p>

          '
        raw: 'correct, in my case, I just started with a fresh environment and installed
          all packages again then It was resolved. It seems there is a hidden package
          dependency. '
        updatedAt: '2023-10-16T07:28:41.920Z'
      numEdits: 0
      reactions: []
    id: 652ce6294267f8c8028c8f1f
    type: comment
  author: omidai
  content: 'correct, in my case, I just started with a fresh environment and installed
    all packages again then It was resolved. It seems there is a hidden package dependency. '
  created_at: 2023-10-16 06:28:41+00:00
  edited: false
  hidden: false
  id: 652ce6294267f8c8028c8f1f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd006801e37e0139d955efbb9ca609a2.svg
      fullname: Kha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AyouKha
      type: user
    createdAt: '2023-10-16T08:11:27.000Z'
    data:
      edited: true
      editors:
      - AyouKha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9851800203323364
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd006801e37e0139d955efbb9ca609a2.svg
          fullname: Kha
          isHf: false
          isPro: false
          name: AyouKha
          type: user
        html: '<p>@omida I''ve attempted this a few times using the latest packages,
          but it still doesn''t seem to work for me. Could you please share the packages
          versions you''re using?</p>

          '
        raw: '@omida I''ve attempted this a few times using the latest packages, but
          it still doesn''t seem to work for me. Could you please share the packages
          versions you''re using?'
        updatedAt: '2023-10-16T08:12:17.886Z'
      numEdits: 1
      reactions: []
    id: 652cf02f4267f8c8028de361
    type: comment
  author: AyouKha
  content: '@omida I''ve attempted this a few times using the latest packages, but
    it still doesn''t seem to work for me. Could you please share the packages versions
    you''re using?'
  created_at: 2023-10-16 07:11:27+00:00
  edited: true
  hidden: false
  id: 652cf02f4267f8c8028de361
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-16T18:12:26.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: tr
        probability: 0.10496710985898972
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>cc <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span></p>\n"
        raw: cc @ybelkada
        updatedAt: '2023-10-16T18:12:26.787Z'
      numEdits: 0
      reactions: []
    id: 652d7d0a656cca7ce9e71727
    type: comment
  author: lysandre
  content: cc @ybelkada
  created_at: 2023-10-16 17:12:26+00:00
  edited: false
  hidden: false
  id: 652d7d0a656cca7ce9e71727
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-16T19:32:35.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.844767689704895
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>hi everyone,<br>To benefit from quantization <code>load_in_8bit</code>
          or <code>load_in_4bit</code> , please make sure to install the latest bitsandbytes
          package:</p>

          <pre><code class="language-bash">pip install -U bitsandbytes

          </code></pre>

          <p>I would also advise to use <code>load_in_4bit</code> for a more memory
          efficient inference. Let us know how that goes</p>

          '
        raw: "hi everyone, \nTo benefit from quantization `load_in_8bit` or `load_in_4bit`\
          \ , please make sure to install the latest bitsandbytes package:\n\n```bash\n\
          pip install -U bitsandbytes\n```\n\nI would also advise to use `load_in_4bit`\
          \ for a more memory efficient inference. Let us know how that goes"
        updatedAt: '2023-10-16T19:32:35.919Z'
      numEdits: 0
      reactions: []
    id: 652d8fd3656cca7ce9e99304
    type: comment
  author: ybelkada
  content: "hi everyone, \nTo benefit from quantization `load_in_8bit` or `load_in_4bit`\
    \ , please make sure to install the latest bitsandbytes package:\n\n```bash\n\
    pip install -U bitsandbytes\n```\n\nI would also advise to use `load_in_4bit`\
    \ for a more memory efficient inference. Let us know how that goes"
  created_at: 2023-10-16 18:32:35+00:00
  edited: false
  hidden: false
  id: 652d8fd3656cca7ce9e99304
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bd006801e37e0139d955efbb9ca609a2.svg
      fullname: Kha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AyouKha
      type: user
    createdAt: '2023-10-17T16:08:44.000Z'
    data:
      edited: false
      editors:
      - AyouKha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9474639296531677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bd006801e37e0139d955efbb9ca609a2.svg
          fullname: Kha
          isHf: false
          isPro: false
          name: AyouKha
          type: user
        html: '<p>I''ve attempted today again to install the latest package versions
          (accelerate==0.23.0 and bitsandbytes==0.41.1), but it still doesn''t seem
          to be working, neither in 4-bit nor in 8-bit.</p>

          '
        raw: I've attempted today again to install the latest package versions (accelerate==0.23.0
          and bitsandbytes==0.41.1), but it still doesn't seem to be working, neither
          in 4-bit nor in 8-bit.
        updatedAt: '2023-10-17T16:08:44.163Z'
      numEdits: 0
      reactions: []
    id: 652eb18c5d91b1c2956e82df
    type: comment
  author: AyouKha
  content: I've attempted today again to install the latest package versions (accelerate==0.23.0
    and bitsandbytes==0.41.1), but it still doesn't seem to be working, neither in
    4-bit nor in 8-bit.
  created_at: 2023-10-17 15:08:44+00:00
  edited: false
  hidden: false
  id: 652eb18c5d91b1c2956e82df
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-18T01:28:22.000Z'
    data:
      edited: true
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7346010804176331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>Hi @Ayoba<br>I see OK - can you share a full reproducible snippet
          here?</p>

          '
        raw: "Hi @Ayoba \nI see OK - can you share a full reproducible snippet here?"
        updatedAt: '2023-10-18T01:28:35.528Z'
      numEdits: 1
      reactions: []
    id: 652f34b617096ceb6be4b8f5
    type: comment
  author: ybelkada
  content: "Hi @Ayoba \nI see OK - can you share a full reproducible snippet here?"
  created_at: 2023-10-18 00:28:22+00:00
  edited: true
  hidden: false
  id: 652f34b617096ceb6be4b8f5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/624d3f19571eef744f16ddf1/o4EoNpFI6czUT_Wspiz2h.jpeg?w=200&h=200&f=face
      fullname: Sukkrit Sharma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sukkritsharma
      type: user
    createdAt: '2023-10-18T10:21:51.000Z'
    data:
      edited: true
      editors:
      - sukkritsharma
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.407770574092865
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/624d3f19571eef744f16ddf1/o4EoNpFI6czUT_Wspiz2h.jpeg?w=200&h=200&f=face
          fullname: Sukkrit Sharma
          isHf: false
          isPro: false
          name: sukkritsharma
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;ybelkada&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ybelkada\"\
          >@<span class=\"underline\">ybelkada</span></a></span>\n\n\t</span></span>,\
          \ I'm also facing the same issue. Please help with the fix :</p>\n<p>Steps\
          \ to recreate: </p>\n<pre><code class=\"language-bash\">!pip install -U\
          \ bitsandbytes\n!pip install -U git+https://github.com/huggingface/transformers.git\n\
          !pip install -U git+https://github.com/huggingface/peft.git\n!pip install\
          \ -U git+https://github.com/huggingface/accelerate.git\n!pip install -U\
          \ datasets scipy ipywidgets matplotlib\n</code></pre>\n<pre><code class=\"\
          language-python\"><span class=\"hljs-keyword\">import</span> torch\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\
          \nbase_model_id = <span class=\"hljs-string\">\"mistralai/Mistral-7B-v0.1\"\
          </span>\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=<span class=\"\
          hljs-literal\">True</span>,\n    bnb_4bit_use_double_quant=<span class=\"\
          hljs-literal\">True</span>,\n    bnb_4bit_quant_type=<span class=\"hljs-string\"\
          >\"nf4\"</span>,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n\
          </code></pre>\n<p>ERROR : </p>\n<pre><code class=\"language-bash\">---------------------------------------------------------------------------\n\
          ImportError                               Traceback (most recent call last)\n\
          Cell In[1], line 12\n      4 base_model_id = <span class=\"hljs-string\"\
          >\"mistralai/Mistral-7B-v0.1\"</span>\n      5 bnb_config = BitsAndBytesConfig(\n\
          \      6     load_in_4bit=True,\n      7     bnb_4bit_use_double_quant=True,\n\
          \      8     bnb_4bit_quant_type=<span class=\"hljs-string\">\"nf4\"</span>,\n\
          \      9     bnb_4bit_compute_dtype=torch.bfloat16\n     10 )\n---&gt; 12\
          \ model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n\
          \nFile ~/transformers/src/transformers/models/auto/auto_factory.py:565,\
          \ <span class=\"hljs-keyword\">in</span> _BaseAutoModelClass.from_pretrained(cls,\
          \ pretrained_model_name_or_path, *model_args, **kwargs)\n    563 <span class=\"\
          hljs-keyword\">elif</span> <span class=\"hljs-built_in\">type</span>(config)\
          \ <span class=\"hljs-keyword\">in</span> cls._model_mapping.keys():\n  \
          \  564     model_class = _get_model_class(config, cls._model_mapping)\n\
          --&gt; 565     <span class=\"hljs-built_in\">return</span> model_class.from_pretrained(\n\
          \    566         pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs, **kwargs\n    567     )\n    568 raise ValueError(\n   \
          \ 569     f<span class=\"hljs-string\">\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"</span>\n\
          \    570     f<span class=\"hljs-string\">\"Model type should be one of\
          \ {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"</span>\n\
          \    571 )\n\nFile ~/transformers/src/transformers/modeling_utils.py:2634,\
          \ <span class=\"hljs-keyword\">in</span> PreTrainedModel.from_pretrained(cls,\
          \ pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes,\
          \ force_download, local_files_only, token, revision, use_safetensors, *model_args,\
          \ **kwargs)\n   2632 <span class=\"hljs-keyword\">if</span> load_in_8bit\
          \ or load_in_4bit:\n   2633     <span class=\"hljs-keyword\">if</span> not\
          \ (is_accelerate_available() and is_bitsandbytes_available()):\n-&gt; 2634\
          \         raise ImportError(\n   2635             <span class=\"hljs-string\"\
          >\"Using `load_in_8bit=True` requires Accelerate: `pip install accelerate`\
          \ and the latest version of\"</span>\n   2636             <span class=\"\
          hljs-string\">\" bitsandbytes `pip install -i https://test.pypi.org/simple/\
          \ bitsandbytes` or\"</span>\n   2637             <span class=\"hljs-string\"\
          >\" pip install bitsandbytes` \"</span>\n   2638         )\n   2640    \
          \ <span class=\"hljs-keyword\">if</span> torch_dtype is None:\n   2641 \
          \        <span class=\"hljs-comment\"># We force the `dtype` to be float16,\
          \ this is a requirement from `bitsandbytes`</span>\n   2642         logger.info(\n\
          \   2643             f<span class=\"hljs-string\">\"Overriding torch_dtype={torch_dtype}\
          \ with `torch_dtype=torch.float16` due to \"</span>\n   2644           \
          \  <span class=\"hljs-string\">\"requirements of `bitsandbytes` to enable\
          \ model loading in 8-bit or 4-bit. \"</span>\n   2645             <span\
          \ class=\"hljs-string\">\"Pass your own torch_dtype to specify the dtype\
          \ of the remaining non-linear layers or pass\"</span>\n   2646         \
          \    <span class=\"hljs-string\">\" torch_dtype=torch.float16 to remove\
          \ this warning.\"</span>\n   2647         )\n\nImportError: Using `load_in_8bit=True`\
          \ requires Accelerate: `pip install accelerate` and the latest version of\
          \ bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes`\
          \ or pip install bitsandbytes` \n</code></pre>\n"
        raw: "Hi @ybelkada, I'm also facing the same issue. Please help with the fix\
          \ :\n\nSteps to recreate: \n\n```bash\n!pip install -U bitsandbytes\n!pip\
          \ install -U git+https://github.com/huggingface/transformers.git\n!pip install\
          \ -U git+https://github.com/huggingface/peft.git\n!pip install -U git+https://github.com/huggingface/accelerate.git\n\
          !pip install -U datasets scipy ipywidgets matplotlib\n```\n```python\nimport\
          \ torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\
          \nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id,\
          \ quantization_config=bnb_config)\n```\n\nERROR : \n```bash\n---------------------------------------------------------------------------\n\
          ImportError                               Traceback (most recent call last)\n\
          Cell In[1], line 12\n      4 base_model_id = \"mistralai/Mistral-7B-v0.1\"\
          \n      5 bnb_config = BitsAndBytesConfig(\n      6     load_in_4bit=True,\n\
          \      7     bnb_4bit_use_double_quant=True,\n      8     bnb_4bit_quant_type=\"\
          nf4\",\n      9     bnb_4bit_compute_dtype=torch.bfloat16\n     10 )\n--->\
          \ 12 model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n\
          \nFile ~/transformers/src/transformers/models/auto/auto_factory.py:565,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    563 elif type(config) in cls._model_mapping.keys():\n\
          \    564     model_class = _get_model_class(config, cls._model_mapping)\n\
          --> 565     return model_class.from_pretrained(\n    566         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    567     )\n  \
          \  568 raise ValueError(\n    569     f\"Unrecognized configuration class\
          \ {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n\
          \    570     f\"Model type should be one of {', '.join(c.__name__ for c\
          \ in cls._model_mapping.keys())}.\"\n    571 )\n\nFile ~/transformers/src/transformers/modeling_utils.py:2634,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\n   2632 if load_in_8bit\
          \ or load_in_4bit:\n   2633     if not (is_accelerate_available() and is_bitsandbytes_available()):\n\
          -> 2634         raise ImportError(\n   2635             \"Using `load_in_8bit=True`\
          \ requires Accelerate: `pip install accelerate` and the latest version of\"\
          \n   2636             \" bitsandbytes `pip install -i https://test.pypi.org/simple/\
          \ bitsandbytes` or\"\n   2637             \" pip install bitsandbytes` \"\
          \n   2638         )\n   2640     if torch_dtype is None:\n   2641      \
          \   # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n\
          \   2642         logger.info(\n   2643             f\"Overriding torch_dtype={torch_dtype}\
          \ with `torch_dtype=torch.float16` due to \"\n   2644             \"requirements\
          \ of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n   2645\
          \             \"Pass your own torch_dtype to specify the dtype of the remaining\
          \ non-linear layers or pass\"\n   2646             \" torch_dtype=torch.float16\
          \ to remove this warning.\"\n   2647         )\n\nImportError: Using `load_in_8bit=True`\
          \ requires Accelerate: `pip install accelerate` and the latest version of\
          \ bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes`\
          \ or pip install bitsandbytes` \n```"
        updatedAt: '2023-10-18T10:22:38.917Z'
      numEdits: 1
      reactions: []
    id: 652fb1bfc22d404ebfad6ca9
    type: comment
  author: sukkritsharma
  content: "Hi @ybelkada, I'm also facing the same issue. Please help with the fix\
    \ :\n\nSteps to recreate: \n\n```bash\n!pip install -U bitsandbytes\n!pip install\
    \ -U git+https://github.com/huggingface/transformers.git\n!pip install -U git+https://github.com/huggingface/peft.git\n\
    !pip install -U git+https://github.com/huggingface/accelerate.git\n!pip install\
    \ -U datasets scipy ipywidgets matplotlib\n```\n```python\nimport torch\nfrom\
    \ transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\
    \nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\nbnb_config = BitsAndBytesConfig(\n\
    \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
    nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id,\
    \ quantization_config=bnb_config)\n```\n\nERROR : \n```bash\n---------------------------------------------------------------------------\n\
    ImportError                               Traceback (most recent call last)\n\
    Cell In[1], line 12\n      4 base_model_id = \"mistralai/Mistral-7B-v0.1\"\n \
    \     5 bnb_config = BitsAndBytesConfig(\n      6     load_in_4bit=True,\n   \
    \   7     bnb_4bit_use_double_quant=True,\n      8     bnb_4bit_quant_type=\"\
    nf4\",\n      9     bnb_4bit_compute_dtype=torch.bfloat16\n     10 )\n---> 12\
    \ model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n\
    \nFile ~/transformers/src/transformers/models/auto/auto_factory.py:565, in _BaseAutoModelClass.from_pretrained(cls,\
    \ pretrained_model_name_or_path, *model_args, **kwargs)\n    563 elif type(config)\
    \ in cls._model_mapping.keys():\n    564     model_class = _get_model_class(config,\
    \ cls._model_mapping)\n--> 565     return model_class.from_pretrained(\n    566\
    \         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\n    567     )\n    568 raise ValueError(\n    569     f\"Unrecognized\
    \ configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\\
    n\"\n    570     f\"Model type should be one of {', '.join(c.__name__ for c in\
    \ cls._model_mapping.keys())}.\"\n    571 )\n\nFile ~/transformers/src/transformers/modeling_utils.py:2634,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\n   2632 if load_in_8bit or\
    \ load_in_4bit:\n   2633     if not (is_accelerate_available() and is_bitsandbytes_available()):\n\
    -> 2634         raise ImportError(\n   2635             \"Using `load_in_8bit=True`\
    \ requires Accelerate: `pip install accelerate` and the latest version of\"\n\
    \   2636             \" bitsandbytes `pip install -i https://test.pypi.org/simple/\
    \ bitsandbytes` or\"\n   2637             \" pip install bitsandbytes` \"\n  \
    \ 2638         )\n   2640     if torch_dtype is None:\n   2641         # We force\
    \ the `dtype` to be float16, this is a requirement from `bitsandbytes`\n   2642\
    \         logger.info(\n   2643             f\"Overriding torch_dtype={torch_dtype}\
    \ with `torch_dtype=torch.float16` due to \"\n   2644             \"requirements\
    \ of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n   2645   \
    \          \"Pass your own torch_dtype to specify the dtype of the remaining non-linear\
    \ layers or pass\"\n   2646             \" torch_dtype=torch.float16 to remove\
    \ this warning.\"\n   2647         )\n\nImportError: Using `load_in_8bit=True`\
    \ requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes\
    \ `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`\
    \ \n```"
  created_at: 2023-10-18 09:21:51+00:00
  edited: true
  hidden: false
  id: 652fb1bfc22d404ebfad6ca9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-10-18T18:09:35.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.47849950194358826
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;sukkritsharma&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sukkritsharma\"\
          >@<span class=\"underline\">sukkritsharma</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I just ran :</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoTokenizer,\
          \ AutoModelForCausalLM, BitsAndBytesConfig\n\nbase_model_id = <span class=\"\
          hljs-string\">\"mistralai/Mistral-7B-v0.1\"</span>\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=<span class=\"hljs-literal\">True</span>,\n    bnb_4bit_use_double_quant=<span\
          \ class=\"hljs-literal\">True</span>,\n    bnb_4bit_quant_type=<span class=\"\
          hljs-string\">\"nf4\"</span>,\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
          )\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n\
          tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n\ntext = <span\
          \ class=\"hljs-string\">\"Hello my name is\"</span>\ninput_ids = tokenizer.encode(text,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(<span class=\"\
          hljs-number\">0</span>)\n\noutput = model.generate(input_ids, do_sample=<span\
          \ class=\"hljs-literal\">False</span>, max_length=<span class=\"hljs-number\"\
          >10</span>)\n<span class=\"hljs-built_in\">print</span>(tokenizer.decode(output[<span\
          \ class=\"hljs-number\">0</span>], skip_special_tokens=<span class=\"hljs-literal\"\
          >True</span>))\n</code></pre>\n<p>On a fresh new environment and it seemed\
          \ to work fine on my end. Can you re-try everything on a fresh new environment?</p>\n"
        raw: "Hi @sukkritsharma \n\nI just ran :\n\n```python\nimport torch\nfrom\
          \ transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\
          \nbase_model_id = \"mistralai/Mistral-7B-v0.1\"\nbnb_config = BitsAndBytesConfig(\n\
          \    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"\
          nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id,\
          \ quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(base_model_id)\n\
          \ntext = \"Hello my name is\"\ninput_ids = tokenizer.encode(text, return_tensors=\"\
          pt\").to(0)\n\noutput = model.generate(input_ids, do_sample=False, max_length=10)\n\
          print(tokenizer.decode(output[0], skip_special_tokens=True))\n```\nOn a\
          \ fresh new environment and it seemed to work fine on my end. Can you re-try\
          \ everything on a fresh new environment?"
        updatedAt: '2023-10-18T18:09:35.716Z'
      numEdits: 0
      reactions: []
    id: 65301f5f3976e5f44112dbd7
    type: comment
  author: ybelkada
  content: "Hi @sukkritsharma \n\nI just ran :\n\n```python\nimport torch\nfrom transformers\
    \ import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbase_model_id\
    \ = \"mistralai/Mistral-7B-v0.1\"\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n\
    \    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
    )\n\nmodel = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n\
    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n\ntext = \"Hello my\
    \ name is\"\ninput_ids = tokenizer.encode(text, return_tensors=\"pt\").to(0)\n\
    \noutput = model.generate(input_ids, do_sample=False, max_length=10)\nprint(tokenizer.decode(output[0],\
    \ skip_special_tokens=True))\n```\nOn a fresh new environment and it seemed to\
    \ work fine on my end. Can you re-try everything on a fresh new environment?"
  created_at: 2023-10-18 17:09:35+00:00
  edited: false
  hidden: false
  id: 65301f5f3976e5f44112dbd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/301dc0b03df3a8fbd6907da1d93be61c.svg
      fullname: InkOgnito
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ink
      type: user
    createdAt: '2023-11-24T20:00:03.000Z'
    data:
      edited: false
      editors:
      - Ink
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8409925103187561
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/301dc0b03df3a8fbd6907da1d93be61c.svg
          fullname: InkOgnito
          isHf: false
          isPro: false
          name: Ink
          type: user
        html: '<p>On Windows, I had the same problem. If you try:</p>

          <pre><code>torch.cuda.is_available()

          </code></pre>

          <p> it would show as False because the cuda version it needs was different
          from the cuda version that pytorch uses.</p>

          <p>Hence, I found this solution from here: <a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues/793#issuecomment-1806979077">https://github.com/TimDettmers/bitsandbytes/issues/793#issuecomment-1806979077</a></p>

          <p>Instead of installing bitsandbytes from the latest pip package, install
          it using the following command:</p>

          <pre><code>python.exe -m pip install https://github.com/TimDettmers/bitsandbytes/releases/download/0.41.0/bitsandbytes-0.41.0-py3-none-any.whl

          </code></pre>

          <p>Hope this helps</p>

          '
        raw: "On Windows, I had the same problem. If you try:\n\n```\ntorch.cuda.is_available()\n\
          ```\n\n it would show as False because the cuda version it needs was different\
          \ from the cuda version that pytorch uses.\n\nHence, I found this solution\
          \ from here: https://github.com/TimDettmers/bitsandbytes/issues/793#issuecomment-1806979077\n\
          \ \nInstead of installing bitsandbytes from the latest pip package, install\
          \ it using the following command:\n\n```\npython.exe -m pip install https://github.com/TimDettmers/bitsandbytes/releases/download/0.41.0/bitsandbytes-0.41.0-py3-none-any.whl\n\
          ```\n\nHope this helps\n"
        updatedAt: '2023-11-24T20:00:03.825Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - sukkritsharma
    id: 656100c3771319d93b906fea
    type: comment
  author: Ink
  content: "On Windows, I had the same problem. If you try:\n\n```\ntorch.cuda.is_available()\n\
    ```\n\n it would show as False because the cuda version it needs was different\
    \ from the cuda version that pytorch uses.\n\nHence, I found this solution from\
    \ here: https://github.com/TimDettmers/bitsandbytes/issues/793#issuecomment-1806979077\n\
    \ \nInstead of installing bitsandbytes from the latest pip package, install it\
    \ using the following command:\n\n```\npython.exe -m pip install https://github.com/TimDettmers/bitsandbytes/releases/download/0.41.0/bitsandbytes-0.41.0-py3-none-any.whl\n\
    ```\n\nHope this helps\n"
  created_at: 2023-11-24 20:00:03+00:00
  edited: false
  hidden: false
  id: 656100c3771319d93b906fea
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 44
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Install latest BitsandBytes
