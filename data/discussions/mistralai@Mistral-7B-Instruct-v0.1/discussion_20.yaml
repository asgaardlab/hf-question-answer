!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tarruda
conflicting_files: null
created_at: 2023-09-28 21:40:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-09-28T22:40:34.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4071900546550751
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: "<p>This model runs very well on my laptop's rtx3070 (8gb, running in\
          \ 4-bit)</p>\n<p>Here's a script that will spawn a local chat web ui to\
          \ try mistral 7b (based on <a href=\"https://huggingface.co/spaces/Sentdex/StableBeluga-7B-Chat/blob/main/app.py\"\
          >https://huggingface.co/spaces/Sentdex/StableBeluga-7B-Chat/blob/main/app.py</a>).\
          \ </p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> gradio <span class=\"hljs-keyword\">as</span> gr\n<span class=\"\
          hljs-keyword\">import</span> transformers\n<span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> threading\
          \ <span class=\"hljs-keyword\">import</span> Thread\n<span class=\"hljs-keyword\"\
          >from</span> gradio.themes.utils.colors <span class=\"hljs-keyword\">import</span>\
          \ Color\n\n\nmodel_id = <span class=\"hljs-string\">\"mistralai/Mistral-7B-Instruct-v0.1\"\
          </span>\n\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=<span\
          \ class=\"hljs-literal\">True</span>,\n    bnb_4bit_use_double_quant=<span\
          \ class=\"hljs-literal\">True</span>,\n    bnb_4bit_quant_type=<span class=\"\
          hljs-string\">\"nf4\"</span>,\n    bnb_4bit_compute_dtype=torch.bfloat16\n\
          )\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n\
          \    trust_remote_code=<span class=\"hljs-literal\">True</span>,\n    quantization_config=bnb_config,\n\
          \    device_map=<span class=\"hljs-string\">'auto'</span>,\n)\n\ntokenizer\
          \ = transformers.AutoTokenizer.from_pretrained(\n    model_id,\n)\n\ntext_color\
          \ = <span class=\"hljs-string\">\"#FFFFFF\"</span>\napp_background = <span\
          \ class=\"hljs-string\">\"#0A0A0A\"</span>\nuser_inputs_background = <span\
          \ class=\"hljs-string\">\"#193C4C\"</span><span class=\"hljs-comment\">#14303D\"\
          #\"#091820\"</span>\nwidget_bg = <span class=\"hljs-string\">\"#000100\"\
          </span>\nbutton_bg = <span class=\"hljs-string\">\"#141414\"</span>\n\n\
          dark = Color(\n    name=<span class=\"hljs-string\">\"dark\"</span>,\n \
          \   c50=<span class=\"hljs-string\">\"#F4F3EE\"</span>,  <span class=\"\
          hljs-comment\"># not sure</span>\n    <span class=\"hljs-comment\"># all\
          \ text color:</span>\n    c100=text_color, <span class=\"hljs-comment\"\
          ># Title color, input text color, and all chat text color.</span>\n    c200=text_color,\
          \ <span class=\"hljs-comment\"># Widget name colors (system prompt and \"\
          chatbot\")</span>\n    c300=<span class=\"hljs-string\">\"#F4F3EE\"</span>,\
          \ <span class=\"hljs-comment\"># not sure</span>\n    c400=<span class=\"\
          hljs-string\">\"#F4F3EE\"</span>, <span class=\"hljs-comment\"># Possibly\
          \ gradio link color. Maybe other unlicked link colors.</span>\n    <span\
          \ class=\"hljs-comment\"># suggestion text color...</span>\n    c500=text_color,\
          \ <span class=\"hljs-comment\"># text suggestion text. Maybe other stuff.</span>\n\
          \    c600=button_bg,<span class=\"hljs-comment\">#\"#444444\", # button\
          \ background color, also outline of user msg.</span>\n    <span class=\"\
          hljs-comment\"># user msg/inputs color:</span>\n    c700=user_inputs_background,\
          \ <span class=\"hljs-comment\"># text input background AND user message\
          \ color. And bot reply outline.</span>\n    <span class=\"hljs-comment\"\
          ># widget bg.</span>\n    c800=widget_bg, <span class=\"hljs-comment\">#\
          \ widget background (like, block background. Not whole bg), and bot-reply\
          \ background.</span>\n    c900=app_background, <span class=\"hljs-comment\"\
          ># app/jpage background. (v light blue)</span>\n    c950=<span class=\"\
          hljs-string\">\"#F4F3EE\"</span>, <span class=\"hljs-comment\"># not sure\
          \ atm.</span>\n)\n\nDESCRIPTION = <span class=\"hljs-string\">\"\"\"</span>\n\
          <span class=\"hljs-string\"># Mistral 7B Instruct Chat \U0001F5E8\uFE0F\
          </span>\n<span class=\"hljs-string\">This is a streaming Chat Interface\
          \ implementation of [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)</span>\n\
          <span class=\"hljs-string\">\"\"\"</span>\n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">chat</span>(<span class=\"\
          hljs-params\">user_input, history</span>):\n    messages = []\n    <span\
          \ class=\"hljs-keyword\">for</span> pair <span class=\"hljs-keyword\">in</span>\
          \ history:\n        messages.append({<span class=\"hljs-string\">'role'</span>:\
          \ <span class=\"hljs-string\">'user'</span>, <span class=\"hljs-string\"\
          >'content'</span>: pair[<span class=\"hljs-number\">0</span>]})\n      \
          \  messages.append({<span class=\"hljs-string\">'role'</span>: <span class=\"\
          hljs-string\">'assistant'</span>, <span class=\"hljs-string\">'content'</span>:\
          \ pair[<span class=\"hljs-number\">1</span>]})\n    messages.append({<span\
          \ class=\"hljs-string\">'role'</span>: <span class=\"hljs-string\">'user'</span>,\
          \ <span class=\"hljs-string\">'content'</span>: user_input})\n\n    encodeds\
          \ = tokenizer.apply_chat_template(messages, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>)\n    device = <span class=\"hljs-string\">'cuda'</span>\n\
          \    model_inputs = {<span class=\"hljs-string\">'input_ids'</span>: encodeds.to(device)}\n\
          \n    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=<span\
          \ class=\"hljs-number\">10.</span>, skip_prompt=<span class=\"hljs-literal\"\
          >True</span>, skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\
          \n    generate_kwargs = <span class=\"hljs-built_in\">dict</span>(\n   \
          \     model_inputs,\n        streamer=streamer,\n        max_new_tokens=<span\
          \ class=\"hljs-number\">2000</span>,\n        do_sample=<span class=\"hljs-literal\"\
          >True</span>,\n        <span class=\"hljs-comment\">#top_p=0.95,</span>\n\
          \        <span class=\"hljs-comment\">#temperature=0.8,</span>\n       \
          \ <span class=\"hljs-comment\">#top_k=50</span>\n    )\n    t = Thread(target=model.generate,\
          \ kwargs=generate_kwargs)\n    t.start()\n\n    model_output = <span class=\"\
          hljs-string\">\"\"</span>\n    <span class=\"hljs-keyword\">for</span> new_text\
          \ <span class=\"hljs-keyword\">in</span> streamer:\n        model_output\
          \ += new_text\n        <span class=\"hljs-keyword\">yield</span> model_output\n\
          \    <span class=\"hljs-keyword\">return</span> model_output\n\n\n<span\
          \ class=\"hljs-keyword\">with</span> gr.Blocks(theme=gr.themes.Monochrome(\n\
          \               font=[gr.themes.GoogleFont(<span class=\"hljs-string\">\"\
          Montserrat\"</span>), <span class=\"hljs-string\">\"Arial\"</span>, <span\
          \ class=\"hljs-string\">\"sans-serif\"</span>],\n               primary_hue=<span\
          \ class=\"hljs-string\">\"sky\"</span>,  <span class=\"hljs-comment\">#\
          \ when loading</span>\n               secondary_hue=<span class=\"hljs-string\"\
          >\"sky\"</span>, <span class=\"hljs-comment\"># something with links</span>\n\
          \               neutral_hue=<span class=\"hljs-string\">\"dark\"</span>),)\
          \ <span class=\"hljs-keyword\">as</span> demo:  <span class=\"hljs-comment\"\
          >#main.</span>\n\n    gr.Markdown(DESCRIPTION)\n    chatbot = gr.ChatInterface(fn=chat)\n\
          \ndemo.queue(api_open=<span class=\"hljs-literal\">False</span>).launch(server_name=<span\
          \ class=\"hljs-string\">'0.0.0.0'</span>,show_api=<span class=\"hljs-literal\"\
          >False</span>,share=<span class=\"hljs-literal\">False</span>)\n</code></pre>\n"
        raw: "This model runs very well on my laptop's rtx3070 (8gb, running in 4-bit)\r\
          \n\r\nHere's a script that will spawn a local chat web ui to try mistral\
          \ 7b (based on https://huggingface.co/spaces/Sentdex/StableBeluga-7B-Chat/blob/main/app.py).\
          \ \r\n\r\n```python\r\nimport gradio as gr\r\nimport transformers\r\nimport\
          \ torch\r\nfrom threading import Thread\r\nfrom gradio.themes.utils.colors\
          \ import Color\r\n\r\n\r\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\
          \r\n\r\nbnb_config = transformers.BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\
          \n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type=\"nf4\"\
          ,\r\n    bnb_4bit_compute_dtype=torch.bfloat16\r\n)\r\n\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n    model_id,\r\n    trust_remote_code=True,\r\n    quantization_config=bnb_config,\r\
          \n    device_map='auto',\r\n)\r\n\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(\r\
          \n    model_id,\r\n)\r\n\r\ntext_color = \"#FFFFFF\"\r\napp_background =\
          \ \"#0A0A0A\"\r\nuser_inputs_background = \"#193C4C\"#14303D\"#\"#091820\"\
          \r\nwidget_bg = \"#000100\"\r\nbutton_bg = \"#141414\"\r\n\r\ndark = Color(\r\
          \n    name=\"dark\",\r\n    c50=\"#F4F3EE\",  # not sure\r\n    # all text\
          \ color:\r\n    c100=text_color, # Title color, input text color, and all\
          \ chat text color.\r\n    c200=text_color, # Widget name colors (system\
          \ prompt and \"chatbot\")\r\n    c300=\"#F4F3EE\", # not sure\r\n    c400=\"\
          #F4F3EE\", # Possibly gradio link color. Maybe other unlicked link colors.\r\
          \n    # suggestion text color...\r\n    c500=text_color, # text suggestion\
          \ text. Maybe other stuff.\r\n    c600=button_bg,#\"#444444\", # button\
          \ background color, also outline of user msg.\r\n    # user msg/inputs color:\r\
          \n    c700=user_inputs_background, # text input background AND user message\
          \ color. And bot reply outline.\r\n    # widget bg.\r\n    c800=widget_bg,\
          \ # widget background (like, block background. Not whole bg), and bot-reply\
          \ background.\r\n    c900=app_background, # app/jpage background. (v light\
          \ blue)\r\n    c950=\"#F4F3EE\", # not sure atm.\r\n)\r\n\r\nDESCRIPTION\
          \ = \"\"\"\r\n# Mistral 7B Instruct Chat \U0001F5E8\uFE0F\r\nThis is a streaming\
          \ Chat Interface implementation of [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\r\
          \n\"\"\"\r\n\r\ndef chat(user_input, history):\r\n    messages = []\r\n\
          \    for pair in history:\r\n        messages.append({'role': 'user', 'content':\
          \ pair[0]})\r\n        messages.append({'role': 'assistant', 'content':\
          \ pair[1]})\r\n    messages.append({'role': 'user', 'content': user_input})\r\
          \n\r\n    encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"\
          pt\")\r\n    device = 'cuda'\r\n    model_inputs = {'input_ids': encodeds.to(device)}\r\
          \n\r\n    streamer = transformers.TextIteratorStreamer(tokenizer, timeout=10.,\
          \ skip_prompt=True, skip_special_tokens=True)\r\n\r\n    generate_kwargs\
          \ = dict(\r\n        model_inputs,\r\n        streamer=streamer,\r\n   \
          \     max_new_tokens=2000,\r\n        do_sample=True,\r\n        #top_p=0.95,\r\
          \n        #temperature=0.8,\r\n        #top_k=50\r\n    )\r\n    t = Thread(target=model.generate,\
          \ kwargs=generate_kwargs)\r\n    t.start()\r\n\r\n    model_output = \"\"\
          \r\n    for new_text in streamer:\r\n        model_output += new_text\r\n\
          \        yield model_output\r\n    return model_output\r\n\r\n\r\nwith gr.Blocks(theme=gr.themes.Monochrome(\r\
          \n               font=[gr.themes.GoogleFont(\"Montserrat\"), \"Arial\",\
          \ \"sans-serif\"],\r\n               primary_hue=\"sky\",  # when loading\r\
          \n               secondary_hue=\"sky\", # something with links\r\n     \
          \          neutral_hue=\"dark\"),) as demo:  #main.\r\n\r\n    gr.Markdown(DESCRIPTION)\r\
          \n    chatbot = gr.ChatInterface(fn=chat)\r\n\r\ndemo.queue(api_open=False).launch(server_name='0.0.0.0',show_api=False,share=False)\r\
          \n```"
        updatedAt: '2023-09-28T22:40:34.961Z'
      numEdits: 0
      reactions: []
    id: 651600e2c2aedaa7e69ae48e
    type: comment
  author: tarruda
  content: "This model runs very well on my laptop's rtx3070 (8gb, running in 4-bit)\r\
    \n\r\nHere's a script that will spawn a local chat web ui to try mistral 7b (based\
    \ on https://huggingface.co/spaces/Sentdex/StableBeluga-7B-Chat/blob/main/app.py).\
    \ \r\n\r\n```python\r\nimport gradio as gr\r\nimport transformers\r\nimport torch\r\
    \nfrom threading import Thread\r\nfrom gradio.themes.utils.colors import Color\r\
    \n\r\n\r\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\r\n\r\nbnb_config\
    \ = transformers.BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\
    \n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.bfloat16\r\
    \n)\r\n\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\n    model_id,\r\
    \n    trust_remote_code=True,\r\n    quantization_config=bnb_config,\r\n    device_map='auto',\r\
    \n)\r\n\r\ntokenizer = transformers.AutoTokenizer.from_pretrained(\r\n    model_id,\r\
    \n)\r\n\r\ntext_color = \"#FFFFFF\"\r\napp_background = \"#0A0A0A\"\r\nuser_inputs_background\
    \ = \"#193C4C\"#14303D\"#\"#091820\"\r\nwidget_bg = \"#000100\"\r\nbutton_bg =\
    \ \"#141414\"\r\n\r\ndark = Color(\r\n    name=\"dark\",\r\n    c50=\"#F4F3EE\"\
    ,  # not sure\r\n    # all text color:\r\n    c100=text_color, # Title color,\
    \ input text color, and all chat text color.\r\n    c200=text_color, # Widget\
    \ name colors (system prompt and \"chatbot\")\r\n    c300=\"#F4F3EE\", # not sure\r\
    \n    c400=\"#F4F3EE\", # Possibly gradio link color. Maybe other unlicked link\
    \ colors.\r\n    # suggestion text color...\r\n    c500=text_color, # text suggestion\
    \ text. Maybe other stuff.\r\n    c600=button_bg,#\"#444444\", # button background\
    \ color, also outline of user msg.\r\n    # user msg/inputs color:\r\n    c700=user_inputs_background,\
    \ # text input background AND user message color. And bot reply outline.\r\n \
    \   # widget bg.\r\n    c800=widget_bg, # widget background (like, block background.\
    \ Not whole bg), and bot-reply background.\r\n    c900=app_background, # app/jpage\
    \ background. (v light blue)\r\n    c950=\"#F4F3EE\", # not sure atm.\r\n)\r\n\
    \r\nDESCRIPTION = \"\"\"\r\n# Mistral 7B Instruct Chat \U0001F5E8\uFE0F\r\nThis\
    \ is a streaming Chat Interface implementation of [Mistral 7B Instruct](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\r\
    \n\"\"\"\r\n\r\ndef chat(user_input, history):\r\n    messages = []\r\n    for\
    \ pair in history:\r\n        messages.append({'role': 'user', 'content': pair[0]})\r\
    \n        messages.append({'role': 'assistant', 'content': pair[1]})\r\n    messages.append({'role':\
    \ 'user', 'content': user_input})\r\n\r\n    encodeds = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\r\n    device = 'cuda'\r\n    model_inputs = {'input_ids':\
    \ encodeds.to(device)}\r\n\r\n    streamer = transformers.TextIteratorStreamer(tokenizer,\
    \ timeout=10., skip_prompt=True, skip_special_tokens=True)\r\n\r\n    generate_kwargs\
    \ = dict(\r\n        model_inputs,\r\n        streamer=streamer,\r\n        max_new_tokens=2000,\r\
    \n        do_sample=True,\r\n        #top_p=0.95,\r\n        #temperature=0.8,\r\
    \n        #top_k=50\r\n    )\r\n    t = Thread(target=model.generate, kwargs=generate_kwargs)\r\
    \n    t.start()\r\n\r\n    model_output = \"\"\r\n    for new_text in streamer:\r\
    \n        model_output += new_text\r\n        yield model_output\r\n    return\
    \ model_output\r\n\r\n\r\nwith gr.Blocks(theme=gr.themes.Monochrome(\r\n     \
    \          font=[gr.themes.GoogleFont(\"Montserrat\"), \"Arial\", \"sans-serif\"\
    ],\r\n               primary_hue=\"sky\",  # when loading\r\n               secondary_hue=\"\
    sky\", # something with links\r\n               neutral_hue=\"dark\"),) as demo:\
    \  #main.\r\n\r\n    gr.Markdown(DESCRIPTION)\r\n    chatbot = gr.ChatInterface(fn=chat)\r\
    \n\r\ndemo.queue(api_open=False).launch(server_name='0.0.0.0',show_api=False,share=False)\r\
    \n```"
  created_at: 2023-09-28 21:40:34+00:00
  edited: false
  hidden: false
  id: 651600e2c2aedaa7e69ae48e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dc49eb623611174281230bef77db1f4a.svg
      fullname: "Ahmet Emin Ko\xE7al"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aeminkocal
      type: user
    createdAt: '2023-09-28T23:20:00.000Z'
    data:
      edited: false
      editors:
      - aeminkocal
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9557628035545349
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dc49eb623611174281230bef77db1f4a.svg
          fullname: "Ahmet Emin Ko\xE7al"
          isHf: false
          isPro: false
          name: aeminkocal
          type: user
        html: '<p>Can I set system prompt so that I can make it behave in a way that
          I want?</p>

          '
        raw: Can I set system prompt so that I can make it behave in a way that I
          want?
        updatedAt: '2023-09-28T23:20:00.630Z'
      numEdits: 0
      reactions: []
    id: 65160a20ca07b26143a46323
    type: comment
  author: aeminkocal
  content: Can I set system prompt so that I can make it behave in a way that I want?
  created_at: 2023-09-28 22:20:00+00:00
  edited: false
  hidden: false
  id: 65160a20ca07b26143a46323
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
      fullname: Thiago de Arruda Padilha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tarruda
      type: user
    createdAt: '2023-09-29T09:26:04.000Z'
    data:
      edited: false
      editors:
      - tarruda
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.983647346496582
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70a745746569b264a2ea4815dd04d3a7.svg
          fullname: Thiago de Arruda Padilha
          isHf: false
          isPro: false
          name: tarruda
          type: user
        html: '<p>I didn''t see an option to set system prompt in the model card</p>

          '
        raw: I didn't see an option to set system prompt in the model card
        updatedAt: '2023-09-29T09:26:04.948Z'
      numEdits: 0
      reactions: []
    id: 6516982c7f18cec973c3ff00
    type: comment
  author: tarruda
  content: I didn't see an option to set system prompt in the model card
  created_at: 2023-09-29 08:26:04+00:00
  edited: false
  hidden: false
  id: 6516982c7f18cec973c3ff00
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 20
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Sharing a script to run a local streaming chat interface
