!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jzarka
conflicting_files: null
created_at: 2024-01-13 23:38:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1474a6b5f699855f0436aeda60e5ed1.svg
      fullname: Jeremie
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jzarka
      type: user
    createdAt: '2024-01-13T23:38:33.000Z'
    data:
      edited: false
      editors:
      - jzarka
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4939146339893341
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1474a6b5f699855f0436aeda60e5ed1.svg
          fullname: Jeremie
          isHf: false
          isPro: false
          name: jzarka
          type: user
        html: "<p>Hi,</p>\n<p>I have the following error when running the model in\
          \ a docker<br>root@scw-busy-fermat:~# docker run --gpus all     -e HF_TOKEN=$HF_TOKEN\
          \ -p 8000:8000     ghcr.io/mistralai/mistral-src/vllm:latest     --host\
          \ 0.0.0.0     --model mistralai/Mistral-7B-Instruct-v0.2 --dtype half<br>The\
          \ HF_TOKEN environment variable set, logging to Hugging Face.<br>Token will\
          \ not been saved to git credential helper. Pass <code>add_to_git_credential=True</code>\
          \ if you want to set the git credential as well.<br>Token is valid (permission:\
          \ read).<br>Your token has been saved to /root/.cache/huggingface/token<br>Login\
          \ successful<br>INFO 01-13 23:23:35 api_server.py:719] args: Namespace(host='0.0.0.0',\
          \ port=8000, allow_credentials=False, allowed_origins=['<em>'], allowed_methods=['</em>'],\
          \ allowed_headers=['*'], served_model_name=None, chat_template=None, response_role='assistant',\
          \ model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer=None, revision=None,\
          \ tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False,\
          \ download_dir=None, load_format='auto', dtype='half', max_model_len=None,\
          \ worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1,\
          \ max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=4,\
          \ gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256,\
          \ max_paddings=256, disable_log_stats=False, quantization=None, engine_use_ray=False,\
          \ disable_log_requests=False, max_log_len=None)<br>config.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596/596 [00:00&lt;00:00,\
          \ 1.44MB/s]<br>WARNING 01-13 23:23:35 config.py:447] Casting torch.bfloat16\
          \ to torch.float16.<br>INFO 01-13 23:23:35 llm_engine.py:73] Initializing\
          \ an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2',\
          \ tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None,\
          \ tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16,\
          \ max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1,\
          \ quantization=None, seed=0)<br>tokenizer_config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.46k/1.46k [00:00&lt;00:00,\
          \ 3.89MB/s]<br>tokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 493k/493k [00:00&lt;00:00, 11.9MB/s]<br>tokenizer.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.80M/1.80M\
          \ [00:00&lt;00:00, 4.28MB/s]<br>special_tokens_map.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72.0/72.0 [00:00&lt;00:00,\
          \ 194kB/s]<br>model-00001-of-00003.safetensors: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 4.94G/4.94G [00:24&lt;00:00, 200MB/s]<br>model-00002-of-00003.safetensors:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G\
          \ [00:26&lt;00:00, 192MB/s]<br>model-00003-of-00003.safetensors: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54G/4.54G [00:29&lt;00:00,\
          \ 156MB/s]<br>model-00003-of-00003.safetensors:  88%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u258A | 4.00G/4.54G [00:25&lt;00:03, 162MB/s]<br>model-00003-of-00003.safetensors:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 4.54G/4.54G\
          \ [00:29&lt;00:00, 177MB/s]</p>\n<p>Traceback (most recent call last):<br>\
          \  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main<br>\
          \    return _run_code(code, main_globals, None,<br>  File \"/usr/lib/python3.10/runpy.py\"\
          , line 86, in _run_code<br>    exec(code, run_globals)<br>  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\"\
          , line 729, in <br>    engine = AsyncLLMEngine.from_engine_args(engine_args)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
          , line 495, in from_engine_args<br>    engine = cls(parallel_config.worker_use_ray,<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
          , line 269, in <strong>init</strong><br>    self.engine = self._init_engine(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
          , line 314, in _init_engine<br>    return engine_class(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 112, in <strong>init</strong><br>    self._init_cache()<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 208, in _init_cache<br>    num_blocks = self._run_workers(<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 750, in _run_workers<br>    self._run_workers_in_batch(workers, method,\
          \ *args, **kwargs))<br>  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 724, in _run_workers_in_batch<br>    output = executor(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\"\
          , line 86, in profile_num_available_blocks<br>    self.model_runner.profile_run()<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\"\
          , line 321, in profile_run<br>    self.execute_model(seqs, kv_caches)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context<br>    return func(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\"\
          , line 279, in execute_model<br>    hidden_states = self.model(<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 290, in forward<br>    hidden_states = self.model(input_ids, positions,\
          \ kv_caches,<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 256, in forward<br>    hidden_states, residual = layer(<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 206, in forward<br>    hidden_states = self.self_attn(<br>  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 150, in forward<br>    qkv, _ = self.qkv_proj(hidden_states)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl<br>    return self._call_impl(*args,\
          \ **kwargs)<br>  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl<br>    return forward_call(*args, **kwargs)<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\"\
          , line 203, in forward<br>    output_parallel = self.linear_method.apply_weights(<br>\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\"\
          , line 68, in apply_weights<br>    return F.linear(x, weight, bias)<br>RuntimeError:\
          \ CUDA error: no kernel image is available for execution on the device<br>CUDA\
          \ kernel errors might be asynchronously reported at some other API call,\
          \ so the stacktrace below might be incorrect.<br>For debugging consider\
          \ passing CUDA_LAUNCH_BLOCKING=1.<br>Compile with <code>TORCH_USE_CUDA_DSA</code>\
          \ to enable device-side assertions.</p>\n<p>Is this due to hardware specification\
          \ or other?</p>\n<p>Looking forward to reading you!</p>\n<p>Kind regards,<br>J\xE9\
          r\xE9mie</p>\n"
        raw: "Hi,\r\n\r\nI have the following error when running the model in a docker\r\
          \nroot@scw-busy-fermat:~# docker run --gpus all     -e HF_TOKEN=$HF_TOKEN\
          \ -p 8000:8000     ghcr.io/mistralai/mistral-src/vllm:latest     --host\
          \ 0.0.0.0     --model mistralai/Mistral-7B-Instruct-v0.2 --dtype half\r\n\
          The HF_TOKEN environment variable set, logging to Hugging Face.\r\nToken\
          \ will not been saved to git credential helper. Pass `add_to_git_credential=True`\
          \ if you want to set the git credential as well.\r\nToken is valid (permission:\
          \ read).\r\nYour token has been saved to /root/.cache/huggingface/token\r\
          \nLogin successful\r\nINFO 01-13 23:23:35 api_server.py:719] args: Namespace(host='0.0.0.0',\
          \ port=8000, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'],\
          \ allowed_headers=['*'], served_model_name=None, chat_template=None, response_role='assistant',\
          \ model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer=None, revision=None,\
          \ tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False,\
          \ download_dir=None, load_format='auto', dtype='half', max_model_len=None,\
          \ worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1,\
          \ max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=4,\
          \ gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256,\
          \ max_paddings=256, disable_log_stats=False, quantization=None, engine_use_ray=False,\
          \ disable_log_requests=False, max_log_len=None)\r\nconfig.json: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 596/596 [00:00<00:00,\
          \ 1.44MB/s]\r\nWARNING 01-13 23:23:35 config.py:447] Casting torch.bfloat16\
          \ to torch.float16.\r\nINFO 01-13 23:23:35 llm_engine.py:73] Initializing\
          \ an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2',\
          \ tokenizer='mistralai/Mistral-7B-Instruct-v0.2', tokenizer_mode=auto, revision=None,\
          \ tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16,\
          \ max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1,\
          \ quantization=None, seed=0)\r\ntokenizer_config.json: 100%|\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.46k/1.46k [00:00<00:00,\
          \ 3.89MB/s]\r\ntokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 493k/493k [00:00<00:00, 11.9MB/s]\r\ntokenizer.json:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.80M/1.80M\
          \ [00:00<00:00, 4.28MB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588| 72.0/72.0 [00:00<00:00, 194kB/s]\r\
          \nmodel-00001-of-00003.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588| 4.94G/4.94G [00:24<00:00, 200MB/s]\r\nmodel-00002-of-00003.safetensors:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G\
          \ [00:26<00:00, 192MB/s] \r\nmodel-00003-of-00003.safetensors: 100%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.54G/4.54G [00:29<00:00,\
          \ 156MB/s] \r\nmodel-00003-of-00003.safetensors:  88%|\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u258A | 4.00G/4.54G [00:25<00:03, 162MB/s]\r\
          \nmodel-00003-of-00003.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2589| 4.54G/4.54G [00:29<00:00, 177MB/s]\r\n\r\n\r\n\
          \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\"\
          , line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals,\
          \ None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\
          \n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\"\
          , line 729, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
          , line 495, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
          , line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
          , line 314, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 112, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 208, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 750, in _run_workers\r\n    self._run_workers_in_batch(workers, method,\
          \ *args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
          , line 724, in _run_workers_in_batch\r\n    output = executor(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\"\
          , line 86, in profile_num_available_blocks\r\n    self.model_runner.profile_run()\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\"\
          , line 321, in profile_run\r\n    self.execute_model(seqs, kv_caches)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\"\
          , line 279, in execute_model\r\n    hidden_states = self.model(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 290, in forward\r\n    hidden_states = self.model(input_ids, positions,\
          \ kv_caches,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 256, in forward\r\n    hidden_states, residual = layer(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 206, in forward\r\n    hidden_states = self.self_attn(\r\n  File\
          \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
          , line 150, in forward\r\n    qkv, _ = self.qkv_proj(hidden_states)\r\n\
          \  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\"\
          , line 203, in forward\r\n    output_parallel = self.linear_method.apply_weights(\r\
          \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\"\
          , line 68, in apply_weights\r\n    return F.linear(x, weight, bias)\r\n\
          RuntimeError: CUDA error: no kernel image is available for execution on\
          \ the device\r\nCUDA kernel errors might be asynchronously reported at some\
          \ other API call, so the stacktrace below might be incorrect.\r\nFor debugging\
          \ consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA`\
          \ to enable device-side assertions.\r\n\r\nIs this due to hardware specification\
          \ or other?\r\n\r\nLooking forward to reading you!\r\n\r\nKind regards,\r\
          \nJ\xE9r\xE9mie"
        updatedAt: '2024-01-13T23:38:33.948Z'
      numEdits: 0
      reactions: []
    id: 65a31ef9215aabac4870efc7
    type: comment
  author: jzarka
  content: "Hi,\r\n\r\nI have the following error when running the model in a docker\r\
    \nroot@scw-busy-fermat:~# docker run --gpus all     -e HF_TOKEN=$HF_TOKEN -p 8000:8000\
    \     ghcr.io/mistralai/mistral-src/vllm:latest     --host 0.0.0.0     --model\
    \ mistralai/Mistral-7B-Instruct-v0.2 --dtype half\r\nThe HF_TOKEN environment\
    \ variable set, logging to Hugging Face.\r\nToken will not been saved to git credential\
    \ helper. Pass `add_to_git_credential=True` if you want to set the git credential\
    \ as well.\r\nToken is valid (permission: read).\r\nYour token has been saved\
    \ to /root/.cache/huggingface/token\r\nLogin successful\r\nINFO 01-13 23:23:35\
    \ api_server.py:719] args: Namespace(host='0.0.0.0', port=8000, allow_credentials=False,\
    \ allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], served_model_name=None,\
    \ chat_template=None, response_role='assistant', model='mistralai/Mistral-7B-Instruct-v0.2',\
    \ tokenizer=None, revision=None, tokenizer_revision=None, tokenizer_mode='auto',\
    \ trust_remote_code=False, download_dir=None, load_format='auto', dtype='half',\
    \ max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1,\
    \ max_parallel_loading_workers=None, block_size=16, seed=0, swap_space=4, gpu_memory_utilization=0.9,\
    \ max_num_batched_tokens=None, max_num_seqs=256, max_paddings=256, disable_log_stats=False,\
    \ quantization=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\
    \nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 596/596 [00:00<00:00, 1.44MB/s]\r\nWARNING 01-13 23:23:35 config.py:447] Casting\
    \ torch.bfloat16 to torch.float16.\r\nINFO 01-13 23:23:35 llm_engine.py:73] Initializing\
    \ an LLM engine with config: model='mistralai/Mistral-7B-Instruct-v0.2', tokenizer='mistralai/Mistral-7B-Instruct-v0.2',\
    \ tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False,\
    \ dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto,\
    \ tensor_parallel_size=1, quantization=None, seed=0)\r\ntokenizer_config.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.46k/1.46k\
    \ [00:00<00:00, 3.89MB/s]\r\ntokenizer.model: 100%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588| 493k/493k [00:00<00:00, 11.9MB/s]\r\ntokenizer.json:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.80M/1.80M\
    \ [00:00<00:00, 4.28MB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588| 72.0/72.0 [00:00<00:00, 194kB/s]\r\nmodel-00001-of-00003.safetensors:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.94G/4.94G\
    \ [00:24<00:00, 200MB/s]\r\nmodel-00002-of-00003.safetensors: 100%|\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.00G/5.00G [00:26<00:00, 192MB/s]\
    \ \r\nmodel-00003-of-00003.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588| 4.54G/4.54G [00:29<00:00, 156MB/s] \r\nmodel-00003-of-00003.safetensors:\
    \  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A | 4.00G/4.54G [00:25<00:03,\
    \ 162MB/s]\r\nmodel-00003-of-00003.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2589| 4.54G/4.54G [00:29<00:00, 177MB/s]\r\n\r\n\r\n\
    \r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\"\
    , line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals,\
    \ None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n\
    \    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\"\
    , line 729, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
    , line 495, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
    , line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\"\
    , line 314, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line\
    \ 112, in __init__\r\n    self._init_cache()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
    , line 208, in _init_cache\r\n    num_blocks = self._run_workers(\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 750,\
    \ in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\"\
    , line 724, in _run_workers_in_batch\r\n    output = executor(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 86,\
    \ in profile_num_available_blocks\r\n    self.model_runner.profile_run()\r\n \
    \ File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line\
    \ 321, in profile_run\r\n    self.execute_model(seqs, kv_caches)\r\n  File \"\
    /usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115,\
    \ in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\"\
    , line 279, in execute_model\r\n    hidden_states = self.model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
    , line 290, in forward\r\n    hidden_states = self.model(input_ids, positions,\
    \ kv_caches,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
    , line 256, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
    , line 206, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mistral.py\"\
    , line 150, in forward\r\n    qkv, _ = self.qkv_proj(hidden_states)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line\
    \ 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\"\
    , line 203, in forward\r\n    output_parallel = self.linear_method.apply_weights(\r\
    \n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/linear.py\"\
    , line 68, in apply_weights\r\n    return F.linear(x, weight, bias)\r\nRuntimeError:\
    \ CUDA error: no kernel image is available for execution on the device\r\nCUDA\
    \ kernel errors might be asynchronously reported at some other API call, so the\
    \ stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\
    \nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\
    Is this due to hardware specification or other?\r\n\r\nLooking forward to reading\
    \ you!\r\n\r\nKind regards,\r\nJ\xE9r\xE9mie"
  created_at: 2024-01-13 23:38:33+00:00
  edited: false
  hidden: false
  id: 65a31ef9215aabac4870efc7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 97
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Error running model inside a container
