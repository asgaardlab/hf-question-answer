!!python/object:huggingface_hub.community.DiscussionWithDetails
author: logan39522361tq
conflicting_files: null
created_at: 2023-12-13 08:51:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-13T08:51:16.000Z'
    data:
      edited: false
      editors:
      - logan39522361tq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8717058300971985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
          fullname: tang
          isHf: false
          isPro: false
          name: logan39522361tq
          type: user
        html: '<p>I use NVIDIA GeForce RTX 3090 GPU with 24GBRAM.<br>When I run this
          demo code, it turns out these tips:</p>

          <p>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00
          MiB. GPU 0 has a total capacty of 23.69 GiB of which 185.62 MiB is free.
          Including non-PyTorch memory, this process has 23.50 GiB memory in use.
          Of the allocated memory 22.83 GiB is allocated by PyTorch, and 1.17 MiB
          is reserved by PyTorch but unallocated. If reserved but unallocated memory
          is large try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          '
        raw: "I use NVIDIA GeForce RTX 3090 GPU with 24GBRAM.\r\nWhen I run this demo\
          \ code, it turns out these tips:\r\n\r\ntorch.cuda.OutOfMemoryError: CUDA\
          \ out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty\
          \ of 23.69 GiB of which 185.62 MiB is free. Including non-PyTorch memory,\
          \ this process has 23.50 GiB memory in use. Of the allocated memory 22.83\
          \ GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated.\
          \ If reserved but unallocated memory is large try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-12-13T08:51:16.149Z'
      numEdits: 0
      reactions: []
    id: 65797084a415f6989a57f899
    type: comment
  author: logan39522361tq
  content: "I use NVIDIA GeForce RTX 3090 GPU with 24GBRAM.\r\nWhen I run this demo\
    \ code, it turns out these tips:\r\n\r\ntorch.cuda.OutOfMemoryError: CUDA out\
    \ of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 23.69\
    \ GiB of which 185.62 MiB is free. Including non-PyTorch memory, this process\
    \ has 23.50 GiB memory in use. Of the allocated memory 22.83 GiB is allocated\
    \ by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved\
    \ but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-12-13 08:51:16+00:00
  edited: false
  hidden: false
  id: 65797084a415f6989a57f899
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-13T08:56:51.000Z'
    data:
      edited: true
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8912803530693054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Could you specify which demo? Are you loading in float16, bfloat16?
          You should use <code>accelerate</code> with <code>device_map = "auto"</code>
          to overcome potential RAM issues. It should fit in 24GB </p>

          '
        raw: 'Could you specify which demo? Are you loading in float16, bfloat16?
          You should use `accelerate` with `device_map = "auto"` to overcome potential
          RAM issues. It should fit in 24GB '
        updatedAt: '2023-12-13T08:58:00.734Z'
      numEdits: 1
      reactions: []
    id: 657971d362d3ac18172393a1
    type: comment
  author: ArthurZ
  content: 'Could you specify which demo? Are you loading in float16, bfloat16? You
    should use `accelerate` with `device_map = "auto"` to overcome potential RAM issues.
    It should fit in 24GB '
  created_at: 2023-12-13 08:56:51+00:00
  edited: true
  hidden: false
  id: 657971d362d3ac18172393a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-13T09:04:30.000Z'
    data:
      edited: false
      editors:
      - logan39522361tq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6691657900810242
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
          fullname: tang
          isHf: false
          isPro: false
          name: logan39522361tq
          type: user
        html: '<blockquote>

          <p>Could you specify which demo? Are you loading in float16, bfloat16? You
          should use <code>accelerate</code> with <code>device_map = "auto"</code>
          to overcome potential RAM issues. It should fit in 24GB</p>

          </blockquote>

          <p>using this demo:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65796f107f5da1deb6b21338/2hJwU35CwUY8_kRAwnBZ7.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/65796f107f5da1deb6b21338/2hJwU35CwUY8_kRAwnBZ7.png"></a></p>

          <p>config.json file is:<br>"torch_dtype": "bfloat16"</p>

          <p>device_map = "auto"<br>using this in start commond or some where?</p>

          '
        raw: '> Could you specify which demo? Are you loading in float16, bfloat16?
          You should use `accelerate` with `device_map = "auto"` to overcome potential
          RAM issues. It should fit in 24GB


          using this demo:

          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65796f107f5da1deb6b21338/2hJwU35CwUY8_kRAwnBZ7.png)


          config.json file is:

          "torch_dtype": "bfloat16"


          device_map = "auto"

          using this in start commond or some where?

          '
        updatedAt: '2023-12-13T09:04:30.577Z'
      numEdits: 0
      reactions: []
    id: 6579739e197a6182c37a575a
    type: comment
  author: logan39522361tq
  content: '> Could you specify which demo? Are you loading in float16, bfloat16?
    You should use `accelerate` with `device_map = "auto"` to overcome potential RAM
    issues. It should fit in 24GB


    using this demo:

    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65796f107f5da1deb6b21338/2hJwU35CwUY8_kRAwnBZ7.png)


    config.json file is:

    "torch_dtype": "bfloat16"


    device_map = "auto"

    using this in start commond or some where?

    '
  created_at: 2023-12-13 09:04:30+00:00
  edited: false
  hidden: false
  id: 6579739e197a6182c37a575a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-13T09:07:35.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9791034460067749
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Maybe try to reduce the number of tokens that are generated, 1000
          seems like a lot. Try with a smaller number like 40 and go from there</p>

          '
        raw: Maybe try to reduce the number of tokens that are generated, 1000 seems
          like a lot. Try with a smaller number like 40 and go from there
        updatedAt: '2023-12-13T09:07:35.344Z'
      numEdits: 0
      reactions: []
    id: 6579745758d7a2cc8975dd18
    type: comment
  author: ArthurZ
  content: Maybe try to reduce the number of tokens that are generated, 1000 seems
    like a lot. Try with a smaller number like 40 and go from there
  created_at: 2023-12-13 09:07:35+00:00
  edited: false
  hidden: false
  id: 6579745758d7a2cc8975dd18
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-13T09:11:00.000Z'
    data:
      edited: false
      editors:
      - logan39522361tq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.965885579586029
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
          fullname: tang
          isHf: false
          isPro: false
          name: logan39522361tq
          type: user
        html: '<blockquote>

          <p>Maybe try to reduce the number of tokens that are generated, 1000 seems
          like a lot. Try with a smaller number like 40 and go from there</p>

          </blockquote>

          <p>It''s doesnt help, the error appears at &lt;model.to(device1)&gt;</p>

          '
        raw: '> Maybe try to reduce the number of tokens that are generated, 1000
          seems like a lot. Try with a smaller number like 40 and go from there


          It''s doesnt help, the error appears at <model.to(device1)>'
        updatedAt: '2023-12-13T09:11:00.909Z'
      numEdits: 0
      reactions: []
    id: 65797524224758a1fcaeacc1
    type: comment
  author: logan39522361tq
  content: '> Maybe try to reduce the number of tokens that are generated, 1000 seems
    like a lot. Try with a smaller number like 40 and go from there


    It''s doesnt help, the error appears at <model.to(device1)>'
  created_at: 2023-12-13 09:11:00+00:00
  edited: false
  hidden: false
  id: 65797524224758a1fcaeacc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
      fullname: Arthur Zucker
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ArthurZ
      type: user
    createdAt: '2023-12-13T09:12:20.000Z'
    data:
      edited: false
      editors:
      - ArthurZ
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.683521032333374
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1674683851722-62441cb7456803e95009a08f.jpeg?w=200&h=200&f=face
          fullname: Arthur Zucker
          isHf: true
          isPro: false
          name: ArthurZ
          type: user
        html: '<p>Then try load in 8 bits or use accelerate and device_map = "auto"
          :hug: </p>

          '
        raw: 'Then try load in 8 bits or use accelerate and device_map = "auto" :hug: '
        updatedAt: '2023-12-13T09:12:20.310Z'
      numEdits: 0
      reactions: []
    id: 65797574e7880298b001a15f
    type: comment
  author: ArthurZ
  content: 'Then try load in 8 bits or use accelerate and device_map = "auto" :hug: '
  created_at: 2023-12-13 09:12:20+00:00
  edited: false
  hidden: false
  id: 65797574e7880298b001a15f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-13T09:16:37.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5810279846191406
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;logan39522361tq&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/logan39522361tq\"\
          >@<span class=\"underline\">logan39522361tq</span></a></span>\n\n\t</span></span><br>Can\
          \ you try to load the model as such:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(<span class=\"\
          hljs-string\">\"mistralai/Mistral-7B-Instruct-v0.1\"</span>, torch_dtype=torch.float16,\
          \ device_map=<span class=\"hljs-string\">\"auto\"</span>)\n...\n</code></pre>\n\
          <p>The snippet you shared will load the model in full precision (28GB),\
          \ hence the GPU error you get</p>\n<p>Alternatively you can also do:</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-comment\"># pip\
          \ install bitsandbytes</span>\n<span class=\"hljs-keyword\">import</span>\
          \ torch\n<span class=\"hljs-keyword\">from</span> transformers <span class=\"\
          hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\">\"\
          mistralai/Mistral-7B-Instruct-v0.1\"</span>, load_in_4bit=<span class=\"\
          hljs-literal\">True</span>)\n...\n</code></pre>\n"
        raw: "@logan39522361tq \nCan you try to load the model as such:\n\n```python\n\
          import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          , torch_dtype=torch.float16, device_map=\"auto\")\n...\n```\n\nThe snippet\
          \ you shared will load the model in full precision (28GB), hence the GPU\
          \ error you get\n\nAlternatively you can also do:\n\n```python\n# pip install\
          \ bitsandbytes\nimport torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          , load_in_4bit=True)\n...\n```"
        updatedAt: '2023-12-13T09:16:37.085Z'
      numEdits: 0
      reactions: []
    id: 65797675c37954680aed7676
    type: comment
  author: ybelkada
  content: "@logan39522361tq \nCan you try to load the model as such:\n\n```python\n\
    import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\
    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
    , torch_dtype=torch.float16, device_map=\"auto\")\n...\n```\n\nThe snippet you\
    \ shared will load the model in full precision (28GB), hence the GPU error you\
    \ get\n\nAlternatively you can also do:\n\n```python\n# pip install bitsandbytes\n\
    import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\
    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
    , load_in_4bit=True)\n...\n```"
  created_at: 2023-12-13 09:16:37+00:00
  edited: false
  hidden: false
  id: 65797675c37954680aed7676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-14T12:37:37.000Z'
    data:
      edited: false
      editors:
      - logan39522361tq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6160997748374939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
          fullname: tang
          isHf: false
          isPro: false
          name: logan39522361tq
          type: user
        html: '<p>I used this to init model<br>&lt;model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1",
          torch_dtype=torch.float16, device_map="auto")&gt;</p>

          <p>but  when I run<br>&lt;model.to(device)&gt;<br>it turns out this error(using
          python virtual env):<br>lib/python3.10/site-packages/accelerate/big_modeling.py",
          line 425, in wrapper<br>    raise RuntimeError("You can''t move a model
          that has some modules offloaded to cpu or disk."</p>

          '
        raw: "I used this to init model\n<model = AutoModelForCausalLM.from_pretrained(\"\
          mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.float16, device_map=\"\
          auto\")>\n\nbut  when I run \n<model.to(device)>\nit turns out this error(using\
          \ python virtual env):\nlib/python3.10/site-packages/accelerate/big_modeling.py\"\
          , line 425, in wrapper\n    raise RuntimeError(\"You can't move a model\
          \ that has some modules offloaded to cpu or disk.\""
        updatedAt: '2023-12-14T12:37:37.255Z'
      numEdits: 0
      reactions: []
    id: 657af7117e310c3702f0046e
    type: comment
  author: logan39522361tq
  content: "I used this to init model\n<model = AutoModelForCausalLM.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.float16, device_map=\"\
    auto\")>\n\nbut  when I run \n<model.to(device)>\nit turns out this error(using\
    \ python virtual env):\nlib/python3.10/site-packages/accelerate/big_modeling.py\"\
    , line 425, in wrapper\n    raise RuntimeError(\"You can't move a model that has\
    \ some modules offloaded to cpu or disk.\""
  created_at: 2023-12-14 12:37:37+00:00
  edited: false
  hidden: false
  id: 657af7117e310c3702f0046e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-14T12:39:28.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.911769688129425
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>this is because device_map="auto" has automatically offloaded your
          model into cpu or disk. What is your GPU total VRAM?</p>

          '
        raw: this is because device_map="auto" has automatically offloaded your model
          into cpu or disk. What is your GPU total VRAM?
        updatedAt: '2023-12-14T12:39:28.648Z'
      numEdits: 0
      reactions: []
    id: 657af780fcba5f698cfe25ec
    type: comment
  author: ybelkada
  content: this is because device_map="auto" has automatically offloaded your model
    into cpu or disk. What is your GPU total VRAM?
  created_at: 2023-12-14 12:39:28+00:00
  edited: false
  hidden: false
  id: 657af780fcba5f698cfe25ec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-14T12:40:08.000Z'
    data:
      edited: false
      editors:
      - logan39522361tq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8406417369842529
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
          fullname: tang
          isHf: false
          isPro: false
          name: logan39522361tq
          type: user
        html: '<blockquote>

          <p>this is because device_map="auto" has automatically offloaded your model
          into cpu or disk. What is your GPU total VRAM?</p>

          </blockquote>

          <p>NVIDIA GeForce RTX 3090 GPU with 24GBRAM</p>

          '
        raw: '> this is because device_map="auto" has automatically offloaded your
          model into cpu or disk. What is your GPU total VRAM?


          NVIDIA GeForce RTX 3090 GPU with 24GBRAM'
        updatedAt: '2023-12-14T12:40:08.976Z'
      numEdits: 0
      reactions: []
    id: 657af7a810609bba27194dd3
    type: comment
  author: logan39522361tq
  content: '> this is because device_map="auto" has automatically offloaded your model
    into cpu or disk. What is your GPU total VRAM?


    NVIDIA GeForce RTX 3090 GPU with 24GBRAM'
  created_at: 2023-12-14 12:40:08+00:00
  edited: false
  hidden: false
  id: 657af7a810609bba27194dd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-14T12:41:34.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8872315883636475
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>the model has 44B parameters (you need ~90GB VRAM to fit your GPU
          in half-precision), it will not fit into your GPU. Please consider running
          the model in 4bit precision - or use cpu / disk offloading at the risk of
          not being able to call <code>model.to(device)</code></p>

          '
        raw: the model has 44B parameters (you need ~90GB VRAM to fit your GPU in
          half-precision), it will not fit into your GPU. Please consider running
          the model in 4bit precision - or use cpu / disk offloading at the risk of
          not being able to call `model.to(device)`
        updatedAt: '2023-12-14T12:41:34.052Z'
      numEdits: 0
      reactions: []
    id: 657af7fe9f62ed61a2eeb079
    type: comment
  author: ybelkada
  content: the model has 44B parameters (you need ~90GB VRAM to fit your GPU in half-precision),
    it will not fit into your GPU. Please consider running the model in 4bit precision
    - or use cpu / disk offloading at the risk of not being able to call `model.to(device)`
  created_at: 2023-12-14 12:41:34+00:00
  edited: false
  hidden: false
  id: 657af7fe9f62ed61a2eeb079
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-16T03:32:21.000Z'
    data:
      edited: false
      editors:
      - logan39522361tq
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8439382910728455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
          fullname: tang
          isHf: false
          isPro: false
          name: logan39522361tq
          type: user
        html: '<p>code:<br>model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True)<br>config.json:<br>"torch_dtype":
          "bfloat16",</p>

          <p>it turns out error:<br><code>model.to(device)</code> is not supported
          for <code>4-bit</code> or <code>8-bit</code> bitsandbytes models. Please
          use the model as it is, since the model has already been set to the correct
          devices and casted to the correct <code>dtype</code>.</p>

          '
        raw: 'code:

          model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True)

          config.json:

          "torch_dtype": "bfloat16",


          it turns out error:

          `model.to(device)` is not supported for `4-bit` or `8-bit` bitsandbytes
          models. Please use the model as it is, since the model has already been
          set to the correct devices and casted to the correct `dtype`.

          '
        updatedAt: '2023-12-16T03:32:21.176Z'
      numEdits: 0
      reactions: []
      relatedEventId: 657d1a453480ce8aae632e31
    id: 657d1a453480ce8aae632e2f
    type: comment
  author: logan39522361tq
  content: 'code:

    model = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True)

    config.json:

    "torch_dtype": "bfloat16",


    it turns out error:

    `model.to(device)` is not supported for `4-bit` or `8-bit` bitsandbytes models.
    Please use the model as it is, since the model has already been set to the correct
    devices and casted to the correct `dtype`.

    '
  created_at: 2023-12-16 03:32:21+00:00
  edited: false
  hidden: false
  id: 657d1a453480ce8aae632e2f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ba4f5279ea39983b2484913c2e425e76.svg
      fullname: tang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: logan39522361tq
      type: user
    createdAt: '2023-12-16T03:32:21.000Z'
    data:
      status: closed
    id: 657d1a453480ce8aae632e31
    type: status-change
  author: logan39522361tq
  created_at: 2023-12-16 03:32:21+00:00
  id: 657d1a453480ce8aae632e31
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2023-12-18T19:19:53.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9566138982772827
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: '<p>loading the model with quantization will automatically dispatch
          the model in the available devices, hence there is no need to call .to as
          it will also create issues with offloading as well</p>

          '
        raw: 'loading the model with quantization will automatically dispatch the
          model in the available devices, hence there is no need to call .to as it
          will also create issues with offloading as well

          '
        updatedAt: '2023-12-18T19:19:53.714Z'
      numEdits: 0
      reactions: []
    id: 65809b5962b6ed53c4cb074e
    type: comment
  author: ybelkada
  content: 'loading the model with quantization will automatically dispatch the model
    in the available devices, hence there is no need to call .to as it will also create
    issues with offloading as well

    '
  created_at: 2023-12-18 19:19:53+00:00
  edited: false
  hidden: false
  id: 65809b5962b6ed53c4cb074e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 91
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: closed
target_branch: null
title: CUDA out of memory
