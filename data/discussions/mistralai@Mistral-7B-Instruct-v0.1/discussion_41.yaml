!!python/object:huggingface_hub.community.DiscussionWithDetails
author: sakshat98
conflicting_files: null
created_at: 2023-10-05 14:57:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42bd81f3ca47f23a9a2d147043cbc5ed.svg
      fullname: katyarmal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sakshat98
      type: user
    createdAt: '2023-10-05T15:57:21.000Z'
    data:
      edited: false
      editors:
      - sakshat98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9768580198287964
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42bd81f3ca47f23a9a2d147043cbc5ed.svg
          fullname: katyarmal
          isHf: false
          isPro: false
          name: sakshat98
          type: user
        html: '<p>Hi, I love Mistral. I just had a use case where I wanted to have
          a system prompt during conversation. I saw that Mistral does not accept
          {""role":"system"}. Is there any way to have this?</p>

          '
        raw: Hi, I love Mistral. I just had a use case where I wanted to have a system
          prompt during conversation. I saw that Mistral does not accept {""role":"system"}.
          Is there any way to have this?
        updatedAt: '2023-10-05T15:57:21.984Z'
      numEdits: 0
      reactions: []
    id: 651edce13c6ed3a19375a17d
    type: comment
  author: sakshat98
  content: Hi, I love Mistral. I just had a use case where I wanted to have a system
    prompt during conversation. I saw that Mistral does not accept {""role":"system"}.
    Is there any way to have this?
  created_at: 2023-10-05 14:57:21+00:00
  edited: false
  hidden: false
  id: 651edce13c6ed3a19375a17d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441ab605d600fb0951f2f38/ZoVDvjvkUADAoLe1t1xYM.png?w=200&h=200&f=face
      fullname: Hokyung (Andy) Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: techandy42
      type: user
    createdAt: '2023-10-08T02:50:05.000Z'
    data:
      edited: false
      editors:
      - techandy42
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5916611552238464
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6441ab605d600fb0951f2f38/ZoVDvjvkUADAoLe1t1xYM.png?w=200&h=200&f=face
          fullname: Hokyung (Andy) Lee
          isHf: false
          isPro: false
          name: techandy42
          type: user
        html: '<h1 id="try-the-following">Try the following</h1>

          <p>sys_prompt = "You are a helpful assistant, who always provide explanation.
          Think like you are answering to a five year old."<br>prompt = "What kind
          of task would test someone''s ability to perform physical reasoning?"</p>

          <p>prefix = "&lt;|im_start|&gt;"<br>suffix = "&lt;|im_end|&gt;\n"<br>sys_format
          = prefix + "system\n" + sys_prompt + suffix<br>user_format = prefix + "user\n"
          + prompt + suffix<br>assistant_format = prefix + "assistant\n"<br>input_text
          = sys_format + user_format + assistant_format</p>

          <p>messages = [<br>    {"role": "user", "content": input_text},<br>]</p>

          <p>encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")</p>

          <p>model_inputs = encodeds.to(device)<br>model.to(device)</p>

          <p>generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)<br>decoded
          = tokenizer.batch_decode(generated_ids)<br>print(decoded[0])</p>

          '
        raw: "# Try the following\n\nsys_prompt = \"You are a helpful assistant, who\
          \ always provide explanation. Think like you are answering to a five year\
          \ old.\"\nprompt = \"What kind of task would test someone's ability to perform\
          \ physical reasoning?\"\n\nprefix = \"<|im_start|>\"\nsuffix = \"<|im_end|>\\\
          n\"\nsys_format = prefix + \"system\\n\" + sys_prompt + suffix\nuser_format\
          \ = prefix + \"user\\n\" + prompt + suffix\nassistant_format = prefix +\
          \ \"assistant\\n\"\ninput_text = sys_format + user_format + assistant_format\n\
          \nmessages = [\n    {\"role\": \"user\", \"content\": input_text},\n]\n\n\
          encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\"\
          )\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids\
          \ = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n\
          decoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])"
        updatedAt: '2023-10-08T02:50:05.075Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - techandy42
    id: 652218dd777019ca30c46db4
    type: comment
  author: techandy42
  content: "# Try the following\n\nsys_prompt = \"You are a helpful assistant, who\
    \ always provide explanation. Think like you are answering to a five year old.\"\
    \nprompt = \"What kind of task would test someone's ability to perform physical\
    \ reasoning?\"\n\nprefix = \"<|im_start|>\"\nsuffix = \"<|im_end|>\\n\"\nsys_format\
    \ = prefix + \"system\\n\" + sys_prompt + suffix\nuser_format = prefix + \"user\\\
    n\" + prompt + suffix\nassistant_format = prefix + \"assistant\\n\"\ninput_text\
    \ = sys_format + user_format + assistant_format\n\nmessages = [\n    {\"role\"\
    : \"user\", \"content\": input_text},\n]\n\nencodeds = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\n\nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\
    \ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n\
    decoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])"
  created_at: 2023-10-08 01:50:05+00:00
  edited: false
  hidden: false
  id: 652218dd777019ca30c46db4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c71878c87bce36d3193ea9e13ad23722.svg
      fullname: Venkata Bhanu Teja Pallakonda
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pvbhanuteja
      type: user
    createdAt: '2023-10-12T19:39:46.000Z'
    data:
      edited: false
      editors:
      - pvbhanuteja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5838821530342102
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c71878c87bce36d3193ea9e13ad23722.svg
          fullname: Venkata Bhanu Teja Pallakonda
          isHf: false
          isPro: false
          name: pvbhanuteja
          type: user
        html: "<p>Use this</p>\n<p><s>[INST] System Prompt + Instruction [/INST] Model\
          \ answer</s>[INST] Follow-up instruction [/INST]</p>\n<p>From their official\
          \ site. </p>\n<p><a rel=\"nofollow\" href=\"https://docs.mistral.ai/usage/guardrailing\"\
          >https://docs.mistral.ai/usage/guardrailing</a></p>\n<p>If you want it for\
          \ gradio. I wrote a formatting function. Remove/add accordingly. </p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">format_chat_prompt_mistral</span>(<span\
          \ class=\"hljs-params\">message: <span class=\"hljs-built_in\">str</span>,\
          \ chat_history, instructions: <span class=\"hljs-built_in\">str</span></span>)\
          \ -&gt; <span class=\"hljs-built_in\">str</span>:\n    <span class=\"hljs-keyword\"\
          >if</span> <span class=\"hljs-built_in\">len</span>(chat_history) == <span\
          \ class=\"hljs-number\">0</span>:\n        <span class=\"hljs-comment\"\
          ># If chat_history is empty, return instructions and message</span>\n  \
          \      prompt = <span class=\"hljs-string\">f\"&lt;s&gt;[INST] <span class=\"\
          hljs-subst\">{instructions}</span> Hi [/INST] Hello! how can I help you&lt;/s&gt;[INST]\
          \ <span class=\"hljs-subst\">{message}</span> [/INST]\"</span>\n       \
          \ <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >\"sending this prompt\\n==============\\n\"</span>,prompt,<span class=\"\
          hljs-string\">'\\n---------\\n'</span>)\n        <span class=\"hljs-keyword\"\
          >return</span> prompt\n    <span class=\"hljs-keyword\">else</span>:\n \
          \       <span class=\"hljs-comment\"># Initialize chat history text with\
          \ the first user message and instructions</span>\n        user_message,\
          \ bot_message = chat_history[<span class=\"hljs-number\">0</span>]\n   \
          \     chat_history_text = <span class=\"hljs-string\">f\"&lt;s&gt;[INST]\
          \ <span class=\"hljs-subst\">{instructions}</span> <span class=\"hljs-subst\"\
          >{user_message}</span> [/INST] <span class=\"hljs-subst\">{bot_message}</span>&lt;/s&gt;\"\
          </span>\n\n        <span class=\"hljs-comment\"># Use a list comprehension\
          \ to build the rest of the chat history text</span>\n        chat_history_text\
          \ += <span class=\"hljs-string\">\"\"</span>.join(<span class=\"hljs-string\"\
          >f\"[INST] <span class=\"hljs-subst\">{user_message}</span> [/INST] <span\
          \ class=\"hljs-subst\">{bot_message}</span>&lt;/s&gt;\"</span> <span class=\"\
          hljs-keyword\">for</span> user_message, bot_message <span class=\"hljs-keyword\"\
          >in</span> chat_history[<span class=\"hljs-number\">1</span>:])\n</code></pre>\n"
        raw: "Use this\n\n<s>[INST] System Prompt + Instruction [/INST] Model answer</s>[INST]\
          \ Follow-up instruction [/INST]\n\nFrom their official site. \n\nhttps://docs.mistral.ai/usage/guardrailing\n\
          \nIf you want it for gradio. I wrote a formatting function. Remove/add accordingly.\
          \ \n\n```python\ndef format_chat_prompt_mistral(message: str, chat_history,\
          \ instructions: str) -> str:\n    if len(chat_history) == 0:\n        #\
          \ If chat_history is empty, return instructions and message\n        prompt\
          \ = f\"<s>[INST] {instructions} Hi [/INST] Hello! how can I help you</s>[INST]\
          \ {message} [/INST]\"\n        print(\"sending this prompt\\n==============\\\
          n\",prompt,'\\n---------\\n')\n        return prompt\n    else:\n      \
          \  # Initialize chat history text with the first user message and instructions\n\
          \        user_message, bot_message = chat_history[0]\n        chat_history_text\
          \ = f\"<s>[INST] {instructions} {user_message} [/INST] {bot_message}</s>\"\
          \n\n        # Use a list comprehension to build the rest of the chat history\
          \ text\n        chat_history_text += \"\".join(f\"[INST] {user_message}\
          \ [/INST] {bot_message}</s>\" for user_message, bot_message in chat_history[1:])\n"
        updatedAt: '2023-10-12T19:39:46.308Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - cleexiang
        - Ibrahim-Ola
    id: 65284b827b99d91baa1037bb
    type: comment
  author: pvbhanuteja
  content: "Use this\n\n<s>[INST] System Prompt + Instruction [/INST] Model answer</s>[INST]\
    \ Follow-up instruction [/INST]\n\nFrom their official site. \n\nhttps://docs.mistral.ai/usage/guardrailing\n\
    \nIf you want it for gradio. I wrote a formatting function. Remove/add accordingly.\
    \ \n\n```python\ndef format_chat_prompt_mistral(message: str, chat_history, instructions:\
    \ str) -> str:\n    if len(chat_history) == 0:\n        # If chat_history is empty,\
    \ return instructions and message\n        prompt = f\"<s>[INST] {instructions}\
    \ Hi [/INST] Hello! how can I help you</s>[INST] {message} [/INST]\"\n       \
    \ print(\"sending this prompt\\n==============\\n\",prompt,'\\n---------\\n')\n\
    \        return prompt\n    else:\n        # Initialize chat history text with\
    \ the first user message and instructions\n        user_message, bot_message =\
    \ chat_history[0]\n        chat_history_text = f\"<s>[INST] {instructions} {user_message}\
    \ [/INST] {bot_message}</s>\"\n\n        # Use a list comprehension to build the\
    \ rest of the chat history text\n        chat_history_text += \"\".join(f\"[INST]\
    \ {user_message} [/INST] {bot_message}</s>\" for user_message, bot_message in\
    \ chat_history[1:])\n"
  created_at: 2023-10-12 18:39:46+00:00
  edited: false
  hidden: false
  id: 65284b827b99d91baa1037bb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628140467307-610b70452719facd4ea85e28.jpeg?w=200&h=200&f=face
      fullname: Chujie Zheng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chujiezheng
      type: user
    createdAt: '2023-12-21T20:08:37.000Z'
    data:
      edited: false
      editors:
      - chujiezheng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6090222597122192
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1628140467307-610b70452719facd4ea85e28.jpeg?w=200&h=200&f=face
          fullname: Chujie Zheng
          isHf: false
          isPro: false
          name: chujiezheng
          type: user
        html: '<p>I collected official chat templates in <a rel="nofollow" href="https://github.com/chujiezheng/chat_templates">this
          repo</a>. You may use it with the <code>apply_chat_template</code> method</p>

          '
        raw: I collected official chat templates in [this repo](https://github.com/chujiezheng/chat_templates).
          You may use it with the `apply_chat_template` method
        updatedAt: '2023-12-21T20:08:37.426Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mayank1-khurana
    id: 65849b450e68ae8756fad1ee
    type: comment
  author: chujiezheng
  content: I collected official chat templates in [this repo](https://github.com/chujiezheng/chat_templates).
    You may use it with the `apply_chat_template` method
  created_at: 2023-12-21 20:08:37+00:00
  edited: false
  hidden: false
  id: 65849b450e68ae8756fad1ee
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 41
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: System Prompt
