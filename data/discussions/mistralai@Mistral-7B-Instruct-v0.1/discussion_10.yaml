!!python/object:huggingface_hub.community.DiscussionWithDetails
author: NS-Y
conflicting_files: null
created_at: 2023-09-28 06:01:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4cf3b27915a242b6efa5f87bd94a1cc7.svg
      fullname: Nissan Yaron
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NS-Y
      type: user
    createdAt: '2023-09-28T07:01:08.000Z'
    data:
      edited: false
      editors:
      - NS-Y
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7885374426841736
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4cf3b27915a242b6efa5f87bd94a1cc7.svg
          fullname: Nissan Yaron
          isHf: false
          isPro: false
          name: NS-Y
          type: user
        html: '<p>Hi, I''m running the suggested code on Colab and getting this warning:</p>

          <p>Using sep_token, but it is not set yet.<br>Using pad_token, but it is
          not set yet.<br>Using cls_token, but it is not set yet.<br>Using mask_token,
          but it is not set yet.<br>The attention mask and the pad token id were not
          set. As a consequence, you may observe unexpected behavior. Please pass
          your input''s <code>attention_mask</code> to obtain reliable results.<br>Setting
          <code>pad_token_id</code> to <code>eos_token_id</code>:2 for open-end generation.</p>

          <p>Is there an easy way to pass those parameters?<br>Also, what variables
          are available - temperature, top_k, repetition penalty, stop tokens etc,
          and how to pass them?<br>I couldn''t find information about it.</p>

          <p>This is the code I run:</p>

          <p>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>device = "cuda" # the device to load the model onto</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br>model
          = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")</p>

          <p>messages = [{"role":"system","content":"You are a seller at a store that
          sells only shoes. You are friendly and polite"},{"role":"user","content":"Client:
          Hello. I want to buy Pizza."}]<br>encodeds = tokenizer.apply_chat_template(messages,
          return_tensors="pt")</p>

          <p>model_inputs = encodeds.to(device)<br>model.to(device)</p>

          <p>generated_ids = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000,
          do_sample=True)<br>decoded = tokenizer.batch_decode(generated_ids)<br>print(decoded[0])</p>

          <p>By the way, this model is truly amazing.</p>

          '
        raw: "Hi, I'm running the suggested code on Colab and getting this warning:\r\
          \n\r\nUsing sep_token, but it is not set yet.\r\nUsing pad_token, but it\
          \ is not set yet.\r\nUsing cls_token, but it is not set yet.\r\nUsing mask_token,\
          \ but it is not set yet.\r\nThe attention mask and the pad token id were\
          \ not set. As a consequence, you may observe unexpected behavior. Please\
          \ pass your input's `attention_mask` to obtain reliable results.\r\nSetting\
          \ `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\n\r\nIs\
          \ there an easy way to pass those parameters?\r\nAlso, what variables are\
          \ available - temperature, top_k, repetition penalty, stop tokens etc, and\
          \ how to pass them? \r\nI couldn't find information about it.\r\n\r\nThis\
          \ is the code I run:\r\n\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\n\r\ndevice = \"cuda\" # the device to load the model\
          \ onto\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\r\n\r\nmessages = [{\"role\":\"system\",\"content\":\"You are a seller\
          \ at a store that sells only shoes. You are friendly and polite\"},{\"role\"\
          :\"user\",\"content\":\"Client: Hello. I want to buy Pizza.\"}]\r\nencodeds\
          \ = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\r\n\r\
          \nmodel_inputs = encodeds.to(device)\r\nmodel.to(device)\r\n\r\ngenerated_ids\
          \ = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000, do_sample=True)\r\
          \ndecoded = tokenizer.batch_decode(generated_ids)\r\nprint(decoded[0])\r\
          \n\r\nBy the way, this model is truly amazing."
        updatedAt: '2023-09-28T07:01:08.267Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - emma-gu-groupama
    id: 651524b4a8849e84283f0394
    type: comment
  author: NS-Y
  content: "Hi, I'm running the suggested code on Colab and getting this warning:\r\
    \n\r\nUsing sep_token, but it is not set yet.\r\nUsing pad_token, but it is not\
    \ set yet.\r\nUsing cls_token, but it is not set yet.\r\nUsing mask_token, but\
    \ it is not set yet.\r\nThe attention mask and the pad token id were not set.\
    \ As a consequence, you may observe unexpected behavior. Please pass your input's\
    \ `attention_mask` to obtain reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:2\
    \ for open-end generation.\r\n\r\nIs there an easy way to pass those parameters?\r\
    \nAlso, what variables are available - temperature, top_k, repetition penalty,\
    \ stop tokens etc, and how to pass them? \r\nI couldn't find information about\
    \ it.\r\n\r\nThis is the code I run:\r\n\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\n\r\ndevice = \"cuda\" # the device to load the model onto\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
    )\r\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
    )\r\n\r\nmessages = [{\"role\":\"system\",\"content\":\"You are a seller at a\
    \ store that sells only shoes. You are friendly and polite\"},{\"role\":\"user\"\
    ,\"content\":\"Client: Hello. I want to buy Pizza.\"}]\r\nencodeds = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\r\n\r\nmodel_inputs = encodeds.to(device)\r\nmodel.to(device)\r\
    \n\r\ngenerated_ids = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000,\
    \ do_sample=True)\r\ndecoded = tokenizer.batch_decode(generated_ids)\r\nprint(decoded[0])\r\
    \n\r\nBy the way, this model is truly amazing."
  created_at: 2023-09-28 06:01:08+00:00
  edited: false
  hidden: false
  id: 651524b4a8849e84283f0394
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9bd39791b3a203b22a836d9ede489b6.svg
      fullname: adrien sade
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sade-adrien
      type: user
    createdAt: '2023-10-02T19:31:03.000Z'
    data:
      edited: false
      editors:
      - sade-adrien
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.34184134006500244
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9bd39791b3a203b22a836d9ede489b6.svg
          fullname: adrien sade
          isHf: false
          isPro: false
          name: sade-adrien
          type: user
        html: '<p>To generate the attention mask, you can replace this :</p>

          <p>encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")</p>

          <p>model_inputs = encodeds.to(device)<br>model.to(device)</p>

          <p>generated_ids = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000,
          do_sample=True)</p>

          <p>with:</p>

          <p>encodeds = tokenizer(messages, return_tensors=''pt)</p>

          <p>encodeds[''input_ids''] = encodeds[''input_ids''].to(device)<br>encodeds[''attention_mask'']
          = encodeds[''attention_mask''].to(device)<br>model.to(device)</p>

          <p>generated_ids = model.generate(**encodeds, eos_token_id=2, max_new_tokens=1000,
          do_sample=True)</p>

          <p>but really it should not matter that you set all those things or not.</p>

          <p>for the parameters check out the documentation of huggingface''s model.generate</p>

          '
        raw: 'To generate the attention mask, you can replace this :


          encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")


          model_inputs = encodeds.to(device)

          model.to(device)


          generated_ids = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000,
          do_sample=True)


          with:


          encodeds = tokenizer(messages, return_tensors=''pt)


          encodeds[''input_ids''] = encodeds[''input_ids''].to(device)

          encodeds[''attention_mask''] = encodeds[''attention_mask''].to(device)

          model.to(device)


          generated_ids = model.generate(**encodeds, eos_token_id=2, max_new_tokens=1000,
          do_sample=True)



          but really it should not matter that you set all those things or not.



          for the parameters check out the documentation of huggingface''s model.generate'
        updatedAt: '2023-10-02T19:31:03.656Z'
      numEdits: 0
      reactions: []
    id: 651b1a77fa4bf59ced6d7834
    type: comment
  author: sade-adrien
  content: 'To generate the attention mask, you can replace this :


    encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")


    model_inputs = encodeds.to(device)

    model.to(device)


    generated_ids = model.generate(model_inputs, eos_token_id=2, max_new_tokens=1000,
    do_sample=True)


    with:


    encodeds = tokenizer(messages, return_tensors=''pt)


    encodeds[''input_ids''] = encodeds[''input_ids''].to(device)

    encodeds[''attention_mask''] = encodeds[''attention_mask''].to(device)

    model.to(device)


    generated_ids = model.generate(**encodeds, eos_token_id=2, max_new_tokens=1000,
    do_sample=True)



    but really it should not matter that you set all those things or not.



    for the parameters check out the documentation of huggingface''s model.generate'
  created_at: 2023-10-02 18:31:03+00:00
  edited: false
  hidden: false
  id: 651b1a77fa4bf59ced6d7834
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/08f1b039de2f8521ca6d1ffa94b7bcf8.svg
      fullname: Demetrios Skeletor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: narai
      type: user
    createdAt: '2023-10-17T19:50:30.000Z'
    data:
      edited: false
      editors:
      - narai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5683088302612305
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/08f1b039de2f8521ca6d1ffa94b7bcf8.svg
          fullname: Demetrios Skeletor
          isHf: false
          isPro: false
          name: narai
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sade-adrien&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sade-adrien\"\
          >@<span class=\"underline\">sade-adrien</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>Since the messages variable is a list of dictionaries, your code\
          \ gives an error<br>encodeds = tokenizer(messages, return_tensors='pt')</p>\n\
          <p>ValueError: text input must of type <code>str</code> (single example),\
          \ <code>List[str]</code> (batch or single pretokenized example) or <code>List[List[str]]</code>\
          \ (batch of pretokenized examples).</p>\n<p>It looks like apply_chat_template\
          \ returns input_ids, and does not have the ability to return the mask.</p>\n"
        raw: "@sade-adrien \n\nSince the messages variable is a list of dictionaries,\
          \ your code gives an error\nencodeds = tokenizer(messages, return_tensors='pt')\n\
          \nValueError: text input must of type `str` (single example), `List[str]`\
          \ (batch or single pretokenized example) or `List[List[str]]` (batch of\
          \ pretokenized examples).\n\nIt looks like apply_chat_template returns input_ids,\
          \ and does not have the ability to return the mask."
        updatedAt: '2023-10-17T19:50:30.847Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sade-adrien
    id: 652ee586f08bb52d9961a08b
    type: comment
  author: narai
  content: "@sade-adrien \n\nSince the messages variable is a list of dictionaries,\
    \ your code gives an error\nencodeds = tokenizer(messages, return_tensors='pt')\n\
    \nValueError: text input must of type `str` (single example), `List[str]` (batch\
    \ or single pretokenized example) or `List[List[str]]` (batch of pretokenized\
    \ examples).\n\nIt looks like apply_chat_template returns input_ids, and does\
    \ not have the ability to return the mask."
  created_at: 2023-10-17 18:50:30+00:00
  edited: false
  hidden: false
  id: 652ee586f08bb52d9961a08b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Missing parameters
