!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gxxxz
conflicting_files: null
created_at: 2023-10-13 07:29:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
      fullname: Gevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gxxxz
      type: user
    createdAt: '2023-10-13T08:29:16.000Z'
    data:
      edited: false
      editors:
      - gxxxz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9526092410087585
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
          fullname: Gevin
          isHf: false
          isPro: false
          name: gxxxz
          type: user
        html: '<p>How can i use this model for question answering, I want to pass
          some context and the question and model should get the data from context
          and answer question. Is there any prompt format for this?</p>

          '
        raw: How can i use this model for question answering, I want to pass some
          context and the question and model should get the data from context and
          answer question. Is there any prompt format for this?
        updatedAt: '2023-10-13T08:29:16.959Z'
      numEdits: 0
      reactions: []
    id: 6528ffdcad4a7ea3a5362445
    type: comment
  author: gxxxz
  content: How can i use this model for question answering, I want to pass some context
    and the question and model should get the data from context and answer question.
    Is there any prompt format for this?
  created_at: 2023-10-13 07:29:16+00:00
  edited: false
  hidden: false
  id: 6528ffdcad4a7ea3a5362445
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a03cb392267a7286ec95273143db4131.svg
      fullname: Dikshant Shah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 808Code
      type: user
    createdAt: '2023-10-16T14:10:48.000Z'
    data:
      edited: false
      editors:
      - 808Code
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7466489672660828
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a03cb392267a7286ec95273143db4131.svg
          fullname: Dikshant Shah
          isHf: false
          isPro: false
          name: 808Code
          type: user
        html: '<p>Pass this to llm  <code>&lt;s&gt;[INST] Using this information :
          {context} answer the Question : {query} [/INST] </code> ,  you can look
          into prompt templating , through langchain too if you haven''t.</p>

          '
        raw: 'Pass this to llm  ``` <s>[INST] Using this information : {context} answer
          the Question : {query} [/INST]  ``` ,  you can look into prompt templating
          , through langchain too if you haven''t.

          '
        updatedAt: '2023-10-16T14:10:48.538Z'
      numEdits: 0
      reactions: []
    id: 652d44682c4aaa44a1d1ab01
    type: comment
  author: 808Code
  content: 'Pass this to llm  ``` <s>[INST] Using this information : {context} answer
    the Question : {query} [/INST]  ``` ,  you can look into prompt templating , through
    langchain too if you haven''t.

    '
  created_at: 2023-10-16 13:10:48+00:00
  edited: false
  hidden: false
  id: 652d44682c4aaa44a1d1ab01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
      fullname: Gevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gxxxz
      type: user
    createdAt: '2023-10-16T16:52:14.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
          fullname: Gevin
          isHf: false
          isPro: false
          name: gxxxz
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-10-16T16:52:34.950Z'
      numEdits: 0
      reactions: []
    id: 652d6a3ed838856129e50ca0
    type: comment
  author: gxxxz
  content: This comment has been hidden
  created_at: 2023-10-16 15:52:14+00:00
  edited: true
  hidden: true
  id: 652d6a3ed838856129e50ca0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
      fullname: Gevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gxxxz
      type: user
    createdAt: '2023-10-16T16:52:45.000Z'
    data:
      edited: false
      editors:
      - gxxxz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.19450727105140686
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
          fullname: Gevin
          isHf: false
          isPro: false
          name: gxxxz
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;808Code&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/808Code\">@<span class=\"\
          underline\">808Code</span></a></span>\n\n\t</span></span> Tysm</p>\n"
        raw: '@808Code Tysm


          '
        updatedAt: '2023-10-16T16:52:45.484Z'
      numEdits: 0
      reactions: []
    id: 652d6a5d10874027ce6bacd2
    type: comment
  author: gxxxz
  content: '@808Code Tysm


    '
  created_at: 2023-10-16 15:52:45+00:00
  edited: false
  hidden: false
  id: 652d6a5d10874027ce6bacd2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
      fullname: Gevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gxxxz
      type: user
    createdAt: '2023-10-16T16:57:30.000Z'
    data:
      edited: false
      editors:
      - gxxxz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8055241107940674
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
          fullname: Gevin
          isHf: false
          isPro: false
          name: gxxxz
          type: user
        html: '<p>Btw for future peeps, below also sets the context inside the tags.
          </p>

          <p> {"role": "system", "content": "You are 8 years old"},<br> {"role": "user",
          "content": "How old are you?"},</p>

          '
        raw: "Btw for future peeps, below also sets the context inside the tags. \n\
          \n {\"role\": \"system\", \"content\": \"You are 8 years old\"},\n {\"role\"\
          : \"user\", \"content\": \"How old are you?\"},\n"
        updatedAt: '2023-10-16T16:57:30.533Z'
      numEdits: 0
      reactions: []
    id: 652d6b7ac3424254ba627956
    type: comment
  author: gxxxz
  content: "Btw for future peeps, below also sets the context inside the tags. \n\n\
    \ {\"role\": \"system\", \"content\": \"You are 8 years old\"},\n {\"role\": \"\
    user\", \"content\": \"How old are you?\"},\n"
  created_at: 2023-10-16 15:57:30+00:00
  edited: false
  hidden: false
  id: 652d6b7ac3424254ba627956
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
      fullname: Matthew Carrigan
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rocketknight1
      type: user
    createdAt: '2023-10-16T18:16:32.000Z'
    data:
      edited: false
      editors:
      - Rocketknight1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8534248471260071
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
          fullname: Matthew Carrigan
          isHf: true
          isPro: false
          name: Rocketknight1
          type: user
        html: '<p>You can try something like this:</p>

          <pre><code class="language-python">messages = [

          {<span class="hljs-string">"role"</span>: <span class="hljs-string">"system"</span>,
          <span class="hljs-string">"content"</span>: <span class="hljs-string">"You
          are a helpful bot who reads texts and answers questions about them."</span>},

          {<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>,
          <span class="hljs-string">"content"</span>: <span class="hljs-string">"[text]
          QUESTION: [question]"</span>},

          ]

          <span class="hljs-built_in">input</span> = tokenizer.apply_chat_template(messages)

          answer = model.generate(**{key: tensor.to(model.device) <span class="hljs-keyword">for</span>
          key, tensor <span class="hljs-keyword">in</span> <span class="hljs-built_in">input</span>.items()})

          </code></pre>

          <p>In general, there are lots of ways to do this and no single right answer
          - try using some of the tips from <a rel="nofollow" href="https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api">OpenAI''s
          prompt engineering handbook</a>, which also apply to other instruction-following
          models like Mistral-Instruct. Finally, you may have better luck with <a
          href="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha">Zephyr</a>,
          which is also based on Mistral-7B but was trained to follow instructions
          with more advanced methods.</p>

          '
        raw: 'You can try something like this:


          ```python

          messages = [

          {"role": "system", "content": "You are a helpful bot who reads texts and
          answers questions about them."},

          {"role": "user", "content": "[text] QUESTION: [question]"},

          ]

          input = tokenizer.apply_chat_template(messages)

          answer = model.generate(**{key: tensor.to(model.device) for key, tensor
          in input.items()})

          ```


          In general, there are lots of ways to do this and no single right answer
          - try using some of the tips from [OpenAI''s prompt engineering handbook](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api),
          which also apply to other instruction-following models like Mistral-Instruct.
          Finally, you may have better luck with [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha),
          which is also based on Mistral-7B but was trained to follow instructions
          with more advanced methods.'
        updatedAt: '2023-10-16T18:16:32.447Z'
      numEdits: 0
      reactions: []
    id: 652d7e00244f88dd650cf456
    type: comment
  author: Rocketknight1
  content: 'You can try something like this:


    ```python

    messages = [

    {"role": "system", "content": "You are a helpful bot who reads texts and answers
    questions about them."},

    {"role": "user", "content": "[text] QUESTION: [question]"},

    ]

    input = tokenizer.apply_chat_template(messages)

    answer = model.generate(**{key: tensor.to(model.device) for key, tensor in input.items()})

    ```


    In general, there are lots of ways to do this and no single right answer - try
    using some of the tips from [OpenAI''s prompt engineering handbook](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api),
    which also apply to other instruction-following models like Mistral-Instruct.
    Finally, you may have better luck with [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha),
    which is also based on Mistral-7B but was trained to follow instructions with
    more advanced methods.'
  created_at: 2023-10-16 17:16:32+00:00
  edited: false
  hidden: false
  id: 652d7e00244f88dd650cf456
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
      fullname: Gevin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gxxxz
      type: user
    createdAt: '2023-10-18T04:49:52.000Z'
    data:
      edited: false
      editors:
      - gxxxz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6801409125328064
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bcd3a1a23ca72ae659bcffb7d5b34762.svg
          fullname: Gevin
          isHf: false
          isPro: false
          name: gxxxz
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Rocketknight1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Rocketknight1\"\
          >@<span class=\"underline\">Rocketknight1</span></a></span>\n\n\t</span></span>\
          \ Thanks</p>\n"
        raw: '@Rocketknight1 Thanks'
        updatedAt: '2023-10-18T04:49:52.058Z'
      numEdits: 0
      reactions: []
    id: 652f63f017096ceb6beac184
    type: comment
  author: gxxxz
  content: '@Rocketknight1 Thanks'
  created_at: 2023-10-18 03:49:52+00:00
  edited: false
  hidden: false
  id: 652f63f017096ceb6beac184
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4bacb47c4495a9d4e17c83d446e6245b.svg
      fullname: Adam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adam12104
      type: user
    createdAt: '2023-11-09T00:10:56.000Z'
    data:
      edited: false
      editors:
      - adam12104
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6228980422019958
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4bacb47c4495a9d4e17c83d446e6245b.svg
          fullname: Adam
          isHf: false
          isPro: false
          name: adam12104
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Rocketknight1&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Rocketknight1\"\
          >@<span class=\"underline\">Rocketknight1</span></a></span>\n\n\t</span></span>\
          \ I tried your suggestion and got the following error jinja2.exceptions.TemplateError:\
          \ Conversation roles must alternate user/assistant/user/assistant/...</p>\n\
          <p>I've added my python code below to see if you can see anything out of\
          \ the ordinary with it.</p>\n<p>from transformers import AutoModelForCausalLM,\
          \ AutoTokenizer</p>\n<p>device = \"cpu\"  # Use GPU if available, otherwise\
          \ use CPU</p>\n<p>model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )<br>tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )</p>\n<p>messages = [<br>    {\"role\": \"system\", \"content\": \"You\
          \ are a hunan who loves to dance.\"},<br>    {\"role\": \"user\", \"content\"\
          : \"What do you like to do in your spare time?\"}<br>]</p>\n<p>encodeds\
          \ = tokenizer.apply_chat_template(messages)</p>\n<p>model_inputs = encodeds.to(device)<br>model.to(device)</p>\n\
          <p>generated_ids = model.generate(**{key: tensor.to(model.device) for key,\
          \ tensor in input.items()})<br>decoded = tokenizer.batch_decode(generated_ids)<br>print(decoded[0])</p>\n"
        raw: "@Rocketknight1 I tried your suggestion and got the following error jinja2.exceptions.TemplateError:\
          \ Conversation roles must alternate user/assistant/user/assistant/...\n\n\
          I've added my python code below to see if you can see anything out of the\
          \ ordinary with it.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \ndevice = \"cpu\"  # Use GPU if available, otherwise use CPU\n\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a hunan\
          \ who loves to dance.\"},\n    {\"role\": \"user\", \"content\": \"What\
          \ do you like to do in your spare time?\"}\n]\n\nencodeds = tokenizer.apply_chat_template(messages)\n\
          \nmodel_inputs = encodeds.to(device)\nmodel.to(device)\n\ngenerated_ids\
          \ = model.generate(**{key: tensor.to(model.device) for key, tensor in input.items()})\n\
          decoded = tokenizer.batch_decode(generated_ids)\nprint(decoded[0])\n\n"
        updatedAt: '2023-11-09T00:10:56.555Z'
      numEdits: 0
      reactions: []
    id: 654c239017d83697c7508043
    type: comment
  author: adam12104
  content: "@Rocketknight1 I tried your suggestion and got the following error jinja2.exceptions.TemplateError:\
    \ Conversation roles must alternate user/assistant/user/assistant/...\n\nI've\
    \ added my python code below to see if you can see anything out of the ordinary\
    \ with it.\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\
    device = \"cpu\"  # Use GPU if available, otherwise use CPU\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\")\n\nmessages = [\n    {\"role\": \"system\"\
    , \"content\": \"You are a hunan who loves to dance.\"},\n    {\"role\": \"user\"\
    , \"content\": \"What do you like to do in your spare time?\"}\n]\n\nencodeds\
    \ = tokenizer.apply_chat_template(messages)\n\nmodel_inputs = encodeds.to(device)\n\
    model.to(device)\n\ngenerated_ids = model.generate(**{key: tensor.to(model.device)\
    \ for key, tensor in input.items()})\ndecoded = tokenizer.batch_decode(generated_ids)\n\
    print(decoded[0])\n\n"
  created_at: 2023-11-09 00:10:56+00:00
  edited: false
  hidden: false
  id: 654c239017d83697c7508043
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
      fullname: Matthew Carrigan
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Rocketknight1
      type: user
    createdAt: '2023-11-09T14:27:01.000Z'
    data:
      edited: false
      editors:
      - Rocketknight1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9271047115325928
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1660312628256-60ba519750effef3a58beac3.png?w=200&h=200&f=face
          fullname: Matthew Carrigan
          isHf: true
          isPro: false
          name: Rocketknight1
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;adam12104&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/adam12104\"\
          >@<span class=\"underline\">adam12104</span></a></span>\n\n\t</span></span>\
          \ this is a known issue caused by your version of <code>jinja</code> being\
          \ out of date. Try <code>pip install --upgrade jinja2</code>. We'll be adding\
          \ version checks in the next update to <code>transformers</code> so that\
          \ this stops happening!</p>\n"
        raw: Hi @adam12104 this is a known issue caused by your version of `jinja`
          being out of date. Try `pip install --upgrade jinja2`. We'll be adding version
          checks in the next update to `transformers` so that this stops happening!
        updatedAt: '2023-11-09T14:27:01.966Z'
      numEdits: 0
      reactions: []
    id: 654cec35e6c577e11b443aa4
    type: comment
  author: Rocketknight1
  content: Hi @adam12104 this is a known issue caused by your version of `jinja` being
    out of date. Try `pip install --upgrade jinja2`. We'll be adding version checks
    in the next update to `transformers` so that this stops happening!
  created_at: 2023-11-09 14:27:01+00:00
  edited: false
  hidden: false
  id: 654cec35e6c577e11b443aa4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638ae4bcf6a0bc48582c4239/LlixeQbFoK0p47FQpa7lz.png?w=200&h=200&f=face
      fullname: Harper Carroll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: harpercarroll
      type: user
    createdAt: '2023-11-15T22:44:58.000Z'
    data:
      edited: false
      editors:
      - harpercarroll
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9655471444129944
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/638ae4bcf6a0bc48582c4239/LlixeQbFoK0p47FQpa7lz.png?w=200&h=200&f=face
          fullname: Harper Carroll
          isHf: false
          isPro: false
          name: harpercarroll
          type: user
        html: '<p>Hmm, still getting that error about how conversation roles must
          alternate, and it seems my jinja2 is up-to-date</p>

          '
        raw: Hmm, still getting that error about how conversation roles must alternate,
          and it seems my jinja2 is up-to-date
        updatedAt: '2023-11-15T22:44:58.288Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Gabriellamin
        - navidmadani
    id: 655549eabcf02b4c4c5f8617
    type: comment
  author: harpercarroll
  content: Hmm, still getting that error about how conversation roles must alternate,
    and it seems my jinja2 is up-to-date
  created_at: 2023-11-15 22:44:58+00:00
  edited: false
  hidden: false
  id: 655549eabcf02b4c4c5f8617
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/031abab70d382918bf07241d56b70b3b.svg
      fullname: Wahyu Adi Nugroho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wahyu-adi-n
      type: user
    createdAt: '2023-11-21T03:41:09.000Z'
    data:
      edited: false
      editors:
      - wahyu-adi-n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6343507170677185
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/031abab70d382918bf07241d56b70b3b.svg
          fullname: Wahyu Adi Nugroho
          isHf: false
          isPro: false
          name: wahyu-adi-n
          type: user
        html: '<p>You can follow these format on your chat template (user first, then
          followed by assistant): </p>

          <p>messages = [<br>    {"role": "user", "content": "Hey there!"},<br>    {"role":
          "assistant", "content": "Nice to meet you!"}<br>]</p>

          '
        raw: "You can follow these format on your chat template (user first, then\
          \ followed by assistant): \n\nmessages = [\n    {\"role\": \"user\", \"\
          content\": \"Hey there!\"},\n    {\"role\": \"assistant\", \"content\":\
          \ \"Nice to meet you!\"}\n]\n"
        updatedAt: '2023-11-21T03:41:09.224Z'
      numEdits: 0
      reactions: []
    id: 655c26d50c27b3fb70828afb
    type: comment
  author: wahyu-adi-n
  content: "You can follow these format on your chat template (user first, then followed\
    \ by assistant): \n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hey\
    \ there!\"},\n    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"\
    }\n]\n"
  created_at: 2023-11-21 03:41:09+00:00
  edited: false
  hidden: false
  id: 655c26d50c27b3fb70828afb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/29dbd2404547f3863cdb1f535fc3bf02.svg
      fullname: Atharva Mungee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atharvamungee10
      type: user
    createdAt: '2023-11-29T07:41:20.000Z'
    data:
      edited: false
      editors:
      - atharvamungee10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5540022253990173
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/29dbd2404547f3863cdb1f535fc3bf02.svg
          fullname: Atharva Mungee
          isHf: false
          isPro: false
          name: atharvamungee10
          type: user
        html: '<p>My prompt is<br> human_input = B_INST + f"""<br>        Reminder:
          {reminder}<br>        Process steps: {process_steps}<br>        Query: {query}<br>        Answer:<br>        """
          + E_INST<br>And my chatprompttemplate in langchain is<br>prompt = ChatPromptTemplate.from_messages([<br>    SystemMessage(content=system_template),  #
          The persistent system prompt<br>    MessagesPlaceholder(variable_name="chat_history"),  #
          Where the memory will be stored.<br>    ChatPromptTemplate.from_template("{human_input}"),  #
          Where the human input will be injected<br>])</p>

          <p>The response it generates has a prefix of AI: AI:<br>When in a conversation
          session<br>Let us say i have passed in 5 queries the answer to the 5th query
          will have a prefix of AI: AI: AI: AI: AI:</p>

          <p>Is this a prompt problem?</p>

          '
        raw: "My prompt is \n human_input = B_INST + f\"\"\"\n        Reminder: {reminder}\n\
          \        Process steps: {process_steps}\n        Query: {query}\n      \
          \  Answer:<Insert your answer here>\n        \"\"\" + E_INST\nAnd my chatprompttemplate\
          \ in langchain is \nprompt = ChatPromptTemplate.from_messages([\n    SystemMessage(content=system_template),\
          \  # The persistent system prompt\n    MessagesPlaceholder(variable_name=\"\
          chat_history\"),  # Where the memory will be stored.\n    ChatPromptTemplate.from_template(\"\
          {human_input}\"),  # Where the human input will be injected\n])\n\n\nThe\
          \ response it generates has a prefix of AI: AI: \nWhen in a conversation\
          \ session \nLet us say i have passed in 5 queries the answer to the 5th\
          \ query will have a prefix of AI: AI: AI: AI: AI:\n\nIs this a prompt problem?\n"
        updatedAt: '2023-11-29T07:41:20.052Z'
      numEdits: 0
      reactions: []
    id: 6566eb20d4b0cda0f46c702c
    type: comment
  author: atharvamungee10
  content: "My prompt is \n human_input = B_INST + f\"\"\"\n        Reminder: {reminder}\n\
    \        Process steps: {process_steps}\n        Query: {query}\n        Answer:<Insert\
    \ your answer here>\n        \"\"\" + E_INST\nAnd my chatprompttemplate in langchain\
    \ is \nprompt = ChatPromptTemplate.from_messages([\n    SystemMessage(content=system_template),\
    \  # The persistent system prompt\n    MessagesPlaceholder(variable_name=\"chat_history\"\
    ),  # Where the memory will be stored.\n    ChatPromptTemplate.from_template(\"\
    {human_input}\"),  # Where the human input will be injected\n])\n\n\nThe response\
    \ it generates has a prefix of AI: AI: \nWhen in a conversation session \nLet\
    \ us say i have passed in 5 queries the answer to the 5th query will have a prefix\
    \ of AI: AI: AI: AI: AI:\n\nIs this a prompt problem?\n"
  created_at: 2023-11-29 07:41:20+00:00
  edited: false
  hidden: false
  id: 6566eb20d4b0cda0f46c702c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5037116d5c1097769cf4a5fa982a106d.svg
      fullname: Navid Madani
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navidmadani
      type: user
    createdAt: '2023-12-19T21:28:19.000Z'
    data:
      edited: true
      editors:
      - navidmadani
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8463112115859985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5037116d5c1097769cf4a5fa982a106d.svg
          fullname: Navid Madani
          isHf: false
          isPro: false
          name: navidmadani
          type: user
        html: '<p>I run into the same problem as well. The problem is with the system
          message there. When I remove the system message I won''t get the <code>jinja2.exceptions.TemplateError:
          Conversation roles must alternate user/assistant/user/assistant/...</code>
          error anymore..</p>

          <p>[UPDATE]<br>okay, according to the tokenizer config <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json">here</a>,
          the chat template does not  support system messages. So I''m wondering how
          the model was trained for supporting system messages and how should we use
          it?</p>

          '
        raw: 'I run into the same problem as well. The problem is with the system
          message there. When I remove the system message I won''t get the `jinja2.exceptions.TemplateError:
          Conversation roles must alternate user/assistant/user/assistant/...` error
          anymore..


          [UPDATE]

          okay, according to the tokenizer config [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json),
          the chat template does not  support system messages. So I''m wondering how
          the model was trained for supporting system messages and how should we use
          it?'
        updatedAt: '2023-12-19T21:45:16.626Z'
      numEdits: 2
      reactions: []
    id: 65820af3d73d6402f79f17e4
    type: comment
  author: navidmadani
  content: 'I run into the same problem as well. The problem is with the system message
    there. When I remove the system message I won''t get the `jinja2.exceptions.TemplateError:
    Conversation roles must alternate user/assistant/user/assistant/...` error anymore..


    [UPDATE]

    okay, according to the tokenizer config [here](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/blob/main/tokenizer_config.json),
    the chat template does not  support system messages. So I''m wondering how the
    model was trained for supporting system messages and how should we use it?'
  created_at: 2023-12-19 21:28:19+00:00
  edited: true
  hidden: false
  id: 65820af3d73d6402f79f17e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
      fullname: coyote79
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyt79
      type: user
    createdAt: '2024-01-04T13:43:12.000Z'
    data:
      edited: false
      editors:
      - cyt79
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9504422545433044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3ac32bd6191b55bc71d2d6294f000a39.svg
          fullname: coyote79
          isHf: false
          isPro: false
          name: cyt79
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;navidmadani&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/navidmadani\"\
          >@<span class=\"underline\">navidmadani</span></a></span>\n\n\t</span></span>\
          \ Did you find an answer to your question?</p>\n"
        raw: '@navidmadani Did you find an answer to your question?

          '
        updatedAt: '2024-01-04T13:43:12.846Z'
      numEdits: 0
      reactions: []
    id: 6596b5f072fce9e0665a9e37
    type: comment
  author: cyt79
  content: '@navidmadani Did you find an answer to your question?

    '
  created_at: 2024-01-04 13:43:12+00:00
  edited: false
  hidden: false
  id: 6596b5f072fce9e0665a9e37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
      fullname: Celso F
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: celsowm
      type: user
    createdAt: '2024-01-11T14:28:42.000Z'
    data:
      edited: true
      editors:
      - celsowm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.1446848064661026
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a278c3f30c4642278d4259/W0U2_asElVWplHF6sLsDf.png?w=200&h=200&f=face
          fullname: Celso F
          isHf: false
          isPro: false
          name: celsowm
          type: user
        html: "<p>this worked for me:</p>\n<pre><code>messages = [\n    {\n      \
          \  \"role\": \"user\",\n        \"content\": \"You are a chatbot who always\
          \ responds in Portuguese\",\n    },\n    {\n        \"role\": \"assistant\"\
          ,\n        \"content\": \"Entendido! Responderei sempre na l\xEDngua portuguesa!\"\
          ,\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Qual\
          \ \xE9 maior: sol ou a terra?\",\n    }\n]\n\nprint(tokenizer.apply_chat_template(messages,\
          \ tokenize=False))\n</code></pre>\n"
        raw: "this worked for me:\n\n```\nmessages = [\n    {\n        \"role\": \"\
          user\",\n        \"content\": \"You are a chatbot who always responds in\
          \ Portuguese\",\n    },\n    {\n        \"role\": \"assistant\",\n     \
          \   \"content\": \"Entendido! Responderei sempre na l\xEDngua portuguesa!\"\
          ,\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Qual\
          \ \xE9 maior: sol ou a terra?\",\n    }\n]\n\nprint(tokenizer.apply_chat_template(messages,\
          \ tokenize=False))\n```"
        updatedAt: '2024-01-11T14:29:13.415Z'
      numEdits: 1
      reactions: []
    id: 659ffb1a96cd935fabaa9e8d
    type: comment
  author: celsowm
  content: "this worked for me:\n\n```\nmessages = [\n    {\n        \"role\": \"\
    user\",\n        \"content\": \"You are a chatbot who always responds in Portuguese\"\
    ,\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"Entendido!\
    \ Responderei sempre na l\xEDngua portuguesa!\",\n    },\n    {\n        \"role\"\
    : \"user\",\n        \"content\": \"Qual \xE9 maior: sol ou a terra?\",\n    }\n\
    ]\n\nprint(tokenizer.apply_chat_template(messages, tokenize=False))\n```"
  created_at: 2024-01-11 14:28:42+00:00
  edited: true
  hidden: false
  id: 659ffb1a96cd935fabaa9e8d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
      fullname: Maziyar Panahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MaziyarPanahi
      type: user
    createdAt: '2024-01-12T08:33:50.000Z'
    data:
      edited: false
      editors:
      - MaziyarPanahi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5830569863319397
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png?w=200&h=200&f=face
          fullname: Maziyar Panahi
          isHf: false
          isPro: false
          name: MaziyarPanahi
          type: user
        html: "<p>This is what HuggingChat is using:</p>\n<pre><code>{\n    \"name\"\
          \ : \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    \"description\" : \"\
          The latest MoE model from Mistral AI! 8x7B and outperforms Llama 2 70B in\
          \ most benchmarks.\",\n    \"websiteUrl\" : \"https://mistral.ai/news/mixtral-of-experts/\"\
          ,\n    \"preprompt\" : \"\",\n    \"chatPromptTemplate\": \"&lt;s&gt; {{#each\
          \ messages}}{{#ifUser}}[INST]{{#if <span data-props=\"{&quot;user&quot;:&quot;first&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/first\"\
          >@<span class=\"underline\">first</span></a></span>\n\n\t</span></span>}}{{#if\
          \ <span data-props=\"{&quot;user&quot;:&quot;root&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/root\">@<span class=\"\
          underline\">root</span></a></span>\n\n\t</span></span>.preprompt}}{{@root.preprompt}}\\\
          n{{/if}}{{/if}} {{content}} [/INST]{{/ifUser}}{{#ifAssistant}} {{content}}&lt;/s&gt;\
          \ {{/ifAssistant}}{{/each}}\",\n    \"parameters\" : {\n      \"temperature\"\
          \ : 0.6,\n      \"top_p\" : 0.95,\n      \"repetition_penalty\" : 1.2,\n\
          \      \"top_k\" : 50,\n      \"truncate\" : 24576,\n      \"max_new_tokens\"\
          \ : 8192,\n      \"stop\" : [\"&lt;/s&gt;\"]\n    }\n</code></pre>\n<p>So\
          \ they consider the System Prompt as the first message separated via <code>\\\
          n</code>:</p>\n<pre><code>&lt;s&gt; [INST]Previous conversation context\
          \ or system prompt\n How's the weather today? [/INST] It's sunny and warm.\n\
          &lt;s&gt; [INST] What about tomorrow? [/INST] Tomorrow is expected to be\
          \ cloudy.&lt;/s&gt; \n</code></pre>\n<p>This is how it is in <code>text-generation-webui</code>,\
          \ which reuses the same thing from Mistral:</p>\n<pre><code>.*(mistral|mixtral).*instruct:\n\
          \  instruction_template: 'Mistral'\n</code></pre>\n<pre><code>instruction_template:\
          \ |-\n  {%- for message in messages %}\n      {%- if message['role'] ==\
          \ 'system' -%}\n          {{- message['content'] -}}\n      {%- else -%}\n\
          \          {%- if message['role'] == 'user' -%}\n              {{-' [INST]\
          \ ' + message['content'].rstrip() + ' [/INST] '-}}\n          {%- else -%}\n\
          \              {{-'' + message['content'] + '&lt;/s&gt;' -}}\n         \
          \ {%- endif -%}\n      {%- endif -%}\n  {%- endfor -%}\n  {%- if add_generation_prompt\
          \ -%}\n      {{-''-}}\n  {%- endif -%}\n</code></pre>\n<p>and some users\
          \ in <code>SillyTavern</code> are using the same Llama-2 format:</p>\n<pre><code>[INST]\
          \ &lt;&lt;SYS&gt;&gt;\nWrite character's next reply.\n&lt;&lt;/SYS&gt;&gt;\n\
          \nCharacter card\n&lt;/s&gt;&lt;s&gt;[INST] {prompt} [/INST] {response}\
          \ &lt;/s&gt;&lt;s&gt;[INST] {prompt} [/INST] etc.\n</code></pre>\n<p>I personally\
          \ tried all 3, in some cases I got better results with <code>Llama-2</code>\
          \ format for some reasons! I wish we had a good evaluation just for a System\
          \ Prompt to see which formats does a better job.</p>\n"
        raw: "This is what HuggingChat is using:\n\n\n```\n{\n    \"name\" : \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
          ,\n    \"description\" : \"The latest MoE model from Mistral AI! 8x7B and\
          \ outperforms Llama 2 70B in most benchmarks.\",\n    \"websiteUrl\" : \"\
          https://mistral.ai/news/mixtral-of-experts/\",\n    \"preprompt\" : \"\"\
          ,\n    \"chatPromptTemplate\": \"<s> {{#each messages}}{{#ifUser}}[INST]{{#if\
          \ @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\n{{/if}}{{/if}} {{content}}\
          \ [/INST]{{/ifUser}}{{#ifAssistant}} {{content}}</s> {{/ifAssistant}}{{/each}}\"\
          ,\n    \"parameters\" : {\n      \"temperature\" : 0.6,\n      \"top_p\"\
          \ : 0.95,\n      \"repetition_penalty\" : 1.2,\n      \"top_k\" : 50,\n\
          \      \"truncate\" : 24576,\n      \"max_new_tokens\" : 8192,\n      \"\
          stop\" : [\"</s>\"]\n    }\n```\nSo they consider the System Prompt as the\
          \ first message separated via `\\n`:\n\n```\n<s> [INST]Previous conversation\
          \ context or system prompt\n How's the weather today? [/INST] It's sunny\
          \ and warm.\n<s> [INST] What about tomorrow? [/INST] Tomorrow is expected\
          \ to be cloudy.</s> \n```\n\nThis is how it is in `text-generation-webui`,\
          \ which reuses the same thing from Mistral:\n\n```\n.*(mistral|mixtral).*instruct:\n\
          \  instruction_template: 'Mistral'\n```\n```\ninstruction_template: |-\n\
          \  {%- for message in messages %}\n      {%- if message['role'] == 'system'\
          \ -%}\n          {{- message['content'] -}}\n      {%- else -%}\n      \
          \    {%- if message['role'] == 'user' -%}\n              {{-' [INST] ' +\
          \ message['content'].rstrip() + ' [/INST] '-}}\n          {%- else -%}\n\
          \              {{-'' + message['content'] + '</s>' -}}\n          {%- endif\
          \ -%}\n      {%- endif -%}\n  {%- endfor -%}\n  {%- if add_generation_prompt\
          \ -%}\n      {{-''-}}\n  {%- endif -%}\n```\n\nand some users in `SillyTavern`\
          \ are using the same Llama-2 format:\n\n```\n[INST] <<SYS>>\nWrite character's\
          \ next reply.\n<</SYS>>\n\nCharacter card\n</s><s>[INST] {prompt} [/INST]\
          \ {response} </s><s>[INST] {prompt} [/INST] etc.\n```\n\nI personally tried\
          \ all 3, in some cases I got better results with `Llama-2` format for some\
          \ reasons! I wish we had a good evaluation just for a System Prompt to see\
          \ which formats does a better job."
        updatedAt: '2024-01-12T08:33:50.518Z'
      numEdits: 0
      reactions: []
    id: 65a0f96e90ea9d75eac61bb8
    type: comment
  author: MaziyarPanahi
  content: "This is what HuggingChat is using:\n\n\n```\n{\n    \"name\" : \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\
    ,\n    \"description\" : \"The latest MoE model from Mistral AI! 8x7B and outperforms\
    \ Llama 2 70B in most benchmarks.\",\n    \"websiteUrl\" : \"https://mistral.ai/news/mixtral-of-experts/\"\
    ,\n    \"preprompt\" : \"\",\n    \"chatPromptTemplate\": \"<s> {{#each messages}}{{#ifUser}}[INST]{{#if\
    \ @first}}{{#if @root.preprompt}}{{@root.preprompt}}\\n{{/if}}{{/if}} {{content}}\
    \ [/INST]{{/ifUser}}{{#ifAssistant}} {{content}}</s> {{/ifAssistant}}{{/each}}\"\
    ,\n    \"parameters\" : {\n      \"temperature\" : 0.6,\n      \"top_p\" : 0.95,\n\
    \      \"repetition_penalty\" : 1.2,\n      \"top_k\" : 50,\n      \"truncate\"\
    \ : 24576,\n      \"max_new_tokens\" : 8192,\n      \"stop\" : [\"</s>\"]\n  \
    \  }\n```\nSo they consider the System Prompt as the first message separated via\
    \ `\\n`:\n\n```\n<s> [INST]Previous conversation context or system prompt\n How's\
    \ the weather today? [/INST] It's sunny and warm.\n<s> [INST] What about tomorrow?\
    \ [/INST] Tomorrow is expected to be cloudy.</s> \n```\n\nThis is how it is in\
    \ `text-generation-webui`, which reuses the same thing from Mistral:\n\n```\n\
    .*(mistral|mixtral).*instruct:\n  instruction_template: 'Mistral'\n```\n```\n\
    instruction_template: |-\n  {%- for message in messages %}\n      {%- if message['role']\
    \ == 'system' -%}\n          {{- message['content'] -}}\n      {%- else -%}\n\
    \          {%- if message['role'] == 'user' -%}\n              {{-' [INST] ' +\
    \ message['content'].rstrip() + ' [/INST] '-}}\n          {%- else -%}\n     \
    \         {{-'' + message['content'] + '</s>' -}}\n          {%- endif -%}\n \
    \     {%- endif -%}\n  {%- endfor -%}\n  {%- if add_generation_prompt -%}\n  \
    \    {{-''-}}\n  {%- endif -%}\n```\n\nand some users in `SillyTavern` are using\
    \ the same Llama-2 format:\n\n```\n[INST] <<SYS>>\nWrite character's next reply.\n\
    <</SYS>>\n\nCharacter card\n</s><s>[INST] {prompt} [/INST] {response} </s><s>[INST]\
    \ {prompt} [/INST] etc.\n```\n\nI personally tried all 3, in some cases I got\
    \ better results with `Llama-2` format for some reasons! I wish we had a good\
    \ evaluation just for a System Prompt to see which formats does a better job."
  created_at: 2024-01-12 08:33:50+00:00
  edited: false
  hidden: false
  id: 65a0f96e90ea9d75eac61bb8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 49
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Prompt template for question answering
