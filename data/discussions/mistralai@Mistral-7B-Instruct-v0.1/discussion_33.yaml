!!python/object:huggingface_hub.community.DiscussionWithDetails
author: adityaasish
conflicting_files: null
created_at: 2023-10-02 15:33:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/163ba19b3a22b79466f9cbe1ced142a1.svg
      fullname: dharmala aditya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: adityaasish
      type: user
    createdAt: '2023-10-02T16:33:32.000Z'
    data:
      edited: false
      editors:
      - adityaasish
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6825643181800842
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/163ba19b3a22b79466f9cbe1ced142a1.svg
          fullname: dharmala aditya
          isHf: false
          isPro: false
          name: adityaasish
          type: user
        html: '<p>I am getting the below error while running the code </p>

          <p>RuntimeError: Failed to import transformers.models.mistral.modeling_mistral
          because of the following error (look up to see its traceback):<br>Failed
          to import transformers.generation.utils because of the following error (look
          up to see its traceback):<br>cannot import name ''DEFAULT_CIPHERS'' from
          ''urllib3.util.ssl_'' (/root/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py)</p>

          <p>code:<br>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>device = "cuda" # the device to load the model onto</p>

          <p>model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")<br>tokenizer
          = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")</p>

          <p>messages = [<br>    {"role": "user", "content": "What is your favourite
          condiment?"},<br>    {"role": "assistant", "content": "Well, I''m quite
          partial to a good squeeze of fresh lemon juice. It adds just the right amount
          of zesty flavour to whatever I''m cooking up in the kitchen!"},<br>    {"role":
          "user", "content": "Do you have mayonnaise recipes?"}<br>]</p>

          <p>encodeds = tokenizer.apply_chat_template(messages, return_tensors="pt")</p>

          <p>model_inputs = encodeds.to(device)<br>model.to(device)</p>

          <p>generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)<br>decoded
          = tokenizer.batch_decode(generated_ids)<br>print(decoded[0])</p>

          '
        raw: "I am getting the below error while running the code \r\n\r\nRuntimeError:\
          \ Failed to import transformers.models.mistral.modeling_mistral because\
          \ of the following error (look up to see its traceback):\r\nFailed to import\
          \ transformers.generation.utils because of the following error (look up\
          \ to see its traceback):\r\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_'\
          \ (/root/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py)\r\n\
          \r\n\r\ncode:\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
          \n\r\ndevice = \"cuda\" # the device to load the model onto\r\n\r\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\r\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\"\
          )\r\n\r\nmessages = [\r\n    {\"role\": \"user\", \"content\": \"What is\
          \ your favourite condiment?\"},\r\n    {\"role\": \"assistant\", \"content\"\
          : \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds\
          \ just the right amount of zesty flavour to whatever I'm cooking up in the\
          \ kitchen!\"},\r\n    {\"role\": \"user\", \"content\": \"Do you have mayonnaise\
          \ recipes?\"}\r\n]\r\n\r\nencodeds = tokenizer.apply_chat_template(messages,\
          \ return_tensors=\"pt\")\r\n\r\nmodel_inputs = encodeds.to(device)\r\nmodel.to(device)\r\
          \n\r\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000,\
          \ do_sample=True)\r\ndecoded = tokenizer.batch_decode(generated_ids)\r\n\
          print(decoded[0])\r\n"
        updatedAt: '2023-10-02T16:33:32.507Z'
      numEdits: 0
      reactions: []
    id: 651af0dc2590c4c6244f58c5
    type: comment
  author: adityaasish
  content: "I am getting the below error while running the code \r\n\r\nRuntimeError:\
    \ Failed to import transformers.models.mistral.modeling_mistral because of the\
    \ following error (look up to see its traceback):\r\nFailed to import transformers.generation.utils\
    \ because of the following error (look up to see its traceback):\r\ncannot import\
    \ name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' (/root/anaconda3/lib/python3.11/site-packages/urllib3/util/ssl_.py)\r\
    \n\r\n\r\ncode:\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\
    \n\r\ndevice = \"cuda\" # the device to load the model onto\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    mistralai/Mistral-7B-Instruct-v0.1\")\r\n\r\nmessages = [\r\n    {\"role\": \"\
    user\", \"content\": \"What is your favourite condiment?\"},\r\n    {\"role\"\
    : \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh\
    \ lemon juice. It adds just the right amount of zesty flavour to whatever I'm\
    \ cooking up in the kitchen!\"},\r\n    {\"role\": \"user\", \"content\": \"Do\
    \ you have mayonnaise recipes?\"}\r\n]\r\n\r\nencodeds = tokenizer.apply_chat_template(messages,\
    \ return_tensors=\"pt\")\r\n\r\nmodel_inputs = encodeds.to(device)\r\nmodel.to(device)\r\
    \n\r\ngenerated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\r\
    \ndecoded = tokenizer.batch_decode(generated_ids)\r\nprint(decoded[0])\r\n"
  created_at: 2023-10-02 15:33:32+00:00
  edited: false
  hidden: false
  id: 651af0dc2590c4c6244f58c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a0f7f6a327106967df207bf6fb9d379a.svg
      fullname: TG
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: QendelG
      type: user
    createdAt: '2023-10-05T05:14:36.000Z'
    data:
      edited: true
      editors:
      - QendelG
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6644287705421448
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a0f7f6a327106967df207bf6fb9d379a.svg
          fullname: TG
          isHf: false
          isPro: false
          name: QendelG
          type: user
        html: '<p>This might help: <a rel="nofollow" href="https://medium.com/@qendelai/bye-bye-llama-2-mistral-7b-is-taking-over-get-started-with-mistral-7b-instruct-1504ff5f373c">https://medium.com/@qendelai/bye-bye-llama-2-mistral-7b-is-taking-over-get-started-with-mistral-7b-instruct-1504ff5f373c</a></p>

          '
        raw: 'This might help: https://medium.com/@qendelai/bye-bye-llama-2-mistral-7b-is-taking-over-get-started-with-mistral-7b-instruct-1504ff5f373c'
        updatedAt: '2023-10-05T05:23:44.016Z'
      numEdits: 1
      reactions: []
    id: 651e463cb03d6d35bb4cc415
    type: comment
  author: QendelG
  content: 'This might help: https://medium.com/@qendelai/bye-bye-llama-2-mistral-7b-is-taking-over-get-started-with-mistral-7b-instruct-1504ff5f373c'
  created_at: 2023-10-05 04:14:36+00:00
  edited: true
  hidden: false
  id: 651e463cb03d6d35bb4cc415
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 33
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: error while running the model
