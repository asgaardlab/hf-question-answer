!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aiworld44
conflicting_files: null
created_at: 2023-09-29 19:48:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6809fc881781ecd53425e4434eb57c82.svg
      fullname: ai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aiworld44
      type: user
    createdAt: '2023-09-29T20:48:39.000Z'
    data:
      edited: false
      editors:
      - aiworld44
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5007999539375305
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6809fc881781ecd53425e4434eb57c82.svg
          fullname: ai
          isHf: false
          isPro: false
          name: aiworld44
          type: user
        html: '<p>I tried to do interpreter --local --model mistralai/Mistral-7B-Instruct-v0.1.
          didnt work </p>

          '
        raw: 'I tried to do interpreter --local --model mistralai/Mistral-7B-Instruct-v0.1.
          didnt work '
        updatedAt: '2023-09-29T20:48:39.660Z'
      numEdits: 0
      reactions: []
    id: 65173827a1a5e5d6177922cf
    type: comment
  author: aiworld44
  content: 'I tried to do interpreter --local --model mistralai/Mistral-7B-Instruct-v0.1.
    didnt work '
  created_at: 2023-09-29 19:48:39+00:00
  edited: false
  hidden: false
  id: 65173827a1a5e5d6177922cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3b86fd14515214fff62eb5be99c2a83a.svg
      fullname: Adam James
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stelarion
      type: user
    createdAt: '2023-09-30T02:53:37.000Z'
    data:
      edited: true
      editors:
      - Stelarion
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6052649021148682
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3b86fd14515214fff62eb5be99c2a83a.svg
          fullname: Adam James
          isHf: false
          isPro: false
          name: Stelarion
          type: user
        html: "<p>Here is my code. You need to locally save the model in a subfolder\
          \ ( ./Mistral/ depending on your .py file)  </p>\n<p>It does work for 1-3\
          \ queries. Until it breaks down. As there is absofucking no documentation\
          \ of how to implement the workflow of Interference to a local pipeline this\
          \ is the best I got. If people are interested in reverse engineering it.\
          \ Shot me a message.  As it stands now, this is an add front to promote\
          \ paid services, let's change that.</p>\n<pre><code>import gradio as gr\n\
          from transformers import pipeline, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"\
          ./Mistral/\")\npipe = pipeline(\"text-generation\", model=\"./Mistral/\"\
          , max_new_tokens=512)\n\nchat_history_tokens = []\n\ndef generate(chatlog,\
          \ is_finished):\n    global chat_history_tokens\n\n    # Get the latest\
          \ message from chat\n    new_message = chatlog[-1]['content'] if isinstance(chatlog,\
          \ list) else chatlog\n\n    # Tokenize new message and extend chat history\n\
          \    new_message_tokens = tokenizer.encode(new_message, add_special_tokens=False)\n\
          \    chat_history_tokens = new_message_tokens  # We only keep the last message\
          \ now\n\n    # Decode tokens to string for the prompt\n    prompt = tokenizer.decode(chat_history_tokens)\n\
          \    \n    try:\n        print(\"Debug: Sending this prompt to the model:\"\
          , prompt)\n        outputs = pipe(prompt, pad_token_id=tokenizer.eos_token_id)\n\
          \        print(\"Debug: Model's raw output:\", outputs)\n\n        # Cleanup\
          \ the generated text\n        generated_text = outputs[0]['generated_text'].replace(prompt,\
          \ \"\").strip()\n        generated_text = generated_text.replace(\"Answer:\"\
          , \"\").replace(\"A:\", \"\").strip()\n\n        print(\"Debug: Generated\
          \ Text After Cleanup:\", generated_text)\n\n        # Tokenize the model's\
          \ reply and add it to the history\n        bot_reply_tokens = tokenizer.encode(generated_text,\
          \ add_special_tokens=False)\n        chat_history_tokens.extend(bot_reply_tokens)\n\
          \n    except Exception as e:\n        print(\"Debug: Caught an exception:\"\
          , str(e))\n        return str(e)\n\n    return generated_text\n\niface =\
          \ gr.ChatInterface(fn=generate)\niface.launch()\n</code></pre>\n"
        raw: "Here is my code. You need to locally save the model in a subfolder (\
          \ ./Mistral/ depending on your .py file)  \n\nIt does work for 1-3 queries.\
          \ Until it breaks down. As there is absofucking no documentation of how\
          \ to implement the workflow of Interference to a local pipeline this is\
          \ the best I got. If people are interested in reverse engineering it. Shot\
          \ me a message.  As it stands now, this is an add front to promote paid\
          \ services, let's change that.\n\n\timport gradio as gr\n\tfrom transformers\
          \ import pipeline, AutoTokenizer\n\t\n\ttokenizer = AutoTokenizer.from_pretrained(\"\
          ./Mistral/\")\n\tpipe = pipeline(\"text-generation\", model=\"./Mistral/\"\
          , max_new_tokens=512)\n\t\n\tchat_history_tokens = []\n\t\n\tdef generate(chatlog,\
          \ is_finished):\n\t\tglobal chat_history_tokens\n\t\n\t\t# Get the latest\
          \ message from chat\n\t\tnew_message = chatlog[-1]['content'] if isinstance(chatlog,\
          \ list) else chatlog\n\t\n\t\t# Tokenize new message and extend chat history\n\
          \t\tnew_message_tokens = tokenizer.encode(new_message, add_special_tokens=False)\n\
          \t\tchat_history_tokens = new_message_tokens  # We only keep the last message\
          \ now\n\t\n\t\t# Decode tokens to string for the prompt\n\t\tprompt = tokenizer.decode(chat_history_tokens)\n\
          \t\t\n\t\ttry:\n\t\t\tprint(\"Debug: Sending this prompt to the model:\"\
          , prompt)\n\t\t\toutputs = pipe(prompt, pad_token_id=tokenizer.eos_token_id)\n\
          \t\t\tprint(\"Debug: Model's raw output:\", outputs)\n\t\n\t\t\t# Cleanup\
          \ the generated text\n\t\t\tgenerated_text = outputs[0]['generated_text'].replace(prompt,\
          \ \"\").strip()\n\t\t\tgenerated_text = generated_text.replace(\"Answer:\"\
          , \"\").replace(\"A:\", \"\").strip()\n\t\n\t\t\tprint(\"Debug: Generated\
          \ Text After Cleanup:\", generated_text)\n\t\n\t\t\t# Tokenize the model's\
          \ reply and add it to the history\n\t\t\tbot_reply_tokens = tokenizer.encode(generated_text,\
          \ add_special_tokens=False)\n\t\t\tchat_history_tokens.extend(bot_reply_tokens)\n\
          \t\n\t\texcept Exception as e:\n\t\t\tprint(\"Debug: Caught an exception:\"\
          , str(e))\n\t\t\treturn str(e)\n\t\n\t\treturn generated_text\n\t\n\tiface\
          \ = gr.ChatInterface(fn=generate)\n\tiface.launch()"
        updatedAt: '2023-09-30T03:04:26.624Z'
      numEdits: 5
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Roul-JS
        - jinhowe
        - Elias2049
    id: 65178db1335b4c728b3fcca3
    type: comment
  author: Stelarion
  content: "Here is my code. You need to locally save the model in a subfolder ( ./Mistral/\
    \ depending on your .py file)  \n\nIt does work for 1-3 queries. Until it breaks\
    \ down. As there is absofucking no documentation of how to implement the workflow\
    \ of Interference to a local pipeline this is the best I got. If people are interested\
    \ in reverse engineering it. Shot me a message.  As it stands now, this is an\
    \ add front to promote paid services, let's change that.\n\n\timport gradio as\
    \ gr\n\tfrom transformers import pipeline, AutoTokenizer\n\t\n\ttokenizer = AutoTokenizer.from_pretrained(\"\
    ./Mistral/\")\n\tpipe = pipeline(\"text-generation\", model=\"./Mistral/\", max_new_tokens=512)\n\
    \t\n\tchat_history_tokens = []\n\t\n\tdef generate(chatlog, is_finished):\n\t\t\
    global chat_history_tokens\n\t\n\t\t# Get the latest message from chat\n\t\tnew_message\
    \ = chatlog[-1]['content'] if isinstance(chatlog, list) else chatlog\n\t\n\t\t\
    # Tokenize new message and extend chat history\n\t\tnew_message_tokens = tokenizer.encode(new_message,\
    \ add_special_tokens=False)\n\t\tchat_history_tokens = new_message_tokens  # We\
    \ only keep the last message now\n\t\n\t\t# Decode tokens to string for the prompt\n\
    \t\tprompt = tokenizer.decode(chat_history_tokens)\n\t\t\n\t\ttry:\n\t\t\tprint(\"\
    Debug: Sending this prompt to the model:\", prompt)\n\t\t\toutputs = pipe(prompt,\
    \ pad_token_id=tokenizer.eos_token_id)\n\t\t\tprint(\"Debug: Model's raw output:\"\
    , outputs)\n\t\n\t\t\t# Cleanup the generated text\n\t\t\tgenerated_text = outputs[0]['generated_text'].replace(prompt,\
    \ \"\").strip()\n\t\t\tgenerated_text = generated_text.replace(\"Answer:\", \"\
    \").replace(\"A:\", \"\").strip()\n\t\n\t\t\tprint(\"Debug: Generated Text After\
    \ Cleanup:\", generated_text)\n\t\n\t\t\t# Tokenize the model's reply and add\
    \ it to the history\n\t\t\tbot_reply_tokens = tokenizer.encode(generated_text,\
    \ add_special_tokens=False)\n\t\t\tchat_history_tokens.extend(bot_reply_tokens)\n\
    \t\n\t\texcept Exception as e:\n\t\t\tprint(\"Debug: Caught an exception:\", str(e))\n\
    \t\t\treturn str(e)\n\t\n\t\treturn generated_text\n\t\n\tiface = gr.ChatInterface(fn=generate)\n\
    \tiface.launch()"
  created_at: 2023-09-30 01:53:37+00:00
  edited: true
  hidden: false
  id: 65178db1335b4c728b3fcca3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: mistralai/Mistral-7B-Instruct-v0.1
repo_type: model
status: open
target_branch: null
title: Does it work with local open interpreter, and how many gigs of ram is required?
