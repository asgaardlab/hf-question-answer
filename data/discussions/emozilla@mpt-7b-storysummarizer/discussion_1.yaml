!!python/object:huggingface_hub.community.DiscussionWithDetails
author: odellus
conflicting_files: null
created_at: 2023-06-22 08:20:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-06-22T09:20:56.000Z'
    data:
      edited: false
      editors:
      - odellus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.907641589641571
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
          fullname: Thomas Wood
          isHf: false
          isPro: true
          name: odellus
          type: user
        html: "<p>I fine tuned mpt-7b-storysummarizer on my own dataset and I was\
          \ wondering how I'm supposed to load the model back in after saving. The\
          \ contents of the saved directory are the folowing</p>\n<pre><code>    config.json\n\
          \    training_args.bin\n    generation_config.json\n    pytorch_model.bin\n\
          </code></pre>\n<p>Which isn't the standard output for QLORA adapaters, so\
          \ when I try to use the standard method for QLORA models</p>\n<pre><code\
          \ class=\"language-python\">adapter_name = &lt;path_to_saved_model_dir&gt;\n\
          tokenizer, model = load_model() <span class=\"hljs-comment\"># I've got\
          \ it loading successfully</span>\nmodel =  PeftModel.from_pretrained(model,\
          \ adapter_name)\n</code></pre>\n<p>of course it complains <code>adapter_config.json</code>\
          \ is not there. </p>\n<p>So how do I load the fine tuned model I saved?\
          \ Loading them like a normal huggingface model doesn't work any better.\
          \ Any help would be most greatly appreciated.</p>\n"
        raw: "I fine tuned mpt-7b-storysummarizer on my own dataset and I was wondering\
          \ how I'm supposed to load the model back in after saving. The contents\
          \ of the saved directory are the folowing\r\n```\r\n    config.json\r\n\
          \    training_args.bin\r\n    generation_config.json\r\n    pytorch_model.bin\r\
          \n```\r\nWhich isn't the standard output for QLORA adapaters, so when I\
          \ try to use the standard method for QLORA models\r\n```python\r\nadapter_name\
          \ = <path_to_saved_model_dir>\r\ntokenizer, model = load_model() # I've\
          \ got it loading successfully\r\nmodel =  PeftModel.from_pretrained(model,\
          \ adapter_name)\r\n```\r\nof course it complains `adapter_config.json` is\
          \ not there. \r\n\r\nSo how do I load the fine tuned model I saved? Loading\
          \ them like a normal huggingface model doesn't work any better. Any help\
          \ would be most greatly appreciated."
        updatedAt: '2023-06-22T09:20:56.048Z'
      numEdits: 0
      reactions: []
    id: 6494127841504a6d5980a676
    type: comment
  author: odellus
  content: "I fine tuned mpt-7b-storysummarizer on my own dataset and I was wondering\
    \ how I'm supposed to load the model back in after saving. The contents of the\
    \ saved directory are the folowing\r\n```\r\n    config.json\r\n    training_args.bin\r\
    \n    generation_config.json\r\n    pytorch_model.bin\r\n```\r\nWhich isn't the\
    \ standard output for QLORA adapaters, so when I try to use the standard method\
    \ for QLORA models\r\n```python\r\nadapter_name = <path_to_saved_model_dir>\r\n\
    tokenizer, model = load_model() # I've got it loading successfully\r\nmodel =\
    \  PeftModel.from_pretrained(model, adapter_name)\r\n```\r\nof course it complains\
    \ `adapter_config.json` is not there. \r\n\r\nSo how do I load the fine tuned\
    \ model I saved? Loading them like a normal huggingface model doesn't work any\
    \ better. Any help would be most greatly appreciated."
  created_at: 2023-06-22 08:20:56+00:00
  edited: false
  hidden: false
  id: 6494127841504a6d5980a676
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
      fullname: Thomas Wood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: odellus
      type: user
    createdAt: '2023-06-22T09:55:23.000Z'
    data:
      edited: false
      editors:
      - odellus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8649829626083374
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6090856b1e62cfa4f5c23ccb/XtetET8dL65viJDOKIWzl.jpeg?w=200&h=200&f=face
          fullname: Thomas Wood
          isHf: false
          isPro: true
          name: odellus
          type: user
        html: '<h1 id="update">Update</h1>

          <p>I''m using this to load in the saved model</p>

          <pre><code class="language-python"><span class="hljs-keyword">import</span>
          torch

          <span class="hljs-keyword">from</span> my_package <span class="hljs-keyword">import</span>
          load_model

          tokenizer, model = load_model()

          state_dict = torch.load(<span class="hljs-string">''/path/to/saved/pytorch_model.bin''</span>)

          model.load_state_dict(state_dict)

          <span class="hljs-comment"># Do a bunch of inference now</span>

          </code></pre>

          <p><strong>but</strong> something seems to be broken because the weights
          from my saved model cause immediate termination. Like it just generates
          EOS and that''s it. Is this the correct way to load in the saved weights?
          Feels like a hack as I''ve always used some form of from_pretrained out
          of transformers.</p>

          '
        raw: '# Update

          I''m using this to load in the saved model

          ```python

          import torch

          from my_package import load_model

          tokenizer, model = load_model()

          state_dict = torch.load(''/path/to/saved/pytorch_model.bin'')

          model.load_state_dict(state_dict)

          # Do a bunch of inference now

          ```

          **but** something seems to be broken because the weights from my saved model
          cause immediate termination. Like it just generates EOS and that''s it.
          Is this the correct way to load in the saved weights? Feels like a hack
          as I''ve always used some form of from_pretrained out of transformers.


          '
        updatedAt: '2023-06-22T09:55:23.272Z'
      numEdits: 0
      reactions: []
    id: 64941a8b6c7aff63cc13bfab
    type: comment
  author: odellus
  content: '# Update

    I''m using this to load in the saved model

    ```python

    import torch

    from my_package import load_model

    tokenizer, model = load_model()

    state_dict = torch.load(''/path/to/saved/pytorch_model.bin'')

    model.load_state_dict(state_dict)

    # Do a bunch of inference now

    ```

    **but** something seems to be broken because the weights from my saved model cause
    immediate termination. Like it just generates EOS and that''s it. Is this the
    correct way to load in the saved weights? Feels like a hack as I''ve always used
    some form of from_pretrained out of transformers.


    '
  created_at: 2023-06-22 08:55:23+00:00
  edited: false
  hidden: false
  id: 64941a8b6c7aff63cc13bfab
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: emozilla/mpt-7b-storysummarizer
repo_type: model
status: open
target_branch: null
title: How to load saved model after fine tuning
