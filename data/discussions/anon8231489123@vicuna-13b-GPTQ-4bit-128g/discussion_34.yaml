!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ltnn1
conflicting_files: null
created_at: 2023-04-14 20:54:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
      fullname: Ngon Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltnn1
      type: user
    createdAt: '2023-04-14T21:54:55.000Z'
    data:
      edited: true
      editors:
      - ltnn1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
          fullname: Ngon Nguyen
          isHf: false
          isPro: false
          name: ltnn1
          type: user
        html: '<p>I''ve been banging my head against the wall for the last few days
          trying to get this to work on Linux, so I wrote this guide that fellow newbs
          don''t have to go through what I went through. My distro is Arch (btw) but
          the guide itself is distro-agnostic and I believe it can even work on MacOS
          or Windows WSL2 (do try and let me know if it does). </p>

          <ol>

          <li>Install conda (or microconda, or mamba, or micromamba, choose your poison).
          </li>

          <li><code>conda create -n vicuna-matata pytorch torchvision torchaudio pytorch-cuda=11.7
          cuda-toolkit -c ''nvidia/label/cuda-11.7.0'' -c pytorch -c nvidia</code><br><code>conda
          activate vicuna-matata</code></li>

          <li><code>git clone https://github.com/oobabooga/text-generation-webui</code><br><code>cd
          text-generation-webui</code><br><code>pip install -r requirements.txt </code></li>

          <li><code>python download-model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g</code></li>

          <li><code>mkdir repositories</code><br><code>cd repositories</code><br><code>git
          clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda</code><br><code>cd
          GPTQ-for-LLaMa</code><br><code>pip install -r requirements.txt </code><br><code>python
          setup_cuda.py install</code></li>

          <li>Go back to text-generation-webui</li>

          <li><code>python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g
          --auto-devices --wbits 4 --groupsize 128 --chat</code></li>

          </ol>

          <p>I go into a bit more detail, giving some tips, and a debug at the end
          in <a rel="nofollow" href="https://github.com/ltngonnguyen/vicuna-matata">this
          more detailed version of the guide</a> so that I won''t clutter it up here.
          Let me know if you have any questions.</p>

          '
        raw: "I've been banging my head against the wall for the last few days trying\
          \ to get this to work on Linux, so I wrote this guide that fellow newbs\
          \ don't have to go through what I went through. My distro is Arch (btw)\
          \ but the guide itself is distro-agnostic and I believe it can even work\
          \ on MacOS or Windows WSL2 (do try and let me know if it does). \n\n1. Install\
          \ conda (or microconda, or mamba, or micromamba, choose your poison). \n\
          2. `conda create -n vicuna-matata pytorch torchvision torchaudio pytorch-cuda=11.7\
          \ cuda-toolkit -c 'nvidia/label/cuda-11.7.0' -c pytorch -c nvidia`\n`conda\
          \ activate vicuna-matata`\n3. `git clone https://github.com/oobabooga/text-generation-webui`\n\
          `cd text-generation-webui`\n`pip install -r requirements.txt `\n4. `python\
          \ download-model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g`\n5. `mkdir\
          \ repositories`\n`cd repositories`\n`git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git\
          \ -b cuda`\n`cd GPTQ-for-LLaMa`\n`pip install -r requirements.txt `\n`python\
          \ setup_cuda.py install`\n6. Go back to text-generation-webui\n7. `python\
          \ server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g --auto-devices\
          \ --wbits 4 --groupsize 128 --chat`\n\nI go into a bit more detail, giving\
          \ some tips, and a debug at the end in [this more detailed version of the\
          \ guide](https://github.com/ltngonnguyen/vicuna-matata) so that I won't\
          \ clutter it up here. Let me know if you have any questions."
        updatedAt: '2023-04-16T20:08:47.493Z'
      numEdits: 2
      reactions:
      - count: 5
        reaction: "\U0001F91D"
        users:
        - Fire-Hound
        - easonruan
        - LaferriereJC
        - reipared
        - EatEmAll
      - count: 1
        reaction: "\U0001F44D"
        users:
        - stoneball
    id: 6439cbafeb7c5616ef417aac
    type: comment
  author: ltnn1
  content: "I've been banging my head against the wall for the last few days trying\
    \ to get this to work on Linux, so I wrote this guide that fellow newbs don't\
    \ have to go through what I went through. My distro is Arch (btw) but the guide\
    \ itself is distro-agnostic and I believe it can even work on MacOS or Windows\
    \ WSL2 (do try and let me know if it does). \n\n1. Install conda (or microconda,\
    \ or mamba, or micromamba, choose your poison). \n2. `conda create -n vicuna-matata\
    \ pytorch torchvision torchaudio pytorch-cuda=11.7 cuda-toolkit -c 'nvidia/label/cuda-11.7.0'\
    \ -c pytorch -c nvidia`\n`conda activate vicuna-matata`\n3. `git clone https://github.com/oobabooga/text-generation-webui`\n\
    `cd text-generation-webui`\n`pip install -r requirements.txt `\n4. `python download-model.py\
    \ anon8231489123/vicuna-13b-GPTQ-4bit-128g`\n5. `mkdir repositories`\n`cd repositories`\n\
    `git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda`\n`cd GPTQ-for-LLaMa`\n\
    `pip install -r requirements.txt `\n`python setup_cuda.py install`\n6. Go back\
    \ to text-generation-webui\n7. `python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g\
    \ --auto-devices --wbits 4 --groupsize 128 --chat`\n\nI go into a bit more detail,\
    \ giving some tips, and a debug at the end in [this more detailed version of the\
    \ guide](https://github.com/ltngonnguyen/vicuna-matata) so that I won't clutter\
    \ it up here. Let me know if you have any questions."
  created_at: 2023-04-14 20:54:55+00:00
  edited: true
  hidden: false
  id: 6439cbafeb7c5616ef417aac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a8062a5858f939ba72e2a031993063e.svg
      fullname: Vikram Cothur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fire-Hound
      type: user
    createdAt: '2023-04-15T11:56:54.000Z'
    data:
      edited: false
      editors:
      - Fire-Hound
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a8062a5858f939ba72e2a031993063e.svg
          fullname: Vikram Cothur
          isHf: false
          isPro: false
          name: Fire-Hound
          type: user
        html: '<p>Small correction:</p>

          <blockquote>

          <p>python download_model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g</p>

          </blockquote>

          <p>This should be download-model.py</p>

          '
        raw: 'Small correction:

          > python download_model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g


          This should be download-model.py'
        updatedAt: '2023-04-15T11:56:54.439Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - ltnn1
    id: 643a9106e756b67eee1c5573
    type: comment
  author: Fire-Hound
  content: 'Small correction:

    > python download_model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g


    This should be download-model.py'
  created_at: 2023-04-15 10:56:54+00:00
  edited: false
  hidden: false
  id: 643a9106e756b67eee1c5573
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
      fullname: Ngon Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltnn1
      type: user
    createdAt: '2023-04-15T13:34:34.000Z'
    data:
      edited: false
      editors:
      - ltnn1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
          fullname: Ngon Nguyen
          isHf: false
          isPro: false
          name: ltnn1
          type: user
        html: '<blockquote>

          <p>Small correction:</p>

          <blockquote>

          <p>python download_model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g</p>

          </blockquote>

          <p>This should be download-model.py</p>

          </blockquote>

          <p>Fixed, thanks!</p>

          '
        raw: "> Small correction:\n> > python download_model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g\n\
          > \n> This should be download-model.py\n\nFixed, thanks!"
        updatedAt: '2023-04-15T13:34:34.455Z'
      numEdits: 0
      reactions: []
    id: 643aa7ea9f4e4abaf85155d8
    type: comment
  author: ltnn1
  content: "> Small correction:\n> > python download_model.py anon8231489123/vicuna-13b-GPTQ-4bit-128g\n\
    > \n> This should be download-model.py\n\nFixed, thanks!"
  created_at: 2023-04-15 12:34:34+00:00
  edited: false
  hidden: false
  id: 643aa7ea9f4e4abaf85155d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
      fullname: Ralph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: szeta
      type: user
    createdAt: '2023-04-16T15:39:15.000Z'
    data:
      edited: false
      editors:
      - szeta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
          fullname: Ralph
          isHf: false
          isPro: false
          name: szeta
          type: user
        html: '<p>Hi, I tried this - thanks for the preparation!<br>However, running
          into the following error:</p>

          <p>python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g --auto-devices
          --wbits 4 --groupsize 128 --chat<br>bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so<br>Loading
          anon8231489123/vicuna-13b-GPTQ-4bit-128g...<br>Could not find the quantized
          model in .pt or .safetensors format, exiting...</p>

          <p>When searching for it, it is here:</p>

          <p>find ./ -name "*safetensors"<br>./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors</p>

          <p>What might I be missing?</p>

          '
        raw: 'Hi, I tried this - thanks for the preparation!

          However, running into the following error:


          python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g --auto-devices
          --wbits 4 --groupsize 128 --chat

          bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so

          Loading anon8231489123/vicuna-13b-GPTQ-4bit-128g...

          Could not find the quantized model in .pt or .safetensors format, exiting...


          When searching for it, it is here:


          find ./ -name "*safetensors"

          ./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors


          What might I be missing?'
        updatedAt: '2023-04-16T15:39:15.099Z'
      numEdits: 0
      reactions: []
    id: 643c16a3d9a06e038df10c4e
    type: comment
  author: szeta
  content: 'Hi, I tried this - thanks for the preparation!

    However, running into the following error:


    python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g --auto-devices
    --wbits 4 --groupsize 128 --chat

    bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so

    Loading anon8231489123/vicuna-13b-GPTQ-4bit-128g...

    Could not find the quantized model in .pt or .safetensors format, exiting...


    When searching for it, it is here:


    find ./ -name "*safetensors"

    ./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors


    What might I be missing?'
  created_at: 2023-04-16 14:39:15+00:00
  edited: false
  hidden: false
  id: 643c16a3d9a06e038df10c4e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
      fullname: Ngon Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltnn1
      type: user
    createdAt: '2023-04-16T16:52:18.000Z'
    data:
      edited: true
      editors:
      - ltnn1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
          fullname: Ngon Nguyen
          isHf: false
          isPro: false
          name: ltnn1
          type: user
        html: '<blockquote>

          <p>Hi, I tried this - thanks for the preparation!<br>However, running into
          the following error:</p>

          <p>python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g --auto-devices
          --wbits 4 --groupsize 128 --chat<br>bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so<br>Loading
          anon8231489123/vicuna-13b-GPTQ-4bit-128g...<br>Could not find the quantized
          model in .pt or .safetensors format, exiting...</p>

          <p>When searching for it, it is here:</p>

          <p>find ./ -name "*safetensors"<br>./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors</p>

          <p>What might I be missing?</p>

          </blockquote>

          <p>How did you download the model? Using the download-model.py script or
          clone directly from hugging face? Have you try <code>conda deactivate</code>,
          then <code>conda remove -n vicuna-matata --all</code> and do all steps above
          again? Might help if you specify the exact python version as well in the
          <code>conda create</code> command i.e. <code>conda create -n vicuna-matata
          pytorch torchvision torchaudio python=3.10.9 pytorch-cuda=11.7 cuda-toolkit
          -c ''nvidia/label/cuda-11.7.0'' -c pytorch -c nvidia</code></p>

          <p>And finally, most of my problems with CUDA had been with GPTQ (step 5
          above), when you ran the <code>python setup_cuda.py install</code> did it
          throw any errors?</p>

          '
        raw: "> Hi, I tried this - thanks for the preparation!\n> However, running\
          \ into the following error:\n> \n> python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g\
          \ --auto-devices --wbits 4 --groupsize 128 --chat\n> bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n\
          > Loading anon8231489123/vicuna-13b-GPTQ-4bit-128g...\n> Could not find\
          \ the quantized model in .pt or .safetensors format, exiting...\n> \n> When\
          \ searching for it, it is here:\n> \n> find ./ -name \"*safetensors\"\n\
          > ./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\n\
          > \n> What might I be missing?\n\nHow did you download the model? Using\
          \ the download-model.py script or clone directly from hugging face? Have\
          \ you try `conda deactivate`, then `conda remove -n vicuna-matata --all`\
          \ and do all steps above again? Might help if you specify the exact python\
          \ version as well in the `conda create` command i.e. `conda create -n vicuna-matata\
          \ pytorch torchvision torchaudio python=3.10.9 pytorch-cuda=11.7 cuda-toolkit\
          \ -c 'nvidia/label/cuda-11.7.0' -c pytorch -c nvidia`\n\nAnd finally, most\
          \ of my problems with CUDA had been with GPTQ (step 5 above), when you ran\
          \ the `python setup_cuda.py install` did it throw any errors?"
        updatedAt: '2023-04-16T16:52:45.388Z'
      numEdits: 1
      reactions: []
    id: 643c27c221686867003fe51d
    type: comment
  author: ltnn1
  content: "> Hi, I tried this - thanks for the preparation!\n> However, running into\
    \ the following error:\n> \n> python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g\
    \ --auto-devices --wbits 4 --groupsize 128 --chat\n> bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n\
    > Loading anon8231489123/vicuna-13b-GPTQ-4bit-128g...\n> Could not find the quantized\
    \ model in .pt or .safetensors format, exiting...\n> \n> When searching for it,\
    \ it is here:\n> \n> find ./ -name \"*safetensors\"\n> ./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\n\
    > \n> What might I be missing?\n\nHow did you download the model? Using the download-model.py\
    \ script or clone directly from hugging face? Have you try `conda deactivate`,\
    \ then `conda remove -n vicuna-matata --all` and do all steps above again? Might\
    \ help if you specify the exact python version as well in the `conda create` command\
    \ i.e. `conda create -n vicuna-matata pytorch torchvision torchaudio python=3.10.9\
    \ pytorch-cuda=11.7 cuda-toolkit -c 'nvidia/label/cuda-11.7.0' -c pytorch -c nvidia`\n\
    \nAnd finally, most of my problems with CUDA had been with GPTQ (step 5 above),\
    \ when you ran the `python setup_cuda.py install` did it throw any errors?"
  created_at: 2023-04-16 15:52:18+00:00
  edited: true
  hidden: false
  id: 643c27c221686867003fe51d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
      fullname: Ralph
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: szeta
      type: user
    createdAt: '2023-04-16T19:44:51.000Z'
    data:
      edited: false
      editors:
      - szeta
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/52a92eb692207c756f3769bc2bd77fa4.svg
          fullname: Ralph
          isHf: false
          isPro: false
          name: szeta
          type: user
        html: "<p>thanks for the hint,</p>\n<p>yes I downloaded thorugh the download-model.py\
          \ script. When running python setup_cuda.py install I see a few warnings,\
          \ like the following one:</p>\n<p>/home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:244:1:\
          \ note: declared here<br>  244 |   T * data() const {<br>      | ^ ~~<br>quant_cuda_kernel.cu:507:2019:\
          \ warning: \u2018T* at::Tensor::data() const [with T = int]\u2019 is deprecated:\
          \ Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [-Wdeprecated-declarations]<br>\
          \  507 |   AT_DISPATCH_FLOATING_TYPES(<br>      |</p>\n<p>But I thought\
          \ if something is failing I would see an error. </p>\n<p>Are there other\
          \ dependencies I may have missed? And why is it not using the \"./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\
          , which can be found?</p>\n<p>I will try a bit more research, but in case\
          \ you spot something, much appreciated!</p>\n"
        raw: "thanks for the hint,\n\nyes I downloaded thorugh the download-model.py\
          \ script. When running python setup_cuda.py install I see a few warnings,\
          \ like the following one:\n\n/home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:244:1:\
          \ note: declared here\n  244 |   T * data() const {\n      | ^ ~~\nquant_cuda_kernel.cu:507:2019:\
          \ warning: \u2018T* at::Tensor::data() const [with T = int]\u2019 is deprecated:\
          \ Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead.\
          \ [-Wdeprecated-declarations]\n  507 |   AT_DISPATCH_FLOATING_TYPES(\n \
          \     |\n\nBut I thought if something is failing I would see an error. \n\
          \nAre there other dependencies I may have missed? And why is it not using\
          \ the \"./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\
          , which can be found?\n\nI will try a bit more research, but in case you\
          \ spot something, much appreciated!"
        updatedAt: '2023-04-16T19:44:51.797Z'
      numEdits: 0
      reactions: []
    id: 643c5033c16819aac1350e95
    type: comment
  author: szeta
  content: "thanks for the hint,\n\nyes I downloaded thorugh the download-model.py\
    \ script. When running python setup_cuda.py install I see a few warnings, like\
    \ the following one:\n\n/home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:244:1:\
    \ note: declared here\n  244 |   T * data() const {\n      | ^ ~~\nquant_cuda_kernel.cu:507:2019:\
    \ warning: \u2018T* at::Tensor::data() const [with T = int]\u2019 is deprecated:\
    \ Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n\
    \  507 |   AT_DISPATCH_FLOATING_TYPES(\n      |\n\nBut I thought if something\
    \ is failing I would see an error. \n\nAre there other dependencies I may have\
    \ missed? And why is it not using the \"./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\
    , which can be found?\n\nI will try a bit more research, but in case you spot\
    \ something, much appreciated!"
  created_at: 2023-04-16 18:44:51+00:00
  edited: false
  hidden: false
  id: 643c5033c16819aac1350e95
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
      fullname: Ngon Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltnn1
      type: user
    createdAt: '2023-04-16T20:08:15.000Z'
    data:
      edited: false
      editors:
      - ltnn1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
          fullname: Ngon Nguyen
          isHf: false
          isPro: false
          name: ltnn1
          type: user
        html: "<blockquote>\n<p>thanks for the hint,</p>\n<p>yes I downloaded thorugh\
          \ the download-model.py script. When running python setup_cuda.py install\
          \ I see a few warnings, like the following one:</p>\n<p>/home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:244:1:\
          \ note: declared here<br>  244 |   T * data() const {<br>      | ^ ~~<br>quant_cuda_kernel.cu:507:2019:\
          \ warning: \u2018T* at::Tensor::data() const [with T = int]\u2019 is deprecated:\
          \ Tensor.data() is deprecated. Please use Tensor.data_ptr() instead. [-Wdeprecated-declarations]<br>\
          \  507 |   AT_DISPATCH_FLOATING_TYPES(<br>      |</p>\n<p>But I thought\
          \ if something is failing I would see an error. </p>\n<p>Are there other\
          \ dependencies I may have missed? And why is it not using the \"./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\
          , which can be found?</p>\n<p>I will try a bit more research, but in case\
          \ you spot something, much appreciated!</p>\n</blockquote>\n<p>It might\
          \ have something to do with <a rel=\"nofollow\" href=\"https://github.com/oobabooga/text-generation-webui/issues/991\"\
          >this issue</a> or <a rel=\"nofollow\" href=\"https://github.com/pytorch/pytorch/issues/28472\"\
          >this one</a>. </p>\n<p>From my experience, problems with quantized models\
          \ are almost always related to GPTQ (and their CUDA implementation, see\
          \ if you're using their Triton or CUDA branch, Triton is only for Linux).\
          \ Is the problem only with quantized models, have you tried loading up any\
          \ other like GPT-J or GPT-NeoX or the vanilla LLaMAs? Also, are you using\
          \ GPU or CPU only?</p>\n<p>If just simply <code>conda deactivate</code>\
          \ and removing the env doesn't work, then maybe you should try deleting\
          \ the <code>/text-generation-webui/</code> folder and try again from scratch.\
          \ Try doing <code>pip install -r requirements.txt</code> in BOTH text-generation-webui\
          \ and GPTQ-for-LLaMa as well (updated in the guide above). </p>\n<p>From\
          \ what I can see, the oogabooga's fork of GPTQ had some changes in their\
          \ requirements.txt 2 days ago, that might be the cause. </p>\n<p>If you've\
          \ tried all the above and are really desperate, try one of those <a rel=\"\
          nofollow\" href=\"https://github.com/oobabooga/one-click-installers\">one-click-installer\
          \ scrips</a> they might do a better job of preparing the env and dependencies\
          \ than I did.</p>\n"
        raw: "> thanks for the hint,\n> \n> yes I downloaded thorugh the download-model.py\
          \ script. When running python setup_cuda.py install I see a few warnings,\
          \ like the following one:\n> \n> /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:244:1:\
          \ note: declared here\n>   244 |   T * data() const {\n>       | ^ ~~\n\
          > quant_cuda_kernel.cu:507:2019: warning: \u2018T* at::Tensor::data() const\
          \ [with T = int]\u2019 is deprecated: Tensor.data<T>() is deprecated. Please\
          \ use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n>   507\
          \ |   AT_DISPATCH_FLOATING_TYPES(\n>       |\n> \n> But I thought if something\
          \ is failing I would see an error. \n> \n> Are there other dependencies\
          \ I may have missed? And why is it not using the \"./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\
          , which can be found?\n> \n> I will try a bit more research, but in case\
          \ you spot something, much appreciated!\n\nIt might have something to do\
          \ with [this issue](https://github.com/oobabooga/text-generation-webui/issues/991)\
          \ or [this one](https://github.com/pytorch/pytorch/issues/28472). \n\nFrom\
          \ my experience, problems with quantized models are almost always related\
          \ to GPTQ (and their CUDA implementation, see if you're using their Triton\
          \ or CUDA branch, Triton is only for Linux). Is the problem only with quantized\
          \ models, have you tried loading up any other like GPT-J or GPT-NeoX or\
          \ the vanilla LLaMAs? Also, are you using GPU or CPU only?\n\nIf just simply\
          \ `conda deactivate` and removing the env doesn't work, then maybe you should\
          \ try deleting the `/text-generation-webui/` folder and try again from scratch.\
          \ Try doing `pip install -r requirements.txt` in BOTH text-generation-webui\
          \ and GPTQ-for-LLaMa as well (updated in the guide above). \n\nFrom what\
          \ I can see, the oogabooga's fork of GPTQ had some changes in their requirements.txt\
          \ 2 days ago, that might be the cause. \n\nIf you've tried all the above\
          \ and are really desperate, try one of those [one-click-installer scrips](https://github.com/oobabooga/one-click-installers)\
          \ they might do a better job of preparing the env and dependencies than\
          \ I did."
        updatedAt: '2023-04-16T20:08:15.015Z'
      numEdits: 0
      reactions: []
    id: 643c55afd9a06e038df29eba
    type: comment
  author: ltnn1
  content: "> thanks for the hint,\n> \n> yes I downloaded thorugh the download-model.py\
    \ script. When running python setup_cuda.py install I see a few warnings, like\
    \ the following one:\n> \n> /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:244:1:\
    \ note: declared here\n>   244 |   T * data() const {\n>       | ^ ~~\n> quant_cuda_kernel.cu:507:2019:\
    \ warning: \u2018T* at::Tensor::data() const [with T = int]\u2019 is deprecated:\
    \ Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n\
    >   507 |   AT_DISPATCH_FLOATING_TYPES(\n>       |\n> \n> But I thought if something\
    \ is failing I would see an error. \n> \n> Are there other dependencies I may\
    \ have missed? And why is it not using the \"./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\"\
    , which can be found?\n> \n> I will try a bit more research, but in case you spot\
    \ something, much appreciated!\n\nIt might have something to do with [this issue](https://github.com/oobabooga/text-generation-webui/issues/991)\
    \ or [this one](https://github.com/pytorch/pytorch/issues/28472). \n\nFrom my\
    \ experience, problems with quantized models are almost always related to GPTQ\
    \ (and their CUDA implementation, see if you're using their Triton or CUDA branch,\
    \ Triton is only for Linux). Is the problem only with quantized models, have you\
    \ tried loading up any other like GPT-J or GPT-NeoX or the vanilla LLaMAs? Also,\
    \ are you using GPU or CPU only?\n\nIf just simply `conda deactivate` and removing\
    \ the env doesn't work, then maybe you should try deleting the `/text-generation-webui/`\
    \ folder and try again from scratch. Try doing `pip install -r requirements.txt`\
    \ in BOTH text-generation-webui and GPTQ-for-LLaMa as well (updated in the guide\
    \ above). \n\nFrom what I can see, the oogabooga's fork of GPTQ had some changes\
    \ in their requirements.txt 2 days ago, that might be the cause. \n\nIf you've\
    \ tried all the above and are really desperate, try one of those [one-click-installer\
    \ scrips](https://github.com/oobabooga/one-click-installers) they might do a better\
    \ job of preparing the env and dependencies than I did."
  created_at: 2023-04-16 19:08:15+00:00
  edited: false
  hidden: false
  id: 643c55afd9a06e038df29eba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
      fullname: Ngon Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltnn1
      type: user
    createdAt: '2023-04-16T20:18:03.000Z'
    data:
      edited: false
      editors:
      - ltnn1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
          fullname: Ngon Nguyen
          isHf: false
          isPro: false
          name: ltnn1
          type: user
        html: '<p>Another thing that I''ve noticed is that my CLI doesn''t have this
          line <code>bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so</code>,
          so that can be another thing to pay attention to. </p>

          <p>libbitsandbytes had an error that required a <a rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975">dirty
          fix</a> in the previous version of text-generation-webui, so you might want
          to check the thread out.</p>

          '
        raw: "Another thing that I've noticed is that my CLI doesn't have this line\
          \ `bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so`,\
          \ so that can be another thing to pay attention to. \n\nlibbitsandbytes\
          \ had an error that required a [dirty fix](https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975)\
          \ in the previous version of text-generation-webui, so you might want to\
          \ check the thread out."
        updatedAt: '2023-04-16T20:18:03.303Z'
      numEdits: 0
      reactions: []
    id: 643c57fb823f2cb2713ce11a
    type: comment
  author: ltnn1
  content: "Another thing that I've noticed is that my CLI doesn't have this line\
    \ `bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so`,\
    \ so that can be another thing to pay attention to. \n\nlibbitsandbytes had an\
    \ error that required a [dirty fix](https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975)\
    \ in the previous version of text-generation-webui, so you might want to check\
    \ the thread out."
  created_at: 2023-04-16 19:18:03+00:00
  edited: false
  hidden: false
  id: 643c57fb823f2cb2713ce11a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fc6783b036e6e73fb388d068265dfde8.svg
      fullname: BinaryWood
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BinaryWood
      type: user
    createdAt: '2023-04-19T07:28:47.000Z'
    data:
      edited: false
      editors:
      - BinaryWood
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fc6783b036e6e73fb388d068265dfde8.svg
          fullname: BinaryWood
          isHf: false
          isPro: false
          name: BinaryWood
          type: user
        html: '<blockquote>

          <p>Hi, I tried this - thanks for the preparation!<br>However, running into
          the following error:</p>

          <p>python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g --auto-devices
          --wbits 4 --groupsize 128 --chat<br>bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so<br>Loading
          anon8231489123/vicuna-13b-GPTQ-4bit-128g...<br>Could not find the quantized
          model in .pt or .safetensors format, exiting...</p>

          <p>When searching for it, it is here:</p>

          <p>find ./ -name "*safetensors"<br>./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors</p>

          <p>What might I be missing?</p>

          </blockquote>

          <p>try --model anon8231489123_vicuna-13b-GPTQ-4bit-128g</p>

          '
        raw: "> Hi, I tried this - thanks for the preparation!\n> However, running\
          \ into the following error:\n> \n> python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g\
          \ --auto-devices --wbits 4 --groupsize 128 --chat\n> bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n\
          > Loading anon8231489123/vicuna-13b-GPTQ-4bit-128g...\n> Could not find\
          \ the quantized model in .pt or .safetensors format, exiting...\n> \n> When\
          \ searching for it, it is here:\n> \n> find ./ -name \"*safetensors\"\n\
          > ./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\n\
          > \n> What might I be missing?\n\ntry --model anon8231489123_vicuna-13b-GPTQ-4bit-128g"
        updatedAt: '2023-04-19T07:28:47.230Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brunokreiner
    id: 643f982fba7506b57e389963
    type: comment
  author: BinaryWood
  content: "> Hi, I tried this - thanks for the preparation!\n> However, running into\
    \ the following error:\n> \n> python server.py --model anon8231489123/vicuna-13b-GPTQ-4bit-128g\
    \ --auto-devices --wbits 4 --groupsize 128 --chat\n> bin /home/ubuntu/anaconda3/envs/vicuna-matata/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n\
    > Loading anon8231489123/vicuna-13b-GPTQ-4bit-128g...\n> Could not find the quantized\
    \ model in .pt or .safetensors format, exiting...\n> \n> When searching for it,\
    \ it is here:\n> \n> find ./ -name \"*safetensors\"\n> ./models/anon8231489123_vicuna-13b-GPTQ-4bit-128g/vicuna-13b-4bit-128g.safetensors\n\
    > \n> What might I be missing?\n\ntry --model anon8231489123_vicuna-13b-GPTQ-4bit-128g"
  created_at: 2023-04-19 06:28:47+00:00
  edited: false
  hidden: false
  id: 643f982fba7506b57e389963
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
      fullname: "Gediz G\xDCRSU"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TeaCult
      type: user
    createdAt: '2023-04-20T07:26:07.000Z'
    data:
      edited: false
      editors:
      - TeaCult
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c6feba914ead95667e0aa31a90081133.svg
          fullname: "Gediz G\xDCRSU"
          isHf: false
          isPro: false
          name: TeaCult
          type: user
        html: '<p>Kind of hit the wall with this. Tried on Jarvislabs.ai, ninja did
          not built due to cuda version mismatch hmf is missing thus lots of division
          to zero errors ...<br>My local server is also kvm virtualised and then containerized
          Arch lxd containers on Arch KVM on arch host (Zen). </p>

          <p>So my question is is there a freaking way to run this using transformers
          or accelerate or it is not possible because they not support 4 bit models
          or something. I am new to these language models (2 weeks)<br>Trying to grasp
          llma gpt gptq. Can you give me a hand. </p>

          <p>Thank you for sharing and best regards.</p>

          '
        raw: "Kind of hit the wall with this. Tried on Jarvislabs.ai, ninja did not\
          \ built due to cuda version mismatch hmf is missing thus lots of division\
          \ to zero errors ... \nMy local server is also kvm virtualised and then\
          \ containerized Arch lxd containers on Arch KVM on arch host (Zen). \n\n\
          So my question is is there a freaking way to run this using transformers\
          \ or accelerate or it is not possible because they not support 4 bit models\
          \ or something. I am new to these language models (2 weeks) \nTrying to\
          \ grasp llma gpt gptq. Can you give me a hand. \n\nThank you for sharing\
          \ and best regards."
        updatedAt: '2023-04-20T07:26:07.345Z'
      numEdits: 0
      reactions: []
    id: 6440e90fe46e14ed557feafe
    type: comment
  author: TeaCult
  content: "Kind of hit the wall with this. Tried on Jarvislabs.ai, ninja did not\
    \ built due to cuda version mismatch hmf is missing thus lots of division to zero\
    \ errors ... \nMy local server is also kvm virtualised and then containerized\
    \ Arch lxd containers on Arch KVM on arch host (Zen). \n\nSo my question is is\
    \ there a freaking way to run this using transformers or accelerate or it is not\
    \ possible because they not support 4 bit models or something. I am new to these\
    \ language models (2 weeks) \nTrying to grasp llma gpt gptq. Can you give me a\
    \ hand. \n\nThank you for sharing and best regards."
  created_at: 2023-04-20 06:26:07+00:00
  edited: false
  hidden: false
  id: 6440e90fe46e14ed557feafe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4a8062a5858f939ba72e2a031993063e.svg
      fullname: Vikram Cothur
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Fire-Hound
      type: user
    createdAt: '2023-04-27T23:11:53.000Z'
    data:
      edited: false
      editors:
      - Fire-Hound
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4a8062a5858f939ba72e2a031993063e.svg
          fullname: Vikram Cothur
          isHf: false
          isPro: false
          name: Fire-Hound
          type: user
        html: '<p>Do you still need help?</p>

          '
        raw: Do you still need help?
        updatedAt: '2023-04-27T23:11:53.221Z'
      numEdits: 0
      reactions: []
    id: 644b0139af97dfd24c1550af
    type: comment
  author: Fire-Hound
  content: Do you still need help?
  created_at: 2023-04-27 22:11:53+00:00
  edited: false
  hidden: false
  id: 644b0139af97dfd24c1550af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d5aae7fda8e982374aa78d36c496d782.svg
      fullname: Chuck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: csprocket777
      type: user
    createdAt: '2023-04-28T20:49:41.000Z'
    data:
      edited: false
      editors:
      - csprocket777
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d5aae7fda8e982374aa78d36c496d782.svg
          fullname: Chuck
          isHf: false
          isPro: false
          name: csprocket777
          type: user
        html: '<p>Hi, I''m trying to run this in a headless environment without GPUs.
          Possible? What changes about the process?</p>

          '
        raw: Hi, I'm trying to run this in a headless environment without GPUs. Possible?
          What changes about the process?
        updatedAt: '2023-04-28T20:49:41.998Z'
      numEdits: 0
      reactions: []
    id: 644c3165ed08a4fdf4e3528b
    type: comment
  author: csprocket777
  content: Hi, I'm trying to run this in a headless environment without GPUs. Possible?
    What changes about the process?
  created_at: 2023-04-28 19:49:41+00:00
  edited: false
  hidden: false
  id: 644c3165ed08a4fdf4e3528b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
      fullname: Ngon Nguyen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ltnn1
      type: user
    createdAt: '2023-05-02T16:37:30.000Z'
    data:
      edited: false
      editors:
      - ltnn1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/25534b5500322938113f01813b7fc898.svg
          fullname: Ngon Nguyen
          isHf: false
          isPro: false
          name: ltnn1
          type: user
        html: '<blockquote>

          <p>Hi, I''m trying to run this in a headless environment without GPUs. Possible?
          What changes about the process?</p>

          </blockquote>

          <p>Check out <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></p>

          '
        raw: '> Hi, I''m trying to run this in a headless environment without GPUs.
          Possible? What changes about the process?


          Check out https://github.com/ggerganov/llama.cpp'
        updatedAt: '2023-05-02T16:37:30.939Z'
      numEdits: 0
      reactions: []
    id: 64513c4a9d916c596e2cb4a2
    type: comment
  author: ltnn1
  content: '> Hi, I''m trying to run this in a headless environment without GPUs.
    Possible? What changes about the process?


    Check out https://github.com/ggerganov/llama.cpp'
  created_at: 2023-05-02 15:37:30+00:00
  edited: false
  hidden: false
  id: 64513c4a9d916c596e2cb4a2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7c244653842b3266409e0764f96b9b58.svg
      fullname: SM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: smhmhmd
      type: user
    createdAt: '2023-05-17T17:05:54.000Z'
    data:
      edited: false
      editors:
      - smhmhmd
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7c244653842b3266409e0764f96b9b58.svg
          fullname: SM
          isHf: false
          isPro: false
          name: smhmhmd
          type: user
        html: '<p>Thanks for the above description, this command works on a g4dn.4xlarge<br>"(vicuna-matata)
          ubuntu@ip-10-0-0-69:~/text-generation-webui$ python server.py --auto-devices
          --wbits 4 --groupsize 128 --chat "</p>

          '
        raw: 'Thanks for the above description, this command works on a g4dn.4xlarge

          "(vicuna-matata) ubuntu@ip-10-0-0-69:~/text-generation-webui$ python server.py
          --auto-devices --wbits 4 --groupsize 128 --chat "'
        updatedAt: '2023-05-17T17:05:54.242Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ltnn1
    id: 646509726ceebdc7fd8b9cb6
    type: comment
  author: smhmhmd
  content: 'Thanks for the above description, this command works on a g4dn.4xlarge

    "(vicuna-matata) ubuntu@ip-10-0-0-69:~/text-generation-webui$ python server.py
    --auto-devices --wbits 4 --groupsize 128 --chat "'
  created_at: 2023-05-17 16:05:54+00:00
  edited: false
  hidden: false
  id: 646509726ceebdc7fd8b9cb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31533c794fdf7e2ce683133d9402e1ae.svg
      fullname: "Le\xF4nidas Augusto Amaro"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: augustoamaro
      type: user
    createdAt: '2023-06-15T13:58:04.000Z'
    data:
      edited: false
      editors:
      - augustoamaro
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7682111263275146
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31533c794fdf7e2ce683133d9402e1ae.svg
          fullname: "Le\xF4nidas Augusto Amaro"
          isHf: false
          isPro: false
          name: augustoamaro
          type: user
        html: '<p>I have an error when I send a message: </p>

          <p>INFO:Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>ERROR:No
          model is loaded! Select one in the Model tab.</p>

          <p>Even if the model is selected</p>

          '
        raw: "I have an error when I send a message: \n\nINFO:Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
          ERROR:No model is loaded! Select one in the Model tab.\n\n\nEven if the\
          \ model is selected\n\n"
        updatedAt: '2023-06-15T13:58:04.581Z'
      numEdits: 0
      reactions: []
    id: 648b18ecb01d6671638dee5a
    type: comment
  author: augustoamaro
  content: "I have an error when I send a message: \n\nINFO:Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\n\
    ERROR:No model is loaded! Select one in the Model tab.\n\n\nEven if the model\
    \ is selected\n\n"
  created_at: 2023-06-15 12:58:04+00:00
  edited: false
  hidden: false
  id: 648b18ecb01d6671638dee5a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6499dba16558dafec6a5e5a3/g16RlMs2PeBq7fOh8O4Tw.png?w=200&h=200&f=face
      fullname: Mauricio Gleizer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: GleizerRunner
      type: user
    createdAt: '2023-06-26T18:47:45.000Z'
    data:
      edited: false
      editors:
      - GleizerRunner
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9433147311210632
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6499dba16558dafec6a5e5a3/g16RlMs2PeBq7fOh8O4Tw.png?w=200&h=200&f=face
          fullname: Mauricio Gleizer
          isHf: false
          isPro: false
          name: GleizerRunner
          type: user
        html: "<p>I tried everything I could, it didn\u2019t work, giving bizarre\
          \ errors more and more. I give up. And now, how do I uninstall all these\
          \ things from python etc etc from my system?<br>Can anyone help me with\
          \ that? Please\u2026</p>\n"
        raw: "I tried everything I could, it didn\u2019t work, giving bizarre errors\
          \ more and more. I give up. And now, how do I uninstall all these things\
          \ from python etc etc from my system?\nCan anyone help me with that? Please\u2026"
        updatedAt: '2023-06-26T18:47:45.860Z'
      numEdits: 0
      reactions: []
    id: 6499dd51baa047336fc4d6e1
    type: comment
  author: GleizerRunner
  content: "I tried everything I could, it didn\u2019t work, giving bizarre errors\
    \ more and more. I give up. And now, how do I uninstall all these things from\
    \ python etc etc from my system?\nCan anyone help me with that? Please\u2026"
  created_at: 2023-06-26 17:47:45+00:00
  edited: false
  hidden: false
  id: 6499dd51baa047336fc4d6e1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: '[GUIDE] Installing Vicuna on Linux'
