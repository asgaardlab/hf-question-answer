!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Reggie
conflicting_files: null
created_at: 2023-04-04 05:54:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-04T06:54:51.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: "<p>This is awesome but the conversion to ggml for llama.cpp seems to\
          \ be erroring out.<br>My command: <code>python3 convert-gptq-to-ggml.py\
          \ ../llama_models/vicuna-13b-GPTQ-4bit-128g/vicuna-13b-GPTQ-4bit-128g.pt\
          \ ../llama_models/vicuna-13b-GPTQ-4bit-128g/tokenizer.model ggml-vicuna-13b-GPTQ-4bit-128g</code></p>\n\
          <p>The error:</p>\n<pre><code>Processing non-Q4 variable: model.embed_tokens.weight\
          \ with shape: torch.Size([32001, 5120]) and type: torch.float16\nProcessing\
          \ non-Q4 variable: model.norm.weight with shape: torch.Size([5120]) and\
          \ type: torch.float16\n  Converting to float32\nProcessing non-Q4 variable:\
          \ lm_head.weight with shape: torch.Size([32001, 5120]) and type: torch.float16\n\
          Traceback (most recent call last):\n  File \"/home/sravanth/llama.cpp/convert-gptq-to-ggml.py\"\
          , line 156, in &lt;module&gt;\n    convert_q4(f\"model.layers.{i}.self_attn.q_proj\"\
          , f\"layers.{i}.attention.wq.weight\", permute=True)\n  File \"/home/sravanth/llama.cpp/convert-gptq-to-ggml.py\"\
          , line 97, in convert_q4\n    zeros = model[f\"{src_name}.zeros\"].numpy()\n\
          KeyError: 'model.layers.0.self_attn.q_proj.zeros'\n</code></pre>\n<p>Any\
          \ pointers on fixing it?</p>\n"
        raw: "This is awesome but the conversion to ggml for llama.cpp seems to be\
          \ erroring out.\r\nMy command: `python3 convert-gptq-to-ggml.py ../llama_models/vicuna-13b-GPTQ-4bit-128g/vicuna-13b-GPTQ-4bit-128g.pt\
          \ ../llama_models/vicuna-13b-GPTQ-4bit-128g/tokenizer.model ggml-vicuna-13b-GPTQ-4bit-128g`\r\
          \n\r\nThe error:\r\n```\r\nProcessing non-Q4 variable: model.embed_tokens.weight\
          \ with shape: torch.Size([32001, 5120]) and type: torch.float16\r\nProcessing\
          \ non-Q4 variable: model.norm.weight with shape: torch.Size([5120]) and\
          \ type: torch.float16\r\n  Converting to float32\r\nProcessing non-Q4 variable:\
          \ lm_head.weight with shape: torch.Size([32001, 5120]) and type: torch.float16\r\
          \nTraceback (most recent call last):\r\n  File \"/home/sravanth/llama.cpp/convert-gptq-to-ggml.py\"\
          , line 156, in <module>\r\n    convert_q4(f\"model.layers.{i}.self_attn.q_proj\"\
          , f\"layers.{i}.attention.wq.weight\", permute=True)\r\n  File \"/home/sravanth/llama.cpp/convert-gptq-to-ggml.py\"\
          , line 97, in convert_q4\r\n    zeros = model[f\"{src_name}.zeros\"].numpy()\r\
          \nKeyError: 'model.layers.0.self_attn.q_proj.zeros'\r\n```\r\nAny pointers\
          \ on fixing it?"
        updatedAt: '2023-04-04T06:54:51.015Z'
      numEdits: 0
      reactions: []
    id: 642bc9bbbabc9a614a013bd7
    type: comment
  author: Reggie
  content: "This is awesome but the conversion to ggml for llama.cpp seems to be erroring\
    \ out.\r\nMy command: `python3 convert-gptq-to-ggml.py ../llama_models/vicuna-13b-GPTQ-4bit-128g/vicuna-13b-GPTQ-4bit-128g.pt\
    \ ../llama_models/vicuna-13b-GPTQ-4bit-128g/tokenizer.model ggml-vicuna-13b-GPTQ-4bit-128g`\r\
    \n\r\nThe error:\r\n```\r\nProcessing non-Q4 variable: model.embed_tokens.weight\
    \ with shape: torch.Size([32001, 5120]) and type: torch.float16\r\nProcessing\
    \ non-Q4 variable: model.norm.weight with shape: torch.Size([5120]) and type:\
    \ torch.float16\r\n  Converting to float32\r\nProcessing non-Q4 variable: lm_head.weight\
    \ with shape: torch.Size([32001, 5120]) and type: torch.float16\r\nTraceback (most\
    \ recent call last):\r\n  File \"/home/sravanth/llama.cpp/convert-gptq-to-ggml.py\"\
    , line 156, in <module>\r\n    convert_q4(f\"model.layers.{i}.self_attn.q_proj\"\
    , f\"layers.{i}.attention.wq.weight\", permute=True)\r\n  File \"/home/sravanth/llama.cpp/convert-gptq-to-ggml.py\"\
    , line 97, in convert_q4\r\n    zeros = model[f\"{src_name}.zeros\"].numpy()\r\
    \nKeyError: 'model.layers.0.self_attn.q_proj.zeros'\r\n```\r\nAny pointers on\
    \ fixing it?"
  created_at: 2023-04-04 05:54:51+00:00
  edited: false
  hidden: false
  id: 642bc9bbbabc9a614a013bd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c9c09029317af408f9804df02f645ed.svg
      fullname: z.
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: anon8231489123
      type: user
    createdAt: '2023-04-04T07:01:38.000Z'
    data:
      edited: false
      editors:
      - anon8231489123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c9c09029317af408f9804df02f645ed.svg
          fullname: z.
          isHf: false
          isPro: false
          name: anon8231489123
          type: user
        html: '<p>Use the safetensors, if possible.</p>

          '
        raw: Use the safetensors, if possible.
        updatedAt: '2023-04-04T07:01:38.602Z'
      numEdits: 0
      reactions: []
    id: 642bcb52713f4970a1cb61bc
    type: comment
  author: anon8231489123
  content: Use the safetensors, if possible.
  created_at: 2023-04-04 06:01:38+00:00
  edited: false
  hidden: false
  id: 642bcb52713f4970a1cb61bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-04T07:37:03.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Doesn''t seem to support safetensors yet. But may be coming soon:
          <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp/issues/688">https://github.com/ggerganov/llama.cpp/issues/688</a><br>Guess
          I''ll wait.<br>Thanks</p>

          '
        raw: 'Doesn''t seem to support safetensors yet. But may be coming soon: https://github.com/ggerganov/llama.cpp/issues/688

          Guess I''ll wait.

          Thanks'
        updatedAt: '2023-04-04T07:37:03.350Z'
      numEdits: 0
      reactions: []
    id: 642bd39f8b820fae7323f604
    type: comment
  author: Reggie
  content: 'Doesn''t seem to support safetensors yet. But may be coming soon: https://github.com/ggerganov/llama.cpp/issues/688

    Guess I''ll wait.

    Thanks'
  created_at: 2023-04-04 06:37:03+00:00
  edited: false
  hidden: false
  id: 642bd39f8b820fae7323f604
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
      fullname: D
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Reggie
      type: user
    createdAt: '2023-04-04T09:30:51.000Z'
    data:
      edited: false
      editors:
      - Reggie
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7bea6c9753f431ded9df2ddf08ec7034.svg
          fullname: D
          isHf: false
          isPro: false
          name: Reggie
          type: user
        html: '<p>Is this: <a href="https://huggingface.co/eachadea/ggml-vicuna-13b-4bit">https://huggingface.co/eachadea/ggml-vicuna-13b-4bit</a><br>the
          ggml version of your repo by any chance. Doesn''t say if gptq was used etc.
          So was kind of confused.</p>

          '
        raw: 'Is this: https://huggingface.co/eachadea/ggml-vicuna-13b-4bit

          the ggml version of your repo by any chance. Doesn''t say if gptq was used
          etc. So was kind of confused.'
        updatedAt: '2023-04-04T09:30:51.208Z'
      numEdits: 0
      reactions: []
    id: 642bee4b9b94afd3deae09b6
    type: comment
  author: Reggie
  content: 'Is this: https://huggingface.co/eachadea/ggml-vicuna-13b-4bit

    the ggml version of your repo by any chance. Doesn''t say if gptq was used etc.
    So was kind of confused.'
  created_at: 2023-04-04 08:30:51+00:00
  edited: false
  hidden: false
  id: 642bee4b9b94afd3deae09b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
      fullname: amkdg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eachadea
      type: user
    createdAt: '2023-04-06T02:34:50.000Z'
    data:
      edited: false
      editors:
      - eachadea
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
          fullname: amkdg
          isHf: false
          isPro: false
          name: eachadea
          type: user
        html: '<blockquote>

          <p>Is this: <a href="https://huggingface.co/eachadea/ggml-vicuna-13b-4bit">https://huggingface.co/eachadea/ggml-vicuna-13b-4bit</a><br>the
          ggml version of your repo by any chance. Doesn''t say if gptq was used etc.
          So was kind of confused.</p>

          </blockquote>

          <p>I converted into ggml using <a href="https://huggingface.co/eachadea/vicuna-13b">https://huggingface.co/eachadea/vicuna-13b</a>
          (merged it from delta weights on my own system).</p>

          '
        raw: '> Is this: https://huggingface.co/eachadea/ggml-vicuna-13b-4bit

          > the ggml version of your repo by any chance. Doesn''t say if gptq was
          used etc. So was kind of confused.


          I converted into ggml using https://huggingface.co/eachadea/vicuna-13b (merged
          it from delta weights on my own system).'
        updatedAt: '2023-04-06T02:34:50.558Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Reggie
        - ai2p
    id: 642e2fca24bf366459b6e762
    type: comment
  author: eachadea
  content: '> Is this: https://huggingface.co/eachadea/ggml-vicuna-13b-4bit

    > the ggml version of your repo by any chance. Doesn''t say if gptq was used etc.
    So was kind of confused.


    I converted into ggml using https://huggingface.co/eachadea/vicuna-13b (merged
    it from delta weights on my own system).'
  created_at: 2023-04-06 01:34:50+00:00
  edited: false
  hidden: false
  id: 642e2fca24bf366459b6e762
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
      fullname: Aoum
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ai2p
      type: user
    createdAt: '2023-04-08T04:26:07.000Z'
    data:
      edited: false
      editors:
      - ai2p
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/22c019ff9b23d81ec9457c461d9076cf.svg
          fullname: Aoum
          isHf: false
          isPro: false
          name: ai2p
          type: user
        html: '<blockquote>

          <p>I converted into ggml using <a href="https://huggingface.co/eachadea/vicuna-13b">https://huggingface.co/eachadea/vicuna-13b</a>
          (merged it from delta weights on my own system).</p>

          </blockquote>

          <p>Does GPTQ was used in that conversion?</p>

          '
        raw: '> I converted into ggml using https://huggingface.co/eachadea/vicuna-13b
          (merged it from delta weights on my own system).


          Does GPTQ was used in that conversion?'
        updatedAt: '2023-04-08T04:26:07.302Z'
      numEdits: 0
      reactions: []
    id: 6430ecdfcb875f71392603cd
    type: comment
  author: ai2p
  content: '> I converted into ggml using https://huggingface.co/eachadea/vicuna-13b
    (merged it from delta weights on my own system).


    Does GPTQ was used in that conversion?'
  created_at: 2023-04-08 03:26:07+00:00
  edited: false
  hidden: false
  id: 6430ecdfcb875f71392603cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
      fullname: amkdg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eachadea
      type: user
    createdAt: '2023-04-09T03:22:23.000Z'
    data:
      edited: false
      editors:
      - eachadea
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642b2aa85df44ff24543d8be/mEnbGB_0Flleoa6SWp5b6.jpeg?w=200&h=200&f=face
          fullname: amkdg
          isHf: false
          isPro: false
          name: eachadea
          type: user
        html: '<p>No, ggml is a separate format with its own quantization implementation
          - gptq is not and shouldn''t be involved</p>

          '
        raw: No, ggml is a separate format with its own quantization implementation
          - gptq is not and shouldn't be involved
        updatedAt: '2023-04-09T03:22:23.933Z'
      numEdits: 0
      reactions: []
    id: 64322f6fb62bdfa25c0edfa1
    type: comment
  author: eachadea
  content: No, ggml is a separate format with its own quantization implementation
    - gptq is not and shouldn't be involved
  created_at: 2023-04-09 02:22:23+00:00
  edited: false
  hidden: false
  id: 64322f6fb62bdfa25c0edfa1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: ggml conversion error
