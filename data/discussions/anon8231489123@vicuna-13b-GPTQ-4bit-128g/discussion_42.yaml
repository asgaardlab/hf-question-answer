!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Setphus
conflicting_files: null
created_at: 2023-04-27 06:06:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a1101f199b57572b7dfba6431282e1a.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Setphus
      type: user
    createdAt: '2023-04-27T07:06:02.000Z'
    data:
      edited: false
      editors:
      - Setphus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a1101f199b57572b7dfba6431282e1a.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: Setphus
          type: user
        html: '<p>Okay, I cannot figure out what, or what not, I may be doing to cause
          this, but when I start a conversation, after a while, the Vicuna model starts
          pausing. I can try to press continue, but it will only continue with one
          word, and then end up pausing again. Is there any way that I can fix this?
          Thanks for the help.<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/63f085966309c84d5f47d0a2/_YjmtrG47JUR_ovdcHVvq.png"><img
          alt="Screenshot 2023-04-27 020517.png" src="https://cdn-uploads.huggingface.co/production/uploads/63f085966309c84d5f47d0a2/_YjmtrG47JUR_ovdcHVvq.png"></a></p>

          '
        raw: "Okay, I cannot figure out what, or what not, I may be doing to cause\
          \ this, but when I start a conversation, after a while, the Vicuna model\
          \ starts pausing. I can try to press continue, but it will only continue\
          \ with one word, and then end up pausing again. Is there any way that I\
          \ can fix this? Thanks for the help.\r\n![Screenshot 2023-04-27 020517.png](https://cdn-uploads.huggingface.co/production/uploads/63f085966309c84d5f47d0a2/_YjmtrG47JUR_ovdcHVvq.png)\r\
          \n"
        updatedAt: '2023-04-27T07:06:02.374Z'
      numEdits: 0
      reactions: []
    id: 644a1edaed295eb43e684bb8
    type: comment
  author: Setphus
  content: "Okay, I cannot figure out what, or what not, I may be doing to cause this,\
    \ but when I start a conversation, after a while, the Vicuna model starts pausing.\
    \ I can try to press continue, but it will only continue with one word, and then\
    \ end up pausing again. Is there any way that I can fix this? Thanks for the help.\r\
    \n![Screenshot 2023-04-27 020517.png](https://cdn-uploads.huggingface.co/production/uploads/63f085966309c84d5f47d0a2/_YjmtrG47JUR_ovdcHVvq.png)\r\
    \n"
  created_at: 2023-04-27 06:06:02+00:00
  edited: false
  hidden: false
  id: 644a1edaed295eb43e684bb8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a1101f199b57572b7dfba6431282e1a.svg
      fullname: Seth
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Setphus
      type: user
    createdAt: '2023-04-27T07:26:50.000Z'
    data:
      edited: true
      editors:
      - Setphus
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a1101f199b57572b7dfba6431282e1a.svg
          fullname: Seth
          isHf: false
          isPro: false
          name: Setphus
          type: user
        html: '<p>Oh dang! I think my GPU is crap. I have a 3080 with only 10gb of
          vram. I have a Ryzen 5950x right now, I can hear them all now, "should have
          gone for intel," I know already. Sheeshk, but should I try to offload it
          onto my CPU anyway, or is it even worth it? Also, does this mean that I
          cannot have long conversations, or is it just this much for each response
          usually? Will I have to delete previous messages in order to make new ones,
          or can I somehow save them in a way that the AI model can view them for
          context? Thanks again for the help.</p>

          <p>[UPDATE]: Alright, alright. I thought about it for a bit, and could I
          not, technically, allocate excess ram to my C: drive so that I can use it
          for AI purposes? Does anyone have thoughts on this, or am I an absolute
          nut case? IDK, maybe you can even use an SSD NVMe if you have one that can
          do 7gb per second....</p>

          '
        raw: 'Oh dang! I think my GPU is crap. I have a 3080 with only 10gb of vram.
          I have a Ryzen 5950x right now, I can hear them all now, "should have gone
          for intel," I know already. Sheeshk, but should I try to offload it onto
          my CPU anyway, or is it even worth it? Also, does this mean that I cannot
          have long conversations, or is it just this much for each response usually?
          Will I have to delete previous messages in order to make new ones, or can
          I somehow save them in a way that the AI model can view them for context?
          Thanks again for the help.


          [UPDATE]: Alright, alright. I thought about it for a bit, and could I not,
          technically, allocate excess ram to my C: drive so that I can use it for
          AI purposes? Does anyone have thoughts on this, or am I an absolute nut
          case? IDK, maybe you can even use an SSD NVMe if you have one that can do
          7gb per second....'
        updatedAt: '2023-04-27T08:16:21.231Z'
      numEdits: 3
      reactions: []
    id: 644a23baca8809b635d9f24b
    type: comment
  author: Setphus
  content: 'Oh dang! I think my GPU is crap. I have a 3080 with only 10gb of vram.
    I have a Ryzen 5950x right now, I can hear them all now, "should have gone for
    intel," I know already. Sheeshk, but should I try to offload it onto my CPU anyway,
    or is it even worth it? Also, does this mean that I cannot have long conversations,
    or is it just this much for each response usually? Will I have to delete previous
    messages in order to make new ones, or can I somehow save them in a way that the
    AI model can view them for context? Thanks again for the help.


    [UPDATE]: Alright, alright. I thought about it for a bit, and could I not, technically,
    allocate excess ram to my C: drive so that I can use it for AI purposes? Does
    anyone have thoughts on this, or am I an absolute nut case? IDK, maybe you can
    even use an SSD NVMe if you have one that can do 7gb per second....'
  created_at: 2023-04-27 06:26:50+00:00
  edited: true
  hidden: false
  id: 644a23baca8809b635d9f24b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
      fullname: James Edward
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Goldenblood56
      type: user
    createdAt: '2023-04-30T01:15:27.000Z'
    data:
      edited: true
      editors:
      - Goldenblood56
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a43a73a77be89cc7443347d1a0aef9fe.svg
          fullname: James Edward
          isHf: false
          isPro: false
          name: Goldenblood56
          type: user
        html: '<blockquote>

          <p>Oh dang! I think my GPU is crap. I have a 3080 with only 10gb of vram.
          I have a Ryzen 5950x right now, I can hear them all now, "should have gone
          for intel," I know already. Sheeshk, but should I try to offload it onto
          my CPU anyway, or is it even worth it? Also, does this mean that I cannot
          have long conversations, or is it just this much for each response usually?
          Will I have to delete previous messages in order to make new ones, or can
          I somehow save them in a way that the AI model can view them for context?
          Thanks again for the help.</p>

          <p>[UPDATE]: Alright, alright. I thought about it for a bit, and could I
          not, technically, allocate excess ram to my C: drive so that I can use it
          for AI purposes? Does anyone have thoughts on this, or am I an absolute
          nut case? IDK, maybe you can even use an SSD NVMe if you have one that can
          do 7gb per second....</p>

          </blockquote>

          <p>I think you can kind of do that. I am not an expert but if you split
          the model between GPU and CPU. By allocating layers etc. It''s now giving
          you more ram to use since it''s not only letting you use VRAM. The trade
          off is speed. I would not suggest dumping to the NVMe though. I still think
          system memory is a lot faster. Let us know how it goes as I am interested.</p>

          '
        raw: "> Oh dang! I think my GPU is crap. I have a 3080 with only 10gb of vram.\
          \ I have a Ryzen 5950x right now, I can hear them all now, \"should have\
          \ gone for intel,\" I know already. Sheeshk, but should I try to offload\
          \ it onto my CPU anyway, or is it even worth it? Also, does this mean that\
          \ I cannot have long conversations, or is it just this much for each response\
          \ usually? Will I have to delete previous messages in order to make new\
          \ ones, or can I somehow save them in a way that the AI model can view them\
          \ for context? Thanks again for the help.\n> \n> [UPDATE]: Alright, alright.\
          \ I thought about it for a bit, and could I not, technically, allocate excess\
          \ ram to my C: drive so that I can use it for AI purposes? Does anyone have\
          \ thoughts on this, or am I an absolute nut case? IDK, maybe you can even\
          \ use an SSD NVMe if you have one that can do 7gb per second....\n\nI think\
          \ you can kind of do that. I am not an expert but if you split the model\
          \ between GPU and CPU. By allocating layers etc. It's now giving you more\
          \ ram to use since it's not only letting you use VRAM. The trade off is\
          \ speed. I would not suggest dumping to the NVMe though. I still think system\
          \ memory is a lot faster. Let us know how it goes as I am interested."
        updatedAt: '2023-04-30T01:16:16.509Z'
      numEdits: 2
      reactions: []
    id: 644dc12ffa94e93b0ed1b117
    type: comment
  author: Goldenblood56
  content: "> Oh dang! I think my GPU is crap. I have a 3080 with only 10gb of vram.\
    \ I have a Ryzen 5950x right now, I can hear them all now, \"should have gone\
    \ for intel,\" I know already. Sheeshk, but should I try to offload it onto my\
    \ CPU anyway, or is it even worth it? Also, does this mean that I cannot have\
    \ long conversations, or is it just this much for each response usually? Will\
    \ I have to delete previous messages in order to make new ones, or can I somehow\
    \ save them in a way that the AI model can view them for context? Thanks again\
    \ for the help.\n> \n> [UPDATE]: Alright, alright. I thought about it for a bit,\
    \ and could I not, technically, allocate excess ram to my C: drive so that I can\
    \ use it for AI purposes? Does anyone have thoughts on this, or am I an absolute\
    \ nut case? IDK, maybe you can even use an SSD NVMe if you have one that can do\
    \ 7gb per second....\n\nI think you can kind of do that. I am not an expert but\
    \ if you split the model between GPU and CPU. By allocating layers etc. It's now\
    \ giving you more ram to use since it's not only letting you use VRAM. The trade\
    \ off is speed. I would not suggest dumping to the NVMe though. I still think\
    \ system memory is a lot faster. Let us know how it goes as I am interested."
  created_at: 2023-04-30 00:15:27+00:00
  edited: true
  hidden: false
  id: 644dc12ffa94e93b0ed1b117
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 42
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: Responses stop loading in.
