!!python/object:huggingface_hub.community.DiscussionWithDetails
author: CR2022
conflicting_files: null
created_at: 2023-04-07 06:33:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-04-07T07:33:22.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>===================================BUG REPORT===================================<br>Welcome
          to bitsandbytes. For bug reports, please submit your error trace to: <a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>CUDA
          SETUP: CUDA runtime path found: D:\one-click-installers\installer_files\env\bin\cudart64_110.dll<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary D:\one-click-installers\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll...<br>Loading
          vicuna-13b-GPTQ-4bit-128g...<br>Auto-assiging --gpu-memory 11 for your GPU
          to try to prevent out-of-memory errors.<br>You can manually set other values.<br>Loading
          checkpoint shards:   0%|                                                                 |
          0/3 [00:00&lt;?, ?it/s]<br>Traceback (most recent call last):<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 442, in load_state_dict<br>    return torch.load(checkpoint_file, map_location="cpu")<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\torch\serialization.py",
          line 791, in load<br>    with _open_file_like(f, ''rb'') as opened_file:<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\torch\serialization.py",
          line 271, in _open_file_like<br>    return _open_file(name_or_buffer, mode)<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\torch\serialization.py",
          line 252, in <strong>init</strong><br>    super().<strong>init</strong>(open(name,
          mode))<br>FileNotFoundError: [Errno 2] No such file or directory: ''models\vicuna-13b-GPTQ-4bit-128g\pytorch_model-00001-of-00003.bin''</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "D:\one-click-installers\text-generation-webui\server.py",
          line 308, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "D:\one-click-installers\text-generation-webui\modules\models.py", line
          170, in load_model<br>    model = AutoModelForCausalLM.from_pretrained(checkpoint,
          **params)<br>  File "D:\one-click-installers\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 471, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 2736, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 3050, in _load_pretrained_model<br>    state_dict = load_state_dict(shard_file)<br>  File
          "D:\one-click-installers\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 445, in load_state_dict<br>    with open(checkpoint_file) as f:<br>FileNotFoundError:
          [Errno 2] No such file or directory: ''models\vicuna-13b-GPTQ-4bit-128g\pytorch_model-00001-of-00003.bin''<br>Press
          any key to continue . . .</p>

          '
        raw: "===================================BUG REPORT===================================\r\
          \nWelcome to bitsandbytes. For bug reports, please submit your error trace\
          \ to: https://github.com/TimDettmers/bitsandbytes/issues\r\n================================================================================\r\
          \nCUDA SETUP: CUDA runtime path found: D:\\one-click-installers\\installer_files\\\
          env\\bin\\cudart64_110.dll\r\nCUDA SETUP: Highest compute capability among\
          \ GPUs detected: 8.6\r\nCUDA SETUP: Detected CUDA version 117\r\nCUDA SETUP:\
          \ Loading binary D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
          bitsandbytes\\libbitsandbytes_cuda117.dll...\r\nLoading vicuna-13b-GPTQ-4bit-128g...\r\
          \nAuto-assiging --gpu-memory 11 for your GPU to try to prevent out-of-memory\
          \ errors.\r\nYou can manually set other values.\r\nLoading checkpoint shards:\
          \   0%|                                                                \
          \ | 0/3 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File\
          \ \"D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\", line 442, in load_state_dict\r\n    return\
          \ torch.load(checkpoint_file, map_location=\"cpu\")\r\n  File \"D:\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\", line\
          \ 791, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n\
          \  File \"D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\", line 271, in _open_file_like\r\n    return _open_file(name_or_buffer,\
          \ mode)\r\n  File \"D:\\one-click-installers\\installer_files\\env\\lib\\\
          site-packages\\torch\\serialization.py\", line 252, in __init__\r\n    super().__init__(open(name,\
          \ mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models\\\
          \\vicuna-13b-GPTQ-4bit-128g\\\\pytorch_model-00001-of-00003.bin'\r\n\r\n\
          During handling of the above exception, another exception occurred:\r\n\r\
          \nTraceback (most recent call last):\r\n  File \"D:\\one-click-installers\\\
          text-generation-webui\\server.py\", line 308, in <module>\r\n    shared.model,\
          \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"D:\\one-click-installers\\\
          text-generation-webui\\modules\\models.py\", line 170, in load_model\r\n\
          \    model = AutoModelForCausalLM.from_pretrained(checkpoint, **params)\r\
          \n  File \"D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\r\
          \n    return model_class.from_pretrained(\r\n  File \"D:\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 2736, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\", line 3050, in _load_pretrained_model\r\
          \n    state_dict = load_state_dict(shard_file)\r\n  File \"D:\\one-click-installers\\\
          installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 445, in load_state_dict\r\n    with open(checkpoint_file) as f:\r\
          \nFileNotFoundError: [Errno 2] No such file or directory: 'models\\\\vicuna-13b-GPTQ-4bit-128g\\\
          \\pytorch_model-00001-of-00003.bin'\r\nPress any key to continue . . ."
        updatedAt: '2023-04-07T07:33:22.195Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - devilsnare
        - rowdster
    id: 642fc74278fffcb6fc5bb21a
    type: comment
  author: CR2022
  content: "===================================BUG REPORT===================================\r\
    \nWelcome to bitsandbytes. For bug reports, please submit your error trace to:\
    \ https://github.com/TimDettmers/bitsandbytes/issues\r\n================================================================================\r\
    \nCUDA SETUP: CUDA runtime path found: D:\\one-click-installers\\installer_files\\\
    env\\bin\\cudart64_110.dll\r\nCUDA SETUP: Highest compute capability among GPUs\
    \ detected: 8.6\r\nCUDA SETUP: Detected CUDA version 117\r\nCUDA SETUP: Loading\
    \ binary D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
    bitsandbytes\\libbitsandbytes_cuda117.dll...\r\nLoading vicuna-13b-GPTQ-4bit-128g...\r\
    \nAuto-assiging --gpu-memory 11 for your GPU to try to prevent out-of-memory errors.\r\
    \nYou can manually set other values.\r\nLoading checkpoint shards:   0%|     \
    \                                                            | 0/3 [00:00<?, ?it/s]\r\
    \nTraceback (most recent call last):\r\n  File \"D:\\one-click-installers\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 442, in load_state_dict\r\
    \n    return torch.load(checkpoint_file, map_location=\"cpu\")\r\n  File \"D:\\\
    one-click-installers\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py\"\
    , line 791, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n \
    \ File \"D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
    torch\\serialization.py\", line 271, in _open_file_like\r\n    return _open_file(name_or_buffer,\
    \ mode)\r\n  File \"D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\\
    torch\\serialization.py\", line 252, in __init__\r\n    super().__init__(open(name,\
    \ mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models\\\\\
    vicuna-13b-GPTQ-4bit-128g\\\\pytorch_model-00001-of-00003.bin'\r\n\r\nDuring handling\
    \ of the above exception, another exception occurred:\r\n\r\nTraceback (most recent\
    \ call last):\r\n  File \"D:\\one-click-installers\\text-generation-webui\\server.py\"\
    , line 308, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
    \n  File \"D:\\one-click-installers\\text-generation-webui\\modules\\models.py\"\
    , line 170, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(checkpoint,\
    \ **params)\r\n  File \"D:\\one-click-installers\\installer_files\\env\\lib\\\
    site-packages\\transformers\\models\\auto\\auto_factory.py\", line 471, in from_pretrained\r\
    \n    return model_class.from_pretrained(\r\n  File \"D:\\one-click-installers\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\", line\
    \ 2736, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"\
    D:\\one-click-installers\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\", line 3050, in _load_pretrained_model\r\n    state_dict =\
    \ load_state_dict(shard_file)\r\n  File \"D:\\one-click-installers\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 445, in load_state_dict\r\
    \n    with open(checkpoint_file) as f:\r\nFileNotFoundError: [Errno 2] No such\
    \ file or directory: 'models\\\\vicuna-13b-GPTQ-4bit-128g\\\\pytorch_model-00001-of-00003.bin'\r\
    \nPress any key to continue . . ."
  created_at: 2023-04-07 06:33:22+00:00
  edited: false
  hidden: false
  id: 642fc74278fffcb6fc5bb21a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-04-07T08:48:14.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>Error might be related to Obabagoo and not the model.</p>

          '
        raw: Error might be related to Obabagoo and not the model.
        updatedAt: '2023-04-07T08:48:14.223Z'
      numEdits: 0
      reactions: []
      relatedEventId: 642fd8ceec768944a8bc915c
    id: 642fd8ceec768944a8bc915b
    type: comment
  author: CR2022
  content: Error might be related to Obabagoo and not the model.
  created_at: 2023-04-07 07:48:14+00:00
  edited: false
  hidden: false
  id: 642fd8ceec768944a8bc915b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-04-07T08:48:14.000Z'
    data:
      status: closed
    id: 642fd8ceec768944a8bc915c
    type: status-change
  author: CR2022
  created_at: 2023-04-07 07:48:14+00:00
  id: 642fd8ceec768944a8bc915c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f9e360a131174b812976e5f934c00e5.svg
      fullname: Kazim Afzal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devilsnare
      type: user
    createdAt: '2023-04-08T18:00:30.000Z'
    data:
      edited: false
      editors:
      - devilsnare
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f9e360a131174b812976e5f934c00e5.svg
          fullname: Kazim Afzal
          isHf: false
          isPro: false
          name: devilsnare
          type: user
        html: '<p>Did you find a fix for this error? I''m seeing the exact same error  (yes
          Im using Oobabooga as the Web UI) when launching without specifying wbits,
          group size (python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g
          --chat --gpu-memory 4294967296 --cpu-memory 10GB --auto-devices --verbose)?
          When I do use wbits, groupsize args I get outofmemory error (running 3060Ti
          8GB VRAM)</p>

          '
        raw: Did you find a fix for this error? I'm seeing the exact same error  (yes
          Im using Oobabooga as the Web UI) when launching without specifying wbits,
          group size (python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g
          --chat --gpu-memory 4294967296 --cpu-memory 10GB --auto-devices --verbose)?
          When I do use wbits, groupsize args I get outofmemory error (running 3060Ti
          8GB VRAM)
        updatedAt: '2023-04-08T18:00:30.107Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - christiandarkin
        - RodgerE1
    id: 6431abbe95c4b7eeb851a8cd
    type: comment
  author: devilsnare
  content: Did you find a fix for this error? I'm seeing the exact same error  (yes
    Im using Oobabooga as the Web UI) when launching without specifying wbits, group
    size (python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --chat
    --gpu-memory 4294967296 --cpu-memory 10GB --auto-devices --verbose)? When I do
    use wbits, groupsize args I get outofmemory error (running 3060Ti 8GB VRAM)
  created_at: 2023-04-08 17:00:30+00:00
  edited: false
  hidden: false
  id: 6431abbe95c4b7eeb851a8cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6a9e300ac79663d0813d50ae597d821f.svg
      fullname: alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: x295982
      type: user
    createdAt: '2023-04-10T01:42:16.000Z'
    data:
      edited: false
      editors:
      - x295982
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6a9e300ac79663d0813d50ae597d821f.svg
          fullname: alex
          isHf: false
          isPro: false
          name: x295982
          type: user
        html: '<p>seeing this too</p>

          '
        raw: seeing this too
        updatedAt: '2023-04-10T01:42:16.101Z'
      numEdits: 0
      reactions: []
    id: 6433697821c6b87a1af709e4
    type: comment
  author: x295982
  content: seeing this too
  created_at: 2023-04-10 00:42:16+00:00
  edited: false
  hidden: false
  id: 6433697821c6b87a1af709e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
      fullname: CR2022
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: CR2022
      type: user
    createdAt: '2023-04-10T11:36:40.000Z'
    data:
      edited: false
      editors:
      - CR2022
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63abbef00ed3c32528509700/N7CUJEuCgYqVU8ntzgggz.jpeg?w=200&h=200&f=face
          fullname: CR2022
          isHf: false
          isPro: false
          name: CR2022
          type: user
        html: '<p>It is fixed here but I am not sure which steps actually solved it
          and if the problem is related to this model or Oobabooga.</p>

          <p>You can try to download this model by using the download-model.bat</p>

          <ol>

          <li>Choose option L None of the above</li>

          <li>Input anon8231489123/vicuna-13b-GPTQ-4bit-128g</li>

          </ol>

          '
        raw: 'It is fixed here but I am not sure which steps actually solved it and
          if the problem is related to this model or Oobabooga.


          You can try to download this model by using the download-model.bat


          1. Choose option L None of the above

          2. Input anon8231489123/vicuna-13b-GPTQ-4bit-128g'
        updatedAt: '2023-04-10T11:36:40.228Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - manfred-exz
    id: 6433f4c82a81c2a4847c4558
    type: comment
  author: CR2022
  content: 'It is fixed here but I am not sure which steps actually solved it and
    if the problem is related to this model or Oobabooga.


    You can try to download this model by using the download-model.bat


    1. Choose option L None of the above

    2. Input anon8231489123/vicuna-13b-GPTQ-4bit-128g'
  created_at: 2023-04-10 10:36:40+00:00
  edited: false
  hidden: false
  id: 6433f4c82a81c2a4847c4558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6321d30595d6f717a8c2799b/VUS1r0X1XuEB9-LTGIF-Q.jpeg?w=200&h=200&f=face
      fullname: Aliasfox
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aliasfox
      type: user
    createdAt: '2023-04-10T23:39:33.000Z'
    data:
      edited: false
      editors:
      - aliasfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6321d30595d6f717a8c2799b/VUS1r0X1XuEB9-LTGIF-Q.jpeg?w=200&h=200&f=face
          fullname: Aliasfox
          isHf: false
          isPro: false
          name: aliasfox
          type: user
        html: '<p>This is still a bug and has not been resolved.</p>

          '
        raw: This is still a bug and has not been resolved.
        updatedAt: '2023-04-10T23:39:33.026Z'
      numEdits: 0
      reactions: []
    id: 64349e351d83dc03c8eb3ea9
    type: comment
  author: aliasfox
  content: This is still a bug and has not been resolved.
  created_at: 2023-04-10 22:39:33+00:00
  edited: false
  hidden: false
  id: 64349e351d83dc03c8eb3ea9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f9e360a131174b812976e5f934c00e5.svg
      fullname: Kazim Afzal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devilsnare
      type: user
    createdAt: '2023-04-11T00:20:27.000Z'
    data:
      edited: false
      editors:
      - devilsnare
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f9e360a131174b812976e5f934c00e5.svg
          fullname: Kazim Afzal
          isHf: false
          isPro: false
          name: devilsnare
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;CR2022&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/CR2022\">@<span class=\"\
          underline\">CR2022</span></a></span>\n\n\t</span></span> -- you're likely\
          \ referring to the guide posted by troublechute (<a rel=\"nofollow\" href=\"\
          https://youtu.be/ByV5w1ES38A\">https://youtu.be/ByV5w1ES38A</a>), that is\
          \ what I used and I did choose L and input the anon8231489123/vicuna-13b-GPTQ-4bit-128g\
          \ and if you run the server.py file with the default args, for me atleast\
          \ it fails and complains of OOM [torch.cuda.OutOfMemoryError: CUDA out of\
          \ memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.08\
          \ GiB already allocated; 0 bytes free; 7.32 GiB reserved in total by PyTorch)\
          \ If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>Output\
          \ generated in 3.30 seconds (0.00 tokens/s, 0 tokens, context 43)]</p>\n"
        raw: '@CR2022 -- you''re likely referring to the guide posted by troublechute
          (https://youtu.be/ByV5w1ES38A), that is what I used and I did choose L and
          input the anon8231489123/vicuna-13b-GPTQ-4bit-128g and if you run the server.py
          file with the default args, for me atleast it fails and complains of OOM
          [torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00
          MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already allocated; 0 bytes
          free; 7.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          Output generated in 3.30 seconds (0.00 tokens/s, 0 tokens, context 43)]'
        updatedAt: '2023-04-11T00:20:27.689Z'
      numEdits: 0
      reactions: []
    id: 6434a7cbd72427b82482d088
    type: comment
  author: devilsnare
  content: '@CR2022 -- you''re likely referring to the guide posted by troublechute
    (https://youtu.be/ByV5w1ES38A), that is what I used and I did choose L and input
    the anon8231489123/vicuna-13b-GPTQ-4bit-128g and if you run the server.py file
    with the default args, for me atleast it fails and complains of OOM [torch.cuda.OutOfMemoryError:
    CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity;
    7.08 GiB already allocated; 0 bytes free; 7.32 GiB reserved in total by PyTorch)
    If reserved memory is >> allocated memory try setting max_split_size_mb to avoid
    fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

    Output generated in 3.30 seconds (0.00 tokens/s, 0 tokens, context 43)]'
  created_at: 2023-04-10 23:20:27+00:00
  edited: false
  hidden: false
  id: 6434a7cbd72427b82482d088
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cac78b5bc20f78c8091926681d048957.svg
      fullname: Robert Castles
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rowdster
      type: user
    createdAt: '2023-04-11T02:08:01.000Z'
    data:
      edited: false
      editors:
      - rowdster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cac78b5bc20f78c8091926681d048957.svg
          fullname: Robert Castles
          isHf: false
          isPro: false
          name: rowdster
          type: user
        html: '<p>Just here to share I have the same issue, and using download-model.bat
          with ''L'' and specifying anon8231489123/vicuna-13b-GPTQ-4bit-128g (which
          does download) did not resolve the issue when choosing the model, you get
          the same type of message.</p>

          <pre><code>FileNotFoundError: [Errno 2] No such file or directory: ''models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\pytorch_model-00001-of-00003.bin''

          </code></pre>

          '
        raw: 'Just here to share I have the same issue, and using download-model.bat
          with ''L'' and specifying anon8231489123/vicuna-13b-GPTQ-4bit-128g (which
          does download) did not resolve the issue when choosing the model, you get
          the same type of message.


          ```

          FileNotFoundError: [Errno 2] No such file or directory: ''models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\pytorch_model-00001-of-00003.bin''

          ```'
        updatedAt: '2023-04-11T02:08:01.717Z'
      numEdits: 0
      reactions: []
    id: 6434c1015408e9c12affdbd1
    type: comment
  author: rowdster
  content: 'Just here to share I have the same issue, and using download-model.bat
    with ''L'' and specifying anon8231489123/vicuna-13b-GPTQ-4bit-128g (which does
    download) did not resolve the issue when choosing the model, you get the same
    type of message.


    ```

    FileNotFoundError: [Errno 2] No such file or directory: ''models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\pytorch_model-00001-of-00003.bin''

    ```'
  created_at: 2023-04-11 01:08:01+00:00
  edited: false
  hidden: false
  id: 6434c1015408e9c12affdbd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cac78b5bc20f78c8091926681d048957.svg
      fullname: Robert Castles
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rowdster
      type: user
    createdAt: '2023-04-11T02:15:07.000Z'
    data:
      edited: false
      editors:
      - rowdster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cac78b5bc20f78c8091926681d048957.svg
          fullname: Robert Castles
          isHf: false
          isPro: false
          name: rowdster
          type: user
        html: "<p>It seems like in this process we should find a set of .bin files\
          \ which appear to be a memory usage limiting option which splits the model\
          \ files out into smaller partitions based on this link <a href=\"https://huggingface.co/docs/transformers/big_models\"\
          >https://huggingface.co/docs/transformers/big_models</a></p>\n<p>the file\
          \ <em>pytorch_model.bin.index.json</em> refers to a set of entries like\
          \ below, but in my <em>models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g</em>\
          \ folder for example, I do not find these .bin files</p>\n<pre><code>  \
          \  \"lm_head.weight\": \"pytorch_model-00003-of-00003.bin\",\n    \"model.embed_tokens.weight\"\
          : \"pytorch_model-00001-of-00003.bin\",\n    \"model.layers.0.input_layernorm.weight\"\
          : \"pytorch_model-00001-of-00003.bin\",\n    \"model.layers.0.mlp.down_proj.weight\"\
          : \"pytorch_model-00001-of-00003.bin\",\n</code></pre>\n"
        raw: "It seems like in this process we should find a set of .bin files which\
          \ appear to be a memory usage limiting option which splits the model files\
          \ out into smaller partitions based on this link https://huggingface.co/docs/transformers/big_models\n\
          \nthe file *pytorch_model.bin.index.json* refers to a set of entries like\
          \ below, but in my *models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g* folder\
          \ for example, I do not find these .bin files\n\n```\n    \"lm_head.weight\"\
          : \"pytorch_model-00003-of-00003.bin\",\n    \"model.embed_tokens.weight\"\
          : \"pytorch_model-00001-of-00003.bin\",\n    \"model.layers.0.input_layernorm.weight\"\
          : \"pytorch_model-00001-of-00003.bin\",\n    \"model.layers.0.mlp.down_proj.weight\"\
          : \"pytorch_model-00001-of-00003.bin\",\n```"
        updatedAt: '2023-04-11T02:15:07.158Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - christiandarkin
    id: 6434c2abd12a239d72e84f07
    type: comment
  author: rowdster
  content: "It seems like in this process we should find a set of .bin files which\
    \ appear to be a memory usage limiting option which splits the model files out\
    \ into smaller partitions based on this link https://huggingface.co/docs/transformers/big_models\n\
    \nthe file *pytorch_model.bin.index.json* refers to a set of entries like below,\
    \ but in my *models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g* folder for example,\
    \ I do not find these .bin files\n\n```\n    \"lm_head.weight\": \"pytorch_model-00003-of-00003.bin\"\
    ,\n    \"model.embed_tokens.weight\": \"pytorch_model-00001-of-00003.bin\",\n\
    \    \"model.layers.0.input_layernorm.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n    \"model.layers.0.mlp.down_proj.weight\": \"pytorch_model-00001-of-00003.bin\"\
    ,\n```"
  created_at: 2023-04-11 01:15:07+00:00
  edited: false
  hidden: false
  id: 6434c2abd12a239d72e84f07
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6e2671b48f44cdda360d27307af2f9ad.svg
      fullname: BD
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hap2y
      type: user
    createdAt: '2023-04-11T03:06:56.000Z'
    data:
      edited: false
      editors:
      - hap2y
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6e2671b48f44cdda360d27307af2f9ad.svg
          fullname: BD
          isHf: false
          isPro: false
          name: hap2y
          type: user
        html: '<p>Same here.</p>

          <p>FileNotFoundError: [Errno 2] No such file or directory: ''models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\pytorch_model-00001-of-00003.bin''</p>

          '
        raw: 'Same here.


          FileNotFoundError: [Errno 2] No such file or directory: ''models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\pytorch_model-00001-of-00003.bin'''
        updatedAt: '2023-04-11T03:06:56.231Z'
      numEdits: 0
      reactions: []
    id: 6434ced01ceac17d773d6cc1
    type: comment
  author: hap2y
  content: 'Same here.


    FileNotFoundError: [Errno 2] No such file or directory: ''models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\pytorch_model-00001-of-00003.bin'''
  created_at: 2023-04-11 02:06:56+00:00
  edited: false
  hidden: false
  id: 6434ced01ceac17d773d6cc1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6321d30595d6f717a8c2799b/VUS1r0X1XuEB9-LTGIF-Q.jpeg?w=200&h=200&f=face
      fullname: Aliasfox
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aliasfox
      type: user
    createdAt: '2023-04-11T04:57:56.000Z'
    data:
      edited: false
      editors:
      - aliasfox
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6321d30595d6f717a8c2799b/VUS1r0X1XuEB9-LTGIF-Q.jpeg?w=200&h=200&f=face
          fullname: Aliasfox
          isHf: false
          isPro: false
          name: aliasfox
          type: user
        html: '<p>I found the issue and perhaps not the best "fix", because it requires
          a lot of extra space.. but it appears that the script is looking for the
          original "vicuna-13b-delta-v0" that "anon8231489123_vicuna-13b-GPTQ-4bit-128g"
          was based on. It contained "pytorch_model-0000X-of-00003.bin" and I just
          downloaded that repo and created symbolic links (so both share the same
          files). It takes a while to cache/load but it works now. Pardon my ignorance,
          I''m likely doing something wrong but it doesn''t feel like the extra 24gb
          of data shouldn''t be needed. Thanks</p>

          '
        raw: I found the issue and perhaps not the best "fix", because it requires
          a lot of extra space.. but it appears that the script is looking for the
          original "vicuna-13b-delta-v0" that "anon8231489123_vicuna-13b-GPTQ-4bit-128g"
          was based on. It contained "pytorch_model-0000X-of-00003.bin" and I just
          downloaded that repo and created symbolic links (so both share the same
          files). It takes a while to cache/load but it works now. Pardon my ignorance,
          I'm likely doing something wrong but it doesn't feel like the extra 24gb
          of data shouldn't be needed. Thanks
        updatedAt: '2023-04-11T04:57:56.156Z'
      numEdits: 0
      reactions: []
    id: 6434e8d45408e9c12a00ee7c
    type: comment
  author: aliasfox
  content: I found the issue and perhaps not the best "fix", because it requires a
    lot of extra space.. but it appears that the script is looking for the original
    "vicuna-13b-delta-v0" that "anon8231489123_vicuna-13b-GPTQ-4bit-128g" was based
    on. It contained "pytorch_model-0000X-of-00003.bin" and I just downloaded that
    repo and created symbolic links (so both share the same files). It takes a while
    to cache/load but it works now. Pardon my ignorance, I'm likely doing something
    wrong but it doesn't feel like the extra 24gb of data shouldn't be needed. Thanks
  created_at: 2023-04-11 03:57:56+00:00
  edited: false
  hidden: false
  id: 6434e8d45408e9c12a00ee7c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cac78b5bc20f78c8091926681d048957.svg
      fullname: Robert Castles
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rowdster
      type: user
    createdAt: '2023-04-19T14:51:03.000Z'
    data:
      edited: false
      editors:
      - rowdster
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cac78b5bc20f78c8091926681d048957.svg
          fullname: Robert Castles
          isHf: false
          isPro: false
          name: rowdster
          type: user
        html: '<p>Just want to share that this issue is now gone for me. Yesterday
          I downloaded the <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui">https://github.com/oobabooga/text-generation-webui</a>
          both oobabooga-windows.zip, oobabooga-linux.zip (for WSL as an experiment)
          and just run the start script once or twice, and when time to pick a model,
          say "L" and specify  anon8231489123/vicuna-13b-GPTQ-4bit-128g and wait for
          the site to start.</p>

          <p>Both in Windows and WSL the packages worked first time through.</p>

          '
        raw: 'Just want to share that this issue is now gone for me. Yesterday I downloaded
          the https://github.com/oobabooga/text-generation-webui both oobabooga-windows.zip,
          oobabooga-linux.zip (for WSL as an experiment) and just run the start script
          once or twice, and when time to pick a model, say "L" and specify  anon8231489123/vicuna-13b-GPTQ-4bit-128g
          and wait for the site to start.


          Both in Windows and WSL the packages worked first time through.'
        updatedAt: '2023-04-19T14:51:03.687Z'
      numEdits: 0
      reactions: []
    id: 643fffd7d4229e14ae965a1d
    type: comment
  author: rowdster
  content: 'Just want to share that this issue is now gone for me. Yesterday I downloaded
    the https://github.com/oobabooga/text-generation-webui both oobabooga-windows.zip,
    oobabooga-linux.zip (for WSL as an experiment) and just run the start script once
    or twice, and when time to pick a model, say "L" and specify  anon8231489123/vicuna-13b-GPTQ-4bit-128g
    and wait for the site to start.


    Both in Windows and WSL the packages worked first time through.'
  created_at: 2023-04-19 13:51:03+00:00
  edited: false
  hidden: false
  id: 643fffd7d4229e14ae965a1d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ea5a31f2b213352ae5f8ca7d69dd73c.svg
      fullname: David Carroll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dwcar49us
      type: user
    createdAt: '2023-04-29T23:54:51.000Z'
    data:
      edited: true
      editors:
      - dwcar49us
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ea5a31f2b213352ae5f8ca7d69dd73c.svg
          fullname: David Carroll
          isHf: false
          isPro: true
          name: dwcar49us
          type: user
        html: "<p>When running this in Windows WSL Linux with Text Generation Web\
          \ UI --  I get \"No module named \u2018llama_inference_offload'\" error\
          \ - this was a clean install of  oobabooga-linux manual install - second\
          \ release</p>\n"
        raw: "When running this in Windows WSL Linux with Text Generation Web UI --\
          \  I get \"No module named \u2018llama_inference_offload'\" error - this\
          \ was a clean install of  oobabooga-linux manual install - second release"
        updatedAt: '2023-04-30T02:15:18.129Z'
      numEdits: 2
      reactions: []
    id: 644dae4b0dc952d245a9fe52
    type: comment
  author: dwcar49us
  content: "When running this in Windows WSL Linux with Text Generation Web UI --\
    \  I get \"No module named \u2018llama_inference_offload'\" error - this was a\
    \ clean install of  oobabooga-linux manual install - second release"
  created_at: 2023-04-29 22:54:51+00:00
  edited: true
  hidden: false
  id: 644dae4b0dc952d245a9fe52
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6ea5a31f2b213352ae5f8ca7d69dd73c.svg
      fullname: David Carroll
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: dwcar49us
      type: user
    createdAt: '2023-04-30T02:17:57.000Z'
    data:
      edited: false
      editors:
      - dwcar49us
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6ea5a31f2b213352ae5f8ca7d69dd73c.svg
          fullname: David Carroll
          isHf: false
          isPro: true
          name: dwcar49us
          type: user
        html: '<p>I got help on reddit -</p>

          <p>cd text-generation-webui and then:</p>

          <p>python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits
          4 --groupsize 128 --model_type llama</p>

          <p>This worked for me. Note: I also installed the GPTQ conversion repository
          - I don''t know if that helped. I would tri the above command first.</p>

          '
        raw: 'I got help on reddit -


          cd text-generation-webui and then:


          python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits
          4 --groupsize 128 --model_type llama


          This worked for me. Note: I also installed the GPTQ conversion repository
          - I don''t know if that helped. I would tri the above command first.'
        updatedAt: '2023-04-30T02:17:57.915Z'
      numEdits: 0
      reactions: []
    id: 644dcfd5328c1aa30e52811a
    type: comment
  author: dwcar49us
  content: 'I got help on reddit -


    cd text-generation-webui and then:


    python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize
    128 --model_type llama


    This worked for me. Note: I also installed the GPTQ conversion repository - I
    don''t know if that helped. I would tri the above command first.'
  created_at: 2023-04-30 01:17:57+00:00
  edited: false
  hidden: false
  id: 644dcfd5328c1aa30e52811a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/wh1i5UD0vtHyhxyivp-9I.png?w=200&h=200&f=face
      fullname: Gabriel Velazquez
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: REM1
      type: user
    createdAt: '2023-05-03T00:26:02.000Z'
    data:
      edited: false
      editors:
      - REM1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/wh1i5UD0vtHyhxyivp-9I.png?w=200&h=200&f=face
          fullname: Gabriel Velazquez
          isHf: false
          isPro: false
          name: REM1
          type: user
        html: '<p>hy Yall,</p>

          <p>Getting this result after placing the previous command<br>"python server.py
          --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize 128
          --model_type llama"<br>-<br>C:\AIProject\oobabooga_windows\text-generation-webui&gt;python
          server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize
          128 --model_type llama<br>Traceback (most recent call last):<br>  File "C:\AIProject\oobabooga_windows\text-generation-webui\server.py",
          line 17, in <br>    import gradio as gr<br>ModuleNotFoundError: No module
          named ''gradio''</p>

          <p>C:\AIProject\oobabooga_windows\text-generation-webui&gt; </p>

          <p>What else could I try to get this up and running? ANy thought?</p>

          '
        raw: "hy Yall,\n\nGetting this result after placing the previous command\n\
          \"python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits\
          \ 4 --groupsize 128 --model_type llama\"\n-\nC:\\AIProject\\oobabooga_windows\\\
          text-generation-webui>python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g\
          \ --wbits 4 --groupsize 128 --model_type llama\nTraceback (most recent call\
          \ last):\n  File \"C:\\AIProject\\oobabooga_windows\\text-generation-webui\\\
          server.py\", line 17, in <module>\n    import gradio as gr\nModuleNotFoundError:\
          \ No module named 'gradio'\n\nC:\\AIProject\\oobabooga_windows\\text-generation-webui>\
          \ \n\nWhat else could I try to get this up and running? ANy thought?"
        updatedAt: '2023-05-03T00:26:02.790Z'
      numEdits: 0
      reactions: []
    id: 6451aa1a9d916c596e391394
    type: comment
  author: REM1
  content: "hy Yall,\n\nGetting this result after placing the previous command\n\"\
    python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize\
    \ 128 --model_type llama\"\n-\nC:\\AIProject\\oobabooga_windows\\text-generation-webui>python\
    \ server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize\
    \ 128 --model_type llama\nTraceback (most recent call last):\n  File \"C:\\AIProject\\\
    oobabooga_windows\\text-generation-webui\\server.py\", line 17, in <module>\n\
    \    import gradio as gr\nModuleNotFoundError: No module named 'gradio'\n\nC:\\\
    AIProject\\oobabooga_windows\\text-generation-webui> \n\nWhat else could I try\
    \ to get this up and running? ANy thought?"
  created_at: 2023-05-02 23:26:02+00:00
  edited: false
  hidden: false
  id: 6451aa1a9d916c596e391394
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/95f2f6b8251bf61731e80a297e12484c.svg
      fullname: R.A.M.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rmmss
      type: user
    createdAt: '2023-06-02T03:47:25.000Z'
    data:
      edited: false
      editors:
      - rmmss
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/95f2f6b8251bf61731e80a297e12484c.svg
          fullname: R.A.M.
          isHf: false
          isPro: false
          name: rmmss
          type: user
        html: '<blockquote>

          <p>hy Yall,</p>

          <p>Getting this result after placing the previous command<br>"python server.py
          --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize 128
          --model_type llama"<br>-<br>C:\AIProject\oobabooga_windows\text-generation-webui&gt;python
          server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits 4 --groupsize
          128 --model_type llama<br>Traceback (most recent call last):<br>  File "C:\AIProject\oobabooga_windows\text-generation-webui\server.py",
          line 17, in <br>    import gradio as gr<br>ModuleNotFoundError: No module
          named ''gradio''</p>

          <p>C:\AIProject\oobabooga_windows\text-generation-webui&gt; </p>

          <p>What else could I try to get this up and running? ANy thought?</p>

          </blockquote>

          <p>Run <code>pip install -r requirements.txt</code> inside<code>\text-generation-webui</code>
          and try again</p>

          '
        raw: "> hy Yall,\n> \n> Getting this result after placing the previous command\n\
          > \"python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits\
          \ 4 --groupsize 128 --model_type llama\"\n> -\n> C:\\AIProject\\oobabooga_windows\\\
          text-generation-webui>python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g\
          \ --wbits 4 --groupsize 128 --model_type llama\n> Traceback (most recent\
          \ call last):\n>   File \"C:\\AIProject\\oobabooga_windows\\text-generation-webui\\\
          server.py\", line 17, in <module>\n>     import gradio as gr\n> ModuleNotFoundError:\
          \ No module named 'gradio'\n> \n> C:\\AIProject\\oobabooga_windows\\text-generation-webui>\
          \ \n> \n> What else could I try to get this up and running? ANy thought?\n\
          \nRun `pip install -r requirements.txt` inside`\\text-generation-webui`\
          \ and try again"
        updatedAt: '2023-06-02T03:47:25.260Z'
      numEdits: 0
      reactions: []
    id: 6479664dfaa0a209531e3bb0
    type: comment
  author: rmmss
  content: "> hy Yall,\n> \n> Getting this result after placing the previous command\n\
    > \"python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g --wbits\
    \ 4 --groupsize 128 --model_type llama\"\n> -\n> C:\\AIProject\\oobabooga_windows\\\
    text-generation-webui>python server.py --model anon8231489123_vicuna-13b-GPTQ-4bit-128g\
    \ --wbits 4 --groupsize 128 --model_type llama\n> Traceback (most recent call\
    \ last):\n>   File \"C:\\AIProject\\oobabooga_windows\\text-generation-webui\\\
    server.py\", line 17, in <module>\n>     import gradio as gr\n> ModuleNotFoundError:\
    \ No module named 'gradio'\n> \n> C:\\AIProject\\oobabooga_windows\\text-generation-webui>\
    \ \n> \n> What else could I try to get this up and running? ANy thought?\n\nRun\
    \ `pip install -r requirements.txt` inside`\\text-generation-webui` and try again"
  created_at: 2023-06-02 02:47:25+00:00
  edited: false
  hidden: false
  id: 6479664dfaa0a209531e3bb0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/71587efb2c1c2606032cce088d21535f.svg
      fullname: Chunyen Hu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zxc4314314
      type: user
    createdAt: '2023-12-04T12:08:29.000Z'
    data:
      edited: false
      editors:
      - zxc4314314
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6417722702026367
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/71587efb2c1c2606032cce088d21535f.svg
          fullname: Chunyen Hu
          isHf: false
          isPro: false
          name: zxc4314314
          type: user
        html: '<p>I follow the <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui#one-click-installers">One-click
          installers</a> and run the <code>start_windows.bat</code>.<br>And then download
          the <code>anon8231489123/vicuna-13b-GPTQ-4bit-128g</code> model by using
          web UI.<br>But I got the same error after loading this model.<br>Is there
          any solution or suggestion?</p>

          '
        raw: "I follow the [One-click installers](https://github.com/oobabooga/text-generation-webui#one-click-installers)\
          \ and run the `start_windows.bat`. \nAnd then download the `anon8231489123/vicuna-13b-GPTQ-4bit-128g`\
          \ model by using web UI.\nBut I got the same error after loading this model.\n\
          Is there any solution or suggestion?\n"
        updatedAt: '2023-12-04T12:08:29.398Z'
      numEdits: 0
      reactions: []
    id: 656dc13d9dcedd16d568fa92
    type: comment
  author: zxc4314314
  content: "I follow the [One-click installers](https://github.com/oobabooga/text-generation-webui#one-click-installers)\
    \ and run the `start_windows.bat`. \nAnd then download the `anon8231489123/vicuna-13b-GPTQ-4bit-128g`\
    \ model by using web UI.\nBut I got the same error after loading this model.\n\
    Is there any solution or suggestion?\n"
  created_at: 2023-12-04 12:08:29+00:00
  edited: false
  hidden: false
  id: 656dc13d9dcedd16d568fa92
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: closed
target_branch: null
title: 'FileNotFoundError: [Errno 2] No such file or directory: ''models\\vicuna-13b-GPTQ-4bit-128g\\pytorch_model-00001-of-00003.bin'''
