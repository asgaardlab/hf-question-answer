!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Zelan
conflicting_files: null
created_at: 2023-04-07 16:00:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/47802713acf37c548ed32cfce8f5ef99.svg
      fullname: "Germ\xE1n Molt\xF3"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zelan
      type: user
    createdAt: '2023-04-07T17:00:25.000Z'
    data:
      edited: false
      editors:
      - Zelan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/47802713acf37c548ed32cfce8f5ef99.svg
          fullname: "Germ\xE1n Molt\xF3"
          isHf: false
          isPro: false
          name: Zelan
          type: user
        html: '<p>I have already installed Accelerate, made sure it was in the PATH,
          yet I  keep getting this message when I try to load the model. Also, I tried
          installing bitsandby, but after I did the webui wouldn''t work at all.</p>

          '
        raw: I have already installed Accelerate, made sure it was in the PATH, yet
          I  keep getting this message when I try to load the model. Also, I tried
          installing bitsandby, but after I did the webui wouldn't work at all.
        updatedAt: '2023-04-07T17:00:25.604Z'
      numEdits: 0
      reactions: []
    id: 64304c296b621b9798b52c96
    type: comment
  author: Zelan
  content: I have already installed Accelerate, made sure it was in the PATH, yet
    I  keep getting this message when I try to load the model. Also, I tried installing
    bitsandby, but after I did the webui wouldn't work at all.
  created_at: 2023-04-07 16:00:25+00:00
  edited: false
  hidden: false
  id: 64304c296b621b9798b52c96
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633658078552-615fa3293f6d24d67c1b5c82.jpeg?w=200&h=200&f=face
      fullname: George1202
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Dorjzodovsuren
      type: user
    createdAt: '2023-05-07T11:41:08.000Z'
    data:
      edited: false
      editors:
      - Dorjzodovsuren
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1633658078552-615fa3293f6d24d67c1b5c82.jpeg?w=200&h=200&f=face
          fullname: George1202
          isHf: false
          isPro: false
          name: Dorjzodovsuren
          type: user
        html: '<p>you need to restart kernel</p>

          '
        raw: you need to restart kernel
        updatedAt: '2023-05-07T11:41:08.280Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F91D"
        users:
        - KashMkj18
        - skrieg
        - adaAtLLM
      - count: 2
        reaction: "\U0001F44D"
        users:
        - jdavis761
        - nhatkhtn
    id: 64578e5498a8724fa60f1e58
    type: comment
  author: Dorjzodovsuren
  content: you need to restart kernel
  created_at: 2023-05-07 10:41:08+00:00
  edited: false
  hidden: false
  id: 64578e5498a8724fa60f1e58
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4e8b48b43e4b8e1dc6e5c404d0e01a1e.svg
      fullname: asd kladfhjklj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: appleszapples
      type: user
    createdAt: '2023-07-23T01:20:07.000Z'
    data:
      edited: false
      editors:
      - appleszapples
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4282703697681427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4e8b48b43e4b8e1dc6e5c404d0e01a1e.svg
          fullname: asd kladfhjklj
          isHf: false
          isPro: false
          name: appleszapples
          type: user
        html: '<p>same</p>

          '
        raw: same
        updatedAt: '2023-07-23T01:20:07.297Z'
      numEdits: 0
      reactions: []
    id: 64bc8047afd1e46c55fa1060
    type: comment
  author: appleszapples
  content: same
  created_at: 2023-07-23 00:20:07+00:00
  edited: false
  hidden: false
  id: 64bc8047afd1e46c55fa1060
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c170cdbb2649862927a099dc6199619.svg
      fullname: Shahab
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shahabty
      type: user
    createdAt: '2023-08-01T00:14:35.000Z'
    data:
      edited: false
      editors:
      - shahabty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.984218955039978
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c170cdbb2649862927a099dc6199619.svg
          fullname: Shahab
          isHf: false
          isPro: false
          name: shahabty
          type: user
        html: '<p>I have the same issue but I found no solution for it. Is there any
          solution?</p>

          '
        raw: 'I have the same issue but I found no solution for it. Is there any solution?

          '
        updatedAt: '2023-08-01T00:14:35.815Z'
      numEdits: 0
      reactions: []
    id: 64c84e6b1c23fb9a2bd76d9c
    type: comment
  author: shahabty
  content: 'I have the same issue but I found no solution for it. Is there any solution?

    '
  created_at: 2023-07-31 23:14:35+00:00
  edited: false
  hidden: false
  id: 64c84e6b1c23fb9a2bd76d9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a944de2b32a4e8e9d32204d954229617.svg
      fullname: Yin King Law
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JohnLaw
      type: user
    createdAt: '2023-08-02T16:29:35.000Z'
    data:
      edited: false
      editors:
      - JohnLaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4282703697681427
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a944de2b32a4e8e9d32204d954229617.svg
          fullname: Yin King Law
          isHf: false
          isPro: false
          name: JohnLaw
          type: user
        html: '<p>same</p>

          '
        raw: 'same

          '
        updatedAt: '2023-08-02T16:29:35.311Z'
      numEdits: 0
      reactions: []
    id: 64ca846f86d8dc0caa25b92d
    type: comment
  author: JohnLaw
  content: 'same

    '
  created_at: 2023-08-02 15:29:35+00:00
  edited: false
  hidden: false
  id: 64ca846f86d8dc0caa25b92d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f3eb041cb175f6f07854ae1ed1330844.svg
      fullname: Arjun
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vishyrjun
      type: user
    createdAt: '2023-08-02T21:34:47.000Z'
    data:
      edited: true
      editors:
      - vishyrjun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9854488968849182
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f3eb041cb175f6f07854ae1ed1330844.svg
          fullname: Arjun
          isHf: false
          isPro: false
          name: vishyrjun
          type: user
        html: '<p>I was trying this in Google Colab and keep getting this error as
          well. I ensured accelerate and bitsandbytes are installed. still keeps getting
          this issue. Then realized i was having a particular line in my train command
          that was causing this issue</p>

          <p>autotrain llm --train --project_name ''&lt;project_name&gt;'' \<br>--model
          TinyPixel/Llama-2-7B-bf16-sharded \<br>--data_path timdettmers/openassistant-guanaco
          \<br>--text_column text \<br>--use_int4 \<br>--use_peft \<br>--learning_rate
          2e-4 \<br>--train_batch_size 2 \<br>--num_train_epochs 3 \<br>--trainer
          sft \<br>--model_max_length 2048 \<br>--push_to_hub \<br>--repo_id &lt;repo_id&gt;/&lt;project_name&gt;
          \<br>--block_size 2048 &gt; training.log &amp;</p>

          <p>After i removed --use_int4 line and executed, the issue got resolved.
          I hope this helps. Please ensure to use ''\'' at the end of everyline incase
          of using this command to train.</p>

          <p>Thanks.</p>

          '
        raw: 'I was trying this in Google Colab and keep getting this error as well.
          I ensured accelerate and bitsandbytes are installed. still keeps getting
          this issue. Then realized i was having a particular line in my train command
          that was causing this issue


          autotrain llm --train --project_name ''\<project_name\>'' \\

          --model TinyPixel/Llama-2-7B-bf16-sharded \\

          --data_path timdettmers/openassistant-guanaco \\

          --text_column text \\

          --use_int4 \\

          --use_peft \\

          --learning_rate 2e-4 \\

          --train_batch_size 2 \\

          --num_train_epochs 3 \\

          --trainer sft \\

          --model_max_length 2048 \\

          --push_to_hub \\

          --repo_id \<repo_id\>/\<project_name\> \\

          --block_size 2048 > training.log &


          After i removed --use_int4 line and executed, the issue got resolved. I
          hope this helps. Please ensure to use ''\\'' at the end of everyline incase
          of using this command to train.


          Thanks.'
        updatedAt: '2023-08-02T21:40:37.008Z'
      numEdits: 6
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - poundian
        - JohnLaw
        - nauhc
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - poundian
        - sahaparamjit
    id: 64cacbf7667f4f8085251e68
    type: comment
  author: vishyrjun
  content: 'I was trying this in Google Colab and keep getting this error as well.
    I ensured accelerate and bitsandbytes are installed. still keeps getting this
    issue. Then realized i was having a particular line in my train command that was
    causing this issue


    autotrain llm --train --project_name ''\<project_name\>'' \\

    --model TinyPixel/Llama-2-7B-bf16-sharded \\

    --data_path timdettmers/openassistant-guanaco \\

    --text_column text \\

    --use_int4 \\

    --use_peft \\

    --learning_rate 2e-4 \\

    --train_batch_size 2 \\

    --num_train_epochs 3 \\

    --trainer sft \\

    --model_max_length 2048 \\

    --push_to_hub \\

    --repo_id \<repo_id\>/\<project_name\> \\

    --block_size 2048 > training.log &


    After i removed --use_int4 line and executed, the issue got resolved. I hope this
    helps. Please ensure to use ''\\'' at the end of everyline incase of using this
    command to train.


    Thanks.'
  created_at: 2023-08-02 20:34:47+00:00
  edited: true
  hidden: false
  id: 64cacbf7667f4f8085251e68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce6fb54b798872b4156382bae1af9c28.svg
      fullname: Adish Shah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: apshah
      type: user
    createdAt: '2023-08-03T00:19:43.000Z'
    data:
      edited: false
      editors:
      - apshah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9930446743965149
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce6fb54b798872b4156382bae1af9c28.svg
          fullname: Adish Shah
          isHf: false
          isPro: false
          name: apshah
          type: user
        html: '<p>having this issue as well, tried restarting the kernel like a 1000
          times, still didn''t work</p>

          '
        raw: having this issue as well, tried restarting the kernel like a 1000 times,
          still didn't work
        updatedAt: '2023-08-03T00:19:43.225Z'
      numEdits: 0
      reactions: []
    id: 64caf29f275c763046db6e6c
    type: comment
  author: apshah
  content: having this issue as well, tried restarting the kernel like a 1000 times,
    still didn't work
  created_at: 2023-08-02 23:19:43+00:00
  edited: false
  hidden: false
  id: 64caf29f275c763046db6e6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2c170cdbb2649862927a099dc6199619.svg
      fullname: Shahab
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shahabty
      type: user
    createdAt: '2023-08-03T00:23:34.000Z'
    data:
      edited: false
      editors:
      - shahabty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8945154547691345
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2c170cdbb2649862927a099dc6199619.svg
          fullname: Shahab
          isHf: false
          isPro: false
          name: shahabty
          type: user
        html: '<p>Here is the solution that worked for me:</p>

          <ol>

          <li>Install Nvidia docker + docker.</li>

          <li>Download an nvidia PyTorch 2.0 docker image with Cuda 12.</li>

          <li>Create and execute a container.</li>

          <li>Install all packages within the container and run your code.<br>(you
          might still need to try different version of dependencies but it finally
          worked for me)</li>

          </ol>

          '
        raw: "Here is the solution that worked for me:\n1. Install Nvidia docker +\
          \ docker.\n2. Download an nvidia PyTorch 2.0 docker image with Cuda 12.\n\
          3. Create and execute a container.\n4. Install all packages within the container\
          \ and run your code. \n(you might still need to try different version of\
          \ dependencies but it finally worked for me) "
        updatedAt: '2023-08-03T00:23:34.429Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Dorjzodovsuren
    id: 64caf386e984d09bed8a8066
    type: comment
  author: shahabty
  content: "Here is the solution that worked for me:\n1. Install Nvidia docker + docker.\n\
    2. Download an nvidia PyTorch 2.0 docker image with Cuda 12.\n3. Create and execute\
    \ a container.\n4. Install all packages within the container and run your code.\
    \ \n(you might still need to try different version of dependencies but it finally\
    \ worked for me) "
  created_at: 2023-08-02 23:23:34+00:00
  edited: false
  hidden: false
  id: 64caf386e984d09bed8a8066
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63c4deb2768b9a0bf8e056a56dd01694.svg
      fullname: Nehad Hirmiz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nehadhirmiz
      type: user
    createdAt: '2023-08-18T19:14:48.000Z'
    data:
      edited: true
      editors:
      - nehadhirmiz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6580156087875366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63c4deb2768b9a0bf8e056a56dd01694.svg
          fullname: Nehad Hirmiz
          isHf: false
          isPro: false
          name: nehadhirmiz
          type: user
        html: '<p>That was not the issue. My apologies &gt;&gt;&gt;</p>

          <p>In the import_utils.py the code used to check if package exist does not
          work for all packages. </p>

          <h1 id="todo-this-doesnt-work-for-all-packages-bs4-faiss-etc-talk-to-sylvain-to-see-how-to-do-with-it-better">TODO:
          This doesn''t work for all packages (<code>bs4</code>, <code>faiss</code>,
          etc.) Talk to Sylvain to see how to do with it better.</h1>

          <p>def _is_package_available(pkg_name: str, return_version: bool = False)
          -&gt; Union[Tuple[bool, str], bool]:<br>    # Check we''re not importing
          a "pkg_name" directory somewhere but the actual library by trying to grab
          the version<br>    package_exists = importlib.util.find_spec(pkg_name) is
          not None<br>    package_version = "N/A"<br>    if package_exists:<br>        try:<br>            package_version
          = importlib.metadata.version(pkg_name)<br>            package_exists = True<br>        except
          importlib.metadata.PackageNotFoundError:<br>            package_exists =
          False<br>        logger.debug(f"Detected {pkg_name} version {package_version}")<br>    if
          return_version:<br>        return package_exists, package_version<br>    else:<br>        return
          package_exists</p>

          '
        raw: "That was not the issue. My apologies >>>\n\nIn the import_utils.py the\
          \ code used to check if package exist does not work for all packages. \n\
          \n# TODO: This doesn't work for all packages (`bs4`, `faiss`, etc.) Talk\
          \ to Sylvain to see how to do with it better.\ndef _is_package_available(pkg_name:\
          \ str, return_version: bool = False) -> Union[Tuple[bool, str], bool]:\n\
          \    # Check we're not importing a \"pkg_name\" directory somewhere but\
          \ the actual library by trying to grab the version\n    package_exists =\
          \ importlib.util.find_spec(pkg_name) is not None\n    package_version =\
          \ \"N/A\"\n    if package_exists:\n        try:\n            package_version\
          \ = importlib.metadata.version(pkg_name)\n            package_exists = True\n\
          \        except importlib.metadata.PackageNotFoundError:\n            package_exists\
          \ = False\n        logger.debug(f\"Detected {pkg_name} version {package_version}\"\
          )\n    if return_version:\n        return package_exists, package_version\n\
          \    else:\n        return package_exists"
        updatedAt: '2023-08-18T19:26:14.532Z'
      numEdits: 1
      reactions: []
    id: 64dfc328e8b6f3f3baa82f70
    type: comment
  author: nehadhirmiz
  content: "That was not the issue. My apologies >>>\n\nIn the import_utils.py the\
    \ code used to check if package exist does not work for all packages. \n\n# TODO:\
    \ This doesn't work for all packages (`bs4`, `faiss`, etc.) Talk to Sylvain to\
    \ see how to do with it better.\ndef _is_package_available(pkg_name: str, return_version:\
    \ bool = False) -> Union[Tuple[bool, str], bool]:\n    # Check we're not importing\
    \ a \"pkg_name\" directory somewhere but the actual library by trying to grab\
    \ the version\n    package_exists = importlib.util.find_spec(pkg_name) is not\
    \ None\n    package_version = \"N/A\"\n    if package_exists:\n        try:\n\
    \            package_version = importlib.metadata.version(pkg_name)\n        \
    \    package_exists = True\n        except importlib.metadata.PackageNotFoundError:\n\
    \            package_exists = False\n        logger.debug(f\"Detected {pkg_name}\
    \ version {package_version}\")\n    if return_version:\n        return package_exists,\
    \ package_version\n    else:\n        return package_exists"
  created_at: 2023-08-18 18:14:48+00:00
  edited: true
  hidden: false
  id: 64dfc328e8b6f3f3baa82f70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/de9f1e8ea28f78c917358963bb720e74.svg
      fullname: 'pham llm '
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pham-llm
      type: user
    createdAt: '2023-09-13T23:03:12.000Z'
    data:
      edited: false
      editors:
      - pham-llm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9746699929237366
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/de9f1e8ea28f78c917358963bb720e74.svg
          fullname: 'pham llm '
          isHf: false
          isPro: false
          name: pham-llm
          type: user
        html: '<p>Did this work for anyone? I am facing the same issue</p>

          '
        raw: Did this work for anyone? I am facing the same issue
        updatedAt: '2023-09-13T23:03:12.696Z'
      numEdits: 0
      reactions: []
    id: 65023fb06840f21513ec9395
    type: comment
  author: pham-llm
  content: Did this work for anyone? I am facing the same issue
  created_at: 2023-09-13 22:03:12+00:00
  edited: false
  hidden: false
  id: 65023fb06840f21513ec9395
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8d5fbee6172069a921c9932a97352157.svg
      fullname: i
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gidz
      type: user
    createdAt: '2023-09-15T01:53:13.000Z'
    data:
      edited: true
      editors:
      - Gidz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6466420292854309
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8d5fbee6172069a921c9932a97352157.svg
          fullname: i
          isHf: false
          isPro: false
          name: Gidz
          type: user
        html: '<p>same issue</p>

          <p>I think there might be an underlying problem with bitsandbytes.</p>

          <p>I''m using slightly different scenario, but the same library and same
          results..<br>I''m on on a cpu only and been relying on tutorials from:<br><a
          href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">https://huggingface.co/blog/4bit-transformers-bitsandbytes</a><br><a
          href="https://huggingface.co/docs/transformers/main_classes/quantization#general-usage">https://huggingface.co/docs/transformers/main_classes/quantization#general-usage</a></p>

          <p>$ sudo pip install bitsandbytes accelerate transformers</p>

          <p>''&gt;&gt;&gt; from accelerate import Accelerator<br>''&gt;&gt;&gt; from
          transformers import AutoModelForCausalLM</p>

          <p>''&gt;&gt;&gt; path = Path(''/models/summarization/bart-large-cnn'')<br>''&gt;&gt;&gt;
          model_8bit = AutoModelForCausalLM.from_pretrained(path, load_in_8bit=True,
          device_map="auto")<br>''&gt;&gt;&gt; model_4bit = AutoModelForCausalLM.from_pretrained(path,
          load_in_4bit=True, device_map="auto")</p>

          <p>ImportError: Using <code>load_in_8bit=True</code> requires Accelerate:
          <code>pip install accelerate</code> and the latest version of bitsandbytes
          <code>pip install -i https://test.pypi.org/simple/ bitsandbytes</code> or
          pip install bitsandbytes`</p>

          '
        raw: "same issue\n\nI think there might be an underlying problem with bitsandbytes.\n\
          \nI'm using slightly different scenario, but the same library and same results..\
          \ \nI'm on on a cpu only and been relying on tutorials from: \nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\n\
          https://huggingface.co/docs/transformers/main_classes/quantization#general-usage\n\
          \n$ sudo pip install bitsandbytes accelerate transformers\n\n\n'>>> from\
          \ accelerate import Accelerator\n'>>> from transformers import AutoModelForCausalLM\n\
          \n'>>> path = Path('/models/summarization/bart-large-cnn')\n'>>> model_8bit\
          \ = AutoModelForCausalLM.from_pretrained(path, load_in_8bit=True, device_map=\"\
          auto\")\n'>>> model_4bit = AutoModelForCausalLM.from_pretrained(path, load_in_4bit=True,\
          \ device_map=\"auto\")\n\nImportError: Using `load_in_8bit=True` requires\
          \ Accelerate: `pip install accelerate` and the latest version of bitsandbytes\
          \ `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install\
          \ bitsandbytes`\n\n\n"
        updatedAt: '2023-09-15T03:24:49.973Z'
      numEdits: 3
      reactions: []
    id: 6503b9096107342343795293
    type: comment
  author: Gidz
  content: "same issue\n\nI think there might be an underlying problem with bitsandbytes.\n\
    \nI'm using slightly different scenario, but the same library and same results..\
    \ \nI'm on on a cpu only and been relying on tutorials from: \nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\n\
    https://huggingface.co/docs/transformers/main_classes/quantization#general-usage\n\
    \n$ sudo pip install bitsandbytes accelerate transformers\n\n\n'>>> from accelerate\
    \ import Accelerator\n'>>> from transformers import AutoModelForCausalLM\n\n'>>>\
    \ path = Path('/models/summarization/bart-large-cnn')\n'>>> model_8bit = AutoModelForCausalLM.from_pretrained(path,\
    \ load_in_8bit=True, device_map=\"auto\")\n'>>> model_4bit = AutoModelForCausalLM.from_pretrained(path,\
    \ load_in_4bit=True, device_map=\"auto\")\n\nImportError: Using `load_in_8bit=True`\
    \ requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes\
    \ `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`\n\
    \n\n"
  created_at: 2023-09-15 00:53:13+00:00
  edited: true
  hidden: false
  id: 6503b9096107342343795293
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8d5fbee6172069a921c9932a97352157.svg
      fullname: i
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Gidz
      type: user
    createdAt: '2023-09-15T03:26:47.000Z'
    data:
      edited: false
      editors:
      - Gidz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9194660782814026
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8d5fbee6172069a921c9932a97352157.svg
          fullname: i
          isHf: false
          isPro: false
          name: Gidz
          type: user
        html: '<p>after more digging around, you have to downgrade your version of
          transformers &gt; pip install transformers==4.32.0<br>that enabled the load_in_8bit
          to be recognised, but still doesn''t work for CPU.. accepts GPU only.</p>

          '
        raw: 'after more digging around, you have to downgrade your version of transformers
          > pip install transformers==4.32.0

          that enabled the load_in_8bit to be recognised, but still doesn''t work
          for CPU.. accepts GPU only.'
        updatedAt: '2023-09-15T03:26:47.734Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - thefoot
        - rahulrajpv
        - larpig
        - fagbavor
    id: 6503cef73c45013f4d129124
    type: comment
  author: Gidz
  content: 'after more digging around, you have to downgrade your version of transformers
    > pip install transformers==4.32.0

    that enabled the load_in_8bit to be recognised, but still doesn''t work for CPU..
    accepts GPU only.'
  created_at: 2023-09-15 02:26:47+00:00
  edited: false
  hidden: false
  id: 6503cef73c45013f4d129124
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647f5c73e9c81260ff87a3b4/w1ej8dhwV02F6tBfvP3zx.jpeg?w=200&h=200&f=face
      fullname: zaur samedov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zaursamedov1
      type: user
    createdAt: '2023-09-17T10:08:58.000Z'
    data:
      edited: false
      editors:
      - zaursamedov1
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7621933817863464
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647f5c73e9c81260ff87a3b4/w1ej8dhwV02F6tBfvP3zx.jpeg?w=200&h=200&f=face
          fullname: zaur samedov
          isHf: false
          isPro: false
          name: zaursamedov1
          type: user
        html: '<p>I''m getting this error too<br>"ImportError: Using <code>load_in_8bit=True</code>
          requires Accelerate: <code>pip install accelerate</code> and the latest
          version of bitsandbytes <code>pip install -i https://test.pypi.org/simple/
          bitsandbytes</code> or pip install bitsandbytes`" ~ while using AutoModelCasualLM.<br>downgrading
          transformer didn''t  workout, installing/upgrading accelerate and bitsandbytes
          either didn''t work. I''m using vscode on Mac m2.</p>

          '
        raw: "I'm getting this error too \n\"ImportError: Using `load_in_8bit=True`\
          \ requires Accelerate: `pip install accelerate` and the latest version of\
          \ bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes`\
          \ or pip install bitsandbytes`\" ~ while using AutoModelCasualLM.\ndowngrading\
          \ transformer didn't  workout, installing/upgrading accelerate and bitsandbytes\
          \ either didn't work. I'm using vscode on Mac m2."
        updatedAt: '2023-09-17T10:08:58.855Z'
      numEdits: 0
      reactions: []
    id: 6506d03a9622235d7dbd2c87
    type: comment
  author: zaursamedov1
  content: "I'm getting this error too \n\"ImportError: Using `load_in_8bit=True`\
    \ requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes\
    \ `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes`\"\
    \ ~ while using AutoModelCasualLM.\ndowngrading transformer didn't  workout, installing/upgrading\
    \ accelerate and bitsandbytes either didn't work. I'm using vscode on Mac m2."
  created_at: 2023-09-17 09:08:58+00:00
  edited: false
  hidden: false
  id: 6506d03a9622235d7dbd2c87
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/967904a1b2c274ee47fef4c04f464f02.svg
      fullname: Vikram Nitin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vikramnitin9
      type: user
    createdAt: '2023-09-17T21:55:02.000Z'
    data:
      edited: false
      editors:
      - vikramnitin9
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9922274947166443
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/967904a1b2c274ee47fef4c04f464f02.svg
          fullname: Vikram Nitin
          isHf: false
          isPro: false
          name: vikramnitin9
          type: user
        html: '<p>You might also need scipy.</p>

          '
        raw: You might also need scipy.
        updatedAt: '2023-09-17T21:55:02.472Z'
      numEdits: 0
      reactions: []
    id: 650775b6a7ba30bf62c52c9b
    type: comment
  author: vikramnitin9
  content: You might also need scipy.
  created_at: 2023-09-17 20:55:02+00:00
  edited: false
  hidden: false
  id: 650775b6a7ba30bf62c52c9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e4f2b81e625cbde08bdfd1c6bfda9dec.svg
      fullname: Adraju Anudeep
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anudeepadi
      type: user
    createdAt: '2023-09-21T14:09:40.000Z'
    data:
      edited: false
      editors:
      - anudeepadi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8781555891036987
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e4f2b81e625cbde08bdfd1c6bfda9dec.svg
          fullname: Adraju Anudeep
          isHf: false
          isPro: false
          name: anudeepadi
          type: user
        html: '<p>I downgraded transformers library to version 4.30 using the following
          command:<br>pip install transformers==4.30<br>Then I restarted the kernel
          it worked </p>

          '
        raw: "I downgraded transformers library to version 4.30 using the following\
          \ command:\npip install transformers==4.30 \nThen I restarted the kernel\
          \ it worked "
        updatedAt: '2023-09-21T14:09:40.053Z'
      numEdits: 0
      reactions:
      - count: 42
        reaction: "\U0001F44D"
        users:
        - hg402
        - pol000
        - veyselaytekinbyte
        - Andalf
        - gsgbills
        - strawhat
        - shahzainmehboob
        - bharatcoder
        - chitransh
        - zabirauf
        - EulerCat
        - berkouille
        - chaine09
        - gpreddy685
        - halacli
        - jinut
        - bagu
        - DevWC
        - Zohair101
        - prasun12377
        - Hadeel7
        - WillzWayn
        - phosseini
        - tinaaaaalee
        - youyu0105
        - Parisan
        - excellentso
        - morgana-rodrigues
        - geronimaw
        - mimimimiguo
        - achmedzhanov
        - yturkunov
        - deblagoj
        - StringCheese
        - juliarozanova
        - olivergrace006
        - Prasant
        - CiliaMadni
        - vapa
        - rajdeep8709
        - dohonba
        - Akarsth
      - count: 18
        reaction: "\u2764\uFE0F"
        users:
        - veyselaytekinbyte
        - Andalf
        - EulerCat
        - bagu
        - Hadeel7
        - WillzWayn
        - tinaaaaalee
        - youyu0105
        - Parisan
        - excellentso
        - jdIcf
        - morgana-rodrigues
        - geronimaw
        - juliarozanova
        - CiliaMadni
        - rajdeep8709
        - duongkstn
        - dohonba
    id: 650c4ea403e1ec1fc2ff0714
    type: comment
  author: anudeepadi
  content: "I downgraded transformers library to version 4.30 using the following\
    \ command:\npip install transformers==4.30 \nThen I restarted the kernel it worked "
  created_at: 2023-09-21 13:09:40+00:00
  edited: false
  hidden: false
  id: 650c4ea403e1ec1fc2ff0714
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/79344d6dd6c36d98a03e9f9153b1e8e2.svg
      fullname: Manikanta P
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mani22
      type: user
    createdAt: '2023-09-22T21:30:49.000Z'
    data:
      edited: false
      editors:
      - mani22
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9171276688575745
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/79344d6dd6c36d98a03e9f9153b1e8e2.svg
          fullname: Manikanta P
          isHf: false
          isPro: false
          name: mani22
          type: user
        html: '<p>Worked after downgrading transformers to 4.30.</p>

          '
        raw: Worked after downgrading transformers to 4.30.
        updatedAt: '2023-09-22T21:30:49.541Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\u2764\uFE0F"
        users:
        - veyselaytekinbyte
        - Andalf
        - strawhat
        - EulerCat
        - Hadeel7
        - WillzWayn
        - tinaaaaalee
        - Parisan
        - geronimaw
      - count: 6
        reaction: "\U0001F917"
        users:
        - veyselaytekinbyte
        - Andalf
        - EulerCat
        - Hadeel7
        - tinaaaaalee
        - Parisan
    id: 650e0789c305ec67e6f25873
    type: comment
  author: mani22
  content: Worked after downgrading transformers to 4.30.
  created_at: 2023-09-22 20:30:49+00:00
  edited: false
  hidden: false
  id: 650e0789c305ec67e6f25873
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/333ade364cc29eba01b5f3c2b2d985cf.svg
      fullname: Niranjan Sarkar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bharatcoder
      type: user
    createdAt: '2023-09-29T16:34:13.000Z'
    data:
      edited: false
      editors:
      - bharatcoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8778879642486572
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/333ade364cc29eba01b5f3c2b2d985cf.svg
          fullname: Niranjan Sarkar
          isHf: false
          isPro: false
          name: bharatcoder
          type: user
        html: '<p>Was getting the same issue, resolved after downgrading transformers.<br>pip
          install transformers==4.30</p>

          '
        raw: "Was getting the same issue, resolved after downgrading transformers.\
          \ \npip install transformers==4.30"
        updatedAt: '2023-09-29T16:34:13.386Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Hadeel7
        - stone315
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Hadeel7
        - geronimaw
    id: 6516fc85a28f86d3e9b450fc
    type: comment
  author: bharatcoder
  content: "Was getting the same issue, resolved after downgrading transformers. \n\
    pip install transformers==4.30"
  created_at: 2023-09-29 15:34:13+00:00
  edited: false
  hidden: false
  id: 6516fc85a28f86d3e9b450fc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/D5fy6v5eFpdVcg43yoB1r.png?w=200&h=200&f=face
      fullname: Yuki Sasaki
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nss-ysasaki
      type: user
    createdAt: '2023-10-03T08:21:33.000Z'
    data:
      edited: true
      editors:
      - nss-ysasaki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.715629518032074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/D5fy6v5eFpdVcg43yoB1r.png?w=200&h=200&f=face
          fullname: Yuki Sasaki
          isHf: false
          isPro: false
          name: nss-ysasaki
          type: user
        html: '<p>It turns out that the <code>ImportError: Using load_in_8bit=True
          requires Accelerate...</code> error message is shown when:</p>

          <ol>

          <li>The <code>accelerate</code> module is not found,</li>

          <li>The <code>bitsandbytes</code> module is not found, or</li>

          <li><strong><code>torch</code> does not recognize CUDA</strong> (<a rel="nofollow"
          href="https://github.com/huggingface/transformers/commit/d3ce048c2029c829e23c7d82f9abd4919610db87">PR</a>).</li>

          </ol>

          <p>If you can import both <code>accelerate</code> and <code>bitsandbytes</code>
          and still get this error, it might be that PyTorch is unable to see CUDA.</p>

          <p>You can check the CUDA availability with:</p>

          <pre><code class="language-python">torch.cuda.is_available()

          </code></pre>

          <p>If this evaluates to <code>False</code>, you might want to head to <a
          rel="nofollow" href="https://stackoverflow.com/q/55717751/13301046">StackOverflow</a>
          to see why PyTorch cannot recognize CUDA.</p>

          <p>(In my case, Docker misconfiguration prevented CUDA from loading, which
          resulted in <code>nvidia-smi</code> showing <code>CUDA Version: ERR!</code>,
          causing the cryptic <code>ImportError</code>...)</p>

          '
        raw: 'It turns out that the `ImportError: Using load_in_8bit=True requires
          Accelerate...` error message is shown when:


          1. The `accelerate` module is not found,

          2. The `bitsandbytes` module is not found, or

          3. **`torch` does not recognize CUDA** ([PR](https://github.com/huggingface/transformers/commit/d3ce048c2029c829e23c7d82f9abd4919610db87)).


          If you can import both `accelerate` and `bitsandbytes` and still get this
          error, it might be that PyTorch is unable to see CUDA.


          You can check the CUDA availability with:


          ```python

          torch.cuda.is_available()

          ```


          If this evaluates to `False`, you might want to head to [StackOverflow](https://stackoverflow.com/q/55717751/13301046)
          to see why PyTorch cannot recognize CUDA.


          (In my case, Docker misconfiguration prevented CUDA from loading, which
          resulted in `nvidia-smi` showing `CUDA Version: ERR!`, causing the cryptic
          `ImportError`...)'
        updatedAt: '2023-10-03T08:57:25.830Z'
      numEdits: 2
      reactions:
      - count: 2
        reaction: "\U0001F91D"
        users:
        - WillzWayn
        - unit99
      - count: 2
        reaction: "\U0001F44D"
        users:
        - QiiofRagnarok
        - unit99
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - EggerMarc
        - unit99
    id: 651bcf0ddea81981d506ee2d
    type: comment
  author: nss-ysasaki
  content: 'It turns out that the `ImportError: Using load_in_8bit=True requires Accelerate...`
    error message is shown when:


    1. The `accelerate` module is not found,

    2. The `bitsandbytes` module is not found, or

    3. **`torch` does not recognize CUDA** ([PR](https://github.com/huggingface/transformers/commit/d3ce048c2029c829e23c7d82f9abd4919610db87)).


    If you can import both `accelerate` and `bitsandbytes` and still get this error,
    it might be that PyTorch is unable to see CUDA.


    You can check the CUDA availability with:


    ```python

    torch.cuda.is_available()

    ```


    If this evaluates to `False`, you might want to head to [StackOverflow](https://stackoverflow.com/q/55717751/13301046)
    to see why PyTorch cannot recognize CUDA.


    (In my case, Docker misconfiguration prevented CUDA from loading, which resulted
    in `nvidia-smi` showing `CUDA Version: ERR!`, causing the cryptic `ImportError`...)'
  created_at: 2023-10-03 07:21:33+00:00
  edited: true
  hidden: false
  id: 651bcf0ddea81981d506ee2d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d506a22b7245462b20560bddd95f5b3a.svg
      fullname: Bruno Dirkx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: programmeurke
      type: user
    createdAt: '2023-10-03T19:00:02.000Z'
    data:
      edited: false
      editors:
      - programmeurke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7552886605262756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d506a22b7245462b20560bddd95f5b3a.svg
          fullname: Bruno Dirkx
          isHf: false
          isPro: false
          name: programmeurke
          type: user
        html: '<p>Hi, I got the same error using a wrapper library called Ludwig.<br>It
          had a configuration parameter<br>quantization: bits: 4<br>By removing this
          configuration setting, the error disappeared.</p>

          '
        raw: 'Hi, I got the same error using a wrapper library called Ludwig.

          It had a configuration parameter

          quantization: bits: 4

          By removing this configuration setting, the error disappeared.'
        updatedAt: '2023-10-03T19:00:02.807Z'
      numEdits: 0
      reactions: []
    id: 651c64b2830ddd7cb71f5ae2
    type: comment
  author: programmeurke
  content: 'Hi, I got the same error using a wrapper library called Ludwig.

    It had a configuration parameter

    quantization: bits: 4

    By removing this configuration setting, the error disappeared.'
  created_at: 2023-10-03 18:00:02+00:00
  edited: false
  hidden: false
  id: 651c64b2830ddd7cb71f5ae2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/hKuFvz6IAqUuP4vIPexoc.png?w=200&h=200&f=face
      fullname: IntelliDynamics
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IntelliDynamics
      type: user
    createdAt: '2023-10-04T05:14:30.000Z'
    data:
      edited: true
      editors:
      - IntelliDynamics
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710983633995056
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/hKuFvz6IAqUuP4vIPexoc.png?w=200&h=200&f=face
          fullname: IntelliDynamics
          isHf: false
          isPro: false
          name: IntelliDynamics
          type: user
        html: '<p>At first, I installed the current version of Accelerate and had
          the error.  I then backdated by installing an old version of Accelerate
          which worked, but it didn''t support another pkg, so I installed the current
          version again, and it worked.  Behavior like this suggests there is a history
          of versions that are required, with lurking version control issues in Accelerate
          and/or one of the packages it depends on.  This is a mess, but I''m finally
          running after many hours.</p>

          '
        raw: At first, I installed the current version of Accelerate and had the error.  I
          then backdated by installing an old version of Accelerate which worked,
          but it didn't support another pkg, so I installed the current version again,
          and it worked.  Behavior like this suggests there is a history of versions
          that are required, with lurking version control issues in Accelerate and/or
          one of the packages it depends on.  This is a mess, but I'm finally running
          after many hours.
        updatedAt: '2023-10-04T05:15:12.323Z'
      numEdits: 1
      reactions: []
    id: 651cf4b60c0c6b8fc86aff2a
    type: comment
  author: IntelliDynamics
  content: At first, I installed the current version of Accelerate and had the error.  I
    then backdated by installing an old version of Accelerate which worked, but it
    didn't support another pkg, so I installed the current version again, and it worked.  Behavior
    like this suggests there is a history of versions that are required, with lurking
    version control issues in Accelerate and/or one of the packages it depends on.  This
    is a mess, but I'm finally running after many hours.
  created_at: 2023-10-04 04:14:30+00:00
  edited: true
  hidden: false
  id: 651cf4b60c0c6b8fc86aff2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d2801210a7fff4fd972f15f8172d4071.svg
      fullname: Umberto Cocca
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: okamirvs
      type: user
    createdAt: '2023-10-08T20:42:43.000Z'
    data:
      edited: false
      editors:
      - okamirvs
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5389030575752258
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d2801210a7fff4fd972f15f8172d4071.svg
          fullname: Umberto Cocca
          isHf: false
          isPro: false
          name: okamirvs
          type: user
        html: '<p>This worked for me:</p>

          <pre><code>pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

          python -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui

          pip install transformers==4.34.0

          pip install trl==0.7.1

          pip install datasets==2.14.5

          </code></pre>

          <p>Basically you have to use that <a rel="nofollow" href="https://github.com/jllllll/bitsandbytes-windows-webui">bitsandbytes</a>
          instead of <em>pip install bitsandbytes</em></p>

          '
        raw: 'This worked for me:


          ```

          pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

          python -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui

          pip install transformers==4.34.0

          pip install trl==0.7.1

          pip install datasets==2.14.5

          ```


          Basically you have to use that [bitsandbytes](https://github.com/jllllll/bitsandbytes-windows-webui)
          instead of *pip install bitsandbytes*'
        updatedAt: '2023-10-08T20:42:43.450Z'
      numEdits: 0
      reactions: []
    id: 652314430f935fa8fd600776
    type: comment
  author: okamirvs
  content: 'This worked for me:


    ```

    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

    python -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui

    pip install transformers==4.34.0

    pip install trl==0.7.1

    pip install datasets==2.14.5

    ```


    Basically you have to use that [bitsandbytes](https://github.com/jllllll/bitsandbytes-windows-webui)
    instead of *pip install bitsandbytes*'
  created_at: 2023-10-08 19:42:43+00:00
  edited: false
  hidden: false
  id: 652314430f935fa8fd600776
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/643286d3b910912fd2ac1b8bf2429104.svg
      fullname: Rui C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZRuiii
      type: user
    createdAt: '2023-10-19T07:16:49.000Z'
    data:
      edited: false
      editors:
      - ZRuiii
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8817770481109619
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/643286d3b910912fd2ac1b8bf2429104.svg
          fullname: Rui C
          isHf: false
          isPro: false
          name: ZRuiii
          type: user
        html: '<p>I guess this can be a bug while importing packages. I have met the
          same problems, and add <code>from peft import PeftModel, PeftConfig</code>
          at the beginning worked for me.</p>

          '
        raw: I guess this can be a bug while importing packages. I have met the same
          problems, and add `from peft import PeftModel, PeftConfig` at the beginning
          worked for me.
        updatedAt: '2023-10-19T07:16:49.082Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jaeboum
    id: 6530d7e10b8cc3a8dea23252
    type: comment
  author: ZRuiii
  content: I guess this can be a bug while importing packages. I have met the same
    problems, and add `from peft import PeftModel, PeftConfig` at the beginning worked
    for me.
  created_at: 2023-10-19 06:16:49+00:00
  edited: false
  hidden: false
  id: 6530d7e10b8cc3a8dea23252
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7f2f505c105e12eb0abaf80ed672adcc.svg
      fullname: ddk
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dgkd
      type: user
    createdAt: '2023-10-19T08:38:48.000Z'
    data:
      edited: false
      editors:
      - dgkd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8864359855651855
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7f2f505c105e12eb0abaf80ed672adcc.svg
          fullname: ddk
          isHf: false
          isPro: false
          name: dgkd
          type: user
        html: "<p>I pip install requirements==4.28.0\uFF0Cand it works! I found in\
          \ environments installation,the requirements.txt shows transformers&gt;=4.28.0,so\
          \ I did so.</p>\n"
        raw: "I pip install requirements==4.28.0\uFF0Cand it works! I found in environments\
          \ installation,the requirements.txt shows transformers>=4.28.0,so I did\
          \ so."
        updatedAt: '2023-10-19T08:38:48.347Z'
      numEdits: 0
      reactions: []
    id: 6530eb18108a96b7150d5f75
    type: comment
  author: dgkd
  content: "I pip install requirements==4.28.0\uFF0Cand it works! I found in environments\
    \ installation,the requirements.txt shows transformers>=4.28.0,so I did so."
  created_at: 2023-10-19 07:38:48+00:00
  edited: false
  hidden: false
  id: 6530eb18108a96b7150d5f75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qPBqh5x8SbBtx0Aaut4X2.png?w=200&h=200&f=face
      fullname: Elnara Mammadova
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: elnaram
      type: user
    createdAt: '2023-11-03T10:55:59.000Z'
    data:
      edited: false
      editors:
      - elnaram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7076525688171387
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/qPBqh5x8SbBtx0Aaut4X2.png?w=200&h=200&f=face
          fullname: Elnara Mammadova
          isHf: false
          isPro: false
          name: elnaram
          type: user
        html: '<p>I used <code>llm_int8_enable_fp32_cpu_offload=True</code> instead
          of <code>load_in_8bit=True</code>, with transformer version 4.30, and it
          worked. </p>

          '
        raw: 'I used `llm_int8_enable_fp32_cpu_offload=True` instead of `load_in_8bit=True`,
          with transformer version 4.30, and it worked. '
        updatedAt: '2023-11-03T10:55:59.018Z'
      numEdits: 0
      reactions: []
    id: 6544d1bf5b5d9185ba3e1581
    type: comment
  author: elnaram
  content: 'I used `llm_int8_enable_fp32_cpu_offload=True` instead of `load_in_8bit=True`,
    with transformer version 4.30, and it worked. '
  created_at: 2023-11-03 09:55:59+00:00
  edited: false
  hidden: false
  id: 6544d1bf5b5d9185ba3e1581
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d518d538552bce32f9f1412bbc7e05c6.svg
      fullname: Aryan Jangid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aryanjangid
      type: user
    createdAt: '2023-11-04T10:14:41.000Z'
    data:
      edited: false
      editors:
      - aryanjangid
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.42881646752357483
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d518d538552bce32f9f1412bbc7e05c6.svg
          fullname: Aryan Jangid
          isHf: false
          isPro: false
          name: aryanjangid
          type: user
        html: '<p>I tried all the above solutions, but still getting this error<br>Here
          is my model.py code</p>

          <p>from config import config<br>from prompts import get_vlm_prompt, get_llm_prompt</p>

          <p>import torch</p>

          <p>from transformers import (<br>    BitsAndBytesConfig,<br>    InstructBlipProcessor,<br>    InstructBlipForConditionalGeneration,<br>)</p>

          <p>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br>double_quant_config
          = BitsAndBytesConfig(load_in_4bit=True,<br>                                            bnb_4bit_quant_type="nf4",<br>                                            bnb_4bit_use_double_quant=True,<br>                                            bnb_4bit_compute_dtype=torch.bfloat16)</p>

          <p>Here is my configuration (requirements.txt)</p>

          <p>fastapi==0.103.2<br>langchain==0.0.311<br>multion==0.2.2<br>openai==0.27.10<br>Pillow==10.0.1<br>pydantic==2.4.2<br>python-dotenv==1.0.0<br>torch==1.13.1<br>transformers==4.33.3<br>sentencepiece==0.1.99<br>accelerate==0.23.0<br>bitsandbytes==0.41.1<br>pydantic-settings==2.0.3<br>python-multipart==0.0.6</p>

          '
        raw: "I tried all the above solutions, but still getting this error\nHere\
          \ is my model.py code\n\nfrom config import config\nfrom prompts import\
          \ get_vlm_prompt, get_llm_prompt\n\nimport torch\n\nfrom transformers import\
          \ (\n    BitsAndBytesConfig,\n    InstructBlipProcessor, \n    InstructBlipForConditionalGeneration,\n\
          )\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"\
          cpu\")\ndouble_quant_config = BitsAndBytesConfig(load_in_4bit=True,\n  \
          \                                          bnb_4bit_quant_type=\"nf4\",\n\
          \                                            bnb_4bit_use_double_quant=True,\n\
          \                                            bnb_4bit_compute_dtype=torch.bfloat16)\n\
          \n\nHere is my configuration (requirements.txt)\n\nfastapi==0.103.2\nlangchain==0.0.311\n\
          multion==0.2.2\nopenai==0.27.10\nPillow==10.0.1\npydantic==2.4.2\npython-dotenv==1.0.0\n\
          torch==1.13.1\ntransformers==4.33.3\nsentencepiece==0.1.99\naccelerate==0.23.0\n\
          bitsandbytes==0.41.1\npydantic-settings==2.0.3\npython-multipart==0.0.6"
        updatedAt: '2023-11-04T10:14:41.469Z'
      numEdits: 0
      reactions: []
    id: 65461991cf50edb69fcef1f6
    type: comment
  author: aryanjangid
  content: "I tried all the above solutions, but still getting this error\nHere is\
    \ my model.py code\n\nfrom config import config\nfrom prompts import get_vlm_prompt,\
    \ get_llm_prompt\n\nimport torch\n\nfrom transformers import (\n    BitsAndBytesConfig,\n\
    \    InstructBlipProcessor, \n    InstructBlipForConditionalGeneration,\n)\n\n\
    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndouble_quant_config\
    \ = BitsAndBytesConfig(load_in_4bit=True,\n                                  \
    \          bnb_4bit_quant_type=\"nf4\",\n                                    \
    \        bnb_4bit_use_double_quant=True,\n                                   \
    \         bnb_4bit_compute_dtype=torch.bfloat16)\n\n\nHere is my configuration\
    \ (requirements.txt)\n\nfastapi==0.103.2\nlangchain==0.0.311\nmultion==0.2.2\n\
    openai==0.27.10\nPillow==10.0.1\npydantic==2.4.2\npython-dotenv==1.0.0\ntorch==1.13.1\n\
    transformers==4.33.3\nsentencepiece==0.1.99\naccelerate==0.23.0\nbitsandbytes==0.41.1\n\
    pydantic-settings==2.0.3\npython-multipart==0.0.6"
  created_at: 2023-11-04 09:14:41+00:00
  edited: false
  hidden: false
  id: 65461991cf50edb69fcef1f6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f128535c2a339a031d5f7571e7ef4250.svg
      fullname: Surya Pratap Singh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: suryasinghmv
      type: user
    createdAt: '2023-11-08T17:16:40.000Z'
    data:
      edited: false
      editors:
      - suryasinghmv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9663569331169128
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f128535c2a339a031d5f7571e7ef4250.svg
          fullname: Surya Pratap Singh
          isHf: false
          isPro: false
          name: suryasinghmv
          type: user
        html: '<p>I encountered a similar problem where I was facing an ImportError
          with the message "Using load_in_8bit=True requires Accelerate." </p>

          <p>To resolve this issue, I checked my transformers version, which was initially
          at 4.30.2. After downgrading it to version 4.30, the problem was successfully
          resolved.</p>

          '
        raw: "I encountered a similar problem where I was facing an ImportError with\
          \ the message \"Using load_in_8bit=True requires Accelerate.\" \n\nTo resolve\
          \ this issue, I checked my transformers version, which was initially at\
          \ 4.30.2. After downgrading it to version 4.30, the problem was successfully\
          \ resolved.\n"
        updatedAt: '2023-11-08T17:16:40.863Z'
      numEdits: 0
      reactions: []
    id: 654bc278d625e08338418ec0
    type: comment
  author: suryasinghmv
  content: "I encountered a similar problem where I was facing an ImportError with\
    \ the message \"Using load_in_8bit=True requires Accelerate.\" \n\nTo resolve\
    \ this issue, I checked my transformers version, which was initially at 4.30.2.\
    \ After downgrading it to version 4.30, the problem was successfully resolved.\n"
  created_at: 2023-11-08 17:16:40+00:00
  edited: false
  hidden: false
  id: 654bc278d625e08338418ec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d506a22b7245462b20560bddd95f5b3a.svg
      fullname: Bruno Dirkx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: programmeurke
      type: user
    createdAt: '2023-11-08T19:05:31.000Z'
    data:
      edited: false
      editors:
      - programmeurke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9850020408630371
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d506a22b7245462b20560bddd95f5b3a.svg
          fullname: Bruno Dirkx
          isHf: false
          isPro: false
          name: programmeurke
          type: user
        html: '<p>Anyone here with a Mac with M2 chip? That''s apparently cause of
          my problems. (everything seems to be made for NVidia GPUs)</p>

          '
        raw: Anyone here with a Mac with M2 chip? That's apparently cause of my problems.
          (everything seems to be made for NVidia GPUs)
        updatedAt: '2023-11-08T19:05:31.303Z'
      numEdits: 0
      reactions: []
    id: 654bdbfb24d9d9a1a58b2d5c
    type: comment
  author: programmeurke
  content: Anyone here with a Mac with M2 chip? That's apparently cause of my problems.
    (everything seems to be made for NVidia GPUs)
  created_at: 2023-11-08 19:05:31+00:00
  edited: false
  hidden: false
  id: 654bdbfb24d9d9a1a58b2d5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a180e865423b8824e1ca6ad8e4caa977.svg
      fullname: Khalesi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Amirkhalesi
      type: user
    createdAt: '2023-11-12T18:31:36.000Z'
    data:
      edited: false
      editors:
      - Amirkhalesi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9821248054504395
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a180e865423b8824e1ca6ad8e4caa977.svg
          fullname: Khalesi
          isHf: false
          isPro: false
          name: Amirkhalesi
          type: user
        html: '<p>Couldn''t find a workaround for CPU yet, but worked with transformers==4.35.0
          on CUDA</p>

          '
        raw: Couldn't find a workaround for CPU yet, but worked with transformers==4.35.0
          on CUDA
        updatedAt: '2023-11-12T18:31:36.226Z'
      numEdits: 0
      reactions: []
    id: 65511a08bebf0c4c51148c2c
    type: comment
  author: Amirkhalesi
  content: Couldn't find a workaround for CPU yet, but worked with transformers==4.35.0
    on CUDA
  created_at: 2023-11-12 18:31:36+00:00
  edited: false
  hidden: false
  id: 65511a08bebf0c4c51148c2c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e343d91b3cd26803757ca3f1e18d810c.svg
      fullname: sherri hadian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sherbika
      type: user
    createdAt: '2023-11-17T12:20:01.000Z'
    data:
      edited: false
      editors:
      - sherbika
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9471471309661865
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e343d91b3cd26803757ca3f1e18d810c.svg
          fullname: sherri hadian
          isHf: false
          isPro: false
          name: sherbika
          type: user
        html: '<p>I am using an apple M2 chip and can''t get around this error</p>

          '
        raw: 'I am using an apple M2 chip and can''t get around this error

          '
        updatedAt: '2023-11-17T12:20:01.628Z'
      numEdits: 0
      reactions: []
    id: 65575a71f7457c98b2111679
    type: comment
  author: sherbika
  content: 'I am using an apple M2 chip and can''t get around this error

    '
  created_at: 2023-11-17 12:20:01+00:00
  edited: false
  hidden: false
  id: 65575a71f7457c98b2111679
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/27e95cd0bfba06870be12822762d99cb.svg
      fullname: Peter Herdenborg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Petrux99
      type: user
    createdAt: '2023-11-24T13:22:12.000Z'
    data:
      edited: false
      editors:
      - Petrux99
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8766905069351196
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/27e95cd0bfba06870be12822762d99cb.svg
          fullname: Peter Herdenborg
          isHf: false
          isPro: false
          name: Petrux99
          type: user
        html: "<p>I'm on an M1 and also can't get this to work. I dug around a bit\
          \ quickly in the Transformers  source code and found this which seems to\
          \ indicate there is no hope to get this working for us Mac users:</p>\n\
          <pre><code>def is_bitsandbytes_available():\n    if not is_torch_available():\n\
          \        return False\n\n    # bitsandbytes throws an error if cuda is not\
          \ available\n    # let's avoid that by adding a simple check\n    import\
          \ torch\n\n    return _bitsandbytes_available and torch.cuda.is_available()\n\
          </code></pre>\n<p>Note the requirement for <code>torch.cuda.is_available()</code></p>\n\
          <p>Also, bitsandbytes seems to officially only support CUDA, with this issue\
          \ about supporting MPS being open and not acted upon: <a rel=\"nofollow\"\
          \ href=\"https://github.com/TimDettmers/bitsandbytes/issues/252\">https://github.com/TimDettmers/bitsandbytes/issues/252</a></p>\n\
          <p>Apologies if any of the above analysis is wrong. I'm pretty new to this\
          \ all.</p>\n"
        raw: "I'm on an M1 and also can't get this to work. I dug around a bit quickly\
          \ in the Transformers  source code and found this which seems to indicate\
          \ there is no hope to get this working for us Mac users:\n\n```\ndef is_bitsandbytes_available():\n\
          \    if not is_torch_available():\n        return False\n\n    # bitsandbytes\
          \ throws an error if cuda is not available\n    # let's avoid that by adding\
          \ a simple check\n    import torch\n\n    return _bitsandbytes_available\
          \ and torch.cuda.is_available()\n```\nNote the requirement for `torch.cuda.is_available()`\n\
          \nAlso, bitsandbytes seems to officially only support CUDA, with this issue\
          \ about supporting MPS being open and not acted upon: https://github.com/TimDettmers/bitsandbytes/issues/252\n\
          \nApologies if any of the above analysis is wrong. I'm pretty new to this\
          \ all."
        updatedAt: '2023-11-24T13:22:12.565Z'
      numEdits: 0
      reactions: []
    id: 6560a384e0a7720b6ad52904
    type: comment
  author: Petrux99
  content: "I'm on an M1 and also can't get this to work. I dug around a bit quickly\
    \ in the Transformers  source code and found this which seems to indicate there\
    \ is no hope to get this working for us Mac users:\n\n```\ndef is_bitsandbytes_available():\n\
    \    if not is_torch_available():\n        return False\n\n    # bitsandbytes\
    \ throws an error if cuda is not available\n    # let's avoid that by adding a\
    \ simple check\n    import torch\n\n    return _bitsandbytes_available and torch.cuda.is_available()\n\
    ```\nNote the requirement for `torch.cuda.is_available()`\n\nAlso, bitsandbytes\
    \ seems to officially only support CUDA, with this issue about supporting MPS\
    \ being open and not acted upon: https://github.com/TimDettmers/bitsandbytes/issues/252\n\
    \nApologies if any of the above analysis is wrong. I'm pretty new to this all."
  created_at: 2023-11-24 13:22:12+00:00
  edited: false
  hidden: false
  id: 6560a384e0a7720b6ad52904
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/17b673346a54193f22d4af76ca050bdb.svg
      fullname: Draco Dev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DracoDev
      type: user
    createdAt: '2023-11-30T10:53:04.000Z'
    data:
      edited: false
      editors:
      - DracoDev
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3852468729019165
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/17b673346a54193f22d4af76ca050bdb.svg
          fullname: Draco Dev
          isHf: false
          isPro: false
          name: DracoDev
          type: user
        html: '<p>This is the kind of error that can occur when downgrading transformers
          to accomodate this error on other new models like Zephry-7b-beta based on
          mistral. </p>

          <p>----&gt; 8 llm = HuggingFaceLLM(<br>      9     model_name="HuggingFaceH4/zephyr-7b-beta",<br>     10     tokenizer_name="HuggingFaceH4/zephyr-7b-beta",</p>

          <p>/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py
          in <strong>getitem</strong>(self, key)<br>    669             return self._extra_content[key]<br>    670         if
          key not in self._mapping:<br>--&gt; 671             raise KeyError(key)<br>    672         value
          = self._mapping[key]<br>    673         module_name = model_type_to_module_name(key)<br>KeyError:
          ''mistral''</p>

          '
        raw: "This is the kind of error that can occur when downgrading transformers\
          \ to accomodate this error on other new models like Zephry-7b-beta based\
          \ on mistral. \n\n----> 8 llm = HuggingFaceLLM(\n      9     model_name=\"\
          HuggingFaceH4/zephyr-7b-beta\",\n     10     tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\"\
          ,\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\
          \ in __getitem__(self, key)\n    669             return self._extra_content[key]\n\
          \    670         if key not in self._mapping:\n--> 671             raise\
          \ KeyError(key)\n    672         value = self._mapping[key]\n    673   \
          \      module_name = model_type_to_module_name(key)\nKeyError: 'mistral'\n\
          \n"
        updatedAt: '2023-11-30T10:53:04.986Z'
      numEdits: 0
      reactions: []
    id: 65686990ad0006b85282cb8a
    type: comment
  author: DracoDev
  content: "This is the kind of error that can occur when downgrading transformers\
    \ to accomodate this error on other new models like Zephry-7b-beta based on mistral.\
    \ \n\n----> 8 llm = HuggingFaceLLM(\n      9     model_name=\"HuggingFaceH4/zephyr-7b-beta\"\
    ,\n     10     tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\
    \ in __getitem__(self, key)\n    669             return self._extra_content[key]\n\
    \    670         if key not in self._mapping:\n--> 671             raise KeyError(key)\n\
    \    672         value = self._mapping[key]\n    673         module_name = model_type_to_module_name(key)\n\
    KeyError: 'mistral'\n\n"
  created_at: 2023-11-30 10:53:04+00:00
  edited: false
  hidden: false
  id: 65686990ad0006b85282cb8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/81d0d5c948976f797dec76631cf68fc8.svg
      fullname: odewole
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: segestic
      type: user
    createdAt: '2023-12-01T15:19:45.000Z'
    data:
      edited: false
      editors:
      - segestic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8286547064781189
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/81d0d5c948976f797dec76631cf68fc8.svg
          fullname: odewole
          isHf: false
          isPro: false
          name: segestic
          type: user
        html: '<p>If using Colab or any other notebooks, Ensure to change from CPU
          to GPU. This solved this error in my case!</p>

          '
        raw: If using Colab or any other notebooks, Ensure to change from CPU to GPU.
          This solved this error in my case!
        updatedAt: '2023-12-01T15:19:45.303Z'
      numEdits: 0
      reactions: []
    id: 6569f9917fd1d42138249ec2
    type: comment
  author: segestic
  content: If using Colab or any other notebooks, Ensure to change from CPU to GPU.
    This solved this error in my case!
  created_at: 2023-12-01 15:19:45+00:00
  edited: false
  hidden: false
  id: 6569f9917fd1d42138249ec2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/639f05c4beb95d698dd5ca73/GPtKkYPJDV_oe3jiYsahF.jpeg?w=200&h=200&f=face
      fullname: Louis de Beaumont
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: poundian
      type: user
    createdAt: '2023-12-02T14:36:05.000Z'
    data:
      edited: false
      editors:
      - poundian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.970543384552002
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/639f05c4beb95d698dd5ca73/GPtKkYPJDV_oe3jiYsahF.jpeg?w=200&h=200&f=face
          fullname: Louis de Beaumont
          isHf: false
          isPro: false
          name: poundian
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;vishyrjun&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/vishyrjun\">@<span class=\"\
          underline\">vishyrjun</span></a></span>\n\n\t</span></span>'s answer was\
          \ the solution for me, using AutoTrain Advanced hosted UI.</p>\n"
        raw: '@vishyrjun''s answer was the solution for me, using AutoTrain Advanced
          hosted UI.'
        updatedAt: '2023-12-02T14:36:05.737Z'
      numEdits: 0
      reactions: []
    id: 656b40d53e60cb2621806cf7
    type: comment
  author: poundian
  content: '@vishyrjun''s answer was the solution for me, using AutoTrain Advanced
    hosted UI.'
  created_at: 2023-12-02 14:36:05+00:00
  edited: false
  hidden: false
  id: 656b40d53e60cb2621806cf7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a944de2b32a4e8e9d32204d954229617.svg
      fullname: Yin King Law
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JohnLaw
      type: user
    createdAt: '2023-12-04T03:30:58.000Z'
    data:
      edited: false
      editors:
      - JohnLaw
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7028888463973999
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a944de2b32a4e8e9d32204d954229617.svg
          fullname: Yin King Law
          isHf: false
          isPro: false
          name: JohnLaw
          type: user
        html: '<blockquote>

          <p>I was trying this in Google Colab and keep getting this error as well.
          I ensured accelerate and bitsandbytes are installed. still keeps getting
          this issue. Then realized i was having a particular line in my train command
          that was causing this issue</p>

          <p>autotrain llm --train --project_name ''&lt;project_name&gt;'' \<br>--model
          TinyPixel/Llama-2-7B-bf16-sharded \<br>--data_path timdettmers/openassistant-guanaco
          \<br>--text_column text \<br>--use_int4 \<br>--use_peft \<br>--learning_rate
          2e-4 \<br>--train_batch_size 2 \<br>--num_train_epochs 3 \<br>--trainer
          sft \<br>--model_max_length 2048 \<br>--push_to_hub \<br>--repo_id &lt;repo_id&gt;/&lt;project_name&gt;
          \<br>--block_size 2048 &gt; training.log &amp;</p>

          <p>After i removed --use_int4 line and executed, the issue got resolved.
          I hope this helps. Please ensure to use ''\'' at the end of everyline incase
          of using this command to train.</p>

          <p>Thanks.</p>

          </blockquote>

          <p>Thanks! it works for me in this case in Jupyter lab:</p>

          <p>quantization_config = BitsAndBytesConfig(<br>    # load_in_4bit=True,<br>    bnb_4bit_compute_dtype=torch.float16,<br>    bnb_4bit_quant_type="nf4",<br>    bnb_4bit_use_double_quant=True,<br>)</p>

          '
        raw: "> I was trying this in Google Colab and keep getting this error as well.\
          \ I ensured accelerate and bitsandbytes are installed. still keeps getting\
          \ this issue. Then realized i was having a particular line in my train command\
          \ that was causing this issue\n> \n> autotrain llm --train --project_name\
          \ '\\<project_name\\>' \\\\\n> --model TinyPixel/Llama-2-7B-bf16-sharded\
          \ \\\\\n> --data_path timdettmers/openassistant-guanaco \\\\\n> --text_column\
          \ text \\\\\n> --use_int4 \\\\\n> --use_peft \\\\\n> --learning_rate 2e-4\
          \ \\\\\n> --train_batch_size 2 \\\\\n> --num_train_epochs 3 \\\\\n> --trainer\
          \ sft \\\\\n> --model_max_length 2048 \\\\\n> --push_to_hub \\\\\n> --repo_id\
          \ \\<repo_id\\>/\\<project_name\\> \\\\\n> --block_size 2048 > training.log\
          \ &\n> \n> After i removed --use_int4 line and executed, the issue got resolved.\
          \ I hope this helps. Please ensure to use '\\\\' at the end of everyline\
          \ incase of using this command to train.\n> \n> Thanks.\n\nThanks! it works\
          \ for me in this case in Jupyter lab:\n\nquantization_config = BitsAndBytesConfig(\n\
          \    # load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n \
          \   bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n\
          )\n\n"
        updatedAt: '2023-12-04T03:30:58.722Z'
      numEdits: 0
      reactions: []
    id: 656d47f2e9f2c52b98c79aa4
    type: comment
  author: JohnLaw
  content: "> I was trying this in Google Colab and keep getting this error as well.\
    \ I ensured accelerate and bitsandbytes are installed. still keeps getting this\
    \ issue. Then realized i was having a particular line in my train command that\
    \ was causing this issue\n> \n> autotrain llm --train --project_name '\\<project_name\\\
    >' \\\\\n> --model TinyPixel/Llama-2-7B-bf16-sharded \\\\\n> --data_path timdettmers/openassistant-guanaco\
    \ \\\\\n> --text_column text \\\\\n> --use_int4 \\\\\n> --use_peft \\\\\n> --learning_rate\
    \ 2e-4 \\\\\n> --train_batch_size 2 \\\\\n> --num_train_epochs 3 \\\\\n> --trainer\
    \ sft \\\\\n> --model_max_length 2048 \\\\\n> --push_to_hub \\\\\n> --repo_id\
    \ \\<repo_id\\>/\\<project_name\\> \\\\\n> --block_size 2048 > training.log &\n\
    > \n> After i removed --use_int4 line and executed, the issue got resolved. I\
    \ hope this helps. Please ensure to use '\\\\' at the end of everyline incase\
    \ of using this command to train.\n> \n> Thanks.\n\nThanks! it works for me in\
    \ this case in Jupyter lab:\n\nquantization_config = BitsAndBytesConfig(\n   \
    \ # load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"\
    nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\n"
  created_at: 2023-12-04 03:30:58+00:00
  edited: false
  hidden: false
  id: 656d47f2e9f2c52b98c79aa4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630497746315-612f63edd793cf70b1adf8a4.png?w=200&h=200&f=face
      fullname: serkanars
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serkanarslan
      type: user
    createdAt: '2023-12-10T17:35:48.000Z'
    data:
      edited: false
      editors:
      - serkanarslan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9266950488090515
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1630497746315-612f63edd793cf70b1adf8a4.png?w=200&h=200&f=face
          fullname: serkanars
          isHf: false
          isPro: false
          name: serkanarslan
          type: user
        html: '<p>Hello Everyone,</p>

          <p>I was getting the same error and thought the problem might be related
          to pytorch.</p>

          <p><a rel="nofollow" href="https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with">https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with</a>  the
          solution here worked for me.</p>

          <p>After checking the cuda with torch.zeros(1).cuda(), I received the error
          that the video card was not up to date, and after installing the updated
          video card driver, the problem disappeared.</p>

          '
        raw: 'Hello Everyone,


          I was getting the same error and thought the problem might be related to
          pytorch.


          https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with  the
          solution here worked for me.


          After checking the cuda with torch.zeros(1).cuda(), I received the error
          that the video card was not up to date, and after installing the updated
          video card driver, the problem disappeared.'
        updatedAt: '2023-12-10T17:35:48.630Z'
      numEdits: 0
      reactions: []
    id: 6575f6f4ec3bf96e4374b7ac
    type: comment
  author: serkanarslan
  content: 'Hello Everyone,


    I was getting the same error and thought the problem might be related to pytorch.


    https://stackoverflow.com/questions/60987997/why-torch-cuda-is-available-returns-false-even-after-installing-pytorch-with  the
    solution here worked for me.


    After checking the cuda with torch.zeros(1).cuda(), I received the error that
    the video card was not up to date, and after installing the updated video card
    driver, the problem disappeared.'
  created_at: 2023-12-10 17:35:48+00:00
  edited: false
  hidden: false
  id: 6575f6f4ec3bf96e4374b7ac
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8cb643ef105014f0024a223331d98c17.svg
      fullname: Marios Mark
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marios88
      type: user
    createdAt: '2023-12-21T21:51:30.000Z'
    data:
      edited: false
      editors:
      - Marios88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8737272620201111
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8cb643ef105014f0024a223331d98c17.svg
          fullname: Marios Mark
          isHf: false
          isPro: false
          name: Marios88
          type: user
        html: '<p>!pip install transformers==4.34.0 worked for me in Google Colab
          GPU</p>

          '
        raw: '!pip install transformers==4.34.0 worked for me in Google Colab GPU'
        updatedAt: '2023-12-21T21:51:30.052Z'
      numEdits: 0
      reactions: []
    id: 6584b362c8b8bd66f79cf5b1
    type: comment
  author: Marios88
  content: '!pip install transformers==4.34.0 worked for me in Google Colab GPU'
  created_at: 2023-12-21 21:51:30+00:00
  edited: false
  hidden: false
  id: 6584b362c8b8bd66f79cf5b1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/96721f848388fdf2e15afdfb15f9d63b.svg
      fullname: Sumit Kumar Tyagi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sumittyagi25
      type: user
    createdAt: '2023-12-29T22:26:58.000Z'
    data:
      edited: false
      editors:
      - sumittyagi25
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9449480175971985
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/96721f848388fdf2e15afdfb15f9d63b.svg
          fullname: Sumit Kumar Tyagi
          isHf: false
          isPro: false
          name: sumittyagi25
          type: user
        html: '<p>I am using Mac M2 chip.<br>transformers=4.30.0 worked for me, </p>

          '
        raw: "I am using Mac M2 chip.  \ntransformers=4.30.0 worked for me, "
        updatedAt: '2023-12-29T22:26:58.073Z'
      numEdits: 0
      reactions: []
    id: 658f47b25070805494208d41
    type: comment
  author: sumittyagi25
  content: "I am using Mac M2 chip.  \ntransformers=4.30.0 worked for me, "
  created_at: 2023-12-29 22:26:58+00:00
  edited: false
  hidden: false
  id: 658f47b25070805494208d41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d506a22b7245462b20560bddd95f5b3a.svg
      fullname: Bruno Dirkx
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: programmeurke
      type: user
    createdAt: '2024-01-03T20:24:25.000Z'
    data:
      edited: true
      editors:
      - programmeurke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7839261889457703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d506a22b7245462b20560bddd95f5b3a.svg
          fullname: Bruno Dirkx
          isHf: false
          isPro: false
          name: programmeurke
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;sumittyagi25&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/sumittyagi25\"\
          >@<span class=\"underline\">sumittyagi25</span></a></span>\n\n\t</span></span>\
          \ thanks for sharing ! Are you also using the M2 chip with PyTorch, or the\
          \ CPU? Do you use model.to('mps')? Or can you share an example script ?\
          \ Thanks</p>\n"
        raw: '@sumittyagi25 thanks for sharing ! Are you also using the M2 chip with
          PyTorch, or the CPU? Do you use model.to(''mps'')? Or can you share an example
          script ? Thanks'
        updatedAt: '2024-01-03T20:24:39.053Z'
      numEdits: 1
      reactions: []
    id: 6595c2798d9156dbdce190fe
    type: comment
  author: programmeurke
  content: '@sumittyagi25 thanks for sharing ! Are you also using the M2 chip with
    PyTorch, or the CPU? Do you use model.to(''mps'')? Or can you share an example
    script ? Thanks'
  created_at: 2024-01-03 20:24:25+00:00
  edited: true
  hidden: false
  id: 6595c2798d9156dbdce190fe
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'I keep getting this: ImportError: Using `load_in_8bit=True` requires Accelerate'
