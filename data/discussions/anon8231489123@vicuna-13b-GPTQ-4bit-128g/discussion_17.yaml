!!python/object:huggingface_hub.community.DiscussionWithDetails
author: PaintedDreams
conflicting_files: null
created_at: 2023-04-08 22:56:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67012a414f1c68bbd5b02b9602eda111.svg
      fullname: J G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaintedDreams
      type: user
    createdAt: '2023-04-08T23:56:50.000Z'
    data:
      edited: false
      editors:
      - PaintedDreams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67012a414f1c68bbd5b02b9602eda111.svg
          fullname: J G
          isHf: false
          isPro: false
          name: PaintedDreams
          type: user
        html: "<p>Not to sure why this is happening, launching from cpu (AMD Ryzen\
          \ 5 3600) (32gb ram) and i get an error;</p>\n<hr>\n<p>CUDA SETUP: Loading\
          \ binary C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.dll...<br>C:\\\
          Users\\Jacob\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          bitsandbytes\\cextension.py:31: UserWarning: The installed version of bitsandbytes\
          \ was compiled without GPU support. 8-bit optimizers and GPU quantization\
          \ are unavailable.<br>  warn(\"The installed version of bitsandbytes was\
          \ compiled without GPU support. \"<br>Loading vicuna-13b-GPTQ-4bit-128g...<br>Traceback\
          \ (most recent call last):<br>  File \"C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\\
          text-generation-webui\\server.py\", line 302, in <br>    shared.model, shared.tokenizer\
          \ = load_model(shared.model_name)<br>  File \"C:\\Users\\Jacob\\Desktop\\\
          oobabooga-windows\\text-generation-webui\\modules\\models.py\", line 100,\
          \ in load_model<br>    from modules.GPTQ_loader import load_quantized<br>\
          \  File \"C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\", line 14, in <br>    import llama_inference_offload<br>ModuleNotFoundError:\
          \ No module named 'llama_inference_offload'<br>Press any key to continue\
          \ . . .</p>\n<hr>\n<p>more info:</p>\n<hr>\n<p><span data-props=\"{&quot;user&quot;:&quot;echo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/echo\"\
          >@<span class=\"underline\">echo</span></a></span>\n\n\t</span></span> off</p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;echo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/echo\">@<span class=\"\
          underline\">echo</span></a></span>\n\n\t</span></span> Starting the web\
          \ UI...</p>\n<p>cd /D \"%~dp0\"</p>\n<p>set MAMBA_ROOT_PREFIX=%cd%\\installer_files\\\
          mamba<br>set INSTALL_ENV_DIR=%cd%\\installer_files\\env</p>\n<p>if not exist\
          \ \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (<br>  call \"%MAMBA_ROOT_PREFIX%\\\
          micromamba.exe\" shell hook &gt;nul 2&gt;&amp;1<br>)<br>call \"%MAMBA_ROOT_PREFIX%\\\
          condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba\
          \ hook not found. &amp;&amp; goto end )<br>cd text-generation-webui</p>\n\
          <p>call python server.py --auto-devices --chat --wbits 4 --groupsize 128\
          \ --cpu --cpu-memory 3500MiB --pre_layer 30</p>\n<p>:end<br>pause</p>\n\
          <hr>\n<p>Anyone know why this happens?</p>\n"
        raw: "Not to sure why this is happening, launching from cpu (AMD Ryzen 5 3600)\
          \ (32gb ram) and i get an error;\r\n\r\n-------------------------------------------------------------\r\
          \nCUDA SETUP: Loading binary C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.dll...\r\
          \nC:\\Users\\Jacob\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\bitsandbytes\\cextension.py:31: UserWarning: The installed\
          \ version of bitsandbytes was compiled without GPU support. 8-bit optimizers\
          \ and GPU quantization are unavailable.\r\n  warn(\"The installed version\
          \ of bitsandbytes was compiled without GPU support. \"\r\nLoading vicuna-13b-GPTQ-4bit-128g...\r\
          \nTraceback (most recent call last):\r\n  File \"C:\\Users\\Jacob\\Desktop\\\
          oobabooga-windows\\text-generation-webui\\server.py\", line 302, in <module>\r\
          \n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\n\
          \  File \"C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\text-generation-webui\\\
          modules\\models.py\", line 100, in load_model\r\n    from modules.GPTQ_loader\
          \ import load_quantized\r\n  File \"C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\\
          text-generation-webui\\modules\\GPTQ_loader.py\", line 14, in <module>\r\
          \n    import llama_inference_offload\r\nModuleNotFoundError: No module named\
          \ 'llama_inference_offload'\r\nPress any key to continue . . .\r\n-------------------------------------------------------------\r\
          \n\r\nmore info:\r\n\r\n-------------------------------------------------------------\r\
          \n@echo off\r\n\r\n@echo Starting the web UI...\r\n\r\ncd /D \"%~dp0\"\r\
          \n\r\nset MAMBA_ROOT_PREFIX=%cd%\\installer_files\\mamba\r\nset INSTALL_ENV_DIR=%cd%\\\
          installer_files\\env\r\n\r\nif not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" (\r\n  call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell\
          \ hook >nul 2>&1\r\n)\r\ncall \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\"\
          \ activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba hook not found. &&\
          \ goto end )\r\ncd text-generation-webui\r\n\r\ncall python server.py --auto-devices\
          \ --chat --wbits 4 --groupsize 128 --cpu --cpu-memory 3500MiB --pre_layer\
          \ 30\r\n\r\n:end\r\npause\r\n-------------------------------------------------------------\r\
          \n\r\n\r\nAnyone know why this happens?"
        updatedAt: '2023-04-08T23:56:50.276Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - WowThatsCool
    id: 6431ff42dec2a70d813526cc
    type: comment
  author: PaintedDreams
  content: "Not to sure why this is happening, launching from cpu (AMD Ryzen 5 3600)\
    \ (32gb ram) and i get an error;\r\n\r\n-------------------------------------------------------------\r\
    \nCUDA SETUP: Loading binary C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.dll...\r\nC:\\Users\\\
    Jacob\\Desktop\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    bitsandbytes\\cextension.py:31: UserWarning: The installed version of bitsandbytes\
    \ was compiled without GPU support. 8-bit optimizers and GPU quantization are\
    \ unavailable.\r\n  warn(\"The installed version of bitsandbytes was compiled\
    \ without GPU support. \"\r\nLoading vicuna-13b-GPTQ-4bit-128g...\r\nTraceback\
    \ (most recent call last):\r\n  File \"C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\\
    text-generation-webui\\server.py\", line 302, in <module>\r\n    shared.model,\
    \ shared.tokenizer = load_model(shared.model_name)\r\n  File \"C:\\Users\\Jacob\\\
    Desktop\\oobabooga-windows\\text-generation-webui\\modules\\models.py\", line\
    \ 100, in load_model\r\n    from modules.GPTQ_loader import load_quantized\r\n\
    \  File \"C:\\Users\\Jacob\\Desktop\\oobabooga-windows\\text-generation-webui\\\
    modules\\GPTQ_loader.py\", line 14, in <module>\r\n    import llama_inference_offload\r\
    \nModuleNotFoundError: No module named 'llama_inference_offload'\r\nPress any\
    \ key to continue . . .\r\n-------------------------------------------------------------\r\
    \n\r\nmore info:\r\n\r\n-------------------------------------------------------------\r\
    \n@echo off\r\n\r\n@echo Starting the web UI...\r\n\r\ncd /D \"%~dp0\"\r\n\r\n\
    set MAMBA_ROOT_PREFIX=%cd%\\installer_files\\mamba\r\nset INSTALL_ENV_DIR=%cd%\\\
    installer_files\\env\r\n\r\nif not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\"\
    \ (\r\n  call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell hook >nul 2>&1\r\n\
    )\r\ncall \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\"\
    \ || ( echo MicroMamba hook not found. && goto end )\r\ncd text-generation-webui\r\
    \n\r\ncall python server.py --auto-devices --chat --wbits 4 --groupsize 128 --cpu\
    \ --cpu-memory 3500MiB --pre_layer 30\r\n\r\n:end\r\npause\r\n-------------------------------------------------------------\r\
    \n\r\n\r\nAnyone know why this happens?"
  created_at: 2023-04-08 22:56:50+00:00
  edited: false
  hidden: false
  id: 6431ff42dec2a70d813526cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/373c2d764772b68217fd44644a9a5ca0.svg
      fullname: Mr Borec
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: WowThatsCool
      type: user
    createdAt: '2023-04-11T11:15:05.000Z'
    data:
      edited: false
      editors:
      - WowThatsCool
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/373c2d764772b68217fd44644a9a5ca0.svg
          fullname: Mr Borec
          isHf: false
          isPro: false
          name: WowThatsCool
          type: user
        html: '<p>I have the same problem, have you managed to fix it?</p>

          '
        raw: I have the same problem, have you managed to fix it?
        updatedAt: '2023-04-11T11:15:05.132Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PaintedDreams
    id: 64354139dd2653057643d09b
    type: comment
  author: WowThatsCool
  content: I have the same problem, have you managed to fix it?
  created_at: 2023-04-11 10:15:05+00:00
  edited: false
  hidden: false
  id: 64354139dd2653057643d09b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67012a414f1c68bbd5b02b9602eda111.svg
      fullname: J G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PaintedDreams
      type: user
    createdAt: '2023-04-12T15:53:51.000Z'
    data:
      edited: false
      editors:
      - PaintedDreams
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67012a414f1c68bbd5b02b9602eda111.svg
          fullname: J G
          isHf: false
          isPro: false
          name: PaintedDreams
          type: user
        html: '<blockquote>

          <p>I have the same problem, have you managed to fix it?</p>

          </blockquote>

          <p><a rel="nofollow" href="https://www.youtube.com/watch?v=d4dk_7FptXk&amp;list=LL&amp;index=5&amp;ab_channel=TroubleChute">https://www.youtube.com/watch?v=d4dk_7FptXk&amp;list=LL&amp;index=5&amp;ab_channel=TroubleChute</a></p>

          '
        raw: '> I have the same problem, have you managed to fix it?


          https://www.youtube.com/watch?v=d4dk_7FptXk&list=LL&index=5&ab_channel=TroubleChute'
        updatedAt: '2023-04-12T15:53:51.953Z'
      numEdits: 0
      reactions: []
    id: 6436d40f1ed4fe4c0e938043
    type: comment
  author: PaintedDreams
  content: '> I have the same problem, have you managed to fix it?


    https://www.youtube.com/watch?v=d4dk_7FptXk&list=LL&index=5&ab_channel=TroubleChute'
  created_at: 2023-04-12 14:53:51+00:00
  edited: false
  hidden: false
  id: 6436d40f1ed4fe4c0e938043
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: New to this . . .
