!!python/object:huggingface_hub.community.DiscussionWithDetails
author: TheFairyMan
conflicting_files: null
created_at: 2023-04-06 21:59:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-06T22:59:35.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: "<p>Been getting this error whenever I try to get anything from the\
          \ model, I'm having to run with GPU + CPU, since I have a 3070ti with only\
          \ 8GB of VRAM.</p>\n<p>these are the arguments I'm using in the start-webui.bat:<br><code>call\
          \ python server.py --auto-devices --cai-chat --gpu-memory 7 --wbits 4 --groupsize\
          \ 128</code><br>Other models works normally, of course without the \" --wbits\
          \ 4 --groupsize 128\" part, which I'm not sure what it does.<br>Also,w ebui's\
          \ newest update had some trouble with gradio that dev fixed by removing\
          \ the requirement for \"llama-cpp-python==0.1.23\", apart from that no other\
          \ errors were reported, jsut wanna know if the issue is something on my\
          \ side or something I should report on the repo of oobabooga.</p>\n<p>This\
          \ is the error I get whenever I receiving anything</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"D:\\AI-TEXT\\oobabooga-windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 64, in gentask\n    ret = self.mfunc(callback=_callback,\
          \ **self.kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\text-generation-webui\\\
          modules\\text_generation.py\", line 220, in generate_with_callback\n   \
          \ shared.model.generate(**kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 1485, in generate\n    return\
          \ self.sample(\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2524,\
          \ in sample\n    outputs = self(\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\n    output = old_forward(*args,\
          \ **kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 687, in forward\n    outputs = self.model(\n  File \"D:\\AI-TEXT\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 577, in forward\n    layer_outputs = decoder_layer(\n  File \"D:\\\
          AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\n\
          \    output = old_forward(*args, **kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 292, in forward\n    hidden_states, self_attn_weights,\
          \ present_key_value = self.self_attn(\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 165, in new_forward\n    output = old_forward(*args,\
          \ **kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 196, in forward\n    query_states = self.q_proj(hidden_states).view(bsz,\
          \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\n  File \"D:\\AI-TEXT\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\accelerate\\hooks.py\", line 160, in new_forward\n\
          \    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)\n\
          \  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 280, in pre_forward\n    set_module_tensor_to_device(module,\
          \ name, self.execution_device, value=self.weights_map[name])\n  File \"\
          D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\utils\\offload.py\", line 123, in __getitem__\n    return self.dataset[f\"\
          {self.prefix}{key}\"]\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\accelerate\\utils\\offload.py\", line 170, in __getitem__\n\
          \    weight_info = self.index[key]\nKeyError: 'model.layers.39.self_attn.q_proj.wf1'\n\
          </code></pre>\n"
        raw: "Been getting this error whenever I try to get anything from the model,\
          \ I'm having to run with GPU + CPU, since I have a 3070ti with only 8GB\
          \ of VRAM.\r\n\r\nthese are the arguments I'm using in the start-webui.bat:\r\
          \n```call python server.py --auto-devices --cai-chat --gpu-memory 7 --wbits\
          \ 4 --groupsize 128```\r\nOther models works normally, of course without\
          \ the \" --wbits 4 --groupsize 128\" part, which I'm not sure what it does.\r\
          \nAlso,w ebui's newest update had some trouble with gradio that dev fixed\
          \ by removing the requirement for \"llama-cpp-python==0.1.23\", apart from\
          \ that no other errors were reported, jsut wanna know if the issue is something\
          \ on my side or something I should report on the repo of oobabooga.\r\n\r\
          \nThis is the error I get whenever I receiving anything\r\n```\r\nTraceback\
          \ (most recent call last):\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          text-generation-webui\\modules\\callbacks.py\", line 64, in gentask\r\n\
          \    ret = self.mfunc(callback=_callback, **self.kwargs)\r\n  File \"D:\\\
          AI-TEXT\\oobabooga-windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 220, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
          \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return\
          \ func(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 1485,\
          \ in generate\r\n    return self.sample(\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 2524, in sample\r\n    outputs = self(\r\n  File \"D:\\AI-TEXT\\\
          oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\\
          modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
          \ **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\
          \n    output = old_forward(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 687, in forward\r\n    outputs = self.model(\r\
          \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 577, in forward\r\n    layer_outputs = decoder_layer(\r\
          \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py\", line 165,\
          \ in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
          \ \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 292, in forward\r\
          \n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\
          \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return\
          \ forward_call(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py\", line 165,\
          \ in new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File\
          \ \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 196, in forward\r\
          \n    query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads,\
          \ self.head_dim).transpose(1, 2)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\hooks.py\", line 160, in new_forward\r\n    args, kwargs = module._hf_hook.pre_forward(module,\
          \ *args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\accelerate\\hooks.py\", line 280, in pre_forward\r\
          \n    set_module_tensor_to_device(module, name, self.execution_device, value=self.weights_map[name])\r\
          \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          accelerate\\utils\\offload.py\", line 123, in __getitem__\r\n    return\
          \ self.dataset[f\"{self.prefix}{key}\"]\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\accelerate\\utils\\offload.py\"\
          , line 170, in __getitem__\r\n    weight_info = self.index[key]\r\nKeyError:\
          \ 'model.layers.39.self_attn.q_proj.wf1'\r\n```"
        updatedAt: '2023-04-06T22:59:35.174Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ZoeKatherine
    id: 642f4ed7c953ca48ccd1767f
    type: comment
  author: TheFairyMan
  content: "Been getting this error whenever I try to get anything from the model,\
    \ I'm having to run with GPU + CPU, since I have a 3070ti with only 8GB of VRAM.\r\
    \n\r\nthese are the arguments I'm using in the start-webui.bat:\r\n```call python\
    \ server.py --auto-devices --cai-chat --gpu-memory 7 --wbits 4 --groupsize 128```\r\
    \nOther models works normally, of course without the \" --wbits 4 --groupsize\
    \ 128\" part, which I'm not sure what it does.\r\nAlso,w ebui's newest update\
    \ had some trouble with gradio that dev fixed by removing the requirement for\
    \ \"llama-cpp-python==0.1.23\", apart from that no other errors were reported,\
    \ jsut wanna know if the issue is something on my side or something I should report\
    \ on the repo of oobabooga.\r\n\r\nThis is the error I get whenever I receiving\
    \ anything\r\n```\r\nTraceback (most recent call last):\r\n  File \"D:\\AI-TEXT\\\
    oobabooga-windows\\text-generation-webui\\modules\\callbacks.py\", line 64, in\
    \ gentask\r\n    ret = self.mfunc(callback=_callback, **self.kwargs)\r\n  File\
    \ \"D:\\AI-TEXT\\oobabooga-windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 220, in generate_with_callback\r\n    shared.model.generate(**kwargs)\r\
    \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\utils\\_contextlib.py\", line 115, in decorate_context\r\n    return func(*args,\
    \ **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\generation\\utils.py\", line 1485, in generate\r\
    \n    return self.sample(\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2524, in sample\r\
    \n    outputs = self(\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\
    \n    return forward_call(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\accelerate\\hooks.py\", line 165, in\
    \ new_forward\r\n    output = old_forward(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\models\\\
    llama\\modeling_llama.py\", line 687, in forward\r\n    outputs = self.model(\r\
    \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line 577,\
    \ in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"\
    D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\accelerate\\\
    hooks.py\", line 165, in new_forward\r\n    output = old_forward(*args, **kwargs)\r\
    \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 292, in forward\r\n   \
    \ hidden_states, self_attn_weights, present_key_value = self.self_attn(\r\n  File\
    \ \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    torch\\nn\\modules\\module.py\", line 1501, in _call_impl\r\n    return forward_call(*args,\
    \ **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\accelerate\\hooks.py\", line 165, in new_forward\r\n    output\
    \ = old_forward(*args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
    , line 196, in forward\r\n    query_states = self.q_proj(hidden_states).view(bsz,\
    \ q_len, self.num_heads, self.head_dim).transpose(1, 2)\r\n  File \"D:\\AI-TEXT\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
    \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    accelerate\\hooks.py\", line 160, in new_forward\r\n    args, kwargs = module._hf_hook.pre_forward(module,\
    \ *args, **kwargs)\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\accelerate\\hooks.py\", line 280, in pre_forward\r\n\
    \    set_module_tensor_to_device(module, name, self.execution_device, value=self.weights_map[name])\r\
    \n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    accelerate\\utils\\offload.py\", line 123, in __getitem__\r\n    return self.dataset[f\"\
    {self.prefix}{key}\"]\r\n  File \"D:\\AI-TEXT\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\accelerate\\utils\\offload.py\", line 170, in __getitem__\r\
    \n    weight_info = self.index[key]\r\nKeyError: 'model.layers.39.self_attn.q_proj.wf1'\r\
    \n```"
  created_at: 2023-04-06 21:59:35+00:00
  edited: false
  hidden: false
  id: 642f4ed7c953ca48ccd1767f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8955dab64b4364f5ae494f5ce6293666.svg
      fullname: Dev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Clinch
      type: user
    createdAt: '2023-04-07T10:46:11.000Z'
    data:
      edited: false
      editors:
      - Clinch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8955dab64b4364f5ae494f5ce6293666.svg
          fullname: Dev
          isHf: false
          isPro: false
          name: Clinch
          type: user
        html: '<p>I''m reproducing this. Haven''t found a fix so far.</p>

          '
        raw: I'm reproducing this. Haven't found a fix so far.
        updatedAt: '2023-04-07T10:46:11.303Z'
      numEdits: 0
      reactions: []
    id: 642ff4733728df73af54eab6
    type: comment
  author: Clinch
  content: I'm reproducing this. Haven't found a fix so far.
  created_at: 2023-04-07 09:46:11+00:00
  edited: false
  hidden: false
  id: 642ff4733728df73af54eab6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60780ae8643328616da7dc52622ffe7b.svg
      fullname: Zalabs Normal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: orasoldul
      type: user
    createdAt: '2023-04-07T21:06:15.000Z'
    data:
      edited: true
      editors:
      - orasoldul
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60780ae8643328616da7dc52622ffe7b.svg
          fullname: Zalabs Normal
          isHf: false
          isPro: false
          name: orasoldul
          type: user
        html: '<p>I had the same issue and was not able to fix it, however I removed
          the --gpu-memory 7 and added --pre_layer 35 to the  start-webui.bat and
          now it works.</p>

          '
        raw: I had the same issue and was not able to fix it, however I removed the
          --gpu-memory 7 and added --pre_layer 35 to the  start-webui.bat and now
          it works.
        updatedAt: '2023-04-07T21:06:45.298Z'
      numEdits: 1
      reactions: []
    id: 643085c7c4ec9b3d82a8572c
    type: comment
  author: orasoldul
  content: I had the same issue and was not able to fix it, however I removed the
    --gpu-memory 7 and added --pre_layer 35 to the  start-webui.bat and now it works.
  created_at: 2023-04-07 20:06:15+00:00
  edited: true
  hidden: false
  id: 643085c7c4ec9b3d82a8572c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-07T21:48:53.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<blockquote>

          <p>I had the same issue and was not able to fix it, however I removed the
          --gpu-memory 7 and added --pre_layer 35 to the  start-webui.bat and now
          it works.</p>

          </blockquote>

          <p>Worked for me as well, except that it became very very slow to start
          up and other problems occurred like the AI just not following my prompt
          at all, and another problem of CUDA runing out of memory sometimes as it
          seems the model is trying to go over my GPU''s memory limit, without the
          "--gpu-memory 7" included, was thinking that the --auto-devices would solve
          that, but ti''s not so good, whileit shared the GPU and CPU memory to load
          the model, it still tries to make the model go over the quota.</p>

          <p>What does the "--pre_layer 35" argument does btw?<br>also, since the
          model was just ignoring everything I said, spitting out random and made
          up information when questioned, my last test was a simple one:<br>"Write
          me a short story about a girl who studies ancients and goes in an adventure"<br>This
          was the response:</p>

          <p>"I am so sorry, but it looks like this story was written by a human.
          It appears that the human did not have enough time to complete their task,
          which is why the story is so brief. However, if you insist, I can try to
          help you out.</p>

          <p>Meanwhile, back at the ranch..."</p>

          <p>I''m guessing the --pre_layer 35 messed with something to do with the
          resposnes.</p>

          '
        raw: '> I had the same issue and was not able to fix it, however I removed
          the --gpu-memory 7 and added --pre_layer 35 to the  start-webui.bat and
          now it works.


          Worked for me as well, except that it became very very slow to start up
          and other problems occurred like the AI just not following my prompt at
          all, and another problem of CUDA runing out of memory sometimes as it seems
          the model is trying to go over my GPU''s memory limit, without the "--gpu-memory
          7" included, was thinking that the --auto-devices would solve that, but
          ti''s not so good, whileit shared the GPU and CPU memory to load the model,
          it still tries to make the model go over the quota.


          What does the "--pre_layer 35" argument does btw?

          also, since the model was just ignoring everything I said, spitting out
          random and made up information when questioned, my last test was a simple
          one:

          "Write me a short story about a girl who studies ancients and goes in an
          adventure"

          This was the response:


          "I am so sorry, but it looks like this story was written by a human. It
          appears that the human did not have enough time to complete their task,
          which is why the story is so brief. However, if you insist, I can try to
          help you out.


          Meanwhile, back at the ranch..."



          I''m guessing the --pre_layer 35 messed with something to do with the resposnes.'
        updatedAt: '2023-04-07T21:48:53.784Z'
      numEdits: 0
      reactions: []
    id: 64308fc5613546b502d19851
    type: comment
  author: TheFairyMan
  content: '> I had the same issue and was not able to fix it, however I removed the
    --gpu-memory 7 and added --pre_layer 35 to the  start-webui.bat and now it works.


    Worked for me as well, except that it became very very slow to start up and other
    problems occurred like the AI just not following my prompt at all, and another
    problem of CUDA runing out of memory sometimes as it seems the model is trying
    to go over my GPU''s memory limit, without the "--gpu-memory 7" included, was
    thinking that the --auto-devices would solve that, but ti''s not so good, whileit
    shared the GPU and CPU memory to load the model, it still tries to make the model
    go over the quota.


    What does the "--pre_layer 35" argument does btw?

    also, since the model was just ignoring everything I said, spitting out random
    and made up information when questioned, my last test was a simple one:

    "Write me a short story about a girl who studies ancients and goes in an adventure"

    This was the response:


    "I am so sorry, but it looks like this story was written by a human. It appears
    that the human did not have enough time to complete their task, which is why the
    story is so brief. However, if you insist, I can try to help you out.


    Meanwhile, back at the ranch..."



    I''m guessing the --pre_layer 35 messed with something to do with the resposnes.'
  created_at: 2023-04-07 20:48:53+00:00
  edited: false
  hidden: false
  id: 64308fc5613546b502d19851
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60780ae8643328616da7dc52622ffe7b.svg
      fullname: Zalabs Normal
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: orasoldul
      type: user
    createdAt: '2023-04-07T22:00:36.000Z'
    data:
      edited: false
      editors:
      - orasoldul
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60780ae8643328616da7dc52622ffe7b.svg
          fullname: Zalabs Normal
          isHf: false
          isPro: false
          name: orasoldul
          type: user
        html: '<p>Yes I have the same CUDA out of memory issues as well (RTX 2070
          8GB), with --pre_layer 35 it took about 2-3 questions to run out of memory,
          and it was around 1,6-1,8 token/s. (without --pre_layer its CUDA memory
          error from the start)</p>

          <p>Currently testing it with --pre_layer 25, so far so good, no out of memory
          and I did not experience any issues like random made up information and
          it stay on topic. It is kinda slow though, around 1 token/s. </p>

          <p>All I know about --pre_layer is that it defines the number of layers
          to allocate to the GPU.</p>

          '
        raw: "Yes I have the same CUDA out of memory issues as well (RTX 2070 8GB),\
          \ with --pre_layer 35 it took about 2-3 questions to run out of memory,\
          \ and it was around 1,6-1,8 token/s. (without --pre_layer its CUDA memory\
          \ error from the start)\n\nCurrently testing it with --pre_layer 25, so\
          \ far so good, no out of memory and I did not experience any issues like\
          \ random made up information and it stay on topic. It is kinda slow though,\
          \ around 1 token/s. \n\nAll I know about --pre_layer is that it defines\
          \ the number of layers to allocate to the GPU."
        updatedAt: '2023-04-07T22:00:36.661Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ZoeKatherine
    id: 643092841a8652d76a3eec2a
    type: comment
  author: orasoldul
  content: "Yes I have the same CUDA out of memory issues as well (RTX 2070 8GB),\
    \ with --pre_layer 35 it took about 2-3 questions to run out of memory, and it\
    \ was around 1,6-1,8 token/s. (without --pre_layer its CUDA memory error from\
    \ the start)\n\nCurrently testing it with --pre_layer 25, so far so good, no out\
    \ of memory and I did not experience any issues like random made up information\
    \ and it stay on topic. It is kinda slow though, around 1 token/s. \n\nAll I know\
    \ about --pre_layer is that it defines the number of layers to allocate to the\
    \ GPU."
  created_at: 2023-04-07 21:00:36+00:00
  edited: false
  hidden: false
  id: 643092841a8652d76a3eec2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-07T22:11:02.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<p>Thank you, I''ll give it a try with --pre_layer 25.<br>While on
          layer 35, it took as well 1 to 2 responses before running out of memory
          too, depended on the response lenght, must be for the long term memory it
          tries to laod everyhting into VRAM, would be good if it could load into
          the RAM itself.</p>

          <p>Also, I tried this one that was shared in another thread: <a href="https://huggingface.co/ShreyasBrill/Vicuna-13B">https://huggingface.co/ShreyasBrill/Vicuna-13B</a><br>that
          one seemingly the same, but was being very accurate and not talking to itself
          and ignoring me even while on layer 35</p>

          '
        raw: 'Thank you, I''ll give it a try with --pre_layer 25.

          While on layer 35, it took as well 1 to 2 responses before running out of
          memory too, depended on the response lenght, must be for the long term memory
          it tries to laod everyhting into VRAM, would be good if it could load into
          the RAM itself.


          Also, I tried this one that was shared in another thread: https://huggingface.co/ShreyasBrill/Vicuna-13B

          that one seemingly the same, but was being very accurate and not talking
          to itself and ignoring me even while on layer 35'
        updatedAt: '2023-04-07T22:11:02.691Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - ZoeKatherine
    id: 643094f6e81822f40dd7eec0
    type: comment
  author: TheFairyMan
  content: 'Thank you, I''ll give it a try with --pre_layer 25.

    While on layer 35, it took as well 1 to 2 responses before running out of memory
    too, depended on the response lenght, must be for the long term memory it tries
    to laod everyhting into VRAM, would be good if it could load into the RAM itself.


    Also, I tried this one that was shared in another thread: https://huggingface.co/ShreyasBrill/Vicuna-13B

    that one seemingly the same, but was being very accurate and not talking to itself
    and ignoring me even while on layer 35'
  created_at: 2023-04-07 21:11:02+00:00
  edited: false
  hidden: false
  id: 643094f6e81822f40dd7eec0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-07T23:15:22.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<p>Did a try and seems like the model in this one seems to attempt
          to turn into an adventure game a few times by using the "You do that and
          that" prefix and waiting for you to input actions, although it did stay
          more on point than before.<br>Another thing, it suddenly starts talking
          as the human and giving itself instructions once it finishes a response
          and for some reason doesn''t stops there, then from there own it''ll keep
          talking as both.</p>

          <p>But alas, super slow compared to the "pytorch_model.bin" version of others
          13B I tried before.<br>Wonder if there is way  to increase it''s performance.<br>indeed
          around 1.07 tokens/s<br><code>Output generated in 649.70 seconds (1.07 tokens/s,
          697 tokens, context 63)</code> when I asked for a short story, thankfully
          it delivered properly-ish.<br>I also see that in the information front it''s
          lacking quite a lot</p>

          '
        raw: 'Did a try and seems like the model in this one seems to attempt to turn
          into an adventure game a few times by using the "You do that and that" prefix
          and waiting for you to input actions, although it did stay more on point
          than before.

          Another thing, it suddenly starts talking as the human and giving itself
          instructions once it finishes a response and for some reason doesn''t stops
          there, then from there own it''ll keep talking as both.


          But alas, super slow compared to the "pytorch_model.bin" version of others
          13B I tried before.

          Wonder if there is way  to increase it''s performance.

          indeed around 1.07 tokens/s

          ```Output generated in 649.70 seconds (1.07 tokens/s, 697 tokens, context
          63)``` when I asked for a short story, thankfully it delivered properly-ish.

          I also see that in the information front it''s lacking quite a lot'
        updatedAt: '2023-04-07T23:15:22.010Z'
      numEdits: 0
      reactions: []
    id: 6430a40ad847a2f89cda4ae6
    type: comment
  author: TheFairyMan
  content: 'Did a try and seems like the model in this one seems to attempt to turn
    into an adventure game a few times by using the "You do that and that" prefix
    and waiting for you to input actions, although it did stay more on point than
    before.

    Another thing, it suddenly starts talking as the human and giving itself instructions
    once it finishes a response and for some reason doesn''t stops there, then from
    there own it''ll keep talking as both.


    But alas, super slow compared to the "pytorch_model.bin" version of others 13B
    I tried before.

    Wonder if there is way  to increase it''s performance.

    indeed around 1.07 tokens/s

    ```Output generated in 649.70 seconds (1.07 tokens/s, 697 tokens, context 63)```
    when I asked for a short story, thankfully it delivered properly-ish.

    I also see that in the information front it''s lacking quite a lot'
  created_at: 2023-04-07 22:15:22+00:00
  edited: false
  hidden: false
  id: 6430a40ad847a2f89cda4ae6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03d232422ce95b34142d51d2104d439d.svg
      fullname: Zoe Stumbaugh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZoeKatherine
      type: user
    createdAt: '2023-04-08T03:29:14.000Z'
    data:
      edited: false
      editors:
      - ZoeKatherine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03d232422ce95b34142d51d2104d439d.svg
          fullname: Zoe Stumbaugh
          isHf: false
          isPro: false
          name: ZoeKatherine
          type: user
        html: '<blockquote>

          <p>Thank you, I''ll give it a try with --pre_layer 25.<br>While on layer
          35, it took as well 1 to 2 responses before running out of memory too, depended
          on the response lenght, must be for the long term memory it tries to laod
          everyhting into VRAM, would be good if it could load into the RAM itself.</p>

          <p>Also, I tried this one that was shared in another thread: <a href="https://huggingface.co/ShreyasBrill/Vicuna-13B">https://huggingface.co/ShreyasBrill/Vicuna-13B</a><br>that
          one seemingly the same, but was being very accurate and not talking to itself
          and ignoring me even while on layer 35</p>

          </blockquote>

          <p>I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs
          of VRAM and was having issues with cuda running out of memory.. Am fairly
          new to running LLM''s, and figure am hitting a hardware limit. This is only
          the second model other than a 125m(not billion) parameter test run with
          Facebook Galactica... and have tested it "against" the OPT 6.7b model.</p>

          <p>Sometimes it''ll auto-prompt itself... posing itself a new prompt...
          I have "fixed" it by clicking "Stop generating at new line character?" in
          the parameters tab. It''s slow, less than 1 token a second, but.... impressive
          outputs! About as slow as running Facebook OPT 6.7b, but... much better
          results.</p>

          '
        raw: "> Thank you, I'll give it a try with --pre_layer 25.\n> While on layer\
          \ 35, it took as well 1 to 2 responses before running out of memory too,\
          \ depended on the response lenght, must be for the long term memory it tries\
          \ to laod everyhting into VRAM, would be good if it could load into the\
          \ RAM itself.\n> \n> Also, I tried this one that was shared in another thread:\
          \ https://huggingface.co/ShreyasBrill/Vicuna-13B\n> that one seemingly the\
          \ same, but was being very accurate and not talking to itself and ignoring\
          \ me even while on layer 35\n\nI am running it on --pre_layer 25 as well,\
          \ using a 2080 witth 8 gigs of VRAM and was having issues with cuda running\
          \ out of memory.. Am fairly new to running LLM's, and figure am hitting\
          \ a hardware limit. This is only the second model other than a 125m(not\
          \ billion) parameter test run with Facebook Galactica... and have tested\
          \ it \"against\" the OPT 6.7b model.\n\nSometimes it'll auto-prompt itself...\
          \ posing itself a new prompt... I have \"fixed\" it by clicking \"Stop generating\
          \ at new line character?\" in the parameters tab. It's slow, less than 1\
          \ token a second, but.... impressive outputs! About as slow as running Facebook\
          \ OPT 6.7b, but... much better results."
        updatedAt: '2023-04-08T03:29:14.047Z'
      numEdits: 0
      reactions: []
    id: 6430df8aba4e05dd8b71e301
    type: comment
  author: ZoeKatherine
  content: "> Thank you, I'll give it a try with --pre_layer 25.\n> While on layer\
    \ 35, it took as well 1 to 2 responses before running out of memory too, depended\
    \ on the response lenght, must be for the long term memory it tries to laod everyhting\
    \ into VRAM, would be good if it could load into the RAM itself.\n> \n> Also,\
    \ I tried this one that was shared in another thread: https://huggingface.co/ShreyasBrill/Vicuna-13B\n\
    > that one seemingly the same, but was being very accurate and not talking to\
    \ itself and ignoring me even while on layer 35\n\nI am running it on --pre_layer\
    \ 25 as well, using a 2080 witth 8 gigs of VRAM and was having issues with cuda\
    \ running out of memory.. Am fairly new to running LLM's, and figure am hitting\
    \ a hardware limit. This is only the second model other than a 125m(not billion)\
    \ parameter test run with Facebook Galactica... and have tested it \"against\"\
    \ the OPT 6.7b model.\n\nSometimes it'll auto-prompt itself... posing itself a\
    \ new prompt... I have \"fixed\" it by clicking \"Stop generating at new line\
    \ character?\" in the parameters tab. It's slow, less than 1 token a second, but....\
    \ impressive outputs! About as slow as running Facebook OPT 6.7b, but... much\
    \ better results."
  created_at: 2023-04-08 02:29:14+00:00
  edited: false
  hidden: false
  id: 6430df8aba4e05dd8b71e301
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-08T07:08:24.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<blockquote>

          <p>I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs
          of VRAM and was having issues with cuda running out of memory.. Am fairly
          new to running LLM''s, and figure am hitting a hardware limit. This is only
          the second model other than a 125m(not billion) parameter test run with
          Facebook Galactica... and have tested it "against" the OPT 6.7b model.</p>

          </blockquote>

          <p>Weird, perhaps you have something else using your VRAM? mine didn''t
          even reach 6GB of VRAM when suing at layer 25, but had spikes of up to 7.3GB
          when in use generating very very long responses, which lead me to think
          something else was using your VRAM, or did this only happen after a very
          very long conversation?</p>

          '
        raw: '> I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs
          of VRAM and was having issues with cuda running out of memory.. Am fairly
          new to running LLM''s, and figure am hitting a hardware limit. This is only
          the second model other than a 125m(not billion) parameter test run with
          Facebook Galactica... and have tested it "against" the OPT 6.7b model.


          Weird, perhaps you have something else using your VRAM? mine didn''t even
          reach 6GB of VRAM when suing at layer 25, but had spikes of up to 7.3GB
          when in use generating very very long responses, which lead me to think
          something else was using your VRAM, or did this only happen after a very
          very long conversation?'
        updatedAt: '2023-04-08T07:08:24.595Z'
      numEdits: 0
      reactions: []
    id: 643112e8f2355217ea42a533
    type: comment
  author: TheFairyMan
  content: '> I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs
    of VRAM and was having issues with cuda running out of memory.. Am fairly new
    to running LLM''s, and figure am hitting a hardware limit. This is only the second
    model other than a 125m(not billion) parameter test run with Facebook Galactica...
    and have tested it "against" the OPT 6.7b model.


    Weird, perhaps you have something else using your VRAM? mine didn''t even reach
    6GB of VRAM when suing at layer 25, but had spikes of up to 7.3GB when in use
    generating very very long responses, which lead me to think something else was
    using your VRAM, or did this only happen after a very very long conversation?'
  created_at: 2023-04-08 06:08:24+00:00
  edited: false
  hidden: false
  id: 643112e8f2355217ea42a533
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/03d232422ce95b34142d51d2104d439d.svg
      fullname: Zoe Stumbaugh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ZoeKatherine
      type: user
    createdAt: '2023-04-08T08:09:11.000Z'
    data:
      edited: false
      editors:
      - ZoeKatherine
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/03d232422ce95b34142d51d2104d439d.svg
          fullname: Zoe Stumbaugh
          isHf: false
          isPro: false
          name: ZoeKatherine
          type: user
        html: '<blockquote>

          <blockquote>

          <p>I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs
          of VRAM and was having issues with cuda running out of memory.. Am fairly
          new to running LLM''s, and figure am hitting a hardware limit. This is only
          the second model other than a 125m(not billion) parameter test run with
          Facebook Galactica... and have tested it "against" the OPT 6.7b model.</p>

          </blockquote>

          <p>Weird, perhaps you have something else using your VRAM? mine didn''t
          even reach 6GB of VRAM when suing at layer 25, but had spikes of up to 7.3GB
          when in use generating very very long responses, which lead me to think
          something else was using your VRAM, or did this only happen after a very
          very long conversation?</p>

          </blockquote>

          <p>I should clarify, I was having issues before this with CUDA running out
          of vram... with the --pre_layer 25 I''ve not had issues.. its just, slow.</p>

          '
        raw: "> > I am running it on --pre_layer 25 as well, using a 2080 witth 8\
          \ gigs of VRAM and was having issues with cuda running out of memory.. Am\
          \ fairly new to running LLM's, and figure am hitting a hardware limit. This\
          \ is only the second model other than a 125m(not billion) parameter test\
          \ run with Facebook Galactica... and have tested it \"against\" the OPT\
          \ 6.7b model.\n> \n> Weird, perhaps you have something else using your VRAM?\
          \ mine didn't even reach 6GB of VRAM when suing at layer 25, but had spikes\
          \ of up to 7.3GB when in use generating very very long responses, which\
          \ lead me to think something else was using your VRAM, or did this only\
          \ happen after a very very long conversation?\n\nI should clarify, I was\
          \ having issues before this with CUDA running out of vram... with the --pre_layer\
          \ 25 I've not had issues.. its just, slow."
        updatedAt: '2023-04-08T08:09:11.705Z'
      numEdits: 0
      reactions: []
    id: 643121276758730899d64f3a
    type: comment
  author: ZoeKatherine
  content: "> > I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs\
    \ of VRAM and was having issues with cuda running out of memory.. Am fairly new\
    \ to running LLM's, and figure am hitting a hardware limit. This is only the second\
    \ model other than a 125m(not billion) parameter test run with Facebook Galactica...\
    \ and have tested it \"against\" the OPT 6.7b model.\n> \n> Weird, perhaps you\
    \ have something else using your VRAM? mine didn't even reach 6GB of VRAM when\
    \ suing at layer 25, but had spikes of up to 7.3GB when in use generating very\
    \ very long responses, which lead me to think something else was using your VRAM,\
    \ or did this only happen after a very very long conversation?\n\nI should clarify,\
    \ I was having issues before this with CUDA running out of vram... with the --pre_layer\
    \ 25 I've not had issues.. its just, slow."
  created_at: 2023-04-08 07:09:11+00:00
  edited: false
  hidden: false
  id: 643121276758730899d64f3a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-08T23:38:37.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>I am running it on --pre_layer 25 as well, using a 2080 witth 8 gigs
          of VRAM and was having issues with cuda running out of memory.. Am fairly
          new to running LLM''s, and figure am hitting a hardware limit. This is only
          the second model other than a 125m(not billion) parameter test run with
          Facebook Galactica... and have tested it "against" the OPT 6.7b model.</p>

          </blockquote>

          <p>Weird, perhaps you have something else using your VRAM? mine didn''t
          even reach 6GB of VRAM when suing at layer 25, but had spikes of up to 7.3GB
          when in use generating very very long responses, which lead me to think
          something else was using your VRAM, or did this only happen after a very
          very long conversation?</p>

          </blockquote>

          <p>I should clarify, I was having issues before this with CUDA running out
          of vram... with the --pre_layer 25 I''ve not had issues.. its just, slow.</p>

          </blockquote>

          <p>I see, sorry for the confusion. Also, I''ll try using layer 30 as well
          see if that goes well or end sup giving CUDA out of memory, then try the
          in between too, to see if we can get just a little bit of a speed up on
          the resposnes</p>

          '
        raw: "> > > I am running it on --pre_layer 25 as well, using a 2080 witth\
          \ 8 gigs of VRAM and was having issues with cuda running out of memory..\
          \ Am fairly new to running LLM's, and figure am hitting a hardware limit.\
          \ This is only the second model other than a 125m(not billion) parameter\
          \ test run with Facebook Galactica... and have tested it \"against\" the\
          \ OPT 6.7b model.\n> > \n> > Weird, perhaps you have something else using\
          \ your VRAM? mine didn't even reach 6GB of VRAM when suing at layer 25,\
          \ but had spikes of up to 7.3GB when in use generating very very long responses,\
          \ which lead me to think something else was using your VRAM, or did this\
          \ only happen after a very very long conversation?\n> \n> I should clarify,\
          \ I was having issues before this with CUDA running out of vram... with\
          \ the --pre_layer 25 I've not had issues.. its just, slow.\n\nI see, sorry\
          \ for the confusion. Also, I'll try using layer 30 as well see if that goes\
          \ well or end sup giving CUDA out of memory, then try the in between too,\
          \ to see if we can get just a little bit of a speed up on the resposnes"
        updatedAt: '2023-04-08T23:38:37.424Z'
      numEdits: 0
      reactions: []
    id: 6431fafd46d9d4f9d89b50fb
    type: comment
  author: TheFairyMan
  content: "> > > I am running it on --pre_layer 25 as well, using a 2080 witth 8\
    \ gigs of VRAM and was having issues with cuda running out of memory.. Am fairly\
    \ new to running LLM's, and figure am hitting a hardware limit. This is only the\
    \ second model other than a 125m(not billion) parameter test run with Facebook\
    \ Galactica... and have tested it \"against\" the OPT 6.7b model.\n> > \n> > Weird,\
    \ perhaps you have something else using your VRAM? mine didn't even reach 6GB\
    \ of VRAM when suing at layer 25, but had spikes of up to 7.3GB when in use generating\
    \ very very long responses, which lead me to think something else was using your\
    \ VRAM, or did this only happen after a very very long conversation?\n> \n> I\
    \ should clarify, I was having issues before this with CUDA running out of vram...\
    \ with the --pre_layer 25 I've not had issues.. its just, slow.\n\nI see, sorry\
    \ for the confusion. Also, I'll try using layer 30 as well see if that goes well\
    \ or end sup giving CUDA out of memory, then try the in between too, to see if\
    \ we can get just a little bit of a speed up on the resposnes"
  created_at: 2023-04-08 22:38:37+00:00
  edited: false
  hidden: false
  id: 6431fafd46d9d4f9d89b50fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
      fullname: Roman Ivanov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: perelmanych
      type: user
    createdAt: '2023-04-09T13:00:13.000Z'
    data:
      edited: true
      editors:
      - perelmanych
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c4e5d7874636416225a39f41d635049.svg
          fullname: Roman Ivanov
          isHf: false
          isPro: false
          name: perelmanych
          type: user
        html: '<p>As I read somewhere --pre_layer N, sets how many layers will be
          loaded into VRAM, the rest goes to RAM. I am using gtx 1060 6Gb, so I had
          to set it to 22 and I have around 0.72 t/s)) The amount of VRAM needed also
          depends on the parameters you set in the upper tab. E.g. setting higher
          max_new_tokens will generate longer answers but requires more memory.</p>

          <p>Update: In my case it crashes after like 15-20 sentences, not outputs.
          It seems that I have to reduce N even further((.</p>

          '
        raw: 'As I read somewhere --pre_layer N, sets how many layers will be loaded
          into VRAM, the rest goes to RAM. I am using gtx 1060 6Gb, so I had to set
          it to 22 and I have around 0.72 t/s)) The amount of VRAM needed also depends
          on the parameters you set in the upper tab. E.g. setting higher max_new_tokens
          will generate longer answers but requires more memory.


          Update: In my case it crashes after like 15-20 sentences, not outputs. It
          seems that I have to reduce N even further((.'
        updatedAt: '2023-04-09T13:08:24.096Z'
      numEdits: 1
      reactions: []
    id: 6432b6dd05e626d3a33c4d5c
    type: comment
  author: perelmanych
  content: 'As I read somewhere --pre_layer N, sets how many layers will be loaded
    into VRAM, the rest goes to RAM. I am using gtx 1060 6Gb, so I had to set it to
    22 and I have around 0.72 t/s)) The amount of VRAM needed also depends on the
    parameters you set in the upper tab. E.g. setting higher max_new_tokens will generate
    longer answers but requires more memory.


    Update: In my case it crashes after like 15-20 sentences, not outputs. It seems
    that I have to reduce N even further((.'
  created_at: 2023-04-09 12:00:13+00:00
  edited: true
  hidden: false
  id: 6432b6dd05e626d3a33c4d5c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5ed88f74224c83b1833f78e2cbd1aa65.svg
      fullname: Michael Ahern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Mightiestmike
      type: user
    createdAt: '2023-04-10T05:32:29.000Z'
    data:
      edited: false
      editors:
      - Mightiestmike
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5ed88f74224c83b1833f78e2cbd1aa65.svg
          fullname: Michael Ahern
          isHf: false
          isPro: false
          name: Mightiestmike
          type: user
        html: '<p>I''m new to AI and basically do not have a significant background
          in programming at all. I was pretty good at Javascript and C++ about 15-20
          yrs ago in high school and first year university. Anyways I''ve been playing
          with Stable Diffusion for image generating, and I have a 3070 in one machine
          and a 3060ti about 6'' away... both are running stable diffusion fairly
          successfully.<br>I''ve been doing some research and wonder if I purchased
          some older graphics cards with lots of VRAM but no actual outputs and hooked
          them up together, I think the term is using SLI if I could improve the functionality
          of these AI programs... I think the answer for stable diffusion is no, but
          I''m curious about a chatBot ?<br>Does anyone with a better understanding
          of how these programs work have any thoughts?<br>I''m trying to avoid caving
          in and buying a 4090 for something I only kind of Play with.</p>

          '
        raw: "I'm new to AI and basically do not have a significant background in\
          \ programming at all. I was pretty good at Javascript and C++ about 15-20\
          \ yrs ago in high school and first year university. Anyways I've been playing\
          \ with Stable Diffusion for image generating, and I have a 3070 in one machine\
          \ and a 3060ti about 6' away... both are running stable diffusion fairly\
          \ successfully. \nI've been doing some research and wonder if I purchased\
          \ some older graphics cards with lots of VRAM but no actual outputs and\
          \ hooked them up together, I think the term is using SLI if I could improve\
          \ the functionality of these AI programs... I think the answer for stable\
          \ diffusion is no, but I'm curious about a chatBot ? \nDoes anyone with\
          \ a better understanding of how these programs work have any thoughts? \n\
          I'm trying to avoid caving in and buying a 4090 for something I only kind\
          \ of Play with."
        updatedAt: '2023-04-10T05:32:29.600Z'
      numEdits: 0
      reactions: []
    id: 64339f6dc8e2047bd9fda97e
    type: comment
  author: Mightiestmike
  content: "I'm new to AI and basically do not have a significant background in programming\
    \ at all. I was pretty good at Javascript and C++ about 15-20 yrs ago in high\
    \ school and first year university. Anyways I've been playing with Stable Diffusion\
    \ for image generating, and I have a 3070 in one machine and a 3060ti about 6'\
    \ away... both are running stable diffusion fairly successfully. \nI've been doing\
    \ some research and wonder if I purchased some older graphics cards with lots\
    \ of VRAM but no actual outputs and hooked them up together, I think the term\
    \ is using SLI if I could improve the functionality of these AI programs... I\
    \ think the answer for stable diffusion is no, but I'm curious about a chatBot\
    \ ? \nDoes anyone with a better understanding of how these programs work have\
    \ any thoughts? \nI'm trying to avoid caving in and buying a 4090 for something\
    \ I only kind of Play with."
  created_at: 2023-04-10 04:32:29+00:00
  edited: false
  hidden: false
  id: 64339f6dc8e2047bd9fda97e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
      fullname: The Fairy Man
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheFairyMan
      type: user
    createdAt: '2023-04-10T05:45:12.000Z'
    data:
      edited: false
      editors:
      - TheFairyMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d41b43a3a8452cc2603f9d786aeb6838.svg
          fullname: The Fairy Man
          isHf: false
          isPro: false
          name: TheFairyMan
          type: user
        html: '<blockquote>

          <p>I''m new to AI and basically do not have a significant background in
          programming at all. I was pretty good at Javascript and C++ about 15-20
          yrs ago in high school and first year university. Anyways I''ve been playing
          with Stable Diffusion for image generating, and I have a 3070 in one machine
          and a 3060ti about 6'' away... both are running stable diffusion fairly
          successfully.<br>I''ve been doing some research and wonder if I purchased
          some older graphics cards with lots of VRAM but no actual outputs and hooked
          them up together, I think the term is using SLI if I could improve the functionality
          of these AI programs... I think the answer for stable diffusion is no, but
          I''m curious about a chatBot ?<br>Does anyone with a better understanding
          of how these programs work have any thoughts?<br>I''m trying to avoid caving
          in and buying a 4090 for something I only kind of Play with.</p>

          </blockquote>

          <p>Just having VRAM with in an old card won''t e enough, as the card itself
          won''t be able to do anything with the information and it would need to
          be processed by the 1st one, most likely an error would occur or it would
          try processing ti with the old card''s CPU, which will make it slower.<br>But
          yes, you can use both, will just not be good with older ones, not sure if
          it would be worse than using the CPU RAM, depending on the model, it might
          be.<br>I also thought about getting a 4090, but it only has 24GB of VRAM
          for the price, albeit super strong, would be too much of an overkill for
          other stuff like gaming, another thought was buying a RTX Quadro, which
          is more specialized for AI and 3D rendering and doesn''t consumes much energy,
          although depending on the one you need it might be expensive as well, but
          should be faster than the gaming GPUs, then you do a SLi.<br>Also, do an
          Sli with your 3060, I imagine yours have 12GB of VRAM, with the 8GB from
          your 3070, things should go very smoothly if you throw the rest of the model
          into CPU RAM, although you may need a better PSU to maintain both on.</p>

          '
        raw: "> I'm new to AI and basically do not have a significant background in\
          \ programming at all. I was pretty good at Javascript and C++ about 15-20\
          \ yrs ago in high school and first year university. Anyways I've been playing\
          \ with Stable Diffusion for image generating, and I have a 3070 in one machine\
          \ and a 3060ti about 6' away... both are running stable diffusion fairly\
          \ successfully. \n> I've been doing some research and wonder if I purchased\
          \ some older graphics cards with lots of VRAM but no actual outputs and\
          \ hooked them up together, I think the term is using SLI if I could improve\
          \ the functionality of these AI programs... I think the answer for stable\
          \ diffusion is no, but I'm curious about a chatBot ? \n> Does anyone with\
          \ a better understanding of how these programs work have any thoughts? \n\
          > I'm trying to avoid caving in and buying a 4090 for something I only kind\
          \ of Play with.\n\nJust having VRAM with in an old card won't e enough,\
          \ as the card itself won't be able to do anything with the information and\
          \ it would need to be processed by the 1st one, most likely an error would\
          \ occur or it would try processing ti with the old card's CPU, which will\
          \ make it slower.\nBut yes, you can use both, will just not be good with\
          \ older ones, not sure if it would be worse than using the CPU RAM, depending\
          \ on the model, it might be.\nI also thought about getting a 4090, but it\
          \ only has 24GB of VRAM for the price, albeit super strong, would be too\
          \ much of an overkill for other stuff like gaming, another thought was buying\
          \ a RTX Quadro, which is more specialized for AI and 3D rendering and doesn't\
          \ consumes much energy, although depending on the one you need it might\
          \ be expensive as well, but should be faster than the gaming GPUs, then\
          \ you do a SLi.\nAlso, do an Sli with your 3060, I imagine yours have 12GB\
          \ of VRAM, with the 8GB from your 3070, things should go very smoothly if\
          \ you throw the rest of the model into CPU RAM, although you may need a\
          \ better PSU to maintain both on."
        updatedAt: '2023-04-10T05:45:12.137Z'
      numEdits: 0
      reactions: []
    id: 6433a268c8e2047bd9fdbc4a
    type: comment
  author: TheFairyMan
  content: "> I'm new to AI and basically do not have a significant background in\
    \ programming at all. I was pretty good at Javascript and C++ about 15-20 yrs\
    \ ago in high school and first year university. Anyways I've been playing with\
    \ Stable Diffusion for image generating, and I have a 3070 in one machine and\
    \ a 3060ti about 6' away... both are running stable diffusion fairly successfully.\
    \ \n> I've been doing some research and wonder if I purchased some older graphics\
    \ cards with lots of VRAM but no actual outputs and hooked them up together, I\
    \ think the term is using SLI if I could improve the functionality of these AI\
    \ programs... I think the answer for stable diffusion is no, but I'm curious about\
    \ a chatBot ? \n> Does anyone with a better understanding of how these programs\
    \ work have any thoughts? \n> I'm trying to avoid caving in and buying a 4090\
    \ for something I only kind of Play with.\n\nJust having VRAM with in an old card\
    \ won't e enough, as the card itself won't be able to do anything with the information\
    \ and it would need to be processed by the 1st one, most likely an error would\
    \ occur or it would try processing ti with the old card's CPU, which will make\
    \ it slower.\nBut yes, you can use both, will just not be good with older ones,\
    \ not sure if it would be worse than using the CPU RAM, depending on the model,\
    \ it might be.\nI also thought about getting a 4090, but it only has 24GB of VRAM\
    \ for the price, albeit super strong, would be too much of an overkill for other\
    \ stuff like gaming, another thought was buying a RTX Quadro, which is more specialized\
    \ for AI and 3D rendering and doesn't consumes much energy, although depending\
    \ on the one you need it might be expensive as well, but should be faster than\
    \ the gaming GPUs, then you do a SLi.\nAlso, do an Sli with your 3060, I imagine\
    \ yours have 12GB of VRAM, with the 8GB from your 3070, things should go very\
    \ smoothly if you throw the rest of the model into CPU RAM, although you may need\
    \ a better PSU to maintain both on."
  created_at: 2023-04-10 04:45:12+00:00
  edited: false
  hidden: false
  id: 6433a268c8e2047bd9fdbc4a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/515af7766bc2ecf4e0e8cbed24b0463a.svg
      fullname: GregDybula
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Obecny
      type: user
    createdAt: '2023-04-10T17:47:29.000Z'
    data:
      edited: false
      editors:
      - Obecny
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/515af7766bc2ecf4e0e8cbed24b0463a.svg
          fullname: GregDybula
          isHf: false
          isPro: false
          name: Obecny
          type: user
        html: '<p>Unfortunately same here, 2070 Super</p>

          '
        raw: Unfortunately same here, 2070 Super
        updatedAt: '2023-04-10T17:47:29.241Z'
      numEdits: 0
      reactions: []
    id: 64344bb1d12a239d72e52e7a
    type: comment
  author: Obecny
  content: Unfortunately same here, 2070 Super
  created_at: 2023-04-10 16:47:29+00:00
  edited: false
  hidden: false
  id: 64344bb1d12a239d72e52e7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b74fe82af7f7d4dc5b69c614393561af.svg
      fullname: Nzdigital
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NZ3digital
      type: user
    createdAt: '2023-04-29T21:33:45.000Z'
    data:
      edited: false
      editors:
      - NZ3digital
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b74fe82af7f7d4dc5b69c614393561af.svg
          fullname: Nzdigital
          isHf: false
          isPro: false
          name: NZ3digital
          type: user
        html: '<blockquote>

          <p>Sometimes it''ll auto-prompt itself... posing itself a new prompt...
          I have "fixed" it by clicking "Stop generating at new line character?" in
          the parameters tab. It''s slow, less than 1 token a second, but.... impressive
          outputs! About as slow as running Facebook OPT 6.7b, but... much better
          results.</p>

          </blockquote>

          <p>you can fix the self prompting by using instruct mode and setting the
          instruction template to vicuna-v0</p>

          '
        raw: '> Sometimes it''ll auto-prompt itself... posing itself a new prompt...
          I have "fixed" it by clicking "Stop generating at new line character?" in
          the parameters tab. It''s slow, less than 1 token a second, but.... impressive
          outputs! About as slow as running Facebook OPT 6.7b, but... much better
          results.


          you can fix the self prompting by using instruct mode and setting the instruction
          template to vicuna-v0'
        updatedAt: '2023-04-29T21:33:45.982Z'
      numEdits: 0
      reactions: []
    id: 644d8d390dc952d245a77ece
    type: comment
  author: NZ3digital
  content: '> Sometimes it''ll auto-prompt itself... posing itself a new prompt...
    I have "fixed" it by clicking "Stop generating at new line character?" in the
    parameters tab. It''s slow, less than 1 token a second, but.... impressive outputs!
    About as slow as running Facebook OPT 6.7b, but... much better results.


    you can fix the self prompting by using instruct mode and setting the instruction
    template to vicuna-v0'
  created_at: 2023-04-29 20:33:45+00:00
  edited: false
  hidden: false
  id: 644d8d390dc952d245a77ece
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b74fe82af7f7d4dc5b69c614393561af.svg
      fullname: Nzdigital
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NZ3digital
      type: user
    createdAt: '2023-04-29T21:34:14.000Z'
    data:
      edited: false
      editors:
      - NZ3digital
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b74fe82af7f7d4dc5b69c614393561af.svg
          fullname: Nzdigital
          isHf: false
          isPro: false
          name: NZ3digital
          type: user
        html: '<p>also in the new vicuna version this was fixed</p>

          '
        raw: also in the new vicuna version this was fixed
        updatedAt: '2023-04-29T21:34:14.163Z'
      numEdits: 0
      reactions: []
    id: 644d8d560dc952d245a780f8
    type: comment
  author: NZ3digital
  content: also in the new vicuna version this was fixed
  created_at: 2023-04-29 20:34:14+00:00
  edited: false
  hidden: false
  id: 644d8d560dc952d245a780f8
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'Error when using with web-ui "KeyError: ''model.layers.39.self_attn.q_proj.wf1''"'
