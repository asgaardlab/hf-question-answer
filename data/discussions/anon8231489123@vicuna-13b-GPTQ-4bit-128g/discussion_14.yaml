!!python/object:huggingface_hub.community.DiscussionWithDetails
author: VAVUSH
conflicting_files: null
created_at: 2023-04-08 11:59:32+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666269746058-noauth.png?w=200&h=200&f=face
      fullname: vavusgh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VAVUSH
      type: user
    createdAt: '2023-04-08T12:59:32.000Z'
    data:
      edited: false
      editors:
      - VAVUSH
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666269746058-noauth.png?w=200&h=200&f=face
          fullname: vavusgh
          isHf: false
          isPro: false
          name: VAVUSH
          type: user
        html: '<p>I''m getting this issue:<br>torch.cuda.OutOfMemoryError: CUDA out
          of memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.07
          GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch)
          If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>Output
          generated in 3.64 seconds (0.00 tokens/s, 0 tokens, context 43)</p>

          <p>I''m wandering if it has to do with the fact that I''m running it on
          a laptop with a RTX 3070 8GB that has also an additional GPU that gives
          the CUDA set up of 8.6.</p>

          <p>CUDA SETUP: CUDA runtime path found: C:\AI\installer_files\env\bin\cudart64_110.dll<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary C:\AI\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll...<br>Loading
          vicuna-13b-GPTQ-4bit-128g...</p>

          '
        raw: "I'm getting this issue:\r\ntorch.cuda.OutOfMemoryError: CUDA out of\
          \ memory. Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.07\
          \ GiB already allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch)\
          \ If reserved memory is >> allocated memory try setting max_split_size_mb\
          \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \nOutput generated in 3.64 seconds (0.00 tokens/s, 0 tokens, context 43)\r\
          \n\r\nI'm wandering if it has to do with the fact that I'm running it on\
          \ a laptop with a RTX 3070 8GB that has also an additional GPU that gives\
          \ the CUDA set up of 8.6.\r\n\r\nCUDA SETUP: CUDA runtime path found: C:\\\
          AI\\installer_files\\env\\bin\\cudart64_110.dll\r\nCUDA SETUP: Highest compute\
          \ capability among GPUs detected: 8.6\r\nCUDA SETUP: Detected CUDA version\
          \ 117\r\nCUDA SETUP: Loading binary C:\\AI\\installer_files\\env\\lib\\\
          site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\r\nLoading vicuna-13b-GPTQ-4bit-128g..."
        updatedAt: '2023-04-08T12:59:32.677Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - mikeyang01
    id: 64316534034ecbefddd312b9
    type: comment
  author: VAVUSH
  content: "I'm getting this issue:\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
    \ Tried to allocate 2.00 MiB (GPU 0; 8.00 GiB total capacity; 7.07 GiB already\
    \ allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved\
    \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\nOutput\
    \ generated in 3.64 seconds (0.00 tokens/s, 0 tokens, context 43)\r\n\r\nI'm wandering\
    \ if it has to do with the fact that I'm running it on a laptop with a RTX 3070\
    \ 8GB that has also an additional GPU that gives the CUDA set up of 8.6.\r\n\r\
    \nCUDA SETUP: CUDA runtime path found: C:\\AI\\installer_files\\env\\bin\\cudart64_110.dll\r\
    \nCUDA SETUP: Highest compute capability among GPUs detected: 8.6\r\nCUDA SETUP:\
    \ Detected CUDA version 117\r\nCUDA SETUP: Loading binary C:\\AI\\installer_files\\\
    env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\r\nLoading\
    \ vicuna-13b-GPTQ-4bit-128g..."
  created_at: 2023-04-08 11:59:32+00:00
  edited: false
  hidden: false
  id: 64316534034ecbefddd312b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/148da5aa39abc223315fc5bdb91be5f7.svg
      fullname: Hans
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Hanssep123
      type: user
    createdAt: '2023-04-08T16:26:33.000Z'
    data:
      edited: true
      editors:
      - Hanssep123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/148da5aa39abc223315fc5bdb91be5f7.svg
          fullname: Hans
          isHf: false
          isPro: false
          name: Hanssep123
          type: user
        html: "<p>edit start-webui.bat and replace all the text with: </p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;echo&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/echo\">@<span class=\"underline\">echo</span></a></span>\n\
          \n\t</span></span> off</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;echo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/echo\"\
          >@<span class=\"underline\">echo</span></a></span>\n\n\t</span></span> Starting\
          \ the web UI...</p>\n<p>cd /D \"%~dp0\"</p>\n<p>set MAMBA_ROOT_PREFIX=%cd%\\\
          installer_files\\mamba<br>set INSTALL_ENV_DIR=%cd%\\installer_files\\env</p>\n\
          <p>if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (<br>\
          \  call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell hook &gt;nul 2&gt;&amp;1<br>)<br>call\
          \ \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\"\
          \ || ( echo MicroMamba hook not found. &amp;&amp; goto end )<br>cd text-generation-webui</p>\n\
          <p>call python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize\
          \ 128 --pre_layer 30</p>\n<p>:end<br>pause</p>\n"
        raw: "edit start-webui.bat and replace all the text with: \n\n@echo off\n\n\
          @echo Starting the web UI...\n\ncd /D \"%~dp0\"\n\nset MAMBA_ROOT_PREFIX=%cd%\\\
          installer_files\\mamba\nset INSTALL_ENV_DIR=%cd%\\installer_files\\env\n\
          \nif not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (\n  call\
          \ \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell hook >nul 2>&1\n)\ncall\
          \ \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\"\
          \ || ( echo MicroMamba hook not found. && goto end )\ncd text-generation-webui\n\
          \ncall python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize\
          \ 128 --pre_layer 30\n\n:end\npause"
        updatedAt: '2023-04-08T16:26:55.656Z'
      numEdits: 1
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - VAVUSH
        - vsewall
        - IC4ness
        - matt0177
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - IC4ness
        - dliedke
        - miguelgargallo
    id: 643195b917b2e5055bb1d66b
    type: comment
  author: Hanssep123
  content: "edit start-webui.bat and replace all the text with: \n\n@echo off\n\n\
    @echo Starting the web UI...\n\ncd /D \"%~dp0\"\n\nset MAMBA_ROOT_PREFIX=%cd%\\\
    installer_files\\mamba\nset INSTALL_ENV_DIR=%cd%\\installer_files\\env\n\nif not\
    \ exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (\n  call \"%MAMBA_ROOT_PREFIX%\\\
    micromamba.exe\" shell hook >nul 2>&1\n)\ncall \"%MAMBA_ROOT_PREFIX%\\condabin\\\
    micromamba.bat\" activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba hook not\
    \ found. && goto end )\ncd text-generation-webui\n\ncall python server.py --auto-devices\
    \ --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer 30\n\n:end\npause"
  created_at: 2023-04-08 15:26:33+00:00
  edited: true
  hidden: false
  id: 643195b917b2e5055bb1d66b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666269746058-noauth.png?w=200&h=200&f=face
      fullname: vavusgh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: VAVUSH
      type: user
    createdAt: '2023-04-08T16:50:57.000Z'
    data:
      edited: false
      editors:
      - VAVUSH
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666269746058-noauth.png?w=200&h=200&f=face
          fullname: vavusgh
          isHf: false
          isPro: false
          name: VAVUSH
          type: user
        html: '<p>It works now, fun fact: the pc makes a noise for every singe word
          of the response... it sound like the noise of a hard drive however here
          all solid state... curious...<br>Anyway it is pretty slow, from videos I
          was expecting it to be a bit faster.<br>Output generated in 25.03 seconds
          (0.96 tokens/s, 24 tokens, context 43)<br>Output generated in 46.60 seconds
          (1.12 tokens/s, 52 tokens, context 81)<br>Output generated in 36.89 seconds
          (1.11 tokens/s, 41 tokens, context 146)<br>Output generated in 92.08 seconds
          (1.11 tokens/s, 102 tokens, context 383)<br>Output generated in 13.66 seconds
          (0.95 tokens/s, 13 tokens, context 26)<br>Output generated in 8.67 seconds
          (0.92 tokens/s, 8 tokens, context 26)<br>Output generated in 32.35 seconds
          (1.08 tokens/s, 35 tokens, context 33)</p>

          '
        raw: 'It works now, fun fact: the pc makes a noise for every singe word of
          the response... it sound like the noise of a hard drive however here all
          solid state... curious...

          Anyway it is pretty slow, from videos I was expecting it to be a bit faster.

          Output generated in 25.03 seconds (0.96 tokens/s, 24 tokens, context 43)

          Output generated in 46.60 seconds (1.12 tokens/s, 52 tokens, context 81)

          Output generated in 36.89 seconds (1.11 tokens/s, 41 tokens, context 146)

          Output generated in 92.08 seconds (1.11 tokens/s, 102 tokens, context 383)

          Output generated in 13.66 seconds (0.95 tokens/s, 13 tokens, context 26)

          Output generated in 8.67 seconds (0.92 tokens/s, 8 tokens, context 26)

          Output generated in 32.35 seconds (1.08 tokens/s, 35 tokens, context 33)'
        updatedAt: '2023-04-08T16:50:57.919Z'
      numEdits: 0
      reactions: []
    id: 64319b719da016992511bf19
    type: comment
  author: VAVUSH
  content: 'It works now, fun fact: the pc makes a noise for every singe word of the
    response... it sound like the noise of a hard drive however here all solid state...
    curious...

    Anyway it is pretty slow, from videos I was expecting it to be a bit faster.

    Output generated in 25.03 seconds (0.96 tokens/s, 24 tokens, context 43)

    Output generated in 46.60 seconds (1.12 tokens/s, 52 tokens, context 81)

    Output generated in 36.89 seconds (1.11 tokens/s, 41 tokens, context 146)

    Output generated in 92.08 seconds (1.11 tokens/s, 102 tokens, context 383)

    Output generated in 13.66 seconds (0.95 tokens/s, 13 tokens, context 26)

    Output generated in 8.67 seconds (0.92 tokens/s, 8 tokens, context 26)

    Output generated in 32.35 seconds (1.08 tokens/s, 35 tokens, context 33)'
  created_at: 2023-04-08 15:50:57+00:00
  edited: false
  hidden: false
  id: 64319b719da016992511bf19
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/abc1eadeb094d8b6de15f377b474d766.svg
      fullname: Juqowel
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Juqowel
      type: user
    createdAt: '2023-04-08T20:51:08.000Z'
    data:
      edited: false
      editors:
      - Juqowel
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/abc1eadeb094d8b6de15f377b474d766.svg
          fullname: Juqowel
          isHf: false
          isPro: false
          name: Juqowel
          type: user
        html: '<blockquote>

          <p>I''m wandering if it has to do with the fact that I''m running it on
          a laptop with a RTX 3070 8GB that has also an additional GPU that gives
          the CUDA set up of 8.6.</p>

          </blockquote>

          <p>Just a low amount of vram for this model.</p>

          <blockquote>

          <p>It works now, fun fact: the pc makes a noise for every singe word of
          the response... it sound like the noise of a hard drive however here all
          solid state... curious...</p>

          </blockquote>

          <p>Same. It could be a coil whine.</p>

          <blockquote>

          <p>Anyway it is pretty slow, from videos I was expecting it to be a bit
          faster.</p>

          </blockquote>

          <p>Now you are sharing it between vram and ram with "--pre_layer 30". It''s
          slow.</p>

          '
        raw: '> I''m wandering if it has to do with the fact that I''m running it
          on a laptop with a RTX 3070 8GB that has also an additional GPU that gives
          the CUDA set up of 8.6.


          Just a low amount of vram for this model.


          > It works now, fun fact: the pc makes a noise for every singe word of the
          response... it sound like the noise of a hard drive however here all solid
          state... curious...


          Same. It could be a coil whine.


          > Anyway it is pretty slow, from videos I was expecting it to be a bit faster.


          Now you are sharing it between vram and ram with "--pre_layer 30". It''s
          slow.'
        updatedAt: '2023-04-08T20:51:08.621Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - VAVUSH
    id: 6431d3bcb62bdfa25c0d182c
    type: comment
  author: Juqowel
  content: '> I''m wandering if it has to do with the fact that I''m running it on
    a laptop with a RTX 3070 8GB that has also an additional GPU that gives the CUDA
    set up of 8.6.


    Just a low amount of vram for this model.


    > It works now, fun fact: the pc makes a noise for every singe word of the response...
    it sound like the noise of a hard drive however here all solid state... curious...


    Same. It could be a coil whine.


    > Anyway it is pretty slow, from videos I was expecting it to be a bit faster.


    Now you are sharing it between vram and ram with "--pre_layer 30". It''s slow.'
  created_at: 2023-04-08 19:51:08+00:00
  edited: false
  hidden: false
  id: 6431d3bcb62bdfa25c0d182c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/857faed5e49085b8d9416ace239d1154.svg
      fullname: IC
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: IC4ness
      type: user
    createdAt: '2023-04-09T14:48:51.000Z'
    data:
      edited: false
      editors:
      - IC4ness
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/857faed5e49085b8d9416ace239d1154.svg
          fullname: IC
          isHf: false
          isPro: false
          name: IC4ness
          type: user
        html: "<blockquote>\n<p>edit start-webui.bat and replace all the text with:\
          \ </p>\n<p><span data-props=\"{&quot;user&quot;:&quot;echo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/echo\">@<span class=\"\
          underline\">echo</span></a></span>\n\n\t</span></span> off</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;echo&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/echo\">@<span class=\"underline\">echo</span></a></span>\n\
          \n\t</span></span> Starting the web UI...</p>\n<p>cd /D \"%~dp0\"</p>\n\
          <p>set MAMBA_ROOT_PREFIX=%cd%\\installer_files\\mamba<br>set INSTALL_ENV_DIR=%cd%\\\
          installer_files\\env</p>\n<p>if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" (<br>  call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell\
          \ hook &gt;nul 2&gt;&amp;1<br>)<br>call \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba hook\
          \ not found. &amp;&amp; goto end )<br>cd text-generation-webui</p>\n<p>call\
          \ python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize\
          \ 128 --pre_layer 30</p>\n<p>:end<br>pause</p>\n</blockquote>\n<p>so far\
          \ so good, as it is working<br>But to summarize a long text - still getting\
          \ the<br>attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)<br>torch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 262.00 MiB (GPU 0; 8.00 GiB total\
          \ capacity; 6.74 GiB already allocated; 0 bytes free; 7.06 GiB reserved\
          \ in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try\
          \ setting max_split_size_mb to avoid fragmentation.  See documentation for\
          \ Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>\n"
        raw: "> edit start-webui.bat and replace all the text with: \n> \n> @echo\
          \ off\n> \n> @echo Starting the web UI...\n> \n> cd /D \"%~dp0\"\n> \n>\
          \ set MAMBA_ROOT_PREFIX=%cd%\\installer_files\\mamba\n> set INSTALL_ENV_DIR=%cd%\\\
          installer_files\\env\n> \n> if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" (\n>   call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell\
          \ hook >nul 2>&1\n> )\n> call \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\"\
          \ activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba hook not found. &&\
          \ goto end )\n> cd text-generation-webui\n> \n> call python server.py --auto-devices\
          \ --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer 30\n> \n> :end\n\
          > pause\n\nso far so good, as it is working\nBut to summarize a long text\
          \ - still getting the \nattn_weights = nn.functional.softmax(attn_weights,\
          \ dim=-1, dtype=torch.float32).to(query_states.dtype)\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 262.00 MiB (GPU 0; 8.00 GiB total\
          \ capacity; 6.74 GiB already allocated; 0 bytes free; 7.06 GiB reserved\
          \ in total by PyTorch) If reserved memory is >> allocated memory try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-04-09T14:48:51.824Z'
      numEdits: 0
      reactions: []
    id: 6432d05337d643c2690ebd9d
    type: comment
  author: IC4ness
  content: "> edit start-webui.bat and replace all the text with: \n> \n> @echo off\n\
    > \n> @echo Starting the web UI...\n> \n> cd /D \"%~dp0\"\n> \n> set MAMBA_ROOT_PREFIX=%cd%\\\
    installer_files\\mamba\n> set INSTALL_ENV_DIR=%cd%\\installer_files\\env\n> \n\
    > if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (\n>   call \"\
    %MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell hook >nul 2>&1\n> )\n> call \"%MAMBA_ROOT_PREFIX%\\\
    condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba\
    \ hook not found. && goto end )\n> cd text-generation-webui\n> \n> call python\
    \ server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer\
    \ 30\n> \n> :end\n> pause\n\nso far so good, as it is working\nBut to summarize\
    \ a long text - still getting the \nattn_weights = nn.functional.softmax(attn_weights,\
    \ dim=-1, dtype=torch.float32).to(query_states.dtype)\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 262.00 MiB (GPU 0; 8.00 GiB total capacity;\
    \ 6.74 GiB already allocated; 0 bytes free; 7.06 GiB reserved in total by PyTorch)\
    \ If reserved memory is >> allocated memory try setting max_split_size_mb to avoid\
    \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-04-09 13:48:51+00:00
  edited: false
  hidden: false
  id: 6432d05337d643c2690ebd9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
      fullname: jj maval
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luminecr
      type: user
    createdAt: '2023-04-09T22:20:42.000Z'
    data:
      edited: true
      editors:
      - luminecr
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/85157fe577ab784f05120ccca7f69482.svg
          fullname: jj maval
          isHf: false
          isPro: false
          name: luminecr
          type: user
        html: '<p>[DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes](RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.)</p>

          <p>I do every hack propose here and on my 3070 still dont working.</p>

          <p>I dont know what i can do for it works.</p>

          '
        raw: '[DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes](RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.)


          I do every hack propose here and on my 3070 still dont working.


          I dont know what i can do for it works.'
        updatedAt: '2023-04-09T22:22:04.706Z'
      numEdits: 2
      reactions: []
    id: 64333a3a3f2096b2c9466633
    type: comment
  author: luminecr
  content: '[DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
    bytes](RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
    data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
    bytes.)


    I do every hack propose here and on my 3070 still dont working.


    I dont know what i can do for it works.'
  created_at: 2023-04-09 21:20:42+00:00
  edited: true
  hidden: false
  id: 64333a3a3f2096b2c9466633
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3148cd27ef6f92c23599bae12d0af88b.svg
      fullname: Sean
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: planetfrog
      type: user
    createdAt: '2023-04-10T09:51:04.000Z'
    data:
      edited: false
      editors:
      - planetfrog
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3148cd27ef6f92c23599bae12d0af88b.svg
          fullname: Sean
          isHf: false
          isPro: false
          name: planetfrog
          type: user
        html: '<blockquote>

          <p>[DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes](RuntimeError: [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 141557760
          bytes.)</p>

          <p>I do every hack propose here and on my 3070 still dont working.</p>

          <p>I dont know what i can do for it works.</p>

          </blockquote>

          <p>Yep same problem here on a 3060 with 12gb vram</p>

          '
        raw: "> [DefaultCPUAllocator: not enough memory: you tried to allocate 141557760\
          \ bytes](RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\\
          work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not\
          \ enough memory: you tried to allocate 141557760 bytes.)\n> \n> I do every\
          \ hack propose here and on my 3070 still dont working.\n> \n> I dont know\
          \ what i can do for it works.\n\nYep same problem here on a 3060 with 12gb\
          \ vram"
        updatedAt: '2023-04-10T09:51:04.443Z'
      numEdits: 0
      reactions: []
    id: 6433dc085408e9c12af96fcd
    type: comment
  author: planetfrog
  content: "> [DefaultCPUAllocator: not enough memory: you tried to allocate 141557760\
    \ bytes](RuntimeError: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\\
    c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory:\
    \ you tried to allocate 141557760 bytes.)\n> \n> I do every hack propose here\
    \ and on my 3070 still dont working.\n> \n> I dont know what i can do for it works.\n\
    \nYep same problem here on a 3060 with 12gb vram"
  created_at: 2023-04-10 08:51:04+00:00
  edited: false
  hidden: false
  id: 6433dc085408e9c12af96fcd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
      fullname: Anton Korunchak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Miltank
      type: user
    createdAt: '2023-04-11T08:23:23.000Z'
    data:
      edited: false
      editors:
      - Miltank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
          fullname: Anton Korunchak
          isHf: false
          isPro: false
          name: Miltank
          type: user
        html: '<p>Works fine for me, but slow. Hmm. Thanks anyway!</p>

          '
        raw: Works fine for me, but slow. Hmm. Thanks anyway!
        updatedAt: '2023-04-11T08:23:23.817Z'
      numEdits: 0
      reactions: []
    id: 643518fba37a87b504ff7ffc
    type: comment
  author: Miltank
  content: Works fine for me, but slow. Hmm. Thanks anyway!
  created_at: 2023-04-11 07:23:23+00:00
  edited: false
  hidden: false
  id: 643518fba37a87b504ff7ffc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7278ac62f085ac60796b08e6a271ef20.svg
      fullname: Miguel Gargallo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miguelgargallo
      type: user
    createdAt: '2023-04-11T09:53:46.000Z'
    data:
      edited: false
      editors:
      - miguelgargallo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7278ac62f085ac60796b08e6a271ef20.svg
          fullname: Miguel Gargallo
          isHf: false
          isPro: false
          name: miguelgargallo
          type: user
        html: "<blockquote>\n<p>edit start-webui.bat and replace all the text with:\
          \ </p>\n<p><span data-props=\"{&quot;user&quot;:&quot;echo&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/echo\">@<span class=\"\
          underline\">echo</span></a></span>\n\n\t</span></span> off</p>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;echo&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/echo\">@<span class=\"underline\">echo</span></a></span>\n\
          \n\t</span></span> Starting the web UI...</p>\n<p>cd /D \"%~dp0\"</p>\n\
          <p>set MAMBA_ROOT_PREFIX=%cd%\\installer_files\\mamba<br>set INSTALL_ENV_DIR=%cd%\\\
          installer_files\\env</p>\n<p>if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" (<br>  call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell\
          \ hook &gt;nul 2&gt;&amp;1<br>)<br>call \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba hook\
          \ not found. &amp;&amp; goto end )<br>cd text-generation-webui</p>\n<p>call\
          \ python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize\
          \ 128 --pre_layer 30</p>\n<p>:end<br>pause</p>\n</blockquote>\n<p>Ilumunati\
          \ confirmed, thanks</p>\n"
        raw: "> edit start-webui.bat and replace all the text with: \n> \n> @echo\
          \ off\n> \n> @echo Starting the web UI...\n> \n> cd /D \"%~dp0\"\n> \n>\
          \ set MAMBA_ROOT_PREFIX=%cd%\\installer_files\\mamba\n> set INSTALL_ENV_DIR=%cd%\\\
          installer_files\\env\n> \n> if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\\
          micromamba.bat\" (\n>   call \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell\
          \ hook >nul 2>&1\n> )\n> call \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\"\
          \ activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba hook not found. &&\
          \ goto end )\n> cd text-generation-webui\n> \n> call python server.py --auto-devices\
          \ --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer 30\n> \n> :end\n\
          > pause\n\nIlumunati confirmed, thanks"
        updatedAt: '2023-04-11T09:53:46.142Z'
      numEdits: 0
      reactions: []
    id: 64352e2a905f7cfd6885e92e
    type: comment
  author: miguelgargallo
  content: "> edit start-webui.bat and replace all the text with: \n> \n> @echo off\n\
    > \n> @echo Starting the web UI...\n> \n> cd /D \"%~dp0\"\n> \n> set MAMBA_ROOT_PREFIX=%cd%\\\
    installer_files\\mamba\n> set INSTALL_ENV_DIR=%cd%\\installer_files\\env\n> \n\
    > if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (\n>   call \"\
    %MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell hook >nul 2>&1\n> )\n> call \"%MAMBA_ROOT_PREFIX%\\\
    condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\" || ( echo MicroMamba\
    \ hook not found. && goto end )\n> cd text-generation-webui\n> \n> call python\
    \ server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer\
    \ 30\n> \n> :end\n> pause\n\nIlumunati confirmed, thanks"
  created_at: 2023-04-11 08:53:46+00:00
  edited: false
  hidden: false
  id: 64352e2a905f7cfd6885e92e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f152893742efc579e42de9cb20d06338.svg
      fullname: euc dee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eucdee
      type: user
    createdAt: '2023-04-13T13:39:29.000Z'
    data:
      edited: true
      editors:
      - eucdee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f152893742efc579e42de9cb20d06338.svg
          fullname: euc dee
          isHf: false
          isPro: false
          name: eucdee
          type: user
        html: '<p>i still had lots of memory errors on my 3080 8gb laptop gpu with
          the --pre_layer 30 flag.<br>especially with large complex personas.<br>since
          setting prelayer to 20 i didnt have a single memory error but its getting
          real slow.</p>

          <p>Output generated in 97.41 seconds (0.55 tokens/s, 54 tokens, context
          856, seed 1479715562)<br>Output generated in 102.62 seconds (0.57 tokens/s,
          58 tokens, context 921, seed 1659762763)<br>Output generated in 161.13 seconds
          (0.58 tokens/s, 93 tokens, context 991, seed 479945416)</p>

          <p>--pre_layer 25 seems to be a good compromise.<br>Output generated in
          255.88 seconds (0.78 tokens/s, 199 tokens, context 276, seed 428896125)</p>

          '
        raw: 'i still had lots of memory errors on my 3080 8gb laptop gpu with the
          --pre_layer 30 flag.

          especially with large complex personas.

          since setting prelayer to 20 i didnt have a single memory error but its
          getting real slow.


          Output generated in 97.41 seconds (0.55 tokens/s, 54 tokens, context 856,
          seed 1479715562)

          Output generated in 102.62 seconds (0.57 tokens/s, 58 tokens, context 921,
          seed 1659762763)

          Output generated in 161.13 seconds (0.58 tokens/s, 93 tokens, context 991,
          seed 479945416)


          --pre_layer 25 seems to be a good compromise.

          Output generated in 255.88 seconds (0.78 tokens/s, 199 tokens, context 276,
          seed 428896125)'
        updatedAt: '2023-04-13T13:40:32.647Z'
      numEdits: 1
      reactions: []
    id: 643806117217743e7ca2ee67
    type: comment
  author: eucdee
  content: 'i still had lots of memory errors on my 3080 8gb laptop gpu with the --pre_layer
    30 flag.

    especially with large complex personas.

    since setting prelayer to 20 i didnt have a single memory error but its getting
    real slow.


    Output generated in 97.41 seconds (0.55 tokens/s, 54 tokens, context 856, seed
    1479715562)

    Output generated in 102.62 seconds (0.57 tokens/s, 58 tokens, context 921, seed
    1659762763)

    Output generated in 161.13 seconds (0.58 tokens/s, 93 tokens, context 991, seed
    479945416)


    --pre_layer 25 seems to be a good compromise.

    Output generated in 255.88 seconds (0.78 tokens/s, 199 tokens, context 276, seed
    428896125)'
  created_at: 2023-04-13 12:39:29+00:00
  edited: true
  hidden: false
  id: 643806117217743e7ca2ee67
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663125249645-noauth.png?w=200&h=200&f=face
      fullname: Michael Derryberry
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Marusame
      type: user
    createdAt: '2023-05-28T00:55:37.000Z'
    data:
      edited: false
      editors:
      - Marusame
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1663125249645-noauth.png?w=200&h=200&f=face
          fullname: Michael Derryberry
          isHf: false
          isPro: false
          name: Marusame
          type: user
        html: '<p>I managed to get rid of the error at least on my 8gb 2060 super
          by opening up webui.py in notepad++ and on line 13, adding in more parameters<br>CMD_FLAGS
          = ''--chat --model-menu  --auto-devices --threads 8 --wbits 4 --groupsize
          128 --pre_layer 30''<br>That was at least what im using for the vicuna model,
          probably where the 128 comes in for group size. </p>

          <p>I wasnt able to get the arguments to launch with the "start windows"
          bat file at all, its on the call pythose webui which "should" add those
          arguments but for some reason  errors out so I did it manually. Seems to
          work, its as slow as dialup internet though when I ask it a question but
          hey, it worked.</p>

          '
        raw: "I managed to get rid of the error at least on my 8gb 2060 super by opening\
          \ up webui.py in notepad++ and on line 13, adding in more parameters \n\
          CMD_FLAGS = '--chat --model-menu  --auto-devices --threads 8 --wbits 4 --groupsize\
          \ 128 --pre_layer 30'\nThat was at least what im using for the vicuna model,\
          \ probably where the 128 comes in for group size. \n\nI wasnt able to get\
          \ the arguments to launch with the \"start windows\" bat file at all, its\
          \ on the call pythose webui which \"should\" add those arguments but for\
          \ some reason  errors out so I did it manually. Seems to work, its as slow\
          \ as dialup internet though when I ask it a question but hey, it worked."
        updatedAt: '2023-05-28T00:55:37.673Z'
      numEdits: 0
      reactions: []
    id: 6472a6890211f852700f278f
    type: comment
  author: Marusame
  content: "I managed to get rid of the error at least on my 8gb 2060 super by opening\
    \ up webui.py in notepad++ and on line 13, adding in more parameters \nCMD_FLAGS\
    \ = '--chat --model-menu  --auto-devices --threads 8 --wbits 4 --groupsize 128\
    \ --pre_layer 30'\nThat was at least what im using for the vicuna model, probably\
    \ where the 128 comes in for group size. \n\nI wasnt able to get the arguments\
    \ to launch with the \"start windows\" bat file at all, its on the call pythose\
    \ webui which \"should\" add those arguments but for some reason  errors out so\
    \ I did it manually. Seems to work, its as slow as dialup internet though when\
    \ I ask it a question but hey, it worked."
  created_at: 2023-05-27 23:55:37+00:00
  edited: false
  hidden: false
  id: 6472a6890211f852700f278f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c89198ded3d23b8f68c8509ecba8b9cc.svg
      fullname: Jattoe Daltni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jattoedaltni
      type: user
    createdAt: '2023-06-07T12:28:09.000Z'
    data:
      edited: true
      editors:
      - jattoedaltni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8947381377220154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c89198ded3d23b8f68c8509ecba8b9cc.svg
          fullname: Jattoe Daltni
          isHf: false
          isPro: false
          name: jattoedaltni
          type: user
        html: '<p>Please handle with care, while we''re on the metaphor, this UPS
          box doesn''t have the kind of maps some of you guys have.<br>So I get this
          message after swapping out the start_windows.bat file</p>

          <p>Starting the web UI...<br>The system cannot find the path specified.<br>MicroMamba
          hook not found.<br>Press any key to continue . . .</p>

          <p>I''m assuming MicroMamba is a python, prerequisite? The kind of thing
          that I''ve been creating isolated python environments for, in certain directories
          in order to install local AI stuff (llms, tts, sd) without screwing up the
          python for everything else?</p>

          <p>8GB rtx 3070 @80-105w</p>

          <p>can I install this MicroMamba with conda? If so do I just go to the master
          directory of [oobabooga] as in one file up from start_windows.bat, then
          do the virtual environment there, or do I install MicroMamba?  somewhere
          else? will installing the mamba work as a base?</p>

          '
        raw: "Please handle with care, while we're on the metaphor, this UPS box doesn't\
          \ have the kind of maps some of you guys have. \nSo I get this message after\
          \ swapping out the start_windows.bat file\n\nStarting the web UI...\nThe\
          \ system cannot find the path specified.\nMicroMamba hook not found.\nPress\
          \ any key to continue . . .\n\nI'm assuming MicroMamba is a python, prerequisite?\
          \ The kind of thing that I've been creating isolated python environments\
          \ for, in certain directories in order to install local AI stuff (llms,\
          \ tts, sd) without screwing up the python for everything else?\n\n8GB rtx\
          \ 3070 @80-105w\n\ncan I install this MicroMamba with conda? If so do I\
          \ just go to the master directory of [oobabooga] as in one file up from\
          \ start_windows.bat, then do the virtual environment there, or do I install\
          \ MicroMamba?  somewhere else? will installing the mamba work as a base?"
        updatedAt: '2023-06-07T14:12:33.972Z'
      numEdits: 3
      reactions: []
    id: 648077d940facadc556c35a0
    type: comment
  author: jattoedaltni
  content: "Please handle with care, while we're on the metaphor, this UPS box doesn't\
    \ have the kind of maps some of you guys have. \nSo I get this message after swapping\
    \ out the start_windows.bat file\n\nStarting the web UI...\nThe system cannot\
    \ find the path specified.\nMicroMamba hook not found.\nPress any key to continue\
    \ . . .\n\nI'm assuming MicroMamba is a python, prerequisite? The kind of thing\
    \ that I've been creating isolated python environments for, in certain directories\
    \ in order to install local AI stuff (llms, tts, sd) without screwing up the python\
    \ for everything else?\n\n8GB rtx 3070 @80-105w\n\ncan I install this MicroMamba\
    \ with conda? If so do I just go to the master directory of [oobabooga] as in\
    \ one file up from start_windows.bat, then do the virtual environment there, or\
    \ do I install MicroMamba?  somewhere else? will installing the mamba work as\
    \ a base?"
  created_at: 2023-06-07 11:28:09+00:00
  edited: true
  hidden: false
  id: 648077d940facadc556c35a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c89198ded3d23b8f68c8509ecba8b9cc.svg
      fullname: Jattoe Daltni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jattoedaltni
      type: user
    createdAt: '2023-06-07T12:51:02.000Z'
    data:
      edited: false
      editors:
      - jattoedaltni
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40868088603019714
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c89198ded3d23b8f68c8509ecba8b9cc.svg
          fullname: Jattoe Daltni
          isHf: false
          isPro: false
          name: jattoedaltni
          type: user
        html: '<p>So I just read (I THINK) that it was <em>supposed</em> to give me
          that response,  (according to the actual info inside the windows_start.bat)
          and then I was supposed to open the cmd prompt and go into that particular
          directory</p>

          <p>cd text-generation-webui</p>

          <p>and then enter</p>

          <p>call python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize
          128 --pre_layer 30</p>

          <p>it gave me these error messages</p>

          <p>C:\Users\jattoedaltni\oobabooga_windows\oobabooga_windows\text-generation-webui&gt;call
          python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize
          128 --pre_layer 30<br>Traceback (most recent call last):<br>  File "C:\Users\jattoedaltni\oobabooga_windows\oobabooga_windows\text-generation-webui\server.py",
          line 47, in <br>    from modules import chat, shared, training, ui, utils<br>  File
          "C:\Users\jattoedaltni\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\chat.py",
          line 18, in <br>    from modules.text_generation import (generate_reply,
          get_encoded_length,<br>  File "C:\Users\jattoedaltni\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\text_generation.py",
          line 17, in <br>    from modules.models import clear_torch_cache, local_rank<br>  File
          "C:\Users\jattoedaltni\oobabooga_windows\oobabooga_windows\text-generation-webui\modules\models.py",
          line 10, in <br>    from transformers import (AutoConfig, AutoModel, AutoModelForCausalLM,</p>

          <p>ImportError: cannot import name ''BitsAndBytesConfig'' from ''transformers''
          (C:\Users\jattoedaltni\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers_<em>init</em>_.py)</p>

          <p>now when I double click window_start.bat it just gives me the same error
          message with a different shell</p>

          '
        raw: "So I just read (I THINK) that it was *supposed* to give me that response,\
          \  (according to the actual info inside the windows_start.bat) and then\
          \ I was supposed to open the cmd prompt and go into that particular directory\n\
          \ncd text-generation-webui\n\nand then enter\n\ncall python server.py --auto-devices\
          \ --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer 30\n\nit gave\
          \ me these error messages\n\nC:\\Users\\jattoedaltni\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui>call python server.py --auto-devices\
          \ --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer 30\nTraceback\
          \ (most recent call last):\n  File \"C:\\Users\\jattoedaltni\\oobabooga_windows\\\
          oobabooga_windows\\text-generation-webui\\server.py\", line 47, in <module>\n\
          \    from modules import chat, shared, training, ui, utils\n  File \"C:\\\
          Users\\jattoedaltni\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
          modules\\chat.py\", line 18, in <module>\n    from modules.text_generation\
          \ import (generate_reply, get_encoded_length,\n  File \"C:\\Users\\jattoedaltni\\\
          oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
          , line 17, in <module>\n    from modules.models import clear_torch_cache,\
          \ local_rank\n  File \"C:\\Users\\jattoedaltni\\oobabooga_windows\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 10, in <module>\n    from\
          \ transformers import (AutoConfig, AutoModel, AutoModelForCausalLM,\n\n\
          ImportError: cannot import name 'BitsAndBytesConfig' from 'transformers'\
          \ (C:\\Users\\jattoedaltni\\AppData\\Local\\Programs\\Python\\Python310\\\
          lib\\site-packages\\transformers\\__init__.py)\n\nnow when I double click\
          \ window_start.bat it just gives me the same error message with a different\
          \ shell"
        updatedAt: '2023-06-07T12:51:02.312Z'
      numEdits: 0
      reactions: []
    id: 64807d36e1421e205fd773bb
    type: comment
  author: jattoedaltni
  content: "So I just read (I THINK) that it was *supposed* to give me that response,\
    \  (according to the actual info inside the windows_start.bat) and then I was\
    \ supposed to open the cmd prompt and go into that particular directory\n\ncd\
    \ text-generation-webui\n\nand then enter\n\ncall python server.py --auto-devices\
    \ --chat --threads 8 --wbits 4 --groupsize 128 --pre_layer 30\n\nit gave me these\
    \ error messages\n\nC:\\Users\\jattoedaltni\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui>call python server.py --auto-devices --chat --threads 8\
    \ --wbits 4 --groupsize 128 --pre_layer 30\nTraceback (most recent call last):\n\
    \  File \"C:\\Users\\jattoedaltni\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    server.py\", line 47, in <module>\n    from modules import chat, shared, training,\
    \ ui, utils\n  File \"C:\\Users\\jattoedaltni\\oobabooga_windows\\oobabooga_windows\\\
    text-generation-webui\\modules\\chat.py\", line 18, in <module>\n    from modules.text_generation\
    \ import (generate_reply, get_encoded_length,\n  File \"C:\\Users\\jattoedaltni\\\
    oobabooga_windows\\oobabooga_windows\\text-generation-webui\\modules\\text_generation.py\"\
    , line 17, in <module>\n    from modules.models import clear_torch_cache, local_rank\n\
    \  File \"C:\\Users\\jattoedaltni\\oobabooga_windows\\oobabooga_windows\\text-generation-webui\\\
    modules\\models.py\", line 10, in <module>\n    from transformers import (AutoConfig,\
    \ AutoModel, AutoModelForCausalLM,\n\nImportError: cannot import name 'BitsAndBytesConfig'\
    \ from 'transformers' (C:\\Users\\jattoedaltni\\AppData\\Local\\Programs\\Python\\\
    Python310\\lib\\site-packages\\transformers\\__init__.py)\n\nnow when I double\
    \ click window_start.bat it just gives me the same error message with a different\
    \ shell"
  created_at: 2023-06-07 11:51:02+00:00
  edited: false
  hidden: false
  id: 64807d36e1421e205fd773bb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: 'torch.cuda.OutOfMemoryError: CUDA out of memory. with  a RTX3070 8gb'
