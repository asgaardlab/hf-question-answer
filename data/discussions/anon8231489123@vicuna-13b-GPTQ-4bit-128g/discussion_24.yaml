!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Miltank
conflicting_files: null
created_at: 2023-04-11 08:40:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
      fullname: Anton Korunchak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Miltank
      type: user
    createdAt: '2023-04-11T09:40:07.000Z'
    data:
      edited: false
      editors:
      - Miltank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
          fullname: Anton Korunchak
          isHf: false
          isPro: false
          name: Miltank
          type: user
        html: '<p>Hi! Was using new AI to test it out. I was starting it all the time
          by "start-webui-vicuna-gpu" bat, and it Worked, but has a problem with "CUDA
          out of memory" that I fixed somehow. Then I decided to start "Start-WebUI"
          bat, and it happened:</p>

          <p>Starting the web UI...<br>Warning: --cai-chat is deprecated. Use --chat
          instead.</p>

          <p>===================================BUG REPORT===================================<br>Welcome
          to bitsandbytes. For bug reports, please submit your error trace to: <a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>CUDA
          SETUP: CUDA runtime path found: C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\bin\cudart64_110.dll<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll...<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>Auto-assiging --gpu-memory
          7 for your GPU to try to prevent out-of-memory errors.<br>You can manually
          set other values.<br>Loading checkpoint shards:   0%|                                                                 |
          0/3 [00:00&lt;?, ?it/s]<br>Traceback (most recent call last):<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 442, in load_state_dict<br>    return torch.load(checkpoint_file, map_location="cpu")<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\serialization.py",
          line 791, in load<br>    with _open_file_like(f, ''rb'') as opened_file:<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\serialization.py",
          line 271, in _open_file_like<br>    return _open_file(name_or_buffer, mode)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\serialization.py",
          line 252, in <strong>init</strong><br>    super().<strong>init</strong>(open(name,
          mode))<br>FileNotFoundError: [Errno 2] No such file or directory: ''models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\pytorch_model-00001-of-00003.bin''</p>

          <p>During handling of the above exception, another exception occurred:</p>

          <p>Traceback (most recent call last):<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\text-generation-webui\server.py",
          line 347, in <br>    shared.model, shared.tokenizer = load_model(shared.model_name)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\text-generation-webui\modules\models.py",
          line 171, in load_model<br>    model = AutoModelForCausalLM.from_pretrained(checkpoint,
          **params)<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\auto\auto_factory.py",
          line 471, in from_pretrained<br>    return model_class.from_pretrained(<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 2736, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 3050, in _load_pretrained_model<br>    state_dict = load_state_dict(shard_file)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\modeling_utils.py",
          line 445, in load_state_dict<br>    with open(checkpoint_file) as f:<br>FileNotFoundError:
          [Errno 2] No such file or directory: ''models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\pytorch_model-00001-of-00003.bin''<br>To
          continue press any key...</p>

          '
        raw: "Hi! Was using new AI to test it out. I was starting it all the time\
          \ by \"start-webui-vicuna-gpu\" bat, and it Worked, but has a problem with\
          \ \"CUDA out of memory\" that I fixed somehow. Then I decided to start \"\
          Start-WebUI\" bat, and it happened:\r\n\r\nStarting the web UI...\r\nWarning:\
          \ --cai-chat is deprecated. Use --chat instead.\r\n\r\n===================================BUG\
          \ REPORT===================================\r\nWelcome to bitsandbytes.\
          \ For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\
          \n================================================================================\r\
          \nCUDA SETUP: CUDA runtime path found: C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\bin\\cudart64_110.dll\r\nCUDA SETUP: Highest compute\
          \ capability among GPUs detected: 8.6\r\nCUDA SETUP: Detected CUDA version\
          \ 117\r\nCUDA SETUP: Loading binary C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\r\
          \nLoading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\r\nAuto-assiging --gpu-memory\
          \ 7 for your GPU to try to prevent out-of-memory errors.\r\nYou can manually\
          \ set other values.\r\nLoading checkpoint shards:   0%|                \
          \                                                 | 0/3 [00:00<?, ?it/s]\r\
          \nTraceback (most recent call last):\r\n  File \"C:\\Users\\anton\\Desktop\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          modeling_utils.py\", line 442, in load_state_dict\r\n    return torch.load(checkpoint_file,\
          \ map_location=\"cpu\")\r\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\serialization.py\", line\
          \ 791, in load\r\n    with _open_file_like(f, 'rb') as opened_file:\r\n\
          \  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\serialization.py\", line 271, in _open_file_like\r\
          \n    return _open_file(name_or_buffer, mode)\r\n  File \"C:\\Users\\anton\\\
          Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          torch\\serialization.py\", line 252, in __init__\r\n    super().__init__(open(name,\
          \ mode))\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models\\\
          \\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\\pytorch_model-00001-of-00003.bin'\r\
          \n\r\nDuring handling of the above exception, another exception occurred:\r\
          \n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\anton\\\
          Desktop\\AI\\oobabooga-windows\\text-generation-webui\\server.py\", line\
          \ 347, in <module>\r\n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\
          \n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\text-generation-webui\\\
          modules\\models.py\", line 171, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(checkpoint,\
          \ **params)\r\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
          , line 471, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2736, in\
          \ from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"C:\\\
          Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\transformers\\modeling_utils.py\", line 3050, in _load_pretrained_model\r\
          \n    state_dict = load_state_dict(shard_file)\r\n  File \"C:\\Users\\anton\\\
          Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\modeling_utils.py\", line 445, in load_state_dict\r\n    with\
          \ open(checkpoint_file) as f:\r\nFileNotFoundError: [Errno 2] No such file\
          \ or directory: 'models\\\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\\\
          pytorch_model-00001-of-00003.bin'\r\nTo continue press any key...\r\n\r\n"
        updatedAt: '2023-04-11T09:40:07.105Z'
      numEdits: 0
      reactions: []
    id: 64352af7df32a2296637d1d8
    type: comment
  author: Miltank
  content: "Hi! Was using new AI to test it out. I was starting it all the time by\
    \ \"start-webui-vicuna-gpu\" bat, and it Worked, but has a problem with \"CUDA\
    \ out of memory\" that I fixed somehow. Then I decided to start \"Start-WebUI\"\
    \ bat, and it happened:\r\n\r\nStarting the web UI...\r\nWarning: --cai-chat is\
    \ deprecated. Use --chat instead.\r\n\r\n===================================BUG\
    \ REPORT===================================\r\nWelcome to bitsandbytes. For bug\
    \ reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\
    \n================================================================================\r\
    \nCUDA SETUP: CUDA runtime path found: C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
    installer_files\\env\\bin\\cudart64_110.dll\r\nCUDA SETUP: Highest compute capability\
    \ among GPUs detected: 8.6\r\nCUDA SETUP: Detected CUDA version 117\r\nCUDA SETUP:\
    \ Loading binary C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\r\nLoading\
    \ anon8231489123_vicuna-13b-GPTQ-4bit-128g...\r\nAuto-assiging --gpu-memory 7\
    \ for your GPU to try to prevent out-of-memory errors.\r\nYou can manually set\
    \ other values.\r\nLoading checkpoint shards:   0%|                          \
    \                                       | 0/3 [00:00<?, ?it/s]\r\nTraceback (most\
    \ recent call last):\r\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\transformers\\modeling_utils.py\", line\
    \ 442, in load_state_dict\r\n    return torch.load(checkpoint_file, map_location=\"\
    cpu\")\r\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\serialization.py\", line 791, in load\r\n    with\
    \ _open_file_like(f, 'rb') as opened_file:\r\n  File \"C:\\Users\\anton\\Desktop\\\
    AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\serialization.py\"\
    , line 271, in _open_file_like\r\n    return _open_file(name_or_buffer, mode)\r\
    \n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\serialization.py\", line 252, in __init__\r\n\
    \    super().__init__(open(name, mode))\r\nFileNotFoundError: [Errno 2] No such\
    \ file or directory: 'models\\\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\\pytorch_model-00001-of-00003.bin'\r\
    \n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\
    \nTraceback (most recent call last):\r\n  File \"C:\\Users\\anton\\Desktop\\AI\\\
    oobabooga-windows\\text-generation-webui\\server.py\", line 347, in <module>\r\
    \n    shared.model, shared.tokenizer = load_model(shared.model_name)\r\n  File\
    \ \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\text-generation-webui\\\
    modules\\models.py\", line 171, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(checkpoint,\
    \ **params)\r\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 471,\
    \ in from_pretrained\r\n    return model_class.from_pretrained(\r\n  File \"C:\\\
    Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 2736, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
    \n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\modeling_utils.py\", line 3050, in _load_pretrained_model\r\
    \n    state_dict = load_state_dict(shard_file)\r\n  File \"C:\\Users\\anton\\\
    Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    modeling_utils.py\", line 445, in load_state_dict\r\n    with open(checkpoint_file)\
    \ as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: 'models\\\\\
    anon8231489123_vicuna-13b-GPTQ-4bit-128g\\\\pytorch_model-00001-of-00003.bin'\r\
    \nTo continue press any key...\r\n\r\n"
  created_at: 2023-04-11 08:40:07+00:00
  edited: false
  hidden: false
  id: 64352af7df32a2296637d1d8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
      fullname: Anton Korunchak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Miltank
      type: user
    createdAt: '2023-04-11T09:40:46.000Z'
    data:
      edited: false
      editors:
      - Miltank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
          fullname: Anton Korunchak
          isHf: false
          isPro: false
          name: Miltank
          type: user
        html: '<p>Someone can help me to fix that? I was downloading custom model
          using "download-model" bat file.</p>

          '
        raw: Someone can help me to fix that? I was downloading custom model using
          "download-model" bat file.
        updatedAt: '2023-04-11T09:40:46.290Z'
      numEdits: 0
      reactions: []
    id: 64352b1e167c68b0291f9991
    type: comment
  author: Miltank
  content: Someone can help me to fix that? I was downloading custom model using "download-model"
    bat file.
  created_at: 2023-04-11 08:40:46+00:00
  edited: false
  hidden: false
  id: 64352b1e167c68b0291f9991
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/70af9095441efa961c270ed05cc51041.svg
      fullname: tzzek
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tzzek
      type: user
    createdAt: '2023-04-12T02:24:22.000Z'
    data:
      edited: true
      editors:
      - tzzek
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/70af9095441efa961c270ed05cc51041.svg
          fullname: tzzek
          isHf: false
          isPro: false
          name: tzzek
          type: user
        html: '<p>In your start-webui.bat try and change this line:</p>

          <p><code>call python server.py --auto-devices --cai-chat</code></p>

          <p>To:</p>

          <p><code>call python server.py --auto-devices --chat --wbits 4 --groupsize
          128</code></p>

          <p>--cai-chat is deprecated</p>

          '
        raw: 'In your start-webui.bat try and change this line:


          `call python server.py --auto-devices --cai-chat`


          To:


          `call python server.py --auto-devices --chat --wbits 4 --groupsize 128`


          --cai-chat is deprecated'
        updatedAt: '2023-04-12T02:24:28.656Z'
      numEdits: 1
      reactions: []
    id: 64361656569e1a4b0cc7a323
    type: comment
  author: tzzek
  content: 'In your start-webui.bat try and change this line:


    `call python server.py --auto-devices --cai-chat`


    To:


    `call python server.py --auto-devices --chat --wbits 4 --groupsize 128`


    --cai-chat is deprecated'
  created_at: 2023-04-12 01:24:22+00:00
  edited: true
  hidden: false
  id: 64361656569e1a4b0cc7a323
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
      fullname: Anton Korunchak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Miltank
      type: user
    createdAt: '2023-04-12T07:47:44.000Z'
    data:
      edited: false
      editors:
      - Miltank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
          fullname: Anton Korunchak
          isHf: false
          isPro: false
          name: Miltank
          type: user
        html: '<p>Hi! It started, yup. But now it has a problem with "CUDA out of
          memory", Like before. I remember that "Start-Webui" bat was saying "Auto-assiging
          --gpu-memory 7 for your GPU to try to prevent out-of-memory errors." before
          I changed this setting. What I should do now?</p>

          <p>===================================BUG REPORT===================================<br>Welcome
          to bitsandbytes. For bug reports, please submit your error trace to: <a
          rel="nofollow" href="https://github.com/TimDettmers/bitsandbytes/issues">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>CUDA
          SETUP: CUDA runtime path found: C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\bin\cudart64_110.dll<br>CUDA
          SETUP: Highest compute capability among GPUs detected: 8.6<br>CUDA SETUP:
          Detected CUDA version 117<br>CUDA SETUP: Loading binary C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\bitsandbytes\libbitsandbytes_cuda117.dll...<br>Loading
          anon8231489123_vicuna-13b-GPTQ-4bit-128g...<br>Found the following quantized
          model: models\anon8231489123_vicuna-13b-GPTQ-4bit-128g\vicuna-13b-4bit-128g.safetensors<br>Loading
          model ...<br>C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\safetensors\torch.py:99:
          UserWarning: TypedStorage is deprecated. It will be removed in the future
          and UntypedStorage will be the only storage class. This should only matter
          to you if you are using storages directly.  To access UntypedStorage directly,
          use tensor.untyped_storage() instead of tensor.storage()<br>  with safe_open(filename,
          framework="pt", device=device) as f:<br>C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch_utils.py:776:
          UserWarning: TypedStorage is deprecated. It will be removed in the future
          and UntypedStorage will be the only storage class. This should only matter
          to you if you are using storages directly.  To access UntypedStorage directly,
          use tensor.untyped_storage() instead of tensor.storage()<br>  return self.fget.<strong>get</strong>(instance,
          owner)()<br>C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\storage.py:899:
          UserWarning: TypedStorage is deprecated. It will be removed in the future
          and UntypedStorage will be the only storage class. This should only matter
          to you if you are using storages directly.  To access UntypedStorage directly,
          use tensor.untyped_storage() instead of tensor.storage()<br>  storage =
          cls(wrap_storage=untyped_storage)<br>Done.<br>Loaded the model in 11.14
          seconds.<br>Loading the extension "gallery"... Ok.<br>Running on local URL:  <a
          rel="nofollow" href="http://127.0.0.1:7860">http://127.0.0.1:7860</a></p>

          <p>To create a public link, set <code>share=True</code> in <code>launch()</code>.<br>Traceback
          (most recent call last):<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\text-generation-webui\modules\callbacks.py",
          line 66, in gentask<br>    ret = self.mfunc(callback=_callback, **self.kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\text-generation-webui\modules\text_generation.py",
          line 228, in generate_with_callback<br>    shared.model.generate(**kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\utils_contextlib.py",
          line 115, in decorate_context<br>    return func(*args, **kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 1485, in generate<br>    return self.sample(<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\generation\utils.py",
          line 2524, in sample<br>    outputs = self(<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 687, in forward<br>    outputs = self.model(<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 577, in forward<br>    layer_outputs = decoder_layer(<br>  File "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 289, in forward<br>    hidden_states = self.input_layernorm(hidden_states)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\torch\nn\modules\module.py",
          line 1501, in _call_impl<br>    return forward_call(*args, **kwargs)<br>  File
          "C:\Users\anton\Desktop\AI\oobabooga-windows\installer_files\env\lib\site-packages\transformers\models\llama\modeling_llama.py",
          line 84, in forward<br>    variance = hidden_states.to(torch.float32).pow(2).mean(-1,
          keepdim=True)<br>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried
          to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already
          allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved
          memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid
          fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF<br>Output
          generated in 0.89 seconds (0.00 tokens/s, 0 tokens, context 983, seed 1850395684)</p>

          '
        raw: "Hi! It started, yup. But now it has a problem with \"CUDA out of memory\"\
          , Like before. I remember that \"Start-Webui\" bat was saying \"Auto-assiging\
          \ --gpu-memory 7 for your GPU to try to prevent out-of-memory errors.\"\
          \ before I changed this setting. What I should do now?\n\n===================================BUG\
          \ REPORT===================================\nWelcome to bitsandbytes. For\
          \ bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
          ================================================================================\n\
          CUDA SETUP: CUDA runtime path found: C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\bin\\cudart64_110.dll\nCUDA SETUP: Highest compute\
          \ capability among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version\
          \ 117\nCUDA SETUP: Loading binary C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n\
          Loading anon8231489123_vicuna-13b-GPTQ-4bit-128g...\nFound the following\
          \ quantized model: models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors\n\
          Loading model ...\nC:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage\
          \ is deprecated. It will be removed in the future and UntypedStorage will\
          \ be the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  with safe_open(filename, framework=\"pt\"\
          , device=device) as f:\nC:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\_utils.py:776: UserWarning:\
          \ TypedStorage is deprecated. It will be removed in the future and UntypedStorage\
          \ will be the only storage class. This should only matter to you if you\
          \ are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n\
          C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\\
          lib\\site-packages\\torch\\storage.py:899: UserWarning: TypedStorage is\
          \ deprecated. It will be removed in the future and UntypedStorage will be\
          \ the only storage class. This should only matter to you if you are using\
          \ storages directly.  To access UntypedStorage directly, use tensor.untyped_storage()\
          \ instead of tensor.storage()\n  storage = cls(wrap_storage=untyped_storage)\n\
          Done.\nLoaded the model in 11.14 seconds.\nLoading the extension \"gallery\"\
          ... Ok.\nRunning on local URL:  http://127.0.0.1:7860\n\nTo create a public\
          \ link, set `share=True` in `launch()`.\nTraceback (most recent call last):\n\
          \  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\text-generation-webui\\\
          modules\\callbacks.py\", line 66, in gentask\n    ret = self.mfunc(callback=_callback,\
          \ **self.kwargs)\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          text-generation-webui\\modules\\text_generation.py\", line 228, in generate_with_callback\n\
          \    shared.model.generate(**kwargs)\n  File \"C:\\Users\\anton\\Desktop\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\\
          utils\\_contextlib.py\", line 115, in decorate_context\n    return func(*args,\
          \ **kwargs)\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\generation\\utils.py\"\
          , line 1485, in generate\n    return self.sample(\n  File \"C:\\Users\\\
          anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\generation\\utils.py\", line 2524, in sample\n    outputs\
          \ = self(\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in\
          \ _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\\
          anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
          transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\n\
          \    outputs = self.model(\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\"\
          , line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
          env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\"\
          , line 577, in forward\n    layer_outputs = decoder_layer(\n  File \"C:\\\
          Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\\
          site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
          \    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\anton\\Desktop\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
          models\\llama\\modeling_llama.py\", line 289, in forward\n    hidden_states\
          \ = self.input_layernorm(hidden_states)\n  File \"C:\\Users\\anton\\Desktop\\\
          AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\\
          nn\\modules\\module.py\", line 1501, in _call_impl\n    return forward_call(*args,\
          \ **kwargs)\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
          installer_files\\env\\lib\\site-packages\\transformers\\models\\llama\\\
          modeling_llama.py\", line 84, in forward\n    variance = hidden_states.to(torch.float32).pow(2).mean(-1,\
          \ keepdim=True)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried\
          \ to allocate 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already\
          \ allocated; 0 bytes free; 7.31 GiB reserved in total by PyTorch) If reserved\
          \ memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n\
          Output generated in 0.89 seconds (0.00 tokens/s, 0 tokens, context 983,\
          \ seed 1850395684)"
        updatedAt: '2023-04-12T07:47:44.661Z'
      numEdits: 0
      reactions: []
    id: 64366220aced93af8c29d40a
    type: comment
  author: Miltank
  content: "Hi! It started, yup. But now it has a problem with \"CUDA out of memory\"\
    , Like before. I remember that \"Start-Webui\" bat was saying \"Auto-assiging\
    \ --gpu-memory 7 for your GPU to try to prevent out-of-memory errors.\" before\
    \ I changed this setting. What I should do now?\n\n===================================BUG\
    \ REPORT===================================\nWelcome to bitsandbytes. For bug\
    \ reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n\
    ================================================================================\n\
    CUDA SETUP: CUDA runtime path found: C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
    installer_files\\env\\bin\\cudart64_110.dll\nCUDA SETUP: Highest compute capability\
    \ among GPUs detected: 8.6\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP:\
    \ Loading binary C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\nLoading\
    \ anon8231489123_vicuna-13b-GPTQ-4bit-128g...\nFound the following quantized model:\
    \ models\\anon8231489123_vicuna-13b-GPTQ-4bit-128g\\vicuna-13b-4bit-128g.safetensors\n\
    Loading model ...\nC:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\safetensors\\torch.py:99: UserWarning: TypedStorage is\
    \ deprecated. It will be removed in the future and UntypedStorage will be the\
    \ only storage class. This should only matter to you if you are using storages\
    \ directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead\
    \ of tensor.storage()\n  with safe_open(filename, framework=\"pt\", device=device)\
    \ as f:\nC:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\\
    lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated.\
    \ It will be removed in the future and UntypedStorage will be the only storage\
    \ class. This should only matter to you if you are using storages directly.  To\
    \ access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n\
    \  return self.fget.__get__(instance, owner)()\nC:\\Users\\anton\\Desktop\\AI\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\storage.py:899:\
    \ UserWarning: TypedStorage is deprecated. It will be removed in the future and\
    \ UntypedStorage will be the only storage class. This should only matter to you\
    \ if you are using storages directly.  To access UntypedStorage directly, use\
    \ tensor.untyped_storage() instead of tensor.storage()\n  storage = cls(wrap_storage=untyped_storage)\n\
    Done.\nLoaded the model in 11.14 seconds.\nLoading the extension \"gallery\"...\
    \ Ok.\nRunning on local URL:  http://127.0.0.1:7860\n\nTo create a public link,\
    \ set `share=True` in `launch()`.\nTraceback (most recent call last):\n  File\
    \ \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\text-generation-webui\\\
    modules\\callbacks.py\", line 66, in gentask\n    ret = self.mfunc(callback=_callback,\
    \ **self.kwargs)\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
    text-generation-webui\\modules\\text_generation.py\", line 228, in generate_with_callback\n\
    \    shared.model.generate(**kwargs)\n  File \"C:\\Users\\anton\\Desktop\\AI\\\
    oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py\"\
    , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"C:\\\
    Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\generation\\utils.py\", line 1485, in generate\n    return self.sample(\n\
    \  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2524, in sample\n\
    \    outputs = self(\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\\
    Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 687, in forward\n    outputs\
    \ = self.model(\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1501, in _call_impl\n\
    \    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\anton\\Desktop\\\
    AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\transformers\\\
    models\\llama\\modeling_llama.py\", line 577, in forward\n    layer_outputs =\
    \ decoder_layer(\n  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\\
    installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line\
    \ 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\\
    Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\\
    transformers\\models\\llama\\modeling_llama.py\", line 289, in forward\n    hidden_states\
    \ = self.input_layernorm(hidden_states)\n  File \"C:\\Users\\anton\\Desktop\\\
    AI\\oobabooga-windows\\installer_files\\env\\lib\\site-packages\\torch\\nn\\modules\\\
    module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n\
    \  File \"C:\\Users\\anton\\Desktop\\AI\\oobabooga-windows\\installer_files\\\
    env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py\", line\
    \ 84, in forward\n    variance = hidden_states.to(torch.float32).pow(2).mean(-1,\
    \ keepdim=True)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate\
    \ 20.00 MiB (GPU 0; 8.00 GiB total capacity; 7.08 GiB already allocated; 0 bytes\
    \ free; 7.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
    \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\nOutput generated in 0.89\
    \ seconds (0.00 tokens/s, 0 tokens, context 983, seed 1850395684)"
  created_at: 2023-04-12 06:47:44+00:00
  edited: false
  hidden: false
  id: 64366220aced93af8c29d40a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
      fullname: Anton Korunchak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Miltank
      type: user
    createdAt: '2023-04-18T10:50:09.000Z'
    data:
      edited: false
      editors:
      - Miltank
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671993606781-noauth.jpeg?w=200&h=200&f=face
          fullname: Anton Korunchak
          isHf: false
          isPro: false
          name: Miltank
          type: user
        html: '<p>Can someone help me with this? I really want to test this out on
          RTX3070TI, but It''s too slow/not working sometimes.</p>

          '
        raw: Can someone help me with this? I really want to test this out on RTX3070TI,
          but It's too slow/not working sometimes.
        updatedAt: '2023-04-18T10:50:09.942Z'
      numEdits: 0
      reactions: []
    id: 643e75e18fc03a0285478f6c
    type: comment
  author: Miltank
  content: Can someone help me with this? I really want to test this out on RTX3070TI,
    but It's too slow/not working sometimes.
  created_at: 2023-04-18 09:50:09+00:00
  edited: false
  hidden: false
  id: 643e75e18fc03a0285478f6c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 24
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: Issue with starting "Start-WebUI" bat.
