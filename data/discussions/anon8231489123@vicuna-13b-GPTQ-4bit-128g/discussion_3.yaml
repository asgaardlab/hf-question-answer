!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Juuuuu
conflicting_files: null
created_at: 2023-04-04 18:03:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2bac8561e83b024f9b624a2a5ec88e53.svg
      fullname: C
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Juuuuu
      type: user
    createdAt: '2023-04-04T19:03:11.000Z'
    data:
      edited: false
      editors:
      - Juuuuu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2bac8561e83b024f9b624a2a5ec88e53.svg
          fullname: C
          isHf: false
          isPro: false
          name: Juuuuu
          type: user
        html: '<p>Can you guys tell me the vram usage of this model. I a 3080ti laptop
          with 8gb.<br>Thanks</p>

          '
        raw: "Can you guys tell me the vram usage of this model. I a 3080ti laptop\
          \ with 8gb.\r\nThanks"
        updatedAt: '2023-04-04T19:03:11.943Z'
      numEdits: 0
      reactions: []
    id: 642c746f53e76b4c2284fd41
    type: comment
  author: Juuuuu
  content: "Can you guys tell me the vram usage of this model. I a 3080ti laptop with\
    \ 8gb.\r\nThanks"
  created_at: 2023-04-04 18:03:11+00:00
  edited: false
  hidden: false
  id: 642c746f53e76b4c2284fd41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-04T19:15:14.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: '<p>8-9 gb of vram is required</p>

          '
        raw: 8-9 gb of vram is required
        updatedAt: '2023-04-04T19:15:14.392Z'
      numEdits: 0
      reactions: []
    id: 642c77420d7975d9427362de
    type: comment
  author: TheYuriLover
  content: 8-9 gb of vram is required
  created_at: 2023-04-04 18:15:14+00:00
  edited: false
  hidden: false
  id: 642c77420d7975d9427362de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90c1c8d1c20c565fd2636c0071b3ae39.svg
      fullname: Gregory Hogan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghogan42
      type: user
    createdAt: '2023-04-04T23:31:54.000Z'
    data:
      edited: false
      editors:
      - ghogan42
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90c1c8d1c20c565fd2636c0071b3ae39.svg
          fullname: Gregory Hogan
          isHf: false
          isPro: false
          name: ghogan42
          type: user
        html: '<p>I see 8.7-8.9 used on my 16GB laptop 3080 with the model loaded
          in oogabooga. It goes up to 12.2 when it''s actually generating text.</p>

          '
        raw: I see 8.7-8.9 used on my 16GB laptop 3080 with the model loaded in oogabooga.
          It goes up to 12.2 when it's actually generating text.
        updatedAt: '2023-04-04T23:31:54.601Z'
      numEdits: 0
      reactions: []
    id: 642cb36aac7e48a03a0ba8ea
    type: comment
  author: ghogan42
  content: I see 8.7-8.9 used on my 16GB laptop 3080 with the model loaded in oogabooga.
    It goes up to 12.2 when it's actually generating text.
  created_at: 2023-04-04 22:31:54+00:00
  edited: false
  hidden: false
  id: 642cb36aac7e48a03a0ba8ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ecab51a4bf22f53f1dbfc1d5e99bba6c.svg
      fullname: M Veselovskiy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yuuru
      type: user
    createdAt: '2023-04-05T20:29:00.000Z'
    data:
      edited: false
      editors:
      - Yuuru
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ecab51a4bf22f53f1dbfc1d5e99bba6c.svg
          fullname: M Veselovskiy
          isHf: false
          isPro: false
          name: Yuuru
          type: user
        html: '<p>8gb cards load it only with 50% layers offload to CPU</p>

          '
        raw: 8gb cards load it only with 50% layers offload to CPU
        updatedAt: '2023-04-05T20:29:00.666Z'
      numEdits: 0
      reactions: []
    id: 642dda0c93f22bbdacfa07af
    type: comment
  author: Yuuru
  content: 8gb cards load it only with 50% layers offload to CPU
  created_at: 2023-04-05 19:29:00+00:00
  edited: false
  hidden: false
  id: 642dda0c93f22bbdacfa07af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/36def6103a7edf58f25c7d1b8e7ed295.svg
      fullname: TheGoldenSmith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheGoldenSmith
      type: user
    createdAt: '2023-04-07T08:06:25.000Z'
    data:
      edited: false
      editors:
      - TheGoldenSmith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/36def6103a7edf58f25c7d1b8e7ed295.svg
          fullname: TheGoldenSmith
          isHf: false
          isPro: false
          name: TheGoldenSmith
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yuuru&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yuuru\">@<span class=\"\
          underline\">Yuuru</span></a></span>\n\n\t</span></span> How can I try this?</p>\n"
        raw: '@Yuuru How can I try this?'
        updatedAt: '2023-04-07T08:06:25.033Z'
      numEdits: 0
      reactions: []
    id: 642fcf01a9818994007f1361
    type: comment
  author: TheGoldenSmith
  content: '@Yuuru How can I try this?'
  created_at: 2023-04-07 07:06:25+00:00
  edited: false
  hidden: false
  id: 642fcf01a9818994007f1361
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5c1a22d8388ea7c2d72dfb1c33f45766.svg
      fullname: Ben Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bash99
      type: user
    createdAt: '2023-04-07T09:34:13.000Z'
    data:
      edited: false
      editors:
      - bash99
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5c1a22d8388ea7c2d72dfb1c33f45766.svg
          fullname: Ben Li
          isHf: false
          isPro: false
          name: bash99
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ghogan42&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ghogan42\">@<span class=\"\
          underline\">ghogan42</span></a></span>\n\n\t</span></span>  12.2, I have\
          \ a desktop 3060 which only has 12GB, can I offload a few layers to CPU\
          \ so I can run it?</p>\n"
        raw: '@ghogan42  12.2, I have a desktop 3060 which only has 12GB, can I offload
          a few layers to CPU so I can run it?'
        updatedAt: '2023-04-07T09:34:13.848Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Chille9
    id: 642fe39543a53c86b3fb8f3e
    type: comment
  author: bash99
  content: '@ghogan42  12.2, I have a desktop 3060 which only has 12GB, can I offload
    a few layers to CPU so I can run it?'
  created_at: 2023-04-07 08:34:13+00:00
  edited: false
  hidden: false
  id: 642fe39543a53c86b3fb8f3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/547e4921b05c69b6c5bf0382a2d82fba.svg
      fullname: sad asd
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cyx123
      type: user
    createdAt: '2023-04-09T07:25:26.000Z'
    data:
      edited: false
      editors:
      - cyx123
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/547e4921b05c69b6c5bf0382a2d82fba.svg
          fullname: sad asd
          isHf: false
          isPro: false
          name: cyx123
          type: user
        html: '<p>curious  wonder  12g 3060 able to run this model or not</p>

          '
        raw: curious  wonder  12g 3060 able to run this model or not
        updatedAt: '2023-04-09T07:25:26.514Z'
      numEdits: 0
      reactions: []
    id: 64326866028e0ea13ace303d
    type: comment
  author: cyx123
  content: curious  wonder  12g 3060 able to run this model or not
  created_at: 2023-04-09 06:25:26+00:00
  edited: false
  hidden: false
  id: 64326866028e0ea13ace303d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
      fullname: Yuri
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TheYuriLover
      type: user
    createdAt: '2023-04-09T07:29:00.000Z'
    data:
      edited: false
      editors:
      - TheYuriLover
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/475262a2b834ae50ec39d453fd54a41d.svg
          fullname: Yuri
          isHf: false
          isPro: false
          name: TheYuriLover
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;cyx123&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/cyx123\">@<span class=\"\
          underline\">cyx123</span></a></span>\n\n\t</span></span> I have a 12g 3060\
          \ and it has no problem running every 13b models as they fluctuate between\
          \ 9 and 11 gb of vram usage.</p>\n"
        raw: '@cyx123 I have a 12g 3060 and it has no problem running every 13b models
          as they fluctuate between 9 and 11 gb of vram usage.'
        updatedAt: '2023-04-09T07:29:00.606Z'
      numEdits: 0
      reactions: []
    id: 6432693ccca1de06ec0e4728
    type: comment
  author: TheYuriLover
  content: '@cyx123 I have a 12g 3060 and it has no problem running every 13b models
    as they fluctuate between 9 and 11 gb of vram usage.'
  created_at: 2023-04-09 06:29:00+00:00
  edited: false
  hidden: false
  id: 6432693ccca1de06ec0e4728
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e86a57cfe616d8e0f7823fbcfd6898d4.svg
      fullname: Daniel Carvalho Liedke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dliedke
      type: user
    createdAt: '2023-04-09T20:37:01.000Z'
    data:
      edited: false
      editors:
      - dliedke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e86a57cfe616d8e0f7823fbcfd6898d4.svg
          fullname: Daniel Carvalho Liedke
          isHf: false
          isPro: false
          name: dliedke
          type: user
        html: '<p>I got RuntimeError: CUDA error: out of memory with NVIDIA 3070 8GB</p>

          '
        raw: 'I got RuntimeError: CUDA error: out of memory with NVIDIA 3070 8GB'
        updatedAt: '2023-04-09T20:37:01.060Z'
      numEdits: 0
      reactions: []
    id: 643321ed05e626d3a33f043e
    type: comment
  author: dliedke
  content: 'I got RuntimeError: CUDA error: out of memory with NVIDIA 3070 8GB'
  created_at: 2023-04-09 19:37:01+00:00
  edited: false
  hidden: false
  id: 643321ed05e626d3a33f043e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e86a57cfe616d8e0f7823fbcfd6898d4.svg
      fullname: Daniel Carvalho Liedke
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dliedke
      type: user
    createdAt: '2023-04-09T22:09:17.000Z'
    data:
      edited: false
      editors:
      - dliedke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e86a57cfe616d8e0f7823fbcfd6898d4.svg
          fullname: Daniel Carvalho Liedke
          isHf: false
          isPro: false
          name: dliedke
          type: user
        html: "<p>I could make it work with 8GB VRAM, slow:<br>Output generated in\
          \ 19.02 seconds (0.63 tokens/s, 12 tokens, context 132)<br>Output generated\
          \ in 47.36 seconds (1.03 tokens/s, 49 tokens, context 230)<br>Output generated\
          \ in 28.04 seconds (0.96 tokens/s, 27 tokens, context 363)</p>\n<p>from\
          \ <a href=\"https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/14\"\
          >https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/14</a><br>edit\
          \ start-webui.bat and replace all the text with:</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;echo&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/echo\">@<span class=\"underline\">echo</span></a></span>\n\
          \n\t</span></span> off</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;echo&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/echo\"\
          >@<span class=\"underline\">echo</span></a></span>\n\n\t</span></span> Starting\
          \ the web UI...</p>\n<p>cd /D \"%~dp0\"</p>\n<p>set MAMBA_ROOT_PREFIX=%cd%\\\
          installer_files\\mamba<br>set INSTALL_ENV_DIR=%cd%\\installer_files\\env</p>\n\
          <p>if not exist \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" (<br>call\
          \ \"%MAMBA_ROOT_PREFIX%\\micromamba.exe\" shell hook &gt;nul 2&gt;&amp;1<br>)<br>call\
          \ \"%MAMBA_ROOT_PREFIX%\\condabin\\micromamba.bat\" activate \"%INSTALL_ENV_DIR%\"\
          \ || ( echo MicroMamba hook not found. &amp;&amp; goto end )<br>cd text-generation-webui</p>\n\
          <p>call python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize\
          \ 128 --pre_layer 30</p>\n<p>:end<br>pause</p>\n"
        raw: 'I could make it work with 8GB VRAM, slow:

          Output generated in 19.02 seconds (0.63 tokens/s, 12 tokens, context 132)

          Output generated in 47.36 seconds (1.03 tokens/s, 49 tokens, context 230)

          Output generated in 28.04 seconds (0.96 tokens/s, 27 tokens, context 363)


          from https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/14

          edit start-webui.bat and replace all the text with:


          @echo off


          @echo Starting the web UI...


          cd /D "%~dp0"


          set MAMBA_ROOT_PREFIX=%cd%\installer_files\mamba

          set INSTALL_ENV_DIR=%cd%\installer_files\env


          if not exist "%MAMBA_ROOT_PREFIX%\condabin\micromamba.bat" (

          call "%MAMBA_ROOT_PREFIX%\micromamba.exe" shell hook >nul 2>&1

          )

          call "%MAMBA_ROOT_PREFIX%\condabin\micromamba.bat" activate "%INSTALL_ENV_DIR%"
          || ( echo MicroMamba hook not found. && goto end )

          cd text-generation-webui


          call python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize
          128 --pre_layer 30


          :end

          pause'
        updatedAt: '2023-04-09T22:09:17.814Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ai-rex
    id: 6433378d21fb4c09457ee461
    type: comment
  author: dliedke
  content: 'I could make it work with 8GB VRAM, slow:

    Output generated in 19.02 seconds (0.63 tokens/s, 12 tokens, context 132)

    Output generated in 47.36 seconds (1.03 tokens/s, 49 tokens, context 230)

    Output generated in 28.04 seconds (0.96 tokens/s, 27 tokens, context 363)


    from https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g/discussions/14

    edit start-webui.bat and replace all the text with:


    @echo off


    @echo Starting the web UI...


    cd /D "%~dp0"


    set MAMBA_ROOT_PREFIX=%cd%\installer_files\mamba

    set INSTALL_ENV_DIR=%cd%\installer_files\env


    if not exist "%MAMBA_ROOT_PREFIX%\condabin\micromamba.bat" (

    call "%MAMBA_ROOT_PREFIX%\micromamba.exe" shell hook >nul 2>&1

    )

    call "%MAMBA_ROOT_PREFIX%\condabin\micromamba.bat" activate "%INSTALL_ENV_DIR%"
    || ( echo MicroMamba hook not found. && goto end )

    cd text-generation-webui


    call python server.py --auto-devices --chat --threads 8 --wbits 4 --groupsize
    128 --pre_layer 30


    :end

    pause'
  created_at: 2023-04-09 21:09:17+00:00
  edited: false
  hidden: false
  id: 6433378d21fb4c09457ee461
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8c58a12038261de466ab6fd21f319977.svg
      fullname: sieg chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cxfcxf
      type: user
    createdAt: '2023-04-17T23:24:45.000Z'
    data:
      edited: false
      editors:
      - cxfcxf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8c58a12038261de466ab6fd21f319977.svg
          fullname: sieg chen
          isHf: false
          isPro: false
          name: cxfcxf
          type: user
        html: '<p>yeah i m also getting 1 token per second with splitting on 8GB VRAM,
          the performance is bad, i was able to achieve the same using ggml model
          + llama.cpp withDRAM</p>

          '
        raw: yeah i m also getting 1 token per second with splitting on 8GB VRAM,
          the performance is bad, i was able to achieve the same using ggml model
          + llama.cpp withDRAM
        updatedAt: '2023-04-17T23:24:45.605Z'
      numEdits: 0
      reactions: []
    id: 643dd53d8249c7c8d10da8cb
    type: comment
  author: cxfcxf
  content: yeah i m also getting 1 token per second with splitting on 8GB VRAM, the
    performance is bad, i was able to achieve the same using ggml model + llama.cpp
    withDRAM
  created_at: 2023-04-17 22:24:45+00:00
  edited: false
  hidden: false
  id: 643dd53d8249c7c8d10da8cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78019dbf6300b4fc1ba53e201a7453cd.svg
      fullname: Shai J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FixYourSelf
      type: user
    createdAt: '2023-04-19T04:58:57.000Z'
    data:
      edited: false
      editors:
      - FixYourSelf
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78019dbf6300b4fc1ba53e201a7453cd.svg
          fullname: Shai J
          isHf: false
          isPro: false
          name: FixYourSelf
          type: user
        html: '<p>is it possible to somehow run it on 6gb vram? i have a laptop with
          3060rtx<br>so far getting CUDA out of memory message</p>

          '
        raw: 'is it possible to somehow run it on 6gb vram? i have a laptop with 3060rtx

          so far getting CUDA out of memory message'
        updatedAt: '2023-04-19T04:58:57.650Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brokeboiflex
    id: 643f751146c418c9c68ba922
    type: comment
  author: FixYourSelf
  content: 'is it possible to somehow run it on 6gb vram? i have a laptop with 3060rtx

    so far getting CUDA out of memory message'
  created_at: 2023-04-19 03:58:57+00:00
  edited: false
  hidden: false
  id: 643f751146c418c9c68ba922
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: anon8231489123/vicuna-13b-GPTQ-4bit-128g
repo_type: model
status: open
target_branch: null
title: Vram usage
