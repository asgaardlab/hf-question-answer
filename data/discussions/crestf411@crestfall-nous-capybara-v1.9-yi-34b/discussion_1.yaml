!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Surprisekitty
conflicting_files: null
created_at: 2023-12-09 00:12:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ad7e934798f60466f6e3e30b61e04667.svg
      fullname: Tyler Heitman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Surprisekitty
      type: user
    createdAt: '2023-12-09T00:12:55.000Z'
    data:
      edited: true
      editors:
      - Surprisekitty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9674343466758728
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ad7e934798f60466f6e3e30b61e04667.svg
          fullname: Tyler Heitman
          isHf: false
          isPro: false
          name: Surprisekitty
          type: user
        html: '<p>Quantizing this at Q5 put''s it just out of the range of being run
          by standard 24GB cards like the 3090. Having it quantized at Q4 would allow
          a single 3090 to run the model rather than having to splurge and get two
          of them. At which point you could run a 70B model. Anyone who wants to run
          a 34B most of the time runs a 4.65bpw for Exllama or a Q4_K_M quant. If
          you could please release a Q4 quantization, that would be much appreciated.</p>

          '
        raw: Quantizing this at Q5 put's it just out of the range of being run by
          standard 24GB cards like the 3090. Having it quantized at Q4 would allow
          a single 3090 to run the model rather than having to splurge and get two
          of them. At which point you could run a 70B model. Anyone who wants to run
          a 34B most of the time runs a 4.65bpw for Exllama or a Q4_K_M quant. If
          you could please release a Q4 quantization, that would be much appreciated.
        updatedAt: '2023-12-09T17:42:24.216Z'
      numEdits: 2
      reactions: []
    id: 6573b10714881863157e731e
    type: comment
  author: Surprisekitty
  content: Quantizing this at Q5 put's it just out of the range of being run by standard
    24GB cards like the 3090. Having it quantized at Q4 would allow a single 3090
    to run the model rather than having to splurge and get two of them. At which point
    you could run a 70B model. Anyone who wants to run a 34B most of the time runs
    a 4.65bpw for Exllama or a Q4_K_M quant. If you could please release a Q4 quantization,
    that would be much appreciated.
  created_at: 2023-12-09 00:12:55+00:00
  edited: true
  hidden: false
  id: 6573b10714881863157e731e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/ad7e934798f60466f6e3e30b61e04667.svg
      fullname: Tyler Heitman
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Surprisekitty
      type: user
    createdAt: '2023-12-09T17:41:56.000Z'
    data:
      from: Why Q5 quant and not Q4?
      to: Could you quantize this at Q4 please?
    id: 6574a6e49091da7dc636a1ea
    type: title-change
  author: Surprisekitty
  created_at: 2023-12-09 17:41:56+00:00
  id: 6574a6e49091da7dc636a1ea
  new_title: Could you quantize this at Q4 please?
  old_title: Why Q5 quant and not Q4?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
      fullname: Crestfall
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: crestf411
      type: user
    createdAt: '2023-12-14T03:26:55.000Z'
    data:
      edited: false
      editors:
      - crestf411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8341662883758545
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
          fullname: Crestfall
          isHf: false
          isPro: false
          name: crestf411
          type: user
        html: '<p>Sorry for late response. Will upload q4_k_m quant in a bit.</p>

          '
        raw: Sorry for late response. Will upload q4_k_m quant in a bit.
        updatedAt: '2023-12-14T03:26:55.643Z'
      numEdits: 0
      reactions: []
    id: 657a75fff55b7314c3d8447e
    type: comment
  author: crestf411
  content: Sorry for late response. Will upload q4_k_m quant in a bit.
  created_at: 2023-12-14 03:26:55+00:00
  edited: false
  hidden: false
  id: 657a75fff55b7314c3d8447e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
      fullname: Crestfall
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: crestf411
      type: user
    createdAt: '2023-12-14T04:32:42.000Z'
    data:
      edited: false
      editors:
      - crestf411
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9919018149375916
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cd49f158261551be35af60f7e58f1deb.svg
          fullname: Crestfall
          isHf: false
          isPro: false
          name: crestf411
          type: user
        html: '<p>Uploaded.</p>

          '
        raw: Uploaded.
        updatedAt: '2023-12-14T04:32:42.327Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Surprisekitty
    id: 657a856a53ffbdd1d64a0979
    type: comment
  author: crestf411
  content: Uploaded.
  created_at: 2023-12-14 04:32:42+00:00
  edited: false
  hidden: false
  id: 657a856a53ffbdd1d64a0979
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: crestf411/crestfall-nous-capybara-v1.9-yi-34b
repo_type: model
status: open
target_branch: null
title: Could you quantize this at Q4 please?
