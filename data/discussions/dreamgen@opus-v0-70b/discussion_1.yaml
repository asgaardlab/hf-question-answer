!!python/object:huggingface_hub.community.DiscussionWithDetails
author: asterix51
conflicting_files: null
created_at: 2023-11-23 15:47:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/688d526cefc4db09c7944af997c15227.svg
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asterix51
      type: user
    createdAt: '2023-11-23T15:47:39.000Z'
    data:
      edited: false
      editors:
      - asterix51
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9141247272491455
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/688d526cefc4db09c7944af997c15227.svg
          fullname: '-'
          isHf: false
          isPro: false
          name: asterix51
          type: user
        html: '<p>Yep, apparently RTX 4080 16GB and 32gigs DRAM won''t cut it.</p>

          <p>"torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00
          MiB. GPU 0 has a total capacty of 15.99 GiB of which 0 bytes is free. Of
          the allocated memory 30.20 GiB is allocated by PyTorch, and 56.35 MiB is
          reserved by PyTorch but unallocated. If reserved but unallocated memory
          is large try setting max_split_size_mb to avoid fragmentation. See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF"</p>

          '
        raw: "Yep, apparently RTX 4080 16GB and 32gigs DRAM won't cut it.\r\n\r\n\"\
          torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00\
          \ MiB. GPU 0 has a total capacty of 15.99 GiB of which 0 bytes is free.\
          \ Of the allocated memory 30.20 GiB is allocated by PyTorch, and 56.35 MiB\
          \ is reserved by PyTorch but unallocated. If reserved but unallocated memory\
          \ is large try setting max_split_size_mb to avoid fragmentation. See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\""
        updatedAt: '2023-11-23T15:47:39.846Z'
      numEdits: 0
      reactions: []
    id: 655f741b48695217d5398e66
    type: comment
  author: asterix51
  content: "Yep, apparently RTX 4080 16GB and 32gigs DRAM won't cut it.\r\n\r\n\"\
    torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 40.00 MiB.\
    \ GPU 0 has a total capacty of 15.99 GiB of which 0 bytes is free. Of the allocated\
    \ memory 30.20 GiB is allocated by PyTorch, and 56.35 MiB is reserved by PyTorch\
    \ but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb\
    \ to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\""
  created_at: 2023-11-23 15:47:39+00:00
  edited: false
  hidden: false
  id: 655f741b48695217d5398e66
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6548b80bb3a7efb9391e19e8/DYCJL22AOn8kDLQhi9TaW.png?w=200&h=200&f=face
      fullname: DreamGen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DreamGenX
      type: user
    createdAt: '2023-11-23T16:05:37.000Z'
    data:
      edited: false
      editors:
      - DreamGenX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9114226698875427
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6548b80bb3a7efb9391e19e8/DYCJL22AOn8kDLQhi9TaW.png?w=200&h=200&f=face
          fullname: DreamGen
          isHf: false
          isPro: false
          name: DreamGenX
          type: user
        html: '<p>Hi there, can you share more information so that I can assist you?</p>

          <ol>

          <li>Which version of the model are you trying to run (this one, or some
          quant?)</li>

          <li>How are you running the model (what is the program and command)</li>

          </ol>

          <p>This is a 70B parameter model, so it''s not possible to run the full
          precision model on any single graphics card (even the H100 :)). The AWQ
          quant <a href="https://huggingface.co/dreamgen/opus-v0-70b-awq">dreamgen/opus-v0-70b-awq</a>
          takes at least 35G (I am running it on a 48GB card).</p>

          <p>You could try a few things:</p>

          <ol>

          <li>Run the 7B model instead <a href="https://huggingface.co/dreamgen/opus-v0-7b">dreamgen/opus-v0-7b</a>
          -- you can also try the GGUF Q8 and Q6 quants, which are not bad (the smaller
          ones are quite bad)</li>

          <li>Run the 70B quantized model on CPU -- this is going to be very slow,
          and I have not tried this myself</li>

          <li>Rent a GPU in the cloud</li>

          <li>Try it on dreamgen.com -- you can try both the 7B and the 70B AWQ versions</li>

          </ol>

          '
        raw: 'Hi there, can you share more information so that I can assist you?


          1. Which version of the model are you trying to run (this one, or some quant?)

          2. How are you running the model (what is the program and command)


          This is a 70B parameter model, so it''s not possible to run the full precision
          model on any single graphics card (even the H100 :)). The AWQ quant [dreamgen/opus-v0-70b-awq](https://huggingface.co/dreamgen/opus-v0-70b-awq)
          takes at least 35G (I am running it on a 48GB card).


          You could try a few things:


          1. Run the 7B model instead [dreamgen/opus-v0-7b](https://huggingface.co/dreamgen/opus-v0-7b)
          -- you can also try the GGUF Q8 and Q6 quants, which are not bad (the smaller
          ones are quite bad)

          2. Run the 70B quantized model on CPU -- this is going to be very slow,
          and I have not tried this myself

          3. Rent a GPU in the cloud

          4. Try it on dreamgen.com -- you can try both the 7B and the 70B AWQ versions'
        updatedAt: '2023-11-23T16:05:37.001Z'
      numEdits: 0
      reactions: []
    id: 655f7851afee0e0078a14ee1
    type: comment
  author: DreamGenX
  content: 'Hi there, can you share more information so that I can assist you?


    1. Which version of the model are you trying to run (this one, or some quant?)

    2. How are you running the model (what is the program and command)


    This is a 70B parameter model, so it''s not possible to run the full precision
    model on any single graphics card (even the H100 :)). The AWQ quant [dreamgen/opus-v0-70b-awq](https://huggingface.co/dreamgen/opus-v0-70b-awq)
    takes at least 35G (I am running it on a 48GB card).


    You could try a few things:


    1. Run the 7B model instead [dreamgen/opus-v0-7b](https://huggingface.co/dreamgen/opus-v0-7b)
    -- you can also try the GGUF Q8 and Q6 quants, which are not bad (the smaller
    ones are quite bad)

    2. Run the 70B quantized model on CPU -- this is going to be very slow, and I
    have not tried this myself

    3. Rent a GPU in the cloud

    4. Try it on dreamgen.com -- you can try both the 7B and the 70B AWQ versions'
  created_at: 2023-11-23 16:05:37+00:00
  edited: false
  hidden: false
  id: 655f7851afee0e0078a14ee1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/688d526cefc4db09c7944af997c15227.svg
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asterix51
      type: user
    createdAt: '2023-11-23T16:15:15.000Z'
    data:
      edited: false
      editors:
      - asterix51
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9703207612037659
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/688d526cefc4db09c7944af997c15227.svg
          fullname: '-'
          isHf: false
          isPro: false
          name: asterix51
          type: user
        html: '<p>Tried v0-70b-awq version with text-gen-web-ui. I can''t use dreamgen
          service either since I can''t comply with their data collection policy as
          my work is protected.</p>

          '
        raw: Tried v0-70b-awq version with text-gen-web-ui. I can't use dreamgen service
          either since I can't comply with their data collection policy as my work
          is protected.
        updatedAt: '2023-11-23T16:15:15.079Z'
      numEdits: 0
      reactions: []
    id: 655f7a9365bc71935d90ac9b
    type: comment
  author: asterix51
  content: Tried v0-70b-awq version with text-gen-web-ui. I can't use dreamgen service
    either since I can't comply with their data collection policy as my work is protected.
  created_at: 2023-11-23 16:15:15+00:00
  edited: false
  hidden: false
  id: 655f7a9365bc71935d90ac9b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6548b80bb3a7efb9391e19e8/DYCJL22AOn8kDLQhi9TaW.png?w=200&h=200&f=face
      fullname: DreamGen
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: DreamGenX
      type: user
    createdAt: '2023-11-23T16:21:13.000Z'
    data:
      edited: false
      editors:
      - DreamGenX
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.896425187587738
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6548b80bb3a7efb9391e19e8/DYCJL22AOn8kDLQhi9TaW.png?w=200&h=200&f=face
          fullname: DreamGen
          isHf: false
          isPro: false
          name: DreamGenX
          type: user
        html: '<p>Thanks for the details, the 70B-AWQ requires at least 35G of VRAM,
          so you can''t run it with your GPU. You can try this quantized version,
          the smallest, from TheBloke: <a href="https://huggingface.co/TheBloke/opus-v0-70B-GGUF/blob/main/opus-v0-70b.Q2_K.gguf">https://huggingface.co/TheBloke/opus-v0-70B-GGUF/blob/main/opus-v0-70b.Q2_K.gguf</a>
          you could then try running this with llama.cpp backend on your CPU (or offload
          some work to GPU as well), but expect it to be slow.</p>

          <p>Otherwise, you could try running on some GPU cloud, I use RunPod. To
          run the 70B AWQ model, I use A6000 with the vLLM backend.</p>

          '
        raw: 'Thanks for the details, the 70B-AWQ requires at least 35G of VRAM, so
          you can''t run it with your GPU. You can try this quantized version, the
          smallest, from TheBloke: https://huggingface.co/TheBloke/opus-v0-70B-GGUF/blob/main/opus-v0-70b.Q2_K.gguf
          you could then try running this with llama.cpp backend on your CPU (or offload
          some work to GPU as well), but expect it to be slow.


          Otherwise, you could try running on some GPU cloud, I use RunPod. To run
          the 70B AWQ model, I use A6000 with the vLLM backend.'
        updatedAt: '2023-11-23T16:21:13.160Z'
      numEdits: 0
      reactions: []
    id: 655f7bf9a3e81145769cdc1e
    type: comment
  author: DreamGenX
  content: 'Thanks for the details, the 70B-AWQ requires at least 35G of VRAM, so
    you can''t run it with your GPU. You can try this quantized version, the smallest,
    from TheBloke: https://huggingface.co/TheBloke/opus-v0-70B-GGUF/blob/main/opus-v0-70b.Q2_K.gguf
    you could then try running this with llama.cpp backend on your CPU (or offload
    some work to GPU as well), but expect it to be slow.


    Otherwise, you could try running on some GPU cloud, I use RunPod. To run the 70B
    AWQ model, I use A6000 with the vLLM backend.'
  created_at: 2023-11-23 16:21:13+00:00
  edited: false
  hidden: false
  id: 655f7bf9a3e81145769cdc1e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/688d526cefc4db09c7944af997c15227.svg
      fullname: '-'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: asterix51
      type: user
    createdAt: '2023-11-23T16:22:43.000Z'
    data:
      edited: false
      editors:
      - asterix51
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9768259525299072
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/688d526cefc4db09c7944af997c15227.svg
          fullname: '-'
          isHf: false
          isPro: false
          name: asterix51
          type: user
        html: '<p>Thanks for the tips. I''ll give it a go.</p>

          '
        raw: Thanks for the tips. I'll give it a go.
        updatedAt: '2023-11-23T16:22:43.661Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - DreamGenX
    id: 655f7c5365bc71935d90ff89
    type: comment
  author: asterix51
  content: Thanks for the tips. I'll give it a go.
  created_at: 2023-11-23 16:22:43+00:00
  edited: false
  hidden: false
  id: 655f7c5365bc71935d90ff89
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: dreamgen/opus-v0-70b
repo_type: model
status: open
target_branch: null
title: 'torch.cuda.OutOfMemoryError: CUDA out of memory'
