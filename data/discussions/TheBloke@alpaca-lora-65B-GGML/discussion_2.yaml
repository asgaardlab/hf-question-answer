!!python/object:huggingface_hub.community.DiscussionWithDetails
author: AiCreatornator
conflicting_files: null
created_at: 2023-04-20 04:38:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6c01655151f75c3bc03e9558e2821355.svg
      fullname: Ai Creator
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AiCreatornator
      type: user
    createdAt: '2023-04-20T05:38:41.000Z'
    data:
      edited: false
      editors:
      - AiCreatornator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6c01655151f75c3bc03e9558e2821355.svg
          fullname: Ai Creator
          isHf: false
          isPro: false
          name: AiCreatornator
          type: user
        html: '<p>I just want to thank you, because I have been waiting for this type
          of model, and you are the first one who made these available here. This
          is the best model I have tried locally this far. Thank you!</p>

          '
        raw: I just want to thank you, because I have been waiting for this type of
          model, and you are the first one who made these available here. This is
          the best model I have tried locally this far. Thank you!
        updatedAt: '2023-04-20T05:38:41.309Z'
      numEdits: 0
      reactions:
      - count: 6
        reaction: "\U0001F44D"
        users:
        - KnutJaegersberg
        - TheBloke
        - Yhyu13
        - mirek190
        - dzupin
        - Hanssep123
    id: 6440cfe1388f31f64cd109d5
    type: comment
  author: AiCreatornator
  content: I just want to thank you, because I have been waiting for this type of
    model, and you are the first one who made these available here. This is the best
    model I have tried locally this far. Thank you!
  created_at: 2023-04-20 04:38:41+00:00
  edited: false
  hidden: false
  id: 6440cfe1388f31f64cd109d5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-20T10:17:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You''re welcome! Glad it''s working well for you.</p>

          '
        raw: You're welcome! Glad it's working well for you.
        updatedAt: '2023-04-20T10:17:00.619Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jeffwadsworth
    id: 6441111ce46e14ed55866c02
    type: comment
  author: TheBloke
  content: You're welcome! Glad it's working well for you.
  created_at: 2023-04-20 09:17:00+00:00
  edited: false
  hidden: false
  id: 6441111ce46e14ed55866c02
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c0593eab4fbb21da9114393e28bbba5e.svg
      fullname: Maximilian Winter
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Lumpen1
      type: user
    createdAt: '2023-04-20T11:36:32.000Z'
    data:
      edited: false
      editors:
      - Lumpen1
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c0593eab4fbb21da9114393e28bbba5e.svg
          fullname: Maximilian Winter
          isHf: false
          isPro: false
          name: Lumpen1
          type: user
        html: '<p>Also want to thank you for this!</p>

          '
        raw: Also want to thank you for this!
        updatedAt: '2023-04-20T11:36:32.925Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - Yhyu13
        - mirek190
    id: 644123c07f13a7b5a25e4ce4
    type: comment
  author: Lumpen1
  content: Also want to thank you for this!
  created_at: 2023-04-20 10:36:32+00:00
  edited: false
  hidden: false
  id: 644123c07f13a7b5a25e4ce4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
      fullname: Samuel Azran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelAzran
      type: user
    createdAt: '2023-04-22T15:02:32.000Z'
    data:
      edited: false
      editors:
      - SamuelAzran
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7aae73caca1473788b82308a55e332d8.svg
          fullname: Samuel Azran
          isHf: false
          isPro: false
          name: SamuelAzran
          type: user
        html: '<p>Is it possible to run it on a GPU with HF?</p>

          '
        raw: Is it possible to run it on a GPU with HF?
        updatedAt: '2023-04-22T15:02:32.172Z'
      numEdits: 0
      reactions: []
    id: 6443f7083dc28377632cd637
    type: comment
  author: SamuelAzran
  content: Is it possible to run it on a GPU with HF?
  created_at: 2023-04-22 14:02:32+00:00
  edited: false
  hidden: false
  id: 6443f7083dc28377632cd637
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-22T15:14:34.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>It''s possible to run on the GPU yes. </p>

          <p>I''ve done these repos:<br>4bit GPTQ quantisation: <a href="https://huggingface.co/TheBloke/alpaca-lora-65B-GPTQ-4bit">https://huggingface.co/TheBloke/alpaca-lora-65B-GPTQ-4bit</a><br>Full
          unquantised HF format: <a href="https://huggingface.co/TheBloke/alpaca-lora-65B-HF">https://huggingface.co/TheBloke/alpaca-lora-65B-HF</a></p>

          <p>The latter would need 128+ GB of VRAM so that''s not likely to be viable
          for most people.  The GPTQ 4bits should hopefully run in 40GB of VRAM, eg
          1 x A100 40GB or 2 x 24GB cards like a 3090 or 4090.  I haven''t actually
          tested them yet, I''m planning to do so soon.  But they should  work OK.</p>

          <p>Here''s an explanation of the three different files on the GPTQ repo.
          I''ve not had a chance to add this to the README yet:</p>

          <p>alpaca-lora-65B-GPTQ-4bit-128g.safetensors :</p>

          <p>GPTQ 4bit 128g with --act-order. Should be highest possible quality quantisation.
          Will require recent GPTQ-for-LLaMA code; will not work with oobaboog''s
          fork, and therefore won''t work with the one-click-installers for Windows.</p>

          <p>alpaca-lora-65B-GPTQ-4bit-1024g.safetensors: Same as the above but with
          a groupsize of 1024. This possibly reduces the quantisation quality slightly,
          but will require less VRAM. Created with the idea of ensuring this file
          could load in 40GB VRAM on an A100 - it''s possible the 128g will need more
          than 40GB.</p>

          <p>alpaca-lora-65B-GPTQ-4bit-128g.no-act-order.safetensors:</p>

          <p>GPTQ 4bit 128g without --act-order. Possibly slightly lower accuracy.
          Will work with oobabooga''s GPTQ-for-LLaMA fork and the one-click installers</p>

          '
        raw: "It's possible to run on the GPU yes. \n\nI've done these repos:\n4bit\
          \ GPTQ quantisation: https://huggingface.co/TheBloke/alpaca-lora-65B-GPTQ-4bit\n\
          Full unquantised HF format: https://huggingface.co/TheBloke/alpaca-lora-65B-HF\n\
          \nThe latter would need 128+ GB of VRAM so that's not likely to be viable\
          \ for most people.  The GPTQ 4bits should hopefully run in 40GB of VRAM,\
          \ eg 1 x A100 40GB or 2 x 24GB cards like a 3090 or 4090.  I haven't actually\
          \ tested them yet, I'm planning to do so soon.  But they should  work OK.\n\
          \nHere's an explanation of the three different files on the GPTQ repo. I've\
          \ not had a chance to add this to the README yet:\n\nalpaca-lora-65B-GPTQ-4bit-128g.safetensors\
          \ :\n\nGPTQ 4bit 128g with --act-order. Should be highest possible quality\
          \ quantisation. Will require recent GPTQ-for-LLaMA code; will not work with\
          \ oobaboog's fork, and therefore won't work with the one-click-installers\
          \ for Windows.\n\nalpaca-lora-65B-GPTQ-4bit-1024g.safetensors: Same as the\
          \ above but with a groupsize of 1024. This possibly reduces the quantisation\
          \ quality slightly, but will require less VRAM. Created with the idea of\
          \ ensuring this file could load in 40GB VRAM on an A100 - it's possible\
          \ the 128g will need more than 40GB.\n\nalpaca-lora-65B-GPTQ-4bit-128g.no-act-order.safetensors:\n\
          \nGPTQ 4bit 128g without --act-order. Possibly slightly lower accuracy.\
          \ Will work with oobabooga's GPTQ-for-LLaMA fork and the one-click installers"
        updatedAt: '2023-04-22T15:14:34.013Z'
      numEdits: 0
      reactions: []
    id: 6443f9da3dc28377632d0155
    type: comment
  author: TheBloke
  content: "It's possible to run on the GPU yes. \n\nI've done these repos:\n4bit\
    \ GPTQ quantisation: https://huggingface.co/TheBloke/alpaca-lora-65B-GPTQ-4bit\n\
    Full unquantised HF format: https://huggingface.co/TheBloke/alpaca-lora-65B-HF\n\
    \nThe latter would need 128+ GB of VRAM so that's not likely to be viable for\
    \ most people.  The GPTQ 4bits should hopefully run in 40GB of VRAM, eg 1 x A100\
    \ 40GB or 2 x 24GB cards like a 3090 or 4090.  I haven't actually tested them\
    \ yet, I'm planning to do so soon.  But they should  work OK.\n\nHere's an explanation\
    \ of the three different files on the GPTQ repo. I've not had a chance to add\
    \ this to the README yet:\n\nalpaca-lora-65B-GPTQ-4bit-128g.safetensors :\n\n\
    GPTQ 4bit 128g with --act-order. Should be highest possible quality quantisation.\
    \ Will require recent GPTQ-for-LLaMA code; will not work with oobaboog's fork,\
    \ and therefore won't work with the one-click-installers for Windows.\n\nalpaca-lora-65B-GPTQ-4bit-1024g.safetensors:\
    \ Same as the above but with a groupsize of 1024. This possibly reduces the quantisation\
    \ quality slightly, but will require less VRAM. Created with the idea of ensuring\
    \ this file could load in 40GB VRAM on an A100 - it's possible the 128g will need\
    \ more than 40GB.\n\nalpaca-lora-65B-GPTQ-4bit-128g.no-act-order.safetensors:\n\
    \nGPTQ 4bit 128g without --act-order. Possibly slightly lower accuracy. Will work\
    \ with oobabooga's GPTQ-for-LLaMA fork and the one-click installers"
  created_at: 2023-04-22 14:14:34+00:00
  edited: false
  hidden: false
  id: 6443f9da3dc28377632d0155
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e30f67adbbf2377a9e4d672127313a84.svg
      fullname: M M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Melbourne
      type: user
    createdAt: '2023-04-26T19:30:34.000Z'
    data:
      edited: true
      editors:
      - Melbourne
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e30f67adbbf2377a9e4d672127313a84.svg
          fullname: M M
          isHf: false
          isPro: false
          name: Melbourne
          type: user
        html: '<p>This probably generates the most ChatGPT 3.5-like responses of any
          local setup I''ve tried. Pretty cool. It''s slow even on a "fast" by consumer
          standards computer but I''d rather wait than get useless output.</p>

          '
        raw: This probably generates the most ChatGPT 3.5-like responses of any local
          setup I've tried. Pretty cool. It's slow even on a "fast" by consumer standards
          computer but I'd rather wait than get useless output.
        updatedAt: '2023-04-26T19:32:07.927Z'
      numEdits: 1
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - TheBloke
        - Yhyu13
        - mirek190
        - dzupin
        - clevnumb
    id: 64497bda111b3bf6877ee7b0
    type: comment
  author: Melbourne
  content: This probably generates the most ChatGPT 3.5-like responses of any local
    setup I've tried. Pretty cool. It's slow even on a "fast" by consumer standards
    computer but I'd rather wait than get useless output.
  created_at: 2023-04-26 18:30:34+00:00
  edited: true
  hidden: false
  id: 64497bda111b3bf6877ee7b0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-02T19:05:06.000Z'
    data:
      edited: true
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Yep - that is the most close model to GPT 3.5 ... or even better
          than GPt 3.5 especially q5_1.<br>Any ultra max mega tuned models 7B or 13B
          are not even close to standard alpaca-lora 65B.</p>

          <p>I am testing by writing stories capability ..So . are better that GPT
          3.5 actually.<br>Coding seems also better ....</p>

          '
        raw: 'Yep - that is the most close model to GPT 3.5 ... or even better than
          GPt 3.5 especially q5_1.

          Any ultra max mega tuned models 7B or 13B are not even close to standard
          alpaca-lora 65B.


          I am testing by writing stories capability ..So . are better that GPT 3.5
          actually.

          Coding seems also better ....'
        updatedAt: '2023-05-10T20:42:03.125Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dzupin
    id: 64515ee241f3c769b9131a34
    type: comment
  author: mirek190
  content: 'Yep - that is the most close model to GPT 3.5 ... or even better than
    GPt 3.5 especially q5_1.

    Any ultra max mega tuned models 7B or 13B are not even close to standard alpaca-lora
    65B.


    I am testing by writing stories capability ..So . are better that GPT 3.5 actually.

    Coding seems also better ....'
  created_at: 2023-05-02 18:05:06+00:00
  edited: true
  hidden: false
  id: 64515ee241f3c769b9131a34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
      fullname: Lance
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: clevnumb
      type: user
    createdAt: '2023-05-10T17:17:46.000Z'
    data:
      edited: false
      editors:
      - clevnumb
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/550e36f3104051a9f97622a1ec5c8c50.svg
          fullname: Lance
          isHf: false
          isPro: false
          name: clevnumb
          type: user
        html: '<p>I have a 65GB RAM system (I9-13900k) system with a 4090 video card,
          but I guess I still need to use the CPU version..how do I most easily install
          this and get it running? the model page says something would have to be
          compiled?  I currently have textual generation-UI set up and it works with
          lower models...thanks.</p>

          '
        raw: I have a 65GB RAM system (I9-13900k) system with a 4090 video card, but
          I guess I still need to use the CPU version..how do I most easily install
          this and get it running? the model page says something would have to be
          compiled?  I currently have textual generation-UI set up and it works with
          lower models...thanks.
        updatedAt: '2023-05-10T17:17:46.674Z'
      numEdits: 0
      reactions: []
    id: 645bd1ba8bbb8592d9196b93
    type: comment
  author: clevnumb
  content: I have a 65GB RAM system (I9-13900k) system with a 4090 video card, but
    I guess I still need to use the CPU version..how do I most easily install this
    and get it running? the model page says something would have to be compiled?  I
    currently have textual generation-UI set up and it works with lower models...thanks.
  created_at: 2023-05-10 16:17:46+00:00
  edited: false
  hidden: false
  id: 645bd1ba8bbb8592d9196b93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-05-10T20:17:10.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>You should be able to use llama.cpp models in text-generation-webui.
          Check out these docs:  <a rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md">https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md</a></p>

          '
        raw: 'You should be able to use llama.cpp models in text-generation-webui.
          Check out these docs:  https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md'
        updatedAt: '2023-05-10T20:17:10.029Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mirek190
    id: 645bfbc6b396a40a8493d24b
    type: comment
  author: TheBloke
  content: 'You should be able to use llama.cpp models in text-generation-webui. Check
    out these docs:  https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md'
  created_at: 2023-05-10 19:17:10+00:00
  edited: false
  hidden: false
  id: 645bfbc6b396a40a8493d24b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
      fullname: ko
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mirek190
      type: user
    createdAt: '2023-05-10T20:40:45.000Z'
    data:
      edited: false
      editors:
      - mirek190
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8796019821146893ce5150cec2573a12.svg
          fullname: ko
          isHf: false
          isPro: false
          name: mirek190
          type: user
        html: '<p>Yep ..llama.cpp is great.</p>

          '
        raw: Yep ..llama.cpp is great.
        updatedAt: '2023-05-10T20:40:45.069Z'
      numEdits: 0
      reactions: []
    id: 645c014db4e65f04f7f8c5ea
    type: comment
  author: mirek190
  content: Yep ..llama.cpp is great.
  created_at: 2023-05-10 19:40:45+00:00
  edited: false
  hidden: false
  id: 645c014db4e65f04f7f8c5ea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/868d616446f781405cbb57952e4f22a1.svg
      fullname: DarVin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dvc071
      type: user
    createdAt: '2023-05-19T12:21:25.000Z'
    data:
      edited: false
      editors:
      - dvc071
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/868d616446f781405cbb57952e4f22a1.svg
          fullname: DarVin
          isHf: false
          isPro: false
          name: dvc071
          type: user
        html: '<p>How would 65B-HF run on the Mac with M1 Ma Studio (with 10-core
          CPU, 24-core GPU, 16-core Neural Engine, 32GB unified memory)?</p>

          '
        raw: How would 65B-HF run on the Mac with M1 Ma Studio (with 10-core CPU,
          24-core GPU, 16-core Neural Engine, 32GB unified memory)?
        updatedAt: '2023-05-19T12:21:25.823Z'
      numEdits: 0
      reactions: []
    id: 646769c5a48c2b6f0d5f84d3
    type: comment
  author: dvc071
  content: How would 65B-HF run on the Mac with M1 Ma Studio (with 10-core CPU, 24-core
    GPU, 16-core Neural Engine, 32GB unified memory)?
  created_at: 2023-05-19 11:21:25+00:00
  edited: false
  hidden: false
  id: 646769c5a48c2b6f0d5f84d3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/alpaca-lora-65B-GGML
repo_type: model
status: open
target_branch: null
title: Thank you very much!
