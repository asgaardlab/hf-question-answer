!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Phil2Sat
conflicting_files: null
created_at: 2023-09-23 08:16:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f50bd0b9bc456262d43235ffba7e8f50.svg
      fullname: Andreas
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Phil2Sat
      type: user
    createdAt: '2023-09-23T09:16:50.000Z'
    data:
      edited: false
      editors:
      - Phil2Sat
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9633483290672302
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f50bd0b9bc456262d43235ffba7e8f50.svg
          fullname: Andreas
          isHf: false
          isPro: false
          name: Phil2Sat
          type: user
        html: '<p>I''ve tried the model but it seems the knowledge is limited to 2018
          and prior. I asked some questions reguarding Wagner Chef Prigoschin and
          only he knows that he 2018 retired.</p>

          <p>I''m an absolute noob to this but is it possible to recreate this with
          actual Datasets?</p>

          <p>Thanks in advance<br>Phil2Sat</p>

          '
        raw: "I've tried the model but it seems the knowledge is limited to 2018 and\
          \ prior. I asked some questions reguarding Wagner Chef Prigoschin and only\
          \ he knows that he 2018 retired.\r\n\r\nI'm an absolute noob to this but\
          \ is it possible to recreate this with actual Datasets?\r\n\r\nThanks in\
          \ advance\r\nPhil2Sat"
        updatedAt: '2023-09-23T09:16:50.911Z'
      numEdits: 0
      reactions: []
    id: 650ead02d4e844fefd9e88b9
    type: comment
  author: Phil2Sat
  content: "I've tried the model but it seems the knowledge is limited to 2018 and\
    \ prior. I asked some questions reguarding Wagner Chef Prigoschin and only he\
    \ knows that he 2018 retired.\r\n\r\nI'm an absolute noob to this but is it possible\
    \ to recreate this with actual Datasets?\r\n\r\nThanks in advance\r\nPhil2Sat"
  created_at: 2023-09-23 08:16:50+00:00
  edited: false
  hidden: false
  id: 650ead02d4e844fefd9e88b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661340992329-noauth.png?w=200&h=200&f=face
      fullname: Yazan Agha-Schrader
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: phi0112358
      type: user
    createdAt: '2023-09-25T09:26:01.000Z'
    data:
      edited: true
      editors:
      - phi0112358
      hidden: false
      identifiedLanguage:
        language: de
        probability: 0.8153420090675354
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1661340992329-noauth.png?w=200&h=200&f=face
          fullname: Yazan Agha-Schrader
          isHf: false
          isPro: false
          name: phi0112358
          type: user
        html: "<p><strong>German below</strong></p>\n<p>Hi, I did not create the model\
          \ myself, I just did a quantization. Therefore, I can't offer any direct\
          \ help in this regard, unfortunately. Furthermore, .ggml is now an outdated\
          \ format. However, I have also found that the model gives relatively poor\
          \ performance (both in German and related to facts), so I have also stopped\
          \ using it. In principle, however, it is always possible to finetune a model\
          \ with a current dataset, although the consensus at the moment is that finetuning\
          \ does not really teach the model any new knowledge and is rather inefficient\
          \ for such a task.</p>\n<p>Anyway, I can highly recommend the model codeCherryPop.\
          \ Although it was not directly trained on it, it can comparatively incredibly\
          \ well German and provides very good facts. Here is a link to it:</p>\n\
          <p><a href=\"https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGUF\"\
          >codeCherryPop 7B gguf</a></p>\n<p>Edit: In my experience, if you want to\
          \ talk to codeCherryPop in German, the prompt should also be in German so\
          \ that you get extra coherent results in German. Something like this: In\
          \ etwa so:</p>\n<pre><code>Im Folgenden findest du eine Anweisung, die eine\
          \ Aufgabe beschreibt. Schreibe eine Antwort, mit der du die Aufgabe angemessen\
          \ erf\xFCllst.\n\n### Anweisung:\n{Here your request}\n\n### Antwort:\n\
          </code></pre>\n<hr>\n<p>Hi, ich habe das Modell nicht selbst erstellt, sondern\
          \ nur eine Quantisierung vorgenommen. Daher kann ich diesbez\xFCglich leider\
          \ keine direkte Hilfe anbieten. Dar\xFCberhinaus ist .ggml inzwischen ein\
          \ veraltetes Format. Allerdings habe ich auch festgestellt, dass das Modell\
          \ relativ schlechte Performance liefert (sowohl in deutscher Sprache als\
          \ auch bezogen auf Fakten), daher habe ich es auch nicht mehr benutzt. Prinzipiell\
          \ ist es aber jederzeit m\xF6glich, ein Modell mit einem aktuellen Datensatz\
          \  zu finetunen, wobei der Konsensus ja momentan ist, dass das Finetuning\
          \ dem Modell nicht wirklich neues Wissen beibringt und eher ineffizient\
          \ ist f\xFCr ein solches Vorhaben.</p>\n<p>Wie dem auch sei, ich kann w\xE4\
          rmstens das Modell codeCherryPop empfehlen. Obwohl es nicht direkt darauf\
          \ trainiert wurde, kann es vergleichsweise unglaublich gut Deutsch und liefert\
          \ sehr gute Fakten. Hier ein Link dazu:</p>\n<p><a href=\"https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGUF\"\
          >codeCherryPop 7B gguf</a></p>\n<p>Edit: Meiner Erfahrung nach, wenn man\
          \ sich auf Deutsch mit codeCherryPop unterhalten m\xF6chte, sollte der Prompt\
          \ ebenfalls auf Deutsch sein, damit man besonders koh\xE4rente Ergebnisse\
          \ auf Deutsch bekommt. In etwa so:</p>\n<pre><code>Im Folgenden findest\
          \ du eine Anweisung, die eine Aufgabe beschreibt. Schreibe eine Antwort,\
          \ mit der du die Aufgabe angemessen erf\xFCllst.\n\n### Anweisung:\n{Hier\
          \ deine Anfrage}\n\n### Antwort:\n</code></pre>\n"
        raw: "**German below**\n\nHi, I did not create the model myself, I just did\
          \ a quantization. Therefore, I can't offer any direct help in this regard,\
          \ unfortunately. Furthermore, .ggml is now an outdated format. However,\
          \ I have also found that the model gives relatively poor performance (both\
          \ in German and related to facts), so I have also stopped using it. In principle,\
          \ however, it is always possible to finetune a model with a current dataset,\
          \ although the consensus at the moment is that finetuning does not really\
          \ teach the model any new knowledge and is rather inefficient for such a\
          \ task.\n\nAnyway, I can highly recommend the model codeCherryPop. Although\
          \ it was not directly trained on it, it can comparatively incredibly well\
          \ German and provides very good facts. Here is a link to it:\n\n[codeCherryPop\
          \ 7B gguf](https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGUF)\n\
          \nEdit: In my experience, if you want to talk to codeCherryPop in German,\
          \ the prompt should also be in German so that you get extra coherent results\
          \ in German. Something like this: In etwa so:\n\n```\nIm Folgenden findest\
          \ du eine Anweisung, die eine Aufgabe beschreibt. Schreibe eine Antwort,\
          \ mit der du die Aufgabe angemessen erf\xFCllst.\n\n### Anweisung:\n{Here\
          \ your request}\n\n### Antwort:\n```\n\n- - -\n\nHi, ich habe das Modell\
          \ nicht selbst erstellt, sondern nur eine Quantisierung vorgenommen. Daher\
          \ kann ich diesbez\xFCglich leider keine direkte Hilfe anbieten. Dar\xFC\
          berhinaus ist .ggml inzwischen ein veraltetes Format. Allerdings habe ich\
          \ auch festgestellt, dass das Modell relativ schlechte Performance liefert\
          \ (sowohl in deutscher Sprache als auch bezogen auf Fakten), daher habe\
          \ ich es auch nicht mehr benutzt. Prinzipiell ist es aber jederzeit m\xF6\
          glich, ein Modell mit einem aktuellen Datensatz  zu finetunen, wobei der\
          \ Konsensus ja momentan ist, dass das Finetuning dem Modell nicht wirklich\
          \ neues Wissen beibringt und eher ineffizient ist f\xFCr ein solches Vorhaben.\n\
          \nWie dem auch sei, ich kann w\xE4rmstens das Modell codeCherryPop empfehlen.\
          \ Obwohl es nicht direkt darauf trainiert wurde, kann es vergleichsweise\
          \ unglaublich gut Deutsch und liefert sehr gute Fakten. Hier ein Link dazu:\n\
          \n[codeCherryPop 7B gguf](https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGUF)\n\
          \nEdit: Meiner Erfahrung nach, wenn man sich auf Deutsch mit codeCherryPop\
          \ unterhalten m\xF6chte, sollte der Prompt ebenfalls auf Deutsch sein, damit\
          \ man besonders koh\xE4rente Ergebnisse auf Deutsch bekommt. In etwa so:\n\
          \n```\nIm Folgenden findest du eine Anweisung, die eine Aufgabe beschreibt.\
          \ Schreibe eine Antwort, mit der du die Aufgabe angemessen erf\xFCllst.\n\
          \n### Anweisung:\n{Hier deine Anfrage}\n\n### Antwort:\n```"
        updatedAt: '2023-09-25T09:34:09.229Z'
      numEdits: 3
      reactions: []
    id: 65115229fe247f8485d43ea1
    type: comment
  author: phi0112358
  content: "**German below**\n\nHi, I did not create the model myself, I just did\
    \ a quantization. Therefore, I can't offer any direct help in this regard, unfortunately.\
    \ Furthermore, .ggml is now an outdated format. However, I have also found that\
    \ the model gives relatively poor performance (both in German and related to facts),\
    \ so I have also stopped using it. In principle, however, it is always possible\
    \ to finetune a model with a current dataset, although the consensus at the moment\
    \ is that finetuning does not really teach the model any new knowledge and is\
    \ rather inefficient for such a task.\n\nAnyway, I can highly recommend the model\
    \ codeCherryPop. Although it was not directly trained on it, it can comparatively\
    \ incredibly well German and provides very good facts. Here is a link to it:\n\
    \n[codeCherryPop 7B gguf](https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGUF)\n\
    \nEdit: In my experience, if you want to talk to codeCherryPop in German, the\
    \ prompt should also be in German so that you get extra coherent results in German.\
    \ Something like this: In etwa so:\n\n```\nIm Folgenden findest du eine Anweisung,\
    \ die eine Aufgabe beschreibt. Schreibe eine Antwort, mit der du die Aufgabe angemessen\
    \ erf\xFCllst.\n\n### Anweisung:\n{Here your request}\n\n### Antwort:\n```\n\n\
    - - -\n\nHi, ich habe das Modell nicht selbst erstellt, sondern nur eine Quantisierung\
    \ vorgenommen. Daher kann ich diesbez\xFCglich leider keine direkte Hilfe anbieten.\
    \ Dar\xFCberhinaus ist .ggml inzwischen ein veraltetes Format. Allerdings habe\
    \ ich auch festgestellt, dass das Modell relativ schlechte Performance liefert\
    \ (sowohl in deutscher Sprache als auch bezogen auf Fakten), daher habe ich es\
    \ auch nicht mehr benutzt. Prinzipiell ist es aber jederzeit m\xF6glich, ein Modell\
    \ mit einem aktuellen Datensatz  zu finetunen, wobei der Konsensus ja momentan\
    \ ist, dass das Finetuning dem Modell nicht wirklich neues Wissen beibringt und\
    \ eher ineffizient ist f\xFCr ein solches Vorhaben.\n\nWie dem auch sei, ich kann\
    \ w\xE4rmstens das Modell codeCherryPop empfehlen. Obwohl es nicht direkt darauf\
    \ trainiert wurde, kann es vergleichsweise unglaublich gut Deutsch und liefert\
    \ sehr gute Fakten. Hier ein Link dazu:\n\n[codeCherryPop 7B gguf](https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGUF)\n\
    \nEdit: Meiner Erfahrung nach, wenn man sich auf Deutsch mit codeCherryPop unterhalten\
    \ m\xF6chte, sollte der Prompt ebenfalls auf Deutsch sein, damit man besonders\
    \ koh\xE4rente Ergebnisse auf Deutsch bekommt. In etwa so:\n\n```\nIm Folgenden\
    \ findest du eine Anweisung, die eine Aufgabe beschreibt. Schreibe eine Antwort,\
    \ mit der du die Aufgabe angemessen erf\xFCllst.\n\n### Anweisung:\n{Hier deine\
    \ Anfrage}\n\n### Antwort:\n```"
  created_at: 2023-09-25 08:26:01+00:00
  edited: true
  hidden: false
  id: 65115229fe247f8485d43ea1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: phi0112358/Zicklein-7B-german_Alpaca-ggml
repo_type: model
status: open
target_branch: null
title: Works great with GPT4All, BUT
