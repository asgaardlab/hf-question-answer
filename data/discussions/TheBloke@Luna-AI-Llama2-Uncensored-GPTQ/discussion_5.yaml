!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nihponex
conflicting_files: null
created_at: 2023-08-07 18:39:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e8930fb4d25852745d68c94a92fc4373.svg
      fullname: Nex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nihponex
      type: user
    createdAt: '2023-08-07T19:39:07.000Z'
    data:
      edited: false
      editors:
      - nihponex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4326588213443756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e8930fb4d25852745d68c94a92fc4373.svg
          fullname: Nex
          isHf: false
          isPro: false
          name: nihponex
          type: user
        html: '<p>Hi all</p>

          <p>Despite even changing my ubuntu swap file file and setting restrictions
          in the webui - I consistently get an error loading this model in text-generation-webui.  </p>

          <p>Relevant info below - any ideas.   Before I did the swap changes - it
          just died with a "Killed" message.</p>

          <p>+---------------------------------------------------------------------------------------+<br>|
          NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:
          12.2     |<br>|-----------------------------------------+----------------------+----------------------+<br>|
          GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile
          Uncorr. ECC |<br>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage
          | GPU-Util  Compute M. |<br>|                                         |                      |               MIG
          M. |<br>|=========================================+======================+======================|<br>|   0  NVIDIA
          GeForce RTX 3080        Off | 00000000:01:00.0  On |                  N/A
          |<br>|  0%   45C    P8              19W / 350W |   1929MiB / 12288MiB |      6%      Default
          |<br>|                                         |                      |                  N/A
          |<br>+-----------------------------------------+----------------------+----------------------+</p>

          <p>2023-08-07 21:27:46 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...<br>2023-08-07
          21:27:46 INFO:The AutoGPTQ params are: {''model_basename'': ''Wizard-Vicuna-30B-Uncensored-GPTQ-4bit--1g.act.order'',
          ''device'': ''cuda:0'', ''use_triton'': False, ''inject_fused_attention'':
          False, ''inject_fused_mlp'': True, ''use_safetensors'': True, ''trust_remote_code'':
          False, ''max_memory'': {0: ''3100MiB'', ''cpu'': ''4000MiB''}, ''quantize_config'':
          None, ''use_cuda_fp16'': True}<br>2023-08-07 21:27:52 ERROR:Failed to load
          the model.</p>

          <p>anaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py",
          line 1218, in load_checkpoint_in_model<br>    raise ValueError(<br>ValueError:
          At least one of the model submodule will be offloaded to disk, please pass
          along an <code>offload_folder</code>.</p>

          '
        raw: "Hi all\r\n\r\nDespite even changing my ubuntu swap file file and setting\
          \ restrictions in the webui - I consistently get an error loading this model\
          \ in text-generation-webui.  \r\n\r\nRelevant info below - any ideas.  \
          \ Before I did the swap changes - it just died with a \"Killed\" message.\r\
          \n\r\n+---------------------------------------------------------------------------------------+\r\
          \n| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA\
          \ Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\
          \n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
          \ Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
          \ | GPU-Util  Compute M. |\r\n|                                        \
          \ |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\
          \n|   0  NVIDIA GeForce RTX 3080        Off | 00000000:01:00.0  On |   \
          \               N/A |\r\n|  0%   45C    P8              19W / 350W |   1929MiB\
          \ / 12288MiB |      6%      Default |\r\n|                             \
          \            |                      |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\
          \n\r\n\r\n\r\n\r\n2023-08-07 21:27:46 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...\r\
          \n2023-08-07 21:27:46 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit--1g.act.order',\
          \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': False,\
          \ 'inject_fused_mlp': True, 'use_safetensors': True, 'trust_remote_code':\
          \ False, 'max_memory': {0: '3100MiB', 'cpu': '4000MiB'}, 'quantize_config':\
          \ None, 'use_cuda_fp16': True}\r\n2023-08-07 21:27:52 ERROR:Failed to load\
          \ the model.\r\n\r\n\r\n\r\n\r\nanaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py\"\
          , line 1218, in load_checkpoint_in_model\r\n    raise ValueError(\r\nValueError:\
          \ At least one of the model submodule will be offloaded to disk, please\
          \ pass along an `offload_folder`.\r\n"
        updatedAt: '2023-08-07T19:39:07.419Z'
      numEdits: 0
      reactions: []
    id: 64d1485bd2a781d3f03e00b2
    type: comment
  author: nihponex
  content: "Hi all\r\n\r\nDespite even changing my ubuntu swap file file and setting\
    \ restrictions in the webui - I consistently get an error loading this model in\
    \ text-generation-webui.  \r\n\r\nRelevant info below - any ideas.   Before I\
    \ did the swap changes - it just died with a \"Killed\" message.\r\n\r\n+---------------------------------------------------------------------------------------+\r\
    \n| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version:\
    \ 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\
    \n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile\
    \ Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage\
    \ | GPU-Util  Compute M. |\r\n|                                         |    \
    \                  |               MIG M. |\r\n|=========================================+======================+======================|\r\
    \n|   0  NVIDIA GeForce RTX 3080        Off | 00000000:01:00.0  On |         \
    \         N/A |\r\n|  0%   45C    P8              19W / 350W |   1929MiB / 12288MiB\
    \ |      6%      Default |\r\n|                                         |    \
    \                  |                  N/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\
    \n\r\n\r\n\r\n\r\n2023-08-07 21:27:46 INFO:Loading TheBloke_Wizard-Vicuna-30B-Uncensored-GPTQ...\r\
    \n2023-08-07 21:27:46 INFO:The AutoGPTQ params are: {'model_basename': 'Wizard-Vicuna-30B-Uncensored-GPTQ-4bit--1g.act.order',\
    \ 'device': 'cuda:0', 'use_triton': False, 'inject_fused_attention': False, 'inject_fused_mlp':\
    \ True, 'use_safetensors': True, 'trust_remote_code': False, 'max_memory': {0:\
    \ '3100MiB', 'cpu': '4000MiB'}, 'quantize_config': None, 'use_cuda_fp16': True}\r\
    \n2023-08-07 21:27:52 ERROR:Failed to load the model.\r\n\r\n\r\n\r\n\r\nanaconda3/envs/textgen/lib/python3.10/site-packages/accelerate/utils/modeling.py\"\
    , line 1218, in load_checkpoint_in_model\r\n    raise ValueError(\r\nValueError:\
    \ At least one of the model submodule will be offloaded to disk, please pass along\
    \ an `offload_folder`.\r\n"
  created_at: 2023-08-07 18:39:07+00:00
  edited: false
  hidden: false
  id: 64d1485bd2a781d3f03e00b2
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: TheBloke/Luna-AI-Llama2-Uncensored-GPTQ
repo_type: model
status: open
target_branch: null
title: Problem loading model in oobabooga/text-generation-webui
