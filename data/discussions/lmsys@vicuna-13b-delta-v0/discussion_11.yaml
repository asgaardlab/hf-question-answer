!!python/object:huggingface_hub.community.DiscussionWithDetails
author: muruan
conflicting_files: null
created_at: 2023-08-18 10:26:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f692802631cfdbb73b1a7a3e9f59fc4.svg
      fullname: MaYing
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muruan
      type: user
    createdAt: '2023-08-18T11:26:50.000Z'
    data:
      edited: false
      editors:
      - muruan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4312417507171631
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f692802631cfdbb73b1a7a3e9f59fc4.svg
          fullname: MaYing
          isHf: false
          isPro: false
          name: muruan
          type: user
        html: "<p>I use vicuna-1.5 to finetune a text2SQL model. And I downloaded\
          \ the model files from <a href=\"https://huggingface.co/lmsys/vicuna-13b-v1.5\"\
          >https://huggingface.co/lmsys/vicuna-13b-v1.5</a>.<br>But when I started\
          \ to train the model, just found this error. Anyone can provide some help?\
          \ It confused me a week.</p>\n<p>===================================BUG\
          \ REPORT===================================<br>Welcome to bitsandbytes.\
          \ For bug reports, please submit your error trace to: <a rel=\"nofollow\"\
          \ href=\"https://github.com/TimDettmers/bitsandbytes/issues\">https://github.com/TimDettmers/bitsandbytes/issues</a><br>================================================================================<br>/home/yinma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/usr/local/cuda/extras/CUPTI/lib64'),\
          \ PosixPath('/usr/local/nvidia/lib')}<br>  warn(msg)<br>CUDA SETUP: CUDA\
          \ runtime path found: /usr/local/cuda/lib64/libcudart.so<br>CUDA SETUP:\
          \ Highest compute capability among GPUs detected: 8.0<br>CUDA SETUP: Detected\
          \ CUDA version 117<br>CUDA SETUP: Loading binary /home/yinma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...<br>[2023-08-18\
          \ 06:17:49,170] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend\
          \ in DeepSpeed not yet implemented<br>[2023-08-18 06:17:49,170] [INFO] [comm.py:616:init_distributed]\
          \ cdb=None<br>[2023-08-18 06:17:49,170] [INFO] [comm.py:643:init_distributed]\
          \ Initializing TorchBackend in DeepSpeed with backend nccl<br>[2023-08-18\
          \ 06:17:49,175] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend\
          \ in DeepSpeed not yet implemented<br>[2023-08-18 06:17:49,175] [INFO] [comm.py:616:init_distributed]\
          \ cdb=None<br>Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          | 3/3 [00:48&lt;00:00, 16.04s/it]<br>Traceback (most recent call last):<br>\
          \  File \"/data/slc-a100/notebooks/yinma/DSS-GPT-FW/./train/fastchat_train_lora.py\"\
          , line 440, in <br>    train()<br>  File \"/data/slc-a100/notebooks/yinma/DSS-GPT-FW/./train/fastchat_train_lora.py\"\
          , line 356, in train<br>    model = transformers.AutoModelForCausalLM.from_pretrained(<br>\
          \  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 493, in from_pretrained<br>    return model_class.from_pretrained(<br>\
          \  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2903, in from_pretrained<br>    ) = cls._load_pretrained_model(<br>\
          \  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 3310, in _load_pretrained_model<br>    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.<strong>class</strong>.<strong>name</strong>}:\\\
          n\\t{error_msg}\")<br>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:<br>\
          \        size mismatch for model.embed_tokens.weight: copying a param with\
          \ shape torch.Size([32000, 5120]) from checkpoint, the shape in current\
          \ model is torch.Size([32001, 5120]).<br>        size mismatch for lm_head.weight:\
          \ copying a param with shape torch.Size([32000, 5120]) from checkpoint,\
          \ the shape in current model is torch.Size([32001, 5120]).<br>        You\
          \ may consider adding <code>ignore_mismatched_sizes=True</code> in the model\
          \ <code>from_pretrained</code> method.<br>Loading checkpoint shards:  67%|\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u258E                     | 2/3 [00:47&lt;00:23,\
          \ 23.89s/it][2023-08-18 06:19:54,339] [INFO] [launch.py:315:sigkill_handler]\
          \ Killing subprocess 72313<br>[2023-08-18 06:19:54,339] [INFO] [launch.py:315:sigkill_handler]\
          \ Killing subprocess 72314<br>[2023-08-18 06:19:56,941] [ERROR] [launch.py:321:sigkill_handler]\
          \ ['/opt/conda/bin/python', '-u', './train/fastchat_train_lora.py', '--local_rank=1',\
          \ '--deepspeed', './dp_config_v3.json', '--model_name_or_path', '/data/slc-a100/data/yinma/model/vicuna-13b-v1.5',\
          \ '--lora_r', '8', '--lora_alpha', '16', '--lora_dropout', '0.05', '--data_path',\
          \ './data/sql_fintune_data.json', '--output_dir', './ddp_models_v1', '--num_train_epochs',\
          \ '1', '--fp16', 'True', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size',\
          \ '8', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'steps',\
          \ '--eval_steps', '100', '--save_strategy', 'steps', '--save_steps', '200',\
          \ '--save_total_limit', '3', '--learning_rate', '2e-5', '--weight_decay',\
          \ '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_strategy',\
          \ 'steps', '--logging_steps', '1', '--tf32', 'True', '--model_max_length',\
          \ '512', '--q_lora', 'False', '--gradient_checkpointing', 'True', '--is_instruction',\
          \ 'True'] exits with return code = 1</p>\n<p>My environment is :<br>fschat\
          \                    0.2.23<br>transformers              4.31.0</p>\n"
        raw: "I use vicuna-1.5 to finetune a text2SQL model. And I downloaded the\
          \ model files from https://huggingface.co/lmsys/vicuna-13b-v1.5.\r\nBut\
          \ when I started to train the model, just found this error. Anyone can provide\
          \ some help? It confused me a week.\r\n\r\n===================================BUG\
          \ REPORT===================================\r\nWelcome to bitsandbytes.\
          \ For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\r\
          \n================================================================================\r\
          \n/home/yinma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136:\
          \ UserWarning: WARNING: The following directories listed in your path were\
          \ found to be non-existent: {PosixPath('/usr/local/cuda/extras/CUPTI/lib64'),\
          \ PosixPath('/usr/local/nvidia/lib')}\r\n  warn(msg)\r\nCUDA SETUP: CUDA\
          \ runtime path found: /usr/local/cuda/lib64/libcudart.so\r\nCUDA SETUP:\
          \ Highest compute capability among GPUs detected: 8.0\r\nCUDA SETUP: Detected\
          \ CUDA version 117\r\nCUDA SETUP: Loading binary /home/yinma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\r\
          \n[2023-08-18 06:17:49,170] [WARNING] [comm.py:152:init_deepspeed_backend]\
          \ NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-18 06:17:49,170]\
          \ [INFO] [comm.py:616:init_distributed] cdb=None\r\n[2023-08-18 06:17:49,170]\
          \ [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed\
          \ with backend nccl\r\n[2023-08-18 06:17:49,175] [WARNING] [comm.py:152:init_deepspeed_backend]\
          \ NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-18 06:17:49,175]\
          \ [INFO] [comm.py:616:init_distributed] cdb=None\r\nLoading checkpoint shards:\
          \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:48<00:00, 16.04s/it]\r\nTraceback\
          \ (most recent call last):\r\n  File \"/data/slc-a100/notebooks/yinma/DSS-GPT-FW/./train/fastchat_train_lora.py\"\
          , line 440, in <module>\r\n    train()\r\n  File \"/data/slc-a100/notebooks/yinma/DSS-GPT-FW/./train/fastchat_train_lora.py\"\
          , line 356, in train\r\n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\
          \n  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 493, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 2903, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 3310, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\
          )\r\nRuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\r\
          \n        size mismatch for model.embed_tokens.weight: copying a param with\
          \ shape torch.Size([32000, 5120]) from checkpoint, the shape in current\
          \ model is torch.Size([32001, 5120]).\r\n        size mismatch for lm_head.weight:\
          \ copying a param with shape torch.Size([32000, 5120]) from checkpoint,\
          \ the shape in current model is torch.Size([32001, 5120]).\r\n        You\
          \ may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained`\
          \ method.\r\nLoading checkpoint shards:  67%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u258E                     | 2/3 [00:47<00:23, 23.89s/it][2023-08-18\
          \ 06:19:54,339] [INFO] [launch.py:315:sigkill_handler] Killing subprocess\
          \ 72313\r\n[2023-08-18 06:19:54,339] [INFO] [launch.py:315:sigkill_handler]\
          \ Killing subprocess 72314\r\n[2023-08-18 06:19:56,941] [ERROR] [launch.py:321:sigkill_handler]\
          \ ['/opt/conda/bin/python', '-u', './train/fastchat_train_lora.py', '--local_rank=1',\
          \ '--deepspeed', './dp_config_v3.json', '--model_name_or_path', '/data/slc-a100/data/yinma/model/vicuna-13b-v1.5',\
          \ '--lora_r', '8', '--lora_alpha', '16', '--lora_dropout', '0.05', '--data_path',\
          \ './data/sql_fintune_data.json', '--output_dir', './ddp_models_v1', '--num_train_epochs',\
          \ '1', '--fp16', 'True', '--per_device_train_batch_size', '8', '--per_device_eval_batch_size',\
          \ '8', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'steps',\
          \ '--eval_steps', '100', '--save_strategy', 'steps', '--save_steps', '200',\
          \ '--save_total_limit', '3', '--learning_rate', '2e-5', '--weight_decay',\
          \ '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_strategy',\
          \ 'steps', '--logging_steps', '1', '--tf32', 'True', '--model_max_length',\
          \ '512', '--q_lora', 'False', '--gradient_checkpointing', 'True', '--is_instruction',\
          \ 'True'] exits with return code = 1\r\n\r\nMy environment is :\r\nfschat\
          \                    0.2.23\r\ntransformers              4.31.0"
        updatedAt: '2023-08-18T11:26:50.742Z'
      numEdits: 0
      reactions: []
    id: 64df557a1961da51220f163d
    type: comment
  author: muruan
  content: "I use vicuna-1.5 to finetune a text2SQL model. And I downloaded the model\
    \ files from https://huggingface.co/lmsys/vicuna-13b-v1.5.\r\nBut when I started\
    \ to train the model, just found this error. Anyone can provide some help? It\
    \ confused me a week.\r\n\r\n===================================BUG REPORT===================================\r\
    \nWelcome to bitsandbytes. For bug reports, please submit your error trace to:\
    \ https://github.com/TimDettmers/bitsandbytes/issues\r\n================================================================================\r\
    \n/home/yinma/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136:\
    \ UserWarning: WARNING: The following directories listed in your path were found\
    \ to be non-existent: {PosixPath('/usr/local/cuda/extras/CUPTI/lib64'), PosixPath('/usr/local/nvidia/lib')}\r\
    \n  warn(msg)\r\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\r\
    \nCUDA SETUP: Highest compute capability among GPUs detected: 8.0\r\nCUDA SETUP:\
    \ Detected CUDA version 117\r\nCUDA SETUP: Loading binary /home/yinma/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\r\
    \n[2023-08-18 06:17:49,170] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL\
    \ backend in DeepSpeed not yet implemented\r\n[2023-08-18 06:17:49,170] [INFO]\
    \ [comm.py:616:init_distributed] cdb=None\r\n[2023-08-18 06:17:49,170] [INFO]\
    \ [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend\
    \ nccl\r\n[2023-08-18 06:17:49,175] [WARNING] [comm.py:152:init_deepspeed_backend]\
    \ NCCL backend in DeepSpeed not yet implemented\r\n[2023-08-18 06:17:49,175] [INFO]\
    \ [comm.py:616:init_distributed] cdb=None\r\nLoading checkpoint shards: 100%|\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3\
    \ [00:48<00:00, 16.04s/it]\r\nTraceback (most recent call last):\r\n  File \"\
    /data/slc-a100/notebooks/yinma/DSS-GPT-FW/./train/fastchat_train_lora.py\", line\
    \ 440, in <module>\r\n    train()\r\n  File \"/data/slc-a100/notebooks/yinma/DSS-GPT-FW/./train/fastchat_train_lora.py\"\
    , line 356, in train\r\n    model = transformers.AutoModelForCausalLM.from_pretrained(\r\
    \n  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 493, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \  File \"/home/yinma/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 2903, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File\
    \ \"/home/yinma/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 3310, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s) in\
    \ loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\r\nRuntimeError:\
    \ Error(s) in loading state_dict for LlamaForCausalLM:\r\n        size mismatch\
    \ for model.embed_tokens.weight: copying a param with shape torch.Size([32000,\
    \ 5120]) from checkpoint, the shape in current model is torch.Size([32001, 5120]).\r\
    \n        size mismatch for lm_head.weight: copying a param with shape torch.Size([32000,\
    \ 5120]) from checkpoint, the shape in current model is torch.Size([32001, 5120]).\r\
    \n        You may consider adding `ignore_mismatched_sizes=True` in the model\
    \ `from_pretrained` method.\r\nLoading checkpoint shards:  67%|\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u258E                     | 2/3 [00:47<00:23, 23.89s/it][2023-08-18 06:19:54,339]\
    \ [INFO] [launch.py:315:sigkill_handler] Killing subprocess 72313\r\n[2023-08-18\
    \ 06:19:54,339] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 72314\r\
    \n[2023-08-18 06:19:56,941] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python',\
    \ '-u', './train/fastchat_train_lora.py', '--local_rank=1', '--deepspeed', './dp_config_v3.json',\
    \ '--model_name_or_path', '/data/slc-a100/data/yinma/model/vicuna-13b-v1.5', '--lora_r',\
    \ '8', '--lora_alpha', '16', '--lora_dropout', '0.05', '--data_path', './data/sql_fintune_data.json',\
    \ '--output_dir', './ddp_models_v1', '--num_train_epochs', '1', '--fp16', 'True',\
    \ '--per_device_train_batch_size', '8', '--per_device_eval_batch_size', '8', '--gradient_accumulation_steps',\
    \ '1', '--evaluation_strategy', 'steps', '--eval_steps', '100', '--save_strategy',\
    \ 'steps', '--save_steps', '200', '--save_total_limit', '3', '--learning_rate',\
    \ '2e-5', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type',\
    \ 'cosine', '--logging_strategy', 'steps', '--logging_steps', '1', '--tf32', 'True',\
    \ '--model_max_length', '512', '--q_lora', 'False', '--gradient_checkpointing',\
    \ 'True', '--is_instruction', 'True'] exits with return code = 1\r\n\r\nMy environment\
    \ is :\r\nfschat                    0.2.23\r\ntransformers              4.31.0"
  created_at: 2023-08-18 10:26:50+00:00
  edited: false
  hidden: false
  id: 64df557a1961da51220f163d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8f692802631cfdbb73b1a7a3e9f59fc4.svg
      fullname: MaYing
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: muruan
      type: user
    createdAt: '2023-08-22T03:39:51.000Z'
    data:
      edited: false
      editors:
      - muruan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9142121076583862
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8f692802631cfdbb73b1a7a3e9f59fc4.svg
          fullname: MaYing
          isHf: false
          isPro: false
          name: muruan
          type: user
        html: '<p>I found the reason by myself. And add the comment to help the successors.<br>I
          finetuned the vicuna-1.0 model for my task, and then I forgot it and downloaded
          a new version of v1.5, then I fine-tuned it again from the old checkpoint,
          which leaded the error messege. Just finetuned it using a new output directory.
          That can resolved the problem.</p>

          '
        raw: 'I found the reason by myself. And add the comment to help the successors.

          I finetuned the vicuna-1.0 model for my task, and then I forgot it and downloaded
          a new version of v1.5, then I fine-tuned it again from the old checkpoint,
          which leaded the error messege. Just finetuned it using a new output directory.
          That can resolved the problem.'
        updatedAt: '2023-08-22T03:39:51.412Z'
      numEdits: 0
      reactions: []
    id: 64e42e078367d2da8bd275f4
    type: comment
  author: muruan
  content: 'I found the reason by myself. And add the comment to help the successors.

    I finetuned the vicuna-1.0 model for my task, and then I forgot it and downloaded
    a new version of v1.5, then I fine-tuned it again from the old checkpoint, which
    leaded the error messege. Just finetuned it using a new output directory. That
    can resolved the problem.'
  created_at: 2023-08-22 02:39:51+00:00
  edited: false
  hidden: false
  id: 64e42e078367d2da8bd275f4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: lmsys/vicuna-13b-delta-v0
repo_type: model
status: open
target_branch: null
title: 'size mismatch for lm_head.weight: copying a param with shape torch.Size([32000,
  5120]) from checkpoint, the shape in current model is torch.Size([32001, 5120]).'
