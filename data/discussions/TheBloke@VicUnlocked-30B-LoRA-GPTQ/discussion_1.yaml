!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kcramp858
conflicting_files: null
created_at: 2023-05-17 14:33:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
      fullname: k cramp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kcramp858
      type: user
    createdAt: '2023-05-17T15:33:34.000Z'
    data:
      edited: false
      editors:
      - kcramp858
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
          fullname: k cramp
          isHf: false
          isPro: false
          name: kcramp858
          type: user
        html: '<p>I have CUDA 11.8 and it currently works with ooba textgen with the
          llama-30b-supercot-4g-128, but when I run this, I get the following error:</p>

          <p>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:<br>size
          mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with
          shape torch.Size([1, 832]) from checkpoint, the shape in current model is
          torch.Size([52, 832]).<br>size mismatch for model.layers.0.self_attn.k_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is torch.Size([52, 6656]).<br>size mismatch for model.layers.0.self_attn.o_proj.qzeros:
          copying a param with shape torch.Size([1, 832]) from checkpoint, the shape
          in current model is torch.Size([52, 832]).<br>size mismatch for model.layers.0.self_attn.o_proj.scales:
          copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape
          in current model is </p>

          <p>[truncated for visiblity]</p>

          <p>I believe it is because my GPTQ version does not match.</p>

          <p>How do we determine which GPTQ version to use?</p>

          '
        raw: "I have CUDA 11.8 and it currently works with ooba textgen with the llama-30b-supercot-4g-128,\
          \ but when I run this, I get the following error:\r\n\r\nRuntimeError: Error(s)\
          \ in loading state_dict for LlamaForCausalLM:\r\nsize mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]).\r\nsize mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]).\r\nsize mismatch for\
          \ model.layers.0.self_attn.o_proj.qzeros: copying a param with shape torch.Size([1,\
          \ 832]) from checkpoint, the shape in current model is torch.Size([52, 832]).\r\
          \nsize mismatch for model.layers.0.self_attn.o_proj.scales: copying a param\
          \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current\
          \ model is \r\n\r\n[truncated for visiblity]\r\n\r\nI believe it is because\
          \ my GPTQ version does not match.\r\n\r\nHow do we determine which GPTQ\
          \ version to use?"
        updatedAt: '2023-05-17T15:33:34.448Z'
      numEdits: 0
      reactions: []
    id: 6464f3ce200b583e1e504d72
    type: comment
  author: kcramp858
  content: "I have CUDA 11.8 and it currently works with ooba textgen with the llama-30b-supercot-4g-128,\
    \ but when I run this, I get the following error:\r\n\r\nRuntimeError: Error(s)\
    \ in loading state_dict for LlamaForCausalLM:\r\nsize mismatch for model.layers.0.self_attn.k_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\r\nsize mismatch for model.layers.0.self_attn.k_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]).\r\nsize mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]).\r\nsize mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is \r\n\r\n[truncated for visiblity]\r\n\r\nI believe it is\
    \ because my GPTQ version does not match.\r\n\r\nHow do we determine which GPTQ\
    \ version to use?"
  created_at: 2023-05-17 14:33:34+00:00
  edited: false
  hidden: false
  id: 6464f3ce200b583e1e504d72
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/561c603668e14d92c4d6577e11bcaa64.svg
      fullname: Mike Hearn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mikehearn
      type: user
    createdAt: '2023-05-17T15:55:52.000Z'
    data:
      edited: false
      editors:
      - mikehearn
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/561c603668e14d92c4d6577e11bcaa64.svg
          fullname: Mike Hearn
          isHf: false
          isPro: false
          name: mikehearn
          type: user
        html: '<p>For what it''s worth, I initially got this error too and I had to
          edit my config-user.yaml file to change <code>groupsize</code> to <code>None</code>
          under the <code>VicUnlocked-30B-LoRA-GPTQ</code> section.</p>

          '
        raw: For what it's worth, I initially got this error too and I had to edit
          my config-user.yaml file to change `groupsize` to `None` under the `VicUnlocked-30B-LoRA-GPTQ`
          section.
        updatedAt: '2023-05-17T15:55:52.064Z'
      numEdits: 0
      reactions: []
    id: 6464f908e8e31202cb4dbd1c
    type: comment
  author: mikehearn
  content: For what it's worth, I initially got this error too and I had to edit my
    config-user.yaml file to change `groupsize` to `None` under the `VicUnlocked-30B-LoRA-GPTQ`
    section.
  created_at: 2023-05-17 14:55:52+00:00
  edited: false
  hidden: false
  id: 6464f908e8e31202cb4dbd1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
      fullname: k cramp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kcramp858
      type: user
    createdAt: '2023-05-17T17:18:23.000Z'
    data:
      edited: false
      editors:
      - kcramp858
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
          fullname: k cramp
          isHf: false
          isPro: false
          name: kcramp858
          type: user
        html: '<p>.. that solved my problem, I am just an idiot. Set groupsize to
          none. </p>

          '
        raw: '.. that solved my problem, I am just an idiot. Set groupsize to none. '
        updatedAt: '2023-05-17T17:18:23.150Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64650c5f86e668ad22e49719
    id: 64650c5f86e668ad22e49718
    type: comment
  author: kcramp858
  content: '.. that solved my problem, I am just an idiot. Set groupsize to none. '
  created_at: 2023-05-17 16:18:23+00:00
  edited: false
  hidden: false
  id: 64650c5f86e668ad22e49718
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/9f81b48bed7cca764eca75c38d601468.svg
      fullname: k cramp
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kcramp858
      type: user
    createdAt: '2023-05-17T17:18:23.000Z'
    data:
      status: closed
    id: 64650c5f86e668ad22e49719
    type: status-change
  author: kcramp858
  created_at: 2023-05-17 16:18:23+00:00
  id: 64650c5f86e668ad22e49719
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/VicUnlocked-30B-LoRA-GPTQ
repo_type: model
status: closed
target_branch: null
title: Error when using this..
