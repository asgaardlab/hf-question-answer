!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kevalmorabia97
conflicting_files: null
created_at: 2023-06-01 18:08:26+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6478ebdb5fa05d12dc32afb9/9Dai4BcbrCyCkOb9o_W9S.jpeg?w=200&h=200&f=face
      fullname: Keval Morabia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kevalmorabia97
      type: user
    createdAt: '2023-06-01T19:08:26.000Z'
    data:
      edited: false
      editors:
      - kevalmorabia97
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6478ebdb5fa05d12dc32afb9/9Dai4BcbrCyCkOb9o_W9S.jpeg?w=200&h=200&f=face
          fullname: Keval Morabia
          isHf: false
          isPro: false
          name: kevalmorabia97
          type: user
        html: '<p>I would like to reproduce the bert-large-uncased-whole-word-masking
          model provided by huggingface. Could you please share more details on the
          experimental setup?</p>

          <ul>

          <li>Was this model trained from scratch or a fine-tuned version of bert-large-uncased
          but with wwm? </li>

          <li>How many epochs / steps, learning rate, batch size, number of gpus,
          etc.?</li>

          <li>There is this <a rel="nofollow" href="https://github.com/huggingface/transformers/tree/main/examples/research_projects/mlm_wwm">reference
          script</a> but the example command uses wikitext dataset while bert was
          pre-trained on book corpus and english Wikipedia so I''m not sure how to
          reproduce these results.</li>

          </ul>

          <p>Thank you :)</p>

          '
        raw: "I would like to reproduce the bert-large-uncased-whole-word-masking\
          \ model provided by huggingface. Could you please share more details on\
          \ the experimental setup?\r\n* Was this model trained from scratch or a\
          \ fine-tuned version of bert-large-uncased but with wwm? \r\n* How many\
          \ epochs / steps, learning rate, batch size, number of gpus, etc.?\r\n*\
          \ There is this [reference script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/mlm_wwm)\
          \ but the example command uses wikitext dataset while bert was pre-trained\
          \ on book corpus and english Wikipedia so I'm not sure how to reproduce\
          \ these results.\r\n\r\nThank you :)"
        updatedAt: '2023-06-01T19:08:26.546Z'
      numEdits: 0
      reactions: []
    id: 6478ecaa50ff700163172c69
    type: comment
  author: kevalmorabia97
  content: "I would like to reproduce the bert-large-uncased-whole-word-masking model\
    \ provided by huggingface. Could you please share more details on the experimental\
    \ setup?\r\n* Was this model trained from scratch or a fine-tuned version of bert-large-uncased\
    \ but with wwm? \r\n* How many epochs / steps, learning rate, batch size, number\
    \ of gpus, etc.?\r\n* There is this [reference script](https://github.com/huggingface/transformers/tree/main/examples/research_projects/mlm_wwm)\
    \ but the example command uses wikitext dataset while bert was pre-trained on\
    \ book corpus and english Wikipedia so I'm not sure how to reproduce these results.\r\
    \n\r\nThank you :)"
  created_at: 2023-06-01 18:08:26+00:00
  edited: false
  hidden: false
  id: 6478ecaa50ff700163172c69
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6478ebdb5fa05d12dc32afb9/9Dai4BcbrCyCkOb9o_W9S.jpeg?w=200&h=200&f=face
      fullname: Keval Morabia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kevalmorabia97
      type: user
    createdAt: '2023-06-23T19:25:12.000Z'
    data:
      edited: false
      editors:
      - kevalmorabia97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9827295541763306
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6478ebdb5fa05d12dc32afb9/9Dai4BcbrCyCkOb9o_W9S.jpeg?w=200&h=200&f=face
          fullname: Keval Morabia
          isHf: false
          isPro: false
          name: kevalmorabia97
          type: user
        html: '<p>Any follow-up would be greatly appreciated!</p>

          '
        raw: Any follow-up would be greatly appreciated!
        updatedAt: '2023-06-23T19:25:12.142Z'
      numEdits: 0
      reactions: []
    id: 6495f19828a5c2a030e0945a
    type: comment
  author: kevalmorabia97
  content: Any follow-up would be greatly appreciated!
  created_at: 2023-06-23 18:25:12+00:00
  edited: false
  hidden: false
  id: 6495f19828a5c2a030e0945a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: bert-large-uncased-whole-word-masking
repo_type: model
status: open
target_branch: null
title: Reproduce pre-training results
