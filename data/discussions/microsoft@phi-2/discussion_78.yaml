!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kirilligum
conflicting_files: null
created_at: 2024-01-11 22:42:10+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0abd6820dae000dd378296ed27aaebde.svg
      fullname: Kirill Igumenshchev
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kirilligum
      type: user
    createdAt: '2024-01-11T22:42:10.000Z'
    data:
      edited: false
      editors:
      - kirilligum
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3694031536579132
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0abd6820dae000dd378296ed27aaebde.svg
          fullname: Kirill Igumenshchev
          isHf: false
          isPro: false
          name: kirilligum
          type: user
        html: "<p>phi-2 is great to run on cpu due to its small size. there aren't\
          \ that many other options.</p>\n<p>can you put the CPU support back? it\
          \ worked well for me.</p>\n<p>with the removal in commit <a href=\"/microsoft/phi-2/commit/cb2f4533604d8b67de604e7df03bfe6f3ca22869\"\
          >cb2f4533604d8b67de604e7df03bfe6f3ca22869</a></p>\n<p>i get an error: </p>\n\
          <pre><code>(.venv) ubuntu@ip-172-31-7-92 ~/t/phi-2 (main)&gt; cat test.py\n\
          import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \ntorch.set_default_device(\"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_cod\n\
          e=True)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\"\
          , trust_remote_code=True)\n\ninputs = tokenizer('''Instruct: what is LLM?\n\
          \                   Output:''', return_tensors=\"pt\", return_attention_mask=False)\n\
          \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
          print(text)\n\n(.venv) ubuntu@ip-172-31-7-92 ~/t/phi-2 (main)&gt; python\
          \ test.py\nTraceback (most recent call last):\n  File \"/home/ubuntu/tmp/phi-2/test.py\"\
          , line 6, in &lt;module&gt;\n    model = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote\n\
          _code=True)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          ^^^^^^^^^^^\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 553, in fro\nm_pretrained\n    model_class = get_class_from_dynamic_module(\n\
          \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_cla\nss_from_dynamic_module\n    final_module = get_cached_module_file(\n\
          \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 315, in get_cac\nhed_module_file\n    modules_needed = check_imports(resolved_module_file)\n\
          \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 180, in check_i\nmports\n    raise ImportError(\nImportError: This\
          \ modeling file requires the following packages that were not found in your\
          \ environment: flash_attn. Run `pip\n install flash_attn`\n</code></pre>\n"
        raw: "phi-2 is great to run on cpu due to its small size. there aren't that\
          \ many other options.\r\n\r\ncan you put the CPU support back? it worked\
          \ well for me.\r\n\r\nwith the removal in commit cb2f4533604d8b67de604e7df03bfe6f3ca22869\r\
          \n\r\ni get an error: \r\n```\r\n(.venv) ubuntu@ip-172-31-7-92 ~/t/phi-2\
          \ (main)> cat test.py\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\r\n\r\ntorch.set_default_device(\"cpu\")\r\n\r\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.float32,\
          \ device_map=\"cpu\", trust_remote_cod\r\ne=True)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-2\", trust_remote_code=True)\r\n\r\ninputs = tokenizer('''Instruct:\
          \ what is LLM?\r\n                   Output:''', return_tensors=\"pt\",\
          \ return_attention_mask=False)\r\n\r\noutputs = model.generate(**inputs,\
          \ max_length=200)\r\ntext = tokenizer.batch_decode(outputs)[0]\r\nprint(text)\r\
          \n\r\n(.venv) ubuntu@ip-172-31-7-92 ~/t/phi-2 (main)> python test.py\r\n\
          Traceback (most recent call last):\r\n  File \"/home/ubuntu/tmp/phi-2/test.py\"\
          , line 6, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote\r\
          \n_code=True)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 553, in fro\r\nm_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
          \n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_cla\r\nss_from_dynamic_module\r\n    final_module = get_cached_module_file(\r\
          \n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 315, in get_cac\r\nhed_module_file\r\n    modules_needed = check_imports(resolved_module_file)\r\
          \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 180, in check_i\r\nmports\r\n    raise ImportError(\r\nImportError:\
          \ This modeling file requires the following packages that were not found\
          \ in your environment: flash_attn. Run `pip\r\n install flash_attn`\r\n\
          ```"
        updatedAt: '2024-01-11T22:42:10.357Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - praveeny
    id: 65a06ec2e969415381e31bd7
    type: comment
  author: kirilligum
  content: "phi-2 is great to run on cpu due to its small size. there aren't that\
    \ many other options.\r\n\r\ncan you put the CPU support back? it worked well\
    \ for me.\r\n\r\nwith the removal in commit cb2f4533604d8b67de604e7df03bfe6f3ca22869\r\
    \n\r\ni get an error: \r\n```\r\n(.venv) ubuntu@ip-172-31-7-92 ~/t/phi-2 (main)>\
    \ cat test.py\r\nimport torch\r\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\r\n\r\ntorch.set_default_device(\"cpu\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-2\", torch_dtype=torch.float32, device_map=\"cpu\", trust_remote_cod\r\
    \ne=True)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\"\
    , trust_remote_code=True)\r\n\r\ninputs = tokenizer('''Instruct: what is LLM?\r\
    \n                   Output:''', return_tensors=\"pt\", return_attention_mask=False)\r\
    \n\r\noutputs = model.generate(**inputs, max_length=200)\r\ntext = tokenizer.batch_decode(outputs)[0]\r\
    \nprint(text)\r\n\r\n(.venv) ubuntu@ip-172-31-7-92 ~/t/phi-2 (main)> python test.py\r\
    \nTraceback (most recent call last):\r\n  File \"/home/ubuntu/tmp/phi-2/test.py\"\
    , line 6, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\"\
    , torch_dtype=torch.float32, device_map=\"cpu\", trust_remote\r\n_code=True)\r\
    \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 553, in fro\r\nm_pretrained\r\n    model_class = get_class_from_dynamic_module(\r\
    \n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 488, in get_cla\r\nss_from_dynamic_module\r\n    final_module = get_cached_module_file(\r\
    \n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 315, in get_cac\r\nhed_module_file\r\n    modules_needed = check_imports(resolved_module_file)\r\
    \n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/ubuntu/tmp/phi-2/.venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 180, in check_i\r\nmports\r\n    raise ImportError(\r\nImportError: This\
    \ modeling file requires the following packages that were not found in your environment:\
    \ flash_attn. Run `pip\r\n install flash_attn`\r\n```"
  created_at: 2024-01-11 22:42:10+00:00
  edited: false
  hidden: false
  id: 65a06ec2e969415381e31bd7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T00:31:46.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8178956508636475
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;kirilligum&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kirilligum\"\
          >@<span class=\"underline\">kirilligum</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>We deployed a fix and it should be working now.</p>\n<p>The issue was\
          \ caused by the combination of using dynamic modules and remote code loading\
          \ in <code>transformers</code>.</p>\n<p>Regards,<br>Gustavo.</p>\n"
        raw: 'Hello @kirilligum!


          We deployed a fix and it should be working now.


          The issue was caused by the combination of using dynamic modules and remote
          code loading in `transformers`.


          Regards,

          Gustavo.'
        updatedAt: '2024-01-12T00:45:49.905Z'
      numEdits: 1
      reactions: []
    id: 65a08872b90a1eb50fd1d9d0
    type: comment
  author: gugarosa
  content: 'Hello @kirilligum!


    We deployed a fix and it should be working now.


    The issue was caused by the combination of using dynamic modules and remote code
    loading in `transformers`.


    Regards,

    Gustavo.'
  created_at: 2024-01-12 00:31:46+00:00
  edited: true
  hidden: false
  id: 65a08872b90a1eb50fd1d9d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T13:07:30.000Z'
    data:
      status: closed
    id: 65a139925fafc248c24dc2d5
    type: status-change
  author: gugarosa
  created_at: 2024-01-12 13:07:30+00:00
  id: 65a139925fafc248c24dc2d5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 78
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: any reason to remove cpu support?
