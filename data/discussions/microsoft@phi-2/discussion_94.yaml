!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fahadh4ilyas
conflicting_files: null
created_at: 2024-01-22 10:27:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2024-01-22T10:27:13.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9379722476005554
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>Usually, <code>hidden_states</code> from <code>self_attn</code>
          will become input into <code>mlp</code>. But, from <code>modeling_phi.py</code>,
          it seems that <code>hidden_states</code> after <code>input_norm</code> is
          becoming input both for <code>self_attn</code> and <code>mlp</code> and
          then added at the end. What kind of transformers implementation is that?</p>

          '
        raw: Usually, `hidden_states` from `self_attn` will become input into `mlp`.
          But, from `modeling_phi.py`, it seems that `hidden_states` after `input_norm`
          is becoming input both for `self_attn` and `mlp` and then added at the end.
          What kind of transformers implementation is that?
        updatedAt: '2024-01-22T10:27:13.164Z'
      numEdits: 0
      reactions: []
    id: 65ae4301d9210fe291085337
    type: comment
  author: fahadh4ilyas
  content: Usually, `hidden_states` from `self_attn` will become input into `mlp`.
    But, from `modeling_phi.py`, it seems that `hidden_states` after `input_norm`
    is becoming input both for `self_attn` and `mlp` and then added at the end. What
    kind of transformers implementation is that?
  created_at: 2024-01-22 10:27:13+00:00
  edited: false
  hidden: false
  id: 65ae4301d9210fe291085337
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-22T17:55:31.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7062157988548279
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;fahadh4ilyas&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/fahadh4ilyas\"\
          >@<span class=\"underline\">fahadh4ilyas</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Attention can also be applied in parallel (instead of sequential, e.g.,\
          \ GPT, Lllama) within regard to the MLP.</p>\n<p>Please check GPT-J/CodeGen's\
          \ implementation: <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L311\"\
          >https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L311</a>.</p>\n"
        raw: 'Hello @fahadh4ilyas!


          Attention can also be applied in parallel (instead of sequential, e.g.,
          GPT, Lllama) within regard to the MLP.


          Please check GPT-J/CodeGen''s implementation: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L311.'
        updatedAt: '2024-01-22T17:55:31.718Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65aeac131edab235a1a41bb7
    id: 65aeac131edab235a1a41bb6
    type: comment
  author: gugarosa
  content: 'Hello @fahadh4ilyas!


    Attention can also be applied in parallel (instead of sequential, e.g., GPT, Lllama)
    within regard to the MLP.


    Please check GPT-J/CodeGen''s implementation: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gptj/modeling_gptj.py#L311.'
  created_at: 2024-01-22 17:55:31+00:00
  edited: false
  hidden: false
  id: 65aeac131edab235a1a41bb6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-22T17:55:31.000Z'
    data:
      status: closed
    id: 65aeac131edab235a1a41bb7
    type: status-change
  author: gugarosa
  created_at: 2024-01-22 17:55:31+00:00
  id: 65aeac131edab235a1a41bb7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 94
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Why inside `modeling_phi.py`, the output from Self Attention is not becoming
  the input of MLP?
