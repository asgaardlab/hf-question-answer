!!python/object:huggingface_hub.community.DiscussionWithDetails
author: XibinBayesZhou
conflicting_files: null
created_at: 2023-12-15 17:05:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ac05-lWQnyeJwGormva_6.jpeg?w=200&h=200&f=face
      fullname: Xibin Bayes Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XibinBayesZhou
      type: user
    createdAt: '2023-12-15T17:05:29.000Z'
    data:
      edited: false
      editors:
      - XibinBayesZhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7443532943725586
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ac05-lWQnyeJwGormva_6.jpeg?w=200&h=200&f=face
          fullname: Xibin Bayes Zhou
          isHf: false
          isPro: false
          name: XibinBayesZhou
          type: user
        html: '<p>Hi there,<br>I''m using your model and trying to decode the output
          of llm by provided tokenizer. It seems that the output of llm might output
          <code>token_id</code> that is larger than the <code>tokenizer.vocab_size</code>,
          which cause <code>tokenizer.decode</code> error.</p>

          <p>After checking the <code>vocab_size</code> in both <code>model</code>
          and <code>tokenizer</code>, it is the difference of <code>vocab_size</code>
          between configs in <code>config.json</code> (51200 from <a href="https://huggingface.co/microsoft/phi-2/blob/d3186761bf5c4409f7679359284066c25ab668ee/config.json#L31">this
          line</a>) and <code>tokenizer_config.json</code>(at most 50295 from <a href="https://huggingface.co/microsoft/phi-2/blob/d3186761bf5c4409f7679359284066c25ab668ee/tokenizer_config.json#L308">this
          line</a>).</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/643bb8ba6eeb746f5ad0a2db/WFOZ1Vkb4zg3WdCR3HvKK.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/643bb8ba6eeb746f5ad0a2db/WFOZ1Vkb4zg3WdCR3HvKK.png"></a><br>The
          difference in <code>vocab_size</code> while debuging also indicate this
          issue.</p>

          <p>How to solve this? Is this avoidable by set some arguments or should
          I modify config file?</p>

          <p>Thank you for your time for reading. Looking forward to your advices!</p>

          '
        raw: "Hi there,\r\nI'm using your model and trying to decode the output of\
          \ llm by provided tokenizer. It seems that the output of llm might output\
          \ `token_id` that is larger than the `tokenizer.vocab_size`, which cause\
          \ `tokenizer.decode` error.\r\n\r\nAfter checking the `vocab_size` in both\
          \ `model` and `tokenizer`, it is the difference of `vocab_size` between\
          \ configs in `config.json` (51200 from [this line](https://huggingface.co/microsoft/phi-2/blob/d3186761bf5c4409f7679359284066c25ab668ee/config.json#L31))\
          \ and `tokenizer_config.json`(at most 50295 from [this line](https://huggingface.co/microsoft/phi-2/blob/d3186761bf5c4409f7679359284066c25ab668ee/tokenizer_config.json#L308)).\r\
          \n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643bb8ba6eeb746f5ad0a2db/WFOZ1Vkb4zg3WdCR3HvKK.png)\r\
          \nThe difference in `vocab_size` while debuging also indicate this issue.\r\
          \n\r\nHow to solve this? Is this avoidable by set some arguments or should\
          \ I modify config file?\r\n\r\nThank you for your time for reading. Looking\
          \ forward to your advices!"
        updatedAt: '2023-12-15T17:05:29.784Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - wassname
    id: 657c8759b12a3c8df331e28b
    type: comment
  author: XibinBayesZhou
  content: "Hi there,\r\nI'm using your model and trying to decode the output of llm\
    \ by provided tokenizer. It seems that the output of llm might output `token_id`\
    \ that is larger than the `tokenizer.vocab_size`, which cause `tokenizer.decode`\
    \ error.\r\n\r\nAfter checking the `vocab_size` in both `model` and `tokenizer`,\
    \ it is the difference of `vocab_size` between configs in `config.json` (51200\
    \ from [this line](https://huggingface.co/microsoft/phi-2/blob/d3186761bf5c4409f7679359284066c25ab668ee/config.json#L31))\
    \ and `tokenizer_config.json`(at most 50295 from [this line](https://huggingface.co/microsoft/phi-2/blob/d3186761bf5c4409f7679359284066c25ab668ee/tokenizer_config.json#L308)).\r\
    \n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/643bb8ba6eeb746f5ad0a2db/WFOZ1Vkb4zg3WdCR3HvKK.png)\r\
    \nThe difference in `vocab_size` while debuging also indicate this issue.\r\n\r\
    \nHow to solve this? Is this avoidable by set some arguments or should I modify\
    \ config file?\r\n\r\nThank you for your time for reading. Looking forward to\
    \ your advices!"
  created_at: 2023-12-15 17:05:29+00:00
  edited: false
  hidden: false
  id: 657c8759b12a3c8df331e28b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-12-16T00:21:20.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9695821404457092
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: '<p>This is curious as it cann''t be explained with added tokens. The
          base CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in the
          model config is outdated.</p>

          <p>It''s present on the azure repo, for the latest v2, as well.</p>

          '
        raw: 'This is curious as it cann''t be explained with added tokens. The base
          CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in the model
          config is outdated.


          It''s present on the azure repo, for the latest v2, as well.'
        updatedAt: '2023-12-16T00:21:20.854Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - XibinBayesZhou
    id: 657ced801ede8a7bb795f7fb
    type: comment
  author: wassname
  content: 'This is curious as it cann''t be explained with added tokens. The base
    CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in the model config
    is outdated.


    It''s present on the azure repo, for the latest v2, as well.'
  created_at: 2023-12-16 00:21:20+00:00
  edited: false
  hidden: false
  id: 657ced801ede8a7bb795f7fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ac05-lWQnyeJwGormva_6.jpeg?w=200&h=200&f=face
      fullname: Xibin Bayes Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: XibinBayesZhou
      type: user
    createdAt: '2023-12-16T05:44:16.000Z'
    data:
      edited: false
      editors:
      - XibinBayesZhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9550561308860779
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/ac05-lWQnyeJwGormva_6.jpeg?w=200&h=200&f=face
          fullname: Xibin Bayes Zhou
          isHf: false
          isPro: false
          name: XibinBayesZhou
          type: user
        html: "<blockquote>\n<p>This is curious as it cann't be explained with added\
          \ tokens. The base CodeGenTokenizer has more than 51200 tokens. Perhaps\
          \ the 51200 in the model config is outdated.</p>\n<p>It's present on the\
          \ azure repo, for the latest v2, as well.</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;wassname&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/wassname\">@<span class=\"underline\">wassname</span></a></span>\n\
          \n\t</span></span> Thank you for your information. Do you mean there's a\
          \ repo that point this issue out? Could you give me a link related to that?\
          \ Thank you very much!</p>\n"
        raw: "> This is curious as it cann't be explained with added tokens. The base\
          \ CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in the\
          \ model config is outdated.\n> \n> It's present on the azure repo, for the\
          \ latest v2, as well.\n\n@wassname Thank you for your information. Do you\
          \ mean there's a repo that point this issue out? Could you give me a link\
          \ related to that? Thank you very much!"
        updatedAt: '2023-12-16T05:44:16.558Z'
      numEdits: 0
      reactions: []
    id: 657d3930869d5bb0e54d876a
    type: comment
  author: XibinBayesZhou
  content: "> This is curious as it cann't be explained with added tokens. The base\
    \ CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in the model\
    \ config is outdated.\n> \n> It's present on the azure repo, for the latest v2,\
    \ as well.\n\n@wassname Thank you for your information. Do you mean there's a\
    \ repo that point this issue out? Could you give me a link related to that? Thank\
    \ you very much!"
  created_at: 2023-12-16 05:44:16+00:00
  edited: false
  hidden: false
  id: 657d3930869d5bb0e54d876a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a3b1e0275825d2c9ae5420/CRSUAxwmfr03RG19x5pWp.jpeg?w=200&h=200&f=face
      fullname: Deepakvictor
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Deepakvictor
      type: user
    createdAt: '2023-12-16T15:47:45.000Z'
    data:
      edited: false
      editors:
      - Deepakvictor
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32561901211738586
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64a3b1e0275825d2c9ae5420/CRSUAxwmfr03RG19x5pWp.jpeg?w=200&h=200&f=face
          fullname: Deepakvictor
          isHf: false
          isPro: false
          name: Deepakvictor
          type: user
        html: '<pre><code class="language-python"><span class="hljs-keyword">from</span>
          transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM,
          AutoTokenizer

          tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"microsoft/phi-2"</span>,)

          tokenizer.add_tokens([<span class="hljs-string">f''&lt;SPL_<span class="hljs-subst">{i}</span>''</span>
          <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span>
          <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span
          class="hljs-number">943</span>)])  <span class="hljs-comment"># returns
          943 </span>

          </code></pre>

          <p>adding new tokens and using the tokenizer can avoid the error</p>

          '
        raw: "``` python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\",)\ntokenizer.add_tokens([f'<SPL_{i}'\
          \ for i in range(0,943)])  # returns 943 \n```\nadding new tokens and using\
          \ the tokenizer can avoid the error"
        updatedAt: '2023-12-16T15:47:45.361Z'
      numEdits: 0
      reactions: []
    id: 657dc6a11815b29c9ac447c8
    type: comment
  author: Deepakvictor
  content: "``` python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\",)\ntokenizer.add_tokens([f'<SPL_{i}'\
    \ for i in range(0,943)])  # returns 943 \n```\nadding new tokens and using the\
    \ tokenizer can avoid the error"
  created_at: 2023-12-16 15:47:45+00:00
  edited: false
  hidden: false
  id: 657dc6a11815b29c9ac447c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
      fullname: wassname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wassname
      type: user
    createdAt: '2023-12-17T10:06:10.000Z'
    data:
      edited: false
      editors:
      - wassname
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9561125636100769
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7e313aebaddb3ec752ec48b619464f2c.svg
          fullname: wassname
          isHf: false
          isPro: false
          name: wassname
          type: user
        html: "<blockquote>\n<blockquote>\n<p>This is curious as it cann't be explained\
          \ with added tokens. The base CodeGenTokenizer has more than 51200 tokens.\
          \ Perhaps the 51200 in the model config is outdated.</p>\n<p>It's present\
          \ on the azure repo, for the latest v2, as well.</p>\n</blockquote>\n<p><span\
          \ data-props=\"{&quot;user&quot;:&quot;wassname&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wassname\">@<span class=\"\
          underline\">wassname</span></a></span>\n\n\t</span></span> Thank you for\
          \ your information. Do you mean there's a repo that point this issue out?\
          \ Could you give me a link related to that? Thank you very much!</p>\n</blockquote>\n\
          <p>Oh there is the huggingface repo, and the azure one. But they both have\
          \ the same discrepancy. </p>\n"
        raw: "> > This is curious as it cann't be explained with added tokens. The\
          \ base CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in\
          \ the model config is outdated.\n> > \n> > It's present on the azure repo,\
          \ for the latest v2, as well.\n> \n> @wassname Thank you for your information.\
          \ Do you mean there's a repo that point this issue out? Could you give me\
          \ a link related to that? Thank you very much!\n\nOh there is the huggingface\
          \ repo, and the azure one. But they both have the same discrepancy. "
        updatedAt: '2023-12-17T10:06:10.443Z'
      numEdits: 0
      reactions: []
    id: 657ec812c7a58172f5139a3e
    type: comment
  author: wassname
  content: "> > This is curious as it cann't be explained with added tokens. The base\
    \ CodeGenTokenizer has more than 51200 tokens. Perhaps the 51200 in the model\
    \ config is outdated.\n> > \n> > It's present on the azure repo, for the latest\
    \ v2, as well.\n> \n> @wassname Thank you for your information. Do you mean there's\
    \ a repo that point this issue out? Could you give me a link related to that?\
    \ Thank you very much!\n\nOh there is the huggingface repo, and the azure one.\
    \ But they both have the same discrepancy. "
  created_at: 2023-12-17 10:06:10+00:00
  edited: false
  hidden: false
  id: 657ec812c7a58172f5139a3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-09T18:08:41.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9597978591918945
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Could you please provide the script which is generating those identifiers?</p>\n\
          <p>We ended up setting 51200 as the vocabulary size just to accommodate\
          \ any new tokens that we might need in the future. You can follow <span\
          \ data-props=\"{&quot;user&quot;:&quot;Deepakvictor&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Deepakvictor\">@<span\
          \ class=\"underline\">Deepakvictor</span></a></span>\n\n\t</span></span>\
          \ answer and it should fix the issue.</p>\n<p>As far as I know, no tokens\
          \ from 50295+ should be generated because those embeddings were not trained.\
          \ Though, depending on the generation's parameters, they could appear (low\
          \ probabilities however).</p>\n"
        raw: 'Could you please provide the script which is generating those identifiers?


          We ended up setting 51200 as the vocabulary size just to accommodate any
          new tokens that we might need in the future. You can follow @Deepakvictor
          answer and it should fix the issue.


          As far as I know, no tokens from 50295+ should be generated because those
          embeddings were not trained. Though, depending on the generation''s parameters,
          they could appear (low probabilities however).'
        updatedAt: '2024-01-09T18:08:41.555Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659d8ba950c1bbee5be6f17c
    id: 659d8ba950c1bbee5be6f179
    type: comment
  author: gugarosa
  content: 'Could you please provide the script which is generating those identifiers?


    We ended up setting 51200 as the vocabulary size just to accommodate any new tokens
    that we might need in the future. You can follow @Deepakvictor answer and it should
    fix the issue.


    As far as I know, no tokens from 50295+ should be generated because those embeddings
    were not trained. Though, depending on the generation''s parameters, they could
    appear (low probabilities however).'
  created_at: 2024-01-09 18:08:41+00:00
  edited: false
  hidden: false
  id: 659d8ba950c1bbee5be6f179
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-09T18:08:41.000Z'
    data:
      status: closed
    id: 659d8ba950c1bbee5be6f17c
    type: status-change
  author: gugarosa
  created_at: 2024-01-09 18:08:41+00:00
  id: 659d8ba950c1bbee5be6f17c
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-09T18:08:44.000Z'
    data:
      status: open
    id: 659d8bacb03a0f0e4f3545b0
    type: status-change
  author: gugarosa
  created_at: 2024-01-09 18:08:44+00:00
  id: 659d8bacb03a0f0e4f3545b0
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-17T18:03:33.000Z'
    data:
      status: closed
    id: 65a81675ae3bd1cc03a920ec
    type: status-change
  author: gugarosa
  created_at: 2024-01-17 18:03:33+00:00
  id: 65a81675ae3bd1cc03a920ec
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 22
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Mismatching tokenizer and LLM model
