!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vvoden
conflicting_files: null
created_at: 2023-12-14 03:44:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67d4f861365a111e110e7d8c367b3b7c.svg
      fullname: vvoden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vvoden
      type: user
    createdAt: '2023-12-14T03:44:17.000Z'
    data:
      edited: false
      editors:
      - vvoden
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8898966312408447
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67d4f861365a111e110e7d8c367b3b7c.svg
          fullname: vvoden
          isHf: false
          isPro: false
          name: vvoden
          type: user
        html: '<p>The model summary here has sample code for "FP32 / CUDA" and "FP32
          / CPU" but the parameters seem to be 16bit.<br>The files on azure seem to
          be 32bit (twice the size and list "torch_dtype": "float32" in the config.json)</p>

          <p>Are the parameters uploaded here quantized?</p>

          '
        raw: "The model summary here has sample code for \"FP32 / CUDA\" and \"FP32\
          \ / CPU\" but the parameters seem to be 16bit.\r\nThe files on azure seem\
          \ to be 32bit (twice the size and list \"torch_dtype\": \"float32\" in the\
          \ config.json)\r\n\r\nAre the parameters uploaded here quantized?"
        updatedAt: '2023-12-14T03:44:17.597Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - okket
        - wassname
        - RalphX1
        - akas911
    id: 657a7a11270ef0b785e912bc
    type: comment
  author: vvoden
  content: "The model summary here has sample code for \"FP32 / CUDA\" and \"FP32\
    \ / CPU\" but the parameters seem to be 16bit.\r\nThe files on azure seem to be\
    \ 32bit (twice the size and list \"torch_dtype\": \"float32\" in the config.json)\r\
    \n\r\nAre the parameters uploaded here quantized?"
  created_at: 2023-12-14 03:44:17+00:00
  edited: false
  hidden: false
  id: 657a7a11270ef0b785e912bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658216359242-noauth.jpeg?w=200&h=200&f=face
      fullname: Bail Adnan Farid
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zakoman
      type: user
    createdAt: '2023-12-14T11:28:02.000Z'
    data:
      edited: false
      editors:
      - zakoman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8869284391403198
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1658216359242-noauth.jpeg?w=200&h=200&f=face
          fullname: Bail Adnan Farid
          isHf: false
          isPro: false
          name: zakoman
          type: user
        html: "<p>It's not, fp16, or half precision is used by almost everyone nowadays\
          \ because it's much efficient and faster than full precision.<br>The only\
          \ reason to use fp32 now is either you have unsupported card or your fp16\
          \ performance is bad (Like P40).<br>You can also upcast it to fp32 iirc.<br>People\
          \ usually use the term 'quantisation' if the bits per weight is 8 or less,\
          \ this one is 16.</p>\n<p>Anyway, from huggingface's <a href=\"https://huggingface.co/docs/transformers/main_classes/configuration\"\
          >docs</a>:</p>\n<blockquote>\n<p>torch_dtype (str, optional) \u2014 The\
          \ dtype of the weights. This attribute can be used to initialize the model\
          \ to a non-default dtype (which is normally float32) and thus allow for\
          \ optimal storage allocation. For example, if the saved model is float16,\
          \ ideally we want to load it back using the minimal amount of memory needed\
          \ to load float16 weights. Since the config object is stored in plain text,\
          \ this attribute contains just the floating type string without the torch.\
          \ prefix. For example, for torch.float16 `torch_dtype is the \"float16\"\
          \ string.<br>This attribute is currently not being used during model loading\
          \ time, but this may change in the future versions. But we can already start\
          \ preparing for the future by saving the dtype with save_pretrained.</p>\n\
          </blockquote>\n"
        raw: "It's not, fp16, or half precision is used by almost everyone nowadays\
          \ because it's much efficient and faster than full precision.\nThe only\
          \ reason to use fp32 now is either you have unsupported card or your fp16\
          \ performance is bad (Like P40).\nYou can also upcast it to fp32 iirc.\n\
          People usually use the term 'quantisation' if the bits per weight is 8 or\
          \ less, this one is 16.\n\nAnyway, from huggingface's [docs](https://huggingface.co/docs/transformers/main_classes/configuration):\n\
          >torch_dtype (str, optional) \u2014 The dtype of the weights. This attribute\
          \ can be used to initialize the model to a non-default dtype (which is normally\
          \ float32) and thus allow for optimal storage allocation. For example, if\
          \ the saved model is float16, ideally we want to load it back using the\
          \ minimal amount of memory needed to load float16 weights. Since the config\
          \ object is stored in plain text, this attribute contains just the floating\
          \ type string without the torch. prefix. For example, for torch.float16\
          \ `torch_dtype is the \"float16\" string.\nThis attribute is currently not\
          \ being used during model loading time, but this may change in the future\
          \ versions. But we can already start preparing for the future by saving\
          \ the dtype with save_pretrained."
        updatedAt: '2023-12-14T11:28:02.566Z'
      numEdits: 0
      reactions: []
    id: 657ae6c22cbb7f638227fe78
    type: comment
  author: zakoman
  content: "It's not, fp16, or half precision is used by almost everyone nowadays\
    \ because it's much efficient and faster than full precision.\nThe only reason\
    \ to use fp32 now is either you have unsupported card or your fp16 performance\
    \ is bad (Like P40).\nYou can also upcast it to fp32 iirc.\nPeople usually use\
    \ the term 'quantisation' if the bits per weight is 8 or less, this one is 16.\n\
    \nAnyway, from huggingface's [docs](https://huggingface.co/docs/transformers/main_classes/configuration):\n\
    >torch_dtype (str, optional) \u2014 The dtype of the weights. This attribute can\
    \ be used to initialize the model to a non-default dtype (which is normally float32)\
    \ and thus allow for optimal storage allocation. For example, if the saved model\
    \ is float16, ideally we want to load it back using the minimal amount of memory\
    \ needed to load float16 weights. Since the config object is stored in plain text,\
    \ this attribute contains just the floating type string without the torch. prefix.\
    \ For example, for torch.float16 `torch_dtype is the \"float16\" string.\nThis\
    \ attribute is currently not being used during model loading time, but this may\
    \ change in the future versions. But we can already start preparing for the future\
    \ by saving the dtype with save_pretrained."
  created_at: 2023-12-14 11:28:02+00:00
  edited: false
  hidden: false
  id: 657ae6c22cbb7f638227fe78
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/67d4f861365a111e110e7d8c367b3b7c.svg
      fullname: vvoden
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vvoden
      type: user
    createdAt: '2023-12-14T15:02:29.000Z'
    data:
      edited: false
      editors:
      - vvoden
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8856962323188782
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/67d4f861365a111e110e7d8c367b3b7c.svg
          fullname: vvoden
          isHf: false
          isPro: false
          name: vvoden
          type: user
        html: '<blockquote>

          <p>fp16, or half precision is used by almost everyone nowadays</p>

          </blockquote>

          <p>That''s correct, but not responsive to my question. The parameters on
          azure are twice the size on disk (viewable in the screenshot below) as they
          are in this huggingface repo, and I''m asking why</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6306f9e7cfbde33ef7e061c0/bwv4b-RX5WyK3qKPaY6Ir.png"><img
          alt="Screenshot 2023-12-14 095655.png" src="https://cdn-uploads.huggingface.co/production/uploads/6306f9e7cfbde33ef7e061c0/bwv4b-RX5WyK3qKPaY6Ir.png"></a></p>

          '
        raw: '>fp16, or half precision is used by almost everyone nowadays


          That''s correct, but not responsive to my question. The parameters on azure
          are twice the size on disk (viewable in the screenshot below) as they are
          in this huggingface repo, and I''m asking why


          ![Screenshot 2023-12-14 095655.png](https://cdn-uploads.huggingface.co/production/uploads/6306f9e7cfbde33ef7e061c0/bwv4b-RX5WyK3qKPaY6Ir.png)

          '
        updatedAt: '2023-12-14T15:02:29.135Z'
      numEdits: 0
      reactions: []
    id: 657b1905be51c126fd34a777
    type: comment
  author: vvoden
  content: '>fp16, or half precision is used by almost everyone nowadays


    That''s correct, but not responsive to my question. The parameters on azure are
    twice the size on disk (viewable in the screenshot below) as they are in this
    huggingface repo, and I''m asking why


    ![Screenshot 2023-12-14 095655.png](https://cdn-uploads.huggingface.co/production/uploads/6306f9e7cfbde33ef7e061c0/bwv4b-RX5WyK3qKPaY6Ir.png)

    '
  created_at: 2023-12-14 15:02:29+00:00
  edited: false
  hidden: false
  id: 657b1905be51c126fd34a777
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-20T12:50:05.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9795830845832825
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;vvoden&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/vvoden\"\
          >@<span class=\"underline\">vvoden</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>The model has been saved in FP32 in Azure, while it was saved in FP16\
          \ in HF.</p>\n<p>Best regards,<br>Gustavo.</p>\n"
        raw: 'Hello @vvoden!


          The model has been saved in FP32 in Azure, while it was saved in FP16 in
          HF.


          Best regards,

          Gustavo.'
        updatedAt: '2023-12-20T12:50:05.527Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6582e2fdf417510bedde56be
    id: 6582e2fdf417510bedde56bd
    type: comment
  author: gugarosa
  content: 'Hello @vvoden!


    The model has been saved in FP32 in Azure, while it was saved in FP16 in HF.


    Best regards,

    Gustavo.'
  created_at: 2023-12-20 12:50:05+00:00
  edited: false
  hidden: false
  id: 6582e2fdf417510bedde56bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-20T12:50:05.000Z'
    data:
      status: closed
    id: 6582e2fdf417510bedde56be
    type: status-change
  author: gugarosa
  created_at: 2023-12-20 12:50:05+00:00
  id: 6582e2fdf417510bedde56be
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: why are these parameters 16bit when the phi-2 hosted on azure is 32bit?
