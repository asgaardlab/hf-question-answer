!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ad6398
conflicting_files: null
created_at: 2024-01-05 00:43:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60cf2e86e1c9d097836112a4eb7d345c.svg
      fullname: Amardeep Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ad6398
      type: user
    createdAt: '2024-01-05T00:43:59.000Z'
    data:
      edited: false
      editors:
      - ad6398
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7361360192298889
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60cf2e86e1c9d097836112a4eb7d345c.svg
          fullname: Amardeep Kumar
          isHf: false
          isPro: false
          name: ad6398
          type: user
        html: "<p>I am asking Phi-2 to explain photosynthesis, using two methods</p>\n\
          <p><em><strong>Greedy decoding/ Normal Method</strong></em></p>\n<pre><code\
          \ class=\"language-python\">base_model_id = <span class=\"hljs-string\"\
          >'microsoft/phi-2'</span>\neval_model = AutoModelForCausalLM.from_pretrained(base_model_id,\
          \ trust_remote_code=<span class=\"hljs-literal\">True</span>, torch_dtype=torch.float16,\
          \ load_in_8bit=<span class=\"hljs-literal\">True</span>)\n\n<span class=\"\
          hljs-keyword\">with</span> torch.no_grad():\n    raw_op_greedy = eval_model.generate(**tok_eval_prompt,\
          \ max_new_tokens=<span class=\"hljs-number\">500</span>, repetition_penalty=<span\
          \ class=\"hljs-number\">1.15</span>)\n</code></pre>\n<p>Output<br><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/UUIfmwXJTU7MKopW9sUNf.png\"\
          ><img alt=\"Screenshot 2024-01-04 at 7.30.01 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/UUIfmwXJTU7MKopW9sUNf.png\"\
          ></a></p>\n<p><em><strong>Nucleus Sampling</strong></em></p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">with</span> torch.no_grad():\n\
          \    rnp = eval_model.generate(**tok_eval_prompt, max_new_tokens=<span class=\"\
          hljs-number\">500</span>, repetition_penalty=<span class=\"hljs-number\"\
          >1.15</span>, do_sample=<span class=\"hljs-literal\">True</span>, top_p=<span\
          \ class=\"hljs-number\">0.90</span>, num_return_sequences=<span class=\"\
          hljs-number\">3</span>)\n</code></pre>\n<p>Outputs:</p>\n<p><a rel=\"nofollow\"\
          \ href=\"https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/2JBoM75__tSmanD58qmrw.png\"\
          ><img alt=\"Screenshot 2024-01-04 at 7.30.50 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/2JBoM75__tSmanD58qmrw.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/ReKtKLm6RcevsKMeXL48m.png\"\
          ><img alt=\"Screenshot 2024-01-04 at 7.31.11 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/ReKtKLm6RcevsKMeXL48m.png\"\
          ></a></p>\n<p>In all cases model is adding some irrelevant piece of text\
          \ after the explanation. I was wondering what could be the reason, it it\
          \ <code>max_new_token</code> parameter? Do we need to set it up explicitly\
          \ for every query, after guessing what could be the length where Phi-2 won't\
          \ add some redundant texts.</p>\n<p>Second question I have is regrading\
          \ support of sampling of generated text. I noticed below statement in model-card\
          \ section, does this mean that along with beam search, Top-k or Top-p samplings\
          \ are irrelevant to Phi-2 somehow and it is best while greedy decoding only?\
          \ </p>\n<p><code>In the generation function, our model currently does not\
          \ support beam search (num_beams &gt; 1). </code></p>\n<p>I tried multiple\
          \ flavour code-generation, Chat-mode, Instruction-output, Sampling method\
          \ gave worst results. Are there any specific reasons or I am doing something\
          \ wrong with sampling or any other parameter passing?</p>\n"
        raw: "I am asking Phi-2 to explain photosynthesis, using two methods\r\n\r\
          \n***Greedy decoding/ Normal Method***\r\n```python\r\nbase_model_id = 'microsoft/phi-2'\r\
          \neval_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True,\
          \ torch_dtype=torch.float16, load_in_8bit=True)\r\n\r\nwith torch.no_grad():\r\
          \n    raw_op_greedy = eval_model.generate(**tok_eval_prompt, max_new_tokens=500,\
          \ repetition_penalty=1.15)\r\n```\r\nOutput\r\n![Screenshot 2024-01-04 at\
          \ 7.30.01 PM.png](https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/UUIfmwXJTU7MKopW9sUNf.png)\r\
          \n\r\n***Nucleus Sampling***\r\n\r\n```python\r\nwith torch.no_grad():\r\
          \n    rnp = eval_model.generate(**tok_eval_prompt, max_new_tokens=500, repetition_penalty=1.15,\
          \ do_sample=True, top_p=0.90, num_return_sequences=3)\r\n```\r\n\r\nOutputs:\r\
          \n\r\n![Screenshot 2024-01-04 at 7.30.50 PM.png](https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/2JBoM75__tSmanD58qmrw.png)\r\
          \n\r\n\r\n![Screenshot 2024-01-04 at 7.31.11 PM.png](https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/ReKtKLm6RcevsKMeXL48m.png)\r\
          \n\r\n\r\nIn all cases model is adding some irrelevant piece of text after\
          \ the explanation. I was wondering what could be the reason, it it `max_new_token`\
          \ parameter? Do we need to set it up explicitly for every query, after guessing\
          \ what could be the length where Phi-2 won't add some redundant texts.\r\
          \n\r\nSecond question I have is regrading support of sampling of generated\
          \ text. I noticed below statement in model-card section, does this mean\
          \ that along with beam search, Top-k or Top-p samplings are irrelevant to\
          \ Phi-2 somehow and it is best while greedy decoding only? \r\n\r\n`In the\
          \ generation function, our model currently does not support beam search\
          \ (num_beams > 1). `\r\n\r\nI tried multiple flavour code-generation, Chat-mode,\
          \ Instruction-output, Sampling method gave worst results. Are there any\
          \ specific reasons or I am doing something wrong with sampling or any other\
          \ parameter passing?\r\n\r\n\r\n"
        updatedAt: '2024-01-05T00:43:59.089Z'
      numEdits: 0
      reactions: []
    id: 659750cf7f63adec59e08760
    type: comment
  author: ad6398
  content: "I am asking Phi-2 to explain photosynthesis, using two methods\r\n\r\n\
    ***Greedy decoding/ Normal Method***\r\n```python\r\nbase_model_id = 'microsoft/phi-2'\r\
    \neval_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True,\
    \ torch_dtype=torch.float16, load_in_8bit=True)\r\n\r\nwith torch.no_grad():\r\
    \n    raw_op_greedy = eval_model.generate(**tok_eval_prompt, max_new_tokens=500,\
    \ repetition_penalty=1.15)\r\n```\r\nOutput\r\n![Screenshot 2024-01-04 at 7.30.01\
    \ PM.png](https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/UUIfmwXJTU7MKopW9sUNf.png)\r\
    \n\r\n***Nucleus Sampling***\r\n\r\n```python\r\nwith torch.no_grad():\r\n   \
    \ rnp = eval_model.generate(**tok_eval_prompt, max_new_tokens=500, repetition_penalty=1.15,\
    \ do_sample=True, top_p=0.90, num_return_sequences=3)\r\n```\r\n\r\nOutputs:\r\
    \n\r\n![Screenshot 2024-01-04 at 7.30.50 PM.png](https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/2JBoM75__tSmanD58qmrw.png)\r\
    \n\r\n\r\n![Screenshot 2024-01-04 at 7.31.11 PM.png](https://cdn-uploads.huggingface.co/production/uploads/613639be6d33338f6533ffe4/ReKtKLm6RcevsKMeXL48m.png)\r\
    \n\r\n\r\nIn all cases model is adding some irrelevant piece of text after the\
    \ explanation. I was wondering what could be the reason, it it `max_new_token`\
    \ parameter? Do we need to set it up explicitly for every query, after guessing\
    \ what could be the length where Phi-2 won't add some redundant texts.\r\n\r\n\
    Second question I have is regrading support of sampling of generated text. I noticed\
    \ below statement in model-card section, does this mean that along with beam search,\
    \ Top-k or Top-p samplings are irrelevant to Phi-2 somehow and it is best while\
    \ greedy decoding only? \r\n\r\n`In the generation function, our model currently\
    \ does not support beam search (num_beams > 1). `\r\n\r\nI tried multiple flavour\
    \ code-generation, Chat-mode, Instruction-output, Sampling method gave worst results.\
    \ Are there any specific reasons or I am doing something wrong with sampling or\
    \ any other parameter passing?\r\n\r\n\r\n"
  created_at: 2024-01-05 00:43:59+00:00
  edited: false
  hidden: false
  id: 659750cf7f63adec59e08760
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/60cf2e86e1c9d097836112a4eb7d345c.svg
      fullname: Amardeep Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ad6398
      type: user
    createdAt: '2024-01-05T00:45:15.000Z'
    data:
      from: Irrelevant results while
      to: Irrelevant text generation while prompting
    id: 6597511bd92a44a76c83e689
    type: title-change
  author: ad6398
  created_at: 2024-01-05 00:45:15+00:00
  id: 6597511bd92a44a76c83e689
  new_title: Irrelevant text generation while prompting
  old_title: Irrelevant results while
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23dce55b32120be8d9144e7bbbfc8823.svg
      fullname: Baasit Sharief
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baasitsh
      type: user
    createdAt: '2024-01-08T16:38:03.000Z'
    data:
      edited: true
      editors:
      - baasitsh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9028080105781555
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23dce55b32120be8d9144e7bbbfc8823.svg
          fullname: Baasit Sharief
          isHf: false
          isPro: false
          name: baasitsh
          type: user
        html: '<p>The instruct template you''re using has a typo: "Instruct: " should
          be used instead of "Instruction".</p>

          <p>More information is on the technical reports for <a rel="nofollow" href="https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/">phi</a></p>

          '
        raw: 'The instruct template you''re using has a typo: "Instruct: " should
          be used instead of "Instruction".


          More information is on the technical reports for [phi](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)

          '
        updatedAt: '2024-01-08T16:46:58.659Z'
      numEdits: 1
      reactions: []
    id: 659c24ebbc5a462bbf9a40f0
    type: comment
  author: baasitsh
  content: 'The instruct template you''re using has a typo: "Instruct: " should be
    used instead of "Instruction".


    More information is on the technical reports for [phi](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)

    '
  created_at: 2024-01-08 16:38:03+00:00
  edited: true
  hidden: false
  id: 659c24ebbc5a462bbf9a40f0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/60cf2e86e1c9d097836112a4eb7d345c.svg
      fullname: Amardeep Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ad6398
      type: user
    createdAt: '2024-01-08T20:10:38.000Z'
    data:
      edited: false
      editors:
      - ad6398
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.964318573474884
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/60cf2e86e1c9d097836112a4eb7d345c.svg
          fullname: Amardeep Kumar
          isHf: false
          isPro: false
          name: ad6398
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;baasitsh&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/baasitsh\">@<span class=\"\
          underline\">baasitsh</span></a></span>\n\n\t</span></span> it gave similar\
          \ results. </p>\n"
        raw: '@baasitsh it gave similar results. '
        updatedAt: '2024-01-08T20:10:38.383Z'
      numEdits: 0
      reactions: []
    id: 659c56be6090ab0a493731db
    type: comment
  author: ad6398
  content: '@baasitsh it gave similar results. '
  created_at: 2024-01-08 20:10:38+00:00
  edited: false
  hidden: false
  id: 659c56be6090ab0a493731db
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/23dce55b32120be8d9144e7bbbfc8823.svg
      fullname: Baasit Sharief
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: baasitsh
      type: user
    createdAt: '2024-01-08T22:03:58.000Z'
    data:
      edited: false
      editors:
      - baasitsh
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9717926383018494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/23dce55b32120be8d9144e7bbbfc8823.svg
          fullname: Baasit Sharief
          isHf: false
          isPro: false
          name: baasitsh
          type: user
        html: '<p>Then it''s probably because none of the phi models are instruction
          tuned or for chat use cases. So, they don''t know when to stop generating.</p>

          '
        raw: Then it's probably because none of the phi models are instruction tuned
          or for chat use cases. So, they don't know when to stop generating.
        updatedAt: '2024-01-08T22:03:58.718Z'
      numEdits: 0
      reactions: []
    id: 659c714e18cb580e70e49081
    type: comment
  author: baasitsh
  content: Then it's probably because none of the phi models are instruction tuned
    or for chat use cases. So, they don't know when to stop generating.
  created_at: 2024-01-08 22:03:58+00:00
  edited: false
  hidden: false
  id: 659c714e18cb580e70e49081
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/766e627499bbe29b7984391d144fb651.svg
      fullname: Tadas Pyragius
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TP6174
      type: user
    createdAt: '2024-01-09T21:57:51.000Z'
    data:
      edited: false
      editors:
      - TP6174
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7388590574264526
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/766e627499bbe29b7984391d144fb651.svg
          fullname: Tadas Pyragius
          isHf: false
          isPro: false
          name: TP6174
          type: user
        html: "<p>normally you need to preset the prompt with \"Instruct\" as well\
          \ as prompt it with output termination string. The model outputs \"end of\
          \ text\" string which you can pick up as it finishes. Normally this works\
          \ very well, though on occasion it will keep going until it reaches the\
          \ output token limit. As people have mentioned here this is a base model\
          \ :)  Here's what I'm using:</p>\n<p>  def generate_llm_response(model,\
          \ tokenizer, device, prompt, max_length):</p>\n<pre><code>  output_termination\
          \ = \"\\nOutput:\"\n  total_input = f\"Instruct:{prompt}{output_termination}\"\
          \n  inputs = tokenizer(total_input, return_tensors=\"pt\", return_attention_mask=True)\n\
          \  inputs = inputs.to(device)\n  eos_token_id = tokenizer.eos_token_id\n\
          \n  outputs = model.generate(**inputs, max_length=max_length, eos_token_id=eos_token_id)\n\
          \n  # Find the position of \"Output:\" and extract the text after it\n \
          \ generated_text = tokenizer.batch_decode(outputs)[0]\n\n  # Split the text\
          \ at \"Output:\" and take the second part\n  split_text = generated_text.split(\"\
          Output:\", 1)\n  assistant_response = split_text[1].strip() if len(split_text)\
          \ &gt; 1 else \"\"\n  assistant_response = assistant_response.replace(\"\
          &lt;|endoftext|&gt;\", \"\").strip()\n\n  return assistant_response\n</code></pre>\n"
        raw: "normally you need to preset the prompt with \"Instruct\" as well as\
          \ prompt it with output termination string. The model outputs \"end of text\"\
          \ string which you can pick up as it finishes. Normally this works very\
          \ well, though on occasion it will keep going until it reaches the output\
          \ token limit. As people have mentioned here this is a base model :)  Here's\
          \ what I'm using:\n\n  def generate_llm_response(model, tokenizer, device,\
          \ prompt, max_length):\n\n      output_termination = \"\\nOutput:\"\n  \
          \    total_input = f\"Instruct:{prompt}{output_termination}\"\n      inputs\
          \ = tokenizer(total_input, return_tensors=\"pt\", return_attention_mask=True)\n\
          \      inputs = inputs.to(device)\n      eos_token_id = tokenizer.eos_token_id\n\
          \n      outputs = model.generate(**inputs, max_length=max_length, eos_token_id=eos_token_id)\n\
          \n      # Find the position of \"Output:\" and extract the text after it\n\
          \      generated_text = tokenizer.batch_decode(outputs)[0]\n\n      # Split\
          \ the text at \"Output:\" and take the second part\n      split_text = generated_text.split(\"\
          Output:\", 1)\n      assistant_response = split_text[1].strip() if len(split_text)\
          \ > 1 else \"\"\n      assistant_response = assistant_response.replace(\"\
          <|endoftext|>\", \"\").strip()\n\n      return assistant_response\n\n"
        updatedAt: '2024-01-09T21:57:51.595Z'
      numEdits: 0
      reactions: []
    id: 659dc15ff2e0e03e457f1ea9
    type: comment
  author: TP6174
  content: "normally you need to preset the prompt with \"Instruct\" as well as prompt\
    \ it with output termination string. The model outputs \"end of text\" string\
    \ which you can pick up as it finishes. Normally this works very well, though\
    \ on occasion it will keep going until it reaches the output token limit. As people\
    \ have mentioned here this is a base model :)  Here's what I'm using:\n\n  def\
    \ generate_llm_response(model, tokenizer, device, prompt, max_length):\n\n   \
    \   output_termination = \"\\nOutput:\"\n      total_input = f\"Instruct:{prompt}{output_termination}\"\
    \n      inputs = tokenizer(total_input, return_tensors=\"pt\", return_attention_mask=True)\n\
    \      inputs = inputs.to(device)\n      eos_token_id = tokenizer.eos_token_id\n\
    \n      outputs = model.generate(**inputs, max_length=max_length, eos_token_id=eos_token_id)\n\
    \n      # Find the position of \"Output:\" and extract the text after it\n   \
    \   generated_text = tokenizer.batch_decode(outputs)[0]\n\n      # Split the text\
    \ at \"Output:\" and take the second part\n      split_text = generated_text.split(\"\
    Output:\", 1)\n      assistant_response = split_text[1].strip() if len(split_text)\
    \ > 1 else \"\"\n      assistant_response = assistant_response.replace(\"<|endoftext|>\"\
    , \"\").strip()\n\n      return assistant_response\n\n"
  created_at: 2024-01-09 21:57:51+00:00
  edited: false
  hidden: false
  id: 659dc15ff2e0e03e457f1ea9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/35a2f09fa3e7a1b31f52fa85c78a20da.svg
      fullname: Francesco Cozzolino
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FrancescoCozzolino
      type: user
    createdAt: '2024-01-11T09:07:22.000Z'
    data:
      edited: true
      editors:
      - FrancescoCozzolino
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8436024188995361
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/35a2f09fa3e7a1b31f52fa85c78a20da.svg
          fullname: Francesco Cozzolino
          isHf: false
          isPro: false
          name: FrancescoCozzolino
          type: user
        html: '<p>I have created a naive solution for this problem which removes the
          extra text at the bottom of the answer. please check the code here github.com/YodaGitMaster/medium-phi2-deploy-finetune-llm</p>

          <p>if you know a way to do it more beautifully, please write me a message,
          really looking forward to it. </p>

          '
        raw: "I have created a naive solution for this problem which removes the extra\
          \ text at the bottom of the answer. please check the code here github.com/YodaGitMaster/medium-phi2-deploy-finetune-llm\n\
          \nif you know a way to do it more beautifully, please write me a message,\
          \ really looking forward to it. \n"
        updatedAt: '2024-01-11T09:09:45.361Z'
      numEdits: 1
      reactions: []
    id: 659fafca24e3a2aa726d7169
    type: comment
  author: FrancescoCozzolino
  content: "I have created a naive solution for this problem which removes the extra\
    \ text at the bottom of the answer. please check the code here github.com/YodaGitMaster/medium-phi2-deploy-finetune-llm\n\
    \nif you know a way to do it more beautifully, please write me a message, really\
    \ looking forward to it. \n"
  created_at: 2024-01-11 09:07:22+00:00
  edited: true
  hidden: false
  id: 659fafca24e3a2aa726d7169
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 63
repo_id: microsoft/phi-2
repo_type: model
status: open
target_branch: null
title: Irrelevant text generation while prompting
