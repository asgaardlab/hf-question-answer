!!python/object:huggingface_hub.community.DiscussionWithDetails
author: praveeny
conflicting_files: null
created_at: 2024-01-11 18:51:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b4f04fbeffd3d7e352150294b68c894.svg
      fullname: Praveen Yerneni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: praveeny
      type: user
    createdAt: '2024-01-11T18:51:33.000Z'
    data:
      edited: false
      editors:
      - praveeny
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9226706624031067
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b4f04fbeffd3d7e352150294b68c894.svg
          fullname: Praveen Yerneni
          isHf: false
          isPro: false
          name: praveeny
          type: user
        html: '<p>I was running Phi-2 on my CPU in a Jupyter notebook. When I just
          tried, it broke :-(( </p>

          <p>I see that the model has been updated. From the little research I did,
          apparently, flash_attn requires that I have Nvidia GPU? How do I run this
          on a CPU now? Or is that no longer an option?</p>

          <p>P.S: - I am unable to install flash_attn, I have updated torch, transformers
          and packages and wheel. Now I see the following error when trying to install
          this package. I don''t have CUDA.</p>

          <p>raise OSError(''CUDA_HOME environment variable is not set. ''<br>      OSError:
          CUDA_HOME environment variable is not set. Please set it to your CUDA install
          root.</p>

          <pre><code>  torch.__version__  = 2.1.2+cpu

          </code></pre>

          '
        raw: "I was running Phi-2 on my CPU in a Jupyter notebook. When I just tried,\
          \ it broke :-(( \r\n\r\nI see that the model has been updated. From the\
          \ little research I did, apparently, flash_attn requires that I have Nvidia\
          \ GPU? How do I run this on a CPU now? Or is that no longer an option?\r\
          \n\r\nP.S: - I am unable to install flash_attn, I have updated torch, transformers\
          \ and packages and wheel. Now I see the following error when trying to install\
          \ this package. I don't have CUDA.\r\n\r\nraise OSError('CUDA_HOME environment\
          \ variable is not set. '\r\n      OSError: CUDA_HOME environment variable\
          \ is not set. Please set it to your CUDA install root.\r\n\r\n\r\n     \
          \ torch.__version__  = 2.1.2+cpu\r\n"
        updatedAt: '2024-01-11T18:51:33.972Z'
      numEdits: 0
      reactions: []
    id: 65a038b514ac6d6f812e863c
    type: comment
  author: praveeny
  content: "I was running Phi-2 on my CPU in a Jupyter notebook. When I just tried,\
    \ it broke :-(( \r\n\r\nI see that the model has been updated. From the little\
    \ research I did, apparently, flash_attn requires that I have Nvidia GPU? How\
    \ do I run this on a CPU now? Or is that no longer an option?\r\n\r\nP.S: - I\
    \ am unable to install flash_attn, I have updated torch, transformers and packages\
    \ and wheel. Now I see the following error when trying to install this package.\
    \ I don't have CUDA.\r\n\r\nraise OSError('CUDA_HOME environment variable is not\
    \ set. '\r\n      OSError: CUDA_HOME environment variable is not set. Please set\
    \ it to your CUDA install root.\r\n\r\n\r\n      torch.__version__  = 2.1.2+cpu\r\
    \n"
  created_at: 2024-01-11 18:51:33+00:00
  edited: false
  hidden: false
  id: 65a038b514ac6d6f812e863c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616387967216-noauth.png?w=200&h=200&f=face
      fullname: Anubhav Anand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aanubhav
      type: user
    createdAt: '2024-01-11T19:49:09.000Z'
    data:
      edited: false
      editors:
      - aanubhav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9582616090774536
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616387967216-noauth.png?w=200&h=200&f=face
          fullname: Anubhav Anand
          isHf: false
          isPro: false
          name: aanubhav
          type: user
        html: '<p>Facing the same issue, I''m trying to download model weights and
          build a docker image with vLLm . It gave the same error. It worked perfectly
          fine 6 hrs back, but with the the latest commit something seems broken.</p>

          <p>In the meantime, how do we pull weights programatically from previous
          commit-id?</p>

          '
        raw: 'Facing the same issue, I''m trying to download model weights and build
          a docker image with vLLm . It gave the same error. It worked perfectly fine
          6 hrs back, but with the the latest commit something seems broken.


          In the meantime, how do we pull weights programatically from previous commit-id?'
        updatedAt: '2024-01-11T19:49:09.092Z'
      numEdits: 0
      reactions: []
    id: 65a0463533d5d9ca30b70caa
    type: comment
  author: aanubhav
  content: 'Facing the same issue, I''m trying to download model weights and build
    a docker image with vLLm . It gave the same error. It worked perfectly fine 6
    hrs back, but with the the latest commit something seems broken.


    In the meantime, how do we pull weights programatically from previous commit-id?'
  created_at: 2024-01-11 19:49:09+00:00
  edited: false
  hidden: false
  id: 65a0463533d5d9ca30b70caa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T00:31:16.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8323292136192322
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Hello everyone!</p>

          <p>We deployed a fix and it should be working now.</p>

          <p>The issue was caused by the combination of using dynamic modules and
          remote code loading in <code>transformers</code>.</p>

          <p>Regards,<br>Gustavo.</p>

          '
        raw: 'Hello everyone!


          We deployed a fix and it should be working now.


          The issue was caused by the combination of using dynamic modules and remote
          code loading in `transformers`.


          Regards,

          Gustavo.'
        updatedAt: '2024-01-12T00:46:07.578Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - praveeny
        - aanubhav
    id: 65a088549185dcca3048af6f
    type: comment
  author: gugarosa
  content: 'Hello everyone!


    We deployed a fix and it should be working now.


    The issue was caused by the combination of using dynamic modules and remote code
    loading in `transformers`.


    Regards,

    Gustavo.'
  created_at: 2024-01-12 00:31:16+00:00
  edited: true
  hidden: false
  id: 65a088549185dcca3048af6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616387967216-noauth.png?w=200&h=200&f=face
      fullname: Anubhav Anand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aanubhav
      type: user
    createdAt: '2024-01-12T04:08:56.000Z'
    data:
      edited: false
      editors:
      - aanubhav
      hidden: false
      identifiedLanguage:
        language: es
        probability: 0.21328818798065186
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616387967216-noauth.png?w=200&h=200&f=face
          fullname: Anubhav Anand
          isHf: false
          isPro: false
          name: aanubhav
          type: user
        html: "<p>Thanks <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'Thanks @gugarosa '
        updatedAt: '2024-01-12T04:08:56.887Z'
      numEdits: 0
      reactions: []
    id: 65a0bb58cefb019bbd39c530
    type: comment
  author: aanubhav
  content: 'Thanks @gugarosa '
  created_at: 2024-01-12 04:08:56+00:00
  edited: false
  hidden: false
  id: 65a0bb58cefb019bbd39c530
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616387967216-noauth.png?w=200&h=200&f=face
      fullname: Anubhav Anand
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aanubhav
      type: user
    createdAt: '2024-01-12T06:25:01.000Z'
    data:
      edited: true
      editors:
      - aanubhav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8583173751831055
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1616387967216-noauth.png?w=200&h=200&f=face
          fullname: Anubhav Anand
          isHf: false
          isPro: false
          name: aanubhav
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> Hey just curious\
          \ to understand the motivation behind renaming of  layer_norm_epsilon to\
          \  layer_norm_eps in the config.json? </p>\n<p> I see vLLM use layer_norm_epsilon\
          \ throughout all the models. So, now the recent commits in this repo is\
          \ breaking things in vLLM<br><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/60581f81cbe9c7542f3501ef/iHQGvOe4jyQjIOcsRjDhM.png\"\
          ><img alt=\"Screenshot 2024-01-12 at 11.54.05 AM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/60581f81cbe9c7542f3501ef/iHQGvOe4jyQjIOcsRjDhM.png\"\
          ></a></p>\n"
        raw: "@gugarosa Hey just curious to understand the motivation behind renaming\
          \ of  layer_norm_epsilon to  layer_norm_eps in the config.json? \n\n I see\
          \ vLLM use layer_norm_epsilon throughout all the models. So, now the recent\
          \ commits in this repo is breaking things in vLLM\n![Screenshot 2024-01-12\
          \ at 11.54.05 AM.png](https://cdn-uploads.huggingface.co/production/uploads/60581f81cbe9c7542f3501ef/iHQGvOe4jyQjIOcsRjDhM.png)\n"
        updatedAt: '2024-01-12T06:25:20.890Z'
      numEdits: 1
      reactions: []
    id: 65a0db3d1754e2f2117dc3e7
    type: comment
  author: aanubhav
  content: "@gugarosa Hey just curious to understand the motivation behind renaming\
    \ of  layer_norm_epsilon to  layer_norm_eps in the config.json? \n\n I see vLLM\
    \ use layer_norm_epsilon throughout all the models. So, now the recent commits\
    \ in this repo is breaking things in vLLM\n![Screenshot 2024-01-12 at 11.54.05\
    \ AM.png](https://cdn-uploads.huggingface.co/production/uploads/60581f81cbe9c7542f3501ef/iHQGvOe4jyQjIOcsRjDhM.png)\n"
  created_at: 2024-01-12 06:25:01+00:00
  edited: true
  hidden: false
  id: 65a0db3d1754e2f2117dc3e7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T13:07:18.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9515432119369507
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>I think we will need to update vLLM as well.</p>

          <p>There is no reason in using <code>layer_norm_eps</code>. It was used
          in the first implementation of Phi (internally in transformers) and we followed
          it minimize friction when merging the integration.</p>

          '
        raw: 'I think we will need to update vLLM as well.


          There is no reason in using `layer_norm_eps`. It was used in the first implementation
          of Phi (internally in transformers) and we followed it minimize friction
          when merging the integration.'
        updatedAt: '2024-01-12T13:07:18.958Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aanubhav
    id: 65a1398626d1e9df4f1f78a3
    type: comment
  author: gugarosa
  content: 'I think we will need to update vLLM as well.


    There is no reason in using `layer_norm_eps`. It was used in the first implementation
    of Phi (internally in transformers) and we followed it minimize friction when
    merging the integration.'
  created_at: 2024-01-12 13:07:18+00:00
  edited: false
  hidden: false
  id: 65a1398626d1e9df4f1f78a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T13:13:35.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8334178328514099
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>By the way, there is an active PR that will fix it: <a rel="nofollow"
          href="https://github.com/vllm-project/vllm/pull/2428/files">https://github.com/vllm-project/vllm/pull/2428/files</a></p>

          '
        raw: 'By the way, there is an active PR that will fix it: https://github.com/vllm-project/vllm/pull/2428/files'
        updatedAt: '2024-01-12T13:13:35.294Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - aanubhav
    id: 65a13aff26d1e9df4f200d00
    type: comment
  author: gugarosa
  content: 'By the way, there is an active PR that will fix it: https://github.com/vllm-project/vllm/pull/2428/files'
  created_at: 2024-01-12 13:13:35+00:00
  edited: false
  hidden: false
  id: 65a13aff26d1e9df4f200d00
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
      fullname: VincentN
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vince62s
      type: user
    createdAt: '2024-01-12T15:21:46.000Z'
    data:
      edited: false
      editors:
      - vince62s
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9699457287788391
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6495b47a74ce69cc4eab61f0/2eg17fMXjshpfQfSq5jyP.png?w=200&h=200&f=face
          fullname: VincentN
          isHf: false
          isPro: false
          name: vince62s
          type: user
        html: '<p>since the layernaming was changed for consistency reasons, don''t
          you think it would be better to align with "layer_norm_epsilon" too ?<br>on
          the other hand llama uses "rms_norm_eps" .... go figure.</p>

          '
        raw: 'since the layernaming was changed for consistency reasons, don''t you
          think it would be better to align with "layer_norm_epsilon" too ?

          on the other hand llama uses "rms_norm_eps" .... go figure.'
        updatedAt: '2024-01-12T15:21:46.013Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - gugarosa
    id: 65a1590a0251d2c6bfd86a62
    type: comment
  author: vince62s
  content: 'since the layernaming was changed for consistency reasons, don''t you
    think it would be better to align with "layer_norm_epsilon" too ?

    on the other hand llama uses "rms_norm_eps" .... go figure.'
  created_at: 2024-01-12 15:21:46+00:00
  edited: false
  hidden: false
  id: 65a1590a0251d2c6bfd86a62
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T17:50:39.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8242616057395935
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>I definitely agree! </p>

          <p>Maybe an <code>attribute_map: {"layer_norm_epsilon": "layer_norm_eps"}</code>
          on the configuration_phi.py would fix the issue. And it would be an easier
          PR.</p>

          '
        raw: "I definitely agree! \n\nMaybe an `attribute_map: {\"layer_norm_epsilon\"\
          : \"layer_norm_eps\"}` on the configuration_phi.py would fix the issue.\
          \ And it would be an easier PR."
        updatedAt: '2024-01-12T17:50:46.656Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - vince62s
    id: 65a17bef0b580e8251a1a999
    type: comment
  author: gugarosa
  content: "I definitely agree! \n\nMaybe an `attribute_map: {\"layer_norm_epsilon\"\
    : \"layer_norm_eps\"}` on the configuration_phi.py would fix the issue. And it\
    \ would be an easier PR."
  created_at: 2024-01-12 17:50:39+00:00
  edited: true
  hidden: false
  id: 65a17bef0b580e8251a1a999
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/5b4f04fbeffd3d7e352150294b68c894.svg
      fullname: Praveen Yerneni
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: praveeny
      type: user
    createdAt: '2024-01-16T15:47:40.000Z'
    data:
      status: closed
    id: 65a6a51c5c58475cf9d01565
    type: status-change
  author: praveeny
  created_at: 2024-01-16 15:47:40+00:00
  id: 65a6a51c5c58475cf9d01565
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 76
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: 'ImportError: This modeling file requires the following packages that were
  not found in your environment: flash_attn. Run `pip install flash_attn`'
