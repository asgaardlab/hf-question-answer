!!python/object:huggingface_hub.community.DiscussionWithDetails
author: navanit
conflicting_files: null
created_at: 2023-12-16 09:40:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba843e4b22ae0a24773dfafa7e3e7c56.svg
      fullname: Navanit Dubey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: navanit
      type: user
    createdAt: '2023-12-16T09:40:17.000Z'
    data:
      edited: false
      editors:
      - navanit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44036614894866943
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba843e4b22ae0a24773dfafa7e3e7c56.svg
          fullname: Navanit Dubey
          isHf: false
          isPro: false
          name: navanit
          type: user
        html: '<p>Hi,<br>so while training the phi-2, I didn''t get it why finetuning
          phi -2 which is just 2B taking more GPU than 7B models like mistral 7b or
          Llama 2 7b. </p>

          <p>Below is the code snippets using QLORA.<br>''''''<br>bnb_config = BitsAndBytesConfig(<br>    load_in_4bit=True,<br>    bnb_4bit_use_double_quant=True,<br>    bnb_4bit_quant_type="nf4",<br>    bnb_4bit_compute_dtype=torch.bfloat16<br>)</p>

          <p>model = transformers.AutoModelForCausalLM.from_pretrained(<br>base_model_id,<br>trust_remote_code=True,<br>config=model_config,<br>quantization_config=bnb_config,<br>device_map=''auto'',</p>

          <p>)</p>

          <p>config = LoraConfig(<br>    r=64,<br>    lora_alpha=16,<br>    target_modules=[<br>        ''Wqkv'',<br>       ''out_proj''<br>    ],<br>    bias="none",<br>    lora_dropout=0.05,  #
          Conventional<br>    task_type="CAUSAL_LM",<br>)</p>

          <p>training_arguments = TrainingArguments(<br>    output_dir= output_dir,<br>    num_train_epochs=
          2,<br>    # max_steps= 1800,<br>    per_device_train_batch_size= 2,<br>    gradient_accumulation_steps=
          1,<br>    optim="paged_adamw_32bit",<br>    save_strategy="epoch",<br>    logging_steps=100,<br>    logging_strategy="steps",<br>    learning_rate=
          2e-4,<br>    fp16= False,<br>    bf16=  True,<br>    group_by_length= True,<br>    disable_tqdm=False,<br>    report_to="tensorboard",<br>)</p>

          <p>trainer = SFTTrainer(<br>    model=model,<br>    peft_config=config,<br>    dataset_text_field="train",<br>    train_dataset=dataset,<br>    max_seq_length=2048,<br>    tokenizer=tokenizer,<br>    args=training_arguments,<br>    packing=False,<br>)<br>''''''</p>

          '
        raw: "Hi, \r\nso while training the phi-2, I didn't get it why finetuning\
          \ phi -2 which is just 2B taking more GPU than 7B models like mistral 7b\
          \ or Llama 2 7b. \r\n\r\nBelow is the code snippets using QLORA. \r\n'''\r\
          \nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\
          \n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.bfloat16\r\
          \n)\r\n\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\n\
          base_model_id,\r\ntrust_remote_code=True,\r\nconfig=model_config,\r\nquantization_config=bnb_config,\r\
          \ndevice_map='auto',\r\n\r\n)\r\n\r\nconfig = LoraConfig(\r\n    r=64,\r\
          \n    lora_alpha=16,\r\n    target_modules=[\r\n        'Wqkv',\r\n    \
          \   'out_proj'\r\n    ],\r\n    bias=\"none\",\r\n    lora_dropout=0.05,\
          \  # Conventional\r\n    task_type=\"CAUSAL_LM\",\r\n)\r\n\r\ntraining_arguments\
          \ = TrainingArguments(\r\n    output_dir= output_dir,\r\n    num_train_epochs=\
          \ 2,\r\n    # max_steps= 1800,\r\n    per_device_train_batch_size= 2,\r\n\
          \    gradient_accumulation_steps= 1,\r\n    optim=\"paged_adamw_32bit\"\
          ,\r\n    save_strategy=\"epoch\",\r\n    logging_steps=100,\r\n    logging_strategy=\"\
          steps\",\r\n    learning_rate= 2e-4,\r\n    fp16= False,\r\n    bf16=  True,\r\
          \n    group_by_length= True,\r\n    disable_tqdm=False,\r\n    report_to=\"\
          tensorboard\",    \r\n)\r\n\r\ntrainer = SFTTrainer(\r\n    model=model,\r\
          \n    peft_config=config,\r\n    dataset_text_field=\"train\",\r\n    train_dataset=dataset,\r\
          \n    max_seq_length=2048,\r\n    tokenizer=tokenizer,\r\n    args=training_arguments,\r\
          \n    packing=False,\r\n)\r\n'''\r\n"
        updatedAt: '2023-12-16T09:40:17.451Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - supreethrao
    id: 657d7081e67599435735f3e4
    type: comment
  author: navanit
  content: "Hi, \r\nso while training the phi-2, I didn't get it why finetuning phi\
    \ -2 which is just 2B taking more GPU than 7B models like mistral 7b or Llama\
    \ 2 7b. \r\n\r\nBelow is the code snippets using QLORA. \r\n'''\r\nbnb_config\
    \ = BitsAndBytesConfig(\r\n    load_in_4bit=True,\r\n    bnb_4bit_use_double_quant=True,\r\
    \n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.bfloat16\r\
    \n)\r\n\r\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\r\nbase_model_id,\r\
    \ntrust_remote_code=True,\r\nconfig=model_config,\r\nquantization_config=bnb_config,\r\
    \ndevice_map='auto',\r\n\r\n)\r\n\r\nconfig = LoraConfig(\r\n    r=64,\r\n   \
    \ lora_alpha=16,\r\n    target_modules=[\r\n        'Wqkv',\r\n       'out_proj'\r\
    \n    ],\r\n    bias=\"none\",\r\n    lora_dropout=0.05,  # Conventional\r\n \
    \   task_type=\"CAUSAL_LM\",\r\n)\r\n\r\ntraining_arguments = TrainingArguments(\r\
    \n    output_dir= output_dir,\r\n    num_train_epochs= 2,\r\n    # max_steps=\
    \ 1800,\r\n    per_device_train_batch_size= 2,\r\n    gradient_accumulation_steps=\
    \ 1,\r\n    optim=\"paged_adamw_32bit\",\r\n    save_strategy=\"epoch\",\r\n \
    \   logging_steps=100,\r\n    logging_strategy=\"steps\",\r\n    learning_rate=\
    \ 2e-4,\r\n    fp16= False,\r\n    bf16=  True,\r\n    group_by_length= True,\r\
    \n    disable_tqdm=False,\r\n    report_to=\"tensorboard\",    \r\n)\r\n\r\ntrainer\
    \ = SFTTrainer(\r\n    model=model,\r\n    peft_config=config,\r\n    dataset_text_field=\"\
    train\",\r\n    train_dataset=dataset,\r\n    max_seq_length=2048,\r\n    tokenizer=tokenizer,\r\
    \n    args=training_arguments,\r\n    packing=False,\r\n)\r\n'''\r\n"
  created_at: 2023-12-16 09:40:17+00:00
  edited: false
  hidden: false
  id: 657d7081e67599435735f3e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-16T17:40:45.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9018564224243164
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Yep, same here. you need to use batch size 1 and gradiant checkint
          &gt;1 to even the deal. </p>

          '
        raw: 'Yep, same here. you need to use batch size 1 and gradiant checkint >1
          to even the deal. '
        updatedAt: '2023-12-16T17:40:45.849Z'
      numEdits: 0
      reactions: []
    id: 657de11dbc9bceccf9e70cd3
    type: comment
  author: Yhyu13
  content: 'Yep, same here. you need to use batch size 1 and gradiant checkint >1
    to even the deal. '
  created_at: 2023-12-16 17:40:45+00:00
  edited: false
  hidden: false
  id: 657de11dbc9bceccf9e70cd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be1bffe38420aabae4f984/i1M2P_12qIauDROLNER6d.jpeg?w=200&h=200&f=face
      fullname: NAVANIT DUBEY
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Navanit-shorthills
      type: user
    createdAt: '2023-12-16T17:52:18.000Z'
    data:
      edited: false
      editors:
      - Navanit-shorthills
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9489641785621643
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64be1bffe38420aabae4f984/i1M2P_12qIauDROLNER6d.jpeg?w=200&h=200&f=face
          fullname: NAVANIT DUBEY
          isHf: false
          isPro: false
          name: Navanit-shorthills
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Yhyu13&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Yhyu13\">@<span class=\"\
          underline\">Yhyu13</span></a></span>\n\n\t</span></span> after changing\
          \ the batch size also it doesn't happens anything extra and regarding gradient\
          \ checkpoint isn;t that False?<br>if you can share your training code it\
          \ will be helpful</p>\n"
        raw: '@Yhyu13 after changing the batch size also it doesn''t happens anything
          extra and regarding gradient checkpoint isn;t that False?

          if you can share your training code it will be helpful'
        updatedAt: '2023-12-16T17:52:18.519Z'
      numEdits: 0
      reactions: []
    id: 657de3d2647c0211e78d936d
    type: comment
  author: Navanit-shorthills
  content: '@Yhyu13 after changing the batch size also it doesn''t happens anything
    extra and regarding gradient checkpoint isn;t that False?

    if you can share your training code it will be helpful'
  created_at: 2023-12-16 17:52:18+00:00
  edited: false
  hidden: false
  id: 657de3d2647c0211e78d936d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-17T03:12:16.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.18036742508411407
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Navanit-shorthills&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Navanit-shorthills\"\
          >@<span class=\"underline\">Navanit-shorthills</span></a></span>\n\n\t</span></span>\
          \ I am using LLaMA_Factory which uses native hf transformer or accelerate\
          \ for training</p>\n<pre><code>#!/bin/bash\n\neval \"$(conda shell.bash\
          \ hook)\"\nconda activate llama_factory\n\nMODEL_NAME=phi-2\nSTAGE=sft\n\
          EPOCH=.01 <a href=\"/microsoft/phi-2/discussions/3\">#3</a>.0\nDATA=alpaca_gpt4_zh\n\
          SAVE_PATH=./models/$STAGE/$MODEL_NAME-$STAGE-$DATA-$EPOCH\nSAVE_PATH_PREDICT=$SAVE_PATH/Predict\n\
          MODEL_PATH=./models/$MODEL_NAME\nLoRA_TARGET=Wqkv #q_proj,v_proj\nTEMPLATE=default\n\
          PREDICTION_SAMPLES=20\n\nif [ ! -d $MODEL_PATH ]; then\n    echo \"Model\
          \ not found: $MODEL_PATH\"\n    return 1\nfi\n\nif [ ! -d $SAVE_PATH ];\
          \ then\n    mkdir -p $SAVE_PATH\nfi\n\nif [ ! -d $SAVE_PATH_PREDICT ]; then\n\
          \    mkdir -p $SAVE_PATH_PREDICT\nfi\n\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py\
          \ \\\n    --seed 42 \\\n    --stage $STAGE \\\n    --model_name_or_path\
          \ $MODEL_PATH \\\n    --dataset $DATA \\\n    --val_size .1 \\\n    --template\
          \ $TEMPLATE \\\n    --finetuning_type lora \\\n    --do_train \\\n    --lora_target\
          \ $LoRA_TARGET \\\n    --output_dir $SAVE_PATH \\\n    --overwrite_output_dir\
          \ \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 1 \\\n\
          \    --gradient_accumulation_steps 4 \\\n    --lr_scheduler_type cosine\
          \ \\\n    --logging_steps 10 \\\n    --save_steps 1000 \\\n    --learning_rate\
          \ 5e-5 \\\n    --num_train_epochs $EPOCH \\\n    --do_eval \\\n    --evaluation_strategy\
          \ steps \\\n    --per_device_eval_batch_size 1 \\\n    --prediction_loss_only\
          \ \\\n    --plot_loss \\\n    --quantization_bit 4 \\\n    |&amp; tee $SAVE_PATH/train_eval_log.txt\n\
          \nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --stage $STAGE\
          \ \\\n    --model_name_or_path $MODEL_PATH \\\n    --do_predict \\\n   \
          \ --max_samples $PREDICTION_SAMPLES \\\n    --predict_with_generate \\\n\
          \    --dataset $DATA \\\n    --template $TEMPLATE \\\n    --finetuning_type\
          \ lora \\\n    --adapter_name_or_path $SAVE_PATH \\\n    --output_dir $SAVE_PATH_PREDICT\
          \ \\\n    --per_device_eval_batch_size 1 \\\n    |&amp; tee $SAVE_PATH_PREDICT/predict_log.txt\n\
          </code></pre>\n"
        raw: "@Navanit-shorthills I am using LLaMA_Factory which uses native hf transformer\
          \ or accelerate for training\n\n```\n#!/bin/bash\n\neval \"$(conda shell.bash\
          \ hook)\"\nconda activate llama_factory\n\nMODEL_NAME=phi-2\nSTAGE=sft\n\
          EPOCH=.01 #3.0\nDATA=alpaca_gpt4_zh\nSAVE_PATH=./models/$STAGE/$MODEL_NAME-$STAGE-$DATA-$EPOCH\n\
          SAVE_PATH_PREDICT=$SAVE_PATH/Predict\nMODEL_PATH=./models/$MODEL_NAME\n\
          LoRA_TARGET=Wqkv #q_proj,v_proj\nTEMPLATE=default\nPREDICTION_SAMPLES=20\n\
          \nif [ ! -d $MODEL_PATH ]; then\n    echo \"Model not found: $MODEL_PATH\"\
          \n    return 1\nfi\n\nif [ ! -d $SAVE_PATH ]; then\n    mkdir -p $SAVE_PATH\n\
          fi\n\nif [ ! -d $SAVE_PATH_PREDICT ]; then\n    mkdir -p $SAVE_PATH_PREDICT\n\
          fi\n\nCUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n    --seed 42\
          \ \\\n    --stage $STAGE \\\n    --model_name_or_path $MODEL_PATH \\\n \
          \   --dataset $DATA \\\n    --val_size .1 \\\n    --template $TEMPLATE \\\
          \n    --finetuning_type lora \\\n    --do_train \\\n    --lora_target $LoRA_TARGET\
          \ \\\n    --output_dir $SAVE_PATH \\\n    --overwrite_output_dir \\\n  \
          \  --overwrite_cache \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps\
          \ 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n \
          \   --save_steps 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs\
          \ $EPOCH \\\n    --do_eval \\\n    --evaluation_strategy steps \\\n    --per_device_eval_batch_size\
          \ 1 \\\n    --prediction_loss_only \\\n    --plot_loss \\\n    --quantization_bit\
          \ 4 \\\n    |& tee $SAVE_PATH/train_eval_log.txt\n\nCUDA_VISIBLE_DEVICES=0\
          \ python src/train_bash.py \\\n    --stage $STAGE \\\n    --model_name_or_path\
          \ $MODEL_PATH \\\n    --do_predict \\\n    --max_samples $PREDICTION_SAMPLES\
          \ \\\n    --predict_with_generate \\\n    --dataset $DATA \\\n    --template\
          \ $TEMPLATE \\\n    --finetuning_type lora \\\n    --adapter_name_or_path\
          \ $SAVE_PATH \\\n    --output_dir $SAVE_PATH_PREDICT \\\n    --per_device_eval_batch_size\
          \ 1 \\\n    |& tee $SAVE_PATH_PREDICT/predict_log.txt\n\n```"
        updatedAt: '2023-12-17T03:12:16.210Z'
      numEdits: 0
      reactions: []
    id: 657e6710a982e9093f839521
    type: comment
  author: Yhyu13
  content: "@Navanit-shorthills I am using LLaMA_Factory which uses native hf transformer\
    \ or accelerate for training\n\n```\n#!/bin/bash\n\neval \"$(conda shell.bash\
    \ hook)\"\nconda activate llama_factory\n\nMODEL_NAME=phi-2\nSTAGE=sft\nEPOCH=.01\
    \ #3.0\nDATA=alpaca_gpt4_zh\nSAVE_PATH=./models/$STAGE/$MODEL_NAME-$STAGE-$DATA-$EPOCH\n\
    SAVE_PATH_PREDICT=$SAVE_PATH/Predict\nMODEL_PATH=./models/$MODEL_NAME\nLoRA_TARGET=Wqkv\
    \ #q_proj,v_proj\nTEMPLATE=default\nPREDICTION_SAMPLES=20\n\nif [ ! -d $MODEL_PATH\
    \ ]; then\n    echo \"Model not found: $MODEL_PATH\"\n    return 1\nfi\n\nif [\
    \ ! -d $SAVE_PATH ]; then\n    mkdir -p $SAVE_PATH\nfi\n\nif [ ! -d $SAVE_PATH_PREDICT\
    \ ]; then\n    mkdir -p $SAVE_PATH_PREDICT\nfi\n\nCUDA_VISIBLE_DEVICES=0 python\
    \ src/train_bash.py \\\n    --seed 42 \\\n    --stage $STAGE \\\n    --model_name_or_path\
    \ $MODEL_PATH \\\n    --dataset $DATA \\\n    --val_size .1 \\\n    --template\
    \ $TEMPLATE \\\n    --finetuning_type lora \\\n    --do_train \\\n    --lora_target\
    \ $LoRA_TARGET \\\n    --output_dir $SAVE_PATH \\\n    --overwrite_output_dir\
    \ \\\n    --overwrite_cache \\\n    --per_device_train_batch_size 1 \\\n    --gradient_accumulation_steps\
    \ 4 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 10 \\\n    --save_steps\
    \ 1000 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs $EPOCH \\\n   \
    \ --do_eval \\\n    --evaluation_strategy steps \\\n    --per_device_eval_batch_size\
    \ 1 \\\n    --prediction_loss_only \\\n    --plot_loss \\\n    --quantization_bit\
    \ 4 \\\n    |& tee $SAVE_PATH/train_eval_log.txt\n\nCUDA_VISIBLE_DEVICES=0 python\
    \ src/train_bash.py \\\n    --stage $STAGE \\\n    --model_name_or_path $MODEL_PATH\
    \ \\\n    --do_predict \\\n    --max_samples $PREDICTION_SAMPLES \\\n    --predict_with_generate\
    \ \\\n    --dataset $DATA \\\n    --template $TEMPLATE \\\n    --finetuning_type\
    \ lora \\\n    --adapter_name_or_path $SAVE_PATH \\\n    --output_dir $SAVE_PATH_PREDICT\
    \ \\\n    --per_device_eval_batch_size 1 \\\n    |& tee $SAVE_PATH_PREDICT/predict_log.txt\n\
    \n```"
  created_at: 2023-12-17 03:12:16+00:00
  edited: false
  hidden: false
  id: 657e6710a982e9093f839521
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f72b4e15c6b1743ef88c6bd9eb5d8de9.svg
      fullname: Miguel Carvalho
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: miguelcarv
      type: user
    createdAt: '2023-12-18T20:28:39.000Z'
    data:
      edited: false
      editors:
      - miguelcarv
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9934070110321045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f72b4e15c6b1743ef88c6bd9eb5d8de9.svg
          fullname: Miguel Carvalho
          isHf: false
          isPro: false
          name: miguelcarv
          type: user
        html: '<p>Have you been able to figure out why this is happening?</p>

          '
        raw: Have you been able to figure out why this is happening?
        updatedAt: '2023-12-18T20:28:39.721Z'
      numEdits: 0
      reactions: []
    id: 6580ab77035c028f3327950c
    type: comment
  author: miguelcarv
  content: Have you been able to figure out why this is happening?
  created_at: 2023-12-18 20:28:39+00:00
  edited: false
  hidden: false
  id: 6580ab77035c028f3327950c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-20T13:02:00.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9445062279701233
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>This could be related to not having gradient checkpointing implemented.</p>

          '
        raw: This could be related to not having gradient checkpointing implemented.
        updatedAt: '2023-12-20T13:02:00.255Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Incomple
    id: 6582e5c8dd48a7c80ad3756e
    type: comment
  author: gugarosa
  content: This could be related to not having gradient checkpointing implemented.
  created_at: 2023-12-20 13:02:00+00:00
  edited: false
  hidden: false
  id: 6582e5c8dd48a7c80ad3756e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-03T14:22:43.000Z'
    data:
      status: closed
    id: 65956db374e471ef4a170047
    type: status-change
  author: gugarosa
  created_at: 2024-01-03 14:22:43+00:00
  id: 65956db374e471ef4a170047
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cae04cbfdd1a7ba11599e9fcbb594233.svg
      fullname: keivan alizadeh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: k1al
      type: user
    createdAt: '2024-01-23T21:46:40.000Z'
    data:
      edited: false
      editors:
      - k1al
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9521966576576233
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cae04cbfdd1a7ba11599e9fcbb594233.svg
          fullname: keivan alizadeh
          isHf: false
          isPro: false
          name: k1al
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> Gradient checkpointing\
          \ will probably improve ~20% . This seems to be more fundamental issue.\
          \ I still have the same issue. I would appreciate it if you guys can find\
          \ a fix </p>\n"
        raw: '@gugarosa Gradient checkpointing will probably improve ~20% . This seems
          to be more fundamental issue. I still have the same issue. I would appreciate
          it if you guys can find a fix '
        updatedAt: '2024-01-23T21:46:40.193Z'
      numEdits: 0
      reactions: []
    id: 65b033c0bd418aab3a8773cd
    type: comment
  author: k1al
  content: '@gugarosa Gradient checkpointing will probably improve ~20% . This seems
    to be more fundamental issue. I still have the same issue. I would appreciate
    it if you guys can find a fix '
  created_at: 2024-01-23 21:46:40+00:00
  edited: false
  hidden: false
  id: 65b033c0bd418aab3a8773cd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: finetuning 2B model taking more gpu than 7B parameter model
