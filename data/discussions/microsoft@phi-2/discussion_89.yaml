!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rasyosef
conflicting_files: null
created_at: 2024-01-17 21:35:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
      fullname: Yosef Worku Alemneh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rasyosef
      type: user
    createdAt: '2024-01-17T21:35:52.000Z'
    data:
      edited: false
      editors:
      - rasyosef
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.649691641330719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
          fullname: Yosef Worku Alemneh
          isHf: false
          isPro: false
          name: rasyosef
          type: user
        html: "<p>The model was working fine until a couple of hours ago, then it\
          \ started generating a bunch of \"!!!!!!!!!!!!!!!!!!!!!\" no matter the\
          \ input text. To my knowledge, this issue is only present with FP16 inference,\
          \ but even the sample code in your model card replicates this problem since\
          \ <code>torch_dtype=\"auto\"</code> defaults to <code>torch.float16</code>\
          \ .</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          \ntorch.set_default_device(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\"\
          >\"microsoft/phi-2\"</span>, torch_dtype=<span class=\"hljs-string\">\"\
          auto\"</span>, trust_remote_code=<span class=\"hljs-literal\">True</span>)\n\
          tokenizer = AutoTokenizer.from_pretrained(<span class=\"hljs-string\">\"\
          microsoft/phi-2\"</span>, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>)\n\ninputs = tokenizer(<span class=\"hljs-string\">'''def print_prime(n):</span>\n\
          <span class=\"hljs-string\">   \"\"\"</span>\n<span class=\"hljs-string\"\
          >   Print all primes between 1 and n</span>\n<span class=\"hljs-string\"\
          >   \"\"\"'''</span>, return_tensors=<span class=\"hljs-string\">\"pt\"\
          </span>, return_attention_mask=<span class=\"hljs-literal\">False</span>)\n\
          \noutputs = model.generate(**inputs, max_length=<span class=\"hljs-number\"\
          >200</span>)\ntext = tokenizer.batch_decode(outputs)[<span class=\"hljs-number\"\
          >0</span>]\n<span class=\"hljs-built_in\">print</span>(text)\n</code></pre>\n\
          <p>Output:</p>\n<pre><code>def print_prime(n):\n   \"\"\"\n   Print all\
          \ primes between 1 and n\n   \"\"\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n\
          </code></pre>\n"
        raw: "The model was working fine until a couple of hours ago, then it started\
          \ generating a bunch of \"!!!!!!!!!!!!!!!!!!!!!\" no matter the input text.\
          \ To my knowledge, this issue is only present with FP16 inference, but even\
          \ the sample code in your model card replicates this problem since `torch_dtype=\"\
          auto\"` defaults to `torch.float16` .\r\n\r\n```python\r\nimport torch\r\
          \nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntorch.set_default_device(\"\
          cuda\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\"\
          , torch_dtype=\"auto\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-2\", trust_remote_code=True)\r\n\r\ninputs = tokenizer('''def\
          \ print_prime(n):\r\n   \"\"\"\r\n   Print all primes between 1 and n\r\n\
          \   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\r\n\r\
          \noutputs = model.generate(**inputs, max_length=200)\r\ntext = tokenizer.batch_decode(outputs)[0]\r\
          \nprint(text)\r\n```\r\n\r\nOutput:\r\n```\r\ndef print_prime(n):\r\n  \
          \ \"\"\"\r\n   Print all primes between 1 and n\r\n   \"\"\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\
          \n```"
        updatedAt: '2024-01-17T21:35:52.081Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F614"
        users:
        - dongDS
        - Dongchan
      - count: 2
        reaction: "\U0001F44D"
        users:
        - ajindal1
        - kuimoani
    id: 65a84838fa00b7c14670f989
    type: comment
  author: rasyosef
  content: "The model was working fine until a couple of hours ago, then it started\
    \ generating a bunch of \"!!!!!!!!!!!!!!!!!!!!!\" no matter the input text. To\
    \ my knowledge, this issue is only present with FP16 inference, but even the sample\
    \ code in your model card replicates this problem since `torch_dtype=\"auto\"\
    ` defaults to `torch.float16` .\r\n\r\n```python\r\nimport torch\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\ntorch.set_default_device(\"\
    cuda\")\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\"\
    , torch_dtype=\"auto\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-2\", trust_remote_code=True)\r\n\r\ninputs = tokenizer('''def print_prime(n):\r\
    \n   \"\"\"\r\n   Print all primes between 1 and n\r\n   \"\"\"''', return_tensors=\"\
    pt\", return_attention_mask=False)\r\n\r\noutputs = model.generate(**inputs, max_length=200)\r\
    \ntext = tokenizer.batch_decode(outputs)[0]\r\nprint(text)\r\n```\r\n\r\nOutput:\r\
    \n```\r\ndef print_prime(n):\r\n   \"\"\"\r\n   Print all primes between 1 and\
    \ n\r\n   \"\"\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\r\
    \n```"
  created_at: 2024-01-17 21:35:52+00:00
  edited: false
  hidden: false
  id: 65a84838fa00b7c14670f989
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
      fullname: Yosef Worku Alemneh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rasyosef
      type: user
    createdAt: '2024-01-17T21:44:07.000Z'
    data:
      edited: false
      editors:
      - rasyosef
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6834409236907959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
          fullname: Yosef Worku Alemneh
          isHf: false
          isPro: false
          name: rasyosef
          type: user
        html: "<p>Here's another example, </p>\n<pre><code class=\"language-python\"\
          >inputs = tokenizer(\n    <span class=\"hljs-string\">'''Write a detailed\
          \ analogy between mathematics and a lighthouse.\\n'''</span>, \n    return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>, \n    return_attention_mask=<span\
          \ class=\"hljs-literal\">False</span>\n)\n\noutputs = model.generate(**inputs,\
          \ max_length=<span class=\"hljs-number\">200</span>)\ntext = tokenizer.batch_decode(outputs)[<span\
          \ class=\"hljs-number\">0</span>]\n<span class=\"hljs-built_in\">print</span>(text)\n\
          </code></pre>\n<p>Output:</p>\n<p><code>Write a detailed analogy between\
          \ mathematics and a lighthouse. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</code></p>\n\
          <p>All of the newly generated tokens are just a bunch of \"!!!!!!!!!!!!!!!!!!!!!!!...\"\
          </p>\n"
        raw: "Here's another example, \n```python\ninputs = tokenizer(\n    '''Write\
          \ a detailed analogy between mathematics and a lighthouse.\\n''', \n   \
          \ return_tensors=\"pt\", \n    return_attention_mask=False\n)\n\noutputs\
          \ = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
          print(text)\n```\n\nOutput:\n```Write a detailed analogy between mathematics\
          \ and a lighthouse.\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!```\n\
          \nAll of the newly generated tokens are just a bunch of \"!!!!!!!!!!!!!!!!!!!!!!!...\""
        updatedAt: '2024-01-17T21:44:07.097Z'
      numEdits: 0
      reactions: []
    id: 65a84a27b33c64c60e92c0ed
    type: comment
  author: rasyosef
  content: "Here's another example, \n```python\ninputs = tokenizer(\n    '''Write\
    \ a detailed analogy between mathematics and a lighthouse.\\n''', \n    return_tensors=\"\
    pt\", \n    return_attention_mask=False\n)\n\noutputs = model.generate(**inputs,\
    \ max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n```\n\
    \nOutput:\n```Write a detailed analogy between mathematics and a lighthouse.\n\
    !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!```\n\
    \nAll of the newly generated tokens are just a bunch of \"!!!!!!!!!!!!!!!!!!!!!!!...\""
  created_at: 2024-01-17 21:44:07+00:00
  edited: false
  hidden: false
  id: 65a84a27b33c64c60e92c0ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
      fullname: Jairo Luciano Dias Alves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jairoalves
      type: user
    createdAt: '2024-01-17T22:27:09.000Z'
    data:
      edited: false
      editors:
      - jairoalves
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9446887373924255
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
          fullname: Jairo Luciano Dias Alves
          isHf: false
          isPro: false
          name: jairoalves
          type: user
        html: '<p>same here. When using as pipeline for text completion it still answers,
          btw.</p>

          '
        raw: same here. When using as pipeline for text completion it still answers,
          btw.
        updatedAt: '2024-01-17T22:27:09.180Z'
      numEdits: 0
      reactions: []
    id: 65a8543dd189ae4627a820ae
    type: comment
  author: jairoalves
  content: same here. When using as pipeline for text completion it still answers,
    btw.
  created_at: 2024-01-17 22:27:09+00:00
  edited: false
  hidden: false
  id: 65a8543dd189ae4627a820ae
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5bdadb1fd3a1ba145425aea7ff95452c.svg
      fullname: Eric Schmitt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eschmitt88
      type: user
    createdAt: '2024-01-17T23:56:18.000Z'
    data:
      edited: false
      editors:
      - eschmitt88
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9726507663726807
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5bdadb1fd3a1ba145425aea7ff95452c.svg
          fullname: Eric Schmitt
          isHf: false
          isPro: false
          name: eschmitt88
          type: user
        html: '<p>Same here. Does someone have sample code where it doesn''t print
          out only "!"?</p>

          <p>Thanks!</p>

          '
        raw: 'Same here. Does someone have sample code where it doesn''t print out
          only "!"?


          Thanks!'
        updatedAt: '2024-01-17T23:56:18.669Z'
      numEdits: 0
      reactions: []
    id: 65a86922d189ae4627af55a3
    type: comment
  author: eschmitt88
  content: 'Same here. Does someone have sample code where it doesn''t print out only
    "!"?


    Thanks!'
  created_at: 2024-01-17 23:56:18+00:00
  edited: false
  hidden: false
  id: 65a86922d189ae4627af55a3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
      fullname: Yosef Worku Alemneh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rasyosef
      type: user
    createdAt: '2024-01-18T01:17:37.000Z'
    data:
      edited: false
      editors:
      - rasyosef
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4767710864543915
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
          fullname: Yosef Worku Alemneh
          isHf: false
          isPro: false
          name: rasyosef
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;eschmitt88&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/eschmitt88\">@<span class=\"\
          underline\">eschmitt88</span></a></span>\n\n\t</span></span> , If you already\
          \ have <code>accelerate</code> installed, you only need to change <code>torch_dtype=\"\
          auto\"</code> to  <code>device_map=\"auto\"</code> when loading the model\
          \ like so. </p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          \ntorch.set_default_device(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          \n<span class=\"hljs-comment\"># changed torch_dtype=\"auto\" to device_map=\"\
          auto\" in following line</span>\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/phi-2\"</span>, device_map=<span class=\"\
          hljs-string\">\"auto\"</span>, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>) \ntokenizer = AutoTokenizer.from_pretrained(<span class=\"\
          hljs-string\">\"microsoft/phi-2\"</span>, trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\n\ninputs = tokenizer(<span class=\"hljs-string\"\
          >'''def print_prime(n):</span>\n<span class=\"hljs-string\">   \"\"\"</span>\n\
          <span class=\"hljs-string\">   Print all primes between 1 and n</span>\n\
          <span class=\"hljs-string\">   \"\"\"'''</span>, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>, return_attention_mask=<span class=\"hljs-literal\"\
          >False</span>)\n\noutputs = model.generate(**inputs, max_length=<span class=\"\
          hljs-number\">200</span>)\ntext = tokenizer.batch_decode(outputs)[<span\
          \ class=\"hljs-number\">0</span>]\n<span class=\"hljs-built_in\">print</span>(text)\n\
          </code></pre>\n"
        raw: "@eschmitt88 , If you already have `accelerate` installed, you only need\
          \ to change `torch_dtype=\"auto\"` to  `device_map=\"auto\"` when loading\
          \ the model like so. \n\n```python\nimport torch\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\"\
          )\n\n# changed torch_dtype=\"auto\" to device_map=\"auto\" in following\
          \ line\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\"\
          , device_map=\"auto\", trust_remote_code=True) \ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-2\", trust_remote_code=True)\n\ninputs = tokenizer('''def\
          \ print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\
          \"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs\
          \ = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
          print(text)\n```"
        updatedAt: '2024-01-18T01:17:37.033Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - jairoalves
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - eschmitt88
    id: 65a87c312984fa7203ce8953
    type: comment
  author: rasyosef
  content: "@eschmitt88 , If you already have `accelerate` installed, you only need\
    \ to change `torch_dtype=\"auto\"` to  `device_map=\"auto\"` when loading the\
    \ model like so. \n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\n# changed torch_dtype=\"\
    auto\" to device_map=\"auto\" in following line\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-2\", device_map=\"auto\", trust_remote_code=True) \ntokenizer =\
    \ AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n\
    \ninputs = tokenizer('''def print_prime(n):\n   \"\"\"\n   Print all primes between\
    \ 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\
    \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
    print(text)\n```"
  created_at: 2024-01-18 01:17:37+00:00
  edited: false
  hidden: false
  id: 65a87c312984fa7203ce8953
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5bdadb1fd3a1ba145425aea7ff95452c.svg
      fullname: Eric Schmitt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: eschmitt88
      type: user
    createdAt: '2024-01-18T02:59:21.000Z'
    data:
      edited: false
      editors:
      - eschmitt88
      hidden: false
      identifiedLanguage:
        language: ceb
        probability: 0.2632627785205841
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5bdadb1fd3a1ba145425aea7ff95452c.svg
          fullname: Eric Schmitt
          isHf: false
          isPro: false
          name: eschmitt88
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;rasyosef&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rasyosef\">@<span class=\"\
          underline\">rasyosef</span></a></span>\n\n\t</span></span> thank you!</p>\n"
        raw: '@rasyosef thank you!'
        updatedAt: '2024-01-18T02:59:21.989Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - rasyosef
    id: 65a894091012559732a5c0aa
    type: comment
  author: eschmitt88
  content: '@rasyosef thank you!'
  created_at: 2024-01-18 02:59:21+00:00
  edited: false
  hidden: false
  id: 65a894091012559732a5c0aa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fcd6d71e80139877b1344c72db5f7990.svg
      fullname: yoda git master
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yodagitmaster
      type: user
    createdAt: '2024-01-18T08:51:04.000Z'
    data:
      edited: false
      editors:
      - yodagitmaster
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9594297409057617
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fcd6d71e80139877b1344c72db5f7990.svg
          fullname: yoda git master
          isHf: false
          isPro: false
          name: yodagitmaster
          type: user
        html: '<p>Same problem here, can someone explain a bit the origin of this
          issue? </p>

          '
        raw: "Same problem here, can someone explain a bit the origin of this issue?\
          \ \n"
        updatedAt: '2024-01-18T08:51:04.903Z'
      numEdits: 0
      reactions: []
    id: 65a8e678503be1d830e084f2
    type: comment
  author: yodagitmaster
  content: "Same problem here, can someone explain a bit the origin of this issue?\
    \ \n"
  created_at: 2024-01-18 08:51:04+00:00
  edited: false
  hidden: false
  id: 65a8e678503be1d830e084f2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
      fullname: Jairo Luciano Dias Alves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jairoalves
      type: user
    createdAt: '2024-01-18T11:26:26.000Z'
    data:
      edited: false
      editors:
      - jairoalves
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.92069011926651
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
          fullname: Jairo Luciano Dias Alves
          isHf: false
          isPro: false
          name: jairoalves
          type: user
        html: '<p>I think it was a patch to prevent errors in an "attention overflow
          issue (with FP16)" that requires autocast to be disabled.  as per this change
          record.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/65883a4d003ceee6932b16d3/OZWVdZ5LlbxIGheY_TqM1.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/65883a4d003ceee6932b16d3/OZWVdZ5LlbxIGheY_TqM1.png"></a></p>

          '
        raw: 'I think it was a patch to prevent errors in an "attention overflow issue
          (with FP16)" that requires autocast to be disabled.  as per this change
          record.


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65883a4d003ceee6932b16d3/OZWVdZ5LlbxIGheY_TqM1.png)

          '
        updatedAt: '2024-01-18T11:26:26.892Z'
      numEdits: 0
      reactions: []
    id: 65a90ae2ae3bd1cc030db171
    type: comment
  author: jairoalves
  content: 'I think it was a patch to prevent errors in an "attention overflow issue
    (with FP16)" that requires autocast to be disabled.  as per this change record.


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/65883a4d003ceee6932b16d3/OZWVdZ5LlbxIGheY_TqM1.png)

    '
  created_at: 2024-01-18 11:26:26+00:00
  edited: false
  hidden: false
  id: 65a90ae2ae3bd1cc030db171
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-18T11:29:52.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.913802444934845
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Could you please re-try with the latest commit?</p>

          <p>Unfortunately, for Phi-2 to work amongst all use cases, we need to upcast
          queries and keys to FP32, and disable the autocast in the attention''s forward
          pass.</p>

          '
        raw: 'Could you please re-try with the latest commit?


          Unfortunately, for Phi-2 to work amongst all use cases, we need to upcast
          queries and keys to FP32, and disable the autocast in the attention''s forward
          pass.'
        updatedAt: '2024-01-18T11:29:52.941Z'
      numEdits: 0
      reactions: []
    id: 65a90bb09db09e1fc172ee4f
    type: comment
  author: gugarosa
  content: 'Could you please re-try with the latest commit?


    Unfortunately, for Phi-2 to work amongst all use cases, we need to upcast queries
    and keys to FP32, and disable the autocast in the attention''s forward pass.'
  created_at: 2024-01-18 11:29:52+00:00
  edited: false
  hidden: false
  id: 65a90bb09db09e1fc172ee4f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
      fullname: Jairo Luciano Dias Alves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jairoalves
      type: user
    createdAt: '2024-01-18T12:02:24.000Z'
    data:
      edited: false
      editors:
      - jairoalves
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9502944350242615
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
          fullname: Jairo Luciano Dias Alves
          isHf: false
          isPro: false
          name: jairoalves
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> do you think\
          \ it is necessary to update the readme as well? Mainly to prevent people\
          \ not aware of the new behaviour running into issues and to adjust the provided\
          \ sample code (if needed)?</p>\n"
        raw: '@gugarosa do you think it is necessary to update the readme as well?
          Mainly to prevent people not aware of the new behaviour running into issues
          and to adjust the provided sample code (if needed)?'
        updatedAt: '2024-01-18T12:02:24.319Z'
      numEdits: 0
      reactions: []
    id: 65a91350eea6e76376acadce
    type: comment
  author: jairoalves
  content: '@gugarosa do you think it is necessary to update the readme as well? Mainly
    to prevent people not aware of the new behaviour running into issues and to adjust
    the provided sample code (if needed)?'
  created_at: 2024-01-18 12:02:24+00:00
  edited: false
  hidden: false
  id: 65a91350eea6e76376acadce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-18T12:12:48.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9849534034729004
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>I don''t think we need to update the readme.</p>

          <p>The goal is to ensure that the model works with any use case (as it was
          working prior to the integration with <code>transformers</code>'' source
          code).</p>

          '
        raw: 'I don''t think we need to update the readme.


          The goal is to ensure that the model works with any use case (as it was
          working prior to the integration with `transformers`'' source code).


          '
        updatedAt: '2024-01-18T12:12:48.156Z'
      numEdits: 0
      reactions: []
    id: 65a915c05d36223f7398f7cc
    type: comment
  author: gugarosa
  content: 'I don''t think we need to update the readme.


    The goal is to ensure that the model works with any use case (as it was working
    prior to the integration with `transformers`'' source code).


    '
  created_at: 2024-01-18 12:12:48+00:00
  edited: false
  hidden: false
  id: 65a915c05d36223f7398f7cc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
      fullname: Jairo Luciano Dias Alves
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jairoalves
      type: user
    createdAt: '2024-01-18T12:47:55.000Z'
    data:
      edited: false
      editors:
      - jairoalves
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9582427144050598
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2602fd2c3085c60b8edce2f79bffe6a9.svg
          fullname: Jairo Luciano Dias Alves
          isHf: false
          isPro: false
          name: jairoalves
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> </p>\n<p>I thought\
          \ we would have to change the <code>torch_dtype=\"auto\"</code> argument\
          \ to <code>device_map=\"auto\"</code> in the model definition line, as per\
          \ the <span data-props=\"{&quot;user&quot;:&quot;rasyosef&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rasyosef\">@<span class=\"\
          underline\">rasyosef</span></a></span>\n\n\t</span></span> post above. In\
          \ fact, yesterday, after I tried that, it solved the \"!!!!!!\" response\
          \ issue for me. In that case, the readme sample code would in fact be outdated.\
          \ </p>\n<p>However, I tested again today, with the new <code>modeling_phi.py</code>,\
          \ and it is no longer the case.<br>The readme sample code, with <code>torch_dtype=\"\
          auto\"</code>, is working fine again now.</p>\n"
        raw: "@gugarosa \n\nI thought we would have to change the `torch_dtype=\"\
          auto\"` argument to `device_map=\"auto\"` in the model definition line,\
          \ as per the @rasyosef post above. In fact, yesterday, after I tried that,\
          \ it solved the \"!!!!!!\" response issue for me. In that case, the readme\
          \ sample code would in fact be outdated. \n\nHowever, I tested again today,\
          \ with the new `modeling_phi.py`, and it is no longer the case.\nThe readme\
          \ sample code, with `torch_dtype=\"auto\"`, is working fine again now."
        updatedAt: '2024-01-18T12:47:55.969Z'
      numEdits: 0
      reactions: []
    id: 65a91dfb2984fa72031102fd
    type: comment
  author: jairoalves
  content: "@gugarosa \n\nI thought we would have to change the `torch_dtype=\"auto\"\
    ` argument to `device_map=\"auto\"` in the model definition line, as per the @rasyosef\
    \ post above. In fact, yesterday, after I tried that, it solved the \"!!!!!!\"\
    \ response issue for me. In that case, the readme sample code would in fact be\
    \ outdated. \n\nHowever, I tested again today, with the new `modeling_phi.py`,\
    \ and it is no longer the case.\nThe readme sample code, with `torch_dtype=\"\
    auto\"`, is working fine again now."
  created_at: 2024-01-18 12:47:55+00:00
  edited: false
  hidden: false
  id: 65a91dfb2984fa72031102fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
      fullname: Yosef Worku Alemneh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rasyosef
      type: user
    createdAt: '2024-01-18T12:53:25.000Z'
    data:
      edited: true
      editors:
      - rasyosef
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8937072157859802
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
          fullname: Yosef Worku Alemneh
          isHf: false
          isPro: false
          name: rasyosef
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> FP16 inference\
          \ is functioning correctly now, including the sample code from the model\
          \ card. Closing this issue.</p>\n"
        raw: '@gugarosa FP16 inference is functioning correctly now, including the
          sample code from the model card. Closing this issue.'
        updatedAt: '2024-01-18T12:53:59.190Z'
      numEdits: 1
      reactions: []
      relatedEventId: 65a91f456a7418d9af958c97
    id: 65a91f456a7418d9af958c91
    type: comment
  author: rasyosef
  content: '@gugarosa FP16 inference is functioning correctly now, including the sample
    code from the model card. Closing this issue.'
  created_at: 2024-01-18 12:53:25+00:00
  edited: true
  hidden: false
  id: 65a91f456a7418d9af958c91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3779adfcac8af21757ffcde75080c581.svg
      fullname: Yosef Worku Alemneh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rasyosef
      type: user
    createdAt: '2024-01-18T12:53:25.000Z'
    data:
      status: closed
    id: 65a91f456a7418d9af958c97
    type: status-change
  author: rasyosef
  created_at: 2024-01-18 12:53:25+00:00
  id: 65a91f456a7418d9af958c97
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-18T13:49:05.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9449498653411865
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ </p>\n<p>I thought we would have to change the <code>torch_dtype=\"auto\"\
          </code> argument to <code>device_map=\"auto\"</code> in the model definition\
          \ line, as per the <span data-props=\"{&quot;user&quot;:&quot;rasyosef&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rasyosef\"\
          >@<span class=\"underline\">rasyosef</span></a></span>\n\n\t</span></span>\
          \ post above. In fact, yesterday, after I tried that, it solved the \"!!!!!!\"\
          \ response issue for me. In that case, the readme sample code would in fact\
          \ be outdated. </p>\n<p>However, I tested again today, with the new <code>modeling_phi.py</code>,\
          \ and it is no longer the case.<br>The readme sample code, with <code>torch_dtype=\"\
          auto\"</code>, is working fine again now.</p>\n</blockquote>\n<p>Removing\
          \ <code>torch_dtype=\"auto\"</code> loads the model's weights in FP32, which\
          \ does not produce an overflow.</p>\n"
        raw: "> @gugarosa \n> \n> I thought we would have to change the `torch_dtype=\"\
          auto\"` argument to `device_map=\"auto\"` in the model definition line,\
          \ as per the @rasyosef post above. In fact, yesterday, after I tried that,\
          \ it solved the \"!!!!!!\" response issue for me. In that case, the readme\
          \ sample code would in fact be outdated. \n> \n> However, I tested again\
          \ today, with the new `modeling_phi.py`, and it is no longer the case.\n\
          > The readme sample code, with `torch_dtype=\"auto\"`, is working fine again\
          \ now.\n\nRemoving `torch_dtype=\"auto\"` loads the model's weights in FP32,\
          \ which does not produce an overflow."
        updatedAt: '2024-01-18T13:49:39.914Z'
      numEdits: 1
      reactions: []
    id: 65a92c51206a2a9fd05ec68e
    type: comment
  author: gugarosa
  content: "> @gugarosa \n> \n> I thought we would have to change the `torch_dtype=\"\
    auto\"` argument to `device_map=\"auto\"` in the model definition line, as per\
    \ the @rasyosef post above. In fact, yesterday, after I tried that, it solved\
    \ the \"!!!!!!\" response issue for me. In that case, the readme sample code would\
    \ in fact be outdated. \n> \n> However, I tested again today, with the new `modeling_phi.py`,\
    \ and it is no longer the case.\n> The readme sample code, with `torch_dtype=\"\
    auto\"`, is working fine again now.\n\nRemoving `torch_dtype=\"auto\"` loads the\
    \ model's weights in FP32, which does not produce an overflow."
  created_at: 2024-01-18 13:49:05+00:00
  edited: true
  hidden: false
  id: 65a92c51206a2a9fd05ec68e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 89
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: New tokens generated with FP16 inference are only exclamation marks "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
