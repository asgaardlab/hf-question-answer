!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Yhyu13
conflicting_files: null
created_at: 2023-12-23 16:09:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-12-23T16:09:28.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8227766156196594
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>Hi,</p>

          <p>When inferencing with vllm, I encoutnered this error :  <a rel="nofollow"
          href="https://github.com/vllm-project/vllm/issues/340">https://github.com/vllm-project/vllm/issues/340</a></p>

          <p>I found the vocab_size in <code>config.json</code> has shown a vocab
          size of 52100</p>

          <p>But by checking out <code>tokenizer.config</code>, the max token id is
          50294<br>And when counting number of tokens in <code>vocab.sjon</code> file,
          there is only 50257 tokens.</p>

          <p>I solved the previously mentioned vllm sampler error by limiting vocab_size
          from 52100 to 50257 in the <code>config.json</code> file</p>

          <p>Could any one explain which is the correctly number of tokens to use?</p>

          <p>THanks!</p>

          '
        raw: "Hi,\r\n\r\nWhen inferencing with vllm, I encoutnered this error :  https://github.com/vllm-project/vllm/issues/340\r\
          \n\r\nI found the vocab_size in `config.json` has shown a vocab size of\
          \ 52100\r\n\r\nBut by checking out `tokenizer.config`, the max token id\
          \ is 50294\r\nAnd when counting number of tokens in `vocab.sjon` file, there\
          \ is only 50257 tokens.\r\n\r\nI solved the previously mentioned vllm sampler\
          \ error by limiting vocab_size from 52100 to 50257 in the `config.json`\
          \ file\r\n\r\nCould any one explain which is the correctly number of tokens\
          \ to use?\r\n\r\nTHanks!"
        updatedAt: '2023-12-23T16:09:28.985Z'
      numEdits: 0
      reactions: []
    id: 658706382021ba68d768eedb
    type: comment
  author: Yhyu13
  content: "Hi,\r\n\r\nWhen inferencing with vllm, I encoutnered this error :  https://github.com/vllm-project/vllm/issues/340\r\
    \n\r\nI found the vocab_size in `config.json` has shown a vocab size of 52100\r\
    \n\r\nBut by checking out `tokenizer.config`, the max token id is 50294\r\nAnd\
    \ when counting number of tokens in `vocab.sjon` file, there is only 50257 tokens.\r\
    \n\r\nI solved the previously mentioned vllm sampler error by limiting vocab_size\
    \ from 52100 to 50257 in the `config.json` file\r\n\r\nCould any one explain which\
    \ is the correctly number of tokens to use?\r\n\r\nTHanks!"
  created_at: 2023-12-23 16:09:28+00:00
  edited: false
  hidden: false
  id: 658706382021ba68d768eedb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-09T18:15:16.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5191230773925781
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Using <code>self.sampler = Sampler(config.tokenizer_vocab_size)</code>
          will use the correct number of tokens we used to train Phi-2 (50295, from
          0 to 50294).</p>

          '
        raw: Using `self.sampler = Sampler(config.tokenizer_vocab_size)` will use
          the correct number of tokens we used to train Phi-2 (50295, from 0 to 50294).
        updatedAt: '2024-01-09T18:15:16.735Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659d8d3418dc7360290a4738
    id: 659d8d3418dc7360290a4734
    type: comment
  author: gugarosa
  content: Using `self.sampler = Sampler(config.tokenizer_vocab_size)` will use the
    correct number of tokens we used to train Phi-2 (50295, from 0 to 50294).
  created_at: 2024-01-09 18:15:16+00:00
  edited: false
  hidden: false
  id: 659d8d3418dc7360290a4734
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-09T18:15:16.000Z'
    data:
      status: closed
    id: 659d8d3418dc7360290a4738
    type: status-change
  author: gugarosa
  created_at: 2024-01-09 18:15:16+00:00
  id: 659d8d3418dc7360290a4738
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 43
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Tokenizer's vocab size and config.json's vocab_size mismatch!
