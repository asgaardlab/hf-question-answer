!!python/object:huggingface_hub.community.DiscussionWithDetails
author: HAvietisov
conflicting_files: null
created_at: 2023-12-13 21:51:36+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-12-13T21:51:36.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.737898588180542
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: "<p>Tried example from model page : </p>\n<pre><code>Instruct: Write\
          \ a detailed analogy between mathematics and a lighthouse.\nOutput:\n</code></pre>\n\
          <p>With following code : </p>\n<pre><code># Load model directly\n# Use a\
          \ pipeline as a high-level helper\nfrom transformers import pipeline\n\n\
          pipe = pipeline(\"text-generation\", model=\"microsoft/phi-2\")\nfrom transformers\
          \ import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\"\
          )\n\nimport time\nstart_time = time.time()\n\nprompt = \"\"\"Instruct: Write\
          \ a detailed analogy between mathematics and a lighthouse.\nOutput: \"\"\
          \"\n\n\n# Function to construct the prompt using the new system prompt template\n\
          def get_prompt_with_template(message: str) -&gt; str:\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\n\
          \nresponse = pipe(prompt)[0]['generated_text']\n\n\nend_time = time.time()\n\
          \ntotal_time = end_time - start_time\n\n\nprint(response)\n\nnum_tokens\
          \ = len(tokenizer.encode(response))\n\nprint(\"The script took\", total_time,\
          \ \"seconds to run.\")\nprint(\"Speed : \", num_tokens / total_time, \"\
          \ t/s\")\n</code></pre>\n<p>My outputs are : </p>\n<pre><code>Special tokens\
          \ have been added in the vocabulary, make sure the associated word embeddings\
          \ are fine-tuned or trained.\nSetting `pad_token_id` to `eos_token_id`:2\
          \ for open-end generation.\nInstruct: Write a detailed analogy between mathematics\
          \ and a lighthouse.\nOutput:  HareRay NobelRay\nThe script took 1.9523024559020996\
          \ seconds to run.\nSpeed :  10.756530032789689  t/s\n</code></pre>\n<p>Tried\
          \ multiple times on multiple different queries. Why is this so bad?</p>\n"
        raw: "Tried example from model page : \r\n```\r\nInstruct: Write a detailed\
          \ analogy between mathematics and a lighthouse.\r\nOutput:\r\n```\r\nWith\
          \ following code : \r\n```\r\n# Load model directly\r\n# Use a pipeline\
          \ as a high-level helper\r\nfrom transformers import pipeline\r\n\r\npipe\
          \ = pipeline(\"text-generation\", model=\"microsoft/phi-2\")\r\nfrom transformers\
          \ import AutoTokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\"\
          )\r\n\r\nimport time\r\nstart_time = time.time()\r\n\r\nprompt = \"\"\"\
          Instruct: Write a detailed analogy between mathematics and a lighthouse.\r\
          \nOutput: \"\"\"\r\n\r\n\r\n# Function to construct the prompt using the\
          \ new system prompt template\r\ndef get_prompt_with_template(message: str)\
          \ -> str:\r\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\r\
          \n\r\nresponse = pipe(prompt)[0]['generated_text']\r\n\r\n\r\nend_time =\
          \ time.time()\r\n\r\ntotal_time = end_time - start_time\r\n\r\n\r\nprint(response)\r\
          \n\r\nnum_tokens = len(tokenizer.encode(response))\r\n\r\nprint(\"The script\
          \ took\", total_time, \"seconds to run.\")\r\nprint(\"Speed : \", num_tokens\
          \ / total_time, \" t/s\")\r\n```\r\nMy outputs are : \r\n```\r\nSpecial\
          \ tokens have been added in the vocabulary, make sure the associated word\
          \ embeddings are fine-tuned or trained.\r\nSetting `pad_token_id` to `eos_token_id`:2\
          \ for open-end generation.\r\nInstruct: Write a detailed analogy between\
          \ mathematics and a lighthouse.\r\nOutput:  HareRay NobelRay\r\nThe script\
          \ took 1.9523024559020996 seconds to run.\r\nSpeed :  10.756530032789689\
          \  t/s\r\n```\r\nTried multiple times on multiple different queries. Why\
          \ is this so bad?"
        updatedAt: '2023-12-13T21:51:36.672Z'
      numEdits: 0
      reactions: []
    id: 657a276853e2fa36fec5a89e
    type: comment
  author: HAvietisov
  content: "Tried example from model page : \r\n```\r\nInstruct: Write a detailed\
    \ analogy between mathematics and a lighthouse.\r\nOutput:\r\n```\r\nWith following\
    \ code : \r\n```\r\n# Load model directly\r\n# Use a pipeline as a high-level\
    \ helper\r\nfrom transformers import pipeline\r\n\r\npipe = pipeline(\"text-generation\"\
    , model=\"microsoft/phi-2\")\r\nfrom transformers import AutoTokenizer\r\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\r\n\r\nimport time\r\n\
    start_time = time.time()\r\n\r\nprompt = \"\"\"Instruct: Write a detailed analogy\
    \ between mathematics and a lighthouse.\r\nOutput: \"\"\"\r\n\r\n\r\n# Function\
    \ to construct the prompt using the new system prompt template\r\ndef get_prompt_with_template(message:\
    \ str) -> str:\r\n    return SYSTEM_PROMPT_TEMPLATE.format(instruction=message)\r\
    \n\r\nresponse = pipe(prompt)[0]['generated_text']\r\n\r\n\r\nend_time = time.time()\r\
    \n\r\ntotal_time = end_time - start_time\r\n\r\n\r\nprint(response)\r\n\r\nnum_tokens\
    \ = len(tokenizer.encode(response))\r\n\r\nprint(\"The script took\", total_time,\
    \ \"seconds to run.\")\r\nprint(\"Speed : \", num_tokens / total_time, \" t/s\"\
    )\r\n```\r\nMy outputs are : \r\n```\r\nSpecial tokens have been added in the\
    \ vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\
    \nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\r\nInstruct:\
    \ Write a detailed analogy between mathematics and a lighthouse.\r\nOutput:  HareRay\
    \ NobelRay\r\nThe script took 1.9523024559020996 seconds to run.\r\nSpeed :  10.756530032789689\
    \  t/s\r\n```\r\nTried multiple times on multiple different queries. Why is this\
    \ so bad?"
  created_at: 2023-12-13 21:51:36+00:00
  edited: false
  hidden: false
  id: 657a276853e2fa36fec5a89e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-13T21:57:26.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.44393840432167053
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Please use <code>trust_remote_code=True</code> when loading the
          model:</p>

          <pre><code>pipeline("text-generation", model="microsoft/phi-2", trust_remote_code=True)

          </code></pre>

          '
        raw: 'Please use `trust_remote_code=True` when loading the model:


          ```

          pipeline("text-generation", model="microsoft/phi-2", trust_remote_code=True)

          ```'
        updatedAt: '2023-12-13T21:57:40.721Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - HAvietisov
    id: 657a28c69ad6bcff7e5014fe
    type: comment
  author: gugarosa
  content: 'Please use `trust_remote_code=True` when loading the model:


    ```

    pipeline("text-generation", model="microsoft/phi-2", trust_remote_code=True)

    ```'
  created_at: 2023-12-13 21:57:26+00:00
  edited: true
  hidden: false
  id: 657a28c69ad6bcff7e5014fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-12-13T21:59:57.000Z'
    data:
      edited: false
      editors:
      - HAvietisov
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.33554336428642273
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
          fullname: Hlib Avietisov
          isHf: false
          isPro: false
          name: HAvietisov
          type: user
        html: "<p>My bad. Thanks, <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ !</p>\n"
        raw: My bad. Thanks, @gugarosa !
        updatedAt: '2023-12-13T21:59:57.686Z'
      numEdits: 0
      reactions: []
    id: 657a295dbbb9afe5fb52d1be
    type: comment
  author: HAvietisov
  content: My bad. Thanks, @gugarosa !
  created_at: 2023-12-13 21:59:57+00:00
  edited: false
  hidden: false
  id: 657a295dbbb9afe5fb52d1be
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/PwaTO6BJYQ1Yjv6q4R2pL.jpeg?w=200&h=200&f=face
      fullname: Hlib Avietisov
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: HAvietisov
      type: user
    createdAt: '2023-12-13T22:00:05.000Z'
    data:
      status: closed
    id: 657a2965a5ebdfee3c1e2613
    type: status-change
  author: HAvietisov
  created_at: 2023-12-13 22:00:05+00:00
  id: 657a2965a5ebdfee3c1e2613
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Model outputs garbage for some reason
