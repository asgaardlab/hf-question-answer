!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fahadh4ilyas
conflicting_files: null
created_at: 2024-01-25 06:15:02+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
      fullname: Fahadh
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fahadh4ilyas
      type: user
    createdAt: '2024-01-25T06:15:02.000Z'
    data:
      edited: false
      editors:
      - fahadh4ilyas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9057486057281494
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bccdd2bb6c11d0315bd96da90eb46297.svg
          fullname: Fahadh
          isHf: false
          isPro: false
          name: fahadh4ilyas
          type: user
        html: '<p>Tokenizer vocab size is <code>50295</code> but embedding and head
          size is <code>51200</code>. is it intentional?</p>

          '
        raw: Tokenizer vocab size is `50295` but embedding and head size is `51200`.
          is it intentional?
        updatedAt: '2024-01-25T06:15:02.913Z'
      numEdits: 0
      reactions: []
    id: 65b1fc66b7aeef8fb1cd5799
    type: comment
  author: fahadh4ilyas
  content: Tokenizer vocab size is `50295` but embedding and head size is `51200`.
    is it intentional?
  created_at: 2024-01-25 06:15:02+00:00
  edited: false
  hidden: false
  id: 65b1fc66b7aeef8fb1cd5799
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b85b1958cf2cb1ee9d68fb4eaff81c30.svg
      fullname: Iliya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: iliyaML
      type: user
    createdAt: '2024-01-25T13:57:49.000Z'
    data:
      edited: false
      editors:
      - iliyaML
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9267135262489319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b85b1958cf2cb1ee9d68fb4eaff81c30.svg
          fullname: Iliya
          isHf: false
          isPro: false
          name: iliyaML
          type: user
        html: "<p>This is a good reference: <a href=\"https://huggingface.co/microsoft/phi-2/discussions/22#659d8ba950c1bbee5be6f179\"\
          >https://huggingface.co/microsoft/phi-2/discussions/22#659d8ba950c1bbee5be6f179</a></p>\n\
          <blockquote>\n<p>We ended up setting 51200 as the vocabulary size just to\
          \ accommodate any new tokens that we might need in the future. You can follow\
          \ <span data-props=\"{&quot;user&quot;:&quot;Deepakvictor&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Deepakvictor\">@<span\
          \ class=\"underline\">Deepakvictor</span></a></span>\n\n\t</span></span>\
          \ answer and it should fix the issue.</p>\n<p>As far as I know, no tokens\
          \ from 50295+ should be generated because those embeddings were not trained.\
          \ Though, depending on the generation's parameters, they could appear (low\
          \ probabilities however).</p>\n</blockquote>\n"
        raw: "This is a good reference: https://huggingface.co/microsoft/phi-2/discussions/22#659d8ba950c1bbee5be6f179\n\
          \n> We ended up setting 51200 as the vocabulary size just to accommodate\
          \ any new tokens that we might need in the future. You can follow @Deepakvictor\
          \ answer and it should fix the issue.\n> \n> As far as I know, no tokens\
          \ from 50295+ should be generated because those embeddings were not trained.\
          \ Though, depending on the generation's parameters, they could appear (low\
          \ probabilities however)."
        updatedAt: '2024-01-25T13:57:49.362Z'
      numEdits: 0
      reactions: []
    id: 65b268dd04eec72abf2f3141
    type: comment
  author: iliyaML
  content: "This is a good reference: https://huggingface.co/microsoft/phi-2/discussions/22#659d8ba950c1bbee5be6f179\n\
    \n> We ended up setting 51200 as the vocabulary size just to accommodate any new\
    \ tokens that we might need in the future. You can follow @Deepakvictor answer\
    \ and it should fix the issue.\n> \n> As far as I know, no tokens from 50295+\
    \ should be generated because those embeddings were not trained. Though, depending\
    \ on the generation's parameters, they could appear (low probabilities however)."
  created_at: 2024-01-25 13:57:49+00:00
  edited: false
  hidden: false
  id: 65b268dd04eec72abf2f3141
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 97
repo_id: microsoft/phi-2
repo_type: model
status: open
target_branch: null
title: Model token size is bigger than tokenizer size?
