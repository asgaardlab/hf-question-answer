!!python/object:huggingface_hub.community.DiscussionWithDetails
author: davinder01
conflicting_files: null
created_at: 2023-12-26 18:14:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0a1595bfca4196ed76fde26c8ab1f9d8.svg
      fullname: davinder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: davinder01
      type: user
    createdAt: '2023-12-26T18:14:20.000Z'
    data:
      edited: true
      editors:
      - davinder01
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40580853819847107
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0a1595bfca4196ed76fde26c8ab1f9d8.svg
          fullname: davinder
          isHf: false
          isPro: false
          name: davinder01
          type: user
        html: "<p>Its taking quite a lot time for simple prompt i.e around 95 seconds.<br>Device\
          \ : cpu<br>OS: MAC</p>\n<p>i.e.</p>\n<p>def LLM_generate(model, tokenizer,\
          \ prompt, length):</p>\n<pre><code>start_time = time.time()\n\ninputs =\
          \ tokenizer(prompt, return_tensors=\"pt\",\n                   return_attention_mask=False)\n\
          \nmodel_inputs = inputs.to(\"cpu\")\nmodel.to(\"cpu\")\n\ninput_token_len\
          \ = len(model_inputs.tokens())\noutputs = model.generate(\n    **model_inputs,\
          \ max_length=length if length &gt;= input_token_len else input_token_len)\n\
          print(f\"generated in {time.time() - start_time: .2f} seconds\")\nreturn\
          \ tokenizer.batch_decode(outputs)[0]\n</code></pre>\n<p>print(LLM_generate(model,\
          \ tokenizer, \"Give me a list of 13 words that have 9 letters.\", 128))</p>\n\
          <p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/63b58c320d5913eee482e2bf/mjWD3otITc9Ek-2gbi8yP.png\"\
          ><img alt=\"Screenshot 2023-12-26 at 11.51.30 PM.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/63b58c320d5913eee482e2bf/mjWD3otITc9Ek-2gbi8yP.png\"\
          ></a></p>\n"
        raw: "Its taking quite a lot time for simple prompt i.e around 95 seconds.\n\
          Device : cpu\nOS: MAC\n\ni.e.\n\ndef LLM_generate(model, tokenizer, prompt,\
          \ length):\n\n    start_time = time.time()\n\n    inputs = tokenizer(prompt,\
          \ return_tensors=\"pt\",\n                       return_attention_mask=False)\n\
          \n    model_inputs = inputs.to(\"cpu\")\n    model.to(\"cpu\")\n\n    input_token_len\
          \ = len(model_inputs.tokens())\n    outputs = model.generate(\n        **model_inputs,\
          \ max_length=length if length >= input_token_len else input_token_len)\n\
          \    print(f\"generated in {time.time() - start_time: .2f} seconds\")\n\
          \    return tokenizer.batch_decode(outputs)[0]\nprint(LLM_generate(model,\
          \ tokenizer, \"Give me a list of 13 words that have 9 letters.\", 128))\n\
          \n\n\n\n\n![Screenshot 2023-12-26 at 11.51.30 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63b58c320d5913eee482e2bf/mjWD3otITc9Ek-2gbi8yP.png)\n"
        updatedAt: '2023-12-26T18:24:07.500Z'
      numEdits: 4
      reactions: []
    id: 658b17fc8ff537204fa19e0e
    type: comment
  author: davinder01
  content: "Its taking quite a lot time for simple prompt i.e around 95 seconds.\n\
    Device : cpu\nOS: MAC\n\ni.e.\n\ndef LLM_generate(model, tokenizer, prompt, length):\n\
    \n    start_time = time.time()\n\n    inputs = tokenizer(prompt, return_tensors=\"\
    pt\",\n                       return_attention_mask=False)\n\n    model_inputs\
    \ = inputs.to(\"cpu\")\n    model.to(\"cpu\")\n\n    input_token_len = len(model_inputs.tokens())\n\
    \    outputs = model.generate(\n        **model_inputs, max_length=length if length\
    \ >= input_token_len else input_token_len)\n    print(f\"generated in {time.time()\
    \ - start_time: .2f} seconds\")\n    return tokenizer.batch_decode(outputs)[0]\n\
    print(LLM_generate(model, tokenizer, \"Give me a list of 13 words that have 9\
    \ letters.\", 128))\n\n\n\n\n\n![Screenshot 2023-12-26 at 11.51.30 PM.png](https://cdn-uploads.huggingface.co/production/uploads/63b58c320d5913eee482e2bf/mjWD3otITc9Ek-2gbi8yP.png)\n"
  created_at: 2023-12-26 18:14:20+00:00
  edited: true
  hidden: false
  id: 658b17fc8ff537204fa19e0e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-03T14:26:33.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9501190781593323
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;davinder01&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/davinder01\"\
          >@<span class=\"underline\">davinder01</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>I would say that value is reasonable since you are inferencing with a\
          \ CPU. Given that you are using a Mac, I would suggest try using the <code>mps</code>\
          \ device: <a href=\"https://huggingface.co/docs/accelerate/usage_guides/mps\"\
          >https://huggingface.co/docs/accelerate/usage_guides/mps</a>.</p>\n"
        raw: 'Hello @davinder01!


          I would say that value is reasonable since you are inferencing with a CPU.
          Given that you are using a Mac, I would suggest try using the `mps` device:
          https://huggingface.co/docs/accelerate/usage_guides/mps.'
        updatedAt: '2024-01-03T14:26:33.446Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65956e99e589f84445def483
    id: 65956e99e589f84445def47d
    type: comment
  author: gugarosa
  content: 'Hello @davinder01!


    I would say that value is reasonable since you are inferencing with a CPU. Given
    that you are using a Mac, I would suggest try using the `mps` device: https://huggingface.co/docs/accelerate/usage_guides/mps.'
  created_at: 2024-01-03 14:26:33+00:00
  edited: false
  hidden: false
  id: 65956e99e589f84445def47d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-03T14:26:33.000Z'
    data:
      status: closed
    id: 65956e99e589f84445def483
    type: status-change
  author: gugarosa
  created_at: 2024-01-03 14:26:33+00:00
  id: 65956e99e589f84445def483
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 49
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Taking ~95 seconds for simple prompt
