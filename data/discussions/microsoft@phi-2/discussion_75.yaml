!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Arigadam
conflicting_files: null
created_at: 2024-01-11 11:27:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Es1zMq6MoGdjy80-URTC6.png?w=200&h=200&f=face
      fullname: Arigadam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Arigadam
      type: user
    createdAt: '2024-01-11T11:27:17.000Z'
    data:
      edited: false
      editors:
      - Arigadam
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.910618007183075
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/Es1zMq6MoGdjy80-URTC6.png?w=200&h=200&f=face
          fullname: Arigadam
          isHf: false
          isPro: false
          name: Arigadam
          type: user
        html: '<p>I have a error:</p>

          <pre><code>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate
          76.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free.
          Of the allocated memory 7.22 GiB is allocated by PyTorch, and 43.56 MiB
          is reserved by PyTorch but unallocated. If reserved but unallocated memory
          is large try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF

          </code></pre>

          <p>How to fix?</p>

          '
        raw: "I have a error:\r\n```\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
          \ Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 8.00 GiB of\
          \ which 0 bytes is free. Of the allocated memory 7.22 GiB is allocated by\
          \ PyTorch, and 43.56 MiB is reserved by PyTorch but unallocated. If reserved\
          \ but unallocated memory is large try setting max_split_size_mb to avoid\
          \ fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n```\r\nHow to fix?"
        updatedAt: '2024-01-11T11:27:17.754Z'
      numEdits: 0
      reactions: []
    id: 659fd0959f682a3147dca7f8
    type: comment
  author: Arigadam
  content: "I have a error:\r\n```\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
    \ Tried to allocate 76.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which\
    \ 0 bytes is free. Of the allocated memory 7.22 GiB is allocated by PyTorch, and\
    \ 43.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
    \ memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n```\r\nHow to fix?"
  created_at: 2024-01-11 11:27:17+00:00
  edited: false
  hidden: false
  id: 659fd0959f682a3147dca7f8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631af7694ef8f5858dcf45c8/vpCklBDqTbBzVH7jqmvS9.jpeg?w=200&h=200&f=face
      fullname: Joey
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: joey00072
      type: user
    createdAt: '2024-01-11T20:34:54.000Z'
    data:
      edited: false
      editors:
      - joey00072
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6844624280929565
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/631af7694ef8f5858dcf45c8/vpCklBDqTbBzVH7jqmvS9.jpeg?w=200&h=200&f=face
          fullname: Joey
          isHf: false
          isPro: false
          name: joey00072
          type: user
        html: '<p>pass load_in_4bit=True  to lorad_pretrained</p>

          '
        raw: pass load_in_4bit=True  to lorad_pretrained
        updatedAt: '2024-01-11T20:34:54.397Z'
      numEdits: 0
      reactions: []
    id: 65a050ee19665f7549461b0d
    type: comment
  author: joey00072
  content: pass load_in_4bit=True  to lorad_pretrained
  created_at: 2024-01-11 20:34:54+00:00
  edited: false
  hidden: false
  id: 65a050ee19665f7549461b0d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T13:06:21.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7287158966064453
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Arigadam&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Arigadam\"\
          >@<span class=\"underline\">Arigadam</span></a></span>\n\n\t</span></span>!\
          \ Please use any quantization approach or try lowering the micro batch-size.</p>\n"
        raw: Hello @Arigadam! Please use any quantization approach or try lowering
          the micro batch-size.
        updatedAt: '2024-01-12T13:06:21.609Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65a1394d07184d32fa57334e
    id: 65a1394d07184d32fa57334d
    type: comment
  author: gugarosa
  content: Hello @Arigadam! Please use any quantization approach or try lowering the
    micro batch-size.
  created_at: 2024-01-12 13:06:21+00:00
  edited: false
  hidden: false
  id: 65a1394d07184d32fa57334d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T13:06:21.000Z'
    data:
      status: closed
    id: 65a1394d07184d32fa57334e
    type: status-change
  author: gugarosa
  created_at: 2024-01-12 13:06:21+00:00
  id: 65a1394d07184d32fa57334e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 75
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: '[Bug] I have a error'
