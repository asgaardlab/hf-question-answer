!!python/object:huggingface_hub.community.DiscussionWithDetails
author: irotem98
conflicting_files: null
created_at: 2023-12-15 16:00:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb50773ac49948940eb231834ee6f2fd.svg
      fullname: rotem israeli
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: irotem98
      type: user
    createdAt: '2023-12-15T16:00:50.000Z'
    data:
      edited: false
      editors:
      - irotem98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.43183454871177673
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb50773ac49948940eb231834ee6f2fd.svg
          fullname: rotem israeli
          isHf: false
          isPro: false
          name: irotem98
          type: user
        html: "<p>whatever i try i cant get the model to end the sequence.<br>here's\
          \ example code that i try to make it work</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\"\
          >from</span> transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(<span class=\"\
          hljs-string\">\"microsoft/phi-2\"</span>, torch_dtype=<span class=\"hljs-string\"\
          >\"auto\"</span>, flash_attn=<span class=\"hljs-literal\">True</span>, flash_rotary=<span\
          \ class=\"hljs-literal\">True</span>, fused_dense=<span class=\"hljs-literal\"\
          >True</span>, device_map=<span class=\"hljs-string\">\"cuda\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/phi-2\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\nmodel.<span class=\"hljs-built_in\"\
          >eval</span>()\n\n<span class=\"hljs-comment\"># Check EOS token configuration</span>\n\
          <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"\
          EOS Token ID: <span class=\"hljs-subst\">{model.config.eos_token_id}</span>\"\
          </span>)\n\n\nbase_prompt = <span class=\"hljs-string\">'''def print_prime(n):</span>\n\
          <span class=\"hljs-string\">   \"\"\"</span>\n<span class=\"hljs-string\"\
          >   Print all primes between 1 and n</span>\n<span class=\"hljs-string\"\
          >   \"\"\"</span>\n<span class=\"hljs-string\">'''</span>\n\ndevice = <span\
          \ class=\"hljs-string\">'cuda'</span>\n\n<span class=\"hljs-comment\">#\
          \ Set EOS token</span>\nmodel.config.eos_token_id = <span class=\"hljs-number\"\
          >50256</span> \ntokenizer.eos_token_id = model.config.eos_token_id\n\n<span\
          \ class=\"hljs-keyword\">with</span> torch.no_grad():\n    inputs = tokenizer(base_prompt,\
          \ return_tensors=<span class=\"hljs-string\">\"pt\"</span>).to(device)\n\
          \    <span class=\"hljs-keyword\">for</span> i <span class=\"hljs-keyword\"\
          >in</span> <span class=\"hljs-built_in\">range</span>(<span class=\"hljs-number\"\
          >2</span>):\n        output = model.generate(**inputs, \n              \
          \                  max_length=<span class=\"hljs-number\">200</span>, \n\
          \                                num_return_sequences=<span class=\"hljs-number\"\
          >1</span>, \n                                eos_token_id=model.config.eos_token_id,\n\
          \                                early_stopping=<span class=\"hljs-literal\"\
          >True</span>)\n        text = tokenizer.decode(output[<span class=\"hljs-number\"\
          >0</span>], skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n\
          \        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\"\
          >f\"Answer <span class=\"hljs-subst\">{i+<span class=\"hljs-number\">1</span>}</span>:\
          \ <span class=\"hljs-subst\">{text}</span>\"</span>)\n        <span class=\"\
          hljs-built_in\">print</span>(<span class=\"hljs-string\">'----------------'</span>)\n\
          </code></pre>\n"
        raw: "whatever i try i cant get the model to end the sequence.\r\nhere's example\
          \ code that i try to make it work\r\n\r\n```python\r\nimport torch\r\nfrom\
          \ transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"\
          auto\", flash_attn=True, flash_rotary=True, fused_dense=True, device_map=\"\
          cuda\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-2\", trust_remote_code=True)\r\nmodel.eval()\r\n\r\n# Check\
          \ EOS token configuration\r\nprint(f\"EOS Token ID: {model.config.eos_token_id}\"\
          )\r\n\r\n\r\nbase_prompt = '''def print_prime(n):\r\n   \"\"\"\r\n   Print\
          \ all primes between 1 and n\r\n   \"\"\"\r\n'''\r\n\r\ndevice = 'cuda'\r\
          \n\r\n# Set EOS token\r\nmodel.config.eos_token_id = 50256 \r\ntokenizer.eos_token_id\
          \ = model.config.eos_token_id\r\n\r\nwith torch.no_grad():\r\n    inputs\
          \ = tokenizer(base_prompt, return_tensors=\"pt\").to(device)\r\n    for\
          \ i in range(2):\r\n        output = model.generate(**inputs, \r\n     \
          \                           max_length=200, \r\n                       \
          \         num_return_sequences=1, \r\n                                eos_token_id=model.config.eos_token_id,\r\
          \n                                early_stopping=True)\r\n        text =\
          \ tokenizer.decode(output[0], skip_special_tokens=True)\r\n        print(f\"\
          Answer {i+1}: {text}\")\r\n        print('----------------')\r\n```"
        updatedAt: '2023-12-15T16:00:50.557Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - t0d4
    id: 657c783291915edb92140388
    type: comment
  author: irotem98
  content: "whatever i try i cant get the model to end the sequence.\r\nhere's example\
    \ code that i try to make it work\r\n\r\n```python\r\nimport torch\r\nfrom transformers\
    \ import AutoModelForCausalLM, AutoTokenizer\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-2\", torch_dtype=\"auto\", flash_attn=True, flash_rotary=True, fused_dense=True,\
    \ device_map=\"cuda\", trust_remote_code=True)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-2\", trust_remote_code=True)\r\nmodel.eval()\r\n\r\n# Check EOS\
    \ token configuration\r\nprint(f\"EOS Token ID: {model.config.eos_token_id}\"\
    )\r\n\r\n\r\nbase_prompt = '''def print_prime(n):\r\n   \"\"\"\r\n   Print all\
    \ primes between 1 and n\r\n   \"\"\"\r\n'''\r\n\r\ndevice = 'cuda'\r\n\r\n# Set\
    \ EOS token\r\nmodel.config.eos_token_id = 50256 \r\ntokenizer.eos_token_id =\
    \ model.config.eos_token_id\r\n\r\nwith torch.no_grad():\r\n    inputs = tokenizer(base_prompt,\
    \ return_tensors=\"pt\").to(device)\r\n    for i in range(2):\r\n        output\
    \ = model.generate(**inputs, \r\n                                max_length=200,\
    \ \r\n                                num_return_sequences=1, \r\n           \
    \                     eos_token_id=model.config.eos_token_id,\r\n            \
    \                    early_stopping=True)\r\n        text = tokenizer.decode(output[0],\
    \ skip_special_tokens=True)\r\n        print(f\"Answer {i+1}: {text}\")\r\n  \
    \      print('----------------')\r\n```"
  created_at: 2023-12-15 16:00:50+00:00
  edited: false
  hidden: false
  id: 657c783291915edb92140388
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8fc619f9eba435219b3292f4d828cd5.svg
      fullname: rochak chadha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rchadha134
      type: user
    createdAt: '2023-12-18T19:09:10.000Z'
    data:
      edited: false
      editors:
      - rchadha134
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8945987224578857
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8fc619f9eba435219b3292f4d828cd5.svg
          fullname: rochak chadha
          isHf: false
          isPro: false
          name: rchadha134
          type: user
        html: '<p>This is to be somewhat expected as it''s not a finetuned model.
          Does your output display the eos token? You may also need to play with the
          no_repeat_ngram_size, to prevent some repetition. Here''s a notebook that
          demonstrates it. For most prompts, it stops- <a rel="nofollow" href="https://colab.research.google.com/drive/12QSdpOqZx697YpmHiZ-SrrejFGAtXnOD?usp=sharing">https://colab.research.google.com/drive/12QSdpOqZx697YpmHiZ-SrrejFGAtXnOD?usp=sharing</a><br>Let
          me know if it works for you and if you find issues. </p>

          '
        raw: 'This is to be somewhat expected as it''s not a finetuned model. Does
          your output display the eos token? You may also need to play with the no_repeat_ngram_size,
          to prevent some repetition. Here''s a notebook that demonstrates it. For
          most prompts, it stops- https://colab.research.google.com/drive/12QSdpOqZx697YpmHiZ-SrrejFGAtXnOD?usp=sharing

          Let me know if it works for you and if you find issues. '
        updatedAt: '2023-12-18T19:09:10.420Z'
      numEdits: 0
      reactions: []
    id: 658098d67508d04097b5ab7a
    type: comment
  author: rchadha134
  content: 'This is to be somewhat expected as it''s not a finetuned model. Does your
    output display the eos token? You may also need to play with the no_repeat_ngram_size,
    to prevent some repetition. Here''s a notebook that demonstrates it. For most
    prompts, it stops- https://colab.research.google.com/drive/12QSdpOqZx697YpmHiZ-SrrejFGAtXnOD?usp=sharing

    Let me know if it works for you and if you find issues. '
  created_at: 2023-12-18 19:09:10+00:00
  edited: false
  hidden: false
  id: 658098d67508d04097b5ab7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-20T12:57:14.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8598777055740356
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;irotem98&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/irotem98\"\
          >@<span class=\"underline\">irotem98</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Since this is a base model, it is expected a continuous generation. You\
          \ could create a custom stopping criteria or fine-tune the model.</p>\n\
          <p>Regards,<br>Gustavo.</p>\n"
        raw: 'Hello @irotem98!


          Since this is a base model, it is expected a continuous generation. You
          could create a custom stopping criteria or fine-tune the model.


          Regards,

          Gustavo.'
        updatedAt: '2023-12-20T12:57:14.905Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6582e4aab5a0b8ec75f851c0
    id: 6582e4aab5a0b8ec75f851bd
    type: comment
  author: gugarosa
  content: 'Hello @irotem98!


    Since this is a base model, it is expected a continuous generation. You could
    create a custom stopping criteria or fine-tune the model.


    Regards,

    Gustavo.'
  created_at: 2023-12-20 12:57:14+00:00
  edited: false
  hidden: false
  id: 6582e4aab5a0b8ec75f851bd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-20T12:57:14.000Z'
    data:
      status: closed
    id: 6582e4aab5a0b8ec75f851c0
    type: status-change
  author: gugarosa
  created_at: 2023-12-20 12:57:14+00:00
  id: 6582e4aab5a0b8ec75f851c0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 21
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: EOS doesn't seems to work
