!!python/object:huggingface_hub.community.DiscussionWithDetails
author: BalajiVS
conflicting_files: null
created_at: 2024-01-16 18:06:43+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eab805c2621ee42931fe6c82beecc17f.svg
      fullname: Balaji VS
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: BalajiVS
      type: user
    createdAt: '2024-01-16T18:06:43.000Z'
    data:
      edited: false
      editors:
      - BalajiVS
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4658435881137848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eab805c2621ee42931fe6c82beecc17f.svg
          fullname: Balaji VS
          isHf: false
          isPro: false
          name: BalajiVS
          type: user
        html: '<p>Special tokens have been added in the vocabulary, make sure the
          associated word embeddings are fine-tuned or trained.<br>Loading checkpoint
          shards:   0%|          | 0/2 [00:12&lt;?, ?it/s]<br>Traceback (most recent
          call last):<br>  File "B:\Zoya\main.py", line 13, in <br>    model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2",
          device_map="auto", trust_remote_code=True, token=token)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "B:\Zoya\venv\Lib\site-packages\transformers\models\auto\auto_factory.py",
          line 561, in from_pretrained<br>    return model_class.from_pretrained(<br>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "B:\Zoya\venv\Lib\site-packages\transformers\modeling_utils.py", line 3706,
          in from_pretrained<br>    ) = cls._load_pretrained_model(<br>        ^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "B:\Zoya\venv\Lib\site-packages\transformers\modeling_utils.py", line 4116,
          in _load_pretrained_model<br>    new_error_msgs, offload_index, state_dict_index
          = _load_state_dict_into_meta_model(<br>                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "B:\Zoya\venv\Lib\site-packages\transformers\modeling_utils.py", line 755,
          in _load_state_dict_into_meta_model<br>    param = param.to(old_param.dtype)<br>            ^^^^^^^^^^^^^^^^^^^^^^^^^<br>  File
          "B:\Zoya\venv\Lib\site-packages\torch\utils_device.py", line 77, in <strong>torch_function</strong><br>    return
          func(*args, **kwargs)<br>           ^^^^^^^^^^^^^^^^^^^^^<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty
          of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.64 GiB is
          allocated by PyTorch, and 177.33 MiB is reserved by PyTorch but unallocated.
          If reserved but unallocated memory is large try setting max_split_size_mb
          to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>Process finished with exit code 1</p>

          '
        raw: "Special tokens have been added in the vocabulary, make sure the associated\
          \ word embeddings are fine-tuned or trained.\r\nLoading checkpoint shards:\
          \   0%|          | 0/2 [00:12<?, ?it/s]\r\nTraceback (most recent call last):\r\
          \n  File \"B:\\Zoya\\main.py\", line 13, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-2\", device_map=\"auto\", trust_remote_code=True, token=token)\r\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\transformers\\models\\auto\\\
          auto_factory.py\", line 561, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"B:\\Zoya\\venv\\Lib\\\
          site-packages\\transformers\\modeling_utils.py\", line 3706, in from_pretrained\r\
          \n    ) = cls._load_pretrained_model(\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 4116, in _load_pretrained_model\r\n    new_error_msgs, offload_index,\
          \ state_dict_index = _load_state_dict_into_meta_model(\r\n             \
          \                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py\"\
          , line 755, in _load_state_dict_into_meta_model\r\n    param = param.to(old_param.dtype)\r\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"B:\\Zoya\\venv\\Lib\\\
          site-packages\\torch\\utils\\_device.py\", line 77, in __torch_function__\r\
          \n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\
          \ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00\
          \ MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of\
          \ the allocated memory 6.64 GiB is allocated by PyTorch, and 177.33 MiB\
          \ is reserved by PyTorch but unallocated. If reserved but unallocated memory\
          \ is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nProcess finished\
          \ with exit code 1\r\n"
        updatedAt: '2024-01-16T18:06:43.768Z'
      numEdits: 0
      reactions: []
    id: 65a6c5b33d3c839408e1bba7
    type: comment
  author: BalajiVS
  content: "Special tokens have been added in the vocabulary, make sure the associated\
    \ word embeddings are fine-tuned or trained.\r\nLoading checkpoint shards:   0%|\
    \          | 0/2 [00:12<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File\
    \ \"B:\\Zoya\\main.py\", line 13, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-2\", device_map=\"auto\", trust_remote_code=True, token=token)\r\
    \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\"\
    , line 561, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 3706, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
    \n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\\
    transformers\\modeling_utils.py\", line 4116, in _load_pretrained_model\r\n  \
    \  new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\r\
    \n                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py\"\
    , line 755, in _load_state_dict_into_meta_model\r\n    param = param.to(old_param.dtype)\r\
    \n            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"B:\\Zoya\\venv\\Lib\\site-packages\\\
    torch\\utils\\_device.py\", line 77, in __torch_function__\r\n    return func(*args,\
    \ **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacty\
    \ of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.64 GiB is allocated\
    \ by PyTorch, and 177.33 MiB is reserved by PyTorch but unallocated. If reserved\
    \ but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.\
    \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\n\
    Process finished with exit code 1\r\n"
  created_at: 2024-01-16 18:06:43+00:00
  edited: false
  hidden: false
  id: 65a6c5b33d3c839408e1bba7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-19T13:59:54.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9152134656906128
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Your GPU does not enough memory to load the model directly with
          CUDA. You will need to load on the CPU or use any size reduction approach
          (e.g., quantization) to be able to load the memory in GPU.</p>

          <p>If this happens during fine-tuning, try lowering the batch size.</p>

          '
        raw: 'Your GPU does not enough memory to load the model directly with CUDA.
          You will need to load on the CPU or use any size reduction approach (e.g.,
          quantization) to be able to load the memory in GPU.


          If this happens during fine-tuning, try lowering the batch size.'
        updatedAt: '2024-01-19T13:59:54.826Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65aa805aa8f716b32ed039b4
    id: 65aa805aa8f716b32ed039a8
    type: comment
  author: gugarosa
  content: 'Your GPU does not enough memory to load the model directly with CUDA.
    You will need to load on the CPU or use any size reduction approach (e.g., quantization)
    to be able to load the memory in GPU.


    If this happens during fine-tuning, try lowering the batch size.'
  created_at: 2024-01-19 13:59:54+00:00
  edited: false
  hidden: false
  id: 65aa805aa8f716b32ed039a8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-19T13:59:54.000Z'
    data:
      status: closed
    id: 65aa805aa8f716b32ed039b4
    type: status-change
  author: gugarosa
  created_at: 2024-01-19 13:59:54+00:00
  id: 65aa805aa8f716b32ed039b4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 87
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: torch.CUDA.OutOfMemoryError while loading shards of Phi-2
