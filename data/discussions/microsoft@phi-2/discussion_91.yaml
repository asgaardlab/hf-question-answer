!!python/object:huggingface_hub.community.DiscussionWithDetails
author: yeniceriSGK
conflicting_files: null
created_at: 2024-01-19 10:56:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b43e61c2c4813556693e2cb3f1761abb.svg
      fullname: Shirgaonkar Saad
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yeniceriSGK
      type: user
    createdAt: '2024-01-19T10:56:38.000Z'
    data:
      edited: false
      editors:
      - yeniceriSGK
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7762835025787354
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b43e61c2c4813556693e2cb3f1761abb.svg
          fullname: Shirgaonkar Saad
          isHf: false
          isPro: false
          name: yeniceriSGK
          type: user
        html: '<p>Hey I want to know how to handle Context/ History in this model
          like we do in GPT API like this<br>messages = [<br>    {<br>        "role":
          "system",<br>        "content": "You are a friendly chatbot who always responds
          in the style of a pirate",<br>    },<br>    {"role": "user", "content":
          "How many helicopters can a human eat in one sitting?"},<br>]<br>But How
          to do this in Phi-2 Model I am using it like this only<br>sample = {"topic":"",
          "context":"","rephrased_query":""}<br>sample_prompt = f"Question: What is
          financial planning'': Task: Your task is to return a JSON that will analyze
          the topic, context and rephrsed query of the question and return in JSON
          format like this {sample}"<br>#Wrap the prompt using the right chat template</p>

          <h1 id="inst-questionn-prompt-inst">[INST] Question:\n {prompt} [/INST]</h1>

          <p>instruction = f"[INST] Question:\n {sample_prompt} [/INST] \n\nResponse:\n"<br>%%time<br>pipe
          = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)<br>result
          = pipe(instruction)<br>#Trim the response, remove instruction manually</p>

          '
        raw: "Hey I want to know how to handle Context/ History in this model like\
          \ we do in GPT API like this \r\nmessages = [\r\n    {\r\n        \"role\"\
          : \"system\",\r\n        \"content\": \"You are a friendly chatbot who always\
          \ responds in the style of a pirate\",\r\n    },\r\n    {\"role\": \"user\"\
          , \"content\": \"How many helicopters can a human eat in one sitting?\"\
          },\r\n]\r\nBut How to do this in Phi-2 Model I am using it like this only\r\
          \nsample = {\"topic\":\"\", \"context\":\"\",\"rephrased_query\":\"\"}\r\
          \nsample_prompt = f\"Question: What is financial planning': Task: Your task\
          \ is to return a JSON that will analyze the topic, context and rephrsed\
          \ query of the question and return in JSON format like this {sample}\"\r\
          \n#Wrap the prompt using the right chat template\r\n# [INST] Question:\\\
          n {prompt} [/INST]\r\ninstruction = f\"[INST] Question:\\n {sample_prompt}\
          \ [/INST] \\n\\nResponse:\\n\"\r\n%%time\r\npipe = pipeline(task=\"text-generation\"\
          , model=model, tokenizer=tokenizer, max_length=200)\r\nresult = pipe(instruction)\r\
          \n#Trim the response, remove instruction manually"
        updatedAt: '2024-01-19T10:56:38.666Z'
      numEdits: 0
      reactions: []
    id: 65aa55663e7c8dcbbe94c393
    type: comment
  author: yeniceriSGK
  content: "Hey I want to know how to handle Context/ History in this model like we\
    \ do in GPT API like this \r\nmessages = [\r\n    {\r\n        \"role\": \"system\"\
    ,\r\n        \"content\": \"You are a friendly chatbot who always responds in\
    \ the style of a pirate\",\r\n    },\r\n    {\"role\": \"user\", \"content\":\
    \ \"How many helicopters can a human eat in one sitting?\"},\r\n]\r\nBut How to\
    \ do this in Phi-2 Model I am using it like this only\r\nsample = {\"topic\":\"\
    \", \"context\":\"\",\"rephrased_query\":\"\"}\r\nsample_prompt = f\"Question:\
    \ What is financial planning': Task: Your task is to return a JSON that will analyze\
    \ the topic, context and rephrsed query of the question and return in JSON format\
    \ like this {sample}\"\r\n#Wrap the prompt using the right chat template\r\n#\
    \ [INST] Question:\\n {prompt} [/INST]\r\ninstruction = f\"[INST] Question:\\\
    n {sample_prompt} [/INST] \\n\\nResponse:\\n\"\r\n%%time\r\npipe = pipeline(task=\"\
    text-generation\", model=model, tokenizer=tokenizer, max_length=200)\r\nresult\
    \ = pipe(instruction)\r\n#Trim the response, remove instruction manually"
  created_at: 2024-01-19 10:56:38+00:00
  edited: false
  hidden: false
  id: 65aa55663e7c8dcbbe94c393
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f47b6fc12c9c2b7a93419b0f65de0a32.svg
      fullname: Rexford Nkansah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: khophi
      type: user
    createdAt: '2024-01-23T22:08:47.000Z'
    data:
      edited: false
      editors:
      - khophi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9218867421150208
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f47b6fc12c9c2b7a93419b0f65de0a32.svg
          fullname: Rexford Nkansah
          isHf: false
          isPro: false
          name: khophi
          type: user
        html: '<p>I''ll like to know this too. I currently do something like this,
          and the responses are all over the place</p>

          <p><code>Instruction: You are an AI assistant. Keep your responses concise.
          Always respond using markdown syntax. Context is as follows:\n\n{context}\n\n.
          Chat history is as follows:\n\n{chat_history}\n\n {question}\n Output:</code></p>

          '
        raw: 'I''ll like to know this too. I currently do something like this, and
          the responses are all over the place


          ```Instruction: You are an AI assistant. Keep your responses concise. Always
          respond using markdown syntax. Context is as follows:\n\n{context}\n\n.
          Chat history is as follows:\n\n{chat_history}\n\n {question}\n

          Output:```'
        updatedAt: '2024-01-23T22:08:47.716Z'
      numEdits: 0
      reactions: []
    id: 65b038ef70773c0ab8c7220f
    type: comment
  author: khophi
  content: 'I''ll like to know this too. I currently do something like this, and the
    responses are all over the place


    ```Instruction: You are an AI assistant. Keep your responses concise. Always respond
    using markdown syntax. Context is as follows:\n\n{context}\n\n. Chat history is
    as follows:\n\n{chat_history}\n\n {question}\n

    Output:```'
  created_at: 2024-01-23 22:08:47+00:00
  edited: false
  hidden: false
  id: 65b038ef70773c0ab8c7220f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 91
repo_id: microsoft/phi-2
repo_type: model
status: open
target_branch: null
title: How to handle Context In Phi-2 Model
