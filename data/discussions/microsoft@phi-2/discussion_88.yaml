!!python/object:huggingface_hub.community.DiscussionWithDetails
author: YaYaGeGe
conflicting_files: null
created_at: 2024-01-17 07:51:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
      fullname: Yang Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaYaGeGe
      type: user
    createdAt: '2024-01-17T07:51:41.000Z'
    data:
      edited: false
      editors:
      - YaYaGeGe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7357752323150635
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
          fullname: Yang Yang
          isHf: false
          isPro: false
          name: YaYaGeGe
          type: user
        html: '<p>The checkpoint was finetuned and saved under <code>transformers
          == 4.36.1</code> and phi-2 HF code revision <code><a href="/microsoft/phi-2/commit/834565c23f9b28b96ccbeabe614dd906b6db551a">834565c23f9b28b96ccbeabe614dd906b6db551a</a></code><br>After
          updating to the latest revision and re-installing transformers as a development
          package <code>transformers-4.37.0.dev0</code>, if the same checkpoint loaded,
          it predicts similar but different outputs. The checkpoint loading codeline:<br><code>AutoPeftModelForCausalLM.from_pretrained(ckpt_path,
          device_map=device, torch_dtype="auto", trust_remote_code=True)</code></p>

          '
        raw: "The checkpoint was finetuned and saved under `transformers == 4.36.1`\
          \ and phi-2 HF code revision `834565c23f9b28b96ccbeabe614dd906b6db551a`\r\
          \nAfter updating to the latest revision and re-installing transformers as\
          \ a development package `transformers-4.37.0.dev0`, if the same checkpoint\
          \ loaded, it predicts similar but different outputs. The checkpoint loading\
          \ codeline:\r\n```AutoPeftModelForCausalLM.from_pretrained(ckpt_path, device_map=device,\
          \ torch_dtype=\"auto\", trust_remote_code=True)```\r\n"
        updatedAt: '2024-01-17T07:51:41.368Z'
      numEdits: 0
      reactions: []
    id: 65a7870dae68caef0b94f596
    type: comment
  author: YaYaGeGe
  content: "The checkpoint was finetuned and saved under `transformers == 4.36.1`\
    \ and phi-2 HF code revision `834565c23f9b28b96ccbeabe614dd906b6db551a`\r\nAfter\
    \ updating to the latest revision and re-installing transformers as a development\
    \ package `transformers-4.37.0.dev0`, if the same checkpoint loaded, it predicts\
    \ similar but different outputs. The checkpoint loading codeline:\r\n```AutoPeftModelForCausalLM.from_pretrained(ckpt_path,\
    \ device_map=device, torch_dtype=\"auto\", trust_remote_code=True)```\r\n"
  created_at: 2024-01-17 07:51:41+00:00
  edited: false
  hidden: false
  id: 65a7870dae68caef0b94f596
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
      fullname: Yang Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaYaGeGe
      type: user
    createdAt: '2024-01-17T14:13:28.000Z'
    data:
      edited: false
      editors:
      - YaYaGeGe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9806663393974304
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
          fullname: Yang Yang
          isHf: false
          isPro: false
          name: YaYaGeGe
          type: user
        html: '<p>I just noticed the model layers'' name had been changed at the new
          update. Maybe something not matched? I didn''t dive into the details yet,
          wonder which tool is practical for network structure comparison purpose.</p>

          '
        raw: I just noticed the model layers' name had been changed at the new update.
          Maybe something not matched? I didn't dive into the details yet, wonder
          which tool is practical for network structure comparison purpose.
        updatedAt: '2024-01-17T14:13:28.907Z'
      numEdits: 0
      reactions: []
    id: 65a7e0889fa2a0f9a25b398d
    type: comment
  author: YaYaGeGe
  content: I just noticed the model layers' name had been changed at the new update.
    Maybe something not matched? I didn't dive into the details yet, wonder which
    tool is practical for network structure comparison purpose.
  created_at: 2024-01-17 14:13:28+00:00
  edited: false
  hidden: false
  id: 65a7e0889fa2a0f9a25b398d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-17T18:09:10.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8177942633628845
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>You should be able to convert your old checkpoint to the new format
          using this script: <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py</a>.</p>

          <p>After the conversion, the layers'' name will match back and the generation
          should be as expected.</p>

          '
        raw: 'You should be able to convert your old checkpoint to the new format
          using this script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py.


          After the conversion, the layers'' name will match back and the generation
          should be as expected.'
        updatedAt: '2024-01-17T18:09:10.111Z'
      numEdits: 0
      reactions: []
    id: 65a817c66a7418d9af386dec
    type: comment
  author: gugarosa
  content: 'You should be able to convert your old checkpoint to the new format using
    this script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py.


    After the conversion, the layers'' name will match back and the generation should
    be as expected.'
  created_at: 2024-01-17 18:09:10+00:00
  edited: false
  hidden: false
  id: 65a817c66a7418d9af386dec
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
      fullname: Yang Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaYaGeGe
      type: user
    createdAt: '2024-01-18T06:00:58.000Z'
    data:
      edited: true
      editors:
      - YaYaGeGe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8044425845146179
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
          fullname: Yang Yang
          isHf: false
          isPro: false
          name: YaYaGeGe
          type: user
        html: '<blockquote>

          <p>You should be able to convert your old checkpoint to the new format using
          this script: <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py">https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py</a>.</p>

          <p>After the conversion, the layers'' name will match back and the generation
          should be as expected.</p>

          </blockquote>

          <p>Thanks for your prompt reply.<br>Tried this script, having key mismatched
          error when loading converted weights to new model defination.<br>By checking
          the convert function, I don''t think it can properly convert the old model
          weights (before revision <a href="/microsoft/phi-2/commit/834565c23f9b28b96ccbeabe614dd906b6db551a">834565c23f9b28b96ccbeabe614dd906b6db551a</a>)
          into the new transfermor intergated network defination.<br>For example,
          "Wqkv" layer will be converted into "query_key_value", while can only find
          "q_proj" etc in the latest model defination. The layer should be further
          handled and splited into q, k, v</p>

          '
        raw: "> You should be able to convert your old checkpoint to the new format\
          \ using this script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py.\n\
          > \n> After the conversion, the layers' name will match back and the generation\
          \ should be as expected.\n\nThanks for your prompt reply.\nTried this script,\
          \ having key mismatched error when loading converted weights to new model\
          \ defination.  \nBy checking the convert function, I don't think it can\
          \ properly convert the old model weights (before revision 834565c23f9b28b96ccbeabe614dd906b6db551a)\
          \ into the new transfermor intergated network defination.\nFor example,\
          \ \"Wqkv\" layer will be converted into \"query_key_value\", while can only\
          \ find \"q_proj\" etc in the latest model defination. The layer should be\
          \ further handled and splited into q, k, v\n\n"
        updatedAt: '2024-01-18T06:01:12.916Z'
      numEdits: 1
      reactions: []
    id: 65a8be9a241e1c6c4873fb77
    type: comment
  author: YaYaGeGe
  content: "> You should be able to convert your old checkpoint to the new format\
    \ using this script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/convert_phi_weights_to_hf.py.\n\
    > \n> After the conversion, the layers' name will match back and the generation\
    \ should be as expected.\n\nThanks for your prompt reply.\nTried this script,\
    \ having key mismatched error when loading converted weights to new model defination.\
    \  \nBy checking the convert function, I don't think it can properly convert the\
    \ old model weights (before revision 834565c23f9b28b96ccbeabe614dd906b6db551a)\
    \ into the new transfermor intergated network defination.\nFor example, \"Wqkv\"\
    \ layer will be converted into \"query_key_value\", while can only find \"q_proj\"\
    \ etc in the latest model defination. The layer should be further handled and\
    \ splited into q, k, v\n\n"
  created_at: 2024-01-18 06:00:58+00:00
  edited: true
  hidden: false
  id: 65a8be9a241e1c6c4873fb77
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-18T16:21:39.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9713637232780457
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Oh, my bad, I thought the conversion script had been updated to
          the splitted q, k and v.</p>

          <p>Let me work on that and send a PR with an updated script.</p>

          '
        raw: 'Oh, my bad, I thought the conversion script had been updated to the
          splitted q, k and v.


          Let me work on that and send a PR with an updated script.'
        updatedAt: '2024-01-18T16:21:39.972Z'
      numEdits: 0
      reactions: []
    id: 65a95013246acad996c817c8
    type: comment
  author: gugarosa
  content: 'Oh, my bad, I thought the conversion script had been updated to the splitted
    q, k and v.


    Let me work on that and send a PR with an updated script.'
  created_at: 2024-01-18 16:21:39+00:00
  edited: false
  hidden: false
  id: 65a95013246acad996c817c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
      fullname: Yang Yang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: YaYaGeGe
      type: user
    createdAt: '2024-01-19T06:30:23.000Z'
    data:
      edited: false
      editors:
      - YaYaGeGe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6bbccc3b3dc0877e1dbd6dc3aabd0db7.svg
          fullname: Yang Yang
          isHf: false
          isPro: false
          name: YaYaGeGe
          type: user
        html: "<pre><code>PHI_MAPPING = {\n    \"transformer.embd.wte.weight\": \"\
          model.embed_tokens.weight\",\n    \"lm_head.linear\": \"lm_head\",\n   \
          \ \"lm_head.ln\": \"model.final_layernorm\",\n    \"layers\": \"model.layers\"\
          ,\n    \"transformer\": \"model\",\n    \".h.\": \".layers.\",\n    \"ln\"\
          : \"input_layernorm\",\n    \"mixer\": \"self_attn\",\n    \"out_proj\"\
          : \"dense\",\n}\n\ndef convert_weights(original_weights, mapping, config):\n\
          \    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n\
          \n    for original_weights_key in original_weights_keys:\n\n        new_key\
          \ = original_weights_key\n        if \"rotary_emb\" in original_weights_key:\n\
          \            continue\n        \n        for k, v in mapping.items():\n\
          \            if k in new_key:\n                new_key = new_key.replace(k,\
          \ v)\n        if \"Wqkv\" in original_weights_key:\n            if \"weight\"\
          \ in original_weights_key:\n                weight = original_weights.pop(original_weights_key)\n\
          \                weight = weight.view(3, -1, config.hidden_size)\n     \
          \           q_w, k_w, v_w = weight.chunk(3, dim=0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"q_proj\")] = q_w.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"k_proj\")] = k_w.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"v_proj\")] = v_w.squeeze(0)\n                \n            elif\
          \ \"bias\" in original_weights_key:\n                bias = original_weights.pop(original_weights_key)\n\
          \                bias = bias.view(3, -1)\n                q_b, k_b, v_b\
          \ = bias.chunk(3, dim=0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"q_proj\")] = q_b.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"k_proj\")] = k_b.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"v_proj\")] = v_b.squeeze(0)\n        else:\n            converted_weights[new_key]\
          \ = original_weights.pop(original_weights_key)\n\n    return converted_weights\n\
          </code></pre>\n<p>This works for me and the output is as expected after\
          \ conversion.<br>BTW, by comparing the new model weights and the old one,\
          \ I just found that for the same layer, the weights' value are not the same.\
          \ Is it re-trained? cause simpliy mapping the name shouldn't change the\
          \ weights value</p>\n"
        raw: "```\nPHI_MAPPING = {\n    \"transformer.embd.wte.weight\": \"model.embed_tokens.weight\"\
          ,\n    \"lm_head.linear\": \"lm_head\",\n    \"lm_head.ln\": \"model.final_layernorm\"\
          ,\n    \"layers\": \"model.layers\",\n    \"transformer\": \"model\",\n\
          \    \".h.\": \".layers.\",\n    \"ln\": \"input_layernorm\",\n    \"mixer\"\
          : \"self_attn\",\n    \"out_proj\": \"dense\",\n}\n\ndef convert_weights(original_weights,\
          \ mapping, config):\n    converted_weights = {}\n    original_weights_keys\
          \ = sorted(original_weights.keys())\n\n    for original_weights_key in original_weights_keys:\n\
          \n        new_key = original_weights_key\n        if \"rotary_emb\" in original_weights_key:\n\
          \            continue\n        \n        for k, v in mapping.items():\n\
          \            if k in new_key:\n                new_key = new_key.replace(k,\
          \ v)\n        if \"Wqkv\" in original_weights_key:\n            if \"weight\"\
          \ in original_weights_key:\n                weight = original_weights.pop(original_weights_key)\n\
          \                weight = weight.view(3, -1, config.hidden_size)\n     \
          \           q_w, k_w, v_w = weight.chunk(3, dim=0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"q_proj\")] = q_w.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"k_proj\")] = k_w.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"v_proj\")] = v_w.squeeze(0)\n                \n            elif\
          \ \"bias\" in original_weights_key:\n                bias = original_weights.pop(original_weights_key)\n\
          \                bias = bias.view(3, -1)\n                q_b, k_b, v_b\
          \ = bias.chunk(3, dim=0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"q_proj\")] = q_b.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"k_proj\")] = k_b.squeeze(0)\n                converted_weights[new_key.replace(\"\
          Wqkv\", \"v_proj\")] = v_b.squeeze(0)\n        else:\n            converted_weights[new_key]\
          \ = original_weights.pop(original_weights_key)\n\n    return converted_weights\n\
          ```\nThis works for me and the output is as expected after conversion.\n\
          BTW, by comparing the new model weights and the old one, I just found that\
          \ for the same layer, the weights' value are not the same. Is it re-trained?\
          \ cause simpliy mapping the name shouldn't change the weights value"
        updatedAt: '2024-01-19T06:30:23.973Z'
      numEdits: 0
      reactions: []
    id: 65aa16ffa92a64ef5b1d3dc3
    type: comment
  author: YaYaGeGe
  content: "```\nPHI_MAPPING = {\n    \"transformer.embd.wte.weight\": \"model.embed_tokens.weight\"\
    ,\n    \"lm_head.linear\": \"lm_head\",\n    \"lm_head.ln\": \"model.final_layernorm\"\
    ,\n    \"layers\": \"model.layers\",\n    \"transformer\": \"model\",\n    \"\
    .h.\": \".layers.\",\n    \"ln\": \"input_layernorm\",\n    \"mixer\": \"self_attn\"\
    ,\n    \"out_proj\": \"dense\",\n}\n\ndef convert_weights(original_weights, mapping,\
    \ config):\n    converted_weights = {}\n    original_weights_keys = sorted(original_weights.keys())\n\
    \n    for original_weights_key in original_weights_keys:\n\n        new_key =\
    \ original_weights_key\n        if \"rotary_emb\" in original_weights_key:\n \
    \           continue\n        \n        for k, v in mapping.items():\n       \
    \     if k in new_key:\n                new_key = new_key.replace(k, v)\n    \
    \    if \"Wqkv\" in original_weights_key:\n            if \"weight\" in original_weights_key:\n\
    \                weight = original_weights.pop(original_weights_key)\n       \
    \         weight = weight.view(3, -1, config.hidden_size)\n                q_w,\
    \ k_w, v_w = weight.chunk(3, dim=0)\n                converted_weights[new_key.replace(\"\
    Wqkv\", \"q_proj\")] = q_w.squeeze(0)\n                converted_weights[new_key.replace(\"\
    Wqkv\", \"k_proj\")] = k_w.squeeze(0)\n                converted_weights[new_key.replace(\"\
    Wqkv\", \"v_proj\")] = v_w.squeeze(0)\n                \n            elif \"bias\"\
    \ in original_weights_key:\n                bias = original_weights.pop(original_weights_key)\n\
    \                bias = bias.view(3, -1)\n                q_b, k_b, v_b = bias.chunk(3,\
    \ dim=0)\n                converted_weights[new_key.replace(\"Wqkv\", \"q_proj\"\
    )] = q_b.squeeze(0)\n                converted_weights[new_key.replace(\"Wqkv\"\
    , \"k_proj\")] = k_b.squeeze(0)\n                converted_weights[new_key.replace(\"\
    Wqkv\", \"v_proj\")] = v_b.squeeze(0)\n        else:\n            converted_weights[new_key]\
    \ = original_weights.pop(original_weights_key)\n\n    return converted_weights\n\
    ```\nThis works for me and the output is as expected after conversion.\nBTW, by\
    \ comparing the new model weights and the old one, I just found that for the same\
    \ layer, the weights' value are not the same. Is it re-trained? cause simpliy\
    \ mapping the name shouldn't change the weights value"
  created_at: 2024-01-19 06:30:23+00:00
  edited: false
  hidden: false
  id: 65aa16ffa92a64ef5b1d3dc3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-19T13:58:18.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9602175354957581
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>What do you mean by not the same?</p>

          <p>If you are comparing tensor by tensor (shape by shape), there will be
          a time where a mismatch appears between <code>Wqkv</code> and the separate
          queries, keys and values. However, their internal value should be the same
          (except for the index where it concatenates q, k and v).</p>

          <p>The logits are even expected to be really really close (or even the same).</p>

          '
        raw: 'What do you mean by not the same?


          If you are comparing tensor by tensor (shape by shape), there will be a
          time where a mismatch appears between `Wqkv` and the separate queries, keys
          and values. However, their internal value should be the same (except for
          the index where it concatenates q, k and v).


          The logits are even expected to be really really close (or even the same).'
        updatedAt: '2024-01-19T13:58:28.040Z'
      numEdits: 1
      reactions: []
    id: 65aa7ffacb5b4fb08ecfe235
    type: comment
  author: gugarosa
  content: 'What do you mean by not the same?


    If you are comparing tensor by tensor (shape by shape), there will be a time where
    a mismatch appears between `Wqkv` and the separate queries, keys and values. However,
    their internal value should be the same (except for the index where it concatenates
    q, k and v).


    The logits are even expected to be really really close (or even the same).'
  created_at: 2024-01-19 13:58:18+00:00
  edited: true
  hidden: false
  id: 65aa7ffacb5b4fb08ecfe235
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 88
repo_id: microsoft/phi-2
repo_type: model
status: open
target_branch: null
title: Loading a qlora finetuned checkpoint with the new updates outputs differently
