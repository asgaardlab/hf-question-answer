!!python/object:huggingface_hub.community.DiscussionWithDetails
author: loretoparisi
conflicting_files: null
created_at: 2023-12-14 18:13:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2023-12-14T18:13:55.000Z'
    data:
      edited: true
      editors:
      - loretoparisi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5846842527389526
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: "<p>While model loading is pretty fast (once downloaded) and it takes\
          \ around 1.5 seconds an inference for 2048 token (<code>max_length</code>)\
          \  on a A10G / 24GB took ~80 sec.</p>\n<p>loading function was</p>\n<pre><code\
          \ class=\"language-python\"><span class=\"hljs-keyword\">def</span> <span\
          \ class=\"hljs-title function_\">load_hf_local</span>(<span class=\"hljs-params\"\
          >model_name, device, dtype:torch.float16</span>):\n    <span class=\"hljs-string\"\
          >\"\"\"</span>\n<span class=\"hljs-string\">        load model via Huggingface\
          \ AutoTokenizer, AutoModelForCausalLM</span>\n<span class=\"hljs-string\"\
          >    \"\"\"</span>\n    start_time = time. time()\n    torch.set_default_dtype(dtype)\n\
          \    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,\
          \ local_files_only=<span class=\"hljs-literal\">True</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n    <span class=\"hljs-keyword\"\
          >with</span> torch.device(device):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ local_files_only=<span class=\"hljs-literal\">True</span>, device_map=<span\
          \ class=\"hljs-string\">\"auto\"</span>, torch_dtype=dtype, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n        model.to(device)\n    <span\
          \ class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Loaded\
          \ in <span class=\"hljs-subst\">{time.time() - start_time: <span class=\"\
          hljs-number\">.2</span>f}</span> seconds\"</span>)\n    <span class=\"hljs-keyword\"\
          >return</span> tokenizer, model\n</code></pre>\n<p>generate function was</p>\n\
          <pre><code class=\"language-python\"><span class=\"hljs-keyword\">def</span>\
          \ <span class=\"hljs-title function_\">LLM_generate</span>(<span class=\"\
          hljs-params\">model, tokenizer, prompt, length</span>):\n    start_time\
          \ = time.time()\n    inputs = tokenizer(prompt, return_tensors=<span class=\"\
          hljs-string\">\"pt\"</span>, return_attention_mask=<span class=\"hljs-literal\"\
          >False</span>)\n    \n    model_inputs = inputs.to(device)\n    model.to(device)\n\
          \    \n    input_token_len = <span class=\"hljs-built_in\">len</span>(model_inputs.tokens())\n\
          \    outputs = model.generate(**model_inputs, max_length=length <span class=\"\
          hljs-keyword\">if</span> length &gt;= input_token_len <span class=\"hljs-keyword\"\
          >else</span> input_token_len)\n    <span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">f\"generated in <span class=\"hljs-subst\">{time.time()\
          \ - start_time: <span class=\"hljs-number\">.2</span>f}</span> seconds\"\
          </span>)\n    <span class=\"hljs-keyword\">return</span> tokenizer.batch_decode(outputs)[<span\
          \ class=\"hljs-number\">0</span>]\n</code></pre>\n<p>while setting <code>max_length</code>\
          \ to 512 tokens, led to <code>~20</code> seconds.</p>\n<p>This is a test\
          \ ranging from 128 to 2048</p>\n<pre><code>generated in  4.37 seconds\n\
          max_length:128, elapsed:4.372055530548096\ngenerated in  9.16 seconds\n\
          max_length:256, elapsed:9.158923625946045\ngenerated in  19.05 seconds\n\
          max_length:512, elapsed:19.05333709716797\ngenerated in  38.90 seconds\n\
          max_length:1024, elapsed:38.89565658569336\ngenerated in  79.18 seconds\n\
          max_length:2048, elapsed:79.17627263069153\n</code></pre>\n<p><a rel=\"\
          nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/5e6b7a61d4cd9779932a7601/NcXR2eBrbVoFw6CkT3cwH.png\"\
          ><img alt=\"Screenshot 2023-12-14 at 19.17.18.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/5e6b7a61d4cd9779932a7601/NcXR2eBrbVoFw6CkT3cwH.png\"\
          ></a></p>\n"
        raw: "While model loading is pretty fast (once downloaded) and it takes around\
          \ 1.5 seconds an inference for 2048 token (`max_length`)  on a A10G / 24GB\
          \ took ~80 sec.\n\nloading function was\n```python\ndef load_hf_local(model_name,\
          \ device, dtype:torch.float16):\n    \"\"\"\n        load model via Huggingface\
          \ AutoTokenizer, AutoModelForCausalLM\n    \"\"\"\n    start_time = time.\
          \ time()\n    torch.set_default_dtype(dtype)\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,\
          \ local_files_only=True, trust_remote_code=True)\n    with torch.device(device):\n\
          \        model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
          \ local_files_only=True, device_map=\"auto\", torch_dtype=dtype, trust_remote_code=True)\n\
          \        model.to(device)\n    print(f\"Loaded in {time.time() - start_time:\
          \ .2f} seconds\")\n    return tokenizer, model\n```\n\ngenerate function\
          \ was\n\n```python\ndef LLM_generate(model, tokenizer, prompt, length):\n\
          \    start_time = time.time()\n    inputs = tokenizer(prompt, return_tensors=\"\
          pt\", return_attention_mask=False)\n    \n    model_inputs = inputs.to(device)\n\
          \    model.to(device)\n    \n    input_token_len = len(model_inputs.tokens())\n\
          \    outputs = model.generate(**model_inputs, max_length=length if length\
          \ >= input_token_len else input_token_len)\n    print(f\"generated in {time.time()\
          \ - start_time: .2f} seconds\")\n    return tokenizer.batch_decode(outputs)[0]\n\
          ```\n\nwhile setting `max_length` to 512 tokens, led to `~20` seconds.\n\
          \nThis is a test ranging from 128 to 2048\n```\ngenerated in  4.37 seconds\n\
          max_length:128, elapsed:4.372055530548096\ngenerated in  9.16 seconds\n\
          max_length:256, elapsed:9.158923625946045\ngenerated in  19.05 seconds\n\
          max_length:512, elapsed:19.05333709716797\ngenerated in  38.90 seconds\n\
          max_length:1024, elapsed:38.89565658569336\ngenerated in  79.18 seconds\n\
          max_length:2048, elapsed:79.17627263069153\n```\n\n\n\n![Screenshot 2023-12-14\
          \ at 19.17.18.png](https://cdn-uploads.huggingface.co/production/uploads/5e6b7a61d4cd9779932a7601/NcXR2eBrbVoFw6CkT3cwH.png)\n"
        updatedAt: '2023-12-14T18:17:56.365Z'
      numEdits: 1
      reactions: []
    id: 657b45e3f2dda5456bfbe034
    type: comment
  author: loretoparisi
  content: "While model loading is pretty fast (once downloaded) and it takes around\
    \ 1.5 seconds an inference for 2048 token (`max_length`)  on a A10G / 24GB took\
    \ ~80 sec.\n\nloading function was\n```python\ndef load_hf_local(model_name, device,\
    \ dtype:torch.float16):\n    \"\"\"\n        load model via Huggingface AutoTokenizer,\
    \ AutoModelForCausalLM\n    \"\"\"\n    start_time = time. time()\n    torch.set_default_dtype(dtype)\n\
    \    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, local_files_only=True,\
    \ trust_remote_code=True)\n    with torch.device(device):\n        model = transformers.AutoModelForCausalLM.from_pretrained(model_name,\
    \ local_files_only=True, device_map=\"auto\", torch_dtype=dtype, trust_remote_code=True)\n\
    \        model.to(device)\n    print(f\"Loaded in {time.time() - start_time: .2f}\
    \ seconds\")\n    return tokenizer, model\n```\n\ngenerate function was\n\n```python\n\
    def LLM_generate(model, tokenizer, prompt, length):\n    start_time = time.time()\n\
    \    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n\
    \    \n    model_inputs = inputs.to(device)\n    model.to(device)\n    \n    input_token_len\
    \ = len(model_inputs.tokens())\n    outputs = model.generate(**model_inputs, max_length=length\
    \ if length >= input_token_len else input_token_len)\n    print(f\"generated in\
    \ {time.time() - start_time: .2f} seconds\")\n    return tokenizer.batch_decode(outputs)[0]\n\
    ```\n\nwhile setting `max_length` to 512 tokens, led to `~20` seconds.\n\nThis\
    \ is a test ranging from 128 to 2048\n```\ngenerated in  4.37 seconds\nmax_length:128,\
    \ elapsed:4.372055530548096\ngenerated in  9.16 seconds\nmax_length:256, elapsed:9.158923625946045\n\
    generated in  19.05 seconds\nmax_length:512, elapsed:19.05333709716797\ngenerated\
    \ in  38.90 seconds\nmax_length:1024, elapsed:38.89565658569336\ngenerated in\
    \  79.18 seconds\nmax_length:2048, elapsed:79.17627263069153\n```\n\n\n\n![Screenshot\
    \ 2023-12-14 at 19.17.18.png](https://cdn-uploads.huggingface.co/production/uploads/5e6b7a61d4cd9779932a7601/NcXR2eBrbVoFw6CkT3cwH.png)\n"
  created_at: 2023-12-14 18:13:55+00:00
  edited: true
  hidden: false
  id: 657b45e3f2dda5456bfbe034
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b2a44e8d5daf09a75e2312ddca81e7a2.svg
      fullname: Caio Mendes
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: caiom
      type: user
    createdAt: '2023-12-20T03:24:08.000Z'
    data:
      edited: false
      editors:
      - caiom
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8879812359809875
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b2a44e8d5daf09a75e2312ddca81e7a2.svg
          fullname: Caio Mendes
          isHf: false
          isPro: false
          name: caiom
          type: user
        html: '<p>Phi models are compatible with vLLM, have you considered using it?<br><a
          rel="nofollow" href="https://docs.vllm.ai/en/latest/index.html">https://docs.vllm.ai/en/latest/index.html</a></p>

          '
        raw: "Phi models are compatible with vLLM, have you considered using it? \n\
          https://docs.vllm.ai/en/latest/index.html"
        updatedAt: '2023-12-20T03:24:08.402Z'
      numEdits: 0
      reactions: []
    id: 65825e58d73d6402f7a9f350
    type: comment
  author: caiom
  content: "Phi models are compatible with vLLM, have you considered using it? \n\
    https://docs.vllm.ai/en/latest/index.html"
  created_at: 2023-12-20 03:24:08+00:00
  edited: false
  hidden: false
  id: 65825e58d73d6402f7a9f350
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-03T14:22:07.000Z'
    data:
      status: closed
    id: 65956d8ffbbe972cfb2c31f6
    type: status-change
  author: gugarosa
  created_at: 2024-01-03 14:22:07+00:00
  id: 65956d8ffbbe972cfb2c31f6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d4418e547c3301ec7c0d8d0105685ca7.svg
      fullname: Artyom S.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: deepbrain
      type: user
    createdAt: '2024-01-14T18:52:11.000Z'
    data:
      edited: false
      editors:
      - deepbrain
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4870791733264923
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d4418e547c3301ec7c0d8d0105685ca7.svg
          fullname: Artyom S.
          isHf: false
          isPro: false
          name: deepbrain
          type: user
        html: '<p>vLLM crashes with Phi 2.0: AttributeError: ''PhiConfig'' object
          has no attribute ''layer_norm_epsilon''</p>

          '
        raw: 'vLLM crashes with Phi 2.0: AttributeError: ''PhiConfig'' object has
          no attribute ''layer_norm_epsilon'''
        updatedAt: '2024-01-14T18:52:11.950Z'
      numEdits: 0
      reactions: []
    id: 65a42d5b680cb2eb949c030d
    type: comment
  author: deepbrain
  content: 'vLLM crashes with Phi 2.0: AttributeError: ''PhiConfig'' object has no
    attribute ''layer_norm_epsilon'''
  created_at: 2024-01-14 18:52:11+00:00
  edited: false
  hidden: false
  id: 65a42d5b680cb2eb949c030d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 17
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Slow inference times on gpu
