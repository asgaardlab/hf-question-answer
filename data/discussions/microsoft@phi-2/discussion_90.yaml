!!python/object:huggingface_hub.community.DiscussionWithDetails
author: aflah
conflicting_files: null
created_at: 2024-01-18 17:42:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
      fullname: Mohammad Aflah Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aflah
      type: user
    createdAt: '2024-01-18T17:42:39.000Z'
    data:
      edited: false
      editors:
      - aflah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5759384036064148
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
          fullname: Mohammad Aflah Khan
          isHf: false
          isPro: false
          name: aflah
          type: user
        html: "<p>I got the following error when trying to load it using device_map='auto'</p>\n\
          <pre><code>Traceback (most recent call last):\n  File \"/NS/llm-1/work/afkhan/fact/src/main.py\"\
          , line 46, in main\n    run_project(\n  File \"/NS/llm-1/work/afkhan/fact/src/lib_project/lib_project/project.py\"\
          , line 74, in run_project\n    handle.experiment(experiment_config)\n  File\
          \ \"/NS/llm-1/work/afkhan/fact/src/lib_project/lib_project/experiment.py\"\
          , line 73, in experiment_wrapper\n    value = func(dataclass_config, task_description)\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/NS/llm-1/work/afkhan/fact/src/experiments/knowledge_probing/experiment1.py\"\
          , line 77, in kp_experiment\n    model, tokenizer = load_model_tokenizer(config.model)\n\
          \                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/NS/llm-1/work/afkhan/fact/src/lib_llm/lib_llm/models/load.py\"\
          , line 97, in load_model_tokenizer\n    model = load_model(\n          \
          \  ^^^^^^^^^^^\n  File \"/NS/llm-1/work/afkhan/fact/src/lib_llm/lib_llm/models/load.py\"\
          , line 50, in load_model\n    model = AutoModelForCausalLM.from_pretrained(\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\n    return model_class.from_pretrained(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3606, in from_pretrained\n    no_split_modules = model._get_no_split_modules(device_map)\n\
          \                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File\
          \ \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 1690, in _get_no_split_modules\n    raise ValueError(\nValueError:\
          \ PhiForCausalLM does not support `device_map='auto'`. To implement support,\
          \ the model class needs to implement the `_no_split_modules` attribute.\n\
          </code></pre>\n<p>My code is pretty generic so I wouldn't really like to\
          \ add exceptions for specific models and probably a lot of other people\
          \ would also benefit if you can add this into the main release? It seems\
          \ this <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/23816\"\
          >error</a> is related so I decided to raise a PR myself but I noticed that\
          \ the <code>_no_split_modules</code> is actually already present so I'm\
          \ not sure why this occurs.</p>\n"
        raw: "I got the following error when trying to load it using device_map='auto'\r\
          \n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/NS/llm-1/work/afkhan/fact/src/main.py\"\
          , line 46, in main\r\n    run_project(\r\n  File \"/NS/llm-1/work/afkhan/fact/src/lib_project/lib_project/project.py\"\
          , line 74, in run_project\r\n    handle.experiment(experiment_config)\r\n\
          \  File \"/NS/llm-1/work/afkhan/fact/src/lib_project/lib_project/experiment.py\"\
          , line 73, in experiment_wrapper\r\n    value = func(dataclass_config, task_description)\r\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/llm-1/work/afkhan/fact/src/experiments/knowledge_probing/experiment1.py\"\
          , line 77, in kp_experiment\r\n    model, tokenizer = load_model_tokenizer(config.model)\r\
          \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
          /NS/llm-1/work/afkhan/fact/src/lib_llm/lib_llm/models/load.py\", line 97,\
          \ in load_model_tokenizer\r\n    model = load_model(\r\n            ^^^^^^^^^^^\r\
          \n  File \"/NS/llm-1/work/afkhan/fact/src/lib_llm/lib_llm/models/load.py\"\
          , line 50, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(\r\
          \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 566, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3606, in from_pretrained\r\n    no_split_modules = model._get_no_split_modules(device_map)\r\
          \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File\
          \ \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 1690, in _get_no_split_modules\r\n    raise ValueError(\r\nValueError:\
          \ PhiForCausalLM does not support `device_map='auto'`. To implement support,\
          \ the model class needs to implement the `_no_split_modules` attribute.\r\
          \n```\r\n\r\nMy code is pretty generic so I wouldn't really like to add\
          \ exceptions for specific models and probably a lot of other people would\
          \ also benefit if you can add this into the main release? It seems this\
          \ [error](https://github.com/huggingface/transformers/issues/23816) is related\
          \ so I decided to raise a PR myself but I noticed that the `_no_split_modules`\
          \ is actually already present so I'm not sure why this occurs."
        updatedAt: '2024-01-18T17:42:39.776Z'
      numEdits: 0
      reactions: []
    id: 65a9630fcab1bfcc3ad80295
    type: comment
  author: aflah
  content: "I got the following error when trying to load it using device_map='auto'\r\
    \n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/NS/llm-1/work/afkhan/fact/src/main.py\"\
    , line 46, in main\r\n    run_project(\r\n  File \"/NS/llm-1/work/afkhan/fact/src/lib_project/lib_project/project.py\"\
    , line 74, in run_project\r\n    handle.experiment(experiment_config)\r\n  File\
    \ \"/NS/llm-1/work/afkhan/fact/src/lib_project/lib_project/experiment.py\", line\
    \ 73, in experiment_wrapper\r\n    value = func(dataclass_config, task_description)\r\
    \n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/llm-1/work/afkhan/fact/src/experiments/knowledge_probing/experiment1.py\"\
    , line 77, in kp_experiment\r\n    model, tokenizer = load_model_tokenizer(config.model)\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/llm-1/work/afkhan/fact/src/lib_llm/lib_llm/models/load.py\"\
    , line 97, in load_model_tokenizer\r\n    model = load_model(\r\n            ^^^^^^^^^^^\r\
    \n  File \"/NS/llm-1/work/afkhan/fact/src/lib_llm/lib_llm/models/load.py\", line\
    \ 50, in load_model\r\n    model = AutoModelForCausalLM.from_pretrained(\r\n \
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 566, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3606, in from_pretrained\r\n    no_split_modules = model._get_no_split_modules(device_map)\r\
    \n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\
    /NS/venvs/nobackup/afkhan/pypoetry_cache/virtualenvs/llm-knowledge-extraction-tF3ncB3K-py3.11/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 1690, in _get_no_split_modules\r\n    raise ValueError(\r\nValueError:\
    \ PhiForCausalLM does not support `device_map='auto'`. To implement support, the\
    \ model class needs to implement the `_no_split_modules` attribute.\r\n```\r\n\
    \r\nMy code is pretty generic so I wouldn't really like to add exceptions for\
    \ specific models and probably a lot of other people would also benefit if you\
    \ can add this into the main release? It seems this [error](https://github.com/huggingface/transformers/issues/23816)\
    \ is related so I decided to raise a PR myself but I noticed that the `_no_split_modules`\
    \ is actually already present so I'm not sure why this occurs."
  created_at: 2024-01-18 17:42:39+00:00
  edited: false
  hidden: false
  id: 65a9630fcab1bfcc3ad80295
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-19T14:01:41.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.950668215751648
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>What is your <code>transformers</code> version?</p>

          '
        raw: What is your `transformers` version?
        updatedAt: '2024-01-19T14:01:41.110Z'
      numEdits: 0
      reactions: []
    id: 65aa80c55860f06ff2961fe6
    type: comment
  author: gugarosa
  content: What is your `transformers` version?
  created_at: 2024-01-19 14:01:41+00:00
  edited: false
  hidden: false
  id: 65aa80c55860f06ff2961fe6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
      fullname: Mohammad Aflah Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aflah
      type: user
    createdAt: '2024-01-19T15:36:53.000Z'
    data:
      edited: true
      editors:
      - aflah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3296641409397125
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
          fullname: Mohammad Aflah Khan
          isHf: false
          isPro: false
          name: aflah
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> I'm using version\
          \ 4.36.2, do I need to upgrade it?</p>\n"
        raw: '@gugarosa I''m using version 4.36.2, do I need to upgrade it?'
        updatedAt: '2024-01-19T15:37:18.761Z'
      numEdits: 2
      reactions: []
    id: 65aa9715d2adc31ee3e4f651
    type: comment
  author: aflah
  content: '@gugarosa I''m using version 4.36.2, do I need to upgrade it?'
  created_at: 2024-01-19 15:36:53+00:00
  edited: true
  hidden: false
  id: 65aa9715d2adc31ee3e4f651
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/499199f8e82d5e7d3cd4e4b7ad6038bf.svg
      fullname: Du Jin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DuSurvivi
      type: user
    createdAt: '2024-01-20T02:23:40.000Z'
    data:
      edited: false
      editors:
      - DuSurvivi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7582744359970093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/499199f8e82d5e7d3cd4e4b7ad6038bf.svg
          fullname: Du Jin
          isHf: false
          isPro: false
          name: DuSurvivi
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ I'm using version 4.36.2, do I need to upgrade it?</p>\n</blockquote>\n\
          <p>You may need to use the latest version from the transformers repository\
          \ (4.37.0.dev0) via \u201Cpip uninstall -y transformers &amp;&amp; pip install\
          \ git+<a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers%E2%80%9D\"\
          >https://github.com/huggingface/transformers\u201D</a>.</p>\n"
        raw: "> @gugarosa I'm using version 4.36.2, do I need to upgrade it?\n\nYou\
          \ may need to use the latest version from the transformers repository (4.37.0.dev0)\
          \ via \u201Cpip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\u201D\
          ."
        updatedAt: '2024-01-20T02:23:40.248Z'
      numEdits: 0
      reactions: []
    id: 65ab2eac0150f64adffeff46
    type: comment
  author: DuSurvivi
  content: "> @gugarosa I'm using version 4.36.2, do I need to upgrade it?\n\nYou\
    \ may need to use the latest version from the transformers repository (4.37.0.dev0)\
    \ via \u201Cpip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers\u201D\
    ."
  created_at: 2024-01-20 02:23:40+00:00
  edited: false
  hidden: false
  id: 65ab2eac0150f64adffeff46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
      fullname: Mohammad Aflah Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aflah
      type: user
    createdAt: '2024-01-20T14:11:08.000Z'
    data:
      edited: false
      editors:
      - aflah
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9698733687400818
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
          fullname: Mohammad Aflah Khan
          isHf: false
          isPro: false
          name: aflah
          type: user
        html: '<p>Thanks! I''ll give this a go</p>

          '
        raw: Thanks! I'll give this a go
        updatedAt: '2024-01-20T14:11:08.960Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65abd47d48c718a5746e8273
    id: 65abd47c48c718a5746e8270
    type: comment
  author: aflah
  content: Thanks! I'll give this a go
  created_at: 2024-01-20 14:11:08+00:00
  edited: false
  hidden: false
  id: 65abd47c48c718a5746e8270
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/755f1461e5fc6719bdf616317845ccaa.svg
      fullname: Mohammad Aflah Khan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: aflah
      type: user
    createdAt: '2024-01-20T14:11:09.000Z'
    data:
      status: closed
    id: 65abd47d48c718a5746e8273
    type: status-change
  author: aflah
  created_at: 2024-01-20 14:11:09+00:00
  id: 65abd47d48c718a5746e8273
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 90
repo_id: microsoft/phi-2
repo_type: model
status: closed
target_branch: null
title: Unable to use with device_map='auto'
