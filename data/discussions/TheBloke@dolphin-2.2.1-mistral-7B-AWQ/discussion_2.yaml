!!python/object:huggingface_hub.community.DiscussionWithDetails
author: webslug
conflicting_files: null
created_at: 2023-11-23 03:40:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/100a08fb1d2d9294880437fda47c9f93.svg
      fullname: Tim Smith
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: webslug
      type: user
    createdAt: '2023-11-23T03:40:04.000Z'
    data:
      edited: false
      editors:
      - webslug
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9875506162643433
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/100a08fb1d2d9294880437fda47c9f93.svg
          fullname: Tim Smith
          isHf: false
          isPro: false
          name: webslug
          type: user
        html: '<p>I have been using the GGUF CPU version of Dolphin and works extremely
          well.  I was under the impression this AWQ variant would be faster.  I have
          GTX 3060 and downloaded this model for Oobabooga.  </p>

          <p>This model seems to be inconsistent, sometimes responses take several
          minutes to appear and on occasions the responses are limited to just one
          word.  Nothing has changed in my setup apart from the model.  </p>

          <p>Am I doing something wrong, I was under the impression AWQ models were
          faster.</p>

          '
        raw: "I have been using the GGUF CPU version of Dolphin and works extremely\
          \ well.  I was under the impression this AWQ variant would be faster.  I\
          \ have GTX 3060 and downloaded this model for Oobabooga.  \r\n\r\nThis model\
          \ seems to be inconsistent, sometimes responses take several minutes to\
          \ appear and on occasions the responses are limited to just one word.  Nothing\
          \ has changed in my setup apart from the model.  \r\n\r\nAm I doing something\
          \ wrong, I was under the impression AWQ models were faster."
        updatedAt: '2023-11-23T03:40:04.142Z'
      numEdits: 0
      reactions: []
    id: 655ec994a74c042364d8ba8e
    type: comment
  author: webslug
  content: "I have been using the GGUF CPU version of Dolphin and works extremely\
    \ well.  I was under the impression this AWQ variant would be faster.  I have\
    \ GTX 3060 and downloaded this model for Oobabooga.  \r\n\r\nThis model seems\
    \ to be inconsistent, sometimes responses take several minutes to appear and on\
    \ occasions the responses are limited to just one word.  Nothing has changed in\
    \ my setup apart from the model.  \r\n\r\nAm I doing something wrong, I was under\
    \ the impression AWQ models were faster."
  created_at: 2023-11-23 03:40:04+00:00
  edited: false
  hidden: false
  id: 655ec994a74c042364d8ba8e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/dolphin-2.2.1-mistral-7B-AWQ
repo_type: model
status: open
target_branch: null
title: Slower and inconsistent
