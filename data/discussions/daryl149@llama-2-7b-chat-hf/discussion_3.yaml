!!python/object:huggingface_hub.community.DiscussionWithDetails
author: huggingFace1108
conflicting_files: null
created_at: 2023-08-08 15:04:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9ec9683ec861bab8794de6d11434581a.svg
      fullname: David
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huggingFace1108
      type: user
    createdAt: '2023-08-08T16:04:39.000Z'
    data:
      edited: false
      editors:
      - huggingFace1108
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8120007514953613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9ec9683ec861bab8794de6d11434581a.svg
          fullname: David
          isHf: false
          isPro: false
          name: huggingFace1108
          type: user
        html: '<p>Thanks for this model.</p>

          <p>when printing ''tokenizer.model_max_length'', I got a number like ''1000000000000000019884624838656''.</p>

          <p>the model_max_length is supposed to be 4k? Not sure where  this behavior
          stems from.</p>

          <p>Thanks</p>

          '
        raw: "Thanks for this model.\r\n\r\nwhen printing 'tokenizer.model_max_length',\
          \ I got a number like '1000000000000000019884624838656'.\r\n\r\nthe model_max_length\
          \ is supposed to be 4k? Not sure where  this behavior stems from.\r\n\r\n\
          Thanks"
        updatedAt: '2023-08-08T16:04:39.225Z'
      numEdits: 0
      reactions: []
    id: 64d267971a6275df575dd1c3
    type: comment
  author: huggingFace1108
  content: "Thanks for this model.\r\n\r\nwhen printing 'tokenizer.model_max_length',\
    \ I got a number like '1000000000000000019884624838656'.\r\n\r\nthe model_max_length\
    \ is supposed to be 4k? Not sure where  this behavior stems from.\r\n\r\nThanks"
  created_at: 2023-08-08 15:04:39+00:00
  edited: false
  hidden: false
  id: 64d267971a6275df575dd1c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
      fullname: Daryl Autar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: daryl149
      type: user
    createdAt: '2023-08-11T15:56:46.000Z'
    data:
      edited: false
      editors:
      - daryl149
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8831124305725098
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/620954c0c11a57389ba013d2/FlIIlobCg_OSmD4Gf0idt.jpeg?w=200&h=200&f=face
          fullname: Daryl Autar
          isHf: false
          isPro: false
          name: daryl149
          type: user
        html: '<p>That number is actually correct, because we solved long context.<br>.<br>.<br>.<br>.<br>No
          j/k, idk either, have you tried the same command with meta''s version of
          the llama-2 weights?</p>

          '
        raw: 'That number is actually correct, because we solved long context.

          .

          .

          .

          .

          No j/k, idk either, have you tried the same command with meta''s version
          of the llama-2 weights?'
        updatedAt: '2023-08-11T15:56:46.094Z'
      numEdits: 0
      reactions: []
    id: 64d65a3e6db135cfc8acb1ed
    type: comment
  author: daryl149
  content: 'That number is actually correct, because we solved long context.

    .

    .

    .

    .

    No j/k, idk either, have you tried the same command with meta''s version of the
    llama-2 weights?'
  created_at: 2023-08-11 14:56:46+00:00
  edited: false
  hidden: false
  id: 64d65a3e6db135cfc8acb1ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fdb242cfcdff894a4a49ff238536015.svg
      fullname: Brona Blecha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: esolteric
      type: user
    createdAt: '2023-08-27T16:34:33.000Z'
    data:
      edited: false
      editors:
      - esolteric
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6011438965797424
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fdb242cfcdff894a4a49ff238536015.svg
          fullname: Brona Blecha
          isHf: false
          isPro: false
          name: esolteric
          type: user
        html: "<p>Hello, I\xB4m using this model, but since yesterday, when I run\
          \ it, I\xB4m getting this error. Running on 4090.</p>\n<p>Traceback (most\
          \ recent call last):<br>File \"/root/endpoint.py\", line 43, in chat<br>response\
          \ = miner.forward(messages, num_replies = n)<br>File \"/root/endpoint.py\"\
          , line 106, in forward<br>output = self.model.generate(<br>File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context<br>return func(*args, **kwargs)<br>File \"\
          /opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1485, in generate<br>return self.sample(<br>File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2560, in sample<br>next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)<br>RuntimeError:\
          \ probability tensor contains either <code>inf</code>, <code>nan</code>\
          \ or element &lt; 0</p>\n<p>Thank you for your help in advance.</p>\n"
        raw: "Hello, I\xB4m using this model, but since yesterday, when I run it,\
          \ I\xB4m getting this error. Running on 4090.\n\nTraceback (most recent\
          \ call last):\nFile \"/root/endpoint.py\", line 43, in chat\nresponse =\
          \ miner.forward(messages, num_replies = n)\nFile \"/root/endpoint.py\",\
          \ line 106, in forward\noutput = self.model.generate(\nFile \"/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
          , line 27, in decorate_context\nreturn func(*args, **kwargs)\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1485, in generate\nreturn self.sample(\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2560, in sample\nnext_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\
          RuntimeError: probability tensor contains either `inf`, `nan` or element\
          \ < 0\n\nThank you for your help in advance.\n"
        updatedAt: '2023-08-27T16:34:33.110Z'
      numEdits: 0
      reactions: []
    id: 64eb7b19a91f5a573e29aefa
    type: comment
  author: esolteric
  content: "Hello, I\xB4m using this model, but since yesterday, when I run it, I\xB4\
    m getting this error. Running on 4090.\n\nTraceback (most recent call last):\n\
    File \"/root/endpoint.py\", line 43, in chat\nresponse = miner.forward(messages,\
    \ num_replies = n)\nFile \"/root/endpoint.py\", line 106, in forward\noutput =\
    \ self.model.generate(\nFile \"/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py\"\
    , line 27, in decorate_context\nreturn func(*args, **kwargs)\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1485, in generate\nreturn self.sample(\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2560, in sample\nnext_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\
    RuntimeError: probability tensor contains either `inf`, `nan` or element < 0\n\
    \nThank you for your help in advance.\n"
  created_at: 2023-08-27 15:34:33+00:00
  edited: false
  hidden: false
  id: 64eb7b19a91f5a573e29aefa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8149f493cda4c818c6c815924236891b.svg
      fullname: lichaochao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: chaochaoli
      type: user
    createdAt: '2023-11-20T09:40:37.000Z'
    data:
      edited: false
      editors:
      - chaochaoli
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.49919262528419495
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8149f493cda4c818c6c815924236891b.svg
          fullname: lichaochao
          isHf: false
          isPro: false
          name: chaochaoli
          type: user
        html: "<p>2048 or 4096\uFF1F<br><a href=\"https://huggingface.co/TigerResearch/tigerbot-13b-chat-v1/discussions/1#:~:text=llama2%E7%9A%84%E5%AE%9E,%E5%89%8D%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E5%AF%BC%E8%87%B4%E3%80%82\"\
          >https://huggingface.co/TigerResearch/tigerbot-13b-chat-v1/discussions/1#:~:text=llama2%E7%9A%84%E5%AE%9E,%E5%89%8D%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E5%AF%BC%E8%87%B4%E3%80%82</a></p>\n"
        raw: "2048 or 4096\uFF1F\nhttps://huggingface.co/TigerResearch/tigerbot-13b-chat-v1/discussions/1#:~:text=llama2%E7%9A%84%E5%AE%9E,%E5%89%8D%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E5%AF%BC%E8%87%B4%E3%80%82"
        updatedAt: '2023-11-20T09:40:37.987Z'
      numEdits: 0
      reactions: []
    id: 655b2995180749bf321dbb6e
    type: comment
  author: chaochaoli
  content: "2048 or 4096\uFF1F\nhttps://huggingface.co/TigerResearch/tigerbot-13b-chat-v1/discussions/1#:~:text=llama2%E7%9A%84%E5%AE%9E,%E5%89%8D%E9%BB%98%E8%AE%A4%E5%8F%82%E6%95%B0%E5%AF%BC%E8%87%B4%E3%80%82"
  created_at: 2023-11-20 09:40:37+00:00
  edited: false
  hidden: false
  id: 655b2995180749bf321dbb6e
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: daryl149/llama-2-7b-chat-hf
repo_type: model
status: open
target_branch: null
title: tokenizer.model_max_length for llama-2-7b-chat-hf
