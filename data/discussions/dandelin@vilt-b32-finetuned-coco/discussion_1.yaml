!!python/object:huggingface_hub.community.DiscussionWithDetails
author: luckylight
conflicting_files: null
created_at: 2023-04-27 11:38:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a49f0ce343b1782086a4c79cff2babe3.svg
      fullname: XingYougGuang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luckylight
      type: user
    createdAt: '2023-04-27T12:38:19.000Z'
    data:
      edited: false
      editors:
      - luckylight
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a49f0ce343b1782086a4c79cff2babe3.svg
          fullname: XingYougGuang
          isHf: false
          isPro: false
          name: luckylight
          type: user
        html: '<p>Hello, I want to ask,how can I use batch to inference?<br>Since
          the ViltProcessor can''t encoding texts longer than 40, I cut off it to
          40(cause if i don''t do it, the ViltForImageAndTextRetrieval can not work!).<br>But
          there are the processed texts less than 40(without padding), so I could
          not reorginised it as a whole batch!<br>Is there any solution to solve this
          problem? Thanks!</p>

          <pre><code class="language-python"><span class="hljs-comment"># trunking
          text code</span>

          encoding = processor(image, text, return_tensors=<span class="hljs-string">"pt"</span>)

          encoding[<span class="hljs-string">''input_ids''</span>][<span class="hljs-number">0</span>,
          <span class="hljs-number">39</span>] = encoding[<span class="hljs-string">''input_ids''</span>][<span
          class="hljs-number">0</span>, -<span class="hljs-number">1</span>]

          encoding[<span class="hljs-string">''input_ids''</span>] = encoding[<span
          class="hljs-string">''input_ids''</span>][:, :<span class="hljs-number">40</span>]

          encoding[<span class="hljs-string">''token_type_ids''</span>][<span class="hljs-number">0</span>,
          <span class="hljs-number">39</span>] = encoding[<span class="hljs-string">''token_type_ids''</span>][<span
          class="hljs-number">0</span>, -<span class="hljs-number">1</span>]

          encoding[<span class="hljs-string">''token_type_ids''</span>] = encoding[<span
          class="hljs-string">''token_type_ids''</span>][:, :<span class="hljs-number">40</span>]

          encoding[<span class="hljs-string">''attention_mask''</span>][<span class="hljs-number">0</span>,
          <span class="hljs-number">39</span>] = encoding[<span class="hljs-string">''attention_mask''</span>][<span
          class="hljs-number">0</span>, -<span class="hljs-number">1</span>]

          encoding[<span class="hljs-string">''attention_mask''</span>] = encoding[<span
          class="hljs-string">''attention_mask''</span>][:, :<span class="hljs-number">40</span>]

          </code></pre>

          <pre><code class="language-python"><span class="hljs-comment"># reformat
          it as batch code</span>

          cur_batch_data = {x: torch.concat([y, encoding[x]]) <span class="hljs-keyword">for</span>
          x, y <span class="hljs-keyword">in</span> cur_batch_data.items()}

          </code></pre>

          '
        raw: "Hello, I want to ask,how can I use batch to inference?\r\nSince the\
          \ ViltProcessor can't encoding texts longer than 40, I cut off it to 40(cause\
          \ if i don't do it, the ViltForImageAndTextRetrieval can not work!). \r\n\
          But there are the processed texts less than 40(without padding), so I could\
          \ not reorginised it as a whole batch!\r\nIs there any solution to solve\
          \ this problem? Thanks!\r\n\r\n\r\n```python\r\n# trunking text code\r\n\
          encoding = processor(image, text, return_tensors=\"pt\")\r\nencoding['input_ids'][0,\
          \ 39] = encoding['input_ids'][0, -1]\r\nencoding['input_ids'] = encoding['input_ids'][:,\
          \ :40]\r\nencoding['token_type_ids'][0, 39] = encoding['token_type_ids'][0,\
          \ -1]\r\nencoding['token_type_ids'] = encoding['token_type_ids'][:, :40]\r\
          \nencoding['attention_mask'][0, 39] = encoding['attention_mask'][0, -1]\r\
          \nencoding['attention_mask'] = encoding['attention_mask'][:, :40]\r\n```\r\
          \n\r\n\r\n```python\r\n# reformat it as batch code\r\ncur_batch_data = {x:\
          \ torch.concat([y, encoding[x]]) for x, y in cur_batch_data.items()}\r\n\
          ```"
        updatedAt: '2023-04-27T12:38:19.015Z'
      numEdits: 0
      reactions: []
    id: 644a6cbb7e555e7526d91122
    type: comment
  author: luckylight
  content: "Hello, I want to ask,how can I use batch to inference?\r\nSince the ViltProcessor\
    \ can't encoding texts longer than 40, I cut off it to 40(cause if i don't do\
    \ it, the ViltForImageAndTextRetrieval can not work!). \r\nBut there are the processed\
    \ texts less than 40(without padding), so I could not reorginised it as a whole\
    \ batch!\r\nIs there any solution to solve this problem? Thanks!\r\n\r\n\r\n```python\r\
    \n# trunking text code\r\nencoding = processor(image, text, return_tensors=\"\
    pt\")\r\nencoding['input_ids'][0, 39] = encoding['input_ids'][0, -1]\r\nencoding['input_ids']\
    \ = encoding['input_ids'][:, :40]\r\nencoding['token_type_ids'][0, 39] = encoding['token_type_ids'][0,\
    \ -1]\r\nencoding['token_type_ids'] = encoding['token_type_ids'][:, :40]\r\nencoding['attention_mask'][0,\
    \ 39] = encoding['attention_mask'][0, -1]\r\nencoding['attention_mask'] = encoding['attention_mask'][:,\
    \ :40]\r\n```\r\n\r\n\r\n```python\r\n# reformat it as batch code\r\ncur_batch_data\
    \ = {x: torch.concat([y, encoding[x]]) for x, y in cur_batch_data.items()}\r\n\
    ```"
  created_at: 2023-04-27 11:38:19+00:00
  edited: false
  hidden: false
  id: 644a6cbb7e555e7526d91122
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a49f0ce343b1782086a4c79cff2babe3.svg
      fullname: XingYougGuang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luckylight
      type: user
    createdAt: '2023-04-27T12:40:36.000Z'
    data:
      edited: true
      editors:
      - luckylight
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a49f0ce343b1782086a4c79cff2babe3.svg
          fullname: XingYougGuang
          isHf: false
          isPro: false
          name: luckylight
          type: user
        html: '<p>If this problem can not be solved, I have to evaluate the ViLT for
          mAP metric with batch=1. To be honest, this is very, very slow. Is there
          anyone can help me!</p>

          '
        raw: If this problem can not be solved, I have to evaluate the ViLT for mAP
          metric with batch=1. To be honest, this is very, very slow. Is there anyone
          can help me!
        updatedAt: '2023-04-27T12:40:55.899Z'
      numEdits: 1
      reactions: []
    id: 644a6d44d128b8663a7a1055
    type: comment
  author: luckylight
  content: If this problem can not be solved, I have to evaluate the ViLT for mAP
    metric with batch=1. To be honest, this is very, very slow. Is there anyone can
    help me!
  created_at: 2023-04-27 11:40:36+00:00
  edited: true
  hidden: false
  id: 644a6d44d128b8663a7a1055
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a49f0ce343b1782086a4c79cff2babe3.svg
      fullname: XingYougGuang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: luckylight
      type: user
    createdAt: '2023-07-15T02:24:07.000Z'
    data:
      status: closed
    id: 64b20347f7f82164df8aeef2
    type: status-change
  author: luckylight
  created_at: 2023-07-15 01:24:07+00:00
  id: 64b20347f7f82164df8aeef2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5eac4c154e876668a0c37770/rY95sfLjugbYLgJ9GHYjC.jpeg?w=200&h=200&f=face
      fullname: Luca Di Liello
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucadiliello
      type: user
    createdAt: '2024-01-12T12:59:06.000Z'
    data:
      edited: false
      editors:
      - lucadiliello
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7909846901893616
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5eac4c154e876668a0c37770/rY95sfLjugbYLgJ9GHYjC.jpeg?w=200&h=200&f=face
          fullname: Luca Di Liello
          isHf: false
          isPro: false
          name: lucadiliello
          type: user
        html: '<p>You can simply use <code>BertTokenizerFast</code> and <code>ViltImageProcessor</code>
          for encoding text and images separately, with all the benefits of batch
          encoding and possibility to set parameters by yourself.</p>

          '
        raw: You can simply use `BertTokenizerFast` and `ViltImageProcessor` for encoding
          text and images separately, with all the benefits of batch encoding and
          possibility to set parameters by yourself.
        updatedAt: '2024-01-12T12:59:06.072Z'
      numEdits: 0
      reactions: []
    id: 65a1379adce7f9ec810b1aca
    type: comment
  author: lucadiliello
  content: You can simply use `BertTokenizerFast` and `ViltImageProcessor` for encoding
    text and images separately, with all the benefits of batch encoding and possibility
    to set parameters by yourself.
  created_at: 2024-01-12 12:59:06+00:00
  edited: false
  hidden: false
  id: 65a1379adce7f9ec810b1aca
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: dandelin/vilt-b32-finetuned-coco
repo_type: model
status: closed
target_branch: null
title: batch inference
