!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ishotoli
conflicting_files: null
created_at: 2023-11-15 13:03:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e86b752896195ce66bcc0737eba18cbf.svg
      fullname: Kaiz Zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ishotoli
      type: user
    createdAt: '2023-11-15T13:03:29.000Z'
    data:
      edited: false
      editors:
      - ishotoli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6195641756057739
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e86b752896195ce66bcc0737eba18cbf.svg
          fullname: Kaiz Zhao
          isHf: false
          isPro: false
          name: ishotoli
          type: user
        html: '<p>Using the AutoGPTQ to load an 8-bit 128g model in Text-generation-webui,
          with the trust-remote-code and disable_exllama options enabled, results
          in the following error message:<br>Traceback (most recent call last):<br>File
          "/home/kaiz/workshop/text-generation-webui/modules/ui_model_menu.py", line
          210, in load_model_wrapper<br>  shared.model, shared.tokenizer = load_model(shared.model_name,
          loader)<br>File "/home/kaiz/workshop/text-generation-webui/modules/models.py",
          line 85, in load_model<br>  output = load_func_map<a rel="nofollow" href="model_name">loader</a><br>File
          "/home/kaiz/workshop/text-generation-webui/modules/models.py", line 337,
          in AutoGPTQ_loader<br>  return modules.AutoGPTQ_loader.load_quantized(model_name)<br>File
          "/home/kaiz/workshop/text-generation-webui/modules/AutoGPTQ_loader.py",
          line 58, in load_quantized<br>  model = AutoGPTQForCausalLM.from_quantized(path_to_model,
          **params)<br>File "/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/modeling/auto.py",
          line 108, in from_quantized<br>  return quant_func(<br>File "/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/modeling/_base.py",
          line 902, in from_quantized<br>  cls.fused_attn_module_type.inject_to_model(<br>File
          "/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py",
          line 154, in inject_to_model<br>  qweights = torch.cat([q_proj.qweight,
          k_proj.qweight, v_proj.qweight], dim=1)<br>torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty
          of 44.40 GiB of which 19.31 MiB is free. Including non-PyTorch memory, this
          process has 44.38 GiB memory in use. Of the allocated memory 42.91 GiB is
          allocated by PyTorch, and 698.05 MiB is reserved by PyTorch but unallocated.
          If reserved but unallocated memory is large try setting max_split_size_mb
          to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>AutoGPTQ ver: 0.4.2<br>CUDA ver: 12.1<br>nVidia Driver ver: 530.41.03</p>

          <p>34B 4bit can be successfully loaded, llama2 70B 4bit can also be successfully
          loaded. I''m not sure what caused the failure to load the 34B 8bit model
          (in theory, it should be able to load with 48G VRAM).</p>

          '
        raw: "Using the AutoGPTQ to load an 8-bit 128g model in Text-generation-webui,\
          \ with the trust-remote-code and disable_exllama options enabled, results\
          \ in the following error message:\r\nTraceback (most recent call last):\r\
          \nFile \"/home/kaiz/workshop/text-generation-webui/modules/ui_model_menu.py\"\
          , line 210, in load_model_wrapper\r\n  shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\r\nFile \"/home/kaiz/workshop/text-generation-webui/modules/models.py\"\
          , line 85, in load_model\r\n  output = load_func_map[loader](model_name)\r\
          \nFile \"/home/kaiz/workshop/text-generation-webui/modules/models.py\",\
          \ line 337, in AutoGPTQ_loader\r\n  return modules.AutoGPTQ_loader.load_quantized(model_name)\r\
          \nFile \"/home/kaiz/workshop/text-generation-webui/modules/AutoGPTQ_loader.py\"\
          , line 58, in load_quantized\r\n  model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
          \ **params)\r\nFile \"/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
          , line 108, in from_quantized\r\n  return quant_func(\r\nFile \"/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
          , line 902, in from_quantized\r\n  cls.fused_attn_module_type.inject_to_model(\r\
          \nFile \"/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
          , line 154, in inject_to_model\r\n  qweights = torch.cat([q_proj.qweight,\
          \ k_proj.qweight, v_proj.qweight], dim=1)\r\ntorch.cuda.OutOfMemoryError:\
          \ CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty\
          \ of 44.40 GiB of which 19.31 MiB is free. Including non-PyTorch memory,\
          \ this process has 44.38 GiB memory in use. Of the allocated memory 42.91\
          \ GiB is allocated by PyTorch, and 698.05 MiB is reserved by PyTorch but\
          \ unallocated. If reserved but unallocated memory is large try setting max_split_size_mb\
          \ to avoid fragmentation. See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n\r\nAutoGPTQ ver: 0.4.2\r\nCUDA ver: 12.1\r\nnVidia Driver ver: 530.41.03\r\
          \n\r\n34B 4bit can be successfully loaded, llama2 70B 4bit can also be successfully\
          \ loaded. I'm not sure what caused the failure to load the 34B 8bit model\
          \ (in theory, it should be able to load with 48G VRAM).\r\n"
        updatedAt: '2023-11-15T13:03:29.853Z'
      numEdits: 0
      reactions: []
    id: 6554c1a1af8d1ea0d2e6e966
    type: comment
  author: ishotoli
  content: "Using the AutoGPTQ to load an 8-bit 128g model in Text-generation-webui,\
    \ with the trust-remote-code and disable_exllama options enabled, results in the\
    \ following error message:\r\nTraceback (most recent call last):\r\nFile \"/home/kaiz/workshop/text-generation-webui/modules/ui_model_menu.py\"\
    , line 210, in load_model_wrapper\r\n  shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\r\nFile \"/home/kaiz/workshop/text-generation-webui/modules/models.py\"\
    , line 85, in load_model\r\n  output = load_func_map[loader](model_name)\r\nFile\
    \ \"/home/kaiz/workshop/text-generation-webui/modules/models.py\", line 337, in\
    \ AutoGPTQ_loader\r\n  return modules.AutoGPTQ_loader.load_quantized(model_name)\r\
    \nFile \"/home/kaiz/workshop/text-generation-webui/modules/AutoGPTQ_loader.py\"\
    , line 58, in load_quantized\r\n  model = AutoGPTQForCausalLM.from_quantized(path_to_model,\
    \ **params)\r\nFile \"/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/modeling/auto.py\"\
    , line 108, in from_quantized\r\n  return quant_func(\r\nFile \"/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/modeling/_base.py\"\
    , line 902, in from_quantized\r\n  cls.fused_attn_module_type.inject_to_model(\r\
    \nFile \"/home/kaiz/anaconda3/envs/aigc/lib/python3.10/site-packages/auto_gptq/nn_modules/fused_llama_attn.py\"\
    , line 154, in inject_to_model\r\n  qweights = torch.cat([q_proj.qweight, k_proj.qweight,\
    \ v_proj.qweight], dim=1)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory.\
    \ Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 44.40 GiB of which\
    \ 19.31 MiB is free. Including non-PyTorch memory, this process has 44.38 GiB\
    \ memory in use. Of the allocated memory 42.91 GiB is allocated by PyTorch, and\
    \ 698.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
    \ memory is large try setting max_split_size_mb to avoid fragmentation. See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nAutoGPTQ ver: 0.4.2\r\
    \nCUDA ver: 12.1\r\nnVidia Driver ver: 530.41.03\r\n\r\n34B 4bit can be successfully\
    \ loaded, llama2 70B 4bit can also be successfully loaded. I'm not sure what caused\
    \ the failure to load the 34B 8bit model (in theory, it should be able to load\
    \ with 48G VRAM).\r\n"
  created_at: 2023-11-15 13:03:29+00:00
  edited: false
  hidden: false
  id: 6554c1a1af8d1ea0d2e6e966
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-15T13:24:09.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9508862495422363
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>I''ve not tested with 8-bit GPTQs much.</p>

          <p>One potential issue is that 8-bit models can''t use the ExLlama kernels,
          and the ExLlama kernels use less VRAM.  So it''s possible that 34B 8-bit
          just needs more VRAM than 48GB.</p>

          <p>You could try use the 8-bit group_size None instead, that will use less
          VRAM.</p>

          '
        raw: 'I''ve not tested with 8-bit GPTQs much.


          One potential issue is that 8-bit models can''t use the ExLlama kernels,
          and the ExLlama kernels use less VRAM.  So it''s possible that 34B 8-bit
          just needs more VRAM than 48GB.


          You could try use the 8-bit group_size None instead, that will use less
          VRAM.'
        updatedAt: '2023-11-15T13:24:09.301Z'
      numEdits: 0
      reactions: []
    id: 6554c679e49634a88d5d59e6
    type: comment
  author: TheBloke
  content: 'I''ve not tested with 8-bit GPTQs much.


    One potential issue is that 8-bit models can''t use the ExLlama kernels, and the
    ExLlama kernels use less VRAM.  So it''s possible that 34B 8-bit just needs more
    VRAM than 48GB.


    You could try use the 8-bit group_size None instead, that will use less VRAM.'
  created_at: 2023-11-15 13:24:09+00:00
  edited: false
  hidden: false
  id: 6554c679e49634a88d5d59e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-16T02:27:42.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9590756893157959
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;ishotoli&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ishotoli\">@<span class=\"\
          underline\">ishotoli</span></a></span>\n\n\t</span></span> </p>\n<p>This\
          \ is the 200K ctx len model, my best guess is that fused attention injection\
          \ which is present in your error msg has allocated too much memory? May\
          \ be use --no_inject_fused_attention in cmd arg.</p>\n"
        raw: "@ishotoli \n\nThis is the 200K ctx len model, my best guess is that\
          \ fused attention injection which is present in your error msg has allocated\
          \ too much memory? May be use --no_inject_fused_attention in cmd arg.\n\n"
        updatedAt: '2023-11-16T02:27:42.732Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 65557e1ebce11b9cde8261b3
    type: comment
  author: Yhyu13
  content: "@ishotoli \n\nThis is the 200K ctx len model, my best guess is that fused\
    \ attention injection which is present in your error msg has allocated too much\
    \ memory? May be use --no_inject_fused_attention in cmd arg.\n\n"
  created_at: 2023-11-16 02:27:42+00:00
  edited: false
  hidden: false
  id: 65557e1ebce11b9cde8261b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e86b752896195ce66bcc0737eba18cbf.svg
      fullname: Kaiz Zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ishotoli
      type: user
    createdAt: '2023-11-16T03:18:34.000Z'
    data:
      edited: false
      editors:
      - ishotoli
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8795873522758484
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e86b752896195ce66bcc0737eba18cbf.svg
          fullname: Kaiz Zhao
          isHf: false
          isPro: false
          name: ishotoli
          type: user
        html: '<p>It works! Thx :)</p>

          '
        raw: It works! Thx :)
        updatedAt: '2023-11-16T03:18:34.883Z'
      numEdits: 0
      reactions: []
      relatedEventId: 65558a0a5f7b57f5851a3bbd
    id: 65558a0a5f7b57f5851a3bb7
    type: comment
  author: ishotoli
  content: It works! Thx :)
  created_at: 2023-11-16 03:18:34+00:00
  edited: false
  hidden: false
  id: 65558a0a5f7b57f5851a3bb7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/e86b752896195ce66bcc0737eba18cbf.svg
      fullname: Kaiz Zhao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ishotoli
      type: user
    createdAt: '2023-11-16T03:18:34.000Z'
    data:
      status: closed
    id: 65558a0a5f7b57f5851a3bbd
    type: status-change
  author: ishotoli
  created_at: 2023-11-16 03:18:34+00:00
  id: 65558a0a5f7b57f5851a3bbd
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Yi-34B-200K-Llamafied-GPTQ
repo_type: model
status: closed
target_branch: null
title: 'When loading 8-bits 128g onto A40 (48GB VRAM), an error occurs: CUDA out of
  memory.'
