!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rombodawg
conflicting_files: null
created_at: 2023-10-21 01:16:48+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-10-21T02:16:48.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9603692293167114
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Is it possible to merge 100% of the models instead of only a % of
          each, i just dont see why you would cut off data from either model when
          they each were trained on diffrent coding datasets. Why not merge the entire
          models together. obviously meaning the adapter models of wizardcoder and
          phind not base codellama-34b</p>

          '
        raw: Is it possible to merge 100% of the models instead of only a % of each,
          i just dont see why you would cut off data from either model when they each
          were trained on diffrent coding datasets. Why not merge the entire models
          together. obviously meaning the adapter models of wizardcoder and phind
          not base codellama-34b
        updatedAt: '2023-10-21T02:16:48.124Z'
      numEdits: 0
      reactions: []
    id: 653334903da0ff3c706b5d73
    type: comment
  author: rombodawg
  content: Is it possible to merge 100% of the models instead of only a % of each,
    i just dont see why you would cut off data from either model when they each were
    trained on diffrent coding datasets. Why not merge the entire models together.
    obviously meaning the adapter models of wizardcoder and phind not base codellama-34b
  created_at: 2023-10-21 01:16:48+00:00
  edited: false
  hidden: false
  id: 653334903da0ff3c706b5d73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d0597ff2341424c808b771/XsJsVaLVmqivOG6WxY7Uz.png?w=200&h=200&f=face
      fullname: oobabooga
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: oobabooga
      type: user
    createdAt: '2023-10-21T03:06:16.000Z'
    data:
      edited: false
      editors:
      - oobabooga
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9398140907287598
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d0597ff2341424c808b771/XsJsVaLVmqivOG6WxY7Uz.png?w=200&h=200&f=face
          fullname: oobabooga
          isHf: false
          isPro: false
          name: oobabooga
          type: user
        html: '<p>Both models are used. It''s a weighted average with weights that
          change as a function of position.  </p>

          <ul>

          <li><code>gradient_values: [0.75]</code> means that the weights are merged
          with 0.75 weight for model1 and 0.25 for model2.</li>

          <li><code>gradient_values: [0.75, 0.25]</code> means that the ratio starts
          at 0.75 and ends in 0.25 for that layer.</li>

          </ul>

          '
        raw: "Both models are used. It's a weighted average with weights that change\
          \ as a function of position.  \n\n* `gradient_values: [0.75]` means that\
          \ the weights are merged with 0.75 weight for model1 and 0.25 for model2.\n\
          * `gradient_values: [0.75, 0.25]` means that the ratio starts at 0.75 and\
          \ ends in 0.25 for that layer."
        updatedAt: '2023-10-21T03:06:16.343Z'
      numEdits: 0
      reactions: []
    id: 65334028ae42162a1ebcc38b
    type: comment
  author: oobabooga
  content: "Both models are used. It's a weighted average with weights that change\
    \ as a function of position.  \n\n* `gradient_values: [0.75]` means that the weights\
    \ are merged with 0.75 weight for model1 and 0.25 for model2.\n* `gradient_values:\
    \ [0.75, 0.25]` means that the ratio starts at 0.75 and ends in 0.25 for that\
    \ layer."
  created_at: 2023-10-21 02:06:16+00:00
  edited: false
  hidden: false
  id: 65334028ae42162a1ebcc38b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
      fullname: rombo dawg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rombodawg
      type: user
    createdAt: '2023-10-21T03:08:15.000Z'
    data:
      edited: false
      editors:
      - rombodawg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9784750938415527
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/642cc1c253e76b4c2286c58e/n8h5JKRgaAoYrE36BtHRO.jpeg?w=200&h=200&f=face
          fullname: rombo dawg
          isHf: false
          isPro: false
          name: rombodawg
          type: user
        html: '<p>Oh my bad i thought .75 meant 75% of the model was used and 25%
          of the other model when merging</p>

          '
        raw: Oh my bad i thought .75 meant 75% of the model was used and 25% of the
          other model when merging
        updatedAt: '2023-10-21T03:08:15.329Z'
      numEdits: 0
      reactions: []
    id: 6533409fc65f2e75887fb5b1
    type: comment
  author: rombodawg
  content: Oh my bad i thought .75 meant 75% of the model was used and 25% of the
    other model when merging
  created_at: 2023-10-21 02:08:15+00:00
  edited: false
  hidden: false
  id: 6533409fc65f2e75887fb5b1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: oobabooga/CodeBooga-34B-v0.1
repo_type: model
status: open
target_branch: null
title: Merge 100% of the models instead of only parts
