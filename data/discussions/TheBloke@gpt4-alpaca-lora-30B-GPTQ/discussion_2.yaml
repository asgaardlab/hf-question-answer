!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ryanlarimer
conflicting_files: null
created_at: 2023-04-15 15:37:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb695377c929068931aa5bc052fc467c.svg
      fullname: Ryan Larimer
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ryanlarimer
      type: user
    createdAt: '2023-04-15T16:37:05.000Z'
    data:
      edited: false
      editors:
      - ryanlarimer
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb695377c929068931aa5bc052fc467c.svg
          fullname: Ryan Larimer
          isHf: false
          isPro: false
          name: ryanlarimer
          type: user
        html: "<p>Tried in both repositories with <code>main</code> branch of transformers:<br><a\
          \ rel=\"nofollow\" href=\"https://github.com/qwopqwop200/GPTQ-for-LLaMa\"\
          >https://github.com/qwopqwop200/GPTQ-for-LLaMa</a><br><a rel=\"nofollow\"\
          \ href=\"https://github.com/oobabooga/GPTQ-for-LLaMa\">https://github.com/oobabooga/GPTQ-for-LLaMa</a></p>\n\
          <pre><code>RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM:\
          \              \n        Missing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.bias\"\
          , \"m\nodel.layers.0.self_attn.o_proj.bias\",  ....\n</code></pre>\n<p>then</p>\n\
          <pre><code>        Unexpected key(s) in state_dict: \"model.layers.0.self_attn.k_proj.g_idx\"\
          \n, \"model.layers.0.self_attn.o_proj.g_idx\",...\n</code></pre>\n<p>These\
          \ weights load fine: <a href=\"https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4\"\
          >https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4</a></p>\n"
        raw: "Tried in both repositories with `main` branch of transformers:\r\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMa\r\
          \nhttps://github.com/oobabooga/GPTQ-for-LLaMa\r\n\r\n```\r\nRuntimeError:\
          \ Error(s) in loading state_dict for LlamaForCausalLM:              \r\n\
          \        Missing key(s) in state_dict: \"model.layers.0.self_attn.k_proj.bias\"\
          , \"m\r\nodel.layers.0.self_attn.o_proj.bias\",  ....\r\n```\r\n\r\nthen\r\
          \n\r\n```\r\n        Unexpected key(s) in state_dict: \"model.layers.0.self_attn.k_proj.g_idx\"\
          \r\n, \"model.layers.0.self_attn.o_proj.g_idx\",...\r\n```\r\n\r\nThese\
          \ weights load fine: https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4"
        updatedAt: '2023-04-15T16:37:05.397Z'
      numEdits: 0
      reactions: []
    id: 643ad2b1c5f633a7fa803cfa
    type: comment
  author: ryanlarimer
  content: "Tried in both repositories with `main` branch of transformers:\r\nhttps://github.com/qwopqwop200/GPTQ-for-LLaMa\r\
    \nhttps://github.com/oobabooga/GPTQ-for-LLaMa\r\n\r\n```\r\nRuntimeError: Error(s)\
    \ in loading state_dict for LlamaForCausalLM:              \r\n        Missing\
    \ key(s) in state_dict: \"model.layers.0.self_attn.k_proj.bias\", \"m\r\nodel.layers.0.self_attn.o_proj.bias\"\
    ,  ....\r\n```\r\n\r\nthen\r\n\r\n```\r\n        Unexpected key(s) in state_dict:\
    \ \"model.layers.0.self_attn.k_proj.g_idx\"\r\n, \"model.layers.0.self_attn.o_proj.g_idx\"\
    ,...\r\n```\r\n\r\nThese weights load fine: https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-Int4"
  created_at: 2023-04-15 15:37:05+00:00
  edited: false
  hidden: false
  id: 643ad2b1c5f633a7fa803cfa
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/gpt4-alpaca-lora-30B-GPTQ
repo_type: model
status: open
target_branch: null
title: Weights do not work (non-128g weights do work!)
