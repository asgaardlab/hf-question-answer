!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MithrilMan
conflicting_files: null
created_at: 2023-04-15 15:43:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/477717f056f808eb5e77e6e5c2d6be43.svg
      fullname: Fabio Angela
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MithrilMan
      type: user
    createdAt: '2023-04-15T16:43:41.000Z'
    data:
      edited: false
      editors:
      - MithrilMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/477717f056f808eb5e77e6e5c2d6be43.svg
          fullname: Fabio Angela
          isHf: false
          isPro: false
          name: MithrilMan
          type: user
        html: '<p>Thanks for your models, reading the stats it seems you are on a
          similar setup like mine, I''ve a 3090ti card (CPU is negligible, anyway
          I''ve a Ryzen 7 7700x)<br>Which model would you suggest to use on oobabooga
          for this setup?<br>I''ve actually installed your vicuna 1.1 and I''m going
          to try it, my goal is to try to code some kind of auto gpt (in c#) and make
          use of oobabooga API to communicate between the llm and the app and it will
          be hosted on my machine</p>

          <p>Thanks</p>

          '
        raw: "Thanks for your models, reading the stats it seems you are on a similar\
          \ setup like mine, I've a 3090ti card (CPU is negligible, anyway I've a\
          \ Ryzen 7 7700x)\r\nWhich model would you suggest to use on oobabooga for\
          \ this setup?\r\nI've actually installed your vicuna 1.1 and I'm going to\
          \ try it, my goal is to try to code some kind of auto gpt (in c#) and make\
          \ use of oobabooga API to communicate between the llm and the app and it\
          \ will be hosted on my machine\r\n\r\nThanks"
        updatedAt: '2023-04-15T16:43:41.366Z'
      numEdits: 0
      reactions: []
    id: 643ad43d3b71e2b0cc1dad8c
    type: comment
  author: MithrilMan
  content: "Thanks for your models, reading the stats it seems you are on a similar\
    \ setup like mine, I've a 3090ti card (CPU is negligible, anyway I've a Ryzen\
    \ 7 7700x)\r\nWhich model would you suggest to use on oobabooga for this setup?\r\
    \nI've actually installed your vicuna 1.1 and I'm going to try it, my goal is\
    \ to try to code some kind of auto gpt (in c#) and make use of oobabooga API to\
    \ communicate between the llm and the app and it will be hosted on my machine\r\
    \n\r\nThanks"
  created_at: 2023-04-15 15:43:41+00:00
  edited: false
  hidden: false
  id: 643ad43d3b71e2b0cc1dad8c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T22:58:07.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Hi there</p>

          <p>Vicuna 1.1 is a very good and popular model. I''d definitely recommend
          trying that.</p>

          <p>I haven''t experimented with gpt4-alpaca too much yet. In theory it should
          be even better, but I can''t say for certain.  Because it''s based on a
          30B model that should definitely help in theory. But then I think Vicuna
          had particularly good training data, probably better than this model. So
          I don''t know how those two factors will compare.</p>

          <p>You should just be able GPT4-Alpaca in 24GB, so it''s definitely worth
          trying. </p>

          <p>At home I only have a 16GB AMD 6900XT, which I run in macOS. I do all
          my GPTQ conversions in the cloud, using first Google Colab and now Runpod.
          So I pay a little bit of money per hour to get access to NVidia GPUs, like
          the 3090 and 4090 with 24GB, and the A100 with 40GB or 80GB.</p>

          <p>Eventually I plan to get a GPU at home as well but that will have to
          wait a few months until I can get the cash together! :)</p>

          <p>Good luck with your project, hope it works out well.</p>

          <p>If you want to come and discuss LLMs in more detail and talk about your
          development, there''s a lot of LLM discussion in the Nomic.Ai/GPT4ALL Discord
          server: <a rel="nofollow" href="https://discord.gg/ZHaesTnb">https://discord.gg/ZHaesTnb</a></p>

          '
        raw: "Hi there\n\nVicuna 1.1 is a very good and popular model. I'd definitely\
          \ recommend trying that.\n\nI haven't experimented with gpt4-alpaca too\
          \ much yet. In theory it should be even better, but I can't say for certain.\
          \  Because it's based on a 30B model that should definitely help in theory.\
          \ But then I think Vicuna had particularly good training data, probably\
          \ better than this model. So I don't know how those two factors will compare.\n\
          \nYou should just be able GPT4-Alpaca in 24GB, so it's definitely worth\
          \ trying. \n\nAt home I only have a 16GB AMD 6900XT, which I run in macOS.\
          \ I do all my GPTQ conversions in the cloud, using first Google Colab and\
          \ now Runpod. So I pay a little bit of money per hour to get access to NVidia\
          \ GPUs, like the 3090 and 4090 with 24GB, and the A100 with 40GB or 80GB.\n\
          \nEventually I plan to get a GPU at home as well but that will have to wait\
          \ a few months until I can get the cash together! :)\n\nGood luck with your\
          \ project, hope it works out well.\n\nIf you want to come and discuss LLMs\
          \ in more detail and talk about your development, there's a lot of LLM discussion\
          \ in the Nomic.Ai/GPT4ALL Discord server: https://discord.gg/ZHaesTnb"
        updatedAt: '2023-04-16T22:59:11.489Z'
      numEdits: 1
      reactions: []
      relatedEventId: 643c7d7f8c901ebc901f89d2
    id: 643c7d7f8c901ebc901f89d1
    type: comment
  author: TheBloke
  content: "Hi there\n\nVicuna 1.1 is a very good and popular model. I'd definitely\
    \ recommend trying that.\n\nI haven't experimented with gpt4-alpaca too much yet.\
    \ In theory it should be even better, but I can't say for certain.  Because it's\
    \ based on a 30B model that should definitely help in theory. But then I think\
    \ Vicuna had particularly good training data, probably better than this model.\
    \ So I don't know how those two factors will compare.\n\nYou should just be able\
    \ GPT4-Alpaca in 24GB, so it's definitely worth trying. \n\nAt home I only have\
    \ a 16GB AMD 6900XT, which I run in macOS. I do all my GPTQ conversions in the\
    \ cloud, using first Google Colab and now Runpod. So I pay a little bit of money\
    \ per hour to get access to NVidia GPUs, like the 3090 and 4090 with 24GB, and\
    \ the A100 with 40GB or 80GB.\n\nEventually I plan to get a GPU at home as well\
    \ but that will have to wait a few months until I can get the cash together! :)\n\
    \nGood luck with your project, hope it works out well.\n\nIf you want to come\
    \ and discuss LLMs in more detail and talk about your development, there's a lot\
    \ of LLM discussion in the Nomic.Ai/GPT4ALL Discord server: https://discord.gg/ZHaesTnb"
  created_at: 2023-04-16 21:58:07+00:00
  edited: true
  hidden: false
  id: 643c7d7f8c901ebc901f89d1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-04-16T22:58:07.000Z'
    data:
      status: closed
    id: 643c7d7f8c901ebc901f89d2
    type: status-change
  author: TheBloke
  created_at: 2023-04-16 21:58:07+00:00
  id: 643c7d7f8c901ebc901f89d2
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/477717f056f808eb5e77e6e5c2d6be43.svg
      fullname: Fabio Angela
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MithrilMan
      type: user
    createdAt: '2023-04-16T23:57:21.000Z'
    data:
      edited: false
      editors:
      - MithrilMan
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/477717f056f808eb5e77e6e5c2d6be43.svg
          fullname: Fabio Angela
          isHf: false
          isPro: false
          name: MithrilMan
          type: user
        html: '<p>Thanks for the discord link, I''ll take a look and GL with the GPU
          plan, it''s worth it :)</p>

          '
        raw: Thanks for the discord link, I'll take a look and GL with the GPU plan,
          it's worth it :)
        updatedAt: '2023-04-16T23:57:21.965Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - TheBloke
    id: 643c8b6125c7610a1cd874da
    type: comment
  author: MithrilMan
  content: Thanks for the discord link, I'll take a look and GL with the GPU plan,
    it's worth it :)
  created_at: 2023-04-16 22:57:21+00:00
  edited: false
  hidden: false
  id: 643c8b6125c7610a1cd874da
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/gpt4-alpaca-lora-30B-GPTQ
repo_type: model
status: closed
target_branch: null
title: about 3090ti card
