!!python/object:huggingface_hub.community.DiscussionWithDetails
author: JosephusCheung
conflicting_files: null
created_at: 2023-08-31 09:11:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-08-31T10:11:40.000Z'
    data:
      edited: false
      editors:
      - JosephusCheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8569148778915405
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
          fullname: "Jos\xE9phus Cheung"
          isHf: false
          isPro: false
          name: JosephusCheung
          type: user
        html: '<p><a href="https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat">https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</a></p>

          <p>Similar to <a href="https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13">LLaMAfied
          Qwen-7B-Chat</a>, the visual parts and LLM are separated, and LLM is restructured
          and recalibrated into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer).
          It can be used with any tools that are compatible with LLaMA, such as stream
          output, llama.cpp quantization, and so on.</p>

          '
        raw: "https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\r\n\r\
          \nSimilar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
          \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
          \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be\
          \ used with any tools that are compatible with LLaMA, such as stream output,\
          \ llama.cpp quantization, and so on."
        updatedAt: '2023-08-31T10:11:40.621Z'
      numEdits: 0
      reactions: []
    id: 64f0675cfdc928dc05101b4d
    type: comment
  author: JosephusCheung
  content: "https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\r\n\r\n\
    Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
    \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
    \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be used\
    \ with any tools that are compatible with LLaMA, such as stream output, llama.cpp\
    \ quantization, and so on."
  created_at: 2023-08-31 09:11:40+00:00
  edited: false
  hidden: false
  id: 64f0675cfdc928dc05101b4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
      fullname: charryshi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: charry2000
      type: user
    createdAt: '2023-09-06T15:32:42.000Z'
    data:
      edited: false
      editors:
      - charry2000
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.964038074016571
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6027ab83c3637c8252da98888d0c90b3.svg
          fullname: charryshi
          isHf: false
          isPro: false
          name: charry2000
          type: user
        html: '<p>Thanks for your work. I think the vision.bin file is the visual
          parts, How to  fine tuning this model and inference?</p>

          '
        raw: Thanks for your work. I think the vision.bin file is the visual parts,
          How to  fine tuning this model and inference?
        updatedAt: '2023-09-06T15:32:42.023Z'
      numEdits: 0
      reactions: []
    id: 64f89b9ab2d67ae7150b5f41
    type: comment
  author: charry2000
  content: Thanks for your work. I think the vision.bin file is the visual parts,
    How to  fine tuning this model and inference?
  created_at: 2023-09-06 14:32:42+00:00
  edited: false
  hidden: false
  id: 64f89b9ab2d67ae7150b5f41
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-09-06T15:46:37.000Z'
    data:
      edited: false
      editors:
      - JosephusCheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9014990329742432
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
          fullname: "Jos\xE9phus Cheung"
          isHf: false
          isPro: false
          name: JosephusCheung
          type: user
        html: '<blockquote>

          <p>Thanks for your work. I think the vision.bin file is the visual parts,
          How to  fine tuning this model and inference?</p>

          </blockquote>

          <p>The structure of the LLM part is identical to that of LLaMA, allowing
          you to utilize HF transformers with the LlamaForCausalLM. For the vision
          part, you can utilize visual.py from the original Qwen-VL repository. This
          allows you to convert images into LM input embeddings. You can then manually
          concatenate these with your text instruction input for the LLM.</p>

          <p>It is quite obvious I think.</p>

          '
        raw: '> Thanks for your work. I think the vision.bin file is the visual parts,
          How to  fine tuning this model and inference?


          The structure of the LLM part is identical to that of LLaMA, allowing you
          to utilize HF transformers with the LlamaForCausalLM. For the vision part,
          you can utilize visual.py from the original Qwen-VL repository. This allows
          you to convert images into LM input embeddings. You can then manually concatenate
          these with your text instruction input for the LLM.


          It is quite obvious I think.'
        updatedAt: '2023-09-06T15:46:37.514Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64f89eddbaa3b4ec4e566a74
    id: 64f89eddbaa3b4ec4e566a73
    type: comment
  author: JosephusCheung
  content: '> Thanks for your work. I think the vision.bin file is the visual parts,
    How to  fine tuning this model and inference?


    The structure of the LLM part is identical to that of LLaMA, allowing you to utilize
    HF transformers with the LlamaForCausalLM. For the vision part, you can utilize
    visual.py from the original Qwen-VL repository. This allows you to convert images
    into LM input embeddings. You can then manually concatenate these with your text
    instruction input for the LLM.


    It is quite obvious I think.'
  created_at: 2023-09-06 14:46:37+00:00
  edited: false
  hidden: false
  id: 64f89eddbaa3b4ec4e566a73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-09-06T15:46:37.000Z'
    data:
      status: closed
    id: 64f89eddbaa3b4ec4e566a74
    type: status-change
  author: JosephusCheung
  created_at: 2023-09-06 14:46:37+00:00
  id: 64f89eddbaa3b4ec4e566a74
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-09-06T15:46:40.000Z'
    data:
      status: open
    id: 64f89ee0b2d67ae7150c0572
    type: status-change
  author: JosephusCheung
  created_at: 2023-09-06 14:46:40+00:00
  id: 64f89ee0b2d67ae7150c0572
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8594baba33769ed33171bf154472673b.svg
      fullname: code
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpcvcode
      type: user
    createdAt: '2023-09-26T18:17:11.000Z'
    data:
      edited: false
      editors:
      - nlpcvcode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.861193060874939
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8594baba33769ed33171bf154472673b.svg
          fullname: code
          isHf: false
          isPro: false
          name: nlpcvcode
          type: user
        html: '<blockquote>

          <p><a href="https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat">https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</a></p>

          <p>Similar to <a href="https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13">LLaMAfied
          Qwen-7B-Chat</a>, the visual parts and LLM are separated, and LLM is restructured
          and recalibrated into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer).
          It can be used with any tools that are compatible with LLaMA, such as stream
          output, llama.cpp quantization, and so on.</p>

          </blockquote>

          <p>how to use?</p>

          '
        raw: "> https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n\
          > \n> Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
          \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
          \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be\
          \ used with any tools that are compatible with LLaMA, such as stream output,\
          \ llama.cpp quantization, and so on.\n\nhow to use?"
        updatedAt: '2023-09-26T18:17:11.226Z'
      numEdits: 0
      reactions: []
    id: 65132027b7994cff61d9396a
    type: comment
  author: nlpcvcode
  content: "> https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n> \n\
    > Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
    \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
    \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be used\
    \ with any tools that are compatible with LLaMA, such as stream output, llama.cpp\
    \ quantization, and so on.\n\nhow to use?"
  created_at: 2023-09-26 17:17:11+00:00
  edited: false
  hidden: false
  id: 65132027b7994cff61d9396a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
      fullname: "Jos\xE9phus Cheung"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JosephusCheung
      type: user
    createdAt: '2023-09-26T18:38:54.000Z'
    data:
      edited: false
      editors:
      - JosephusCheung
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8740688562393188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f54abfaa4be54e61c1052720eb498703.svg
          fullname: "Jos\xE9phus Cheung"
          isHf: false
          isPro: false
          name: JosephusCheung
          type: user
        html: '<blockquote>

          <blockquote>

          <p><a href="https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat">https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</a></p>

          <p>Similar to <a href="https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13">LLaMAfied
          Qwen-7B-Chat</a>, the visual parts and LLM are separated, and LLM is restructured
          and recalibrated into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer).
          It can be used with any tools that are compatible with LLaMA, such as stream
          output, llama.cpp quantization, and so on.</p>

          </blockquote>

          <p>how to use?</p>

          </blockquote>

          <p>Use LLM the way you use LLaMA-2, and use visual.py from Qwen-VL for VL
          part, which is obvious for literate people.</p>

          '
        raw: "> > https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n\
          > > \n> > Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
          \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
          \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be\
          \ used with any tools that are compatible with LLaMA, such as stream output,\
          \ llama.cpp quantization, and so on.\n> \n> how to use?\n\nUse LLM the way\
          \ you use LLaMA-2, and use visual.py from Qwen-VL for VL part, which is\
          \ obvious for literate people."
        updatedAt: '2023-09-26T18:38:54.980Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Tonic
    id: 6513253ef60393414aff9dd1
    type: comment
  author: JosephusCheung
  content: "> > https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n\
    > > \n> > Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
    \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
    \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be used\
    \ with any tools that are compatible with LLaMA, such as stream output, llama.cpp\
    \ quantization, and so on.\n> \n> how to use?\n\nUse LLM the way you use LLaMA-2,\
    \ and use visual.py from Qwen-VL for VL part, which is obvious for literate people."
  created_at: 2023-09-26 17:38:54+00:00
  edited: false
  hidden: false
  id: 6513253ef60393414aff9dd1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8594baba33769ed33171bf154472673b.svg
      fullname: code
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nlpcvcode
      type: user
    createdAt: '2023-09-27T03:27:59.000Z'
    data:
      edited: false
      editors:
      - nlpcvcode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.908623456954956
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8594baba33769ed33171bf154472673b.svg
          fullname: code
          isHf: false
          isPro: false
          name: nlpcvcode
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p><a href="https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat">https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</a></p>

          <p>Similar to <a href="https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13">LLaMAfied
          Qwen-7B-Chat</a>, the visual parts and LLM are separated, and LLM is restructured
          and recalibrated into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer).
          It can be used with any tools that are compatible with LLaMA, such as stream
          output, llama.cpp quantization, and so on.</p>

          </blockquote>

          <p>how to use?</p>

          </blockquote>

          <p>Use LLM the way you use LLaMA-2, and use visual.py from Qwen-VL for VL
          part, which is obvious for literate people.</p>

          </blockquote>

          <p>This is difficult for me,  i don''t know how to merge two models for
          inferencing. could you give me some code examples? Thanks!</p>

          '
        raw: "> > > https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n\
          > > > \n> > > Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
          \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
          \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be\
          \ used with any tools that are compatible with LLaMA, such as stream output,\
          \ llama.cpp quantization, and so on.\n> > \n> > how to use?\n> \n> Use LLM\
          \ the way you use LLaMA-2, and use visual.py from Qwen-VL for VL part, which\
          \ is obvious for literate people.\n\nThis is difficult for me,  i don't\
          \ know how to merge two models for inferencing. could you give me some code\
          \ examples? Thanks!"
        updatedAt: '2023-09-27T03:27:59.496Z'
      numEdits: 0
      reactions: []
    id: 6513a13f6de9c503f307727d
    type: comment
  author: nlpcvcode
  content: "> > > https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n\
    > > > \n> > > Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
    \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
    \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be used\
    \ with any tools that are compatible with LLaMA, such as stream output, llama.cpp\
    \ quantization, and so on.\n> > \n> > how to use?\n> \n> Use LLM the way you use\
    \ LLaMA-2, and use visual.py from Qwen-VL for VL part, which is obvious for literate\
    \ people.\n\nThis is difficult for me,  i don't know how to merge two models for\
    \ inferencing. could you give me some code examples? Thanks!"
  created_at: 2023-09-27 02:27:59+00:00
  edited: false
  hidden: false
  id: 6513a13f6de9c503f307727d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4c039d25c853041ebce2714ad906e0dc.svg
      fullname: Riddle Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: riddlechen
      type: user
    createdAt: '2023-12-30T07:13:06.000Z'
    data:
      edited: false
      editors:
      - riddlechen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8729771375656128
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4c039d25c853041ebce2714ad906e0dc.svg
          fullname: Riddle Chen
          isHf: false
          isPro: false
          name: riddlechen
          type: user
        html: '<blockquote>

          <p><a href="https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat">https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat</a></p>

          <p>Similar to <a href="https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13">LLaMAfied
          Qwen-7B-Chat</a>, the visual parts and LLM are separated, and LLM is restructured
          and recalibrated into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer).
          It can be used with any tools that are compatible with LLaMA, such as stream
          output, llama.cpp quantization, and so on.</p>

          </blockquote>

          <p>Great work, thank you very much.</p>

          '
        raw: "> https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n\
          > \n> Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
          \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
          \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be\
          \ used with any tools that are compatible with LLaMA, such as stream output,\
          \ llama.cpp quantization, and so on.\n\nGreat work, thank you very much."
        updatedAt: '2023-12-30T07:13:06.689Z'
      numEdits: 0
      reactions: []
    id: 658fc3023533188375222c66
    type: comment
  author: riddlechen
  content: "> https://huggingface.co/JosephusCheung/Qwen-VL-LLaMAfied-7B-Chat\n> \n\
    > Similar to [LLaMAfied Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat/discussions/13),\
    \ the visual parts and LLM are separated, and LLM is restructured and recalibrated\
    \ into the standard LLaMA/LLaMA2 format (with GPT-2 tokenizer). It can be used\
    \ with any tools that are compatible with LLaMA, such as stream output, llama.cpp\
    \ quantization, and so on.\n\nGreat work, thank you very much."
  created_at: 2023-12-30 07:13:06+00:00
  edited: false
  hidden: false
  id: 658fc3023533188375222c66
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Qwen/Qwen-VL-Chat
repo_type: model
status: open
target_branch: null
title: "Unofficial LLaMAfied Version in HF format - \u975E\u5B98\u65B9\u7684LLaMA\u5316\
  HF\u683C\u5F0F\u7248\u672C"
