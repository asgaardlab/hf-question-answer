!!python/object:huggingface_hub.community.DiscussionWithDetails
author: garrettbaber
conflicting_files: null
created_at: 2023-06-08 21:55:07+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63757a0fb18c6b01497d15a3/M9YJ-C1jtLQAhwZSkIhM8.jpeg?w=200&h=200&f=face
      fullname: Garrett Baber
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: garrettbaber
      type: user
    createdAt: '2023-06-08T22:55:07.000Z'
    data:
      edited: true
      editors:
      - garrettbaber
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9526278972625732
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63757a0fb18c6b01497d15a3/M9YJ-C1jtLQAhwZSkIhM8.jpeg?w=200&h=200&f=face
          fullname: Garrett Baber
          isHf: false
          isPro: false
          name: garrettbaber
          type: user
        html: '<p>Hello, Thank you for a great model! I''m new to NLP so forgive me
          if this is an obvious question. But what were your probability thresholds
          (or whatever they''re called) that you considered an emotion label to be
          correctly identified? Was it .5? I''m checking because I''ve read it can
          vary, but did not see it listed in your model card. If it matters, I''m
          specifically wanting to know the accuracy metric of just the "fear" label,
          as I''m using this in my research. Going by my small dataset of reported
          dreams and dream emotions, the model with a .5 probability cut-off had a
          true positive rate of 57%, a false positive rate of 19%, a true negative
          rate of 77%, and a false negative rate of 43%.</p>

          '
        raw: Hello, Thank you for a great model! I'm new to NLP so forgive me if this
          is an obvious question. But what were your probability thresholds (or whatever
          they're called) that you considered an emotion label to be correctly identified?
          Was it .5? I'm checking because I've read it can vary, but did not see it
          listed in your model card. If it matters, I'm specifically wanting to know
          the accuracy metric of just the "fear" label, as I'm using this in my research.
          Going by my small dataset of reported dreams and dream emotions, the model
          with a .5 probability cut-off had a true positive rate of 57%, a false positive
          rate of 19%, a true negative rate of 77%, and a false negative rate of 43%.
        updatedAt: '2023-06-09T02:00:15.971Z'
      numEdits: 1
      reactions: []
    id: 64825c4b2e73ce18ad324e8c
    type: comment
  author: garrettbaber
  content: Hello, Thank you for a great model! I'm new to NLP so forgive me if this
    is an obvious question. But what were your probability thresholds (or whatever
    they're called) that you considered an emotion label to be correctly identified?
    Was it .5? I'm checking because I've read it can vary, but did not see it listed
    in your model card. If it matters, I'm specifically wanting to know the accuracy
    metric of just the "fear" label, as I'm using this in my research. Going by my
    small dataset of reported dreams and dream emotions, the model with a .5 probability
    cut-off had a true positive rate of 57%, a false positive rate of 19%, a true
    negative rate of 77%, and a false negative rate of 43%.
  created_at: 2023-06-08 21:55:07+00:00
  edited: true
  hidden: false
  id: 64825c4b2e73ce18ad324e8c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: cardiffnlp/twitter-roberta-base-emotion-multilabel-latest
repo_type: model
status: open
target_branch: null
title: Trying to understand probability threshold for determining accuracy of model
