!!python/object:huggingface_hub.community.DiscussionWithDetails
author: qgallouedec
conflicting_files: null
created_at: 2023-04-19 18:31:28+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg?w=200&h=200&f=face
      fullname: "Quentin Gallou\xE9dec"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: qgallouedec
      type: user
    createdAt: '2023-04-19T19:31:28.000Z'
    data:
      edited: false
      editors:
      - qgallouedec
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg?w=200&h=200&f=face
          fullname: "Quentin Gallou\xE9dec"
          isHf: false
          isPro: false
          name: qgallouedec
          type: user
        html: "<p>When I run</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">import</span> torch\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> Speech2TextProcessor,\
          \ Speech2TextForConditionalGeneration\n<span class=\"hljs-keyword\">from</span>\
          \ datasets <span class=\"hljs-keyword\">import</span> load_dataset\n<span\
          \ class=\"hljs-keyword\">import</span> soundfile <span class=\"hljs-keyword\"\
          >as</span> sf\n\nmodel = Speech2TextForConditionalGeneration.from_pretrained(<span\
          \ class=\"hljs-string\">\"facebook/s2t-small-covost2-fr-en-st\"</span>)\n\
          processor = Speech2TextProcessor.from_pretrained(<span class=\"hljs-string\"\
          >\"facebook/s2t-small-covost2-fr-en-st\"</span>)\n\n<span class=\"hljs-keyword\"\
          >def</span> <span class=\"hljs-title function_\">map_to_array</span>(<span\
          \ class=\"hljs-params\">batch</span>):\n    speech, _ = sf.read(batch[<span\
          \ class=\"hljs-string\">\"file\"</span>])\n    batch[<span class=\"hljs-string\"\
          >\"speech\"</span>] = speech\n    <span class=\"hljs-keyword\">return</span>\
          \ batch\n\nds = load_dataset(\n    <span class=\"hljs-string\">\"patrickvonplaten/librispeech_asr_dummy\"\
          </span>,\n    <span class=\"hljs-string\">\"clean\"</span>,\n    split=<span\
          \ class=\"hljs-string\">\"validation\"</span>\n)\nds = ds.<span class=\"\
          hljs-built_in\">map</span>(map_to_array)\n\ninputs = processor(\n    ds[<span\
          \ class=\"hljs-string\">\"speech\"</span>][<span class=\"hljs-number\">0</span>],\n\
          \    sampling_rate=<span class=\"hljs-number\">48_000</span>,\n    return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>\n)\ngenerated_ids = model.generate(input_ids=inputs[<span\
          \ class=\"hljs-string\">\"input_features\"</span>], attention_mask=inputs[<span\
          \ class=\"hljs-string\">\"attention_mask\"</span>])\n\ntranslation = processor.batch_decode(generated_ids,\
          \ skip_special_tokens=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n\
          <p>I get the following error.</p>\n<pre><code>(env) quentingallouedec@MacBook-Pro-de-Quentin\
          \ gia % /Users/quentingallouedec/gia/env/bin/python /Users/qu\nentingallouedec/gia/examples/s2t.py\n\
          WARNING:datasets.builder:Found cached dataset librispeech_asr_dummy (/Users/quentingallouedec/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\n\
          WARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/quentingallouedec/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc/cache-2944a8173f386e6d.arrow\n\
          \u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502 /Users/quentingallouedec/gia/examples/s2t.py:26\
          \ in &lt;module&gt;                                      \u2502\n\u2502\
          \                                                                      \
          \                            \u2502\n\u2502   23 \u2502   sampling_rate=48_000,\
          \                                                                   \u2502\
          \n\u2502   24 \u2502   return_tensors=\"pt\"                           \
          \                                          \u2502\n\u2502   25 )       \
          \                                                                      \
          \              \u2502\n\u2502 \u2771 26 generated_ids = model.generate(input_ids=inputs[\"\
          input_features\"], attention_mask=inputs    \u2502\n\u2502   27        \
          \                                                                      \
          \               \u2502\n\u2502   28 translation = processor.batch_decode(generated_ids,\
          \ skip_special_tokens=True)               \u2502\n\u2502   29          \
          \                                                                      \
          \             \u2502\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\
          \ in  \u2502\n\u2502 decorate_context                                  \
          \                                               \u2502\n\u2502         \
          \                                                                      \
          \                   \u2502\n\u2502   112 \u2502   @functools.wraps(func)\
          \                                                                 \u2502\
          \n\u2502   113 \u2502   def decorate_context(*args, **kwargs):         \
          \                                        \u2502\n\u2502   114 \u2502   \u2502\
          \   with ctx_factory():                                                \
          \                \u2502\n\u2502 \u2771 115 \u2502   \u2502   \u2502   return\
          \ func(*args, **kwargs)                                                \
          \   \u2502\n\u2502   116 \u2502                                        \
          \                                                  \u2502\n\u2502   117\
          \ \u2502   return decorate_context                                     \
          \                           \u2502\n\u2502   118                       \
          \                                                                     \u2502\
          \n\u2502                                                               \
          \                                   \u2502\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/transformers/generation/utils.py:1\
          \ \u2502\n\u2502 197 in generate                                       \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   1194 \u2502   \u2502                  \
          \                                                                   \u2502\
          \n\u2502   1195 \u2502   \u2502   generation_config = copy.deepcopy(generation_config)\
          \                              \u2502\n\u2502   1196 \u2502   \u2502   model_kwargs\
          \ = generation_config.update(**kwargs)  # All unused kwargs must be m  \u2502\
          \n\u2502 \u2771 1197 \u2502   \u2502   self._validate_model_kwargs(model_kwargs.copy())\
          \                                  \u2502\n\u2502   1198 \u2502   \u2502\
          \                                                                      \
          \               \u2502\n\u2502   1199 \u2502   \u2502   # 2. Set generation\
          \ parameters if not already defined                             \u2502\n\
          \u2502   1200 \u2502   \u2502   logits_processor = logits_processor if logits_processor\
          \ is not None else LogitsP  \u2502\n\u2502                             \
          \                                                                     \u2502\
          \n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/transformers/generation/utils.py:1\
          \ \u2502\n\u2502 090 in _validate_model_kwargs                         \
          \                                           \u2502\n\u2502             \
          \                                                                      \
          \               \u2502\n\u2502   1087 \u2502   \u2502   \u2502   \u2502\
          \   unused_model_args.append(key)                                      \
          \       \u2502\n\u2502   1088 \u2502   \u2502                          \
          \                                                           \u2502\n\u2502\
          \   1089 \u2502   \u2502   if unused_model_args:                       \
          \                                      \u2502\n\u2502 \u2771 1090 \u2502\
          \   \u2502   \u2502   raise ValueError(                                \
          \                             \u2502\n\u2502   1091 \u2502   \u2502   \u2502\
          \   \u2502   f\"The following `model_kwargs` are not used by the model:\
          \ {unused_model_  \u2502\n\u2502   1092 \u2502   \u2502   \u2502   \u2502\
          \   \" generate arguments will also show up in this list)\"            \
          \         \u2502\n\u2502   1093 \u2502   \u2502   \u2502   )           \
          \                                                                  \u2502\
          \n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u256F\nValueError: The following `model_kwargs` are not\
          \ used by the model: ['input_ids'] (note: typos in the \ngenerate arguments\
          \ will also show up in this list)\n</code></pre>\n<p>Versions:</p>\n<ul>\n\
          <li>Python: 3.10.10</li>\n<li><code>numpy</code>: 1.24.2</li>\n<li><code>sentencepiece</code>:\
          \ 0.1.98</li>\n<li><code>soundfile</code>: 0.12.1</li>\n<li><code>tokenizers</code>:\
          \ 0.13.2</li>\n<li><code>torch</code>: 2.0.0</li>\n<li><code>torchaudio</code>:\
          \ 2.0.1</li>\n<li><code>transformers</code>: 4.26.1</li>\n</ul>\n"
        raw: "When I run\r\n\r\n```python\r\nimport torch\r\nfrom transformers import\
          \ Speech2TextProcessor, Speech2TextForConditionalGeneration\r\nfrom datasets\
          \ import load_dataset\r\nimport soundfile as sf\r\n\r\nmodel = Speech2TextForConditionalGeneration.from_pretrained(\"\
          facebook/s2t-small-covost2-fr-en-st\")\r\nprocessor = Speech2TextProcessor.from_pretrained(\"\
          facebook/s2t-small-covost2-fr-en-st\")\r\n\r\ndef map_to_array(batch):\r\
          \n    speech, _ = sf.read(batch[\"file\"])\r\n    batch[\"speech\"] = speech\r\
          \n    return batch\r\n\r\nds = load_dataset(\r\n    \"patrickvonplaten/librispeech_asr_dummy\"\
          ,\r\n    \"clean\",\r\n    split=\"validation\"\r\n)\r\nds = ds.map(map_to_array)\r\
          \n\r\ninputs = processor(\r\n    ds[\"speech\"][0],\r\n    sampling_rate=48_000,\r\
          \n    return_tensors=\"pt\"\r\n)\r\ngenerated_ids = model.generate(input_ids=inputs[\"\
          input_features\"], attention_mask=inputs[\"attention_mask\"])\r\n\r\ntranslation\
          \ = processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n\
          ```\r\n\r\nI get the following error.\r\n\r\n```\r\n(env) quentingallouedec@MacBook-Pro-de-Quentin\
          \ gia % /Users/quentingallouedec/gia/env/bin/python /Users/qu\r\nentingallouedec/gia/examples/s2t.py\r\
          \nWARNING:datasets.builder:Found cached dataset librispeech_asr_dummy (/Users/quentingallouedec/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\r\
          \nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/quentingallouedec/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc/cache-2944a8173f386e6d.arrow\r\
          \n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent\
          \ call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256E\r\n\u2502\
          \                                                                      \
          \                            \u2502\r\n\u2502 /Users/quentingallouedec/gia/examples/s2t.py:26\
          \ in <module>                                      \u2502\r\n\u2502    \
          \                                                                      \
          \                        \u2502\r\n\u2502   23 \u2502   sampling_rate=48_000,\
          \                                                                   \u2502\
          \r\n\u2502   24 \u2502   return_tensors=\"pt\"                         \
          \                                            \u2502\r\n\u2502   25 )   \
          \                                                                      \
          \                  \u2502\r\n\u2502 \u2771 26 generated_ids = model.generate(input_ids=inputs[\"\
          input_features\"], attention_mask=inputs    \u2502\r\n\u2502   27      \
          \                                                                      \
          \                 \u2502\r\n\u2502   28 translation = processor.batch_decode(generated_ids,\
          \ skip_special_tokens=True)               \u2502\r\n\u2502   29        \
          \                                                                      \
          \               \u2502\r\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\
          \ in  \u2502\r\n\u2502 decorate_context                                \
          \                                                 \u2502\r\n\u2502     \
          \                                                                      \
          \                       \u2502\r\n\u2502   112 \u2502   @functools.wraps(func)\
          \                                                                 \u2502\
          \r\n\u2502   113 \u2502   def decorate_context(*args, **kwargs):       \
          \                                          \u2502\r\n\u2502   114 \u2502\
          \   \u2502   with ctx_factory():                                       \
          \                         \u2502\r\n\u2502 \u2771 115 \u2502   \u2502  \
          \ \u2502   return func(*args, **kwargs)                                \
          \                   \u2502\r\n\u2502   116 \u2502                      \
          \                                                                    \u2502\
          \r\n\u2502   117 \u2502   return decorate_context                      \
          \                                          \u2502\r\n\u2502   118      \
          \                                                                      \
          \                \u2502\r\n\u2502                                      \
          \                                                            \u2502\r\n\u2502\
          \ /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/transformers/generation/utils.py:1\
          \ \u2502\r\n\u2502 197 in generate                                     \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   1194 \u2502   \u2502            \
          \                                                                      \
          \   \u2502\r\n\u2502   1195 \u2502   \u2502   generation_config = copy.deepcopy(generation_config)\
          \                              \u2502\r\n\u2502   1196 \u2502   \u2502 \
          \  model_kwargs = generation_config.update(**kwargs)  # All unused kwargs\
          \ must be m  \u2502\r\n\u2502 \u2771 1197 \u2502   \u2502   self._validate_model_kwargs(model_kwargs.copy())\
          \                                  \u2502\r\n\u2502   1198 \u2502   \u2502\
          \                                                                      \
          \               \u2502\r\n\u2502   1199 \u2502   \u2502   # 2. Set generation\
          \ parameters if not already defined                             \u2502\r\
          \n\u2502   1200 \u2502   \u2502   logits_processor = logits_processor if\
          \ logits_processor is not None else LogitsP  \u2502\r\n\u2502          \
          \                                                                      \
          \                  \u2502\r\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/transformers/generation/utils.py:1\
          \ \u2502\r\n\u2502 090 in _validate_model_kwargs                       \
          \                                             \u2502\r\n\u2502         \
          \                                                                      \
          \                   \u2502\r\n\u2502   1087 \u2502   \u2502   \u2502   \u2502\
          \   unused_model_args.append(key)                                      \
          \       \u2502\r\n\u2502   1088 \u2502   \u2502                        \
          \                                                             \u2502\r\n\
          \u2502   1089 \u2502   \u2502   if unused_model_args:                  \
          \                                           \u2502\r\n\u2502 \u2771 1090\
          \ \u2502   \u2502   \u2502   raise ValueError(                         \
          \                                    \u2502\r\n\u2502   1091 \u2502   \u2502\
          \   \u2502   \u2502   f\"The following `model_kwargs` are not used by the\
          \ model: {unused_model_  \u2502\r\n\u2502   1092 \u2502   \u2502   \u2502\
          \   \u2502   \" generate arguments will also show up in this list)\"   \
          \                  \u2502\r\n\u2502   1093 \u2502   \u2502   \u2502   )\
          \                                                                      \
          \       \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
          \u2500\u2500\u2500\u2500\u2500\u2500\u256F\r\nValueError: The following\
          \ `model_kwargs` are not used by the model: ['input_ids'] (note: typos in\
          \ the \r\ngenerate arguments will also show up in this list)\r\n```\r\n\r\
          \nVersions:\r\n\r\n- Python: 3.10.10\r\n- `numpy`: 1.24.2\r\n- `sentencepiece`:\
          \ 0.1.98\r\n- `soundfile`: 0.12.1\r\n- `tokenizers`: 0.13.2\r\n- `torch`:\
          \ 2.0.0\r\n- `torchaudio`: 2.0.1\r\n- `transformers`: 4.26.1\r\n"
        updatedAt: '2023-04-19T19:31:28.460Z'
      numEdits: 0
      reactions: []
    id: 64404190757aa3c2ad85e37f
    type: comment
  author: qgallouedec
  content: "When I run\r\n\r\n```python\r\nimport torch\r\nfrom transformers import\
    \ Speech2TextProcessor, Speech2TextForConditionalGeneration\r\nfrom datasets import\
    \ load_dataset\r\nimport soundfile as sf\r\n\r\nmodel = Speech2TextForConditionalGeneration.from_pretrained(\"\
    facebook/s2t-small-covost2-fr-en-st\")\r\nprocessor = Speech2TextProcessor.from_pretrained(\"\
    facebook/s2t-small-covost2-fr-en-st\")\r\n\r\ndef map_to_array(batch):\r\n   \
    \ speech, _ = sf.read(batch[\"file\"])\r\n    batch[\"speech\"] = speech\r\n \
    \   return batch\r\n\r\nds = load_dataset(\r\n    \"patrickvonplaten/librispeech_asr_dummy\"\
    ,\r\n    \"clean\",\r\n    split=\"validation\"\r\n)\r\nds = ds.map(map_to_array)\r\
    \n\r\ninputs = processor(\r\n    ds[\"speech\"][0],\r\n    sampling_rate=48_000,\r\
    \n    return_tensors=\"pt\"\r\n)\r\ngenerated_ids = model.generate(input_ids=inputs[\"\
    input_features\"], attention_mask=inputs[\"attention_mask\"])\r\n\r\ntranslation\
    \ = processor.batch_decode(generated_ids, skip_special_tokens=True)\r\n```\r\n\
    \r\nI get the following error.\r\n\r\n```\r\n(env) quentingallouedec@MacBook-Pro-de-Quentin\
    \ gia % /Users/quentingallouedec/gia/env/bin/python /Users/qu\r\nentingallouedec/gia/examples/s2t.py\r\
    \nWARNING:datasets.builder:Found cached dataset librispeech_asr_dummy (/Users/quentingallouedec/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc)\r\
    \nWARNING:datasets.arrow_dataset:Loading cached processed dataset at /Users/quentingallouedec/.cache/huggingface/datasets/patrickvonplaten___librispeech_asr_dummy/clean/2.1.0/f2c70a4d03ab4410954901bde48c54b85ca1b7f9bf7d616e7e2a72b5ee6ddbfc/cache-2944a8173f386e6d.arrow\r\
    \n\u256D\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u256E\r\n\u2502                                     \
    \                                                             \u2502\r\n\u2502\
    \ /Users/quentingallouedec/gia/examples/s2t.py:26 in <module>                \
    \                      \u2502\r\n\u2502                                      \
    \                                                            \u2502\r\n\u2502\
    \   23 \u2502   sampling_rate=48_000,                                        \
    \                           \u2502\r\n\u2502   24 \u2502   return_tensors=\"pt\"\
    \                                                                     \u2502\r\
    \n\u2502   25 )                                                              \
    \                             \u2502\r\n\u2502 \u2771 26 generated_ids = model.generate(input_ids=inputs[\"\
    input_features\"], attention_mask=inputs    \u2502\r\n\u2502   27            \
    \                                                                            \
    \     \u2502\r\n\u2502   28 translation = processor.batch_decode(generated_ids,\
    \ skip_special_tokens=True)               \u2502\r\n\u2502   29              \
    \                                                                            \
    \   \u2502\r\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\
    \ in  \u2502\r\n\u2502 decorate_context                                      \
    \                                           \u2502\r\n\u2502                 \
    \                                                                            \
    \     \u2502\r\n\u2502   112 \u2502   @functools.wraps(func)                 \
    \                                                \u2502\r\n\u2502   113 \u2502\
    \   def decorate_context(*args, **kwargs):                                   \
    \              \u2502\r\n\u2502   114 \u2502   \u2502   with ctx_factory():  \
    \                                                              \u2502\r\n\u2502\
    \ \u2771 115 \u2502   \u2502   \u2502   return func(*args, **kwargs)         \
    \                                          \u2502\r\n\u2502   116 \u2502     \
    \                                                                            \
    \         \u2502\r\n\u2502   117 \u2502   return decorate_context            \
    \                                                    \u2502\r\n\u2502   118  \
    \                                                                            \
    \              \u2502\r\n\u2502                                              \
    \                                                    \u2502\r\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/transformers/generation/utils.py:1\
    \ \u2502\r\n\u2502 197 in generate                                           \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   1194 \u2502   \u2502                                    \
    \                                                 \u2502\r\n\u2502   1195 \u2502\
    \   \u2502   generation_config = copy.deepcopy(generation_config)            \
    \                  \u2502\r\n\u2502   1196 \u2502   \u2502   model_kwargs = generation_config.update(**kwargs)\
    \  # All unused kwargs must be m  \u2502\r\n\u2502 \u2771 1197 \u2502   \u2502\
    \   self._validate_model_kwargs(model_kwargs.copy())                         \
    \         \u2502\r\n\u2502   1198 \u2502   \u2502                            \
    \                                                         \u2502\r\n\u2502   1199\
    \ \u2502   \u2502   # 2. Set generation parameters if not already defined    \
    \                         \u2502\r\n\u2502   1200 \u2502   \u2502   logits_processor\
    \ = logits_processor if logits_processor is not None else LogitsP  \u2502\r\n\u2502\
    \                                                                            \
    \                      \u2502\r\n\u2502 /Users/quentingallouedec/gia/env/lib/python3.10/site-packages/transformers/generation/utils.py:1\
    \ \u2502\r\n\u2502 090 in _validate_model_kwargs                             \
    \                                       \u2502\r\n\u2502                     \
    \                                                                            \
    \ \u2502\r\n\u2502   1087 \u2502   \u2502   \u2502   \u2502   unused_model_args.append(key)\
    \                                             \u2502\r\n\u2502   1088 \u2502 \
    \  \u2502                                                                    \
    \                 \u2502\r\n\u2502   1089 \u2502   \u2502   if unused_model_args:\
    \                                                             \u2502\r\n\u2502\
    \ \u2771 1090 \u2502   \u2502   \u2502   raise ValueError(                   \
    \                                          \u2502\r\n\u2502   1091 \u2502   \u2502\
    \   \u2502   \u2502   f\"The following `model_kwargs` are not used by the model:\
    \ {unused_model_  \u2502\r\n\u2502   1092 \u2502   \u2502   \u2502   \u2502  \
    \ \" generate arguments will also show up in this list)\"                    \
    \ \u2502\r\n\u2502   1093 \u2502   \u2502   \u2502   )                       \
    \                                                      \u2502\r\n\u2570\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\
    \u2500\u2500\u2500\u2500\u2500\u256F\r\nValueError: The following `model_kwargs`\
    \ are not used by the model: ['input_ids'] (note: typos in the \r\ngenerate arguments\
    \ will also show up in this list)\r\n```\r\n\r\nVersions:\r\n\r\n- Python: 3.10.10\r\
    \n- `numpy`: 1.24.2\r\n- `sentencepiece`: 0.1.98\r\n- `soundfile`: 0.12.1\r\n\
    - `tokenizers`: 0.13.2\r\n- `torch`: 2.0.0\r\n- `torchaudio`: 2.0.1\r\n- `transformers`:\
    \ 4.26.1\r\n"
  created_at: 2023-04-19 18:31:28+00:00
  edited: false
  hidden: false
  id: 64404190757aa3c2ad85e37f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: facebook/s2t-small-covost2-fr-en-st
repo_type: model
status: open
target_branch: null
title: ValueError when running the example script
