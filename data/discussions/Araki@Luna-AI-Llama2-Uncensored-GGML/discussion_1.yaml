!!python/object:huggingface_hub.community.DiscussionWithDetails
author: FenixInDarkSolo
conflicting_files: null
created_at: 2023-07-23 13:04:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
      fullname: fenixlam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: FenixInDarkSolo
      type: user
    createdAt: '2023-07-23T14:04:01.000Z'
    data:
      edited: false
      editors:
      - FenixInDarkSolo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9598590135574341
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cda35181065a8481017e5b05e8ae7d8e.svg
          fullname: fenixlam
          isHf: false
          isPro: false
          name: FenixInDarkSolo
          type: user
        html: '<p>I am just a user, but I am also curious about Llama-2''s structure.<br>Do
          you think it can be trained to be a superhot model?</p>

          '
        raw: "I am just a user, but I am also curious about Llama-2's structure. \r\
          \nDo you think it can be trained to be a superhot model?"
        updatedAt: '2023-07-23T14:04:01.301Z'
      numEdits: 0
      reactions: []
    id: 64bd3351140491ca9f5084b9
    type: comment
  author: FenixInDarkSolo
  content: "I am just a user, but I am also curious about Llama-2's structure. \r\n\
    Do you think it can be trained to be a superhot model?"
  created_at: 2023-07-23 13:04:01+00:00
  edited: false
  hidden: false
  id: 64bd3351140491ca9f5084b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c4b86b59e9988e7fcceb1e/eKLl4BVCp3Hrrl50KGtg7.jpeg?w=200&h=200&f=face
      fullname: Araki
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Araki
      type: user
    createdAt: '2023-07-23T18:18:51.000Z'
    data:
      edited: false
      editors:
      - Araki
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9006127119064331
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c4b86b59e9988e7fcceb1e/eKLl4BVCp3Hrrl50KGtg7.jpeg?w=200&h=200&f=face
          fullname: Araki
          isHf: false
          isPro: false
          name: Araki
          type: user
        html: '<p>Hi, as far as I know, you can''t merge anything with kaiokendev''s
          superhot model except LLaMA itself, which excludes everything, even LLaMA
          2.</p>

          <p>If you want extended context, you can try <a rel="nofollow" href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.36">koboldcpp
          1.36</a> with the following params: <code>--ropeconfig 0.5 10000 --contextsize
          8192</code> (I can''t test it right now, though, you might have to swap
          0.5 with 0.25.). But remember that LLaMA already has the context window
          of 4096 tokens, not 2048 as it was with the original LLaMA: <code>--ropeconfig
          1 10000 --contextsize 4096</code> to utilize it with koboldcpp 1.36.</p>

          <p>If you want super<em>hot</em>''s spice, then we''d have to wait for kaiokendev
          to train another LoRA, over LLaMA 2.</p>

          '
        raw: 'Hi, as far as I know, you can''t merge anything with kaiokendev''s superhot
          model except LLaMA itself, which excludes everything, even LLaMA 2.


          If you want extended context, you can try [koboldcpp 1.36](https://github.com/LostRuins/koboldcpp/releases/tag/v1.36)
          with the following params: `--ropeconfig 0.5 10000 --contextsize 8192` (I
          can''t test it right now, though, you might have to swap 0.5 with 0.25.).
          But remember that LLaMA already has the context window of 4096 tokens, not
          2048 as it was with the original LLaMA: `--ropeconfig 1 10000 --contextsize
          4096` to utilize it with koboldcpp 1.36.


          If you want super*hot*''s spice, then we''d have to wait for kaiokendev
          to train another LoRA, over LLaMA 2.'
        updatedAt: '2023-07-23T18:18:51.689Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - FenixInDarkSolo
    id: 64bd6f0b8496ee0fb6fc4141
    type: comment
  author: Araki
  content: 'Hi, as far as I know, you can''t merge anything with kaiokendev''s superhot
    model except LLaMA itself, which excludes everything, even LLaMA 2.


    If you want extended context, you can try [koboldcpp 1.36](https://github.com/LostRuins/koboldcpp/releases/tag/v1.36)
    with the following params: `--ropeconfig 0.5 10000 --contextsize 8192` (I can''t
    test it right now, though, you might have to swap 0.5 with 0.25.). But remember
    that LLaMA already has the context window of 4096 tokens, not 2048 as it was with
    the original LLaMA: `--ropeconfig 1 10000 --contextsize 4096` to utilize it with
    koboldcpp 1.36.


    If you want super*hot*''s spice, then we''d have to wait for kaiokendev to train
    another LoRA, over LLaMA 2.'
  created_at: 2023-07-23 17:18:51+00:00
  edited: false
  hidden: false
  id: 64bd6f0b8496ee0fb6fc4141
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Araki/Luna-AI-Llama2-Uncensored-GGML
repo_type: model
status: open
target_branch: null
title: Do you think it can be superhot?
