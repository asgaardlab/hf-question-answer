!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lalaser1899
conflicting_files: null
created_at: 2023-06-28 07:58:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c95eba110b1762f31a3ad5787f678a80.svg
      fullname: Alex Seres
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lalaser1899
      type: user
    createdAt: '2023-06-28T08:58:20.000Z'
    data:
      edited: false
      editors:
      - lalaser1899
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6604503989219666
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c95eba110b1762f31a3ad5787f678a80.svg
          fullname: Alex Seres
          isHf: false
          isPro: false
          name: lalaser1899
          type: user
        html: '<p>using "from sagemaker.huggingface import HuggingFaceModel" and deploying
          with the defined task:</p>

          <p>hub = {<br>    ''HF_MODEL_ID'':''google/pix2struct-docvqa-base'',<br>    ''HF_TASK'':
          ''visual-question-answering''<br>}</p>

          <p>Is successfully deploying, but the result is a useless endpoint for inference.
          The issue is that the error &lt;"message": "A header text must be provided
          for VQA models."&gt; pops up, caused by the incompatibility between "/pix2struct/image_processing_pix2struct.py"
          and "/pipelines/visual_question_answering.py":</p>

          <p>File "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py",
          line 1109, in <strong>call</strong><br>return self.run_single(inputs, preprocess_params,
          forward_params, postprocess_params)</p>

          <p>File "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/visual_question_answering.py",
          line 117, in preprocess<br> image_features = self.image_processor(images=image,
          return_tensors=self.framework)</p>

          <p>File "/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py",
          line 458, in <strong>call</strong><br>return self.preprocess(images, **kwargs)</p>

          <p>File "/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py",
          line 390, in<br>ValueError: A header text must be provided for VQA models.</p>

          <p>As the Pix2struct specific image processing function demands the image
          and header_text as inputs, but the standard VQA pipeline image processer
          only passes the image bit.</p>

          <p>I am using:<br>transformers_version=''4.28.1'',<br>    pytorch_version=''2.0.0'',<br>    py_version=''py310'',</p>

          <p>and, after deployment, calling the predictor with:</p>

          <p>predictor.predict({<br>  "image": "<a rel="nofollow" href="https://9to5mac.com/wp-content/uploads/sites/6/2019/04/Screen-Shot-2019-04-18-at-11.29.01-AM.png?resize=1024,746&quot;">https://9to5mac.com/wp-content/uploads/sites/6/2019/04/Screen-Shot-2019-04-18-at-11.29.01-AM.png?resize=1024,746"</a>,<br>  "question":
          text<br>})</p>

          '
        raw: "using \"from sagemaker.huggingface import HuggingFaceModel\" and deploying\
          \ with the defined task:\r\n\r\nhub = {\r\n\t'HF_MODEL_ID':'google/pix2struct-docvqa-base',\r\
          \n\t'HF_TASK': 'visual-question-answering'\r\n}\r\n\r\nIs successfully deploying,\
          \ but the result is a useless endpoint for inference. The issue is that\
          \ the error <\"message\": \"A header text must be provided for VQA models.\"\
          > pops up, caused by the incompatibility between \"/pix2struct/image_processing_pix2struct.py\"\
          \ and \"/pipelines/visual_question_answering.py\":\r\n\r\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1109, in __call__\r\nreturn self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n\r\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/visual_question_answering.py\"\
          , line 117, in preprocess\r\n image_features = self.image_processor(images=image,\
          \ return_tensors=self.framework)\r\n\r\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py\"\
          , line 458, in __call__\r\nreturn self.preprocess(images, **kwargs)\r\n\r\
          \nFile \"/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py\"\
          , line 390, in\r\nValueError: A header text must be provided for VQA models.\r\
          \n\r\nAs the Pix2struct specific image processing function demands the image\
          \ and header_text as inputs, but the standard VQA pipeline image processer\
          \ only passes the image bit.\r\n\r\n\r\nI am using:\r\ntransformers_version='4.28.1',\r\
          \n\tpytorch_version='2.0.0',\r\n\tpy_version='py310',\r\n\r\nand, after\
          \ deployment, calling the predictor with:\r\n\r\npredictor.predict({\r\n\
          \  \"image\": \"https://9to5mac.com/wp-content/uploads/sites/6/2019/04/Screen-Shot-2019-04-18-at-11.29.01-AM.png?resize=1024,746\"\
          , \r\n  \"question\": text\r\n})\r\n"
        updatedAt: '2023-06-28T08:58:20.433Z'
      numEdits: 0
      reactions: []
    id: 649bf62cbbc74ae4d1a10d3c
    type: comment
  author: lalaser1899
  content: "using \"from sagemaker.huggingface import HuggingFaceModel\" and deploying\
    \ with the defined task:\r\n\r\nhub = {\r\n\t'HF_MODEL_ID':'google/pix2struct-docvqa-base',\r\
    \n\t'HF_TASK': 'visual-question-answering'\r\n}\r\n\r\nIs successfully deploying,\
    \ but the result is a useless endpoint for inference. The issue is that the error\
    \ <\"message\": \"A header text must be provided for VQA models.\"> pops up, caused\
    \ by the incompatibility between \"/pix2struct/image_processing_pix2struct.py\"\
    \ and \"/pipelines/visual_question_answering.py\":\r\n\r\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1109, in __call__\r\nreturn self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n\r\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/pipelines/visual_question_answering.py\"\
    , line 117, in preprocess\r\n image_features = self.image_processor(images=image,\
    \ return_tensors=self.framework)\r\n\r\nFile \"/opt/conda/lib/python3.10/site-packages/transformers/image_processing_utils.py\"\
    , line 458, in __call__\r\nreturn self.preprocess(images, **kwargs)\r\n\r\nFile\
    \ \"/opt/conda/lib/python3.10/site-packages/transformers/models/pix2struct/image_processing_pix2struct.py\"\
    , line 390, in\r\nValueError: A header text must be provided for VQA models.\r\
    \n\r\nAs the Pix2struct specific image processing function demands the image and\
    \ header_text as inputs, but the standard VQA pipeline image processer only passes\
    \ the image bit.\r\n\r\n\r\nI am using:\r\ntransformers_version='4.28.1',\r\n\t\
    pytorch_version='2.0.0',\r\n\tpy_version='py310',\r\n\r\nand, after deployment,\
    \ calling the predictor with:\r\n\r\npredictor.predict({\r\n  \"image\": \"https://9to5mac.com/wp-content/uploads/sites/6/2019/04/Screen-Shot-2019-04-18-at-11.29.01-AM.png?resize=1024,746\"\
    , \r\n  \"question\": text\r\n})\r\n"
  created_at: 2023-06-28 07:58:20+00:00
  edited: false
  hidden: false
  id: 649bf62cbbc74ae4d1a10d3c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: google/pix2struct-base
repo_type: model
status: open
target_branch: null
title: Pix2struct Sagemaker deployment Failing because of task Incompatibility
