!!python/object:huggingface_hub.community.DiscussionWithDetails
author: offchan
conflicting_files: null
created_at: 2023-06-09 19:11:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63732587759c1ca82047777f/494SybZgxdi5t5ry0Bdkv.jpeg?w=200&h=200&f=face
      fullname: Chanchana Sornsoontorn
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: offchan
      type: user
    createdAt: '2023-06-09T20:11:40.000Z'
    data:
      edited: false
      editors:
      - offchan
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9513118267059326
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63732587759c1ca82047777f/494SybZgxdi5t5ry0Bdkv.jpeg?w=200&h=200&f=face
          fullname: Chanchana Sornsoontorn
          isHf: false
          isPro: false
          name: offchan
          type: user
        html: '<p>Am I understanding correctly that the control image given to ControlNet
          Tile is just the downscaled version of the original image?</p>

          <p>Let me explain how I understand it:<br>In the training phase, if the
          original image is 512x512, you would downscale it to 256x256 (or something
          like that), then upscale it back to 512x512 then use it as control image.
          This downscaling and upscaling will make the control image more blurry based
          on the resampler that you use. Ideally if you want it to look like a bunch
          of tiles then the resampler is probably Nearest Neighbor based, right?</p>

          <p>At inference, to guarantee that it still works, you still probably need
          to do the same downscaling and upscaling process. But maybe the model generalizes
          well enough that you can just feed the unprocessed 512x512 image directly
          and it would generate a sharper version of the image. It''s like the model
          has learned to make image sharper given any control image (even the one
          that''s already sharp).<br>But I doubt its ability to work without the downscaling/upscaling
          process because the model has never seen such sharp control image in the
          training set before. Does this work in practice?</p>

          <p>Please correct me if I''m wrong on any points.<br>Thanks!</p>

          '
        raw: "Am I understanding correctly that the control image given to ControlNet\
          \ Tile is just the downscaled version of the original image?\r\n\r\nLet\
          \ me explain how I understand it:\r\nIn the training phase, if the original\
          \ image is 512x512, you would downscale it to 256x256 (or something like\
          \ that), then upscale it back to 512x512 then use it as control image. This\
          \ downscaling and upscaling will make the control image more blurry based\
          \ on the resampler that you use. Ideally if you want it to look like a bunch\
          \ of tiles then the resampler is probably Nearest Neighbor based, right?\r\
          \n\r\nAt inference, to guarantee that it still works, you still probably\
          \ need to do the same downscaling and upscaling process. But maybe the model\
          \ generalizes well enough that you can just feed the unprocessed 512x512\
          \ image directly and it would generate a sharper version of the image. It's\
          \ like the model has learned to make image sharper given any control image\
          \ (even the one that's already sharp).\r\nBut I doubt its ability to work\
          \ without the downscaling/upscaling process because the model has never\
          \ seen such sharp control image in the training set before. Does this work\
          \ in practice?\r\n\r\nPlease correct me if I'm wrong on any points.\r\n\
          Thanks!"
        updatedAt: '2023-06-09T20:11:40.012Z'
      numEdits: 0
      reactions: []
    id: 6483877ca91c79de99058659
    type: comment
  author: offchan
  content: "Am I understanding correctly that the control image given to ControlNet\
    \ Tile is just the downscaled version of the original image?\r\n\r\nLet me explain\
    \ how I understand it:\r\nIn the training phase, if the original image is 512x512,\
    \ you would downscale it to 256x256 (or something like that), then upscale it\
    \ back to 512x512 then use it as control image. This downscaling and upscaling\
    \ will make the control image more blurry based on the resampler that you use.\
    \ Ideally if you want it to look like a bunch of tiles then the resampler is probably\
    \ Nearest Neighbor based, right?\r\n\r\nAt inference, to guarantee that it still\
    \ works, you still probably need to do the same downscaling and upscaling process.\
    \ But maybe the model generalizes well enough that you can just feed the unprocessed\
    \ 512x512 image directly and it would generate a sharper version of the image.\
    \ It's like the model has learned to make image sharper given any control image\
    \ (even the one that's already sharp).\r\nBut I doubt its ability to work without\
    \ the downscaling/upscaling process because the model has never seen such sharp\
    \ control image in the training set before. Does this work in practice?\r\n\r\n\
    Please correct me if I'm wrong on any points.\r\nThanks!"
  created_at: 2023-06-09 19:11:40+00:00
  edited: false
  hidden: false
  id: 6483877ca91c79de99058659
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: takuma104/control_v11
repo_type: model
status: open
target_branch: null
title: What is the algorithm used to extract control image for ControlNet Tile?
