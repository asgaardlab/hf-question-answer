!!python/object:huggingface_hub.community.DiscussionWithDetails
author: devanghingu
conflicting_files: null
created_at: 2022-11-09 10:10:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
      fullname: devang hingu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devanghingu
      type: user
    createdAt: '2022-11-09T10:10:57.000Z'
    data:
      edited: false
      editors:
      - devanghingu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
          fullname: devang hingu
          isHf: false
          isPro: false
          name: devanghingu
          type: user
        html: '<p>Hello guys,<br>i just new here, i used paulhindemith/fasttext-classification
          model but while executing it raise the following error in my COLAB notebook.</p>

          <pre><code>HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/paulhindemith/fasttext-classification/resolve/2022.11.7/config.json

          </code></pre>

          '
        raw: "Hello guys,\r\ni just new here, i used paulhindemith/fasttext-classification\
          \ model but while executing it raise the following error in my COLAB notebook.\r\
          \n```\r\nHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/paulhindemith/fasttext-classification/resolve/2022.11.7/config.json\r\
          \n\r\n```"
        updatedAt: '2022-11-09T10:10:57.012Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - paulhindemith
    id: 636b7cb196ac7131fecb3229
    type: comment
  author: devanghingu
  content: "Hello guys,\r\ni just new here, i used paulhindemith/fasttext-classification\
    \ model but while executing it raise the following error in my COLAB notebook.\r\
    \n```\r\nHTTPError: 404 Client Error: Not Found for url: https://huggingface.co/paulhindemith/fasttext-classification/resolve/2022.11.7/config.json\r\
    \n\r\n```"
  created_at: 2022-11-09 10:10:57+00:00
  edited: false
  hidden: false
  id: 636b7cb196ac7131fecb3229
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
      fullname: Taizo Kaneko
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: paulhindemith
      type: user
    createdAt: '2022-11-10T08:49:56.000Z'
    data:
      edited: false
      editors:
      - paulhindemith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
          fullname: Taizo Kaneko
          isHf: false
          isPro: false
          name: paulhindemith
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;devanghingu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/devanghingu\"\
          >@<span class=\"underline\">devanghingu</span></a></span>\n\n\t</span></span><br>Thank\
          \ you for your issue, and I'm new too.</p>\n<p>This error is caused by my\
          \ forgetting to create '2022.11.7' branch.</p>\n<p>I pushed my fixed commit,\
          \ so could you see in your  COLAB notebook?</p>\n"
        raw: "Hello @devanghingu \nThank you for your issue, and I'm new too.\n\n\
          This error is caused by my forgetting to create '2022.11.7' branch.\n\n\
          I pushed my fixed commit, so could you see in your  COLAB notebook?"
        updatedAt: '2022-11-10T08:49:56.561Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - devanghingu
    id: 636cbb341df179e8b8424c33
    type: comment
  author: paulhindemith
  content: "Hello @devanghingu \nThank you for your issue, and I'm new too.\n\nThis\
    \ error is caused by my forgetting to create '2022.11.7' branch.\n\nI pushed my\
    \ fixed commit, so could you see in your  COLAB notebook?"
  created_at: 2022-11-10 08:49:56+00:00
  edited: false
  hidden: false
  id: 636cbb341df179e8b8424c33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
      fullname: devang hingu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devanghingu
      type: user
    createdAt: '2022-11-10T08:54:43.000Z'
    data:
      edited: false
      editors:
      - devanghingu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
          fullname: devang hingu
          isHf: false
          isPro: false
          name: devanghingu
          type: user
        html: "<p>Sure <span data-props=\"{&quot;user&quot;:&quot;paulhindemith&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/paulhindemith\"\
          >@<span class=\"underline\">paulhindemith</span></a></span>\n\n\t</span></span>\
          \ Let me check</p>\n"
        raw: Sure @paulhindemith Let me check
        updatedAt: '2022-11-10T08:54:43.924Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - paulhindemith
    id: 636cbc538cb710cbc994b188
    type: comment
  author: devanghingu
  content: Sure @paulhindemith Let me check
  created_at: 2022-11-10 08:54:43+00:00
  edited: false
  hidden: false
  id: 636cbc538cb710cbc994b188
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
      fullname: devang hingu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devanghingu
      type: user
    createdAt: '2022-11-10T09:04:21.000Z'
    data:
      edited: false
      editors:
      - devanghingu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
          fullname: devang hingu
          isHf: false
          isPro: false
          name: devanghingu
          type: user
        html: "<p>Got another encoding error on it </p>\n<pre><code>     2   classifier\
          \ = pipeline(\"zero-shot-classification\", \"paulhindemith/fasttext-classification\"\
          , revision=\"2022.11.7\", trust_remote_code=True)\n----&gt; 3   output =\
          \ classifier(data, candidate_labels=topics, hypothesis_template=\"{}\",\
          \ multi_label=True)\n      4   return output\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in __call__(self, sequences, *args, **kwargs)\n    180             raise\
          \ ValueError(f\"Unable to understand extra arguments {args}\")\n    181\
          \ \n--&gt; 182         return super().__call__(sequences, **kwargs)\n  \
          \  183 \n    184     def preprocess(self, inputs, candidate_labels=None,\
          \ hypothesis_template=\"This example is {}.\"):\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\
          \ in __call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1072             return self.iterate(inputs, preprocess_params, forward_params,\
          \ postprocess_params)\n   1073         else:\n-&gt; 1074             return\
          \ self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \   1075 \n   1076     def run_multi(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\
          \ in run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n\
          \   1093     def run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):\n   1094         all_outputs = []\n-&gt; 1095   \
          \      for model_inputs in self.preprocess(inputs, **preprocess_params):\n\
          \   1096             model_outputs = self.forward(model_inputs, **forward_params)\n\
          \   1097             all_outputs.append(model_outputs)\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in preprocess(self, inputs, candidate_labels, hypothesis_template)\n \
          \   186 \n    187         for i, (candidate_label, sequence_pair) in enumerate(zip(candidate_labels,\
          \ sequence_pairs)):\n--&gt; 188             model_input = self._parse_and_tokenize([sequence_pair])\n\
          \    189 \n    190             yield {\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in _parse_and_tokenize(self, sequence_pairs, padding, add_special_tokens,\
          \ truncation, **kwargs)\n    115                 )\n    116            \
          \ else:\n--&gt; 117                 raise e\n    118 \n    119         return\
          \ inputs\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in _parse_and_tokenize(self, sequence_pairs, padding, add_special_tokens,\
          \ truncation, **kwargs)\n     97                 return_tensors=return_tensors,\n\
          \     98                 padding=padding,\n---&gt; 99                 truncation=truncation,\n\
          \    100             )\n    101         except Exception as e:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
          \ in __call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens,\
          \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
          \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2486             if not self._in_target_context_manager:\n\
          \   2487                 self._switch_to_input_mode()\n-&gt; 2488      \
          \       encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n\
          \   2489         if text_target is not None:\n   2490             self._switch_to_target_mode()\n\
          \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
          \ in _call_one(self, text, text_pair, add_special_tokens, padding, truncation,\
          \ max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
          \ return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2589                 return_length=return_length,\n   2590\
          \                 verbose=verbose,\n-&gt; 2591                 **kwargs,\n\
          \   2592             )\n   2593         else:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
          \ in batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens,\
          \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
          \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2780             return_length=return_length,\n   2781 \
          \            verbose=verbose,\n-&gt; 2782             **kwargs,\n   2783\
          \         )\n   2784 \n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in _batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens,\
          \ padding_strategy, truncation_strategy, max_length, stride, is_split_into_words,\
          \ pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask,\
          \ return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping,\
          \ return_length, verbose, **kwargs)\n    731                 ids, pair_ids\
          \ = ids_or_pair_ids\n    732 \n--&gt; 733             first_ids = get_input_ids(ids)\n\
          \    734             second_ids = get_input_ids(pair_ids) if pair_ids is\
          \ not None else None\n    735             input_ids.append((first_ids, second_ids))\n\
          \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in get_input_ids(text)\n    699             if isinstance(text, str):\n\
          \    700                 tokens = self.tokenize(text, **kwargs)\n--&gt;\
          \ 701                 return self.convert_tokens_to_ids(tokens)\n    702\
          \             elif isinstance(text, (list, tuple)) and len(text) &gt; 0\
          \ and isinstance(text[0], str):\n    703                 if is_split_into_words:\n\
          \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in convert_tokens_to_ids(self, tokens)\n    577         ids = []\n   \
          \ 578         for token in tokens:\n--&gt; 579             ids.append(self._convert_token_to_id_with_added_voc(token))\n\
          \    580         return ids\n    581 \n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in _convert_token_to_id_with_added_voc(self, token)\n    586         if\
          \ token in self.added_tokens_encoder:\n    587             return self.added_tokens_encoder[token]\n\
          --&gt; 588         return self._convert_token_to_id(token)\n    589 \n \
          \   590     def _convert_token_to_id(self, token):\n\n~/.cache/huggingface/modules/transformers_modules/paulhindemith/fasttext-classification/31d9e69fb14966aa8a72bd91a9623407f573ed28/fasttext_jp_tokenizer.py\
          \ in _convert_token_to_id(self, token)\n    101             int: ID\n  \
          \  102         \"\"\"\n--&gt; 103         return self.stoi[token]\n    104\
          \ \n    105     def _convert_id_to_token(self, index: int) -&gt; str:\n\n\
          KeyError: '\\u3000'\n</code></pre>\n"
        raw: "Got another encoding error on it \n```\n     2   classifier = pipeline(\"\
          zero-shot-classification\", \"paulhindemith/fasttext-classification\", revision=\"\
          2022.11.7\", trust_remote_code=True)\n----> 3   output = classifier(data,\
          \ candidate_labels=topics, hypothesis_template=\"{}\", multi_label=True)\n\
          \      4   return output\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in __call__(self, sequences, *args, **kwargs)\n    180             raise\
          \ ValueError(f\"Unable to understand extra arguments {args}\")\n    181\
          \ \n--> 182         return super().__call__(sequences, **kwargs)\n    183\
          \ \n    184     def preprocess(self, inputs, candidate_labels=None, hypothesis_template=\"\
          This example is {}.\"):\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\
          \ in __call__(self, inputs, num_workers, batch_size, *args, **kwargs)\n\
          \   1072             return self.iterate(inputs, preprocess_params, forward_params,\
          \ postprocess_params)\n   1073         else:\n-> 1074             return\
          \ self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n\
          \   1075 \n   1076     def run_multi(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\
          \ in run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n\
          \   1093     def run_single(self, inputs, preprocess_params, forward_params,\
          \ postprocess_params):\n   1094         all_outputs = []\n-> 1095      \
          \   for model_inputs in self.preprocess(inputs, **preprocess_params):\n\
          \   1096             model_outputs = self.forward(model_inputs, **forward_params)\n\
          \   1097             all_outputs.append(model_outputs)\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in preprocess(self, inputs, candidate_labels, hypothesis_template)\n \
          \   186 \n    187         for i, (candidate_label, sequence_pair) in enumerate(zip(candidate_labels,\
          \ sequence_pairs)):\n--> 188             model_input = self._parse_and_tokenize([sequence_pair])\n\
          \    189 \n    190             yield {\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in _parse_and_tokenize(self, sequence_pairs, padding, add_special_tokens,\
          \ truncation, **kwargs)\n    115                 )\n    116            \
          \ else:\n--> 117                 raise e\n    118 \n    119         return\
          \ inputs\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
          \ in _parse_and_tokenize(self, sequence_pairs, padding, add_special_tokens,\
          \ truncation, **kwargs)\n     97                 return_tensors=return_tensors,\n\
          \     98                 padding=padding,\n---> 99                 truncation=truncation,\n\
          \    100             )\n    101         except Exception as e:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
          \ in __call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens,\
          \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
          \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2486             if not self._in_target_context_manager:\n\
          \   2487                 self._switch_to_input_mode()\n-> 2488         \
          \    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n\
          \   2489         if text_target is not None:\n   2490             self._switch_to_target_mode()\n\
          \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
          \ in _call_one(self, text, text_pair, add_special_tokens, padding, truncation,\
          \ max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
          \ return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2589                 return_length=return_length,\n   2590\
          \                 verbose=verbose,\n-> 2591                 **kwargs,\n\
          \   2592             )\n   2593         else:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
          \ in batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens,\
          \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
          \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
          \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
          \ **kwargs)\n   2780             return_length=return_length,\n   2781 \
          \            verbose=verbose,\n-> 2782             **kwargs,\n   2783  \
          \       )\n   2784 \n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in _batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens,\
          \ padding_strategy, truncation_strategy, max_length, stride, is_split_into_words,\
          \ pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask,\
          \ return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping,\
          \ return_length, verbose, **kwargs)\n    731                 ids, pair_ids\
          \ = ids_or_pair_ids\n    732 \n--> 733             first_ids = get_input_ids(ids)\n\
          \    734             second_ids = get_input_ids(pair_ids) if pair_ids is\
          \ not None else None\n    735             input_ids.append((first_ids, second_ids))\n\
          \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in get_input_ids(text)\n    699             if isinstance(text, str):\n\
          \    700                 tokens = self.tokenize(text, **kwargs)\n--> 701\
          \                 return self.convert_tokens_to_ids(tokens)\n    702   \
          \          elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0],\
          \ str):\n    703                 if is_split_into_words:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in convert_tokens_to_ids(self, tokens)\n    577         ids = []\n   \
          \ 578         for token in tokens:\n--> 579             ids.append(self._convert_token_to_id_with_added_voc(token))\n\
          \    580         return ids\n    581 \n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
          \ in _convert_token_to_id_with_added_voc(self, token)\n    586         if\
          \ token in self.added_tokens_encoder:\n    587             return self.added_tokens_encoder[token]\n\
          --> 588         return self._convert_token_to_id(token)\n    589 \n    590\
          \     def _convert_token_to_id(self, token):\n\n~/.cache/huggingface/modules/transformers_modules/paulhindemith/fasttext-classification/31d9e69fb14966aa8a72bd91a9623407f573ed28/fasttext_jp_tokenizer.py\
          \ in _convert_token_to_id(self, token)\n    101             int: ID\n  \
          \  102         \"\"\"\n--> 103         return self.stoi[token]\n    104\
          \ \n    105     def _convert_id_to_token(self, index: int) -> str:\n\nKeyError:\
          \ '\\u3000'\n\n```"
        updatedAt: '2022-11-10T09:04:21.204Z'
      numEdits: 0
      reactions: []
    id: 636cbe95d963c56793612ee3
    type: comment
  author: devanghingu
  content: "Got another encoding error on it \n```\n     2   classifier = pipeline(\"\
    zero-shot-classification\", \"paulhindemith/fasttext-classification\", revision=\"\
    2022.11.7\", trust_remote_code=True)\n----> 3   output = classifier(data, candidate_labels=topics,\
    \ hypothesis_template=\"{}\", multi_label=True)\n      4   return output\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
    \ in __call__(self, sequences, *args, **kwargs)\n    180             raise ValueError(f\"\
    Unable to understand extra arguments {args}\")\n    181 \n--> 182         return\
    \ super().__call__(sequences, **kwargs)\n    183 \n    184     def preprocess(self,\
    \ inputs, candidate_labels=None, hypothesis_template=\"This example is {}.\"):\n\
    \n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py in __call__(self,\
    \ inputs, num_workers, batch_size, *args, **kwargs)\n   1072             return\
    \ self.iterate(inputs, preprocess_params, forward_params, postprocess_params)\n\
    \   1073         else:\n-> 1074             return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\n   1075 \n   1076     def run_multi(self,\
    \ inputs, preprocess_params, forward_params, postprocess_params):\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/base.py\
    \ in run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n\
    \   1093     def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):\n\
    \   1094         all_outputs = []\n-> 1095         for model_inputs in self.preprocess(inputs,\
    \ **preprocess_params):\n   1096             model_outputs = self.forward(model_inputs,\
    \ **forward_params)\n   1097             all_outputs.append(model_outputs)\n\n\
    /usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
    \ in preprocess(self, inputs, candidate_labels, hypothesis_template)\n    186\
    \ \n    187         for i, (candidate_label, sequence_pair) in enumerate(zip(candidate_labels,\
    \ sequence_pairs)):\n--> 188             model_input = self._parse_and_tokenize([sequence_pair])\n\
    \    189 \n    190             yield {\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
    \ in _parse_and_tokenize(self, sequence_pairs, padding, add_special_tokens, truncation,\
    \ **kwargs)\n    115                 )\n    116             else:\n--> 117   \
    \              raise e\n    118 \n    119         return inputs\n\n/usr/local/lib/python3.7/dist-packages/transformers/pipelines/zero_shot_classification.py\
    \ in _parse_and_tokenize(self, sequence_pairs, padding, add_special_tokens, truncation,\
    \ **kwargs)\n     97                 return_tensors=return_tensors,\n     98 \
    \                padding=padding,\n---> 99                 truncation=truncation,\n\
    \    100             )\n    101         except Exception as e:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
    \ in __call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens,\
    \ padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of,\
    \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
    \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
    \ **kwargs)\n   2486             if not self._in_target_context_manager:\n   2487\
    \                 self._switch_to_input_mode()\n-> 2488             encodings\
    \ = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n   2489    \
    \     if text_target is not None:\n   2490             self._switch_to_target_mode()\n\
    \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
    \ in _call_one(self, text, text_pair, add_special_tokens, padding, truncation,\
    \ max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
    \ return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask,\
    \ return_offsets_mapping, return_length, verbose, **kwargs)\n   2589         \
    \        return_length=return_length,\n   2590                 verbose=verbose,\n\
    -> 2591                 **kwargs,\n   2592             )\n   2593         else:\n\
    \n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\
    \ in batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding,\
    \ truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors,\
    \ return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask,\
    \ return_offsets_mapping, return_length, verbose, **kwargs)\n   2780         \
    \    return_length=return_length,\n   2781             verbose=verbose,\n-> 2782\
    \             **kwargs,\n   2783         )\n   2784 \n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
    \ in _batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy,\
    \ truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of,\
    \ return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens,\
    \ return_special_tokens_mask, return_offsets_mapping, return_length, verbose,\
    \ **kwargs)\n    731                 ids, pair_ids = ids_or_pair_ids\n    732\
    \ \n--> 733             first_ids = get_input_ids(ids)\n    734             second_ids\
    \ = get_input_ids(pair_ids) if pair_ids is not None else None\n    735       \
    \      input_ids.append((first_ids, second_ids))\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
    \ in get_input_ids(text)\n    699             if isinstance(text, str):\n    700\
    \                 tokens = self.tokenize(text, **kwargs)\n--> 701            \
    \     return self.convert_tokens_to_ids(tokens)\n    702             elif isinstance(text,\
    \ (list, tuple)) and len(text) > 0 and isinstance(text[0], str):\n    703    \
    \             if is_split_into_words:\n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
    \ in convert_tokens_to_ids(self, tokens)\n    577         ids = []\n    578  \
    \       for token in tokens:\n--> 579             ids.append(self._convert_token_to_id_with_added_voc(token))\n\
    \    580         return ids\n    581 \n\n/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\
    \ in _convert_token_to_id_with_added_voc(self, token)\n    586         if token\
    \ in self.added_tokens_encoder:\n    587             return self.added_tokens_encoder[token]\n\
    --> 588         return self._convert_token_to_id(token)\n    589 \n    590   \
    \  def _convert_token_to_id(self, token):\n\n~/.cache/huggingface/modules/transformers_modules/paulhindemith/fasttext-classification/31d9e69fb14966aa8a72bd91a9623407f573ed28/fasttext_jp_tokenizer.py\
    \ in _convert_token_to_id(self, token)\n    101             int: ID\n    102 \
    \        \"\"\"\n--> 103         return self.stoi[token]\n    104 \n    105  \
    \   def _convert_id_to_token(self, index: int) -> str:\n\nKeyError: '\\u3000'\n\
    \n```"
  created_at: 2022-11-10 09:04:21+00:00
  edited: false
  hidden: false
  id: 636cbe95d963c56793612ee3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
      fullname: Taizo Kaneko
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: paulhindemith
      type: user
    createdAt: '2022-11-10T09:24:51.000Z'
    data:
      edited: true
      editors:
      - paulhindemith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
          fullname: Taizo Kaneko
          isHf: false
          isPro: false
          name: paulhindemith
          type: user
        html: '<p>umm...<br>I think the reason why KeyError raised is because my model
          can not  handle ''unknown words''.</p>

          <p>I will fix it on this weekend.</p>

          <p>If you use this model soon,  it may be better that you clone this repository,
          and fix by yourself.<br>Because this model is created for my trying zero-shot-pipeline,
          so it is experimental.</p>

          '
        raw: 'umm...

          I think the reason why KeyError raised is because my model can not  handle
          ''unknown words''.


          I will fix it on this weekend.


          If you use this model soon,  it may be better that you clone this repository,
          and fix by yourself.

          Because this model is created for my trying zero-shot-pipeline, so it is
          experimental.'
        updatedAt: '2022-11-10T09:25:27.412Z'
      numEdits: 1
      reactions: []
    id: 636cc36315cd58e915c1e5c4
    type: comment
  author: paulhindemith
  content: 'umm...

    I think the reason why KeyError raised is because my model can not  handle ''unknown
    words''.


    I will fix it on this weekend.


    If you use this model soon,  it may be better that you clone this repository,
    and fix by yourself.

    Because this model is created for my trying zero-shot-pipeline, so it is experimental.'
  created_at: 2022-11-10 09:24:51+00:00
  edited: true
  hidden: false
  id: 636cc36315cd58e915c1e5c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
      fullname: Taizo Kaneko
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: paulhindemith
      type: user
    createdAt: '2022-11-13T12:23:20.000Z'
    data:
      edited: false
      editors:
      - paulhindemith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
          fullname: Taizo Kaneko
          isHf: false
          isPro: false
          name: paulhindemith
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;devanghingu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/devanghingu\"\
          >@<span class=\"underline\">devanghingu</span></a></span>\n\n\t</span></span><br>I\
          \ fixed this issue.<br>Now you can use the model even for unknown words\
          \ \U0001F604</p>\n"
        raw: "@devanghingu \nI fixed this issue. \nNow you can use the model even\
          \ for unknown words \U0001F604"
        updatedAt: '2022-11-13T12:23:20.953Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - devanghingu
    id: 6370e1b8ffc0489ed7ccd644
    type: comment
  author: paulhindemith
  content: "@devanghingu \nI fixed this issue. \nNow you can use the model even for\
    \ unknown words \U0001F604"
  created_at: 2022-11-13 12:23:20+00:00
  edited: false
  hidden: false
  id: 6370e1b8ffc0489ed7ccd644
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
      fullname: Taizo Kaneko
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: paulhindemith
      type: user
    createdAt: '2022-11-13T12:26:49.000Z'
    data:
      edited: false
      editors:
      - paulhindemith
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ff58fbc659fd86aefeac6b2afa6d9334.svg
          fullname: Taizo Kaneko
          isHf: false
          isPro: false
          name: paulhindemith
          type: user
        html: '<p>The branch "2022.11.13" is now available, including this fix.</p>

          '
        raw: The branch "2022.11.13" is now available, including this fix.
        updatedAt: '2022-11-13T12:26:49.886Z'
      numEdits: 0
      reactions: []
    id: 6370e289345f97d07b257957
    type: comment
  author: paulhindemith
  content: The branch "2022.11.13" is now available, including this fix.
  created_at: 2022-11-13 12:26:49+00:00
  edited: false
  hidden: false
  id: 6370e289345f97d07b257957
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
      fullname: devang hingu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devanghingu
      type: user
    createdAt: '2022-11-18T07:03:33.000Z'
    data:
      edited: false
      editors:
      - devanghingu
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
          fullname: devang hingu
          isHf: false
          isPro: false
          name: devanghingu
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;paulhindemith&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/paulhindemith\"\
          >@<span class=\"underline\">paulhindemith</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'Thank you @paulhindemith '
        updatedAt: '2022-11-18T07:03:33.606Z'
      numEdits: 0
      reactions: []
      relatedEventId: 63772e457169ed981702cf9d
    id: 63772e457169ed981702cf9c
    type: comment
  author: devanghingu
  content: 'Thank you @paulhindemith '
  created_at: 2022-11-18 07:03:33+00:00
  edited: false
  hidden: false
  id: 63772e457169ed981702cf9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668071778511-636b7bada3459fe6d06acb91.png?w=200&h=200&f=face
      fullname: devang hingu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: devanghingu
      type: user
    createdAt: '2022-11-18T07:03:33.000Z'
    data:
      status: closed
    id: 63772e457169ed981702cf9d
    type: status-change
  author: devanghingu
  created_at: 2022-11-18 07:03:33+00:00
  id: 63772e457169ed981702cf9d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: paulhindemith/fasttext-classification
repo_type: model
status: closed
target_branch: null
title: 'An issue with paulhindemith/fasttext-classification json not found '
