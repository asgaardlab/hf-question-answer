!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spikezz
conflicting_files: null
created_at: 2023-11-28 23:22:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1e4e75d51f947a035eb903d4eeb3a5da.svg
      fullname: spikezz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spikezz
      type: user
    createdAt: '2023-11-28T23:22:06.000Z'
    data:
      edited: true
      editors:
      - spikezz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3632732629776001
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1e4e75d51f947a035eb903d4eeb3a5da.svg
          fullname: spikezz
          isHf: false
          isPro: false
          name: spikezz
          type: user
        html: "<p>when loading 8-bit version, I get error:</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"/home/spikezz/Project/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\n                              \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/modules/models.py\"\
          , line 85, in load_model\n    output = load_func_map[loader](model_name)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/modules/models.py\"\
          , line 234, in huggingface_loader\n    model = LoaderClass.from_pretrained(path_to_model,\
          \ **params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 561, in from_pretrained\n    return model_class.from_pretrained(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3233, in from_pretrained\n    config = cls._check_and_enable_flash_attn_2(config,\
          \ torch_dtype=torch_dtype, device_map=device_map)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 1267, in _check_and_enable_flash_attn_2\n    raise ValueError(\n\
          ValueError: The current architecture does not support Flash Attention 2.0.\
          \ Please open an issue on GitHub to request support for this architecture:\
          \ https://github.com/huggingface/transformers/issues/new\n</code></pre>\n\
          <p>but base on the update info on <a href=\"https://huggingface.co/Qwen/Qwen-14B-Chat\"\
          >https://huggingface.co/Qwen/Qwen-14B-Chat</a>, It seems to support flash-attention-2,\
          \ do we need an update here as well?</p>\n"
        raw: "when loading 8-bit version, I get error:\n```\nTraceback (most recent\
          \ call last):\n  File \"/home/spikezz/Project/text-generation-webui/modules/ui_model_menu.py\"\
          , line 209, in load_model_wrapper\n    shared.model, shared.tokenizer =\
          \ load_model(shared.model_name, loader)\n                              \
          \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/modules/models.py\"\
          , line 85, in load_model\n    output = load_func_map[loader](model_name)\n\
          \             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/modules/models.py\"\
          , line 234, in huggingface_loader\n    model = LoaderClass.from_pretrained(path_to_model,\
          \ **params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 561, in from_pretrained\n    return model_class.from_pretrained(\n\
          \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 3233, in from_pretrained\n    config = cls._check_and_enable_flash_attn_2(config,\
          \ torch_dtype=torch_dtype, device_map=device_map)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
          , line 1267, in _check_and_enable_flash_attn_2\n    raise ValueError(\n\
          ValueError: The current architecture does not support Flash Attention 2.0.\
          \ Please open an issue on GitHub to request support for this architecture:\
          \ https://github.com/huggingface/transformers/issues/new\n```\nbut base\
          \ on the update info on https://huggingface.co/Qwen/Qwen-14B-Chat, It seems\
          \ to support flash-attention-2, do we need an update here as well?"
        updatedAt: '2023-11-28T23:22:47.510Z'
      numEdits: 2
      reactions: []
    id: 6566761e77fe61d0fcc643d5
    type: comment
  author: spikezz
  content: "when loading 8-bit version, I get error:\n```\nTraceback (most recent\
    \ call last):\n  File \"/home/spikezz/Project/text-generation-webui/modules/ui_model_menu.py\"\
    , line 209, in load_model_wrapper\n    shared.model, shared.tokenizer = load_model(shared.model_name,\
    \ loader)\n                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/spikezz/Project/text-generation-webui/modules/models.py\", line\
    \ 85, in load_model\n    output = load_func_map[loader](model_name)\n        \
    \     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/modules/models.py\"\
    , line 234, in huggingface_loader\n    model = LoaderClass.from_pretrained(path_to_model,\
    \ **params)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 561, in from_pretrained\n    return model_class.from_pretrained(\n    \
    \       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 3233, in from_pretrained\n    config = cls._check_and_enable_flash_attn_2(config,\
    \ torch_dtype=torch_dtype, device_map=device_map)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/home/spikezz/Project/text-generation-webui/installer_files/env/lib/python3.11/site-packages/transformers/modeling_utils.py\"\
    , line 1267, in _check_and_enable_flash_attn_2\n    raise ValueError(\nValueError:\
    \ The current architecture does not support Flash Attention 2.0. Please open an\
    \ issue on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\n\
    ```\nbut base on the update info on https://huggingface.co/Qwen/Qwen-14B-Chat,\
    \ It seems to support flash-attention-2, do we need an update here as well?"
  created_at: 2023-11-28 23:22:06+00:00
  edited: true
  hidden: false
  id: 6566761e77fe61d0fcc643d5
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: TheBloke/Qwen-14B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Need Update due to flash attention suported in original model?
