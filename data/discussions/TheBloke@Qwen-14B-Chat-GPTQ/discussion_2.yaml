!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Neman
conflicting_files: null
created_at: 2023-09-28 18:40:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-09-28T19:40:56.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9502736330032349
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>Plan to release this quant?</p>

          '
        raw: Plan to release this quant?
        updatedAt: '2023-09-28T19:40:56.967Z'
      numEdits: 0
      reactions: []
    id: 6515d6c8fbfe82f36fae330c
    type: comment
  author: Neman
  content: Plan to release this quant?
  created_at: 2023-09-28 18:40:56+00:00
  edited: false
  hidden: false
  id: 6515d6c8fbfe82f36fae330c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
      fullname: Patrick Shechet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kajuberdut
      type: user
    createdAt: '2023-09-28T20:35:08.000Z'
    data:
      edited: false
      editors:
      - kajuberdut
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6d97b20672975167a8355f54b5edf248.svg
          fullname: Patrick Shechet
          isHf: false
          isPro: false
          name: kajuberdut
          type: user
        html: '<p>My understanding is that TheBloke''s scripts automatically create
          the repo when they start running so the fact that this exists suggests it''s
          in a pipeline somewhere processing. Perhaps it failed and needs to re-run
          or perhaps it''s just still baking.</p>

          '
        raw: My understanding is that TheBloke's scripts automatically create the
          repo when they start running so the fact that this exists suggests it's
          in a pipeline somewhere processing. Perhaps it failed and needs to re-run
          or perhaps it's just still baking.
        updatedAt: '2023-09-28T20:35:08.686Z'
      numEdits: 0
      reactions: []
    id: 6515e37c91f9ac60d376571a
    type: comment
  author: kajuberdut
  content: My understanding is that TheBloke's scripts automatically create the repo
    when they start running so the fact that this exists suggests it's in a pipeline
    somewhere processing. Perhaps it failed and needs to re-run or perhaps it's just
    still baking.
  created_at: 2023-09-28 19:35:08+00:00
  edited: false
  hidden: false
  id: 6515e37c91f9ac60d376571a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-28T20:51:36.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.986652672290802
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Yeah it failed yesterday and I''ve not had a chance to try it again.
          This one is a real pain to create. My normal processes don''t work for various
          reasons.</p>

          <p>I''ll try again soon</p>

          '
        raw: 'Yeah it failed yesterday and I''ve not had a chance to try it again.
          This one is a real pain to create. My normal processes don''t work for various
          reasons.


          I''ll try again soon'
        updatedAt: '2023-09-28T20:51:36.007Z'
      numEdits: 0
      reactions:
      - count: 15
        reaction: "\u2764\uFE0F"
        users:
        - Neman
        - Eilian
        - CyberTimon
        - cjenk
        - Elizezen
        - NunYagfdfsa
        - dyoung
        - Beck777
        - jlzhou
        - johnpang
        - heroAI
        - huberto
        - dackdel
        - rexpie
        - tastypear
      - count: 7
        reaction: "\U0001F917"
        users:
        - kajuberdut
        - Eilian
        - CyberTimon
        - Beck777
        - huberto
        - Gya4321
        - tastypear
      - count: 1
        reaction: "\U0001F44D"
        users:
        - tastypear
    id: 6515e75891f9ac60d376ed4b
    type: comment
  author: TheBloke
  content: 'Yeah it failed yesterday and I''ve not had a chance to try it again. This
    one is a real pain to create. My normal processes don''t work for various reasons.


    I''ll try again soon'
  created_at: 2023-09-28 19:51:36+00:00
  edited: false
  hidden: false
  id: 6515e75891f9ac60d376ed4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65567f5e847c6761b1fe382937258466.svg
      fullname: Beck
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Beck777
      type: user
    createdAt: '2023-10-04T08:55:09.000Z'
    data:
      edited: false
      editors:
      - Beck777
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9922470450401306
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65567f5e847c6761b1fe382937258466.svg
          fullname: Beck
          isHf: false
          isPro: false
          name: Beck777
          type: user
        html: '<p>It seems that this is pretty decent model, worth finishing it.</p>

          '
        raw: It seems that this is pretty decent model, worth finishing it.
        updatedAt: '2023-10-04T08:55:09.650Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Neman
        - dyoung
        - rexpie
    id: 651d286d0e4144f535c8d5b6
    type: comment
  author: Beck777
  content: It seems that this is pretty decent model, worth finishing it.
  created_at: 2023-10-04 07:55:09+00:00
  edited: false
  hidden: false
  id: 651d286d0e4144f535c8d5b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2f814626832a4d631e0b3a55fb11b7a6.svg
      fullname: nono
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: huberto
      type: user
    createdAt: '2023-10-25T12:05:07.000Z'
    data:
      edited: false
      editors:
      - huberto
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8415130376815796
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2f814626832a4d631e0b3a55fb11b7a6.svg
          fullname: nono
          isHf: false
          isPro: false
          name: huberto
          type: user
        html: '<p>Can''t wait! </p>

          '
        raw: 'Can''t wait! '
        updatedAt: '2023-10-25T12:05:07.695Z'
      numEdits: 0
      reactions: []
    id: 653904731d28ea99dcdba830
    type: comment
  author: huberto
  content: 'Can''t wait! '
  created_at: 2023-10-25 11:05:07+00:00
  edited: false
  hidden: false
  id: 653904731d28ea99dcdba830
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4651f1cb640305b1a8557af7ee98e48e.svg
      fullname: bb
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sliptech
      type: user
    createdAt: '2023-10-26T13:04:26.000Z'
    data:
      edited: false
      editors:
      - sliptech
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9703453779220581
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4651f1cb640305b1a8557af7ee98e48e.svg
          fullname: bb
          isHf: false
          isPro: false
          name: sliptech
          type: user
        html: '<p>also giving this a bump :)</p>

          '
        raw: also giving this a bump :)
        updatedAt: '2023-10-26T13:04:26.289Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Neman
    id: 653a63dadfce99b57b3363bf
    type: comment
  author: sliptech
  content: also giving this a bump :)
  created_at: 2023-10-26 12:04:26+00:00
  edited: false
  hidden: false
  id: 653a63dadfce99b57b3363bf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d1c1d1bd0d729c6355145a55e4e0d46f.svg
      fullname: tastypear
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tastypear
      type: user
    createdAt: '2023-10-30T17:16:30.000Z'
    data:
      edited: false
      editors:
      - tastypear
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6572603583335876
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d1c1d1bd0d729c6355145a55e4e0d46f.svg
          fullname: tastypear
          isHf: false
          isPro: false
          name: tastypear
          type: user
        html: "<p>Can't wait for the SUPERHOT version\U0001F60B</p>\n"
        raw: "Can't wait for the SUPERHOT version\U0001F60B"
        updatedAt: '2023-10-30T17:16:30.782Z'
      numEdits: 0
      reactions: []
    id: 653fe4eee902e166ae4aef44
    type: comment
  author: tastypear
  content: "Can't wait for the SUPERHOT version\U0001F60B"
  created_at: 2023-10-30 16:16:30+00:00
  edited: false
  hidden: false
  id: 653fe4eee902e166ae4aef44
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-30T17:37:03.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9854421019554138
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry this has taken so long. Multiple GPTQs are being made now
          and will be uploading in the next 1 - 2 hours</p>

          '
        raw: Sorry this has taken so long. Multiple GPTQs are being made now and will
          be uploading in the next 1 - 2 hours
        updatedAt: '2023-10-30T17:37:03.068Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Neman
        - kajuberdut
    id: 653fe9bf625ef3d61e07756a
    type: comment
  author: TheBloke
  content: Sorry this has taken so long. Multiple GPTQs are being made now and will
    be uploading in the next 1 - 2 hours
  created_at: 2023-10-30 16:37:03+00:00
  edited: false
  hidden: false
  id: 653fe9bf625ef3d61e07756a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-30T20:43:35.000Z'
    data:
      edited: true
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49202340841293335
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>Can''t load. I enabled trust_remote_code, got the error ImportError:
          This modeling file requires the following packages that were not found in
          your environment: transformers_stream_generator. Run pip install transformers_stream_generator<br>pip
          installed it, but still get errors I don''t know how to tackle:<br>Traceback
          (most recent call last):</p>

          <p>File "/home/neman/text-generation-webui/modules/ui_model_menu.py", line
          206, in load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(shared.model_name, loader)</p>

          <p>File "/home/neman/text-generation-webui/modules/models.py", line 92,
          in load_model</p>

          <p>tokenizer = load_tokenizer(model_name, model)</p>

          <p>File "/home/neman/text-generation-webui/modules/models.py", line 111,
          in load_tokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained(</p>

          <p>File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py",
          line 738, in from_pretrained</p>

          <p>return tokenizer_class.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)</p>

          <p>File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py",
          line 2017, in from_pretrained</p>

          <p>return cls._from_pretrained(</p>

          <p>File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py",
          line 2249, in _from_pretrained</p>

          <p>tokenizer = cls(*init_inputs, **init_kwargs)</p>

          <p>File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/tokenization_qwen.py",
          line 75, in init</p>

          <p>self.mergeable_ranks = _load_tiktoken_bpe(vocab_file)  # type: Dict[bytes,
          int]</p>

          <p>File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/tokenization_qwen.py",
          line 49, in _load_tiktoken_bpe</p>

          <p>with open(tiktoken_bpe_file, "rb") as f:</p>

          <p>TypeError: expected str, bytes or os.PathLike object, not NoneType</p>

          '
        raw: 'Can''t load. I enabled trust_remote_code, got the error ImportError:
          This modeling file requires the following packages that were not found in
          your environment: transformers_stream_generator. Run pip install transformers_stream_generator

          pip installed it, but still get errors I don''t know how to tackle:

          Traceback (most recent call last):


          File "/home/neman/text-generation-webui/modules/ui_model_menu.py", line
          206, in load_model_wrapper


          shared.model, shared.tokenizer = load_model(shared.model_name, loader)


          File "/home/neman/text-generation-webui/modules/models.py", line 92, in
          load_model


          tokenizer = load_tokenizer(model_name, model)


          File "/home/neman/text-generation-webui/modules/models.py", line 111, in
          load_tokenizer


          tokenizer = AutoTokenizer.from_pretrained(


          File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py",
          line 738, in from_pretrained


          return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,
          **kwargs)


          File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py",
          line 2017, in from_pretrained


          return cls._from_pretrained(


          File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py",
          line 2249, in _from_pretrained


          tokenizer = cls(*init_inputs, **init_kwargs)


          File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/tokenization_qwen.py",
          line 75, in init


          self.mergeable_ranks = _load_tiktoken_bpe(vocab_file)  # type: Dict[bytes,
          int]


          File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/tokenization_qwen.py",
          line 49, in _load_tiktoken_bpe


          with open(tiktoken_bpe_file, "rb") as f:


          TypeError: expected str, bytes or os.PathLike object, not NoneType

          '
        updatedAt: '2023-10-30T20:45:42.467Z'
      numEdits: 1
      reactions: []
    id: 65401577df8f1e9385cc64b5
    type: comment
  author: Neman
  content: 'Can''t load. I enabled trust_remote_code, got the error ImportError: This
    modeling file requires the following packages that were not found in your environment:
    transformers_stream_generator. Run pip install transformers_stream_generator

    pip installed it, but still get errors I don''t know how to tackle:

    Traceback (most recent call last):


    File "/home/neman/text-generation-webui/modules/ui_model_menu.py", line 206, in
    load_model_wrapper


    shared.model, shared.tokenizer = load_model(shared.model_name, loader)


    File "/home/neman/text-generation-webui/modules/models.py", line 92, in load_model


    tokenizer = load_tokenizer(model_name, model)


    File "/home/neman/text-generation-webui/modules/models.py", line 111, in load_tokenizer


    tokenizer = AutoTokenizer.from_pretrained(


    File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py",
    line 738, in from_pretrained


    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs,
    **kwargs)


    File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py",
    line 2017, in from_pretrained


    return cls._from_pretrained(


    File "/home/neman/text-generation-webui/installer_files/env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py",
    line 2249, in _from_pretrained


    tokenizer = cls(*init_inputs, **init_kwargs)


    File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/tokenization_qwen.py",
    line 75, in init


    self.mergeable_ranks = _load_tiktoken_bpe(vocab_file)  # type: Dict[bytes, int]


    File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/tokenization_qwen.py",
    line 49, in _load_tiktoken_bpe


    with open(tiktoken_bpe_file, "rb") as f:


    TypeError: expected str, bytes or os.PathLike object, not NoneType

    '
  created_at: 2023-10-30 19:43:35+00:00
  edited: true
  hidden: false
  id: 65401577df8f1e9385cc64b5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-30T20:49:00.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8929491639137268
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Sorry my bad, missed a file.  Please re-download the branch to get
          missing file <code>qwen.tiktoken</code> and try again</p>

          '
        raw: Sorry my bad, missed a file.  Please re-download the branch to get missing
          file `qwen.tiktoken` and try again
        updatedAt: '2023-10-30T20:49:00.236Z'
      numEdits: 0
      reactions: []
    id: 654016bc422c55a1a75031b3
    type: comment
  author: TheBloke
  content: Sorry my bad, missed a file.  Please re-download the branch to get missing
    file `qwen.tiktoken` and try again
  created_at: 2023-10-30 19:49:00+00:00
  edited: false
  hidden: false
  id: 654016bc422c55a1a75031b3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-30T20:51:51.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9801177978515625
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Actually there might be another problem as well, hang on</p>

          '
        raw: Actually there might be another problem as well, hang on
        updatedAt: '2023-10-30T20:51:51.116Z'
      numEdits: 0
      reactions: []
    id: 654017672b45fe65013aa158
    type: comment
  author: TheBloke
  content: Actually there might be another problem as well, hang on
  created_at: 2023-10-30 19:51:51+00:00
  edited: false
  hidden: false
  id: 654017672b45fe65013aa158
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-30T21:04:20.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9414138197898865
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>OK please re-download the branch to get added <code>qwen.tiktoken</code>\
          \ and fixed <code>config.json</code>.  It's now working for me locally using\
          \ Transformers:</p>\n<pre><code>*** Generate:\nTell me about AI\nAI is a\
          \ field of computer science that aims to create intelligent machines that\
          \ work and react like humans. It involves developing algorithms and techniques\
          \ that can be used to analyze data, make decisions, and perform tasks without\
          \ explicit instructions.\nThere are several different approaches to AI,\
          \ including:\n\n  * Rule-based systems: These systems use pre-defined rules\
          \ to make decisions based on input data.\n  * Machine learning: This approach\
          \ uses statistical models to learn from data and improve over time.\n  *\
          \ Deep learning: A subfield of machine learning that uses neural networks\
          \ with many layers to learn from complex data.\n\nAI has numerous applications\
          \ in various fields such as healthcare, finance, transportation, entertainment,\
          \ and more. Some examples include image recognition, speech recognition,\
          \ natural language processing, autonomous vehicles, and medical diagnosis.\n\
          However, it's important to note that AI also raises ethical concerns regarding\
          \ privacy, bias, accountability, and transparency. As AI becomes more prevalent,\
          \ it is crucial for researchers, policymakers, and the general public to\
          \ consider these issues and develop appropriate guidelines and regulations\
          \ to ensure that AI is developed and deployed responsibly.&lt;|endoftext|&gt;\n\
          </code></pre>\n"
        raw: "OK please re-download the branch to get added `qwen.tiktoken` and fixed\
          \ `config.json`.  It's now working for me locally using Transformers:\n\
          ```\n*** Generate:\nTell me about AI\nAI is a field of computer science\
          \ that aims to create intelligent machines that work and react like humans.\
          \ It involves developing algorithms and techniques that can be used to analyze\
          \ data, make decisions, and perform tasks without explicit instructions.\n\
          There are several different approaches to AI, including:\n\n  * Rule-based\
          \ systems: These systems use pre-defined rules to make decisions based on\
          \ input data.\n  * Machine learning: This approach uses statistical models\
          \ to learn from data and improve over time.\n  * Deep learning: A subfield\
          \ of machine learning that uses neural networks with many layers to learn\
          \ from complex data.\n\nAI has numerous applications in various fields such\
          \ as healthcare, finance, transportation, entertainment, and more. Some\
          \ examples include image recognition, speech recognition, natural language\
          \ processing, autonomous vehicles, and medical diagnosis.\nHowever, it's\
          \ important to note that AI also raises ethical concerns regarding privacy,\
          \ bias, accountability, and transparency. As AI becomes more prevalent,\
          \ it is crucial for researchers, policymakers, and the general public to\
          \ consider these issues and develop appropriate guidelines and regulations\
          \ to ensure that AI is developed and deployed responsibly.<|endoftext|>\n\
          ```"
        updatedAt: '2023-10-30T21:04:20.371Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F917"
        users:
        - Yhyu13
    id: 65401a54c19215b3f79ab731
    type: comment
  author: TheBloke
  content: "OK please re-download the branch to get added `qwen.tiktoken` and fixed\
    \ `config.json`.  It's now working for me locally using Transformers:\n```\n***\
    \ Generate:\nTell me about AI\nAI is a field of computer science that aims to\
    \ create intelligent machines that work and react like humans. It involves developing\
    \ algorithms and techniques that can be used to analyze data, make decisions,\
    \ and perform tasks without explicit instructions.\nThere are several different\
    \ approaches to AI, including:\n\n  * Rule-based systems: These systems use pre-defined\
    \ rules to make decisions based on input data.\n  * Machine learning: This approach\
    \ uses statistical models to learn from data and improve over time.\n  * Deep\
    \ learning: A subfield of machine learning that uses neural networks with many\
    \ layers to learn from complex data.\n\nAI has numerous applications in various\
    \ fields such as healthcare, finance, transportation, entertainment, and more.\
    \ Some examples include image recognition, speech recognition, natural language\
    \ processing, autonomous vehicles, and medical diagnosis.\nHowever, it's important\
    \ to note that AI also raises ethical concerns regarding privacy, bias, accountability,\
    \ and transparency. As AI becomes more prevalent, it is crucial for researchers,\
    \ policymakers, and the general public to consider these issues and develop appropriate\
    \ guidelines and regulations to ensure that AI is developed and deployed responsibly.<|endoftext|>\n\
    ```"
  created_at: 2023-10-30 20:04:20+00:00
  edited: false
  hidden: false
  id: 65401a54c19215b3f79ab731
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-31T08:38:41.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8751981258392334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: "<p>I downloaded qwen.tiktoken and replaced config.json. Can't get it\
          \ to load using Transformers.<br>ValueError: Trying to set a tensor of shape\
          \ torch.Size([1280, 15360]) in \"qweight\" (which has shape torch.Size([640,\
          \ 15360])), this look incorrect.</p>\n<p>It did load with AutoGPTQ, but\
          \ output is gibberish. For reference, I use TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True\
          \ version.<br>*** Generate:<br>Tell me about AI<br> then The\u6700\u65B0\
          \ l\u6700\u65B0 then i</p>\n<p> o the and reform</p>\n<p> is An \u6700 &amp;\
          \ mi f\u3001\u201C\u6700\u65B0 then<br> k A (</p>\n<p> a then c w r\u6700\
          \u65B0 right has his<br> simply &amp;<br> l \uFFFD season In then b<br>\
          \ then</p>\n<p>In any case, thank you for fast response, support and overall\
          \ contribution to the community.</p>\n"
        raw: "I downloaded qwen.tiktoken and replaced config.json. Can't get it to\
          \ load using Transformers. \nValueError: Trying to set a tensor of shape\
          \ torch.Size([1280, 15360]) in \"qweight\" (which has shape torch.Size([640,\
          \ 15360])), this look incorrect.\n\nIt did load with AutoGPTQ, but output\
          \ is gibberish. For reference, I use TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True\
          \ version.\n*** Generate:\nTell me about AI\n then The\u6700\u65B0 l\u6700\
          \u65B0 then i\n\n \n o the and reform\n\n is An \u6700 & mi f\u3001\u201C\
          \u6700\u65B0 then\n k A (\n\n a then c w r\u6700\u65B0 right has his\n simply\
          \ &\n l \uFFFD season In then b\n then\n\nIn any case, thank you for fast\
          \ response, support and overall contribution to the community."
        updatedAt: '2023-10-31T08:38:41.577Z'
      numEdits: 0
      reactions: []
    id: 6540bd11ff86086e1159c1ef
    type: comment
  author: Neman
  content: "I downloaded qwen.tiktoken and replaced config.json. Can't get it to load\
    \ using Transformers. \nValueError: Trying to set a tensor of shape torch.Size([1280,\
    \ 15360]) in \"qweight\" (which has shape torch.Size([640, 15360])), this look\
    \ incorrect.\n\nIt did load with AutoGPTQ, but output is gibberish. For reference,\
    \ I use TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True version.\n***\
    \ Generate:\nTell me about AI\n then The\u6700\u65B0 l\u6700\u65B0 then i\n\n\
    \ \n o the and reform\n\n is An \u6700 & mi f\u3001\u201C\u6700\u65B0 then\n k\
    \ A (\n\n a then c w r\u6700\u65B0 right has his\n simply &\n l \uFFFD season\
    \ In then b\n then\n\nIn any case, thank you for fast response, support and overall\
    \ contribution to the community."
  created_at: 2023-10-31 07:38:41+00:00
  edited: false
  hidden: false
  id: 6540bd11ff86086e1159c1ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-31T09:21:54.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9924538135528564
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Which branch are you testing?</p>

          '
        raw: Which branch are you testing?
        updatedAt: '2023-10-31T09:21:54.223Z'
      numEdits: 0
      reactions: []
    id: 6540c732992b1c560edc884f
    type: comment
  author: TheBloke
  content: Which branch are you testing?
  created_at: 2023-10-31 08:21:54+00:00
  edited: false
  hidden: false
  id: 6540c732992b1c560edc884f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-31T09:25:16.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5292017459869385
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True version.</p>

          '
        raw: TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True version.
        updatedAt: '2023-10-31T09:25:16.552Z'
      numEdits: 0
      reactions: []
    id: 6540c7fc0a4e554c1bf62892
    type: comment
  author: Neman
  content: TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True version.
  created_at: 2023-10-31 08:25:16+00:00
  edited: false
  hidden: false
  id: 6540c7fc0a4e554c1bf62892
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-31T09:28:50.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7534386515617371
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Works fine for me:</p>\n<pre><code> [pytorch2] tomj@MC:/workspace\
          \ \u1405 CUDA_VISIBLE_DEVICES=7 python3 test_transgptq.py /workspace/process/qwen_qwen-14b-chat/gptq/gptq-8bit-32g-actorder_True\n\
          Warning: please make sure that you are using the latest codes and checkpoints,\
          \ especially if you used Qwen-7B before 09.25.2023.\u8BF7\u4F7F\u7528\u6700\
          \u65B0\u6A21\u578B\u548C\u4EE3\u7801\uFF0C\u5C24\u5176\u5982\u679C\u4F60\
          \u57289\u670825\u65E5\u524D\u5DF2\u7ECF\u5F00\u59CB\u4F7F\u7528Qwen-7B\uFF0C\
          \u5343\u4E07\u6CE8\u610F\u4E0D\u8981\u4F7F\u7528\u9519\u8BEF\u4EE3\u7801\
          \u548C\u6A21\u578B\u3002\nTry importing flash-attention for faster inference...\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:11&lt;00:00,\
          \  5.82s/it]\n\n\n*** Generate:\nTell me about AI\nAI, or artificial intelligence,\
          \ refers to the development of computer systems that can perform tasks that\
          \ typically require human intelligence, such as visual perception, speech\
          \ recognition, decision-making, and language translation. AI systems are\
          \ designed to learn from experience, adapt to new inputs, and make decisions\
          \ based on statistical analysis.\n\nThere are several different approaches\
          \ to building AI systems, including rule-based systems, machine learning,\
          \ and deep learning. Rule-based systems use pre-defined rules to make decisions,\
          \ while machine learning involves training a system to recognize patterns\
          \ in data and make predictions based on those patterns. Deep learning is\
          \ a type of machine learning that uses neural networks with many layers\
          \ to analyze complex data sets.\n\nAI has the potential to revolutionize\
          \ many industries, from healthcare and finance to transportation and entertainment.\
          \ It has already been used to improve medical diagnosis, develop self-driving\
          \ cars, and create personalized recommendations for consumers. However,\
          \ there are also concerns about the impact of AI on jobs and society, as\
          \ well as ethical considerations around issues like bias and privacy.&lt;|endoftext|&gt;\n\
          *** Pipeline:\nThe model 'QWenLMHeadModel' is not supported for text-generation.\
          \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel',\
          \ 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM',\
          \ 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTell me about AI\nAI stands for\
          \ Artificial Intelligence, which refers to the ability of a computer or\
          \ machine to perform tasks that would typically require human intelligence,\
          \ such as learning, reasoning, problem-solving, and decision-making.\n\n\
          AI can be categorized into two main types: narrow or weak AI and general\
          \ or strong AI. Narrow AI is designed to perform a specific task or set\
          \ of tasks within a limited domain, such as facial recognition or language\
          \ translation. General AI, on the other hand, has the capability to learn\
          \ and perform any intellectual task that a human can do.\n\nThere are various\
          \ approaches to developing AI systems, including rule-based systems, decision\
          \ trees, neural networks, and deep learning. These techniques use algorithms\
          \ and mathematical models to simulate human thought processes and enable\
          \ machines to learn from data and make decisions based on patterns and trends.\n\
          \nAI has numerous applications across various industries, including healthcare,\
          \ finance, transportation, and manufacturing. Some examples of AI technologies\
          \ include chatbots, autonomous vehicles, predictive analytics, and image\
          \ recognition.\n\nHowever, there are also concerns about the impact of AI\
          \ on society, including job displacement, bias in decision-making, and ethical\
          \ considerations around privacy and security. As AI continues to evolve\
          \ and become more prevalent, it will be important to address these issues\
          \ and ensure that AI is developed and used responsibly and ethically.\n\
          </code></pre>\n<p>Code:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-keyword\">import</span> argparse\nparser = argparse.ArgumentParser(description=<span\
          \ class=\"hljs-string\">'Process and upload quantisations'</span>)\nparser.add_argument(<span\
          \ class=\"hljs-string\">'model_dir'</span>, <span class=\"hljs-built_in\"\
          >type</span>=<span class=\"hljs-built_in\">str</span>, <span class=\"hljs-built_in\"\
          >help</span>=<span class=\"hljs-string\">'model dir'</span>)\nargs = parser.parse_args()\n\
          \n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path\
          \ = args.model_dir\n<span class=\"hljs-comment\"># To use a different branch,\
          \ change revision</span>\n<span class=\"hljs-comment\"># For example: revision=\"\
          gptq-4bit-32g-actorder_True\"</span>\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=<span class=\"\
          hljs-string\">\"auto\"</span>,\n                                       \
          \      trust_remote_code=<span class=\"hljs-literal\">True</span>,\n   \
          \                                          revision=<span class=\"hljs-string\"\
          >\"main\"</span>)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
          \ use_fast=<span class=\"hljs-literal\">True</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n\nprompt = <span class=\"hljs-string\"\
          >\"Tell me about AI\"</span>\nprompt_template=<span class=\"hljs-string\"\
          >f'''<span class=\"hljs-subst\">{prompt}</span></span>\n<span class=\"hljs-string\"\
          >'''</span>\n\n<span class=\"hljs-built_in\">print</span>(<span class=\"\
          hljs-string\">\"\\n\\n*** Generate:\"</span>)\n\ninput_ids = tokenizer(prompt_template,\
          \ return_tensors=<span class=\"hljs-string\">'pt'</span>).input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=<span class=\"hljs-number\"\
          >0.7</span>, do_sample=<span class=\"hljs-literal\">True</span>, top_p=<span\
          \ class=\"hljs-number\">0.95</span>, top_k=<span class=\"hljs-number\">40</span>,\
          \ max_new_tokens=<span class=\"hljs-number\">512</span>)\n<span class=\"\
          hljs-built_in\">print</span>(tokenizer.decode(output[<span class=\"hljs-number\"\
          >0</span>]))\n\n<span class=\"hljs-comment\"># Inference can also be done\
          \ using transformers' pipeline</span>\n\n<span class=\"hljs-built_in\">print</span>(<span\
          \ class=\"hljs-string\">\"*** Pipeline:\"</span>)\npipe = pipeline(\n  \
          \  <span class=\"hljs-string\">\"text-generation\"</span>,\n    model=model,\n\
          \    tokenizer=tokenizer,\n    max_new_tokens=<span class=\"hljs-number\"\
          >512</span>,\n    do_sample=<span class=\"hljs-literal\">True</span>,\n\
          \    temperature=<span class=\"hljs-number\">0.7</span>,\n    top_p=<span\
          \ class=\"hljs-number\">0.95</span>,\n    top_k=<span class=\"hljs-number\"\
          >40</span>,\n    repetition_penalty=<span class=\"hljs-number\">1.1</span>\n\
          )\n\n<span class=\"hljs-built_in\">print</span>(pipe(prompt_template)[<span\
          \ class=\"hljs-number\">0</span>][<span class=\"hljs-string\">'generated_text'</span>])\n\
          </code></pre>\n<p>Please show the code you're trying.</p>\n"
        raw: "Works fine for me:\n```\n [pytorch2] tomj@MC:/workspace \u1405 CUDA_VISIBLE_DEVICES=7\
          \ python3 test_transgptq.py /workspace/process/qwen_qwen-14b-chat/gptq/gptq-8bit-32g-actorder_True\n\
          Warning: please make sure that you are using the latest codes and checkpoints,\
          \ especially if you used Qwen-7B before 09.25.2023.\u8BF7\u4F7F\u7528\u6700\
          \u65B0\u6A21\u578B\u548C\u4EE3\u7801\uFF0C\u5C24\u5176\u5982\u679C\u4F60\
          \u57289\u670825\u65E5\u524D\u5DF2\u7ECF\u5F00\u59CB\u4F7F\u7528Qwen-7B\uFF0C\
          \u5343\u4E07\u6CE8\u610F\u4E0D\u8981\u4F7F\u7528\u9519\u8BEF\u4EE3\u7801\
          \u548C\u6A21\u578B\u3002\nTry importing flash-attention for faster inference...\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:11<00:00,\
          \  5.82s/it]\n\n\n*** Generate:\nTell me about AI\nAI, or artificial intelligence,\
          \ refers to the development of computer systems that can perform tasks that\
          \ typically require human intelligence, such as visual perception, speech\
          \ recognition, decision-making, and language translation. AI systems are\
          \ designed to learn from experience, adapt to new inputs, and make decisions\
          \ based on statistical analysis.\n\nThere are several different approaches\
          \ to building AI systems, including rule-based systems, machine learning,\
          \ and deep learning. Rule-based systems use pre-defined rules to make decisions,\
          \ while machine learning involves training a system to recognize patterns\
          \ in data and make predictions based on those patterns. Deep learning is\
          \ a type of machine learning that uses neural networks with many layers\
          \ to analyze complex data sets.\n\nAI has the potential to revolutionize\
          \ many industries, from healthcare and finance to transportation and entertainment.\
          \ It has already been used to improve medical diagnosis, develop self-driving\
          \ cars, and create personalized recommendations for consumers. However,\
          \ there are also concerns about the impact of AI on jobs and society, as\
          \ well as ethical considerations around issues like bias and privacy.<|endoftext|>\n\
          *** Pipeline:\nThe model 'QWenLMHeadModel' is not supported for text-generation.\
          \ Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder',\
          \ 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM',\
          \ 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM',\
          \ 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM',\
          \ 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM',\
          \ 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel',\
          \ 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM',\
          \ 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM',\
          \ 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM',\
          \ 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel',\
          \ 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM',\
          \ 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
          \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
          \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
          \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
          \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
          \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTell me about AI\nAI stands for\
          \ Artificial Intelligence, which refers to the ability of a computer or\
          \ machine to perform tasks that would typically require human intelligence,\
          \ such as learning, reasoning, problem-solving, and decision-making.\n\n\
          AI can be categorized into two main types: narrow or weak AI and general\
          \ or strong AI. Narrow AI is designed to perform a specific task or set\
          \ of tasks within a limited domain, such as facial recognition or language\
          \ translation. General AI, on the other hand, has the capability to learn\
          \ and perform any intellectual task that a human can do.\n\nThere are various\
          \ approaches to developing AI systems, including rule-based systems, decision\
          \ trees, neural networks, and deep learning. These techniques use algorithms\
          \ and mathematical models to simulate human thought processes and enable\
          \ machines to learn from data and make decisions based on patterns and trends.\n\
          \nAI has numerous applications across various industries, including healthcare,\
          \ finance, transportation, and manufacturing. Some examples of AI technologies\
          \ include chatbots, autonomous vehicles, predictive analytics, and image\
          \ recognition.\n\nHowever, there are also concerns about the impact of AI\
          \ on society, including job displacement, bias in decision-making, and ethical\
          \ considerations around privacy and security. As AI continues to evolve\
          \ and become more prevalent, it will be important to address these issues\
          \ and ensure that AI is developed and used responsibly and ethically.\n\
          ```\n\nCode:\n```python\nimport argparse\nparser = argparse.ArgumentParser(description='Process\
          \ and upload quantisations')\nparser.add_argument('model_dir', type=str,\
          \ help='model dir')\nargs = parser.parse_args()\n\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_name_or_path =\
          \ args.model_dir\n# To use a different branch, change revision\n# For example:\
          \ revision=\"gptq-4bit-32g-actorder_True\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          \                                             device_map=\"auto\",\n   \
          \                                          trust_remote_code=True,\n   \
          \                                          revision=\"main\")\n\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True, trust_remote_code=True)\n\
          \nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n'''\n\nprint(\"\
          \\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n\
          output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True,\
          \ top_p=0.95, top_k=40, max_new_tokens=512)\nprint(tokenizer.decode(output[0]))\n\
          \n# Inference can also be done using transformers' pipeline\n\nprint(\"\
          *** Pipeline:\")\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n\
          \    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
          \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
          )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n\nPlease show\
          \ the code you're trying."
        updatedAt: '2023-10-31T09:28:50.960Z'
      numEdits: 0
      reactions: []
    id: 6540c8d2231ce22e2a9ce48b
    type: comment
  author: TheBloke
  content: "Works fine for me:\n```\n [pytorch2] tomj@MC:/workspace \u1405 CUDA_VISIBLE_DEVICES=7\
    \ python3 test_transgptq.py /workspace/process/qwen_qwen-14b-chat/gptq/gptq-8bit-32g-actorder_True\n\
    Warning: please make sure that you are using the latest codes and checkpoints,\
    \ especially if you used Qwen-7B before 09.25.2023.\u8BF7\u4F7F\u7528\u6700\u65B0\
    \u6A21\u578B\u548C\u4EE3\u7801\uFF0C\u5C24\u5176\u5982\u679C\u4F60\u57289\u6708\
    25\u65E5\u524D\u5DF2\u7ECF\u5F00\u59CB\u4F7F\u7528Qwen-7B\uFF0C\u5343\u4E07\u6CE8\
    \u610F\u4E0D\u8981\u4F7F\u7528\u9519\u8BEF\u4EE3\u7801\u548C\u6A21\u578B\u3002\
    \nTry importing flash-attention for faster inference...\nLoading checkpoint shards:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:11<00:00,\
    \  5.82s/it]\n\n\n*** Generate:\nTell me about AI\nAI, or artificial intelligence,\
    \ refers to the development of computer systems that can perform tasks that typically\
    \ require human intelligence, such as visual perception, speech recognition, decision-making,\
    \ and language translation. AI systems are designed to learn from experience,\
    \ adapt to new inputs, and make decisions based on statistical analysis.\n\nThere\
    \ are several different approaches to building AI systems, including rule-based\
    \ systems, machine learning, and deep learning. Rule-based systems use pre-defined\
    \ rules to make decisions, while machine learning involves training a system to\
    \ recognize patterns in data and make predictions based on those patterns. Deep\
    \ learning is a type of machine learning that uses neural networks with many layers\
    \ to analyze complex data sets.\n\nAI has the potential to revolutionize many\
    \ industries, from healthcare and finance to transportation and entertainment.\
    \ It has already been used to improve medical diagnosis, develop self-driving\
    \ cars, and create personalized recommendations for consumers. However, there\
    \ are also concerns about the impact of AI on jobs and society, as well as ethical\
    \ considerations around issues like bias and privacy.<|endoftext|>\n*** Pipeline:\n\
    The model 'QWenLMHeadModel' is not supported for text-generation. Supported models\
    \ are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM',\
    \ 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM',\
    \ 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM',\
    \ 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM',\
    \ 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel',\
    \ 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM',\
    \ 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM',\
    \ 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM',\
    \ 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM',\
    \ 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM',\
    \ 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead',\
    \ 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM',\
    \ 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM',\
    \ 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel',\
    \ 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM',\
    \ 'XLNetLMHeadModel', 'XmodForCausalLM'].\nTell me about AI\nAI stands for Artificial\
    \ Intelligence, which refers to the ability of a computer or machine to perform\
    \ tasks that would typically require human intelligence, such as learning, reasoning,\
    \ problem-solving, and decision-making.\n\nAI can be categorized into two main\
    \ types: narrow or weak AI and general or strong AI. Narrow AI is designed to\
    \ perform a specific task or set of tasks within a limited domain, such as facial\
    \ recognition or language translation. General AI, on the other hand, has the\
    \ capability to learn and perform any intellectual task that a human can do.\n\
    \nThere are various approaches to developing AI systems, including rule-based\
    \ systems, decision trees, neural networks, and deep learning. These techniques\
    \ use algorithms and mathematical models to simulate human thought processes and\
    \ enable machines to learn from data and make decisions based on patterns and\
    \ trends.\n\nAI has numerous applications across various industries, including\
    \ healthcare, finance, transportation, and manufacturing. Some examples of AI\
    \ technologies include chatbots, autonomous vehicles, predictive analytics, and\
    \ image recognition.\n\nHowever, there are also concerns about the impact of AI\
    \ on society, including job displacement, bias in decision-making, and ethical\
    \ considerations around privacy and security. As AI continues to evolve and become\
    \ more prevalent, it will be important to address these issues and ensure that\
    \ AI is developed and used responsibly and ethically.\n```\n\nCode:\n```python\n\
    import argparse\nparser = argparse.ArgumentParser(description='Process and upload\
    \ quantisations')\nparser.add_argument('model_dir', type=str, help='model dir')\n\
    args = parser.parse_args()\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
    \ pipeline\n\nmodel_name_or_path = args.model_dir\n# To use a different branch,\
    \ change revision\n# For example: revision=\"gptq-4bit-32g-actorder_True\"\nmodel\
    \ = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n               \
    \                              device_map=\"auto\",\n                        \
    \                     trust_remote_code=True,\n                              \
    \               revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\
    \ use_fast=True, trust_remote_code=True)\n\nprompt = \"Tell me about AI\"\nprompt_template=f'''{prompt}\n\
    '''\n\nprint(\"\\n\\n*** Generate:\")\n\ninput_ids = tokenizer(prompt_template,\
    \ return_tensors='pt').input_ids.cuda()\noutput = model.generate(inputs=input_ids,\
    \ temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n\
    print(tokenizer.decode(output[0]))\n\n# Inference can also be done using transformers'\
    \ pipeline\n\nprint(\"*** Pipeline:\")\npipe = pipeline(\n    \"text-generation\"\
    ,\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=512,\n    do_sample=True,\n\
    \    temperature=0.7,\n    top_p=0.95,\n    top_k=40,\n    repetition_penalty=1.1\n\
    )\n\nprint(pipe(prompt_template)[0]['generated_text'])\n```\n\nPlease show the\
    \ code you're trying."
  created_at: 2023-10-31 08:28:50+00:00
  edited: false
  hidden: false
  id: 6540c8d2231ce22e2a9ce48b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-31T09:52:55.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8746463656425476
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>Ah, OK, you are using transformers pipeline directly in code and
          I tried in text-generation-webui. Must be something with textgen.<br>I will
          try in python and report here.</p>

          '
        raw: "Ah, OK, you are using transformers pipeline directly in code and I tried\
          \ in text-generation-webui. Must be something with textgen. \nI will try\
          \ in python and report here."
        updatedAt: '2023-10-31T09:52:55.229Z'
      numEdits: 0
      reactions: []
    id: 6540ce77422c55a1a76d1eef
    type: comment
  author: Neman
  content: "Ah, OK, you are using transformers pipeline directly in code and I tried\
    \ in text-generation-webui. Must be something with textgen. \nI will try in python\
    \ and report here."
  created_at: 2023-10-31 08:52:55+00:00
  edited: false
  hidden: false
  id: 6540ce77422c55a1a76d1eef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-31T10:03:26.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8045516610145569
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>I tried and get an error at model loading:<br>Exception has occurred:
          ValueError<br>Trying to set a tensor of shape torch.Size([1280, 15360])
          in "qweight" (which has shape torch.Size([640, 15360])), this look incorrect.<br>  File
          "/home/neman/PROGRAMMING/PYTHON/CroTranscribe/qwen_chat_14b_test.py", line
          6, in <br>    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>ValueError:
          Trying to set a tensor of shape torch.Size([1280, 15360]) in "qweight" (which
          has shape torch.Size([640, 15360])), this look incorrect.</p>

          <p>Here is the code:<br>from transformers import AutoModelForCausalLM, AutoTokenizer,
          pipeline</p>

          <p>model_name_or_path = ''/mnt/disk2/LLM_MODELS/models/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True''<br>model
          = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br>                                             device_map="auto",<br>                                             trust_remote_code=True)</p>

          '
        raw: "I tried and get an error at model loading:\nException has occurred:\
          \ ValueError\nTrying to set a tensor of shape torch.Size([1280, 15360])\
          \ in \"qweight\" (which has shape torch.Size([640, 15360])), this look incorrect.\n\
          \  File \"/home/neman/PROGRAMMING/PYTHON/CroTranscribe/qwen_chat_14b_test.py\"\
          , line 6, in <module>\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
          ValueError: Trying to set a tensor of shape torch.Size([1280, 15360]) in\
          \ \"qweight\" (which has shape torch.Size([640, 15360])), this look incorrect.\n\
          \nHere is the code:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer,\
          \ pipeline\n\nmodel_name_or_path = '/mnt/disk2/LLM_MODELS/models/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True'\n\
          model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n     \
          \                                        device_map=\"auto\",\n        \
          \                                     trust_remote_code=True)\n"
        updatedAt: '2023-10-31T10:03:26.638Z'
      numEdits: 0
      reactions: []
    id: 6540d0ee3a60baceec710511
    type: comment
  author: Neman
  content: "I tried and get an error at model loading:\nException has occurred: ValueError\n\
    Trying to set a tensor of shape torch.Size([1280, 15360]) in \"qweight\" (which\
    \ has shape torch.Size([640, 15360])), this look incorrect.\n  File \"/home/neman/PROGRAMMING/PYTHON/CroTranscribe/qwen_chat_14b_test.py\"\
    , line 6, in <module>\n    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n\
    ValueError: Trying to set a tensor of shape torch.Size([1280, 15360]) in \"qweight\"\
    \ (which has shape torch.Size([640, 15360])), this look incorrect.\n\nHere is\
    \ the code:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\
    \nmodel_name_or_path = '/mnt/disk2/LLM_MODELS/models/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True'\n\
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n           \
    \                                  device_map=\"auto\",\n                    \
    \                         trust_remote_code=True)\n"
  created_at: 2023-10-31 09:03:26+00:00
  edited: false
  hidden: false
  id: 6540d0ee3a60baceec710511
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-31T10:05:46.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9733725786209106
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Trigger a re-download of the branch - you might have an incompletely
          downloaded file.</p>

          '
        raw: Trigger a re-download of the branch - you might have an incompletely
          downloaded file.
        updatedAt: '2023-10-31T10:05:46.147Z'
      numEdits: 0
      reactions: []
    id: 6540d17aeddfff04354b9e28
    type: comment
  author: TheBloke
  content: Trigger a re-download of the branch - you might have an incompletely downloaded
    file.
  created_at: 2023-10-31 09:05:46+00:00
  edited: false
  hidden: false
  id: 6540d17aeddfff04354b9e28
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-31T10:44:20.000Z'
    data:
      edited: false
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6433392763137817
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>Well, I thought that was the problem (I redownloaded the branch)
          when it loaded the shards, but now it broke in modeling_qwen.py<br>Exception
          has occurred: RuntimeError<br>Unrecognized tensor type ID: AutocastCUDA<br>  File
          "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py",
          line 467, in forward<br>    mixed_x_layer = self.c_attn(hidden_states)<br>  File
          "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py",
          line 654, in forward<br>    attn_outputs = self.attn(<br>  File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py",
          line 951, in forward<br>    outputs = block(<br>  File "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py",
          line 1121, in forward<br>    transformer_outputs = self.transformer(<br>  File
          "/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py",
          line 1337, in generate<br>    return super().generate(<br>  File "/home/neman/PROGRAMMING/PYTHON/CroTranscribe/qwen_chat_14b_test.py",
          line 21, in <br>    output = model.generate(inputs=input_ids, temperature=0.7,
          do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)<br>RuntimeError:
          Unrecognized tensor type ID: AutocastCUDA</p>

          <p>So, the same branch(gptq-8bit-32g-actorder_True) works for you? Strange...<br>I''ll
          try fiddling with it some more later. Will report if I succeed to document
          for others if they encounter same issues.<br>Thanks!</p>

          '
        raw: "Well, I thought that was the problem (I redownloaded the branch) when\
          \ it loaded the shards, but now it broke in modeling_qwen.py\nException\
          \ has occurred: RuntimeError\nUnrecognized tensor type ID: AutocastCUDA\n\
          \  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
          , line 467, in forward\n    mixed_x_layer = self.c_attn(hidden_states)\n\
          \  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
          , line 654, in forward\n    attn_outputs = self.attn(\n  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
          , line 951, in forward\n    outputs = block(\n  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
          , line 1121, in forward\n    transformer_outputs = self.transformer(\n \
          \ File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
          , line 1337, in generate\n    return super().generate(\n  File \"/home/neman/PROGRAMMING/PYTHON/CroTranscribe/qwen_chat_14b_test.py\"\
          , line 21, in <module>\n    output = model.generate(inputs=input_ids, temperature=0.7,\
          \ do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nRuntimeError:\
          \ Unrecognized tensor type ID: AutocastCUDA\n\nSo, the same branch(gptq-8bit-32g-actorder_True)\
          \ works for you? Strange... \nI'll try fiddling with it some more later.\
          \ Will report if I succeed to document for others if they encounter same\
          \ issues.\nThanks!"
        updatedAt: '2023-10-31T10:44:20.451Z'
      numEdits: 0
      reactions: []
    id: 6540da84b3f9aad245dc27e5
    type: comment
  author: Neman
  content: "Well, I thought that was the problem (I redownloaded the branch) when\
    \ it loaded the shards, but now it broke in modeling_qwen.py\nException has occurred:\
    \ RuntimeError\nUnrecognized tensor type ID: AutocastCUDA\n  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
    , line 467, in forward\n    mixed_x_layer = self.c_attn(hidden_states)\n  File\
    \ \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
    , line 654, in forward\n    attn_outputs = self.attn(\n  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
    , line 951, in forward\n    outputs = block(\n  File \"/home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
    , line 1121, in forward\n    transformer_outputs = self.transformer(\n  File \"\
    /home/neman/.cache/huggingface/modules/transformers_modules/TheBloke_Qwen-14B-Chat-GPTQ_gptq-8bit-32g-actorder_True/modeling_qwen.py\"\
    , line 1337, in generate\n    return super().generate(\n  File \"/home/neman/PROGRAMMING/PYTHON/CroTranscribe/qwen_chat_14b_test.py\"\
    , line 21, in <module>\n    output = model.generate(inputs=input_ids, temperature=0.7,\
    \ do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\nRuntimeError: Unrecognized\
    \ tensor type ID: AutocastCUDA\n\nSo, the same branch(gptq-8bit-32g-actorder_True)\
    \ works for you? Strange... \nI'll try fiddling with it some more later. Will\
    \ report if I succeed to document for others if they encounter same issues.\n\
    Thanks!"
  created_at: 2023-10-31 09:44:20+00:00
  edited: false
  hidden: false
  id: 6540da84b3f9aad245dc27e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-31T11:16:10.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9279285073280334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Oh, AutoCastCUDA is a different problem. That happens when you are
          using PyTorch 2.1, which is not supported by the AutoGPTQ pre-built wheels.  So
          almost certainly it would be working for you now if you had PyTorch 2.0.1</p>

          <p>Steps to fix are:</p>

          <ol>

          <li>Downgrade to PyTorch 2.0.1, or</li>

          <li>Clone and build AutoGPTQ 0.4.2 from source, or</li>

          <li>Clone and build latest AutoGPTQ 0.5 from source, or</li>

          <li>Wait a day or two until the release of AutoGPTQ 0.5, which will have
          pre-built wheels for PyTorch 2.1.</li>

          </ol>

          '
        raw: 'Oh, AutoCastCUDA is a different problem. That happens when you are using
          PyTorch 2.1, which is not supported by the AutoGPTQ pre-built wheels.  So
          almost certainly it would be working for you now if you had PyTorch 2.0.1


          Steps to fix are:

          1. Downgrade to PyTorch 2.0.1, or

          2. Clone and build AutoGPTQ 0.4.2 from source, or

          3. Clone and build latest AutoGPTQ 0.5 from source, or

          4. Wait a day or two until the release of AutoGPTQ 0.5, which will have
          pre-built wheels for PyTorch 2.1.'
        updatedAt: '2023-10-31T11:16:34.951Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - Neman
    id: 6540e1fa3a60baceec7382c1
    type: comment
  author: TheBloke
  content: 'Oh, AutoCastCUDA is a different problem. That happens when you are using
    PyTorch 2.1, which is not supported by the AutoGPTQ pre-built wheels.  So almost
    certainly it would be working for you now if you had PyTorch 2.0.1


    Steps to fix are:

    1. Downgrade to PyTorch 2.0.1, or

    2. Clone and build AutoGPTQ 0.4.2 from source, or

    3. Clone and build latest AutoGPTQ 0.5 from source, or

    4. Wait a day or two until the release of AutoGPTQ 0.5, which will have pre-built
    wheels for PyTorch 2.1.'
  created_at: 2023-10-31 10:16:10+00:00
  edited: true
  hidden: false
  id: 6540e1fa3a60baceec7382c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
      fullname: Zeze Nene
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Neman
      type: user
    createdAt: '2023-10-31T13:28:20.000Z'
    data:
      edited: true
      editors:
      - Neman
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9578128457069397
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/643095dd23c81b522be33a94/jPDsH-gkQDSNhsh1WVmag.png?w=200&h=200&f=face
          fullname: Zeze Nene
          isHf: false
          isPro: false
          name: Neman
          type: user
        html: '<p>You are golden! I have env with PyTorch 2.0.1 already and yes, it
          works :)</p>

          <p>*** Generate:<br>Write a praise for Tom AKA TheBloke for being such a
          helpful guy.<br>Tom, you are the best! You always take the time to answer
          my questions and provide me with valuable advice. Your expertise in LLMs
          is truly impressive, and I am grateful for all the help you have given me.<br>Thank
          you for being such a kind and generous person. Your willingness to share
          your knowledge and support others is truly commendable, and it has made
          a big difference in my life.<br>I feel lucky to know you, Tom, and I hope
          to continue learning from you in the future. Keep up the great work!<br>Sincerely,<br>Neman&lt;|endoftext|&gt;</p>

          '
        raw: 'You are golden! I have env with PyTorch 2.0.1 already and yes, it works
          :)


          *** Generate:

          Write a praise for Tom AKA TheBloke for being such a helpful guy.

          Tom, you are the best! You always take the time to answer my questions and
          provide me with valuable advice. Your expertise in LLMs is truly impressive,
          and I am grateful for all the help you have given me.

          Thank you for being such a kind and generous person. Your willingness to
          share your knowledge and support others is truly commendable, and it has
          made a big difference in my life.

          I feel lucky to know you, Tom, and I hope to continue learning from you
          in the future. Keep up the great work!

          Sincerely,

          Neman<|endoftext|>'
        updatedAt: '2023-10-31T13:34:35.921Z'
      numEdits: 2
      reactions: []
    id: 654100f40a4e554c1bff045f
    type: comment
  author: Neman
  content: 'You are golden! I have env with PyTorch 2.0.1 already and yes, it works
    :)


    *** Generate:

    Write a praise for Tom AKA TheBloke for being such a helpful guy.

    Tom, you are the best! You always take the time to answer my questions and provide
    me with valuable advice. Your expertise in LLMs is truly impressive, and I am
    grateful for all the help you have given me.

    Thank you for being such a kind and generous person. Your willingness to share
    your knowledge and support others is truly commendable, and it has made a big
    difference in my life.

    I feel lucky to know you, Tom, and I hope to continue learning from you in the
    future. Keep up the great work!

    Sincerely,

    Neman<|endoftext|>'
  created_at: 2023-10-31 12:28:20+00:00
  edited: true
  hidden: false
  id: 654100f40a4e554c1bff045f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/Qwen-14B-Chat-GPTQ
repo_type: model
status: open
target_branch: null
title: Will it come?
