!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alloc7260
conflicting_files: null
created_at: 2023-04-19 10:51:20+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e53efdb29c51f5f4f52b3390a04c5741.svg
      fullname: Pranav
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alloc7260
      type: user
    createdAt: '2023-04-19T11:51:20.000Z'
    data:
      edited: false
      editors:
      - alloc7260
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e53efdb29c51f5f4f52b3390a04c5741.svg
          fullname: Pranav
          isHf: false
          isPro: false
          name: alloc7260
          type: user
        html: "<p>I have tried with this code<br>speech_array, sampling_rate = librosa.load(audio_path,\
          \ sr=16000)</p>\n<p>input_values = tokenizer(speech_array, return_tensors=\"\
          pt\").input_values.to(\"cuda\")</p>\n<p>with torch.no_grad():<br>    logits\
          \ = model(input_values).logits<br>predicted_ids = torch.argmax(logits, dim=-1)<br>transcription\
          \ = tokenizer.batch_decode(predicted_ids)[0]</p>\n<p>but it give me this<br>llen\u0120\
          significa\u0120Pass\u0120bands\u0120Need\xC3\xB3\xC5\u0124\xD0\xBB\xD0\xB0\
          \xD1\u0122bus\u0120Hook\u0120r\xC3\xA9pond\u0120questionnaire\u0120assemble\u0120\
          theoretically\u0120cientoODpersonal\u0120LiquidOD\xE0\xB8\u013D\u0120sausages\u0120\
          finding\u0120heaven\u0120\xEB\xAA\xA8\u0120Immer\u0120recognition\u0120\xEB\
          \xAA\xA8\xE0\xB8\u013D\u0120valleyvialgypt\u0120atmosphere\xD7\u013B\xD7\
          \u0136\u0120anyhow\u0120\xD0\xBE\xD0\xB4\xD0\xB8\xD0\xBD\u0120d\xC3\xBC\
          ny\u0120sele\u0120Geoff\u0120Grande\u0106\xD0\xB8\xD0\xBD\xD0\xB0\u0120\
          Museum\u0120merde\xED\u0137\u013Buestas\xD1\u0125\xD0\xBA\xD0\xB8\u0120\
          priorit\u0120unwanted\u0120lawyers\u0120soberplexvial\xE8\xA1\u0122\u0120\
          barkinghips\u0120terrorism\u0120liquor\u0120Aurora\xD0\xB5\xD1</p>\n<p>This\
          \ is not I want.</p>\n"
        raw: "I have tried with this code\r\nspeech_array, sampling_rate = librosa.load(audio_path,\
          \ sr=16000)\r\n\r\ninput_values = tokenizer(speech_array, return_tensors=\"\
          pt\").input_values.to(\"cuda\")\r\n\r\nwith torch.no_grad():\r\n    logits\
          \ = model(input_values).logits\r\npredicted_ids = torch.argmax(logits, dim=-1)\r\
          \ntranscription = tokenizer.batch_decode(predicted_ids)[0]\r\n\r\nbut it\
          \ give me this \r\nllen\u0120significa\u0120Pass\u0120bands\u0120Need\xC3\
          \xB3\xC5\u0124\xD0\xBB\xD0\xB0\xD1\u0122bus\u0120Hook\u0120r\xC3\xA9pond\u0120\
          questionnaire\u0120assemble\u0120theoretically\u0120cientoODpersonal\u0120\
          LiquidOD\xE0\xB8\u013D\u0120sausages\u0120finding\u0120heaven\u0120\xEB\xAA\
          \xA8\u0120Immer\u0120recognition\u0120\xEB\xAA\xA8\xE0\xB8\u013D\u0120valleyvialgypt\u0120\
          atmosphere\xD7\u013B\xD7\u0136\u0120anyhow\u0120\xD0\xBE\xD0\xB4\xD0\xB8\
          \xD0\xBD\u0120d\xC3\xBCny\u0120sele\u0120Geoff\u0120Grande\u0106\xD0\xB8\
          \xD0\xBD\xD0\xB0\u0120Museum\u0120merde\xED\u0137\u013Buestas\xD1\u0125\xD0\
          \xBA\xD0\xB8\u0120priorit\u0120unwanted\u0120lawyers\u0120soberplexvial\xE8\
          \xA1\u0122\u0120barkinghips\u0120terrorism\u0120liquor\u0120Aurora\xD0\xB5\
          \xD1\r\n\r\nThis is not I want."
        updatedAt: '2023-04-19T11:51:20.595Z'
      numEdits: 0
      reactions: []
    id: 643fd5b83e0374802e15baef
    type: comment
  author: alloc7260
  content: "I have tried with this code\r\nspeech_array, sampling_rate = librosa.load(audio_path,\
    \ sr=16000)\r\n\r\ninput_values = tokenizer(speech_array, return_tensors=\"pt\"\
    ).input_values.to(\"cuda\")\r\n\r\nwith torch.no_grad():\r\n    logits = model(input_values).logits\r\
    \npredicted_ids = torch.argmax(logits, dim=-1)\r\ntranscription = tokenizer.batch_decode(predicted_ids)[0]\r\
    \n\r\nbut it give me this \r\nllen\u0120significa\u0120Pass\u0120bands\u0120Need\xC3\
    \xB3\xC5\u0124\xD0\xBB\xD0\xB0\xD1\u0122bus\u0120Hook\u0120r\xC3\xA9pond\u0120\
    questionnaire\u0120assemble\u0120theoretically\u0120cientoODpersonal\u0120LiquidOD\xE0\
    \xB8\u013D\u0120sausages\u0120finding\u0120heaven\u0120\xEB\xAA\xA8\u0120Immer\u0120\
    recognition\u0120\xEB\xAA\xA8\xE0\xB8\u013D\u0120valleyvialgypt\u0120atmosphere\xD7\
    \u013B\xD7\u0136\u0120anyhow\u0120\xD0\xBE\xD0\xB4\xD0\xB8\xD0\xBD\u0120d\xC3\xBC\
    ny\u0120sele\u0120Geoff\u0120Grande\u0106\xD0\xB8\xD0\xBD\xD0\xB0\u0120Museum\u0120\
    merde\xED\u0137\u013Buestas\xD1\u0125\xD0\xBA\xD0\xB8\u0120priorit\u0120unwanted\u0120\
    lawyers\u0120soberplexvial\xE8\xA1\u0122\u0120barkinghips\u0120terrorism\u0120\
    liquor\u0120Aurora\xD0\xB5\xD1\r\n\r\nThis is not I want."
  created_at: 2023-04-19 10:51:20+00:00
  edited: false
  hidden: false
  id: 643fd5b83e0374802e15baef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61958153ddc6b60c90de310b/XQ0bAjflFmGv0TIdMCQ-k.jpeg?w=200&h=200&f=face
      fullname: Vasista Lodagala
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: vasista22
      type: user
    createdAt: '2023-04-24T21:29:51.000Z'
    data:
      edited: false
      editors:
      - vasista22
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/61958153ddc6b60c90de310b/XQ0bAjflFmGv0TIdMCQ-k.jpeg?w=200&h=200&f=face
          fullname: Vasista Lodagala
          isHf: false
          isPro: false
          name: vasista22
          type: user
        html: '<p>The model cards have been updated with relevant information regarding
          their inference.<br>The code snippets in the model card should clarify this
          issue.<br>Do let me know if your issue persists.</p>

          <p>You may also find the fine-tuning and evaluation scripts provided in
          the following repository useful.<br><a rel="nofollow" href="https://github.com/vasistalodagala/whisper-finetune">https://github.com/vasistalodagala/whisper-finetune</a></p>

          '
        raw: 'The model cards have been updated with relevant information regarding
          their inference.

          The code snippets in the model card should clarify this issue.

          Do let me know if your issue persists.


          You may also find the fine-tuning and evaluation scripts provided in the
          following repository useful.

          https://github.com/vasistalodagala/whisper-finetune'
        updatedAt: '2023-04-24T21:29:51.976Z'
      numEdits: 0
      reactions: []
    id: 6446f4cf1dc59ca8aed3c220
    type: comment
  author: vasista22
  content: 'The model cards have been updated with relevant information regarding
    their inference.

    The code snippets in the model card should clarify this issue.

    Do let me know if your issue persists.


    You may also find the fine-tuning and evaluation scripts provided in the following
    repository useful.

    https://github.com/vasistalodagala/whisper-finetune'
  created_at: 2023-04-24 20:29:51+00:00
  edited: false
  hidden: false
  id: 6446f4cf1dc59ca8aed3c220
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: vasista22/whisper-gujarati-base
repo_type: model
status: open
target_branch: null
title: How to use this in colab?
