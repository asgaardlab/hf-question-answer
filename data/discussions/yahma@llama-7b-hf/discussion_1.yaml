!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zokica
conflicting_files: null
created_at: 2023-04-22 12:15:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-04-22T13:15:24.000Z'
    data:
      edited: false
      editors:
      - zokica
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>Hello,</p>

          <p>When I use this code:<br>from transformers import AutoTokenizer, AutoModelForCausalLM<br>tokenizer
          = AutoTokenizer.from_pretrained("yahma/llama-7b-hf")</p>

          <p>##################################################################<br>ValueError:
          Couldn''t instantiate the backend tokenizer from one of:<br>(1) a <code>tokenizers</code>
          library serialization file,<br>(2) a slow tokenizer instance to convert
          or<br>(3) an equivalent slow tokenizer class to instantiate and convert.<br>You
          need to have sentencepiece installed to convert a slow tokenizer to a fast
          one.<br>#################################################################</p>

          <p>I do have sentencepiece installed.</p>

          '
        raw: "Hello,\r\n\r\nWhen I use this code:\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"yahma/llama-7b-hf\"\
          )\r\n\r\n##################################################################\r\
          \nValueError: Couldn't instantiate the backend tokenizer from one of: \r\
          \n(1) a `tokenizers` library serialization file, \r\n(2) a slow tokenizer\
          \ instance to convert or \r\n(3) an equivalent slow tokenizer class to instantiate\
          \ and convert. \r\nYou need to have sentencepiece installed to convert a\
          \ slow tokenizer to a fast one.\r\n#################################################################\r\
          \n\r\nI do have sentencepiece installed.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\
          \r\n"
        updatedAt: '2023-04-22T13:15:24.638Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - annaovesnaatatt
    id: 6443ddec1bc692d87b253c29
    type: comment
  author: zokica
  content: "Hello,\r\n\r\nWhen I use this code:\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\ntokenizer = AutoTokenizer.from_pretrained(\"yahma/llama-7b-hf\"\
    )\r\n\r\n##################################################################\r\n\
    ValueError: Couldn't instantiate the backend tokenizer from one of: \r\n(1) a\
    \ `tokenizers` library serialization file, \r\n(2) a slow tokenizer instance to\
    \ convert or \r\n(3) an equivalent slow tokenizer class to instantiate and convert.\
    \ \r\nYou need to have sentencepiece installed to convert a slow tokenizer to\
    \ a fast one.\r\n#################################################################\r\
    \n\r\nI do have sentencepiece installed.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
  created_at: 2023-04-22 12:15:24+00:00
  edited: false
  hidden: false
  id: 6443ddec1bc692d87b253c29
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yahma/llama-7b-hf
repo_type: model
status: open
target_branch: null
title: Tokenizer does not work
