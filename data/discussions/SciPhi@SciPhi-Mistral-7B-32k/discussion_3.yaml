!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rjmehta
conflicting_files: null
created_at: 2023-10-30 15:53:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-10-30T16:53:06.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.857326865196228
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>The model starts giving gibberish output when the input exceeds
          8-10k tokens. It says 32k, is that the context length it supports?</p>

          '
        raw: The model starts giving gibberish output when the input exceeds 8-10k
          tokens. It says 32k, is that the context length it supports?
        updatedAt: '2023-10-30T16:53:06.430Z'
      numEdits: 0
      reactions: []
    id: 653fdf7266c716609b181b7e
    type: comment
  author: rjmehta
  content: The model starts giving gibberish output when the input exceeds 8-10k tokens.
    It says 32k, is that the context length it supports?
  created_at: 2023-10-30 15:53:06+00:00
  edited: false
  hidden: false
  id: 653fdf7266c716609b181b7e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-10-30T16:53:45.000Z'
    data:
      from: Exllamav2 fails for 12k tokens.
      to: Gibberish output for 10k tokens.
    id: 653fdf99a6bd39ef5dfa8201
    type: title-change
  author: rjmehta
  created_at: 2023-10-30 15:53:45+00:00
  id: 653fdf99a6bd39ef5dfa8201
  new_title: Gibberish output for 10k tokens.
  old_title: Exllamav2 fails for 12k tokens.
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-10-30T16:57:26.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9795045852661133
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: '<p>It was ft''ed with a sliding window on textbook examples which reached
          a full 32k length.</p>

          <p>Can you share the example in question and I will investigate why this
          occurs?</p>

          '
        raw: 'It was ft''ed with a sliding window on textbook examples which reached
          a full 32k length.


          Can you share the example in question and I will investigate why this occurs?'
        updatedAt: '2023-10-30T16:57:26.728Z'
      numEdits: 0
      reactions: []
    id: 653fe0764ee512ad96a7930c
    type: comment
  author: emrgnt-cmplxty
  content: 'It was ft''ed with a sliding window on textbook examples which reached
    a full 32k length.


    Can you share the example in question and I will investigate why this occurs?'
  created_at: 2023-10-30 15:57:26+00:00
  edited: false
  hidden: false
  id: 653fe0764ee512ad96a7930c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-10-31T07:13:49.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9584310054779053
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>rjmehta, what backend are you using?</p>

          <p>I was getting problems at high context as well, but I need to go back
          and test some more.</p>

          '
        raw: 'rjmehta, what backend are you using?


          I was getting problems at high context as well, but I need to go back and
          test some more.'
        updatedAt: '2023-10-31T17:54:37.847Z'
      numEdits: 1
      reactions: []
    id: 6540a92d231ce22e2a980daf
    type: comment
  author: brucethemoose
  content: 'rjmehta, what backend are you using?


    I was getting problems at high context as well, but I need to go back and test
    some more.'
  created_at: 2023-10-31 06:13:49+00:00
  edited: true
  hidden: false
  id: 6540a92d231ce22e2a980daf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-10-31T21:48:53.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7119302153587341
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: "<p>OK, so I tested perplexity on ptb at different context sizes:</p>\n\
          <ul>\n<li><p>8K: 14.1376</p>\n</li>\n<li><p>16K: 32.3866</p>\n</li>\n<li><p>16K\
          \ with 2.5 RoPE alpha: 14.5083</p>\n</li>\n<li><p>32K: 115.6320</p>\n</li>\n\
          <li><p>32k with 5.0 RoPE alpha: 22.2502</p>\n</li>\n</ul>\n<p>Here is Zephyr\
          \ (an 8K model) for reference:</p>\n<ul>\n<li><p>8K: 14.2910</p>\n</li>\n\
          <li><p>16K: 125.4033</p>\n</li>\n<li><p>16K with 2.5 RoPE alpha: 13.8916\
          \ (???)</p>\n</li>\n<li><p>32K: 710.0536</p>\n</li>\n<li><p>32k with 5.0\
          \ RoPE alpha: 23.5875</p>\n</li>\n</ul>\n<p>So... <span data-props=\"{&quot;user&quot;:&quot;rjmehta&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rjmehta\"\
          >@<span class=\"underline\">rjmehta</span></a></span>\n\n\t</span></span>\
          \ by default, it seems to be <em>slightly less</em> catastrophic beyond\
          \ 8K than other finetunes, but still seems to benefit from RoPE alpha scaling\
          \ like a regular 8K model?</p>\n<p>Maybe we have something misconfigured.</p>\n"
        raw: 'OK, so I tested perplexity on ptb at different context sizes:


          - 8K: 14.1376


          - 16K: 32.3866


          - 16K with 2.5 RoPE alpha: 14.5083


          - 32K: 115.6320


          - 32k with 5.0 RoPE alpha: 22.2502


          Here is Zephyr (an 8K model) for reference:


          - 8K: 14.2910


          - 16K: 125.4033


          - 16K with 2.5 RoPE alpha: 13.8916 (???)


          - 32K: 710.0536


          - 32k with 5.0 RoPE alpha: 23.5875



          So... @rjmehta by default, it seems to be *slightly less* catastrophic beyond
          8K than other finetunes, but still seems to benefit from RoPE alpha scaling
          like a regular 8K model?


          Maybe we have something misconfigured.'
        updatedAt: '2023-10-31T22:00:19.930Z'
      numEdits: 4
      reactions: []
    id: 65417645cb18e4e19abe47b9
    type: comment
  author: brucethemoose
  content: 'OK, so I tested perplexity on ptb at different context sizes:


    - 8K: 14.1376


    - 16K: 32.3866


    - 16K with 2.5 RoPE alpha: 14.5083


    - 32K: 115.6320


    - 32k with 5.0 RoPE alpha: 22.2502


    Here is Zephyr (an 8K model) for reference:


    - 8K: 14.2910


    - 16K: 125.4033


    - 16K with 2.5 RoPE alpha: 13.8916 (???)


    - 32K: 710.0536


    - 32k with 5.0 RoPE alpha: 23.5875



    So... @rjmehta by default, it seems to be *slightly less* catastrophic beyond
    8K than other finetunes, but still seems to benefit from RoPE alpha scaling like
    a regular 8K model?


    Maybe we have something misconfigured.'
  created_at: 2023-10-31 20:48:53+00:00
  edited: true
  hidden: false
  id: 65417645cb18e4e19abe47b9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-10-31T22:54:07.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.983951210975647
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: '<p>Thanks for sharing these numbers. They are very interesting.</p>

          <p>What script did you run with btw? I will start including this as part
          of my training procedure.</p>

          <p>The model was not ft''ed with RoPE, rather it uses Mistral''s sliding
          + fixed window approach.</p>

          <p>It has been suggested by some that I run with RoPE or yarn in my next
          fine-tune. I will figure out how to do this in my next iteration.</p>

          '
        raw: 'Thanks for sharing these numbers. They are very interesting.


          What script did you run with btw? I will start including this as part of
          my training procedure.


          The model was not ft''ed with RoPE, rather it uses Mistral''s sliding +
          fixed window approach.


          It has been suggested by some that I run with RoPE or yarn in my next fine-tune.
          I will figure out how to do this in my next iteration.'
        updatedAt: '2023-10-31T22:54:07.499Z'
      numEdits: 0
      reactions: []
    id: 6541858f5242a5242f407469
    type: comment
  author: emrgnt-cmplxty
  content: 'Thanks for sharing these numbers. They are very interesting.


    What script did you run with btw? I will start including this as part of my training
    procedure.


    The model was not ft''ed with RoPE, rather it uses Mistral''s sliding + fixed
    window approach.


    It has been suggested by some that I run with RoPE or yarn in my next fine-tune.
    I will figure out how to do this in my next iteration.'
  created_at: 2023-10-31 21:54:07+00:00
  edited: false
  hidden: false
  id: 6541858f5242a5242f407469
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-10-31T23:08:22.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9253854751586914
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>I am no expert on long context training these days, but you might
          take a look at this?</p>

          <p><a href="https://huggingface.co/Yukang/Llama-2-13b-longlora-32k">https://huggingface.co/Yukang/Llama-2-13b-longlora-32k</a></p>

          <p>I just used exllamav2''s testing script since its very fast and I already
          had it setup on my desktop:</p>

          <p><a rel="nofollow" href="https://github.com/turboderp/exllamav2/blob/master/test_inference.py">https://github.com/turboderp/exllamav2/blob/master/test_inference.py</a></p>

          <p>But it requires a quantized model, so it may be less than ideal for testing
          during training.</p>

          '
        raw: 'I am no expert on long context training these days, but you might take
          a look at this?


          https://huggingface.co/Yukang/Llama-2-13b-longlora-32k


          I just used exllamav2''s testing script since its very fast and I already
          had it setup on my desktop:


          https://github.com/turboderp/exllamav2/blob/master/test_inference.py


          But it requires a quantized model, so it may be less than ideal for testing
          during training.'
        updatedAt: '2023-10-31T23:09:57.821Z'
      numEdits: 2
      reactions: []
    id: 654188e65019954eef9700d7
    type: comment
  author: brucethemoose
  content: 'I am no expert on long context training these days, but you might take
    a look at this?


    https://huggingface.co/Yukang/Llama-2-13b-longlora-32k


    I just used exllamav2''s testing script since its very fast and I already had
    it setup on my desktop:


    https://github.com/turboderp/exllamav2/blob/master/test_inference.py


    But it requires a quantized model, so it may be less than ideal for testing during
    training.'
  created_at: 2023-10-31 22:08:22+00:00
  edited: true
  hidden: false
  id: 654188e65019954eef9700d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-11-01T01:00:16.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8511298894882202
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>\
          \ I am using the exllamav2 backend. The Mistral says it supports 32k but\
          \ when used with exllamav2 mistral derivative gptq quantized models, it\
          \ works fine until 8k. I will try the llongorca-32k. But the true 32k will\
          \ work if the input context really reaches to that limit while training.\
          \ Scaling and compression technique tends to over-compress the inputs. Though\
          \ this works fine for questions like \"Summarize\" but when asked to extract\
          \ specific values from 32k context without \"RAG\" techniques, it fails\
          \ in extracting such specific information.</p>\n"
        raw: '@brucethemoose I am using the exllamav2 backend. The Mistral says it
          supports 32k but when used with exllamav2 mistral derivative gptq quantized
          models, it works fine until 8k. I will try the llongorca-32k. But the true
          32k will work if the input context really reaches to that limit while training.
          Scaling and compression technique tends to over-compress the inputs. Though
          this works fine for questions like "Summarize" but when asked to extract
          specific values from 32k context without "RAG" techniques, it fails in extracting
          such specific information.'
        updatedAt: '2023-11-01T01:00:16.825Z'
      numEdits: 0
      reactions: []
    id: 6541a32055ded3e0939682ef
    type: comment
  author: rjmehta
  content: '@brucethemoose I am using the exllamav2 backend. The Mistral says it supports
    32k but when used with exllamav2 mistral derivative gptq quantized models, it
    works fine until 8k. I will try the llongorca-32k. But the true 32k will work
    if the input context really reaches to that limit while training. Scaling and
    compression technique tends to over-compress the inputs. Though this works fine
    for questions like "Summarize" but when asked to extract specific values from
    32k context without "RAG" techniques, it fails in extracting such specific information.'
  created_at: 2023-11-01 00:00:16+00:00
  edited: false
  hidden: false
  id: 6541a32055ded3e0939682ef
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
      fullname: Raj Mehta
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rjmehta
      type: user
    createdAt: '2023-11-01T01:00:50.000Z'
    data:
      edited: false
      editors:
      - rjmehta
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9283326268196106
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e03299d063da54fa6d8c455d27ca4786.svg
          fullname: Raj Mehta
          isHf: false
          isPro: false
          name: rjmehta
          type: user
        html: '<p>Please correct me if my hypothesis is wrong</p>

          '
        raw: Please correct me if my hypothesis is wrong
        updatedAt: '2023-11-01T01:00:50.302Z'
      numEdits: 0
      reactions: []
    id: 6541a34272a0eadc694d3371
    type: comment
  author: rjmehta
  content: Please correct me if my hypothesis is wrong
  created_at: 2023-11-01 00:00:50+00:00
  edited: false
  hidden: false
  id: 6541a34272a0eadc694d3371
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-01T02:50:59.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8502975106239319
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: "<p>Here's the same test with Amazon's MistralLite:</p>\n<p><a href=\"\
          https://huggingface.co/emozilla/MistralLite\">https://huggingface.co/emozilla/MistralLite</a></p>\n\
          <ul>\n<li><p>8K: 12.9383</p>\n</li>\n<li><p>16k: 13.7877</p>\n</li>\n<li><p>16k\
          \ with 2.5 RoPE alpha: 15.8783</p>\n</li>\n<li><p>32k: 15.1048</p>\n</li>\n\
          <li><p>32k with 2.5 RoPE alpha: 17.9006</p>\n</li>\n</ul>\n<p>That looks\
          \ like its scaling to 32K without any stretching needed.</p>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;rjmehta&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/rjmehta\">@<span class=\"underline\">rjmehta</span></a></span>\n\
          \n\t</span></span> </p>\n<p>Yeah you are correct, training-free scaling\
          \ is a serious compromise.</p>\n<p>I dunno what the best 32K retrieval models\
          \ are atm, but I believe llongma has been succeeded by longlora 70B and\
          \ MistralLite, at the very least.</p>\n"
        raw: "Here's the same test with Amazon's MistralLite:\n\nhttps://huggingface.co/emozilla/MistralLite\n\
          \n- 8K: 12.9383\n\n- 16k: 13.7877\n\n- 16k with 2.5 RoPE alpha: 15.8783\n\
          \n- 32k: 15.1048\n\n- 32k with 2.5 RoPE alpha: 17.9006\n\nThat looks like\
          \ its scaling to 32K without any stretching needed.\n\n@rjmehta \n\nYeah\
          \ you are correct, training-free scaling is a serious compromise.\n\nI dunno\
          \ what the best 32K retrieval models are atm, but I believe llongma has\
          \ been succeeded by longlora 70B and MistralLite, at the very least."
        updatedAt: '2023-11-01T02:51:14.003Z'
      numEdits: 1
      reactions: []
    id: 6541bd13ae02affd743d595e
    type: comment
  author: brucethemoose
  content: "Here's the same test with Amazon's MistralLite:\n\nhttps://huggingface.co/emozilla/MistralLite\n\
    \n- 8K: 12.9383\n\n- 16k: 13.7877\n\n- 16k with 2.5 RoPE alpha: 15.8783\n\n- 32k:\
    \ 15.1048\n\n- 32k with 2.5 RoPE alpha: 17.9006\n\nThat looks like its scaling\
    \ to 32K without any stretching needed.\n\n@rjmehta \n\nYeah you are correct,\
    \ training-free scaling is a serious compromise.\n\nI dunno what the best 32K\
    \ retrieval models are atm, but I believe llongma has been succeeded by longlora\
    \ 70B and MistralLite, at the very least."
  created_at: 2023-11-01 01:50:59+00:00
  edited: true
  hidden: false
  id: 6541bd13ae02affd743d595e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
      fullname: Owen Colegrove
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: emrgnt-cmplxty
      type: user
    createdAt: '2023-11-01T02:56:42.000Z'
    data:
      edited: false
      editors:
      - emrgnt-cmplxty
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9789036512374878
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ba5d988373bb13f8760f56bf38e59428.svg
          fullname: Owen Colegrove
          isHf: false
          isPro: false
          name: emrgnt-cmplxty
          type: user
        html: '<p>Thanks for all the great analysis - it has been informative.</p>

          <p>Have people tested Amazon''s MistralLite in the wild? It seems that their
          approach is very high quality, so I would prefer to use this model as a
          base and then fine-tune it on textbook data for the next pass. I will start
          looking into how hard this will be.</p>

          '
        raw: 'Thanks for all the great analysis - it has been informative.


          Have people tested Amazon''s MistralLite in the wild? It seems that their
          approach is very high quality, so I would prefer to use this model as a
          base and then fine-tune it on textbook data for the next pass. I will start
          looking into how hard this will be.'
        updatedAt: '2023-11-01T02:56:42.731Z'
      numEdits: 0
      reactions: []
    id: 6541be6aa385933e8113b855
    type: comment
  author: emrgnt-cmplxty
  content: 'Thanks for all the great analysis - it has been informative.


    Have people tested Amazon''s MistralLite in the wild? It seems that their approach
    is very high quality, so I would prefer to use this model as a base and then fine-tune
    it on textbook data for the next pass. I will start looking into how hard this
    will be.'
  created_at: 2023-11-01 01:56:42+00:00
  edited: false
  hidden: false
  id: 6541be6aa385933e8113b855
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-01T03:27:59.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9296876788139343
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<blockquote>

          <p>Have people tested Amazon''s MistralLite in the wild?</p>

          </blockquote>

          <p>Its <em>very</em> good at summarization and retrieval. In fact, if you
          give it a full context, it sometimes tries to summarize (and succeeds!)
          or pick out a fact even if you didn''t ask for that in the initial prompt.</p>

          <p>It seems to retain general knowledge though. I think it would be a good
          base... or maybe even an acceptable base merged with Zephyr?</p>

          <p>Now that you got me thinking about it, I am going to see if merging Lite
          into other Mistral models will "extend" their context even though the instruct
          syntax is different.</p>

          '
        raw: '> Have people tested Amazon''s MistralLite in the wild?


          Its *very* good at summarization and retrieval. In fact, if you give it
          a full context, it sometimes tries to summarize (and succeeds!) or pick
          out a fact even if you didn''t ask for that in the initial prompt.


          It seems to retain general knowledge though. I think it would be a good
          base... or maybe even an acceptable base merged with Zephyr?


          Now that you got me thinking about it, I am going to see if merging Lite
          into other Mistral models will "extend" their context even though the instruct
          syntax is different.'
        updatedAt: '2023-11-01T03:32:45.356Z'
      numEdits: 4
      reactions: []
    id: 6541c5bfb0170e9607f31055
    type: comment
  author: brucethemoose
  content: '> Have people tested Amazon''s MistralLite in the wild?


    Its *very* good at summarization and retrieval. In fact, if you give it a full
    context, it sometimes tries to summarize (and succeeds!) or pick out a fact even
    if you didn''t ask for that in the initial prompt.


    It seems to retain general knowledge though. I think it would be a good base...
    or maybe even an acceptable base merged with Zephyr?


    Now that you got me thinking about it, I am going to see if merging Lite into
    other Mistral models will "extend" their context even though the instruct syntax
    is different.'
  created_at: 2023-11-01 02:27:59+00:00
  edited: true
  hidden: false
  id: 6541c5bfb0170e9607f31055
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-11-04T23:28:38.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9946215748786926
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>(I was not successful with this, as the models use different vocab)</p>

          '
        raw: (I was not successful with this, as the models use different vocab)
        updatedAt: '2023-11-04T23:28:38.032Z'
      numEdits: 0
      reactions: []
    id: 6546d3a66b1695ac33ac734f
    type: comment
  author: brucethemoose
  content: (I was not successful with this, as the models use different vocab)
  created_at: 2023-11-04 22:28:38+00:00
  edited: false
  hidden: false
  id: 6546d3a66b1695ac33ac734f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-08T12:36:36.000Z'
    data:
      edited: true
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9090027809143066
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;rjmehta&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rjmehta\">@<span class=\"\
          underline\">rjmehta</span></a></span>\n\n\t</span></span> - how did you\
          \ run this for above 16k context? I'm running into this issue:</p>\n<pre><code>past\
          \ key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,\
          \ head_dim`)\n</code></pre>\n<p>See this issue: past key much have a shape\
          \ of <a href=\"https://huggingface.co/amazon/MistralLite/discussions/14\"\
          >https://huggingface.co/amazon/MistralLite/discussions/14</a></p>\n"
        raw: '@brucethemoose @rjmehta - how did you run this for above 16k context?
          I''m running into this issue:

          ```

          past key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,
          head_dim`)

          ```


          See this issue: past key much have a shape of https://huggingface.co/amazon/MistralLite/discussions/14'
        updatedAt: '2023-12-08T12:36:54.213Z'
      numEdits: 1
      reactions: []
    id: 65730dd4d109988b4c385c8b
    type: comment
  author: RonanMcGovern
  content: '@brucethemoose @rjmehta - how did you run this for above 16k context?
    I''m running into this issue:

    ```

    past key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,
    head_dim`)

    ```


    See this issue: past key much have a shape of https://huggingface.co/amazon/MistralLite/discussions/14'
  created_at: 2023-12-08 12:36:36+00:00
  edited: true
  hidden: false
  id: 65730dd4d109988b4c385c8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-08T18:29:05.000Z'
    data:
      edited: false
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9345167279243469
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: "<blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>\
          \ <span data-props=\"{&quot;user&quot;:&quot;rjmehta&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/rjmehta\">@<span class=\"\
          underline\">rjmehta</span></a></span>\n\n\t</span></span> - how did you\
          \ run this for above 16k context? I'm running into this issue:</p>\n<pre><code>past\
          \ key much have a shape of (`batch_size, num_heads, self.config.sliding_window-1,\
          \ head_dim`)\n</code></pre>\n<p>See this issue: past key much have a shape\
          \ of <a href=\"https://huggingface.co/amazon/MistralLite/discussions/14\"\
          >https://huggingface.co/amazon/MistralLite/discussions/14</a></p>\n</blockquote>\n\
          <p>I was running an exl2 quantization via exllamav2. It doesn't even use\
          \ the sliding window, I don't think, the model \"just worked\" out to 32K.</p>\n"
        raw: "> @brucethemoose @rjmehta - how did you run this for above 16k context?\
          \ I'm running into this issue:\n> ```\n> past key much have a shape of (`batch_size,\
          \ num_heads, self.config.sliding_window-1, head_dim`)\n> ```\n> \n> See\
          \ this issue: past key much have a shape of https://huggingface.co/amazon/MistralLite/discussions/14\n\
          \nI was running an exl2 quantization via exllamav2. It doesn't even use\
          \ the sliding window, I don't think, the model \"just worked\" out to 32K."
        updatedAt: '2023-12-08T18:29:05.674Z'
      numEdits: 0
      reactions: []
    id: 6573607154d17496129ad66a
    type: comment
  author: brucethemoose
  content: "> @brucethemoose @rjmehta - how did you run this for above 16k context?\
    \ I'm running into this issue:\n> ```\n> past key much have a shape of (`batch_size,\
    \ num_heads, self.config.sliding_window-1, head_dim`)\n> ```\n> \n> See this issue:\
    \ past key much have a shape of https://huggingface.co/amazon/MistralLite/discussions/14\n\
    \nI was running an exl2 quantization via exllamav2. It doesn't even use the sliding\
    \ window, I don't think, the model \"just worked\" out to 32K."
  created_at: 2023-12-08 18:29:05+00:00
  edited: false
  hidden: false
  id: 6573607154d17496129ad66a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-08T18:43:09.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416780471801758
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: "<p>Appreciate it <span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>\
          \ , yeah I guess if you go via exllama that side steps the mistral architecture...\
          \ probably not a bad option.</p>\n<p>I'd still like to know how to get this\
          \ working. Seems odd if you have to set the sliding window size to be the\
          \ same as the input context. Seems to defeat the purpose of a sliding window.</p>\n"
        raw: 'Appreciate it @brucethemoose , yeah I guess if you go via exllama that
          side steps the mistral architecture... probably not a bad option.


          I''d still like to know how to get this working. Seems odd if you have to
          set the sliding window size to be the same as the input context. Seems to
          defeat the purpose of a sliding window.'
        updatedAt: '2023-12-08T18:43:09.417Z'
      numEdits: 0
      reactions: []
    id: 657363bd3e0cb21bc7b6ed68
    type: comment
  author: RonanMcGovern
  content: 'Appreciate it @brucethemoose , yeah I guess if you go via exllama that
    side steps the mistral architecture... probably not a bad option.


    I''d still like to know how to get this working. Seems odd if you have to set
    the sliding window size to be the same as the input context. Seems to defeat the
    purpose of a sliding window.'
  created_at: 2023-12-08 18:43:09+00:00
  edited: false
  hidden: false
  id: 657363bd3e0cb21bc7b6ed68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2023-12-08T19:03:55.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9520888328552246
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: "<blockquote>\n<p>Appreciate it <span data-props=\"{&quot;user&quot;:&quot;brucethemoose&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brucethemoose\"\
          >@<span class=\"underline\">brucethemoose</span></a></span>\n\n\t</span></span>\
          \ , yeah I guess if you go via exllama that side steps the mistral architecture...\
          \ probably not a bad option.</p>\n<p>I'd still like to know how to get this\
          \ working. Seems odd if you have to set the sliding window size to be the\
          \ same as the input context. Seems to defeat the purpose of a sliding window.</p>\n\
          </blockquote>\n<p>TBH the sliding window is not that great, and you are\
          \ better off skipping it :P. Full attention at 32K is not very compute expensive,\
          \ unless you use the unoptimized vanilla transformers code.</p>\n"
        raw: "> Appreciate it @brucethemoose , yeah I guess if you go via exllama\
          \ that side steps the mistral architecture... probably not a bad option.\n\
          > \n> I'd still like to know how to get this working. Seems odd if you have\
          \ to set the sliding window size to be the same as the input context. Seems\
          \ to defeat the purpose of a sliding window.\n\nTBH the sliding window is\
          \ not that great, and you are better off skipping it :P. Full attention\
          \ at 32K is not very compute expensive, unless you use the unoptimized vanilla\
          \ transformers code."
        updatedAt: '2023-12-08T19:05:40.457Z'
      numEdits: 2
      reactions: []
    id: 6573689b3e0cb21bc7b7c702
    type: comment
  author: brucethemoose
  content: "> Appreciate it @brucethemoose , yeah I guess if you go via exllama that\
    \ side steps the mistral architecture... probably not a bad option.\n> \n> I'd\
    \ still like to know how to get this working. Seems odd if you have to set the\
    \ sliding window size to be the same as the input context. Seems to defeat the\
    \ purpose of a sliding window.\n\nTBH the sliding window is not that great, and\
    \ you are better off skipping it :P. Full attention at 32K is not very compute\
    \ expensive, unless you use the unoptimized vanilla transformers code."
  created_at: 2023-12-08 19:03:55+00:00
  edited: true
  hidden: false
  id: 6573689b3e0cb21bc7b7c702
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
      fullname: Ronan McGovern
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: RonanMcGovern
      type: user
    createdAt: '2023-12-09T14:07:22.000Z'
    data:
      edited: false
      editors:
      - RonanMcGovern
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9897807240486145
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/-6Yq7oM_Ju6Zi2GEvobvb.jpeg?w=200&h=200&f=face
          fullname: Ronan McGovern
          isHf: false
          isPro: false
          name: RonanMcGovern
          type: user
        html: '<p>Thanks, yeah agreed, I don''t get the sliding window. Doesn''t seem
          an improvement, perhaps a disimprovement.</p>

          '
        raw: Thanks, yeah agreed, I don't get the sliding window. Doesn't seem an
          improvement, perhaps a disimprovement.
        updatedAt: '2023-12-09T14:07:22.838Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - brucethemoose
    id: 6574749a30b73319792a657f
    type: comment
  author: RonanMcGovern
  content: Thanks, yeah agreed, I don't get the sliding window. Doesn't seem an improvement,
    perhaps a disimprovement.
  created_at: 2023-12-09 14:07:22+00:00
  edited: false
  hidden: false
  id: 6574749a30b73319792a657f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: SciPhi/SciPhi-Mistral-7B-32k
repo_type: model
status: open
target_branch: null
title: Gibberish output for 10k tokens.
