!!python/object:huggingface_hub.community.DiscussionWithDetails
author: lucasjin
conflicting_files: null
created_at: 2023-11-16 11:00:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fbbb8eaaa7b19752b336cf228d4679e.svg
      fullname: lucasjin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucasjin
      type: user
    createdAt: '2023-11-16T11:00:59.000Z'
    data:
      edited: false
      editors:
      - lucasjin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46326497197151184
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fbbb8eaaa7b19752b336cf228d4679e.svg
          fullname: lucasjin
          isHf: false
          isPro: false
          name: lucasjin
          type: user
        html: '<p>Why so big</p>

          '
        raw: Why so big
        updatedAt: '2023-11-16T11:00:59.755Z'
      numEdits: 0
      reactions: []
    id: 6555f66b98be29223d3f0f6d
    type: comment
  author: lucasjin
  content: Why so big
  created_at: 2023-11-16 11:00:59+00:00
  edited: false
  hidden: false
  id: 6555f66b98be29223d3f0f6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
      fullname: Yu
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Yhyu13
      type: user
    createdAt: '2023-11-17T06:00:15.000Z'
    data:
      edited: false
      editors:
      - Yhyu13
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9625881910324097
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19f9eeb9281b47b34f68c312092ca468.svg
          fullname: Yu
          isHf: false
          isPro: false
          name: Yhyu13
          type: user
        html: '<p>eh, since fp16 or bf16 are considered as sufficient training precision
          for LLMs. And thus, they are stored in fp16 precisions.</p>

          <p>But most of the time, you''d rather quantize them to 4bit which 1/4 the
          size to make inferencing faster and use less ram</p>

          '
        raw: 'eh, since fp16 or bf16 are considered as sufficient training precision
          for LLMs. And thus, they are stored in fp16 precisions.


          But most of the time, you''d rather quantize them to 4bit which 1/4 the
          size to make inferencing faster and use less ram'
        updatedAt: '2023-11-17T06:00:15.929Z'
      numEdits: 0
      reactions: []
    id: 6557016f85d43542fa265e76
    type: comment
  author: Yhyu13
  content: 'eh, since fp16 or bf16 are considered as sufficient training precision
    for LLMs. And thus, they are stored in fp16 precisions.


    But most of the time, you''d rather quantize them to 4bit which 1/4 the size to
    make inferencing faster and use less ram'
  created_at: 2023-11-17 06:00:15+00:00
  edited: false
  hidden: false
  id: 6557016f85d43542fa265e76
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: NousResearch/Nous-Capybara-34B
repo_type: model
status: open
target_branch: null
title: Does the weights is fp16?
