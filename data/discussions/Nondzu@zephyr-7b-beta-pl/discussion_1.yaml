!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Whitepaper
conflicting_files: null
created_at: 2023-11-18 06:03:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c9a7d05af8bdbd53b38f4b805fd7c77f.svg
      fullname: Michael
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Whitepaper
      type: user
    createdAt: '2023-11-18T06:03:09.000Z'
    data:
      edited: false
      editors:
      - Whitepaper
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9139887094497681
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c9a7d05af8bdbd53b38f4b805fd7c77f.svg
          fullname: Michael
          isHf: false
          isPro: false
          name: Whitepaper
          type: user
        html: '<p>Greetings! Thank you for your work!<br>Could you please share the
          details of the fine tuning apart from the datasets used? Different QLora
          settings and ways of preparing datasets can significantly affect the generation
          results, which is what I am experiencing in my case.</p>

          '
        raw: "Greetings! Thank you for your work!\r\nCould you please share the details\
          \ of the fine tuning apart from the datasets used? Different QLora settings\
          \ and ways of preparing datasets can significantly affect the generation\
          \ results, which is what I am experiencing in my case."
        updatedAt: '2023-11-18T06:03:09.462Z'
      numEdits: 0
      reactions: []
    id: 6558539dc11dee7f7e119437
    type: comment
  author: Whitepaper
  content: "Greetings! Thank you for your work!\r\nCould you please share the details\
    \ of the fine tuning apart from the datasets used? Different QLora settings and\
    \ ways of preparing datasets can significantly affect the generation results,\
    \ which is what I am experiencing in my case."
  created_at: 2023-11-18 06:03:09+00:00
  edited: false
  hidden: false
  id: 6558539dc11dee7f7e119437
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63729f35acef705233c87909/2ApbWLasMZLptlSbk9emj.png?w=200&h=200&f=face
      fullname: Kamil
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Nondzu
      type: user
    createdAt: '2023-11-26T08:50:44.000Z'
    data:
      edited: false
      editors:
      - Nondzu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9466566443443298
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63729f35acef705233c87909/2ApbWLasMZLptlSbk9emj.png?w=200&h=200&f=face
          fullname: Kamil
          isHf: false
          isPro: false
          name: Nondzu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Whitepaper&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Whitepaper\">@<span class=\"\
          underline\">Whitepaper</span></a></span>\n\n\t</span></span> Thanks for\
          \ your comment. </p>\n<p>I'll be sharing the axolotl settings today, but\
          \ keep in mind that tinkering with settings isn't the most crucial aspect\
          \ and the training time isn't brief!<br>The quality of the dataset is paramount.<br>My\
          \ primary motivation for creating this was to construct a model with a PL\
          \ dataset and then generate or translate a new dataset into a high-quality\
          \ synthetic dataset, or a dataset focused on a specific task. </p>\n<p>The\
          \ next level idea is to build multiple lora/qlora adapters that can be loaded\
          \ on without having to restart your backend, similar to a hot swap.</p>\n\
          <p>However, I'm working on this alone and in my spare time. Any collaboration\
          \ would be helpful.</p>\n<p>Remember, the journey of a thousand miles begins\
          \ with a single step. Keep calm, stay focused, and let's make great things\
          \ happen together.<br>Thank you for your time and consideration.</p>\n"
        raw: "@Whitepaper Thanks for your comment. \n\nI'll be sharing the axolotl\
          \ settings today, but keep in mind that tinkering with settings isn't the\
          \ most crucial aspect and the training time isn't brief! \nThe quality of\
          \ the dataset is paramount. \nMy primary motivation for creating this was\
          \ to construct a model with a PL dataset and then generate or translate\
          \ a new dataset into a high-quality synthetic dataset, or a dataset focused\
          \ on a specific task. \n\nThe next level idea is to build multiple lora/qlora\
          \ adapters that can be loaded on without having to restart your backend,\
          \ similar to a hot swap.\n\nHowever, I'm working on this alone and in my\
          \ spare time. Any collaboration would be helpful.\n\nRemember, the journey\
          \ of a thousand miles begins with a single step. Keep calm, stay focused,\
          \ and let's make great things happen together. \nThank you for your time\
          \ and consideration."
        updatedAt: '2023-11-26T08:50:44.914Z'
      numEdits: 0
      reactions: []
    id: 656306e430a88a2f1dedd4f7
    type: comment
  author: Nondzu
  content: "@Whitepaper Thanks for your comment. \n\nI'll be sharing the axolotl settings\
    \ today, but keep in mind that tinkering with settings isn't the most crucial\
    \ aspect and the training time isn't brief! \nThe quality of the dataset is paramount.\
    \ \nMy primary motivation for creating this was to construct a model with a PL\
    \ dataset and then generate or translate a new dataset into a high-quality synthetic\
    \ dataset, or a dataset focused on a specific task. \n\nThe next level idea is\
    \ to build multiple lora/qlora adapters that can be loaded on without having to\
    \ restart your backend, similar to a hot swap.\n\nHowever, I'm working on this\
    \ alone and in my spare time. Any collaboration would be helpful.\n\nRemember,\
    \ the journey of a thousand miles begins with a single step. Keep calm, stay focused,\
    \ and let's make great things happen together. \nThank you for your time and consideration."
  created_at: 2023-11-26 08:50:44+00:00
  edited: false
  hidden: false
  id: 656306e430a88a2f1dedd4f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63729f35acef705233c87909/2ApbWLasMZLptlSbk9emj.png?w=200&h=200&f=face
      fullname: Kamil
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Nondzu
      type: user
    createdAt: '2023-11-26T09:03:19.000Z'
    data:
      edited: false
      editors:
      - Nondzu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.22101397812366486
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63729f35acef705233c87909/2ApbWLasMZLptlSbk9emj.png?w=200&h=200&f=face
          fullname: Kamil
          isHf: false
          isPro: false
          name: Nondzu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Whitepaper&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Whitepaper\">@<span class=\"\
          underline\">Whitepaper</span></a></span>\n\n\t</span></span>  <a href=\"\
          https://huggingface.co/Nondzu/zephyr-7b-beta-pl/blob/main/zephyr-beta-pl-02.yml\"\
          >https://huggingface.co/Nondzu/zephyr-7b-beta-pl/blob/main/zephyr-beta-pl-02.yml</a></p>\n"
        raw: '@Whitepaper  https://huggingface.co/Nondzu/zephyr-7b-beta-pl/blob/main/zephyr-beta-pl-02.yml'
        updatedAt: '2023-11-26T09:03:19.996Z'
      numEdits: 0
      reactions: []
    id: 656309d77a465cdcb380abbd
    type: comment
  author: Nondzu
  content: '@Whitepaper  https://huggingface.co/Nondzu/zephyr-7b-beta-pl/blob/main/zephyr-beta-pl-02.yml'
  created_at: 2023-11-26 09:03:19+00:00
  edited: false
  hidden: false
  id: 656309d77a465cdcb380abbd
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Nondzu/zephyr-7b-beta-pl
repo_type: model
status: open
target_branch: null
title: Fine-tuning features
