!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Cetafe
conflicting_files: null
created_at: 2023-04-18 13:47:55+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4efa0c95331da06247e98aafc89e2f23.svg
      fullname: Ceta Lee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Cetafe
      type: user
    createdAt: '2023-04-18T14:47:55.000Z'
    data:
      edited: false
      editors:
      - Cetafe
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4efa0c95331da06247e98aafc89e2f23.svg
          fullname: Ceta Lee
          isHf: false
          isPro: false
          name: Cetafe
          type: user
        html: '<p>Hi, I am new to DL, just curious how to load the safetensors model?
          I used "model = LlamaForCausalLM.from_pretrained(''E:/vicuna_13b_v1.1_GPTQ_4bit128g_cuda/'').cuda()
          " just like vicuna native and it gives me an error....FileNotFoundError:
          [Errno 2] No such file or directory:<br>''E:/vicuna_13b_v1.1_GPTQ_4bit128g_cuda/pytorch_model-00001-of-00003.bin''   --&gt;
          Do I need to modify the config? Thanks a lot!!!</p>

          '
        raw: "Hi, I am new to DL, just curious how to load the safetensors model?\
          \ I used \"model = LlamaForCausalLM.from_pretrained('E:/vicuna_13b_v1.1_GPTQ_4bit128g_cuda/').cuda()\
          \ \" just like vicuna native and it gives me an error....FileNotFoundError:\
          \ [Errno 2] No such file or directory: \r\n'E:/vicuna_13b_v1.1_GPTQ_4bit128g_cuda/pytorch_model-00001-of-00003.bin'\
          \   --> Do I need to modify the config? Thanks a lot!!!"
        updatedAt: '2023-04-18T14:47:55.056Z'
      numEdits: 0
      reactions: []
    id: 643ead9be5500bc3b9a58371
    type: comment
  author: Cetafe
  content: "Hi, I am new to DL, just curious how to load the safetensors model? I\
    \ used \"model = LlamaForCausalLM.from_pretrained('E:/vicuna_13b_v1.1_GPTQ_4bit128g_cuda/').cuda()\
    \ \" just like vicuna native and it gives me an error....FileNotFoundError: [Errno\
    \ 2] No such file or directory: \r\n'E:/vicuna_13b_v1.1_GPTQ_4bit128g_cuda/pytorch_model-00001-of-00003.bin'\
    \   --> Do I need to modify the config? Thanks a lot!!!"
  created_at: 2023-04-18 13:47:55+00:00
  edited: false
  hidden: false
  id: 643ead9be5500bc3b9a58371
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640261025caf6d21d67d01ba/NHAGdq4SEjt6B0QWXncUS.jpeg?w=200&h=200&f=face
      fullname: Shaun Prince
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Suparious
      type: user
    createdAt: '2023-04-21T07:24:20.000Z'
    data:
      edited: false
      editors:
      - Suparious
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/640261025caf6d21d67d01ba/NHAGdq4SEjt6B0QWXncUS.jpeg?w=200&h=200&f=face
          fullname: Shaun Prince
          isHf: false
          isPro: false
          name: Suparious
          type: user
        html: '<p>I was able to use this after installing the GPTQ-for-LLaMa, by following
          the instructions from the oobabooga textgeneration-iu github README: <a
          rel="nofollow" href="https://github.com/oobabooga/text-generation-webui/wiki/GPTQ-models-(4-bit-mode)">https://github.com/oobabooga/text-generation-webui/wiki/GPTQ-models-(4-bit-mode)</a></p>

          '
        raw: 'I was able to use this after installing the GPTQ-for-LLaMa, by following
          the instructions from the oobabooga textgeneration-iu github README: https://github.com/oobabooga/text-generation-webui/wiki/GPTQ-models-(4-bit-mode)'
        updatedAt: '2023-04-21T07:24:20.576Z'
      numEdits: 0
      reactions: []
    id: 64423a24b11e37f26da2ccb4
    type: comment
  author: Suparious
  content: 'I was able to use this after installing the GPTQ-for-LLaMa, by following
    the instructions from the oobabooga textgeneration-iu github README: https://github.com/oobabooga/text-generation-webui/wiki/GPTQ-models-(4-bit-mode)'
  created_at: 2023-04-21 06:24:20+00:00
  edited: false
  hidden: false
  id: 64423a24b11e37f26da2ccb4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: tsumeone/vicuna-13B-1.1-GPTQ-4bit-128g-cuda
repo_type: model
status: open
target_branch: null
title: How to load safetensors model?
