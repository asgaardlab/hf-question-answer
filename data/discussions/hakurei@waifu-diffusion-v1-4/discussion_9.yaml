!!python/object:huggingface_hub.community.DiscussionWithDetails
author: torridgristle
conflicting_files: null
created_at: 2022-11-22 15:21:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6e10cc454bfe3e980ba55c3187354d6.svg
      fullname: Jacob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: torridgristle
      type: user
    createdAt: '2022-11-22T15:21:19.000Z'
    data:
      edited: false
      editors:
      - torridgristle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6e10cc454bfe3e980ba55c3187354d6.svg
          fullname: Jacob
          isHf: false
          isPro: false
          name: torridgristle
          type: user
        html: '<p>tl;dr create attention masks from the commas to isolate each tag,
          blend concepts only in the image / Stable Diffusion and not in CLIP. </p>

          <p>I strongly believe this would help disentangle features / tags.</p>

          <p>The default attention masking for text in CLIP is to not allow tokens
          to be impacted by following tokens, only previous tokens.</p>

          <p>However, the prompt style of comma separated tags seems ill-suited for
          this since each tag is an isolated concept on its own, separated by commas.
          This results in different tags and commas influencing later tags, meaning
          the order of tags changes the result. </p>

          <p>Skipping a few layers at the end may reduce how much the tokens / words
          are mixed together, but it doesn''t change how they''re mixed together where
          each token is affected by every previous token.</p>

          <p>I propose a new attention masking method to replace the default, where
          individual groups of tokens separated by commas are masked to only allow
          them to modify their own group. Then Stable Diffusion can blend the concepts
          together in the image, rather than by CLIP in the encoded prompt.</p>

          <p>Also to remove commas from the tokens that are actually used when encoding
          the prompt, keeping them in prompts only to form the attention masks.</p>

          <p>It should restart the usual pattern of blocking out previous tokens on
          each comma-separated tag, not just allowing all tokens to modify each other
          regardless of order or arrangement, in order to preserve the interpretation
          of words that are split into multiple tokens.</p>

          <p>Apologies if rambling, the idea has been in my head for a while.</p>

          '
        raw: "tl;dr create attention masks from the commas to isolate each tag, blend\
          \ concepts only in the image / Stable Diffusion and not in CLIP. \r\n\r\n\
          I strongly believe this would help disentangle features / tags.\r\n\r\n\
          The default attention masking for text in CLIP is to not allow tokens to\
          \ be impacted by following tokens, only previous tokens.\r\n\r\nHowever,\
          \ the prompt style of comma separated tags seems ill-suited for this since\
          \ each tag is an isolated concept on its own, separated by commas. This\
          \ results in different tags and commas influencing later tags, meaning the\
          \ order of tags changes the result. \r\n\r\nSkipping a few layers at the\
          \ end may reduce how much the tokens / words are mixed together, but it\
          \ doesn't change how they're mixed together where each token is affected\
          \ by every previous token.\r\n\r\nI propose a new attention masking method\
          \ to replace the default, where individual groups of tokens separated by\
          \ commas are masked to only allow them to modify their own group. Then Stable\
          \ Diffusion can blend the concepts together in the image, rather than by\
          \ CLIP in the encoded prompt.\r\n\r\nAlso to remove commas from the tokens\
          \ that are actually used when encoding the prompt, keeping them in prompts\
          \ only to form the attention masks.\r\n\r\nIt should restart the usual pattern\
          \ of blocking out previous tokens on each comma-separated tag, not just\
          \ allowing all tokens to modify each other regardless of order or arrangement,\
          \ in order to preserve the interpretation of words that are split into multiple\
          \ tokens.\r\n\r\nApologies if rambling, the idea has been in my head for\
          \ a while."
        updatedAt: '2022-11-22T15:21:19.358Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - BadIdea102
    id: 637ce8ef9a5217b88b736111
    type: comment
  author: torridgristle
  content: "tl;dr create attention masks from the commas to isolate each tag, blend\
    \ concepts only in the image / Stable Diffusion and not in CLIP. \r\n\r\nI strongly\
    \ believe this would help disentangle features / tags.\r\n\r\nThe default attention\
    \ masking for text in CLIP is to not allow tokens to be impacted by following\
    \ tokens, only previous tokens.\r\n\r\nHowever, the prompt style of comma separated\
    \ tags seems ill-suited for this since each tag is an isolated concept on its\
    \ own, separated by commas. This results in different tags and commas influencing\
    \ later tags, meaning the order of tags changes the result. \r\n\r\nSkipping a\
    \ few layers at the end may reduce how much the tokens / words are mixed together,\
    \ but it doesn't change how they're mixed together where each token is affected\
    \ by every previous token.\r\n\r\nI propose a new attention masking method to\
    \ replace the default, where individual groups of tokens separated by commas are\
    \ masked to only allow them to modify their own group. Then Stable Diffusion can\
    \ blend the concepts together in the image, rather than by CLIP in the encoded\
    \ prompt.\r\n\r\nAlso to remove commas from the tokens that are actually used\
    \ when encoding the prompt, keeping them in prompts only to form the attention\
    \ masks.\r\n\r\nIt should restart the usual pattern of blocking out previous tokens\
    \ on each comma-separated tag, not just allowing all tokens to modify each other\
    \ regardless of order or arrangement, in order to preserve the interpretation\
    \ of words that are split into multiple tokens.\r\n\r\nApologies if rambling,\
    \ the idea has been in my head for a while."
  created_at: 2022-11-22 15:21:19+00:00
  edited: false
  hidden: false
  id: 637ce8ef9a5217b88b736111
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f6e10cc454bfe3e980ba55c3187354d6.svg
      fullname: Jacob
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: torridgristle
      type: user
    createdAt: '2022-11-22T19:03:13.000Z'
    data:
      edited: false
      editors:
      - torridgristle
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f6e10cc454bfe3e980ba55c3187354d6.svg
          fullname: Jacob
          isHf: false
          isPro: false
          name: torridgristle
          type: user
        html: '<p>Or instead of removing the commas, replace them with  tokens and
          then remove them at the end. This way it isn''t only the first tag that
          gets modified by the start of text token. It definitely needs to keep that
          first one though in the final output.</p>

          <p>Or adjust the attention masks to ignore the start of text token so it
          doesn''t modify anything but I''m not sure how well that would work.</p>

          <p>The  tokens padding the end can also carry on the values of the previous
          tokens, and they''re not necessary for Stable Diffusion to generate a sensible
          image. They''re good for the unconditional encoding though.</p>

          '
        raw: 'Or instead of removing the commas, replace them with <start of text>
          tokens and then remove them at the end. This way it isn''t only the first
          tag that gets modified by the start of text token. It definitely needs to
          keep that first one though in the final output.


          Or adjust the attention masks to ignore the start of text token so it doesn''t
          modify anything but I''m not sure how well that would work.


          The <end of text> tokens padding the end can also carry on the values of
          the previous tokens, and they''re not necessary for Stable Diffusion to
          generate a sensible image. They''re good for the unconditional encoding
          though.'
        updatedAt: '2022-11-22T19:03:13.458Z'
      numEdits: 0
      reactions: []
    id: 637d1cf1bb031d2afee60893
    type: comment
  author: torridgristle
  content: 'Or instead of removing the commas, replace them with <start of text> tokens
    and then remove them at the end. This way it isn''t only the first tag that gets
    modified by the start of text token. It definitely needs to keep that first one
    though in the final output.


    Or adjust the attention masks to ignore the start of text token so it doesn''t
    modify anything but I''m not sure how well that would work.


    The <end of text> tokens padding the end can also carry on the values of the previous
    tokens, and they''re not necessary for Stable Diffusion to generate a sensible
    image. They''re good for the unconditional encoding though.'
  created_at: 2022-11-22 19:03:13+00:00
  edited: false
  hidden: false
  id: 637d1cf1bb031d2afee60893
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 9
repo_id: hakurei/waifu-diffusion-v1-4
repo_type: model
status: open
target_branch: null
title: Proposed change to CLIP text encoding for tag style
