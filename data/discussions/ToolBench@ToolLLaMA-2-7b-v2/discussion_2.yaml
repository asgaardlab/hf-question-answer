!!python/object:huggingface_hub.community.DiscussionWithDetails
author: fortiag
conflicting_files: null
created_at: 2024-01-02 14:59:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/169a236a062a8e86626f5f3b175fecd7.svg
      fullname: Gabriel Fortia
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: fortiag
      type: user
    createdAt: '2024-01-02T14:59:50.000Z'
    data:
      edited: false
      editors:
      - fortiag
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4515018165111542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/169a236a062a8e86626f5f3b175fecd7.svg
          fullname: Gabriel Fortia
          isHf: false
          isPro: false
          name: fortiag
          type: user
        html: "<p>I have been trying to do some tests using the following code:</p>\n\
          <pre><code>tokenizer = AutoTokenizer.from_pretrained(model_hf_id, cache_dir=cache_path)\n\
          model = AutoModelForCausalLM.from_pretrained(model_hf_id, torch_dtype=torch.float16,\
          \ cache_dir=cache_path)\ntest = pipeline(model=model, tokenizer=tokenizer)\n\
          example = \"Hello, how are you?\"\nprint(\"QUESTION: \"+example)\nresult\
          \ = test(example)\n</code></pre>\n<p>Unfortunately, when I run the code\
          \ I get the following error (which seems not to be my fault):</p>\n<pre><code>Traceback\
          \ (most recent call last):\n  File \"/home/eve/Documents/llm-agent-poc/scripts/download_model_hf.py\"\
          , line 24, in &lt;module&gt;\n    test = pipeline(model=model, tokenizer=tokenizer)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 801, in pipeline\n    raise RuntimeError(\nRuntimeError: Inferring\
          \ the task automatically requires to check the hub with a model_id defined\
          \ as a `str`. LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens):\
          \ Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x\
          \ LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n       \
          \   (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          \          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          \          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          \          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n\
          \          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp):\
          \ LlamaMLP(\n          (gate_proj): Linear(in_features=4096, out_features=11008,\
          \ bias=False)\n          (up_proj): Linear(in_features=4096, out_features=11008,\
          \ bias=False)\n          (down_proj): Linear(in_features=11008, out_features=4096,\
          \ bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm):\
          \ LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n \
          \     )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096,\
          \ out_features=32000, bias=False)\n) is not a valid model_id.\n</code></pre>\n\
          <p>I Googled the error and found a thread of someone reporting a similar\
          \ problem with another model. The way to solve the error, they said, was\
          \ to add a parameter to the pipeline:</p>\n<pre><code>test = pipeline(task='text-generation',model=model,\
          \ tokenizer=tokenizer)\n</code></pre>\n<p>In my case it didn't solve the\
          \ error, but generated another one:</p>\n<pre><code>Traceback (most recent\
          \ call last):\n  File \"/home/eve/Documents/llm-agent-poc/scripts/download_model_hf.py\"\
          , line 28, in &lt;module&gt;\n    result = test(example)\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 208, in __call__\n    return super().__call__(text_inputs, **kwargs)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1140, in __call__\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1147, in run_single\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1046, in forward\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 271, in _forward\n    generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\n    return func(*args, **kwargs)\n  File\
          \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1718, in generate\n    return self.greedy_search(\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2579, in greedy_search\n    outputs = self(\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 1181, in forward\n    outputs = self.model(\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 1068, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 796, in forward\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 691, in forward\n    query_states = self.q_proj(hidden_states)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n \
          \ File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\"\
          , line 114, in forward\n    return F.linear(input, self.weight, self.bias)\n\
          RuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\n</code></pre>\n"
        raw: "I have been trying to do some tests using the following code:\r\n```\r\
          \ntokenizer = AutoTokenizer.from_pretrained(model_hf_id, cache_dir=cache_path)\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_hf_id, torch_dtype=torch.float16,\
          \ cache_dir=cache_path)\r\ntest = pipeline(model=model, tokenizer=tokenizer)\r\
          \nexample = \"Hello, how are you?\"\r\nprint(\"QUESTION: \"+example)\r\n\
          result = test(example)\r\n```\r\nUnfortunately, when I run the code I get\
          \ the following error (which seems not to be my fault):\r\n```\r\nTraceback\
          \ (most recent call last):\r\n  File \"/home/eve/Documents/llm-agent-poc/scripts/download_model_hf.py\"\
          , line 24, in <module>\r\n    test = pipeline(model=model, tokenizer=tokenizer)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
          , line 801, in pipeline\r\n    raise RuntimeError(\r\nRuntimeError: Inferring\
          \ the task automatically requires to check the hub with a model_id defined\
          \ as a `str`. LlamaForCausalLM(\r\n  (model): LlamaModel(\r\n    (embed_tokens):\
          \ Embedding(32000, 4096)\r\n    (layers): ModuleList(\r\n      (0-31): 32\
          \ x LlamaDecoderLayer(\r\n        (self_attn): LlamaSdpaAttention(\r\n \
          \         (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\
          \n          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\
          \n          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\
          \n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\r\
          \n          (rotary_emb): LlamaRotaryEmbedding()\r\n        )\r\n      \
          \  (mlp): LlamaMLP(\r\n          (gate_proj): Linear(in_features=4096, out_features=11008,\
          \ bias=False)\r\n          (up_proj): Linear(in_features=4096, out_features=11008,\
          \ bias=False)\r\n          (down_proj): Linear(in_features=11008, out_features=4096,\
          \ bias=False)\r\n          (act_fn): SiLU()\r\n        )\r\n        (input_layernorm):\
          \ LlamaRMSNorm()\r\n        (post_attention_layernorm): LlamaRMSNorm()\r\
          \n      )\r\n    )\r\n    (norm): LlamaRMSNorm()\r\n  )\r\n  (lm_head):\
          \ Linear(in_features=4096, out_features=32000, bias=False)\r\n) is not a\
          \ valid model_id.\r\n```\r\nI Googled the error and found a thread of someone\
          \ reporting a similar problem with another model. The way to solve the error,\
          \ they said, was to add a parameter to the pipeline:\r\n```\r\ntest = pipeline(task='text-generation',model=model,\
          \ tokenizer=tokenizer)\r\n```\r\n\r\nIn my case it didn't solve the error,\
          \ but generated another one:\r\n```\r\nTraceback (most recent call last):\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/scripts/download_model_hf.py\"\
          , line 28, in <module>\r\n    result = test(example)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 208, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1140, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
          \ forward_params, postprocess_params)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1147, in run_single\r\n    model_outputs = self.forward(model_inputs,\
          \ **forward_params)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
          , line 1046, in forward\r\n    model_outputs = self._forward(model_inputs,\
          \ **forward_params)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
          , line 271, in _forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids,\
          \ attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
          , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n\
          \  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 1718, in generate\r\n    return self.greedy_search(\r\n  File \"\
          /home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\"\
          , line 2579, in greedy_search\r\n    outputs = self(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 1181, in forward\r\n    outputs = self.model(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 1068, in forward\r\n    layer_outputs = decoder_layer(\r\n  File\
          \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 796, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
          \ = self.self_attn(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
          , line 691, in forward\r\n    query_states = self.q_proj(hidden_states)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args,\
          \ **kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
          , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\
          \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\"\
          , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
          \nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\r\n```\r\n"
        updatedAt: '2024-01-02T14:59:50.618Z'
      numEdits: 0
      reactions: []
    id: 659424e6de82e1ef7bb69195
    type: comment
  author: fortiag
  content: "I have been trying to do some tests using the following code:\r\n```\r\
    \ntokenizer = AutoTokenizer.from_pretrained(model_hf_id, cache_dir=cache_path)\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_hf_id, torch_dtype=torch.float16,\
    \ cache_dir=cache_path)\r\ntest = pipeline(model=model, tokenizer=tokenizer)\r\
    \nexample = \"Hello, how are you?\"\r\nprint(\"QUESTION: \"+example)\r\nresult\
    \ = test(example)\r\n```\r\nUnfortunately, when I run the code I get the following\
    \ error (which seems not to be my fault):\r\n```\r\nTraceback (most recent call\
    \ last):\r\n  File \"/home/eve/Documents/llm-agent-poc/scripts/download_model_hf.py\"\
    , line 24, in <module>\r\n    test = pipeline(model=model, tokenizer=tokenizer)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py\"\
    , line 801, in pipeline\r\n    raise RuntimeError(\r\nRuntimeError: Inferring\
    \ the task automatically requires to check the hub with a model_id defined as\
    \ a `str`. LlamaForCausalLM(\r\n  (model): LlamaModel(\r\n    (embed_tokens):\
    \ Embedding(32000, 4096)\r\n    (layers): ModuleList(\r\n      (0-31): 32 x LlamaDecoderLayer(\r\
    \n        (self_attn): LlamaSdpaAttention(\r\n          (q_proj): Linear(in_features=4096,\
    \ out_features=4096, bias=False)\r\n          (k_proj): Linear(in_features=4096,\
    \ out_features=4096, bias=False)\r\n          (v_proj): Linear(in_features=4096,\
    \ out_features=4096, bias=False)\r\n          (o_proj): Linear(in_features=4096,\
    \ out_features=4096, bias=False)\r\n          (rotary_emb): LlamaRotaryEmbedding()\r\
    \n        )\r\n        (mlp): LlamaMLP(\r\n          (gate_proj): Linear(in_features=4096,\
    \ out_features=11008, bias=False)\r\n          (up_proj): Linear(in_features=4096,\
    \ out_features=11008, bias=False)\r\n          (down_proj): Linear(in_features=11008,\
    \ out_features=4096, bias=False)\r\n          (act_fn): SiLU()\r\n        )\r\n\
    \        (input_layernorm): LlamaRMSNorm()\r\n        (post_attention_layernorm):\
    \ LlamaRMSNorm()\r\n      )\r\n    )\r\n    (norm): LlamaRMSNorm()\r\n  )\r\n\
    \  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\r\n) is\
    \ not a valid model_id.\r\n```\r\nI Googled the error and found a thread of someone\
    \ reporting a similar problem with another model. The way to solve the error,\
    \ they said, was to add a parameter to the pipeline:\r\n```\r\ntest = pipeline(task='text-generation',model=model,\
    \ tokenizer=tokenizer)\r\n```\r\n\r\nIn my case it didn't solve the error, but\
    \ generated another one:\r\n```\r\nTraceback (most recent call last):\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/scripts/download_model_hf.py\", line 28,\
    \ in <module>\r\n    result = test(example)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
    , line 208, in __call__\r\n    return super().__call__(text_inputs, **kwargs)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1140, in __call__\r\n    return self.run_single(inputs, preprocess_params,\
    \ forward_params, postprocess_params)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1147, in run_single\r\n    model_outputs = self.forward(model_inputs, **forward_params)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py\"\
    , line 1046, in forward\r\n    model_outputs = self._forward(model_inputs, **forward_params)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\"\
    , line 271, in _forward\r\n    generated_sequence = self.model.generate(input_ids=input_ids,\
    \ attention_mask=attention_mask, **generate_kwargs)\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\"\
    , line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 1718, in generate\r\n    return self.greedy_search(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/generation/utils.py\"\
    , line 2579, in greedy_search\r\n    outputs = self(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 1181, in forward\r\n    outputs = self.model(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 1068, in forward\r\n    layer_outputs = decoder_layer(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 796, in forward\r\n    hidden_states, self_attn_weights, present_key_value\
    \ = self.self_attn(\r\n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\"\
    , line 691, in forward\r\n    query_states = self.q_proj(hidden_states)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1518, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\
    \n  File \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\"\
    , line 1527, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File\
    \ \"/home/eve/Documents/llm-agent-poc/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py\"\
    , line 114, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\
    \nRuntimeError: \"addmm_impl_cpu_\" not implemented for 'Half'\r\n```\r\n"
  created_at: 2024-01-02 14:59:50+00:00
  edited: false
  hidden: false
  id: 659424e6de82e1ef7bb69195
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: ToolBench/ToolLLaMA-2-7b-v2
repo_type: model
status: open
target_branch: null
title: Error when running inference
