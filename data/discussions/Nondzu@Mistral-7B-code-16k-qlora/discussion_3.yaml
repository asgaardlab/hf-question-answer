!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Moemamoe
conflicting_files: null
created_at: 2023-10-19 10:02:04+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/152c17985e32df166759081ada410d2a.svg
      fullname: Moema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moemamoe
      type: user
    createdAt: '2023-10-19T11:02:04.000Z'
    data:
      edited: false
      editors:
      - Moemamoe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9609695076942444
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/152c17985e32df166759081ada410d2a.svg
          fullname: Moema
          isHf: false
          isPro: false
          name: Moemamoe
          type: user
        html: '<p>Hey,</p>

          <p>I''d love to help testing this code model. I am an expert in writing
          code, but not in running/developing AI models.<br>How can I start testing
          it meaningfully?</p>

          '
        raw: "Hey,\r\n\r\nI'd love to help testing this code model. I am an expert\
          \ in writing code, but not in running/developing AI models.\r\nHow can I\
          \ start testing it meaningfully?"
        updatedAt: '2023-10-19T11:02:04.184Z'
      numEdits: 0
      reactions: []
    id: 65310cac8d69724f926a8c6f
    type: comment
  author: Moemamoe
  content: "Hey,\r\n\r\nI'd love to help testing this code model. I am an expert in\
    \ writing code, but not in running/developing AI models.\r\nHow can I start testing\
    \ it meaningfully?"
  created_at: 2023-10-19 10:02:04+00:00
  edited: false
  hidden: false
  id: 65310cac8d69724f926a8c6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9229d1ce9500f9b1a1ff1c4f6856ac10.svg
      fullname: L
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: TaidanaHito
      type: user
    createdAt: '2023-10-30T11:42:52.000Z'
    data:
      edited: true
      editors:
      - TaidanaHito
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9043865203857422
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9229d1ce9500f9b1a1ff1c4f6856ac10.svg
          fullname: L
          isHf: false
          isPro: false
          name: TaidanaHito
          type: user
        html: "<p>So.. Under the listed quantizations, you can always try the GGUF.\
          \ You'll need llama.cpp and if you want particularly good speeds, a GPU\
          \ with at least ~5966MB RAM for the full 16k context. (8k context can run\
          \ with 1GB less RAM)<br>If you don't have a GPU supported but you do have\
          \ enough VRAM, I can give you my slightly edited copy of nomic-ai's vulkan\
          \ fork which builds the server with the vulkan support. (it's like.. a few\
          \ lines at most changed/added.)</p>\n<p>Ok, so for my solution at the moment:<br>You\
          \ need to build llama.cpp from source and then launch the server executable.</p>\n\
          <p>For example, with nomic-ai's vulkan fork, it was:<br><code>mkdir build\
          \ &amp;&amp; cd build</code><br><code>cmake .. -DLLAMA_KOMPUTE=ON</code><br>(For\
          \ CUDA, the preceding is, if I recall: <code>cmake .. -DLLAMA_CUBLAS=ON</code>\
          \ for OpenCL with CLBLAST it's <code>cmake .. -DLLAMA_CLBLAST=ON</code>\
          \ )<br><code>cmake --build . --config Release</code><br>The binaries are\
          \ in the bin folder, then. (This is probably obvious, sorry. Just being\
          \ thorough.)<br><code>./bin/server -ngl 32 --host 127.0.0.1 --port 8080\
          \ --path \"./examples/server/public\" -c 16384 -m {path-to-gguf-model}</code></p>\n\
          <p>From there, you can use a plugin in vscode like.. Well, this is the only\
          \ one I found that works with llama.cpp's server implementation: <a rel=\"\
          nofollow\" href=\"https://marketplace.visualstudio.com/items?itemName=ppipada.flexigpt\"\
          >https://marketplace.visualstudio.com/items?itemName=ppipada.flexigpt</a><br>You\
          \ need to edit the plugin settings, set \"Flexigpt: Default Provider\" to\
          \ \"llamacpp\" and point \"Flexigpt \u203A Llamacpp: Default Origin\" to\
          \ \"<a rel=\"nofollow\" href=\"http://127.0.0.1:8080&quot;\">http://127.0.0.1:8080\"\
          </a> then, for good measure, maybe restart vscode. This is my current setup\
          \ for demoing the model at least.<br>I warn it has quirks, like almost always\
          \ putting responses from the model that don't include any markdown in a\
          \ box for markdown text when using llama.cpp for some reason.<br>Do keep\
          \ in mind it won't have context awareness like Codeium, Cody, or Copilot.\
          \ It has a few commands that will take the highlighted code into account,\
          \ but not all do and your messages don't, either. </p>\n<p>You could also,\
          \ of course, always use llama.cpp's main executable for more flexibility\
          \ (though without the convenience of a vscode plugin,) with <code>./main\
          \ -ins -c 16384 --color -ngl 32 -m {path-to-gguf-model}</code> or edit the\
          \ plugin's source code to change the parameters it sends when calling to\
          \ the llama.cpp server over HTTP.</p>\n<p>You can also try another ggml-compatible\
          \ application/library like Koboldcpp, gpt4all (though if I recall that one\
          \ only works with q4_0, Q6_0 and q8_0,) ctransformers, or Oobabooga to name\
          \ a few.<br>Other options exist, though since my RX 5700XT doesn't seem\
          \ to work with ROCM, OpenCL or CUDA-based options (though Vulkan compute\
          \ works excellently) I don't really know about those since CPU inference\
          \ on my Ryzen 7 1800x back when I was getting started was painful outside\
          \ llama.cpp so I kinda got really accustomed to llama.cpp and related applications.</p>\n\
          <p>I don't know if this is meaningful like you asked for, but if you meant\
          \ something else in particular then do let me know! I've been enjoying the\
          \ model so far, with what little I've experienced of it. </p>\n<p>Hope this\
          \ helps! Sorry if it's a bit long-winded or doesn't.</p>\n<p>EDIT: I just\
          \ saw that this was asked 11 days ago.. Sorry if this is irrelevant, now.\
          \ Please let me know how it goes, whether that's it working out, or you\
          \ needing more help. I'll try and check back in intermittently for a while.<br>Another\
          \ edit: I forgot to mention that these instructions assume you're running\
          \ on Linux and have the dependencies installed. If you encounter issues\
          \ figuring that out, please let me know. I could assist further at request.</p>\n"
        raw: "So.. Under the listed quantizations, you can always try the GGUF. You'll\
          \ need llama.cpp and if you want particularly good speeds, a GPU with at\
          \ least ~5966MB RAM for the full 16k context. (8k context can run with 1GB\
          \ less RAM)\nIf you don't have a GPU supported but you do have enough VRAM,\
          \ I can give you my slightly edited copy of nomic-ai's vulkan fork which\
          \ builds the server with the vulkan support. (it's like.. a few lines at\
          \ most changed/added.)\n\nOk, so for my solution at the moment:\nYou need\
          \ to build llama.cpp from source and then launch the server executable.\n\
          \nFor example, with nomic-ai's vulkan fork, it was:\n`mkdir build && cd\
          \ build`\n`cmake .. -DLLAMA_KOMPUTE=ON` \n(For CUDA, the preceding is, if\
          \ I recall: `cmake .. -DLLAMA_CUBLAS=ON` for OpenCL with CLBLAST it's `cmake\
          \ .. -DLLAMA_CLBLAST=ON` )\n`cmake --build . --config Release`\nThe binaries\
          \ are in the bin folder, then. (This is probably obvious, sorry. Just being\
          \ thorough.)\n`./bin/server -ngl 32 --host 127.0.0.1 --port 8080 --path\
          \ \"./examples/server/public\" -c 16384 -m {path-to-gguf-model}`\n\nFrom\
          \ there, you can use a plugin in vscode like.. Well, this is the only one\
          \ I found that works with llama.cpp's server implementation: https://marketplace.visualstudio.com/items?itemName=ppipada.flexigpt\n\
          You need to edit the plugin settings, set \"Flexigpt: Default Provider\"\
          \ to \"llamacpp\" and point \"Flexigpt \u203A Llamacpp: Default Origin\"\
          \ to \"http://127.0.0.1:8080\" then, for good measure, maybe restart vscode.\
          \ This is my current setup for demoing the model at least.\nI warn it has\
          \ quirks, like almost always putting responses from the model that don't\
          \ include any markdown in a box for markdown text when using llama.cpp for\
          \ some reason.\nDo keep in mind it won't have context awareness like Codeium,\
          \ Cody, or Copilot. It has a few commands that will take the highlighted\
          \ code into account, but not all do and your messages don't, either. \n\n\
          \n\nYou could also, of course, always use llama.cpp's main executable for\
          \ more flexibility (though without the convenience of a vscode plugin,)\
          \ with `./main -ins -c 16384 --color -ngl 32 -m {path-to-gguf-model}` or\
          \ edit the plugin's source code to change the parameters it sends when calling\
          \ to the llama.cpp server over HTTP.\n\nYou can also try another ggml-compatible\
          \ application/library like Koboldcpp, gpt4all (though if I recall that one\
          \ only works with q4_0, Q6_0 and q8_0,) ctransformers, or Oobabooga to name\
          \ a few.\nOther options exist, though since my RX 5700XT doesn't seem to\
          \ work with ROCM, OpenCL or CUDA-based options (though Vulkan compute works\
          \ excellently) I don't really know about those since CPU inference on my\
          \ Ryzen 7 1800x back when I was getting started was painful outside llama.cpp\
          \ so I kinda got really accustomed to llama.cpp and related applications.\n\
          \nI don't know if this is meaningful like you asked for, but if you meant\
          \ something else in particular then do let me know! I've been enjoying the\
          \ model so far, with what little I've experienced of it. \n\nHope this helps!\
          \ Sorry if it's a bit long-winded or doesn't.\n\nEDIT: I just saw that this\
          \ was asked 11 days ago.. Sorry if this is irrelevant, now. Please let me\
          \ know how it goes, whether that's it working out, or you needing more help.\
          \ I'll try and check back in intermittently for a while.\nAnother edit:\
          \ I forgot to mention that these instructions assume you're running on Linux\
          \ and have the dependencies installed. If you encounter issues figuring\
          \ that out, please let me know. I could assist further at request."
        updatedAt: '2023-11-06T19:43:10.325Z'
      numEdits: 3
      reactions: []
    id: 653f96bcf2dbf238a9aa979c
    type: comment
  author: TaidanaHito
  content: "So.. Under the listed quantizations, you can always try the GGUF. You'll\
    \ need llama.cpp and if you want particularly good speeds, a GPU with at least\
    \ ~5966MB RAM for the full 16k context. (8k context can run with 1GB less RAM)\n\
    If you don't have a GPU supported but you do have enough VRAM, I can give you\
    \ my slightly edited copy of nomic-ai's vulkan fork which builds the server with\
    \ the vulkan support. (it's like.. a few lines at most changed/added.)\n\nOk,\
    \ so for my solution at the moment:\nYou need to build llama.cpp from source and\
    \ then launch the server executable.\n\nFor example, with nomic-ai's vulkan fork,\
    \ it was:\n`mkdir build && cd build`\n`cmake .. -DLLAMA_KOMPUTE=ON` \n(For CUDA,\
    \ the preceding is, if I recall: `cmake .. -DLLAMA_CUBLAS=ON` for OpenCL with\
    \ CLBLAST it's `cmake .. -DLLAMA_CLBLAST=ON` )\n`cmake --build . --config Release`\n\
    The binaries are in the bin folder, then. (This is probably obvious, sorry. Just\
    \ being thorough.)\n`./bin/server -ngl 32 --host 127.0.0.1 --port 8080 --path\
    \ \"./examples/server/public\" -c 16384 -m {path-to-gguf-model}`\n\nFrom there,\
    \ you can use a plugin in vscode like.. Well, this is the only one I found that\
    \ works with llama.cpp's server implementation: https://marketplace.visualstudio.com/items?itemName=ppipada.flexigpt\n\
    You need to edit the plugin settings, set \"Flexigpt: Default Provider\" to \"\
    llamacpp\" and point \"Flexigpt \u203A Llamacpp: Default Origin\" to \"http://127.0.0.1:8080\"\
    \ then, for good measure, maybe restart vscode. This is my current setup for demoing\
    \ the model at least.\nI warn it has quirks, like almost always putting responses\
    \ from the model that don't include any markdown in a box for markdown text when\
    \ using llama.cpp for some reason.\nDo keep in mind it won't have context awareness\
    \ like Codeium, Cody, or Copilot. It has a few commands that will take the highlighted\
    \ code into account, but not all do and your messages don't, either. \n\n\n\n\
    You could also, of course, always use llama.cpp's main executable for more flexibility\
    \ (though without the convenience of a vscode plugin,) with `./main -ins -c 16384\
    \ --color -ngl 32 -m {path-to-gguf-model}` or edit the plugin's source code to\
    \ change the parameters it sends when calling to the llama.cpp server over HTTP.\n\
    \nYou can also try another ggml-compatible application/library like Koboldcpp,\
    \ gpt4all (though if I recall that one only works with q4_0, Q6_0 and q8_0,) ctransformers,\
    \ or Oobabooga to name a few.\nOther options exist, though since my RX 5700XT\
    \ doesn't seem to work with ROCM, OpenCL or CUDA-based options (though Vulkan\
    \ compute works excellently) I don't really know about those since CPU inference\
    \ on my Ryzen 7 1800x back when I was getting started was painful outside llama.cpp\
    \ so I kinda got really accustomed to llama.cpp and related applications.\n\n\
    I don't know if this is meaningful like you asked for, but if you meant something\
    \ else in particular then do let me know! I've been enjoying the model so far,\
    \ with what little I've experienced of it. \n\nHope this helps! Sorry if it's\
    \ a bit long-winded or doesn't.\n\nEDIT: I just saw that this was asked 11 days\
    \ ago.. Sorry if this is irrelevant, now. Please let me know how it goes, whether\
    \ that's it working out, or you needing more help. I'll try and check back in\
    \ intermittently for a while.\nAnother edit: I forgot to mention that these instructions\
    \ assume you're running on Linux and have the dependencies installed. If you encounter\
    \ issues figuring that out, please let me know. I could assist further at request."
  created_at: 2023-10-30 10:42:52+00:00
  edited: true
  hidden: false
  id: 653f96bcf2dbf238a9aa979c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/152c17985e32df166759081ada410d2a.svg
      fullname: Moema
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Moemamoe
      type: user
    createdAt: '2023-11-07T08:31:06.000Z'
    data:
      edited: true
      editors:
      - Moemamoe
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9438236951828003
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/152c17985e32df166759081ada410d2a.svg
          fullname: Moema
          isHf: false
          isPro: false
          name: Moemamoe
          type: user
        html: '<p>Thanks a lot for the detailed  explanation! Almost everything works
          like a charm so far, running the model on my RTX 3080 in a LXC container.
          I got main running and also the server including the VS Code plugin as you
          explained. So far I have not really tested the model itself, but I am gonna
          use it the next weeks for my coding and try it out. </p>

          <p>Edit: For your information, I found the VS Code Continue extension which
          works with llama.cpp: <a rel="nofollow" href="https://marketplace.visualstudio.com/items?itemName=Continue.continue">https://marketplace.visualstudio.com/items?itemName=Continue.continue</a>
          (<a rel="nofollow" href="https://continue.dev/docs/reference/Models/llamacpp">https://continue.dev/docs/reference/Models/llamacpp</a>)<br>Seems
          to work much better than the other one, at least at first glance.</p>

          '
        raw: "Thanks a lot for the detailed  explanation! Almost everything works\
          \ like a charm so far, running the model on my RTX 3080 in a LXC container.\
          \ I got main running and also the server including the VS Code plugin as\
          \ you explained. So far I have not really tested the model itself, but I\
          \ am gonna use it the next weeks for my coding and try it out. \n\nEdit:\
          \ For your information, I found the VS Code Continue extension which works\
          \ with llama.cpp: https://marketplace.visualstudio.com/items?itemName=Continue.continue\
          \ (https://continue.dev/docs/reference/Models/llamacpp)\nSeems to work much\
          \ better than the other one, at least at first glance."
        updatedAt: '2023-11-07T16:00:52.706Z'
      numEdits: 1
      reactions: []
    id: 6549f5ca137b501e31685f33
    type: comment
  author: Moemamoe
  content: "Thanks a lot for the detailed  explanation! Almost everything works like\
    \ a charm so far, running the model on my RTX 3080 in a LXC container. I got main\
    \ running and also the server including the VS Code plugin as you explained. So\
    \ far I have not really tested the model itself, but I am gonna use it the next\
    \ weeks for my coding and try it out. \n\nEdit: For your information, I found\
    \ the VS Code Continue extension which works with llama.cpp: https://marketplace.visualstudio.com/items?itemName=Continue.continue\
    \ (https://continue.dev/docs/reference/Models/llamacpp)\nSeems to work much better\
    \ than the other one, at least at first glance."
  created_at: 2023-11-07 08:31:06+00:00
  edited: true
  hidden: false
  id: 6549f5ca137b501e31685f33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63729f35acef705233c87909/2ApbWLasMZLptlSbk9emj.png?w=200&h=200&f=face
      fullname: Kamil
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Nondzu
      type: user
    createdAt: '2023-11-24T10:10:37.000Z'
    data:
      edited: false
      editors:
      - Nondzu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9018149971961975
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63729f35acef705233c87909/2ApbWLasMZLptlSbk9emj.png?w=200&h=200&f=face
          fullname: Kamil
          isHf: false
          isPro: false
          name: Nondzu
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;Moemamoe&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Moemamoe\">@<span class=\"\
          underline\">Moemamoe</span></a></span>\n\n\t</span></span> <span data-props=\"\
          {&quot;user&quot;:&quot;TaidanaHito&quot;}\" data-target=\"UserMention\"\
          \ class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"\
          ><span class=\"contents\"><a href=\"/TaidanaHito\">@<span class=\"underline\"\
          >TaidanaHito</span></a></span>\n\n\t</span></span> Thanks for your answer,\
          \ due I had hot time last weeks sorry for late response.<br><span data-props=\"\
          {&quot;user&quot;:&quot;Moemamoe&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/Moemamoe\">@<span class=\"underline\">Moemamoe</span></a></span>\n\
          \n\t</span></span> do you have some updates about vscode integration and\
          \ tests ? </p>\n<p>BR <span data-props=\"{&quot;user&quot;:&quot;Nondzu&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Nondzu\"\
          >@<span class=\"underline\">Nondzu</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: "@Moemamoe @TaidanaHito Thanks for your answer, due I had hot time last\
          \ weeks sorry for late response.\n@Moemamoe do you have some updates about\
          \ vscode integration and tests ? \n\nBR @Nondzu "
        updatedAt: '2023-11-24T10:10:37.461Z'
      numEdits: 0
      reactions: []
    id: 6560769dd192bb399545ef72
    type: comment
  author: Nondzu
  content: "@Moemamoe @TaidanaHito Thanks for your answer, due I had hot time last\
    \ weeks sorry for late response.\n@Moemamoe do you have some updates about vscode\
    \ integration and tests ? \n\nBR @Nondzu "
  created_at: 2023-11-24 10:10:37+00:00
  edited: false
  hidden: false
  id: 6560769dd192bb399545ef72
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Nondzu/Mistral-7B-code-16k-qlora
repo_type: model
status: open
target_branch: null
title: Help testing
