!!python/object:huggingface_hub.community.DiscussionWithDetails
author: SamuelEucker
conflicting_files: null
created_at: 2022-05-31 10:30:52+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aa6a07cfaa0c11781452cd7690c5254.svg
      fullname: Samuel Eucker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelEucker
      type: user
    createdAt: '2022-05-31T11:30:52.000Z'
    data:
      edited: false
      editors:
      - SamuelEucker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aa6a07cfaa0c11781452cd7690c5254.svg
          fullname: Samuel Eucker
          isHf: false
          isPro: false
          name: SamuelEucker
          type: user
        html: '<p>Hi,</p>

          <p>I want to generate document embeddings from the opt models. Also I want
          to make sure that they are always of the same length (for a corpus at least).
          How can I achieve this?<br>Thanks!</p>

          '
        raw: "Hi,\r\n\r\nI want to generate document embeddings from the opt models.\
          \ Also I want to make sure that they are always of the same length (for\
          \ a corpus at least). How can I achieve this?\r\nThanks!"
        updatedAt: '2022-05-31T11:30:52.000Z'
      numEdits: 0
      reactions: []
    id: 6295fc6cced949541d5569da
    type: comment
  author: SamuelEucker
  content: "Hi,\r\n\r\nI want to generate document embeddings from the opt models.\
    \ Also I want to make sure that they are always of the same length (for a corpus\
    \ at least). How can I achieve this?\r\nThanks!"
  created_at: 2022-05-31 10:30:52+00:00
  edited: false
  hidden: false
  id: 6295fc6cced949541d5569da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aa6a07cfaa0c11781452cd7690c5254.svg
      fullname: Samuel Eucker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelEucker
      type: user
    createdAt: '2022-05-31T12:58:13.000Z'
    data:
      edited: false
      editors:
      - SamuelEucker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aa6a07cfaa0c11781452cd7690c5254.svg
          fullname: Samuel Eucker
          isHf: false
          isPro: false
          name: SamuelEucker
          type: user
        html: '<p>Assuming that we have some tokens, I did the following:<br>        vectorized_docs
          = list()<br>        for i in range(len(tokens)):<br>            vectorized_docs.append(self.model.generate(tokens[i]))<br>This
          way I get some vectorized representation of the tokens. However, the model
          stresses, that the max_length parameter needs to be carefully chosen. Once
          I set it high enough, the model wont complain, however, the vectorized_docs
          vectors are still not always the same length (or the max_length).</p>

          <p>Any Comments are much appreciated!</p>

          '
        raw: "Assuming that we have some tokens, I did the following:\n        vectorized_docs\
          \ = list()\n        for i in range(len(tokens)):\n            vectorized_docs.append(self.model.generate(tokens[i]))\
          \ \nThis way I get some vectorized representation of the tokens. However,\
          \ the model stresses, that the max_length parameter needs to be carefully\
          \ chosen. Once I set it high enough, the model wont complain, however, the\
          \ vectorized_docs vectors are still not always the same length (or the max_length).\n\
          \nAny Comments are much appreciated!\n"
        updatedAt: '2022-05-31T12:58:13.000Z'
      numEdits: 0
      reactions: []
    id: 629610e5b5ff03c83016f2b7
    type: comment
  author: SamuelEucker
  content: "Assuming that we have some tokens, I did the following:\n        vectorized_docs\
    \ = list()\n        for i in range(len(tokens)):\n            vectorized_docs.append(self.model.generate(tokens[i]))\
    \ \nThis way I get some vectorized representation of the tokens. However, the\
    \ model stresses, that the max_length parameter needs to be carefully chosen.\
    \ Once I set it high enough, the model wont complain, however, the vectorized_docs\
    \ vectors are still not always the same length (or the max_length).\n\nAny Comments\
    \ are much appreciated!\n"
  created_at: 2022-05-31 11:58:13+00:00
  edited: false
  hidden: false
  id: 629610e5b5ff03c83016f2b7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4aa6a07cfaa0c11781452cd7690c5254.svg
      fullname: Samuel Eucker
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SamuelEucker
      type: user
    createdAt: '2022-05-31T13:44:29.000Z'
    data:
      edited: false
      editors:
      - SamuelEucker
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4aa6a07cfaa0c11781452cd7690c5254.svg
          fullname: Samuel Eucker
          isHf: false
          isPro: false
          name: SamuelEucker
          type: user
        html: '<p>Edit: I found out that model.generate generates the text + continued
          text and not the embedding. So the question remains, how do I get the embedding
          for a text of my choosing. Thanks!</p>

          '
        raw: 'Edit: I found out that model.generate generates the text + continued
          text and not the embedding. So the question remains, how do I get the embedding
          for a text of my choosing. Thanks!'
        updatedAt: '2022-05-31T13:44:29.000Z'
      numEdits: 0
      reactions: []
    id: 62961bbd4ec69e1bed0974fb
    type: comment
  author: SamuelEucker
  content: 'Edit: I found out that model.generate generates the text + continued text
    and not the embedding. So the question remains, how do I get the embedding for
    a text of my choosing. Thanks!'
  created_at: 2022-05-31 12:44:29+00:00
  edited: false
  hidden: false
  id: 62961bbd4ec69e1bed0974fb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
      fullname: Patrick von Platen
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: patrickvonplaten
      type: user
    createdAt: '2022-06-01T11:31:00.000Z'
    data:
      edited: false
      editors:
      - patrickvonplaten
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1584435275418-5dfcb1aada6d0311fd3d5448.jpeg?w=200&h=200&f=face
          fullname: Patrick von Platen
          isHf: true
          isPro: false
          name: patrickvonplaten
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;SamuelEucker&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SamuelEucker\"\
          >@<span class=\"underline\">SamuelEucker</span></a></span>\n\n\t</span></span>,\
          \ </p>\n<p>Good question! </p>\n<p>Would the following example fit your\
          \ needs?</p>\n<pre><code class=\"language-python\"><span class=\"hljs-comment\"\
          >#!/usr/bin/env python3</span>\n<span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> OPTForCausalLM,\
          \ GPT2Tokenizer\n<span class=\"hljs-keyword\">import</span> torch\n\ntokenizer\
          \ = GPT2Tokenizer.from_pretrained(<span class=\"hljs-string\">\"facebook/opt-125m\"\
          </span>)\nmodel = OPTForCausalLM.from_pretrained(<span class=\"hljs-string\"\
          >\"facebook/opt-125m\"</span>)\n\n<span class=\"hljs-comment\"># begin tokens</span>\n\
          start_tokens = torch.tensor(<span class=\"hljs-number\">2</span> * [[[<span\
          \ class=\"hljs-number\">0</span>]]])\n\n<span class=\"hljs-keyword\">for</span>\
          \ i <span class=\"hljs-keyword\">in</span> <span class=\"hljs-built_in\"\
          >range</span>(start_tokens.shape[-<span class=\"hljs-number\">1</span>]):\n\
          \    out_tokens = model.generate(start_tokens[i])\n    opt_embeddings =\
          \ model.get_input_embeddings()\n    <span class=\"hljs-comment\"># generated_embedding_vectors\
          \ has shape [len(opt_embeddings), hidden_size]</span>\n    generated_embedding_vectors\
          \ = opt_embeddings(out_tokens)[<span class=\"hljs-number\">0</span>]\n</code></pre>\n"
        raw: "Hey @SamuelEucker, \n\nGood question! \n\nWould the following example\
          \ fit your needs?\n\n```python\n#!/usr/bin/env python3\nfrom transformers\
          \ import OPTForCausalLM, GPT2Tokenizer\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"\
          facebook/opt-125m\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\"\
          )\n\n# begin tokens\nstart_tokens = torch.tensor(2 * [[[0]]])\n\nfor i in\
          \ range(start_tokens.shape[-1]):\n    out_tokens = model.generate(start_tokens[i])\n\
          \    opt_embeddings = model.get_input_embeddings()\n    # generated_embedding_vectors\
          \ has shape [len(opt_embeddings), hidden_size]\n    generated_embedding_vectors\
          \ = opt_embeddings(out_tokens)[0]\n```"
        updatedAt: '2022-06-01T11:31:00.000Z'
      numEdits: 0
      reactions: []
    id: 62974df479f193515da281c3
    type: comment
  author: patrickvonplaten
  content: "Hey @SamuelEucker, \n\nGood question! \n\nWould the following example\
    \ fit your needs?\n\n```python\n#!/usr/bin/env python3\nfrom transformers import\
    \ OPTForCausalLM, GPT2Tokenizer\nimport torch\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"\
    facebook/opt-125m\")\nmodel = OPTForCausalLM.from_pretrained(\"facebook/opt-125m\"\
    )\n\n# begin tokens\nstart_tokens = torch.tensor(2 * [[[0]]])\n\nfor i in range(start_tokens.shape[-1]):\n\
    \    out_tokens = model.generate(start_tokens[i])\n    opt_embeddings = model.get_input_embeddings()\n\
    \    # generated_embedding_vectors has shape [len(opt_embeddings), hidden_size]\n\
    \    generated_embedding_vectors = opt_embeddings(out_tokens)[0]\n```"
  created_at: 2022-06-01 10:31:00+00:00
  edited: false
  hidden: false
  id: 62974df479f193515da281c3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: facebook/opt-13b
repo_type: model
status: open
target_branch: null
title: Generate Embeddings from OPT Models
