!!python/object:huggingface_hub.community.DiscussionWithDetails
author: josephykwang
conflicting_files: null
created_at: 2024-01-08 14:35:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf46f72987b6c13b1c193bfbdb960a60.svg
      fullname: Joseph Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josephykwang
      type: user
    createdAt: '2024-01-08T14:35:23.000Z'
    data:
      edited: false
      editors:
      - josephykwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9488294720649719
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf46f72987b6c13b1c193bfbdb960a60.svg
          fullname: Joseph Wang
          isHf: false
          isPro: false
          name: josephykwang
          type: user
        html: '<p>Any writeup about</p>

          <ol>

          <li>how did you decide on these two models?</li>

          <li>what merge technique do you use?</li>

          </ol>

          '
        raw: "Any writeup about\r\n1) how did you decide on these two models?\r\n\
          2) what merge technique do you use?"
        updatedAt: '2024-01-08T14:35:23.315Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - shosseini811
        - PacmanIncarnate
    id: 659c082b8ca2c02fd9a74acf
    type: comment
  author: josephykwang
  content: "Any writeup about\r\n1) how did you decide on these two models?\r\n2)\
    \ what merge technique do you use?"
  created_at: 2024-01-08 14:35:23+00:00
  edited: false
  hidden: false
  id: 659c082b8ca2c02fd9a74acf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2024-01-08T23:22:55.000Z'
    data:
      edited: false
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9660069942474365
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: '<p>I am a loyal player of kaggle.<br>The most important thing I learned
          from Kaggle is model ensemble or stacking is all you need.<br>I believe
          this also applies to transformers.</p>

          '
        raw: 'I am a loyal player of kaggle.

          The most important thing I learned from Kaggle is model ensemble or stacking
          is all you need.

          I believe this also applies to transformers.

          '
        updatedAt: '2024-01-08T23:22:55.492Z'
      numEdits: 0
      reactions: []
    id: 659c83cf93d383899ce1d7d3
    type: comment
  author: cloudyu
  content: 'I am a loyal player of kaggle.

    The most important thing I learned from Kaggle is model ensemble or stacking is
    all you need.

    I believe this also applies to transformers.

    '
  created_at: 2024-01-08 23:22:55+00:00
  edited: false
  hidden: false
  id: 659c83cf93d383899ce1d7d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b0bee2c7fad0f2d8bafd4a8bac9c259e.svg
      fullname: Rong Zhou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rongzhou
      type: user
    createdAt: '2024-01-09T06:49:44.000Z'
    data:
      edited: false
      editors:
      - rongzhou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9601782560348511
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b0bee2c7fad0f2d8bafd4a8bac9c259e.svg
          fullname: Rong Zhou
          isHf: false
          isPro: false
          name: rongzhou
          type: user
        html: '<p>How to ensemble two LLMs? I searched the Internet, found a tool
          call LLM-Blender, did you use it?</p>

          '
        raw: How to ensemble two LLMs? I searched the Internet, found a tool call
          LLM-Blender, did you use it?
        updatedAt: '2024-01-09T06:49:44.612Z'
      numEdits: 0
      reactions: []
    id: 659cec881b89affadb7b23e6
    type: comment
  author: rongzhou
  content: How to ensemble two LLMs? I searched the Internet, found a tool call LLM-Blender,
    did you use it?
  created_at: 2024-01-09 06:49:44+00:00
  edited: false
  hidden: false
  id: 659cec881b89affadb7b23e6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2024-01-09T10:45:22.000Z'
    data:
      edited: false
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8870496153831482
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: '<p>maybe this one?</p>

          <p>Chai Research presents Blending Is All You Need</p>

          <p>Cheaper, Better Alternative to Trillion-Parameters LLM</p>

          <p>In conversational AI research, there''s a noticeable trend towards developing
          models with a larger number of parameters, exemplified by models like ChatGPT.
          While these expansive models tend to generate increasingly better chat responses,
          they demand significant computational resources and memory. This study explores
          a pertinent question: Can a combination of smaller models collaboratively
          achieve comparable or enhanced performance relative to a singular large
          model? We introduce an approach termed "blending", a straightforward yet
          effective method of integrating multiple chat AIs. Our empirical evidence
          suggests that when specific smaller models are synergistically blended,
          they can potentially outperform or match the capabilities of much larger
          counterparts. For instance, integrating just three models of moderate size
          (6B/13B paramaeters) can rival or even surpass the performance metrics of
          a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis
          is rigorously tested using A/B testing methodologies with a large user base
          on the Chai research platform over a span of thirty days. The findings underscore
          the potential of the "blending" strategy as a viable approach for enhancing
          chat AI efficacy without a corresponding surge in computational demands.</p>

          '
        raw: 'maybe this one?


          Chai Research presents Blending Is All You Need


          Cheaper, Better Alternative to Trillion-Parameters LLM


          In conversational AI research, there''s a noticeable trend towards developing
          models with a larger number of parameters, exemplified by models like ChatGPT.
          While these expansive models tend to generate increasingly better chat responses,
          they demand significant computational resources and memory. This study explores
          a pertinent question: Can a combination of smaller models collaboratively
          achieve comparable or enhanced performance relative to a singular large
          model? We introduce an approach termed "blending", a straightforward yet
          effective method of integrating multiple chat AIs. Our empirical evidence
          suggests that when specific smaller models are synergistically blended,
          they can potentially outperform or match the capabilities of much larger
          counterparts. For instance, integrating just three models of moderate size
          (6B/13B paramaeters) can rival or even surpass the performance metrics of
          a substantially larger model like ChatGPT (175B+ paramaters). This hypothesis
          is rigorously tested using A/B testing methodologies with a large user base
          on the Chai research platform over a span of thirty days. The findings underscore
          the potential of the "blending" strategy as a viable approach for enhancing
          chat AI efficacy without a corresponding surge in computational demands.'
        updatedAt: '2024-01-09T10:45:22.633Z'
      numEdits: 0
      reactions: []
    id: 659d23c286a0a9f473effa3c
    type: comment
  author: cloudyu
  content: 'maybe this one?


    Chai Research presents Blending Is All You Need


    Cheaper, Better Alternative to Trillion-Parameters LLM


    In conversational AI research, there''s a noticeable trend towards developing
    models with a larger number of parameters, exemplified by models like ChatGPT.
    While these expansive models tend to generate increasingly better chat responses,
    they demand significant computational resources and memory. This study explores
    a pertinent question: Can a combination of smaller models collaboratively achieve
    comparable or enhanced performance relative to a singular large model? We introduce
    an approach termed "blending", a straightforward yet effective method of integrating
    multiple chat AIs. Our empirical evidence suggests that when specific smaller
    models are synergistically blended, they can potentially outperform or match the
    capabilities of much larger counterparts. For instance, integrating just three
    models of moderate size (6B/13B paramaeters) can rival or even surpass the performance
    metrics of a substantially larger model like ChatGPT (175B+ paramaters). This
    hypothesis is rigorously tested using A/B testing methodologies with a large user
    base on the Chai research platform over a span of thirty days. The findings underscore
    the potential of the "blending" strategy as a viable approach for enhancing chat
    AI efficacy without a corresponding surge in computational demands.'
  created_at: 2024-01-09 10:45:22+00:00
  edited: false
  hidden: false
  id: 659d23c286a0a9f473effa3c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
      fullname: hai
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cloudyu
      type: user
    createdAt: '2024-01-09T12:00:38.000Z'
    data:
      edited: false
      editors:
      - cloudyu
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5288522243499756
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64618bc5cf638aa8f856137c/zbUqrIeHjz41P3O2b3eey.jpeg?w=200&h=200&f=face
          fullname: hai
          isHf: false
          isPro: false
          name: cloudyu
          type: user
        html: '<p><a rel="nofollow" href="https://arxiv.org/abs/2401.04088">https://arxiv.org/abs/2401.04088</a></p>

          '
        raw: https://arxiv.org/abs/2401.04088
        updatedAt: '2024-01-09T12:00:38.473Z'
      numEdits: 0
      reactions: []
    id: 659d3566a3d1908ff3b6f5fd
    type: comment
  author: cloudyu
  content: https://arxiv.org/abs/2401.04088
  created_at: 2024-01-09 12:00:38+00:00
  edited: false
  hidden: false
  id: 659d3566a3d1908ff3b6f5fd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf46f72987b6c13b1c193bfbdb960a60.svg
      fullname: Joseph Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josephykwang
      type: user
    createdAt: '2024-01-09T19:12:10.000Z'
    data:
      edited: false
      editors:
      - josephykwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8862878680229187
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf46f72987b6c13b1c193bfbdb960a60.svg
          fullname: Joseph Wang
          isHf: false
          isPro: false
          name: josephykwang
          type: user
        html: '<p><a rel="nofollow" href="https://arxiv.org/abs/2401.04088">https://arxiv.org/abs/2401.04088</a>
          is a sparse moe. in their <a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1</a>,
          there are 8 experts.</p>

          <p>in you model card,</p>

          <ol>

          <li>you are using two models</li>

          <li>it is not clear if you are simply "merging" two dense models'' outputs</li>

          </ol>

          '
        raw: 'https://arxiv.org/abs/2401.04088 is a sparse moe. in their https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1,
          there are 8 experts.


          in you model card,

          1) you are using two models

          2) it is not clear if you are simply "merging" two dense models'' outputs


          '
        updatedAt: '2024-01-09T19:12:10.815Z'
      numEdits: 0
      reactions: []
    id: 659d9a8a15506e97199ffa1c
    type: comment
  author: josephykwang
  content: 'https://arxiv.org/abs/2401.04088 is a sparse moe. in their https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1,
    there are 8 experts.


    in you model card,

    1) you are using two models

    2) it is not clear if you are simply "merging" two dense models'' outputs


    '
  created_at: 2024-01-09 19:12:10+00:00
  edited: false
  hidden: false
  id: 659d9a8a15506e97199ffa1c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/cf46f72987b6c13b1c193bfbdb960a60.svg
      fullname: Joseph Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: josephykwang
      type: user
    createdAt: '2024-01-09T19:14:35.000Z'
    data:
      edited: false
      editors:
      - josephykwang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9254831671714783
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/cf46f72987b6c13b1c193bfbdb960a60.svg
          fullname: Joseph Wang
          isHf: false
          isPro: false
          name: josephykwang
          type: user
        html: '<blockquote>

          <p>maybe this one?</p>

          <p>Chai Research presents Blending Is All You Need</p>

          <p>Cheaper, Better Alternative to Trillion-Parameters LLM</p>

          </blockquote>

          <p>see <code>This means that the different chat AIs are able to implicitly
          influence the output of the current response. As a result, the current response
          is a blending of individual chat AI strengths, as they collaborate to create
          an overall more engaging conversation.</code></p>

          <p>don''t think this is not the same as MOE approach</p>

          '
        raw: "> maybe this one?\n> \n> Chai Research presents Blending Is All You\
          \ Need\n> \n> Cheaper, Better Alternative to Trillion-Parameters LLM\n\n\
          see ```This means that the different chat AIs\nare able to implicitly influence\
          \ the output of the\ncurrent response. As a result, the current response\n\
          is a blending of individual chat AI strengths, as\nthey collaborate to create\
          \ an overall more engaging\nconversation.```\n\ndon't think this is not\
          \ the same as MOE approach"
        updatedAt: '2024-01-09T19:14:35.239Z'
      numEdits: 0
      reactions: []
    id: 659d9b1b56b112cdf2fd4ab7
    type: comment
  author: josephykwang
  content: "> maybe this one?\n> \n> Chai Research presents Blending Is All You Need\n\
    > \n> Cheaper, Better Alternative to Trillion-Parameters LLM\n\nsee ```This means\
    \ that the different chat AIs\nare able to implicitly influence the output of\
    \ the\ncurrent response. As a result, the current response\nis a blending of individual\
    \ chat AI strengths, as\nthey collaborate to create an overall more engaging\n\
    conversation.```\n\ndon't think this is not the same as MOE approach"
  created_at: 2024-01-09 19:14:35+00:00
  edited: false
  hidden: false
  id: 659d9b1b56b112cdf2fd4ab7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
      fullname: "\u5357\u6816"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Minami-su
      type: user
    createdAt: '2024-01-11T02:47:56.000Z'
    data:
      edited: false
      editors:
      - Minami-su
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5113366842269897
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/qR4GHvVyWW9KR83ItUMtr.jpeg?w=200&h=200&f=face
          fullname: "\u5357\u6816"
          isHf: false
          isPro: false
          name: Minami-su
          type: user
        html: '<p><a rel="nofollow" href="https://arxiv.org/pdf/2312.15166.pdf">https://arxiv.org/pdf/2312.15166.pdf</a>
          surely</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/_x8GFaJpXRzz4E-Vwqp_t.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/_x8GFaJpXRzz4E-Vwqp_t.png"></a></p>

          '
        raw: 'https://arxiv.org/pdf/2312.15166.pdf surely


          ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/_x8GFaJpXRzz4E-Vwqp_t.png)

          '
        updatedAt: '2024-01-11T02:47:56.215Z'
      numEdits: 0
      reactions: []
    id: 659f56dc126760c9113d6497
    type: comment
  author: Minami-su
  content: 'https://arxiv.org/pdf/2312.15166.pdf surely


    ![image.png](https://cdn-uploads.huggingface.co/production/uploads/62d7f90b102d144db4b4245b/_x8GFaJpXRzz4E-Vwqp_t.png)

    '
  created_at: 2024-01-11 02:47:56+00:00
  edited: false
  hidden: false
  id: 659f56dc126760c9113d6497
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9fbbb8eaaa7b19752b336cf228d4679e.svg
      fullname: lucasjin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lucasjin
      type: user
    createdAt: '2024-01-11T06:42:45.000Z'
    data:
      edited: false
      editors:
      - lucasjin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8668485283851624
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9fbbb8eaaa7b19752b336cf228d4679e.svg
          fullname: lucasjin
          isHf: false
          isPro: false
          name: lucasjin
          type: user
        html: '<p>Have u train the model? these chat model even have different template</p>

          '
        raw: Have u train the model? these chat model even have different template
        updatedAt: '2024-01-11T06:42:45.931Z'
      numEdits: 0
      reactions: []
    id: 659f8de5b3e864638416ed0b
    type: comment
  author: lucasjin
  content: Have u train the model? these chat model even have different template
  created_at: 2024-01-11 06:42:45+00:00
  edited: false
  hidden: false
  id: 659f8de5b3e864638416ed0b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 6
repo_id: cloudyu/Mixtral_34Bx2_MoE_60B
repo_type: model
status: open
target_branch: null
title: source code and paper?
