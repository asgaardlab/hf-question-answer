!!python/object:huggingface_hub.community.DiscussionWithDetails
author: autoevaluator
conflicting_files: []
created_at: 2022-08-02 09:41:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654180084862-6297236d64501abb8dfde40d.png?w=200&h=200&f=face
      fullname: Evaluation Bot
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: autoevaluator
      type: user
    createdAt: '2022-08-02T10:41:57.000Z'
    data:
      edited: false
      editors:
      - autoevaluator
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1654180084862-6297236d64501abb8dfde40d.png?w=200&h=200&f=face
          fullname: Evaluation Bot
          isHf: true
          isPro: false
          name: autoevaluator
          type: user
        html: "<p>Beep boop, I am a bot from Hugging Face's automatic model evaluator\
          \ \U0001F44B!<br>Your model has been evaluated on the adversarialQA config\
          \ of the <a href=\"https://huggingface.co/datasets/adversarial_qa\">adversarial_qa</a>\
          \ dataset by <span data-props=\"{&quot;user&quot;:&quot;ceyda&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/ceyda\">@<span class=\"\
          underline\">ceyda</span></a></span>\n\n\t</span></span>, using the predictions\
          \ stored <a href=\"https://huggingface.co/datasets/autoevaluate/autoeval-staging-eval-project-adversarial_qa-e34332b7-12205627\"\
          >here</a>.<br>Accept this pull request to see the results displayed on the\
          \ <a href=\"https://huggingface.co/spaces/autoevaluate/leaderboards?dataset=adversarial_qa\"\
          >Hub leaderboard</a>.<br>Evaluate your model on more datasets <a href=\"\
          https://huggingface.co/spaces/autoevaluate/model-evaluator?dataset=adversarial_qa\"\
          >here</a>.</p>\n"
        raw: "Beep boop, I am a bot from Hugging Face's automatic model evaluator\
          \ \U0001F44B!\\\nYour model has been evaluated on the adversarialQA config\
          \ of the [adversarial_qa](https://huggingface.co/datasets/adversarial_qa)\
          \ dataset by @ceyda, using the predictions stored [here](https://huggingface.co/datasets/autoevaluate/autoeval-staging-eval-project-adversarial_qa-e34332b7-12205627).\\\
          \nAccept this pull request to see the results displayed on the [Hub leaderboard](https://huggingface.co/spaces/autoevaluate/leaderboards?dataset=adversarial_qa).\\\
          \nEvaluate your model on more datasets [here](https://huggingface.co/spaces/autoevaluate/model-evaluator?dataset=adversarial_qa)."
        updatedAt: '2022-08-02T10:41:57.590Z'
      numEdits: 0
      reactions: []
    id: 62e8ff75f7e720c6b135ce34
    type: comment
  author: autoevaluator
  content: "Beep boop, I am a bot from Hugging Face's automatic model evaluator \U0001F44B\
    !\\\nYour model has been evaluated on the adversarialQA config of the [adversarial_qa](https://huggingface.co/datasets/adversarial_qa)\
    \ dataset by @ceyda, using the predictions stored [here](https://huggingface.co/datasets/autoevaluate/autoeval-staging-eval-project-adversarial_qa-e34332b7-12205627).\\\
    \nAccept this pull request to see the results displayed on the [Hub leaderboard](https://huggingface.co/spaces/autoevaluate/leaderboards?dataset=adversarial_qa).\\\
    \nEvaluate your model on more datasets [here](https://huggingface.co/spaces/autoevaluate/model-evaluator?dataset=adversarial_qa)."
  created_at: 2022-08-02 09:41:57+00:00
  edited: false
  hidden: false
  id: 62e8ff75f7e720c6b135ce34
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2022-08-02T10:41:58.000Z'
    data:
      oid: 5427efa21592613ed449415bf3773d255877bc34
      parents:
      - 350a9ad58048570793a7f0899dd988c69c641a51
      subject: Add evaluation results on the adversarialQA config of adversarial_qa
    id: 62e8ff760000000000000000
    type: commit
  author: lewtun
  created_at: 2022-08-02 09:41:58+00:00
  id: 62e8ff760000000000000000
  oid: 5427efa21592613ed449415bf3773d255877bc34
  summary: Add evaluation results on the adversarialQA config of adversarial_qa
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648853347063-617721aa4ce8f8cb2c2c7497.jpeg?w=200&h=200&f=face
      fullname: Tuana Celik
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Tuana
      type: user
    createdAt: '2022-08-16T20:21:30.000Z'
    data:
      edited: false
      editors:
      - Tuana
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648853347063-617721aa4ce8f8cb2c2c7497.jpeg?w=200&h=200&f=face
          fullname: Tuana Celik
          isHf: false
          isPro: false
          name: Tuana
          type: user
        html: '<p>Closing this PR as the model was trained on SQuAD data. We think
          it might be confusing to users to have evaluation in adversarialQA data
          on this model, which was not trained on this type of data.</p>

          '
        raw: Closing this PR as the model was trained on SQuAD data. We think it might
          be confusing to users to have evaluation in adversarialQA data on this model,
          which was not trained on this type of data.
        updatedAt: '2022-08-16T20:21:30.269Z'
      numEdits: 0
      reactions: []
      relatedEventId: 62fbfc4a72a7ab50b4b56a69
    id: 62fbfc4a72a7ab50b4b56a68
    type: comment
  author: Tuana
  content: Closing this PR as the model was trained on SQuAD data. We think it might
    be confusing to users to have evaluation in adversarialQA data on this model,
    which was not trained on this type of data.
  created_at: 2022-08-16 19:21:30+00:00
  edited: false
  hidden: false
  id: 62fbfc4a72a7ab50b4b56a68
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648853347063-617721aa4ce8f8cb2c2c7497.jpeg?w=200&h=200&f=face
      fullname: Tuana Celik
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: Tuana
      type: user
    createdAt: '2022-08-16T20:21:30.000Z'
    data:
      status: closed
    id: 62fbfc4a72a7ab50b4b56a69
    type: status-change
  author: Tuana
  created_at: 2022-08-16 19:21:30+00:00
  id: 62fbfc4a72a7ab50b4b56a69
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2022-08-17T11:26:29.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: '<p>Out of curiosity, is this due to something specific about  the<code>adversarial_qa</code>
          dataset? I''m just wondering if you''d apply the same logic to other question
          answering datasets like <code>covid_qa_deepset</code>?</p>

          <p>When we developed the evaluation pipeline for question answering, our
          view was that models trained on SQuAD could be evaluated on any dataset
          that follows the SQuAD format. This would e.g. allow users to know whether
          domain adaptation is required on the vanilla SQuAD model for a given dataset.
          </p>

          <p>Note: it''s perfectly fine to reject this evaluation. I''m simply curious
          on whether my intuition about evaluating QA models is wrong :)</p>

          '
        raw: "Out of curiosity, is this due to something specific about  the`adversarial_qa`\
          \ dataset? I'm just wondering if you'd apply the same logic to other question\
          \ answering datasets like `covid_qa_deepset`?\n\nWhen we developed the evaluation\
          \ pipeline for question answering, our view was that models trained on SQuAD\
          \ could be evaluated on any dataset that follows the SQuAD format. This\
          \ would e.g. allow users to know whether domain adaptation is required on\
          \ the vanilla SQuAD model for a given dataset. \n\nNote: it's perfectly\
          \ fine to reject this evaluation. I'm simply curious on whether my intuition\
          \ about evaluating QA models is wrong :)"
        updatedAt: '2022-08-17T11:26:29.186Z'
      numEdits: 0
      reactions: []
    id: 62fcd06565ba08da9cd0eaf2
    type: comment
  author: lewtun
  content: "Out of curiosity, is this due to something specific about  the`adversarial_qa`\
    \ dataset? I'm just wondering if you'd apply the same logic to other question\
    \ answering datasets like `covid_qa_deepset`?\n\nWhen we developed the evaluation\
    \ pipeline for question answering, our view was that models trained on SQuAD could\
    \ be evaluated on any dataset that follows the SQuAD format. This would e.g. allow\
    \ users to know whether domain adaptation is required on the vanilla SQuAD model\
    \ for a given dataset. \n\nNote: it's perfectly fine to reject this evaluation.\
    \ I'm simply curious on whether my intuition about evaluating QA models is wrong\
    \ :)"
  created_at: 2022-08-17 10:26:29+00:00
  edited: false
  hidden: false
  id: 62fcd06565ba08da9cd0eaf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c28b8b669f6c2bc4126354/vMFQ2BK8mCEFuNSC4kr5P.png?w=200&h=200&f=face
      fullname: Sebastian Husch Lee
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: sjrhuschlee
      type: user
    createdAt: '2022-08-17T11:56:24.000Z'
    data:
      edited: true
      editors:
      - sjrhuschlee
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62c28b8b669f6c2bc4126354/vMFQ2BK8mCEFuNSC4kr5P.png?w=200&h=200&f=face
          fullname: Sebastian Husch Lee
          isHf: false
          isPro: false
          name: sjrhuschlee
          type: user
        html: "<p>We would really like to accumulate these metrics for all question-answering\
          \ datasets! However, as <span data-props=\"{&quot;user&quot;:&quot;Tuana&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tuana\"\
          >@<span class=\"underline\">Tuana</span></a></span>\n\n\t</span></span>\
          \ mentioned we think posting the eval results as they are shown right now\
          \ might confuse users into thinking that this model was also trained on\
          \ that dataset. </p>\n<p>If it is possible to have \"In Domain\" and \"\
          Out of Domain\" labels (or something similar) in the Evaluation Results\
          \ section then I think we can make it clear to users that fine-tuning this\
          \ model on that dataset could boost performance and we would like to add\
          \ results like adversarial_qa to that \"Out of Domain\" section.</p>\n"
        raw: "We would really like to accumulate these metrics for all question-answering\
          \ datasets! However, as @Tuana mentioned we think posting the eval results\
          \ as they are shown right now might confuse users into thinking that this\
          \ model was also trained on that dataset. \n\nIf it is possible to have\
          \ \"In Domain\" and \"Out of Domain\" labels (or something similar) in the\
          \ Evaluation Results section then I think we can make it clear to users\
          \ that fine-tuning this model on that dataset could boost performance and\
          \ we would like to add results like adversarial_qa to that \"Out of Domain\"\
          \ section."
        updatedAt: '2022-08-17T11:57:04.552Z'
      numEdits: 1
      reactions: []
    id: 62fcd768c1588e1d4c65321a
    type: comment
  author: sjrhuschlee
  content: "We would really like to accumulate these metrics for all question-answering\
    \ datasets! However, as @Tuana mentioned we think posting the eval results as\
    \ they are shown right now might confuse users into thinking that this model was\
    \ also trained on that dataset. \n\nIf it is possible to have \"In Domain\" and\
    \ \"Out of Domain\" labels (or something similar) in the Evaluation Results section\
    \ then I think we can make it clear to users that fine-tuning this model on that\
    \ dataset could boost performance and we would like to add results like adversarial_qa\
    \ to that \"Out of Domain\" section."
  created_at: 2022-08-17 10:56:24+00:00
  edited: true
  hidden: false
  id: 62fcd768c1588e1d4c65321a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ff48e9632a3bcb7c9874eb5/4XjaS0NVmdKFO_TZEMvyT.jpeg?w=200&h=200&f=face
      fullname: Julian Risch
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: julianrisch
      type: user
    createdAt: '2022-08-17T11:59:14.000Z'
    data:
      edited: false
      editors:
      - julianrisch
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5ff48e9632a3bcb7c9874eb5/4XjaS0NVmdKFO_TZEMvyT.jpeg?w=200&h=200&f=face
          fullname: Julian Risch
          isHf: false
          isPro: false
          name: julianrisch
          type: user
        html: "<p>The adversarial_qa dataset contains training, validation and test\
          \ splits, (10,000 training, 1,000 validation, and 1,000 test samples if\
          \ I am not mistaken). Therefore, in my opinion, it makes sense to compare\
          \ models on its test split that have been finetuned using its training split.\
          \ I could also imagine that one might want to compare models on its test\
          \ split that have not been finetuned using its training split. That matches\
          \ your vanilla SQuAD model/domain adaptation example. However, I wouldn't\
          \ mix these two kinds of models. It's not surprising that models finetuned\
          \ on the dataset are better than models not finetuned on it. <span data-props=\"\
          {&quot;user&quot;:&quot;lewtun&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/lewtun\">@<span class=\"underline\">lewtun</span></a></span>\n\
          \n\t</span></span> Maybe an idea could be to have two separate evaluation\
          \ dashboards of test split metrics per dataset? One for models not finetuned\
          \ on the training split and the other for models finetuned on the training\
          \ split? I would find mixing both kinds of models in one dashboard confusing.</p>\n\
          <p>covid_qa_deepset is much smaller and does not have designated splits\
          \ (2000 samples in total). If it had splits, I would also prefer to have\
          \ two separate evaluation dashboards for that dataset. :)</p>\n"
        raw: 'The adversarial_qa dataset contains training, validation and test splits,
          (10,000 training, 1,000 validation, and 1,000 test samples if I am not mistaken).
          Therefore, in my opinion, it makes sense to compare models on its test split
          that have been finetuned using its training split. I could also imagine
          that one might want to compare models on its test split that have not been
          finetuned using its training split. That matches your vanilla SQuAD model/domain
          adaptation example. However, I wouldn''t mix these two kinds of models.
          It''s not surprising that models finetuned on the dataset are better than
          models not finetuned on it. @lewtun Maybe an idea could be to have two separate
          evaluation dashboards of test split metrics per dataset? One for models
          not finetuned on the training split and the other for models finetuned on
          the training split? I would find mixing both kinds of models in one dashboard
          confusing.


          covid_qa_deepset is much smaller and does not have designated splits (2000
          samples in total). If it had splits, I would also prefer to have two separate
          evaluation dashboards for that dataset. :)'
        updatedAt: '2022-08-17T11:59:14.008Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - sjrhuschlee
    id: 62fcd812ba691bea5e8fa913
    type: comment
  author: julianrisch
  content: 'The adversarial_qa dataset contains training, validation and test splits,
    (10,000 training, 1,000 validation, and 1,000 test samples if I am not mistaken).
    Therefore, in my opinion, it makes sense to compare models on its test split that
    have been finetuned using its training split. I could also imagine that one might
    want to compare models on its test split that have not been finetuned using its
    training split. That matches your vanilla SQuAD model/domain adaptation example.
    However, I wouldn''t mix these two kinds of models. It''s not surprising that
    models finetuned on the dataset are better than models not finetuned on it. @lewtun
    Maybe an idea could be to have two separate evaluation dashboards of test split
    metrics per dataset? One for models not finetuned on the training split and the
    other for models finetuned on the training split? I would find mixing both kinds
    of models in one dashboard confusing.


    covid_qa_deepset is much smaller and does not have designated splits (2000 samples
    in total). If it had splits, I would also prefer to have two separate evaluation
    dashboards for that dataset. :)'
  created_at: 2022-08-17 10:59:14+00:00
  edited: false
  hidden: false
  id: 62fcd812ba691bea5e8fa913
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
      fullname: Lewis Tunstall
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lewtun
      type: user
    createdAt: '2022-08-17T13:46:30.000Z'
    data:
      edited: false
      editors:
      - lewtun
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1594651707950-noauth.jpeg?w=200&h=200&f=face
          fullname: Lewis Tunstall
          isHf: true
          isPro: false
          name: lewtun
          type: user
        html: "<p>Thanks @sjrlee and <span data-props=\"{&quot;user&quot;:&quot;julianrisch&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/julianrisch\"\
          >@<span class=\"underline\">julianrisch</span></a></span>\n\n\t</span></span>\
          \ for this insightful discussion!</p>\n<blockquote>\n<p>We would really\
          \ like to accumulate these metrics for all question-answering datasets!\
          \ However, as <span data-props=\"{&quot;user&quot;:&quot;Tuana&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Tuana\"\
          >@<span class=\"underline\">Tuana</span></a></span>\n\n\t</span></span>\
          \ mentioned we think posting the eval results as they are shown right now\
          \ might confuse users into thinking that this model was also trained on\
          \ that dataset.</p>\n</blockquote>\n<p>Good point - note that we do explicitly\
          \ show which dataset the model is trained on in the model page:</p>\n<p><a\
          \ rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/1660743364611-5f0c746619cb630495b814fd.png\"\
          ><img alt=\"Screen Shot 2022-08-17 at 15.35.57.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1660743364611-5f0c746619cb630495b814fd.png\"\
          ></a></p>\n<p>Also, in case you're not aware, you can add your own self-reported\
          \ evaluation results to the model card by following the <code>model-index</code>\
          \ spec <a rel=\"nofollow\" href=\"https://github.com/huggingface/hub-docs/blob/main/modelcard.md\"\
          >defined here</a>. This would allow you to signal to users that the core\
          \ results are from SQuAD vs other evaluations performed on other datasets.\
          \ Here's an <a href=\"https://huggingface.co/philschmid/bart-large-cnn-samsum\"\
          >example</a> of what I am talking about:</p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/1660743521197-5f0c746619cb630495b814fd.png\"\
          ><img alt=\"Screen Shot 2022-08-17 at 15.38.35.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/1660743521197-5f0c746619cb630495b814fd.png\"\
          ></a></p>\n<blockquote>\n<p>Therefore, in my opinion, it makes sense to\
          \ compare models on its test split that have been finetuned using its training\
          \ split. I could also imagine that one might want to compare models on its\
          \ test split that have not been finetuned using its training split. That\
          \ matches your vanilla SQuAD model/domain adaptation example. However, I\
          \ wouldn't mix these two kinds of models.</p>\n</blockquote>\n<p>Thanks\
          \ for sharing this interesting perspective. You're right that models finetuned\
          \ / evaluated on the same corpus will typically fare better than those evaluated\
          \ in a cross-domain fashion. I'll have a think about how complex the leaderboard\
          \ UX would be for distinguishing these cases, although my impression is\
          \ that most Hub users are used to looking at single leaderboards per dataset.</p>\n"
        raw: 'Thanks @sjrlee and @julianrisch for this insightful discussion!


          > We would really like to accumulate these metrics for all question-answering
          datasets! However, as @Tuana mentioned we think posting the eval results
          as they are shown right now might confuse users into thinking that this
          model was also trained on that dataset.


          Good point - note that we do explicitly show which dataset the model is
          trained on in the model page:


          ![Screen Shot 2022-08-17 at 15.35.57.png](https://cdn-uploads.huggingface.co/production/uploads/1660743364611-5f0c746619cb630495b814fd.png)


          Also, in case you''re not aware, you can add your own self-reported evaluation
          results to the model card by following the `model-index` spec [defined here](https://github.com/huggingface/hub-docs/blob/main/modelcard.md).
          This would allow you to signal to users that the core results are from SQuAD
          vs other evaluations performed on other datasets. Here''s an [example](https://huggingface.co/philschmid/bart-large-cnn-samsum)
          of what I am talking about:


          ![Screen Shot 2022-08-17 at 15.38.35.png](https://cdn-uploads.huggingface.co/production/uploads/1660743521197-5f0c746619cb630495b814fd.png)


          > Therefore, in my opinion, it makes sense to compare models on its test
          split that have been finetuned using its training split. I could also imagine
          that one might want to compare models on its test split that have not been
          finetuned using its training split. That matches your vanilla SQuAD model/domain
          adaptation example. However, I wouldn''t mix these two kinds of models.


          Thanks for sharing this interesting perspective. You''re right that models
          finetuned / evaluated on the same corpus will typically fare better than
          those evaluated in a cross-domain fashion. I''ll have a think about how
          complex the leaderboard UX would be for distinguishing these cases, although
          my impression is that most Hub users are used to looking at single leaderboards
          per dataset.'
        updatedAt: '2022-08-17T13:46:30.335Z'
      numEdits: 0
      reactions: []
    id: 62fcf136aad97d83a480199e
    type: comment
  author: lewtun
  content: 'Thanks @sjrlee and @julianrisch for this insightful discussion!


    > We would really like to accumulate these metrics for all question-answering
    datasets! However, as @Tuana mentioned we think posting the eval results as they
    are shown right now might confuse users into thinking that this model was also
    trained on that dataset.


    Good point - note that we do explicitly show which dataset the model is trained
    on in the model page:


    ![Screen Shot 2022-08-17 at 15.35.57.png](https://cdn-uploads.huggingface.co/production/uploads/1660743364611-5f0c746619cb630495b814fd.png)


    Also, in case you''re not aware, you can add your own self-reported evaluation
    results to the model card by following the `model-index` spec [defined here](https://github.com/huggingface/hub-docs/blob/main/modelcard.md).
    This would allow you to signal to users that the core results are from SQuAD vs
    other evaluations performed on other datasets. Here''s an [example](https://huggingface.co/philschmid/bart-large-cnn-samsum)
    of what I am talking about:


    ![Screen Shot 2022-08-17 at 15.38.35.png](https://cdn-uploads.huggingface.co/production/uploads/1660743521197-5f0c746619cb630495b814fd.png)


    > Therefore, in my opinion, it makes sense to compare models on its test split
    that have been finetuned using its training split. I could also imagine that one
    might want to compare models on its test split that have not been finetuned using
    its training split. That matches your vanilla SQuAD model/domain adaptation example.
    However, I wouldn''t mix these two kinds of models.


    Thanks for sharing this interesting perspective. You''re right that models finetuned
    / evaluated on the same corpus will typically fare better than those evaluated
    in a cross-domain fashion. I''ll have a think about how complex the leaderboard
    UX would be for distinguishing these cases, although my impression is that most
    Hub users are used to looking at single leaderboards per dataset.'
  created_at: 2022-08-17 12:46:30+00:00
  edited: false
  hidden: false
  id: 62fcf136aad97d83a480199e
  type: comment
is_pull_request: true
merge_commit_oid: null
num: 2
repo_id: deepset/xlm-roberta-base-squad2-distilled
repo_type: model
status: closed
target_branch: refs/heads/main
title: Add evaluation results on the adversarialQA config of adversarial_qa
