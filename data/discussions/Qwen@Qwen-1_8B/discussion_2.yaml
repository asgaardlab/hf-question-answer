!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rollinginthedeep
conflicting_files: null
created_at: 2023-12-12 14:03:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2eef249d67d784f0c1caaf35899e68b1.svg
      fullname: rollinginthedeep
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rollinginthedeep
      type: user
    createdAt: '2023-12-12T14:03:24.000Z'
    data:
      edited: false
      editors:
      - rollinginthedeep
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.32892870903015137
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2eef249d67d784f0c1caaf35899e68b1.svg
          fullname: rollinginthedeep
          isHf: false
          isPro: false
          name: rollinginthedeep
          type: user
        html: "<p>\u5C1D\u8BD5\u5728qwen-1_8B\u548Cqwen-7b\u4F7F\u7528assisted_decoding,\
          \ \u4EE3\u7801\u5982\u4E0B<br>from transformers import (<br>    AutoTokenizer,<br>\
          \    AutoModelForCausalLM,<br>    LogitsProcessorList,<br>    MinLengthLogitsProcessor,<br>\
          \    StoppingCriteriaList,<br>    MaxLengthCriteria,<br>)</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\")<br>model = AutoModelForCausalLM.from_pretrained(\"\
          Qwen/Qwen-7B\")<br>assistant_model = AutoModelForCausalLM.from_pretrained(\"\
          Qwen/Qwen-1_8B\")</p>\n<h1 id=\"set-pad_token_id-to-eos_token_id-because-gpt2-does-not-have-a-pad-token\"\
          >set pad_token_id to eos_token_id because GPT2 does not have a PAD token</h1>\n\
          <p>model.generation_config.pad_token_id = model.generation_config.eos_token_id<br>input_prompt\
          \ = \"It might be possible to\"<br>input_ids = tokenizer(input_prompt, return_tensors=\"\
          pt\").input_ids</p>\n<h1 id=\"instantiate-logits-processors\">instantiate\
          \ logits processors</h1>\n<p>logits_processor = LogitsProcessorList(<br>\
          \    [<br>        MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),<br>\
          \    ]<br>)<br>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])<br>outputs\
          \ = model.assisted_decoding(<br>    input_ids,<br>    assistant_model=assistant_model,<br>\
          \    logits_processor=logits_processor,<br>    stopping_criteria=stopping_criteria,<br>)<br>result\
          \ = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]<br>print(result)</p>\n\
          <p>requirements:<br>transformers==4.32.0<br>accelerate<br>tiktoken<br>einops<br>transformers_stream_generator==0.0.4<br>scipy<br>torch==1.12</p>\n\
          <p>\u62A5\u9519<br>/Qwen-1_8B/modeling_qwen.py, line 778\uFF0Cin forward<br>input_ids\
          \ = input_ids.view(-1, input_shape[-1])<br>RuntimeError: cannot reshape\
          \ tensor of 0 elements into shape [-1, 0] because the unspecific dimension\
          \ size -1 can be any value and is ambiguous</p>\n<p>\u662F\u5426modeling_qwen\u6587\
          \u4EF6\u4E2Dkey, value\u7684transpose\u548Cpermute\u65B9\u6CD5\u6709\u95EE\
          \u9898\uFF1F</p>\n"
        raw: "\u5C1D\u8BD5\u5728qwen-1_8B\u548Cqwen-7b\u4F7F\u7528assisted_decoding,\
          \ \u4EE3\u7801\u5982\u4E0B\r\nfrom transformers import (\r\n    AutoTokenizer,\r\
          \n    AutoModelForCausalLM,\r\n    LogitsProcessorList,\r\n    MinLengthLogitsProcessor,\r\
          \n    StoppingCriteriaList,\r\n    MaxLengthCriteria,\r\n)\r\n\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          Qwen/Qwen-7B\")\r\nassistant_model = AutoModelForCausalLM.from_pretrained(\"\
          Qwen/Qwen-1_8B\")\r\n# set pad_token_id to eos_token_id because GPT2 does\
          \ not have a PAD token\r\nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\r\
          \ninput_prompt = \"It might be possible to\"\r\ninput_ids = tokenizer(input_prompt,\
          \ return_tensors=\"pt\").input_ids\r\n# instantiate logits processors\r\n\
          logits_processor = LogitsProcessorList(\r\n    [\r\n        MinLengthLogitsProcessor(10,\
          \ eos_token_id=model.generation_config.eos_token_id),\r\n    ]\r\n)\r\n\
          stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\r\
          \noutputs = model.assisted_decoding(\r\n    input_ids,\r\n    assistant_model=assistant_model,\r\
          \n    logits_processor=logits_processor,\r\n    stopping_criteria=stopping_criteria,\r\
          \n)\r\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\r\
          \nprint(result)\r\n\r\nrequirements:\r\ntransformers==4.32.0\r\naccelerate\r\
          \ntiktoken\r\neinops\r\ntransformers_stream_generator==0.0.4\r\nscipy\r\n\
          torch==1.12\r\n\r\n\u62A5\u9519\r\n/Qwen-1_8B/modeling_qwen.py, line 778\uFF0C\
          in forward \r\ninput_ids = input_ids.view(-1, input_shape[-1])\r\nRuntimeError:\
          \ cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecific\
          \ dimension size -1 can be any value and is ambiguous\r\n\r\n\u662F\u5426\
          modeling_qwen\u6587\u4EF6\u4E2Dkey, value\u7684transpose\u548Cpermute\u65B9\
          \u6CD5\u6709\u95EE\u9898\uFF1F"
        updatedAt: '2023-12-12T14:03:24.761Z'
      numEdits: 0
      reactions: []
    id: 6578682cf7d98f6f611a1c1d
    type: comment
  author: rollinginthedeep
  content: "\u5C1D\u8BD5\u5728qwen-1_8B\u548Cqwen-7b\u4F7F\u7528assisted_decoding,\
    \ \u4EE3\u7801\u5982\u4E0B\r\nfrom transformers import (\r\n    AutoTokenizer,\r\
    \n    AutoModelForCausalLM,\r\n    LogitsProcessorList,\r\n    MinLengthLogitsProcessor,\r\
    \n    StoppingCriteriaList,\r\n    MaxLengthCriteria,\r\n)\r\n\r\ntokenizer =\
    \ AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    Qwen/Qwen-7B\")\r\nassistant_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-1_8B\"\
    )\r\n# set pad_token_id to eos_token_id because GPT2 does not have a PAD token\r\
    \nmodel.generation_config.pad_token_id = model.generation_config.eos_token_id\r\
    \ninput_prompt = \"It might be possible to\"\r\ninput_ids = tokenizer(input_prompt,\
    \ return_tensors=\"pt\").input_ids\r\n# instantiate logits processors\r\nlogits_processor\
    \ = LogitsProcessorList(\r\n    [\r\n        MinLengthLogitsProcessor(10, eos_token_id=model.generation_config.eos_token_id),\r\
    \n    ]\r\n)\r\nstopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\r\
    \noutputs = model.assisted_decoding(\r\n    input_ids,\r\n    assistant_model=assistant_model,\r\
    \n    logits_processor=logits_processor,\r\n    stopping_criteria=stopping_criteria,\r\
    \n)\r\nresult = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\r\
    \nprint(result)\r\n\r\nrequirements:\r\ntransformers==4.32.0\r\naccelerate\r\n\
    tiktoken\r\neinops\r\ntransformers_stream_generator==0.0.4\r\nscipy\r\ntorch==1.12\r\
    \n\r\n\u62A5\u9519\r\n/Qwen-1_8B/modeling_qwen.py, line 778\uFF0Cin forward \r\
    \ninput_ids = input_ids.view(-1, input_shape[-1])\r\nRuntimeError: cannot reshape\
    \ tensor of 0 elements into shape [-1, 0] because the unspecific dimension size\
    \ -1 can be any value and is ambiguous\r\n\r\n\u662F\u5426modeling_qwen\u6587\u4EF6\
    \u4E2Dkey, value\u7684transpose\u548Cpermute\u65B9\u6CD5\u6709\u95EE\u9898\uFF1F"
  created_at: 2023-12-12 14:03:24+00:00
  edited: false
  hidden: false
  id: 6578682cf7d98f6f611a1c1d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Qwen/Qwen-1_8B
repo_type: model
status: open
target_branch: null
title: "qwen-1_8B\u548Cqwen-7b\u8F85\u52A9\u89E3\u7801\u5931\u8D25"
