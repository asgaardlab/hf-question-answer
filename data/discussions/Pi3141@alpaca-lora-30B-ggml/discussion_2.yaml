!!python/object:huggingface_hub.community.DiscussionWithDetails
author: futureagi
conflicting_files: null
created_at: 2023-03-21 08:19:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677666680677-62e0e0813eb0730f6213c894.png?w=200&h=200&f=face
      fullname: uberwow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: futureagi
      type: user
    createdAt: '2023-03-21T09:19:13.000Z'
    data:
      edited: false
      editors:
      - futureagi
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1677666680677-62e0e0813eb0730f6213c894.png?w=200&h=200&f=face
          fullname: uberwow
          isHf: false
          isPro: false
          name: futureagi
          type: user
        html: '<p>An error while loading 30b with alpaca.cpp.<br>Maybe I do something
          wrong?</p>

          <p>main: seed = 1679388768<br>llama_model_load: loading model from ''D:\alpaca\ggml-alpaca-30b-q4.bin''
          - please wait ...<br>llama_model_load: ggml ctx size = 25631.50 MB<br>llama_model_load:
          memory_size =  6240.00 MB, n_mem = 122880<br>llama_model_load: loading model
          part 1/4 from ''D:\alpaca\ggml-alpaca-30b-q4.bin''<br>llama_model_load:
          llama_model_load: tensor ''tok_embeddings.weight'' has wrong size in model
          file<br>main: failed to load model from ''D:\alpaca\ggml-alpaca-30b-q4.bin''</p>

          '
        raw: "An error while loading 30b with alpaca.cpp.\r\nMaybe I do something\
          \ wrong?\r\n\r\nmain: seed = 1679388768\r\nllama_model_load: loading model\
          \ from 'D:\\alpaca\\ggml-alpaca-30b-q4.bin' - please wait ...\r\nllama_model_load:\
          \ ggml ctx size = 25631.50 MB\r\nllama_model_load: memory_size =  6240.00\
          \ MB, n_mem = 122880\r\nllama_model_load: loading model part 1/4 from 'D:\\\
          alpaca\\ggml-alpaca-30b-q4.bin'\r\nllama_model_load: llama_model_load: tensor\
          \ 'tok_embeddings.weight' has wrong size in model file\r\nmain: failed to\
          \ load model from 'D:\\alpaca\\ggml-alpaca-30b-q4.bin'"
        updatedAt: '2023-03-21T09:19:13.724Z'
      numEdits: 0
      reactions: []
    id: 64197691060a651c415d93eb
    type: comment
  author: futureagi
  content: "An error while loading 30b with alpaca.cpp.\r\nMaybe I do something wrong?\r\
    \n\r\nmain: seed = 1679388768\r\nllama_model_load: loading model from 'D:\\alpaca\\\
    ggml-alpaca-30b-q4.bin' - please wait ...\r\nllama_model_load: ggml ctx size =\
    \ 25631.50 MB\r\nllama_model_load: memory_size =  6240.00 MB, n_mem = 122880\r\
    \nllama_model_load: loading model part 1/4 from 'D:\\alpaca\\ggml-alpaca-30b-q4.bin'\r\
    \nllama_model_load: llama_model_load: tensor 'tok_embeddings.weight' has wrong\
    \ size in model file\r\nmain: failed to load model from 'D:\\alpaca\\ggml-alpaca-30b-q4.bin'"
  created_at: 2023-03-21 08:19:13+00:00
  edited: false
  hidden: false
  id: 64197691060a651c415d93eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/aa846461adb5028728feb70dac4d3f15.svg
      fullname: r2bb1t
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LurkHub
      type: user
    createdAt: '2023-03-21T12:11:28.000Z'
    data:
      edited: false
      editors:
      - LurkHub
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/aa846461adb5028728feb70dac4d3f15.svg
          fullname: r2bb1t
          isHf: false
          isPro: false
          name: LurkHub
          type: user
        html: '<p>\alpaca&gt;.\Release\chat.exe -m ggml-model-q4_0.bin -n 256 --repeat_penalty
          1.0 --color -i -r "ROBOT:" -f -ins<br>main: seed = 1679403424<br>llama_model_load:
          loading model from ''ggml-model-q4_0.bin'' - please wait ...<br>llama_model_load:
          ggml ctx size = 25631.50 MB<br>llama_model_load: memory_size =  6240.00
          MB, n_mem = 122880<br>llama_model_load: loading model part 1/4 from ''ggml-model-q4_0.bin''<br>llama_model_load:
          llama_model_load: tensor ''tok_embeddings.weight'' has wrong size in model
          file<br>main: failed to load model from ''ggml-model-q4_0.bin''</p>

          '
        raw: '\alpaca>.\Release\chat.exe -m ggml-model-q4_0.bin -n 256 --repeat_penalty
          1.0 --color -i -r "ROBOT:" -f -ins

          main: seed = 1679403424

          llama_model_load: loading model from ''ggml-model-q4_0.bin'' - please wait
          ...

          llama_model_load: ggml ctx size = 25631.50 MB

          llama_model_load: memory_size =  6240.00 MB, n_mem = 122880

          llama_model_load: loading model part 1/4 from ''ggml-model-q4_0.bin''

          llama_model_load: llama_model_load: tensor ''tok_embeddings.weight'' has
          wrong size in model file

          main: failed to load model from ''ggml-model-q4_0.bin'''
        updatedAt: '2023-03-21T12:11:28.024Z'
      numEdits: 0
      reactions: []
    id: 64199ef03a524ff07ef391f7
    type: comment
  author: LurkHub
  content: '\alpaca>.\Release\chat.exe -m ggml-model-q4_0.bin -n 256 --repeat_penalty
    1.0 --color -i -r "ROBOT:" -f -ins

    main: seed = 1679403424

    llama_model_load: loading model from ''ggml-model-q4_0.bin'' - please wait ...

    llama_model_load: ggml ctx size = 25631.50 MB

    llama_model_load: memory_size =  6240.00 MB, n_mem = 122880

    llama_model_load: loading model part 1/4 from ''ggml-model-q4_0.bin''

    llama_model_load: llama_model_load: tensor ''tok_embeddings.weight'' has wrong
    size in model file

    main: failed to load model from ''ggml-model-q4_0.bin'''
  created_at: 2023-03-21 11:11:28+00:00
  edited: false
  hidden: false
  id: 64199ef03a524ff07ef391f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-03-21T12:50:08.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>Please see <a href="/Pi3141/alpaca-lora-30B-ggml/discussions/3">#3</a><br><a
          href="https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3">https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3</a></p>

          '
        raw: 'Please see #3

          https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3'
        updatedAt: '2023-03-21T12:50:08.549Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - futureagi
    id: 6419a8003c73fa42caef3b6d
    type: comment
  author: Pi3141
  content: 'Please see #3

    https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3'
  created_at: 2023-03-21 11:50:08+00:00
  edited: false
  hidden: false
  id: 6419a8003c73fa42caef3b6d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8e38c15da439447b07317a5c59789b62.svg
      fullname: Vega
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stavis-dev
      type: user
    createdAt: '2023-03-28T16:01:31.000Z'
    data:
      edited: false
      editors:
      - Stavis-dev
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8e38c15da439447b07317a5c59789b62.svg
          fullname: Vega
          isHf: false
          isPro: false
          name: Stavis-dev
          type: user
        html: '<blockquote>

          <p>Please see <a href="/Pi3141/alpaca-lora-30B-ggml/discussions/3">#3</a><br><a
          href="https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3">https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3</a></p>

          </blockquote>

          <p>Didn''t work for me. Tried to run in colab. Here is the result.</p>

          <p>colab:</p>

          <pre><code class="language-sh">!<span class="hljs-built_in">mkdir</span>
          30b

          !<span class="hljs-built_in">cd</span> 30b &amp;&amp; wget https://huggingface.co/Pi3141/alpaca-lora-30B-ggml/resolve/main/ggml-model-q4_0.bin


          !git <span class="hljs-built_in">clone</span> https://github.com/ItsPi3141/alpaca.cpp.git
          nalpaca

          !<span class="hljs-built_in">cd</span> nalpaca &amp;&amp; make main


          !<span class="hljs-built_in">cd</span> nalpaca &amp;&amp; ./main -m ../30b/ggml-model-q4_0.bin

          </code></pre>

          <p>result</p>

          <pre><code class="language-sh">main: seed = 1680018948

          llama_model_load: loading model from <span class="hljs-string">''../30b/ggml-model-q4_0.bin''</span>
          - please <span class="hljs-built_in">wait</span> ...

          llama_model_load: ggml ctx size = 25631.50 MB

          llama_model_load: memory_size =  6240.00 MB, n_mem = 122880

          llama_model_load: loading model part 1/1 from <span class="hljs-string">''../30b/ggml-model-q4_0.bin''</span>

          llama_model_load: ........................................^C

          </code></pre>

          <p>Please, can you help?</p>

          <p>... what is remarkable this assembly works fine  <a href="https://huggingface.co/Pi3141/alpaca-lora-7B-ggml">alpaca-lora-7B-ggml</a></p>

          '
        raw: '> Please see #3

          > https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3


          Didn''t work for me. Tried to run in colab. Here is the result.


          colab:


          ```sh

          !mkdir 30b

          !cd 30b && wget https://huggingface.co/Pi3141/alpaca-lora-30B-ggml/resolve/main/ggml-model-q4_0.bin


          !git clone https://github.com/ItsPi3141/alpaca.cpp.git nalpaca

          !cd nalpaca && make main


          !cd nalpaca && ./main -m ../30b/ggml-model-q4_0.bin

          ```


          result


          ```sh

          main: seed = 1680018948

          llama_model_load: loading model from ''../30b/ggml-model-q4_0.bin'' - please
          wait ...

          llama_model_load: ggml ctx size = 25631.50 MB

          llama_model_load: memory_size =  6240.00 MB, n_mem = 122880

          llama_model_load: loading model part 1/1 from ''../30b/ggml-model-q4_0.bin''

          llama_model_load: ........................................^C

          ```


          Please, can you help?


          ... what is remarkable this assembly works fine  [alpaca-lora-7B-ggml](https://huggingface.co/Pi3141/alpaca-lora-7B-ggml)'
        updatedAt: '2023-03-28T16:01:31.581Z'
      numEdits: 0
      reactions: []
    id: 64230f5b089706378be1b3f4
    type: comment
  author: Stavis-dev
  content: '> Please see #3

    > https://huggingface.co/Pi3141/alpaca-30B-ggml/discussions/3


    Didn''t work for me. Tried to run in colab. Here is the result.


    colab:


    ```sh

    !mkdir 30b

    !cd 30b && wget https://huggingface.co/Pi3141/alpaca-lora-30B-ggml/resolve/main/ggml-model-q4_0.bin


    !git clone https://github.com/ItsPi3141/alpaca.cpp.git nalpaca

    !cd nalpaca && make main


    !cd nalpaca && ./main -m ../30b/ggml-model-q4_0.bin

    ```


    result


    ```sh

    main: seed = 1680018948

    llama_model_load: loading model from ''../30b/ggml-model-q4_0.bin'' - please wait
    ...

    llama_model_load: ggml ctx size = 25631.50 MB

    llama_model_load: memory_size =  6240.00 MB, n_mem = 122880

    llama_model_load: loading model part 1/1 from ''../30b/ggml-model-q4_0.bin''

    llama_model_load: ........................................^C

    ```


    Please, can you help?


    ... what is remarkable this assembly works fine  [alpaca-lora-7B-ggml](https://huggingface.co/Pi3141/alpaca-lora-7B-ggml)'
  created_at: 2023-03-28 15:01:31+00:00
  edited: false
  hidden: false
  id: 64230f5b089706378be1b3f4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-03-28T17:35:54.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>From the results, it looks like you aborted it (^C). Did you use
          alpaca.cpp or llama.cpp?</p>

          '
        raw: From the results, it looks like you aborted it (^C). Did you use alpaca.cpp
          or llama.cpp?
        updatedAt: '2023-03-28T17:35:54.203Z'
      numEdits: 0
      reactions: []
    id: 6423257accb78df81e822d91
    type: comment
  author: Pi3141
  content: From the results, it looks like you aborted it (^C). Did you use alpaca.cpp
    or llama.cpp?
  created_at: 2023-03-28 16:35:54+00:00
  edited: false
  hidden: false
  id: 6423257accb78df81e822d91
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a3193224fa241524cc9291259abbcd9.svg
      fullname: Jon Snow
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AkhenAtom
      type: user
    createdAt: '2023-04-17T13:39:23.000Z'
    data:
      edited: false
      editors:
      - AkhenAtom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a3193224fa241524cc9291259abbcd9.svg
          fullname: Jon Snow
          isHf: false
          isPro: false
          name: AkhenAtom
          type: user
        html: '<p>I use alpaca.cpp<br>the following settings(30B) works fine for both
          13B and 30B. </p>

          <p>// determine number of model parts based on the dimension<br>const map&lt;int,
          int&gt; LLAMA_N_PARTS = {<br>    //Model 30B<br>    { 4096, 1 },<br>    {
          5120, 1 },<br>    { 6656, 1 },<br>    { 8192, 8 },<br>};</p>

          <p>// default hparams<br>struct llama_hparams {<br>    // Model 30B<br>    int32_t
          n_vocab = 32000;<br>    int32_t n_ctx = 512; // max lenght of the input
          prompt (default, work also 1000)<br>    int32_t n_embd = 6656;<br>    int32_t
          n_mult = 256;<br>    int32_t n_head = 52;<br>    int32_t n_layer = 60;<br>    int32_t
          n_rot = 64;<br>    int32_t f16 = 1;<br>};</p>

          <p>The 30B is awesome.<br>o7</p>

          '
        raw: "I use alpaca.cpp \nthe following settings(30B) works fine for both 13B\
          \ and 30B. \n\n// determine number of model parts based on the dimension\n\
          const map<int, int> LLAMA_N_PARTS = {    \n    //Model 30B\n    { 4096,\
          \ 1 },\n    { 5120, 1 },\n    { 6656, 1 },\n    { 8192, 8 },\n};\n\n// default\
          \ hparams\nstruct llama_hparams {\n    // Model 30B\n    int32_t n_vocab\
          \ = 32000;\n    int32_t n_ctx = 512; // max lenght of the input prompt (default,\
          \ work also 1000)\n    int32_t n_embd = 6656;\n    int32_t n_mult = 256;\n\
          \    int32_t n_head = 52;\n    int32_t n_layer = 60;\n    int32_t n_rot\
          \ = 64;\n    int32_t f16 = 1;\n};\n\nThe 30B is awesome.\no7"
        updatedAt: '2023-04-17T13:39:23.133Z'
      numEdits: 0
      reactions: []
    id: 643d4c0b11ea48e24657bc76
    type: comment
  author: AkhenAtom
  content: "I use alpaca.cpp \nthe following settings(30B) works fine for both 13B\
    \ and 30B. \n\n// determine number of model parts based on the dimension\nconst\
    \ map<int, int> LLAMA_N_PARTS = {    \n    //Model 30B\n    { 4096, 1 },\n   \
    \ { 5120, 1 },\n    { 6656, 1 },\n    { 8192, 8 },\n};\n\n// default hparams\n\
    struct llama_hparams {\n    // Model 30B\n    int32_t n_vocab = 32000;\n    int32_t\
    \ n_ctx = 512; // max lenght of the input prompt (default, work also 1000)\n \
    \   int32_t n_embd = 6656;\n    int32_t n_mult = 256;\n    int32_t n_head = 52;\n\
    \    int32_t n_layer = 60;\n    int32_t n_rot = 64;\n    int32_t f16 = 1;\n};\n\
    \nThe 30B is awesome.\no7"
  created_at: 2023-04-17 12:39:23+00:00
  edited: false
  hidden: false
  id: 643d4c0b11ea48e24657bc76
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Pi3141/alpaca-lora-30B-ggml
repo_type: model
status: open
target_branch: null
title: An error while loading 30b with alpaca.cpp.
