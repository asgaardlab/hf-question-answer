!!python/object:huggingface_hub.community.DiscussionWithDetails
author: vitsum
conflicting_files: null
created_at: 2023-04-01 16:24:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49f3d5dd5f1401ba826cf1e7ae895582.svg
      fullname: vitaly sumin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vitsum
      type: user
    createdAt: '2023-04-01T17:24:08.000Z'
    data:
      edited: false
      editors:
      - vitsum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49f3d5dd5f1401ba826cf1e7ae895582.svg
          fullname: vitaly sumin
          isHf: false
          isPro: false
          name: vitsum
          type: user
        html: '<p>hi, i tried to run it but it failed to load the model. May be I
          am doing something wrong. How do you run?</p>

          <p>$ ./main -m ./models/alpaca/30b/ggml-alpaca-30b-q4_1.bin --color -f ./prompts/alpaca.txt
          -ins --n_parts 1<br>main: seed = 1680368465<br>llama_model_load: loading
          model from ''./models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'' - please wait
          ...<br>llama_model_load: n_vocab = 32000<br>llama_model_load: n_ctx   =
          512<br>llama_model_load: n_embd  = 6656<br>llama_model_load: n_mult  = 256<br>llama_model_load:
          n_head  = 52<br>llama_model_load: n_layer = 60<br>llama_model_load: n_rot   =
          128<br>llama_model_load: f16     = 3<br>llama_model_load: n_ff    = 17920<br>llama_model_load:
          n_parts = 1<br>llama_model_load: type    = 3<br>llama_model_load: failed
          to mmap ''./models/alpaca/30b/ggml-alpaca-30b-q4_1.bin''<br>llama_init_from_file:
          failed to load model<br>main: error: failed to load model ''./models/alpaca/30b/ggml-alpaca-30b-q4_1.bin''</p>

          '
        raw: "hi, i tried to run it but it failed to load the model. May be I am doing\
          \ something wrong. How do you run?\r\n\r\n$ ./main -m ./models/alpaca/30b/ggml-alpaca-30b-q4_1.bin\
          \ --color -f ./prompts/alpaca.txt -ins --n_parts 1\r\nmain: seed = 1680368465\r\
          \nllama_model_load: loading model from './models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'\
          \ - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load:\
          \ n_ctx   = 512\r\nllama_model_load: n_embd  = 6656\r\nllama_model_load:\
          \ n_mult  = 256\r\nllama_model_load: n_head  = 52\r\nllama_model_load: n_layer\
          \ = 60\r\nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     =\
          \ 3\r\nllama_model_load: n_ff    = 17920\r\nllama_model_load: n_parts =\
          \ 1\r\nllama_model_load: type    = 3\r\nllama_model_load: failed to mmap\
          \ './models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'\r\nllama_init_from_file:\
          \ failed to load model\r\nmain: error: failed to load model './models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'\r\
          \n"
        updatedAt: '2023-04-01T17:24:08.682Z'
      numEdits: 0
      reactions: []
    id: 642868b853b748123d4d049e
    type: comment
  author: vitsum
  content: "hi, i tried to run it but it failed to load the model. May be I am doing\
    \ something wrong. How do you run?\r\n\r\n$ ./main -m ./models/alpaca/30b/ggml-alpaca-30b-q4_1.bin\
    \ --color -f ./prompts/alpaca.txt -ins --n_parts 1\r\nmain: seed = 1680368465\r\
    \nllama_model_load: loading model from './models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'\
    \ - please wait ...\r\nllama_model_load: n_vocab = 32000\r\nllama_model_load:\
    \ n_ctx   = 512\r\nllama_model_load: n_embd  = 6656\r\nllama_model_load: n_mult\
    \  = 256\r\nllama_model_load: n_head  = 52\r\nllama_model_load: n_layer = 60\r\
    \nllama_model_load: n_rot   = 128\r\nllama_model_load: f16     = 3\r\nllama_model_load:\
    \ n_ff    = 17920\r\nllama_model_load: n_parts = 1\r\nllama_model_load: type \
    \   = 3\r\nllama_model_load: failed to mmap './models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'\r\
    \nllama_init_from_file: failed to load model\r\nmain: error: failed to load model\
    \ './models/alpaca/30b/ggml-alpaca-30b-q4_1.bin'\r\n"
  created_at: 2023-04-01 16:24:08+00:00
  edited: false
  hidden: false
  id: 642868b853b748123d4d049e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-04-01T17:51:35.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>Oh... Did you use the very latest llama.cpp? When I tested it, it
          was fine. I''ll check again later.</p>

          '
        raw: Oh... Did you use the very latest llama.cpp? When I tested it, it was
          fine. I'll check again later.
        updatedAt: '2023-04-01T17:51:35.791Z'
      numEdits: 0
      reactions: []
    id: 64286f2752f1fac0b9dc4551
    type: comment
  author: Pi3141
  content: Oh... Did you use the very latest llama.cpp? When I tested it, it was fine.
    I'll check again later.
  created_at: 2023-04-01 16:51:35+00:00
  edited: false
  hidden: false
  id: 64286f2752f1fac0b9dc4551
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49f3d5dd5f1401ba826cf1e7ae895582.svg
      fullname: vitaly sumin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vitsum
      type: user
    createdAt: '2023-04-01T17:55:27.000Z'
    data:
      edited: false
      editors:
      - vitsum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49f3d5dd5f1401ba826cf1e7ae895582.svg
          fullname: vitaly sumin
          isHf: false
          isPro: false
          name: vitsum
          type: user
        html: '<p>Yes. Have just downloaded and built it.</p>

          '
        raw: Yes. Have just downloaded and built it.
        updatedAt: '2023-04-01T17:55:27.512Z'
      numEdits: 0
      reactions: []
    id: 6428700f0cf37aade74e328c
    type: comment
  author: vitsum
  content: Yes. Have just downloaded and built it.
  created_at: 2023-04-01 16:55:27+00:00
  edited: false
  hidden: false
  id: 6428700f0cf37aade74e328c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-04-01T17:59:23.000Z'
    data:
      edited: true
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>I just realized, the model is called<br>ggml-model-q4_1.bin</p>

          <p>You put<br>ggml-alpaca-30b-q4_1.bin</p>

          <p>Unless you renamed it</p>

          '
        raw: "I just realized, the model is called \nggml-model-q4_1.bin\n\nYou put\
          \ \nggml-alpaca-30b-q4_1.bin\n\nUnless you renamed it"
        updatedAt: '2023-04-01T17:59:40.297Z'
      numEdits: 1
      reactions: []
    id: 642870fb7b2cfe4e499cdb45
    type: comment
  author: Pi3141
  content: "I just realized, the model is called \nggml-model-q4_1.bin\n\nYou put\
    \ \nggml-alpaca-30b-q4_1.bin\n\nUnless you renamed it"
  created_at: 2023-04-01 16:59:23+00:00
  edited: true
  hidden: false
  id: 642870fb7b2cfe4e499cdb45
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/49f3d5dd5f1401ba826cf1e7ae895582.svg
      fullname: vitaly sumin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: vitsum
      type: user
    createdAt: '2023-04-02T01:14:29.000Z'
    data:
      edited: false
      editors:
      - vitsum
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/49f3d5dd5f1401ba826cf1e7ae895582.svg
          fullname: vitaly sumin
          isHf: false
          isPro: false
          name: vitsum
          type: user
        html: '<p>I renamed it</p>

          '
        raw: I renamed it
        updatedAt: '2023-04-02T01:14:29.143Z'
      numEdits: 0
      reactions: []
    id: 6428d6f5d5f184522af996d0
    type: comment
  author: vitsum
  content: I renamed it
  created_at: 2023-04-02 00:14:29+00:00
  edited: false
  hidden: false
  id: 6428d6f5d5f184522af996d0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd12a8358b7d9aa82d849f76c57bd526.svg
      fullname: kname
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kname
      type: user
    createdAt: '2023-04-02T02:23:57.000Z'
    data:
      edited: false
      editors:
      - kname
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd12a8358b7d9aa82d849f76c57bd526.svg
          fullname: kname
          isHf: false
          isPro: false
          name: kname
          type: user
        html: '<p>Having trouble as well. Getting "bad magic" with current main. </p>

          <p>4_0.bin was probably my favorite model so far, I was excited to see the
          update as I was completely unable to use their convert unversioned ggml
          to ggml tool. Guessing because it''s 4bit? IDK, but couldn''t make it work.</p>

          <p>Here''s what I got.<br>./main -m ./models/30b/ggml-model-q4_1.bin<br>main:
          seed = 1680402127<br>llama_model_load: loading model from ''./models/30b/ggml-model-q4_1.bin''
          - please wait ...<br>llama_model_load: invalid model file ''./models/30b/ggml-model-q4_1.bin''
          (bad magic)<br>llama_init_from_file: failed to load model<br>main: error:
          failed to load model ''./models/30b/ggml-model-q4_1.bin''</p>

          '
        raw: "Having trouble as well. Getting \"bad magic\" with current main. \n\n\
          4_0.bin was probably my favorite model so far, I was excited to see the\
          \ update as I was completely unable to use their convert unversioned ggml\
          \ to ggml tool. Guessing because it's 4bit? IDK, but couldn't make it work.\n\
          \nHere's what I got.\n./main -m ./models/30b/ggml-model-q4_1.bin\nmain:\
          \ seed = 1680402127\nllama_model_load: loading model from './models/30b/ggml-model-q4_1.bin'\
          \ - please wait ...\nllama_model_load: invalid model file './models/30b/ggml-model-q4_1.bin'\
          \ (bad magic)\nllama_init_from_file: failed to load model\nmain: error:\
          \ failed to load model './models/30b/ggml-model-q4_1.bin'"
        updatedAt: '2023-04-02T02:23:57.831Z'
      numEdits: 0
      reactions: []
    id: 6428e73d19f9a9b182a9a7d9
    type: comment
  author: kname
  content: "Having trouble as well. Getting \"bad magic\" with current main. \n\n\
    4_0.bin was probably my favorite model so far, I was excited to see the update\
    \ as I was completely unable to use their convert unversioned ggml to ggml tool.\
    \ Guessing because it's 4bit? IDK, but couldn't make it work.\n\nHere's what I\
    \ got.\n./main -m ./models/30b/ggml-model-q4_1.bin\nmain: seed = 1680402127\n\
    llama_model_load: loading model from './models/30b/ggml-model-q4_1.bin' - please\
    \ wait ...\nllama_model_load: invalid model file './models/30b/ggml-model-q4_1.bin'\
    \ (bad magic)\nllama_init_from_file: failed to load model\nmain: error: failed\
    \ to load model './models/30b/ggml-model-q4_1.bin'"
  created_at: 2023-04-02 01:23:57+00:00
  edited: false
  hidden: false
  id: 6428e73d19f9a9b182a9a7d9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-04-02T02:27:50.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<p>Bad magic is usually because you''re using alpaca.cpp (outdated)
          or the older version of llama.cpp. Try recloning llama.cpp.</p>

          '
        raw: Bad magic is usually because you're using alpaca.cpp (outdated) or the
          older version of llama.cpp. Try recloning llama.cpp.
        updatedAt: '2023-04-02T02:27:50.081Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kname
    id: 6428e826a760fe0bf378d3e5
    type: comment
  author: Pi3141
  content: Bad magic is usually because you're using alpaca.cpp (outdated) or the
    older version of llama.cpp. Try recloning llama.cpp.
  created_at: 2023-04-02 01:27:50+00:00
  edited: false
  hidden: false
  id: 6428e826a760fe0bf378d3e5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fb8f1cd25dfaec8a5fa31bedf78dc32f.svg
      fullname: AlexHz
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: AlexHz
      type: user
    createdAt: '2023-04-02T13:18:50.000Z'
    data:
      edited: true
      editors:
      - AlexHz
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fb8f1cd25dfaec8a5fa31bedf78dc32f.svg
          fullname: AlexHz
          isHf: false
          isPro: false
          name: AlexHz
          type: user
        html: '<p><del>I''m seeing the same "bad magic" error on the latest llama.cpp
          commit.</del> </p>

          <p>Solved: in my case, git lfs was corrupted</p>

          '
        raw: "~I'm seeing the same \"bad magic\" error on the latest llama.cpp commit.~\
          \ \n\nSolved: in my case, git lfs was corrupted"
        updatedAt: '2023-04-02T13:21:40.946Z'
      numEdits: 1
      reactions: []
    id: 642980ba3bccbc25884d516b
    type: comment
  author: AlexHz
  content: "~I'm seeing the same \"bad magic\" error on the latest llama.cpp commit.~\
    \ \n\nSolved: in my case, git lfs was corrupted"
  created_at: 2023-04-02 12:18:50+00:00
  edited: true
  hidden: false
  id: 642980ba3bccbc25884d516b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-04-02T15:26:41.000Z'
    data:
      edited: false
      editors:
      - Pi3141
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
          fullname: Pi
          isHf: false
          isPro: false
          name: Pi3141
          type: user
        html: '<blockquote>

          <p>Solved: in my case, git lfs was corrupted</p>

          </blockquote>

          <p>Yay I''m glad it worked!</p>

          '
        raw: '> Solved: in my case, git lfs was corrupted


          Yay I''m glad it worked!'
        updatedAt: '2023-04-02T15:26:41.966Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64299eb1c9f3ac3c7549cfe4
    id: 64299eb1c9f3ac3c7549cfe3
    type: comment
  author: Pi3141
  content: '> Solved: in my case, git lfs was corrupted


    Yay I''m glad it worked!'
  created_at: 2023-04-02 14:26:41+00:00
  edited: false
  hidden: false
  id: 64299eb1c9f3ac3c7549cfe3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/615a1b7a321f65c4da59c3d3/799xClAlAoFuljSgOlxs2.png?w=200&h=200&f=face
      fullname: Pi
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Pi3141
      type: user
    createdAt: '2023-04-02T15:26:41.000Z'
    data:
      status: closed
    id: 64299eb1c9f3ac3c7549cfe4
    type: status-change
  author: Pi3141
  created_at: 2023-04-02 14:26:41+00:00
  id: 64299eb1c9f3ac3c7549cfe4
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: Pi3141/alpaca-lora-30B-ggml
repo_type: model
status: closed
target_branch: null
title: How do you run ggml-model-q4_1.bin in new llama.cpp?
