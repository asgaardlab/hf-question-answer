!!python/object:huggingface_hub.community.DiscussionWithDetails
author: plowthat1998
conflicting_files: null
created_at: 2023-11-17 22:53:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b6e110507cda5d2df4d5f5fdf21efdfd.svg
      fullname: Darren Wright
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: plowthat1998
      type: user
    createdAt: '2023-11-17T22:53:38.000Z'
    data:
      edited: false
      editors:
      - plowthat1998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9473291039466858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b6e110507cda5d2df4d5f5fdf21efdfd.svg
          fullname: Darren Wright
          isHf: false
          isPro: false
          name: plowthat1998
          type: user
        html: '<p>I''m confused as to whether or not I''m supposed to be downloading
          the entire model library for this model, and if so what the ram requirements
          are? and if it is the whole model library, how to I put the safetensors
          files together to use them? or if it has different options for the download.</p>

          '
        raw: I'm confused as to whether or not I'm supposed to be downloading the
          entire model library for this model, and if so what the ram requirements
          are? and if it is the whole model library, how to I put the safetensors
          files together to use them? or if it has different options for the download.
        updatedAt: '2023-11-17T22:53:38.564Z'
      numEdits: 0
      reactions: []
    id: 6557eef2ec17c88302713ff9
    type: comment
  author: plowthat1998
  content: I'm confused as to whether or not I'm supposed to be downloading the entire
    model library for this model, and if so what the ram requirements are? and if
    it is the whole model library, how to I put the safetensors files together to
    use them? or if it has different options for the download.
  created_at: 2023-11-17 22:53:38+00:00
  edited: false
  hidden: false
  id: 6557eef2ec17c88302713ff9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-11-18T06:50:40.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.925818920135498
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<p>Correct, this repo contains the entire model in fp16 precision.
          It''s intended for further conversions, as you''d need crazy hardware to
          run this model in fp16.</p>

          <p>If you are asking these questions, you probably shouldn''t be downloading
          this.</p>

          <p>This repo contains the quantized model intended to run locally (provided
          you have 2x 3090 or 4090, or an A6000:<br><a href="https://huggingface.co/Doctor-Shotgun/lzlv-limarpv3-l2-70b-exl2">https://huggingface.co/Doctor-Shotgun/lzlv-limarpv3-l2-70b-exl2</a></p>

          '
        raw: 'Correct, this repo contains the entire model in fp16 precision. It''s
          intended for further conversions, as you''d need crazy hardware to run this
          model in fp16.


          If you are asking these questions, you probably shouldn''t be downloading
          this.


          This repo contains the quantized model intended to run locally (provided
          you have 2x 3090 or 4090, or an A6000:

          https://huggingface.co/Doctor-Shotgun/lzlv-limarpv3-l2-70b-exl2'
        updatedAt: '2023-11-18T06:50:40.438Z'
      numEdits: 0
      reactions: []
    id: 65585ec0bf604a77321e5f09
    type: comment
  author: Doctor-Shotgun
  content: 'Correct, this repo contains the entire model in fp16 precision. It''s
    intended for further conversions, as you''d need crazy hardware to run this model
    in fp16.


    If you are asking these questions, you probably shouldn''t be downloading this.


    This repo contains the quantized model intended to run locally (provided you have
    2x 3090 or 4090, or an A6000:

    https://huggingface.co/Doctor-Shotgun/lzlv-limarpv3-l2-70b-exl2'
  created_at: 2023-11-18 06:50:40+00:00
  edited: false
  hidden: false
  id: 65585ec0bf604a77321e5f09
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/b6e110507cda5d2df4d5f5fdf21efdfd.svg
      fullname: Darren Wright
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: plowthat1998
      type: user
    createdAt: '2023-11-18T07:25:18.000Z'
    data:
      edited: false
      editors:
      - plowthat1998
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9223741292953491
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/b6e110507cda5d2df4d5f5fdf21efdfd.svg
          fullname: Darren Wright
          isHf: false
          isPro: false
          name: plowthat1998
          type: user
        html: "<p>Is there a version of this that\u2019s intended to run locally on\
          \ ram and vram?</p>\n"
        raw: "Is there a version of this that\u2019s intended to run locally on ram\
          \ and vram?"
        updatedAt: '2023-11-18T07:25:18.780Z'
      numEdits: 0
      reactions: []
    id: 655866de4cd8d448658ff0da
    type: comment
  author: plowthat1998
  content: "Is there a version of this that\u2019s intended to run locally on ram\
    \ and vram?"
  created_at: 2023-11-18 07:25:18+00:00
  edited: false
  hidden: false
  id: 655866de4cd8d448658ff0da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
      fullname: Doctor Shotgun
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Doctor-Shotgun
      type: user
    createdAt: '2023-11-19T04:50:21.000Z'
    data:
      edited: false
      editors:
      - Doctor-Shotgun
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9822460412979126
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670736706483-632b9f9866f28bf34ae85487.jpeg?w=200&h=200&f=face
          fullname: Doctor Shotgun
          isHf: false
          isPro: false
          name: Doctor-Shotgun
          type: user
        html: '<p>You''d want a GGUF version for that, although it will run extremely
          slowly if it doesn''t fit entirely on vram.</p>

          '
        raw: You'd want a GGUF version for that, although it will run extremely slowly
          if it doesn't fit entirely on vram.
        updatedAt: '2023-11-19T04:50:21.623Z'
      numEdits: 0
      reactions: []
    id: 6559940d029b03128bee93c1
    type: comment
  author: Doctor-Shotgun
  content: You'd want a GGUF version for that, although it will run extremely slowly
    if it doesn't fit entirely on vram.
  created_at: 2023-11-19 04:50:21+00:00
  edited: false
  hidden: false
  id: 6559940d029b03128bee93c1
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: Doctor-Shotgun/lzlv-limarpv3-l2-70b
repo_type: model
status: open
target_branch: null
title: am I supposed to download the entire thing?
