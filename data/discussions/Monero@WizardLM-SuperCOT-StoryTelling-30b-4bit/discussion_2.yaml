!!python/object:huggingface_hub.community.DiscussionWithDetails
author: cleverest
conflicting_files: null
created_at: 2023-06-04 17:38:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-06-04T18:38:15.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5005464553833008
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: "<p>IDEAS on how to fix this?</p>\n<p>I get this error:</p>\n<p>Traceback\
          \ (most recent call last): File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 68, in load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) File \u201CC:\\Users\\\
          cleverest\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201CC:\\\
          Users\\cleverest\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 275, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041,\
          \ in load_state_dict raise RuntimeError(\u2018Error(s) in loading state_dict\
          \ for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM: size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for</p>\n"
        raw: "IDEAS on how to fix this?\r\n\r\nI get this error:\r\n\r\nTraceback\
          \ (most recent call last): File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
          text-generation-webui\\server.py\u201D, line 68, in load_model_wrapper shared.model,\
          \ shared.tokenizer = load_model(shared.model_name) File \u201CC:\\Users\\\
          cleverest\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 95, in load_model output = load_func(model_name) File \u201CC:\\\
          Users\\cleverest\\oobabooga_windows\\text-generation-webui\\modules\\models.py\u201D\
          , line 275, in GPTQ_loader model = modules.GPTQ_loader.load_quantized(model_name)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 177, in load_quantized model = load_quant(str(path_to_model),\
          \ str(pt_path), shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
          \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
          modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
          \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
          env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041,\
          \ in load_state_dict raise RuntimeError(\u2018Error(s) in loading state_dict\
          \ for {}:\\n\\t{}\u2019.format( RuntimeError: Error(s) in loading state_dict\
          \ for LlamaForCausalLM: size mismatch for model.layers.0.self_attn.k_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.k_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
          \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the\
          \ shape in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
          \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape\
          \ in current model is torch.Size([52, 832]). size mismatch for"
        updatedAt: '2023-06-04T18:38:15.053Z'
      numEdits: 0
      reactions: []
    id: 647cda1792182942d7c67cf6
    type: comment
  author: cleverest
  content: "IDEAS on how to fix this?\r\n\r\nI get this error:\r\n\r\nTraceback (most\
    \ recent call last): File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    server.py\u201D, line 68, in load_model_wrapper shared.model, shared.tokenizer\
    \ = load_model(shared.model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 95, in load_model output\
    \ = load_func(model_name) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\u201D, line 275, in GPTQ_loader model\
    \ = modules.GPTQ_loader.load_quantized(model_name) File \u201CC:\\Users\\cleverest\\\
    oobabooga_windows\\text-generation-webui\\modules\\GPTQ_loader.py\u201D, line\
    \ 177, in load_quantized model = load_quant(str(path_to_model), str(pt_path),\
    \ shared.args.wbits, shared.args.groupsize, kernel_switch_threshold=threshold)\
    \ File \u201CC:\\Users\\cleverest\\oobabooga_windows\\text-generation-webui\\\
    modules\\GPTQ_loader.py\u201D, line 84, in _load_quant model.load_state_dict(safe_load(checkpoint),\
    \ strict=False) File \u201CC:\\Users\\cleverest\\oobabooga_windows\\installer_files\\\
    env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201D, line 2041, in load_state_dict\
    \ raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019\
    .format( RuntimeError: Error(s) in loading state_dict for LlamaForCausalLM: size\
    \ mismatch for model.layers.0.self_attn.k_proj.qzeros: copying a param with shape\
    \ torch.Size([1, 832]) from checkpoint, the shape in current model is torch.Size([52,\
    \ 832]). size mismatch for model.layers.0.self_attn.k_proj.scales: copying a param\
    \ with shape torch.Size([1, 6656]) from checkpoint, the shape in current model\
    \ is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.o_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for model.layers.0.self_attn.o_proj.scales:\
    \ copying a param with shape torch.Size([1, 6656]) from checkpoint, the shape\
    \ in current model is torch.Size([52, 6656]). size mismatch for model.layers.0.self_attn.q_proj.qzeros:\
    \ copying a param with shape torch.Size([1, 832]) from checkpoint, the shape in\
    \ current model is torch.Size([52, 832]). size mismatch for"
  created_at: 2023-06-04 17:38:15+00:00
  edited: false
  hidden: false
  id: 647cda1792182942d7c67cf6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-04T18:40:55.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9744030237197876
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<p>I''ve seen similar errors when the group size wasn''t set correctly,
          make sure it''s set to 128</p>

          '
        raw: I've seen similar errors when the group size wasn't set correctly, make
          sure it's set to 128
        updatedAt: '2023-06-04T18:40:55.505Z'
      numEdits: 0
      reactions: []
    id: 647cdab760dfe0f35d5aa057
    type: comment
  author: Monero
  content: I've seen similar errors when the group size wasn't set correctly, make
    sure it's set to 128
  created_at: 2023-06-04 17:40:55+00:00
  edited: false
  hidden: false
  id: 647cdab760dfe0f35d5aa057
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-06-04T18:53:42.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9954414367675781
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>Ah the name didn''t have 128 in it so I didn''t even bother... I
          left home, I''ll try it later, thanks</p>

          '
        raw: Ah the name didn't have 128 in it so I didn't even bother... I left home,
          I'll try it later, thanks
        updatedAt: '2023-06-04T18:53:42.907Z'
      numEdits: 0
      reactions: []
    id: 647cddb660dfe0f35d5b3000
    type: comment
  author: cleverest
  content: Ah the name didn't have 128 in it so I didn't even bother... I left home,
    I'll try it later, thanks
  created_at: 2023-06-04 17:53:42+00:00
  edited: false
  hidden: false
  id: 647cddb660dfe0f35d5b3000
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
      fullname: YellowRoseCx
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: Monero
      type: user
    createdAt: '2023-06-04T21:27:56.000Z'
    data:
      edited: false
      editors:
      - Monero
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9885315299034119
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63d625232e397d9f8e1eccac/AOZv_jnPhcj9t6thSs11d.png?w=200&h=200&f=face
          fullname: YellowRoseCx
          isHf: false
          isPro: false
          name: Monero
          type: user
        html: '<blockquote>

          <p>Ah the name didn''t have 128 in it so I didn''t even bother... I left
          home, I''ll try it later, thanks</p>

          </blockquote>

          <p>the filename does though! :) hope it works</p>

          '
        raw: '> Ah the name didn''t have 128 in it so I didn''t even bother... I left
          home, I''ll try it later, thanks


          the filename does though! :) hope it works'
        updatedAt: '2023-06-04T21:27:56.441Z'
      numEdits: 0
      reactions: []
    id: 647d01dc60dfe0f35d5f418e
    type: comment
  author: Monero
  content: '> Ah the name didn''t have 128 in it so I didn''t even bother... I left
    home, I''ll try it later, thanks


    the filename does though! :) hope it works'
  created_at: 2023-06-04 20:27:56+00:00
  edited: false
  hidden: false
  id: 647d01dc60dfe0f35d5f418e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
      fullname: Brett S
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: cleverest
      type: user
    createdAt: '2023-06-05T01:24:34.000Z'
    data:
      edited: false
      editors:
      - cleverest
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9697774052619934
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1f437f318770edea28ec92d7cb14bd73.svg
          fullname: Brett S
          isHf: false
          isPro: false
          name: cleverest
          type: user
        html: '<p>Yup, that fixed it. Thanks! Is there any chance of getting a non-128G
          model of this model at some point?</p>

          '
        raw: Yup, that fixed it. Thanks! Is there any chance of getting a non-128G
          model of this model at some point?
        updatedAt: '2023-06-05T01:24:34.252Z'
      numEdits: 0
      reactions: []
    id: 647d395283c62f32492ea532
    type: comment
  author: cleverest
  content: Yup, that fixed it. Thanks! Is there any chance of getting a non-128G model
    of this model at some point?
  created_at: 2023-06-05 00:24:34+00:00
  edited: false
  hidden: false
  id: 647d395283c62f32492ea532
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: Monero/WizardLM-SuperCOT-StoryTelling-30b-4bit
repo_type: model
status: open
target_branch: null
title: 4090 test with OobaBooga (In Windows) fails to load the model
