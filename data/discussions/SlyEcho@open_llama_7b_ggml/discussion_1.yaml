!!python/object:huggingface_hub.community.DiscussionWithDetails
author: flyingkiwiguy
conflicting_files: null
created_at: 2023-06-08 10:12:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
      fullname: Gary Mulder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flyingkiwiguy
      type: user
    createdAt: '2023-06-08T11:12:00.000Z'
    data:
      edited: false
      editors:
      - flyingkiwiguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7084664106369019
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
          fullname: Gary Mulder
          isHf: false
          isPro: false
          name: flyingkiwiguy
          type: user
        html: '<ol>

          <li>Perplexities calculated using <code>build = 635 (5c64a09)</code> of
          <a rel="nofollow" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>
          and the first <a rel="nofollow" href="https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/">406
          lines of wiki.test.raw</a></li>

          <li>Previous perplexity benchmarking for llamas indicated that 406 lines
          is enough to compare different sizes and quantization levels</li>

          </ol>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/630498d66dbbb80f16360886/NKRne47bqYUWtBSNutev4.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/630498d66dbbb80f16360886/NKRne47bqYUWtBSNutev4.png"></a></p>

          '
        raw: "1. Perplexities calculated using `build = 635 (5c64a09)` of [llama.cpp](https://github.com/ggerganov/llama.cpp)\
          \ and the first [406 lines of wiki.test.raw](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\
          \n2. Previous perplexity benchmarking for llamas indicated that 406 lines\
          \ is enough to compare different sizes and quantization levels\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630498d66dbbb80f16360886/NKRne47bqYUWtBSNutev4.png)\r\
          \n"
        updatedAt: '2023-06-08T11:12:00.990Z'
      numEdits: 0
      reactions: []
    id: 6481b7808af4675862f17aa2
    type: comment
  author: flyingkiwiguy
  content: "1. Perplexities calculated using `build = 635 (5c64a09)` of [llama.cpp](https://github.com/ggerganov/llama.cpp)\
    \ and the first [406 lines of wiki.test.raw](https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/)\r\
    \n2. Previous perplexity benchmarking for llamas indicated that 406 lines is enough\
    \ to compare different sizes and quantization levels\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/630498d66dbbb80f16360886/NKRne47bqYUWtBSNutev4.png)\r\
    \n"
  created_at: 2023-06-08 10:12:00+00:00
  edited: false
  hidden: false
  id: 6481b7808af4675862f17aa2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
      fullname: Gary Mulder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flyingkiwiguy
      type: user
    createdAt: '2023-06-08T11:15:13.000Z'
    data:
      from: Perplexity scores for a Herd of 7B LLamas
      to: Perplexity scores for a Herd of 7B Llamas
    id: 6481b84117f2fba0008aeb19
    type: title-change
  author: flyingkiwiguy
  created_at: 2023-06-08 10:15:13+00:00
  id: 6481b84117f2fba0008aeb19
  new_title: Perplexity scores for a Herd of 7B Llamas
  old_title: Perplexity scores for a Herd of 7B LLamas
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6461e6d29ca00a1b9af42a8f4549cd63.svg
      fullname: Henri Vasserman
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: SlyEcho
      type: user
    createdAt: '2023-06-08T15:54:41.000Z'
    data:
      edited: true
      editors:
      - SlyEcho
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8919020891189575
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6461e6d29ca00a1b9af42a8f4549cd63.svg
          fullname: Henri Vasserman
          isHf: false
          isPro: false
          name: SlyEcho
          type: user
        html: '<p>Full perplexity on <code>wiki.test.raw</code> is under 7.0 (just
          barely) for the F16 version.</p>

          <p><del>I will have all the numbers shortly.</del></p>

          <p>Numbers are now up in the README.</p>

          '
        raw: 'Full perplexity on `wiki.test.raw` is under 7.0 (just barely) for the
          F16 version.


          ~~I will have all the numbers shortly.~~


          Numbers are now up in the README.'
        updatedAt: '2023-06-08T19:25:49.849Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - flyingkiwiguy
        - Jeximo
    id: 6481f9c13ce52056e9389a4b
    type: comment
  author: SlyEcho
  content: 'Full perplexity on `wiki.test.raw` is under 7.0 (just barely) for the
    F16 version.


    ~~I will have all the numbers shortly.~~


    Numbers are now up in the README.'
  created_at: 2023-06-08 14:54:41+00:00
  edited: true
  hidden: false
  id: 6481f9c13ce52056e9389a4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a9b109d166d851ee3d6710a764243d13.svg
      fullname: Ian G
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ReadySetFly
      type: user
    createdAt: '2023-06-08T16:54:52.000Z'
    data:
      edited: false
      editors:
      - ReadySetFly
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.947068989276886
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a9b109d166d851ee3d6710a764243d13.svg
          fullname: Ian G
          isHf: false
          isPro: false
          name: ReadySetFly
          type: user
        html: '<p>Could you use dashed lines to separate the classes of the quantized
          models, e.g. "---" for q8, "-.-" for q5, "-..-" for q4, etc. It''s hard
          for me to tell which line is which from the colors.</p>

          '
        raw: Could you use dashed lines to separate the classes of the quantized models,
          e.g. "---" for q8, "-.-" for q5, "-..-" for q4, etc. It's hard for me to
          tell which line is which from the colors.
        updatedAt: '2023-06-08T16:54:52.834Z'
      numEdits: 0
      reactions: []
    id: 648207dcb89166d339784034
    type: comment
  author: ReadySetFly
  content: Could you use dashed lines to separate the classes of the quantized models,
    e.g. "---" for q8, "-.-" for q5, "-..-" for q4, etc. It's hard for me to tell
    which line is which from the colors.
  created_at: 2023-06-08 15:54:52+00:00
  edited: false
  hidden: false
  id: 648207dcb89166d339784034
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
      fullname: Gary Mulder
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: flyingkiwiguy
      type: user
    createdAt: '2023-06-08T17:04:25.000Z'
    data:
      edited: false
      editors:
      - flyingkiwiguy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416240453720093
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f8665db99de3c4a4e62bc99c0f5d3b19.svg
          fullname: Gary Mulder
          isHf: false
          isPro: false
          name: flyingkiwiguy
          type: user
        html: '<blockquote>

          <p>Could you use dashed lines to separate the classes of the quantized models,
          e.g. "---" for q8, "-.-" for q5, "-..-" for q4, etc. It''s hard for me to
          tell which line is which from the colors.</p>

          </blockquote>

          <p>If it helps the legend is ordered from highest to lowest perplexities.
          If you can''t tell the difference between the lines, you can assume there''s
          not much observable difference in the quality of the models.</p>

          <p>I published a more general plot here:</p>

          <p><a rel="nofollow" href="https://github.com/openlm-research/open_llama/discussions/41#discussion-5277699">https://github.com/openlm-research/open_llama/discussions/41#discussion-5277699</a></p>

          <p>Hugging Face didn''t allow me to upload the .csv file the plot is generated
          from.</p>

          '
        raw: '> Could you use dashed lines to separate the classes of the quantized
          models, e.g. "---" for q8, "-.-" for q5, "-..-" for q4, etc. It''s hard
          for me to tell which line is which from the colors.


          If it helps the legend is ordered from highest to lowest perplexities. If
          you can''t tell the difference between the lines, you can assume there''s
          not much observable difference in the quality of the models.


          I published a more general plot here:


          https://github.com/openlm-research/open_llama/discussions/41#discussion-5277699


          Hugging Face didn''t allow me to upload the .csv file the plot is generated
          from.'
        updatedAt: '2023-06-08T17:04:25.188Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ReadySetFly
    id: 64820a199d9941bade72efe7
    type: comment
  author: flyingkiwiguy
  content: '> Could you use dashed lines to separate the classes of the quantized
    models, e.g. "---" for q8, "-.-" for q5, "-..-" for q4, etc. It''s hard for me
    to tell which line is which from the colors.


    If it helps the legend is ordered from highest to lowest perplexities. If you
    can''t tell the difference between the lines, you can assume there''s not much
    observable difference in the quality of the models.


    I published a more general plot here:


    https://github.com/openlm-research/open_llama/discussions/41#discussion-5277699


    Hugging Face didn''t allow me to upload the .csv file the plot is generated from.'
  created_at: 2023-06-08 16:04:25+00:00
  edited: false
  hidden: false
  id: 64820a199d9941bade72efe7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
      fullname: Jack Jollimore
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Jeximo
      type: user
    createdAt: '2023-06-08T18:38:58.000Z'
    data:
      edited: true
      editors:
      - Jeximo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9795271754264832
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64554c42273f649830277263/A7Gpv_D00XZwZyJyGdUZ_.jpeg?w=200&h=200&f=face
          fullname: Jack Jollimore
          isHf: false
          isPro: false
          name: Jeximo
          type: user
        html: '<p>Thanks for the scores, that''s very helpful!</p>

          '
        raw: Thanks for the scores, that's very helpful!
        updatedAt: '2023-06-08T18:39:26.310Z'
      numEdits: 1
      reactions: []
    id: 64822042d88db1bf3579b96b
    type: comment
  author: Jeximo
  content: Thanks for the scores, that's very helpful!
  created_at: 2023-06-08 17:38:58+00:00
  edited: true
  hidden: false
  id: 64822042d88db1bf3579b96b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: SlyEcho/open_llama_7b_ggml
repo_type: model
status: open
target_branch: null
title: Perplexity scores for a Herd of 7B Llamas
