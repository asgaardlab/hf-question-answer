!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ChrisTorng
conflicting_files: null
created_at: 2023-10-23 13:08:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/369f4b4407bb637e4466b36030b4727b.svg
      fullname: Chris Torng
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ChrisTorng
      type: user
    createdAt: '2023-10-23T14:08:12.000Z'
    data:
      edited: true
      editors:
      - ChrisTorng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7973271012306213
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/369f4b4407bb637e4466b36030b4727b.svg
          fullname: Chris Torng
          isHf: false
          isPro: false
          name: ChrisTorng
          type: user
        html: '<p>I''ve tried Taiwan LLM ChatUI, it''s result is good.</p>

          <p>I am using Text generation web UI with audreyt/Taiwan-LLM-7B-v2.0.1-chat-GGUF
          Q2 locally, with 3070 8GB, but can''t get well response. Sometimes it runs
          for a very long time to generate the first token (about several mins), but
          later fast. Sometimes it generate empty response.</p>

          <p>I''m new to LLMs. I think the problem is not mainly on Q2, should be
          on the basic parameters. I need some help for the proper settings.<br>Does
          the following right?</p>

          <p>Model:<br>Model loader: ctransformers<br>n-gpu-layers: 128 (max)<br>n_ctx:
          4096 (default)<br>threads: 4 (physical CPU cores)<br>n_batch: 512 (default)<br>model_type:
          llama<br>no-mmap/mlock: checked</p>

          <p>Instruction template: Vicuna-v1.1<br>User string: USER: (default)<br>Bot
          string: ASSISTANT: (default)<br>Context: (my objective in Traditional Chinese)<br>Turn
          template: &lt;|user|&gt; &lt;|user-message|&gt;\n&lt;|bot|&gt; &lt;|bot-message|&gt;\n
          (default)<br>Command for chat-instruct mode: (default)<br>Continue the chat
          dialogue below. Write a single reply for the character "&lt;|character|&gt;".</p>

          <p>&lt;|prompt|&gt;</p>

          <p>Chat:<br>Mode: instruct (not chat/chat-instruct)</p>

          <p>Others keep as default.</p>

          <p>Thanks for your kindly help.</p>

          '
        raw: 'I''ve tried Taiwan LLM ChatUI, it''s result is good.


          I am using Text generation web UI with audreyt/Taiwan-LLM-7B-v2.0.1-chat-GGUF
          Q2 locally, with 3070 8GB, but can''t get well response. Sometimes it runs
          for a very long time to generate the first token (about several mins), but
          later fast. Sometimes it generate empty response.


          I''m new to LLMs. I think the problem is not mainly on Q2, should be on
          the basic parameters. I need some help for the proper settings.

          Does the following right?


          Model:

          Model loader: ctransformers

          n-gpu-layers: 128 (max)

          n_ctx: 4096 (default)

          threads: 4 (physical CPU cores)

          n_batch: 512 (default)

          model_type: llama

          no-mmap/mlock: checked


          Instruction template: Vicuna-v1.1

          User string: USER: (default)

          Bot string: ASSISTANT: (default)

          Context: (my objective in Traditional Chinese)

          Turn template: <|user|> <|user-message|>\n<|bot|> <|bot-message|></s>\n
          (default)

          Command for chat-instruct mode: (default)

          Continue the chat dialogue below. Write a single reply for the character
          "<|character|>".


          <|prompt|>


          Chat:

          Mode: instruct (not chat/chat-instruct)


          Others keep as default.


          Thanks for your kindly help.'
        updatedAt: '2023-10-23T14:30:19.130Z'
      numEdits: 1
      reactions: []
    id: 65367e4c609e9d020f22f2d3
    type: comment
  author: ChrisTorng
  content: 'I''ve tried Taiwan LLM ChatUI, it''s result is good.


    I am using Text generation web UI with audreyt/Taiwan-LLM-7B-v2.0.1-chat-GGUF
    Q2 locally, with 3070 8GB, but can''t get well response. Sometimes it runs for
    a very long time to generate the first token (about several mins), but later fast.
    Sometimes it generate empty response.


    I''m new to LLMs. I think the problem is not mainly on Q2, should be on the basic
    parameters. I need some help for the proper settings.

    Does the following right?


    Model:

    Model loader: ctransformers

    n-gpu-layers: 128 (max)

    n_ctx: 4096 (default)

    threads: 4 (physical CPU cores)

    n_batch: 512 (default)

    model_type: llama

    no-mmap/mlock: checked


    Instruction template: Vicuna-v1.1

    User string: USER: (default)

    Bot string: ASSISTANT: (default)

    Context: (my objective in Traditional Chinese)

    Turn template: <|user|> <|user-message|>\n<|bot|> <|bot-message|></s>\n (default)

    Command for chat-instruct mode: (default)

    Continue the chat dialogue below. Write a single reply for the character "<|character|>".


    <|prompt|>


    Chat:

    Mode: instruct (not chat/chat-instruct)


    Others keep as default.


    Thanks for your kindly help.'
  created_at: 2023-10-23 13:08:12+00:00
  edited: true
  hidden: false
  id: 65367e4c609e9d020f22f2d3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
      fullname: Yen-Ting Lin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yentinglin
      type: user
    createdAt: '2023-10-24T12:55:09.000Z'
    data:
      edited: false
      editors:
      - yentinglin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7411835789680481
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
          fullname: Yen-Ting Lin
          isHf: false
          isPro: false
          name: yentinglin
          type: user
        html: "<p>The default prompt template for Taiwan LLM v2 is </p>\n<pre><code>\u4F60\
          \u662F\u4EBA\u5DE5\u667A\u6167\u52A9\u7406\uFF0C\u4EE5\u4E0B\u662F\u7528\
          \u6236\u548C\u4EBA\u5DE5\u667A\u80FD\u52A9\u7406\u4E4B\u9593\u7684\u5C0D\
          \u8A71\u3002\u4F60\u8981\u5C0D\u7528\u6236\u7684\u554F\u984C\u63D0\u4F9B\
          \u6709\u7528\u3001\u5B89\u5168\u3001\u8A73\u7D30\u548C\u79AE\u8C8C\u7684\
          \u56DE\u7B54\u3002USER: {user} ASSISTANT:\n</code></pre>\n<p>It's still\
          \ in the format of vicuna but with my own system instruction. v2 models\
          \ are trained to be following different system instructions so you can adjust\
          \ it to your use cases or even just skip it.</p>\n"
        raw: "The default prompt template for Taiwan LLM v2 is \n```\n\u4F60\u662F\
          \u4EBA\u5DE5\u667A\u6167\u52A9\u7406\uFF0C\u4EE5\u4E0B\u662F\u7528\u6236\
          \u548C\u4EBA\u5DE5\u667A\u80FD\u52A9\u7406\u4E4B\u9593\u7684\u5C0D\u8A71\
          \u3002\u4F60\u8981\u5C0D\u7528\u6236\u7684\u554F\u984C\u63D0\u4F9B\u6709\
          \u7528\u3001\u5B89\u5168\u3001\u8A73\u7D30\u548C\u79AE\u8C8C\u7684\u56DE\
          \u7B54\u3002USER: {user} ASSISTANT:\n```\n\nIt's still in the format of\
          \ vicuna but with my own system instruction. v2 models are trained to be\
          \ following different system instructions so you can adjust it to your use\
          \ cases or even just skip it."
        updatedAt: '2023-10-24T12:55:09.771Z'
      numEdits: 0
      reactions: []
    id: 6537bead4e82cb275da69237
    type: comment
  author: yentinglin
  content: "The default prompt template for Taiwan LLM v2 is \n```\n\u4F60\u662F\u4EBA\
    \u5DE5\u667A\u6167\u52A9\u7406\uFF0C\u4EE5\u4E0B\u662F\u7528\u6236\u548C\u4EBA\
    \u5DE5\u667A\u80FD\u52A9\u7406\u4E4B\u9593\u7684\u5C0D\u8A71\u3002\u4F60\u8981\
    \u5C0D\u7528\u6236\u7684\u554F\u984C\u63D0\u4F9B\u6709\u7528\u3001\u5B89\u5168\
    \u3001\u8A73\u7D30\u548C\u79AE\u8C8C\u7684\u56DE\u7B54\u3002USER: {user} ASSISTANT:\n\
    ```\n\nIt's still in the format of vicuna but with my own system instruction.\
    \ v2 models are trained to be following different system instructions so you can\
    \ adjust it to your use cases or even just skip it."
  created_at: 2023-10-24 11:55:09+00:00
  edited: false
  hidden: false
  id: 6537bead4e82cb275da69237
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
      fullname: Yen-Ting Lin
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: yentinglin
      type: user
    createdAt: '2023-11-02T14:09:16.000Z'
    data:
      edited: false
      editors:
      - yentinglin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5067722797393799
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/5df9c78eda6d0311fd3d541f/K8m3JEmIhH8WfwrgW-3l8.jpeg?w=200&h=200&f=face
          fullname: Yen-Ting Lin
          isHf: false
          isPro: false
          name: yentinglin
          type: user
        html: "<p>from transformers import AutoTokenizer<br>chat = [</p>\n<h1 id=\"\
          role-system-content-\u4F60\u8B1B\u4E2D\u6587\">{\"role\": \"system\", \"\
          content\": \"\u4F60\u8B1B\u4E2D\u6587\"},</h1>\n<p>  {\"role\": \"user\"\
          , \"content\": \"Hello, how are you?\"},<br>  {\"role\": \"assistant\",\
          \ \"content\": \"I'm doing great. How can I help you today?\"},<br>  {\"\
          role\": \"user\", \"content\": \"I'd like to show off how chat templating\
          \ works!\"},<br>]<br>tokenizer = AutoTokenizer.from_pretrained(\"yentinglin/Taiwan-LLM-7B-v2.0.1-chat\"\
          )<br>prompt_for_completed_conversation = tokenizer.apply_chat_template(chat,\
          \ tokenize=False)<br>prompt_for_generation = tokenizer.apply_chat_template(chat,\
          \ tokenize=False, add_generation_prompt=True)</p>\n"
        raw: "from transformers import AutoTokenizer\nchat = [\n  # {\"role\": \"\
          system\", \"content\": \"\u4F60\u8B1B\u4E2D\u6587\"},\n  {\"role\": \"user\"\
          , \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"\
          content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\"\
          : \"user\", \"content\": \"I'd like to show off how chat templating works!\"\
          },\n]\ntokenizer = AutoTokenizer.from_pretrained(\"yentinglin/Taiwan-LLM-7B-v2.0.1-chat\"\
          )\nprompt_for_completed_conversation = tokenizer.apply_chat_template(chat,\
          \ tokenize=False)\nprompt_for_generation = tokenizer.apply_chat_template(chat,\
          \ tokenize=False, add_generation_prompt=True)"
        updatedAt: '2023-11-02T14:09:16.460Z'
      numEdits: 0
      reactions: []
    id: 6543ad8c91a22f5a119803f7
    type: comment
  author: yentinglin
  content: "from transformers import AutoTokenizer\nchat = [\n  # {\"role\": \"system\"\
    , \"content\": \"\u4F60\u8B1B\u4E2D\u6587\"},\n  {\"role\": \"user\", \"content\"\
    : \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing\
    \ great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"\
    I'd like to show off how chat templating works!\"},\n]\ntokenizer = AutoTokenizer.from_pretrained(\"\
    yentinglin/Taiwan-LLM-7B-v2.0.1-chat\")\nprompt_for_completed_conversation = tokenizer.apply_chat_template(chat,\
    \ tokenize=False)\nprompt_for_generation = tokenizer.apply_chat_template(chat,\
    \ tokenize=False, add_generation_prompt=True)"
  created_at: 2023-11-02 13:09:16+00:00
  edited: false
  hidden: false
  id: 6543ad8c91a22f5a119803f7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: yentinglin/Taiwan-LLM-7B-v2.0.1-chat
repo_type: model
status: open
target_branch: null
title: How to setup parameters?
