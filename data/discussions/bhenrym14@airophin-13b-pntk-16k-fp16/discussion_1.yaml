!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-08-14 15:13:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-08-14T16:13:21.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.888009786605835
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>I''m using this model (its GPTQ version) in competition with Airoboros
          lxctx 16384 PI (GGML version).<br>I enjoy your work, Brandon, and it deserves
          more.. attention!<br>Any chance of some GGML versions (QK_4_M or Q5_S/M)
          to surpass the GPTQ quality (which is more in the QK_3 range usually) and
          do the most with your model?<br>Thanks you in any case.</p>

          '
        raw: "I'm using this model (its GPTQ version) in competition with Airoboros\
          \ lxctx 16384 PI (GGML version).\r\nI enjoy your work, Brandon, and it deserves\
          \ more.. attention!\r\nAny chance of some GGML versions (QK_4_M or Q5_S/M)\
          \ to surpass the GPTQ quality (which is more in the QK_3 range usually)\
          \ and do the most with your model?\r\nThanks you in any case."
        updatedAt: '2023-08-14T16:13:21.552Z'
      numEdits: 0
      reactions: []
    id: 64da52a15f144aa29fdbedba
    type: comment
  author: Nexesenex
  content: "I'm using this model (its GPTQ version) in competition with Airoboros\
    \ lxctx 16384 PI (GGML version).\r\nI enjoy your work, Brandon, and it deserves\
    \ more.. attention!\r\nAny chance of some GGML versions (QK_4_M or Q5_S/M) to\
    \ surpass the GPTQ quality (which is more in the QK_3 range usually) and do the\
    \ most with your model?\r\nThanks you in any case."
  created_at: 2023-08-14 15:13:21+00:00
  edited: false
  hidden: false
  id: 64da52a15f144aa29fdbedba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
      fullname: Brandon
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bhenrym14
      type: user
    createdAt: '2023-08-14T17:34:32.000Z'
    data:
      edited: false
      editors:
      - bhenrym14
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9808647036552429
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d7b0073571f4ff7901d38f38258c365d.svg
          fullname: Brandon
          isHf: false
          isPro: false
          name: bhenrym14
          type: user
        html: '<p>I''d be happy to! My only concern is with how the PNTK embeddings
          would work with GGML. I''m just not very familiar with it. Any idea how
          this might work?</p>

          '
        raw: I'd be happy to! My only concern is with how the PNTK embeddings would
          work with GGML. I'm just not very familiar with it. Any idea how this might
          work?
        updatedAt: '2023-08-14T17:34:32.480Z'
      numEdits: 0
      reactions: []
    id: 64da65a8d68a6ddcc79df6f7
    type: comment
  author: bhenrym14
  content: I'd be happy to! My only concern is with how the PNTK embeddings would
    work with GGML. I'm just not very familiar with it. Any idea how this might work?
  created_at: 2023-08-14 16:34:32+00:00
  edited: false
  hidden: false
  id: 64da65a8d68a6ddcc79df6f7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-08-15T01:08:53.000Z'
    data:
      edited: true
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9251337051391602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>I''m not an expert, and the NTK evolving terminology and lack of
          reference documentation "for end users" confuse me a bit.<br>But here''s
          a llama.cpp PR thread (and its appendixes at its bottom) which might be
          of interest for you to get an idea if PNTK is already implemented (whatever
          the name used for it, and this, even in unmerged PRs).</p>

          '
        raw: 'I''m not an expert, and the NTK evolving terminology and lack of reference
          documentation "for end users" confuse me a bit.

          But here''s a llama.cpp PR thread (and its appendixes at its bottom) which
          might be of interest for you to get an idea if PNTK is already implemented
          (whatever the name used for it, and this, even in unmerged PRs).'
        updatedAt: '2023-08-15T01:09:05.318Z'
      numEdits: 1
      reactions: []
    id: 64dad025210fa7992a55f67c
    type: comment
  author: Nexesenex
  content: 'I''m not an expert, and the NTK evolving terminology and lack of reference
    documentation "for end users" confuse me a bit.

    But here''s a llama.cpp PR thread (and its appendixes at its bottom) which might
    be of interest for you to get an idea if PNTK is already implemented (whatever
    the name used for it, and this, even in unmerged PRs).'
  created_at: 2023-08-15 00:08:53+00:00
  edited: true
  hidden: false
  id: 64dad025210fa7992a55f67c
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bhenrym14/airophin-13b-pntk-16k-fp16
repo_type: model
status: open
target_branch: null
title: Any chance for a ggml version to have a better perplexity?
