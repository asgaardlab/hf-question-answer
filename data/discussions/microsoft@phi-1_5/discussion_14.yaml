!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nmd2k
conflicting_files: null
created_at: 2023-09-13 13:06:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668497271187-noauth.jpeg?w=200&h=200&f=face
      fullname: Nguyen Manh Dung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmd2k
      type: user
    createdAt: '2023-09-13T14:06:17.000Z'
    data:
      edited: false
      editors:
      - nmd2k
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6029987335205078
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668497271187-noauth.jpeg?w=200&h=200&f=face
          fullname: Nguyen Manh Dung
          isHf: false
          isPro: false
          name: nmd2k
          type: user
        html: "<p>Hi, I recently fine-tune phi-1.5. However, I wasn't able to load\
          \ its checkpoint. It seem like the config of MixFormerSequentialForCausalLm\
          \ has been modified.</p>\n<p>Details log:</p>\n<pre><code>WARNING:torch.distributed.run:\n\
          *****************************************\nSetting OMP_NUM_THREADS environment\
          \ variable for each process to be 1 in default, to avoid your system being\
          \ overloaded, please further tune the variable for optimal performance in\
          \ your application as needed. \n*****************************************\n\
          [2023-09-13 13:10:40,735] [INFO] [real_accelerator.py:158:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)\n[2023-09-13 13:10:41,210]\
          \ [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator\
          \ to cuda (auto detect)\nUsing the `WANDB_DISABLED` environment variable\
          \ is deprecated and will be removed in v5. Use the --report_to flag to control\
          \ the integrations used for logging result (for instance --report_to none).\n\
          Using the `WANDB_DISABLED` environment variable is deprecated and will be\
          \ removed in v5. Use the --report_to flag to control the integrations used\
          \ for logging result (for instance --report_to none).\nloading configuration\
          \ file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/config.json\nloading\
          \ configuration file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/config.json\n\
          Model config MixFormerSequentialConfig {\n  \"_name_or_path\": \"/datadrive05/dungnm31/Exp/phi15/checkpoint-3450/\"\
          ,\n  \"activation_function\": \"gelu_new\",\n  \"architecture\": {\n   \
          \ \"block_cls\": \"parallel\",\n    \"mixer\": {},\n    \"mlp\": {\n   \
          \   \"mlp_cls\": \"mlp\"\n    }\n  },\n  \"architectures\": [\n    \"MixFormerSequentialForCausalLM\"\
          \n  ],\n  \"auto_map\": {\n    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\"\
          ,\n    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\
          \n  },\n  \"embd_layer\": \"default\",\n  \"embd_pdrop\": 0.0,\n  \"initializer_range\"\
          : 0.02,\n  \"layer_norm_epsilon\": 1e-05,\n  \"model_type\": \"mixformer-sequential\"\
          ,\n  \"n_embd\": 2048,\n  \"n_head\": 32,\n  \"n_inner\": null,\n  \"n_layer\"\
          : 24,\n  \"n_positions\": 2048,\n  \"phyagi_version\": \"0.0.4.dev\",\n\
          \  \"resid_pdrop\": 0.0,\n  \"rotary_dim\": 32,\n  \"tie_word_embeddings\"\
          : false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\":\
          \ \"4.34.0.dev0\",\n  \"vocab_size\": 50304\n}\n\nloading file vocab.json\n\
          loading file merges.txt\nloading file tokenizer.json\nloading file added_tokens.json\n\
          loading file special_tokens_map.json\nloading file tokenizer_config.json\n\
          loading weights file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/pytorch_model.bin\n\
          Generate config GenerationConfig {\n  \"_from_model_config\": true,\n  \"\
          transformers_version\": \"4.34.0.dev0\"\n}\n\nTraceback (most recent call\
          \ last):\n  File \"/datadrive05/dungnm31/inst/main.py\", line 150, in &lt;module&gt;\n\
          \    main()\n  File \"/datadrive05/dungnm31/inst/main.py\", line 91, in\
          \ main\n    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,\n\
          \  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\n    return model_class.from_pretrained(\n\
          \  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 3180, in from_pretrained\n    ) = cls._load_pretrained_model(\n \
          \ File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 3629, in _load_pretrained_model\n    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\
          )\nRuntimeError: Error(s) in loading state_dict for MixFormerSequentialForCausalLM:\n\
          \        size mismatch for layers.0.wte.weight: copying a param with shape\
          \ torch.Size([50296, 2048]) from checkpoint, the shape in current model\
          \ is torch.Size([50304, 2048]).\n        size mismatch for layers.25.linear.weight:\
          \ copying a param with shape torch.Size([50296, 2048]) from checkpoint,\
          \ the shape in current model is torch.Size([50304, 2048]).\n        size\
          \ mismatch for layers.25.linear.bias: copying a param with shape torch.Size([50296])\
          \ from checkpoint, the shape in current model is torch.Size([50304]).\n\
          \        You may consider adding `ignore_mismatched_sizes=True` in the model\
          \ `from_pretrained` method.\n</code></pre>\n"
        raw: "Hi, I recently fine-tune phi-1.5. However, I wasn't able to load its\
          \ checkpoint. It seem like the config of MixFormerSequentialForCausalLm\
          \ has been modified.\r\n\r\nDetails log:\r\n```\r\nWARNING:torch.distributed.run:\r\
          \n*****************************************\r\nSetting OMP_NUM_THREADS environment\
          \ variable for each process to be 1 in default, to avoid your system being\
          \ overloaded, please further tune the variable for optimal performance in\
          \ your application as needed. \r\n*****************************************\r\
          \n[2023-09-13 13:10:40,735] [INFO] [real_accelerator.py:158:get_accelerator]\
          \ Setting ds_accelerator to cuda (auto detect)\r\n[2023-09-13 13:10:41,210]\
          \ [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator\
          \ to cuda (auto detect)\r\nUsing the `WANDB_DISABLED` environment variable\
          \ is deprecated and will be removed in v5. Use the --report_to flag to control\
          \ the integrations used for logging result (for instance --report_to none).\r\
          \nUsing the `WANDB_DISABLED` environment variable is deprecated and will\
          \ be removed in v5. Use the --report_to flag to control the integrations\
          \ used for logging result (for instance --report_to none).\r\nloading configuration\
          \ file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/config.json\r\nloading\
          \ configuration file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/config.json\r\
          \nModel config MixFormerSequentialConfig {\r\n  \"_name_or_path\": \"/datadrive05/dungnm31/Exp/phi15/checkpoint-3450/\"\
          ,\r\n  \"activation_function\": \"gelu_new\",\r\n  \"architecture\": {\r\
          \n    \"block_cls\": \"parallel\",\r\n    \"mixer\": {},\r\n    \"mlp\"\
          : {\r\n      \"mlp_cls\": \"mlp\"\r\n    }\r\n  },\r\n  \"architectures\"\
          : [\r\n    \"MixFormerSequentialForCausalLM\"\r\n  ],\r\n  \"auto_map\"\
          : {\r\n    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\"\
          ,\r\n    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\
          \r\n  },\r\n  \"embd_layer\": \"default\",\r\n  \"embd_pdrop\": 0.0,\r\n\
          \  \"initializer_range\": 0.02,\r\n  \"layer_norm_epsilon\": 1e-05,\r\n\
          \  \"model_type\": \"mixformer-sequential\",\r\n  \"n_embd\": 2048,\r\n\
          \  \"n_head\": 32,\r\n  \"n_inner\": null,\r\n  \"n_layer\": 24,\r\n  \"\
          n_positions\": 2048,\r\n  \"phyagi_version\": \"0.0.4.dev\",\r\n  \"resid_pdrop\"\
          : 0.0,\r\n  \"rotary_dim\": 32,\r\n  \"tie_word_embeddings\": false,\r\n\
          \  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"4.34.0.dev0\"\
          ,\r\n  \"vocab_size\": 50304\r\n}\r\n\r\nloading file vocab.json\r\nloading\
          \ file merges.txt\r\nloading file tokenizer.json\r\nloading file added_tokens.json\r\
          \nloading file special_tokens_map.json\r\nloading file tokenizer_config.json\r\
          \nloading weights file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/pytorch_model.bin\r\
          \nGenerate config GenerationConfig {\r\n  \"_from_model_config\": true,\r\
          \n  \"transformers_version\": \"4.34.0.dev0\"\r\n}\r\n\r\nTraceback (most\
          \ recent call last):\r\n  File \"/datadrive05/dungnm31/inst/main.py\", line\
          \ 150, in <module>\r\n    main()\r\n  File \"/datadrive05/dungnm31/inst/main.py\"\
          , line 91, in main\r\n    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,\r\
          \n  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 558, in from_pretrained\r\n    return model_class.from_pretrained(\r\
          \n  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 3180, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\
          \n  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
          , line 3629, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s)\
          \ in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\
          )\r\nRuntimeError: Error(s) in loading state_dict for MixFormerSequentialForCausalLM:\r\
          \n        size mismatch for layers.0.wte.weight: copying a param with shape\
          \ torch.Size([50296, 2048]) from checkpoint, the shape in current model\
          \ is torch.Size([50304, 2048]).\r\n        size mismatch for layers.25.linear.weight:\
          \ copying a param with shape torch.Size([50296, 2048]) from checkpoint,\
          \ the shape in current model is torch.Size([50304, 2048]).\r\n        size\
          \ mismatch for layers.25.linear.bias: copying a param with shape torch.Size([50296])\
          \ from checkpoint, the shape in current model is torch.Size([50304]).\r\n\
          \        You may consider adding `ignore_mismatched_sizes=True` in the model\
          \ `from_pretrained` method.\r\n```"
        updatedAt: '2023-09-13T14:06:17.048Z'
      numEdits: 0
      reactions: []
    id: 6501c1d992dad11aa5f843c8
    type: comment
  author: nmd2k
  content: "Hi, I recently fine-tune phi-1.5. However, I wasn't able to load its checkpoint.\
    \ It seem like the config of MixFormerSequentialForCausalLm has been modified.\r\
    \n\r\nDetails log:\r\n```\r\nWARNING:torch.distributed.run:\r\n*****************************************\r\
    \nSetting OMP_NUM_THREADS environment variable for each process to be 1 in default,\
    \ to avoid your system being overloaded, please further tune the variable for\
    \ optimal performance in your application as needed. \r\n*****************************************\r\
    \n[2023-09-13 13:10:40,735] [INFO] [real_accelerator.py:158:get_accelerator] Setting\
    \ ds_accelerator to cuda (auto detect)\r\n[2023-09-13 13:10:41,210] [INFO] [real_accelerator.py:158:get_accelerator]\
    \ Setting ds_accelerator to cuda (auto detect)\r\nUsing the `WANDB_DISABLED` environment\
    \ variable is deprecated and will be removed in v5. Use the --report_to flag to\
    \ control the integrations used for logging result (for instance --report_to none).\r\
    \nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed\
    \ in v5. Use the --report_to flag to control the integrations used for logging\
    \ result (for instance --report_to none).\r\nloading configuration file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/config.json\r\
    \nloading configuration file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/config.json\r\
    \nModel config MixFormerSequentialConfig {\r\n  \"_name_or_path\": \"/datadrive05/dungnm31/Exp/phi15/checkpoint-3450/\"\
    ,\r\n  \"activation_function\": \"gelu_new\",\r\n  \"architecture\": {\r\n   \
    \ \"block_cls\": \"parallel\",\r\n    \"mixer\": {},\r\n    \"mlp\": {\r\n   \
    \   \"mlp_cls\": \"mlp\"\r\n    }\r\n  },\r\n  \"architectures\": [\r\n    \"\
    MixFormerSequentialForCausalLM\"\r\n  ],\r\n  \"auto_map\": {\r\n    \"AutoConfig\"\
    : \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\"\
    ,\r\n    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\
    \r\n  },\r\n  \"embd_layer\": \"default\",\r\n  \"embd_pdrop\": 0.0,\r\n  \"initializer_range\"\
    : 0.02,\r\n  \"layer_norm_epsilon\": 1e-05,\r\n  \"model_type\": \"mixformer-sequential\"\
    ,\r\n  \"n_embd\": 2048,\r\n  \"n_head\": 32,\r\n  \"n_inner\": null,\r\n  \"\
    n_layer\": 24,\r\n  \"n_positions\": 2048,\r\n  \"phyagi_version\": \"0.0.4.dev\"\
    ,\r\n  \"resid_pdrop\": 0.0,\r\n  \"rotary_dim\": 32,\r\n  \"tie_word_embeddings\"\
    : false,\r\n  \"torch_dtype\": \"bfloat16\",\r\n  \"transformers_version\": \"\
    4.34.0.dev0\",\r\n  \"vocab_size\": 50304\r\n}\r\n\r\nloading file vocab.json\r\
    \nloading file merges.txt\r\nloading file tokenizer.json\r\nloading file added_tokens.json\r\
    \nloading file special_tokens_map.json\r\nloading file tokenizer_config.json\r\
    \nloading weights file /datadrive05/dungnm31/Exp/phi15/checkpoint-3450/pytorch_model.bin\r\
    \nGenerate config GenerationConfig {\r\n  \"_from_model_config\": true,\r\n  \"\
    transformers_version\": \"4.34.0.dev0\"\r\n}\r\n\r\nTraceback (most recent call\
    \ last):\r\n  File \"/datadrive05/dungnm31/inst/main.py\", line 150, in <module>\r\
    \n    main()\r\n  File \"/datadrive05/dungnm31/inst/main.py\", line 91, in main\r\
    \n    model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path,\r\
    \n  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 558, in from_pretrained\r\n    return model_class.from_pretrained(\r\n\
    \  File \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 3180, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File\
    \ \"/home/dungnm31/.local/lib/python3.9/site-packages/transformers/modeling_utils.py\"\
    , line 3629, in _load_pretrained_model\r\n    raise RuntimeError(f\"Error(s) in\
    \ loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\")\r\nRuntimeError:\
    \ Error(s) in loading state_dict for MixFormerSequentialForCausalLM:\r\n     \
    \   size mismatch for layers.0.wte.weight: copying a param with shape torch.Size([50296,\
    \ 2048]) from checkpoint, the shape in current model is torch.Size([50304, 2048]).\r\
    \n        size mismatch for layers.25.linear.weight: copying a param with shape\
    \ torch.Size([50296, 2048]) from checkpoint, the shape in current model is torch.Size([50304,\
    \ 2048]).\r\n        size mismatch for layers.25.linear.bias: copying a param\
    \ with shape torch.Size([50296]) from checkpoint, the shape in current model is\
    \ torch.Size([50304]).\r\n        You may consider adding `ignore_mismatched_sizes=True`\
    \ in the model `from_pretrained` method.\r\n```"
  created_at: 2023-09-13 13:06:17+00:00
  edited: false
  hidden: false
  id: 6501c1d992dad11aa5f843c8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1668497271187-noauth.jpeg?w=200&h=200&f=face
      fullname: Nguyen Manh Dung
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nmd2k
      type: user
    createdAt: '2023-09-13T14:36:24.000Z'
    data:
      status: closed
    id: 6501c8e8d39d2f5b06409fe9
    type: status-change
  author: nmd2k
  created_at: 2023-09-13 13:36:24+00:00
  id: 6501c8e8d39d2f5b06409fe9
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: Error(s) in loading state_dict for MixFormerSequentialForCausalLm'
