!!python/object:huggingface_hub.community.DiscussionWithDetails
author: has-c
conflicting_files: null
created_at: 2023-10-01 08:38:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dac7b77bce736f4bc090debad5519c06.svg
      fullname: Hasnain
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: has-c
      type: user
    createdAt: '2023-10-01T09:38:50.000Z'
    data:
      edited: false
      editors:
      - has-c
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9321691393852234
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dac7b77bce736f4bc090debad5519c06.svg
          fullname: Hasnain
          isHf: false
          isPro: false
          name: has-c
          type: user
        html: '<p>As title says, is there any plan to convert this to gguf? </p>

          '
        raw: 'As title says, is there any plan to convert this to gguf? '
        updatedAt: '2023-10-01T09:38:50.234Z'
      numEdits: 0
      reactions: []
    id: 65193e2a0d365fb7761ab62d
    type: comment
  author: has-c
  content: 'As title says, is there any plan to convert this to gguf? '
  created_at: 2023-10-01 08:38:50+00:00
  edited: false
  hidden: false
  id: 65193e2a0d365fb7761ab62d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42d5c75b2f876c189453f4b3417e49b3.svg
      fullname: sopack
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: sopack
      type: user
    createdAt: '2023-10-03T19:18:51.000Z'
    data:
      edited: false
      editors:
      - sopack
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4795861840248108
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42d5c75b2f876c189453f4b3417e49b3.svg
          fullname: sopack
          isHf: false
          isPro: false
          name: sopack
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> </p>\n"
        raw: '@TheBloke '
        updatedAt: '2023-10-03T19:18:51.218Z'
      numEdits: 0
      reactions:
      - count: 5
        reaction: "\U0001F44D"
        users:
        - has-c
        - Raspbfox
        - Arcturuz
        - Miltos
        - ricofix
    id: 651c691bc51efebee1717e2e
    type: comment
  author: sopack
  content: '@TheBloke '
  created_at: 2023-10-03 18:18:51+00:00
  edited: false
  hidden: false
  id: 651c691bc51efebee1717e2e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454aff9273f649830234978/cvVV08YHJpJx9xWVZqgVW.jpeg?w=200&h=200&f=face
      fullname: Victor Nogueira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Felladrin
      type: user
    createdAt: '2023-10-11T10:33:59.000Z'
    data:
      edited: false
      editors:
      - Felladrin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6637833714485168
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6454aff9273f649830234978/cvVV08YHJpJx9xWVZqgVW.jpeg?w=200&h=200&f=face
          fullname: Victor Nogueira
          isHf: false
          isPro: false
          name: Felladrin
          type: user
        html: "<p>\u2139\uFE0F <span data-props=\"{&quot;user&quot;:&quot;lmz&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lmz\"\
          >@<span class=\"underline\">lmz</span></a></span>\n\n\t</span></span> <a\
          \ href=\"https://huggingface.co/lmz/candle-quantized-phi/tree/main\">converted\
          \ it to gguf</a> in a way it runs on <a rel=\"nofollow\" href=\"https://github.com/huggingface/candle/\"\
          >candle framework</a>.</p>\n"
        raw: "\u2139\uFE0F @lmz [converted it to gguf](https://huggingface.co/lmz/candle-quantized-phi/tree/main)\
          \ in a way it runs on [candle framework](https://github.com/huggingface/candle/)."
        updatedAt: '2023-10-11T10:33:59.501Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - lmz
    id: 65267a17871f915e766a733a
    type: comment
  author: Felladrin
  content: "\u2139\uFE0F @lmz [converted it to gguf](https://huggingface.co/lmz/candle-quantized-phi/tree/main)\
    \ in a way it runs on [candle framework](https://github.com/huggingface/candle/)."
  created_at: 2023-10-11 09:33:59+00:00
  edited: false
  hidden: false
  id: 65267a17871f915e766a733a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
      fullname: Laurent Mazare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmz
      type: user
    createdAt: '2023-10-11T10:56:51.000Z'
    data:
      edited: false
      editors:
      - lmz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8910244107246399
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
          fullname: Laurent Mazare
          isHf: false
          isPro: false
          name: lmz
          type: user
        html: "<p>Right, sorry I didn't notice this issue earlier but as mentioned\
          \ by <span data-props=\"{&quot;user&quot;:&quot;Felladrin&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/Felladrin\">@<span class=\"\
          underline\">Felladrin</span></a></span>\n\n\t</span></span> there is indeed\
          \ already a quantized version available in <code>candle-transformers</code>.\
          \ You can try it out through our phi example in the candle repo by using\
          \ the <code>--quantized</code> flag. An example can be seen at the bottom\
          \ of this <a rel=\"nofollow\" href=\"https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi#running-some-example\"\
          >readme</a>.</p>\n"
        raw: Right, sorry I didn't notice this issue earlier but as mentioned by @Felladrin
          there is indeed already a quantized version available in `candle-transformers`.
          You can try it out through our phi example in the candle repo by using the
          `--quantized` flag. An example can be seen at the bottom of this [readme](https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi#running-some-example).
        updatedAt: '2023-10-11T10:56:51.049Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Felladrin
        - has-c
        - loretoparisi
    id: 65267f736e00e9a9d64dcfdf
    type: comment
  author: lmz
  content: Right, sorry I didn't notice this issue earlier but as mentioned by @Felladrin
    there is indeed already a quantized version available in `candle-transformers`.
    You can try it out through our phi example in the candle repo by using the `--quantized`
    flag. An example can be seen at the bottom of this [readme](https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi#running-some-example).
  created_at: 2023-10-11 09:56:51+00:00
  edited: false
  hidden: false
  id: 65267f736e00e9a9d64dcfdf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2023-10-23T11:51:45.000Z'
    data:
      edited: false
      editors:
      - loretoparisi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5436043739318848
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lmz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lmz\">@<span class=\"\
          underline\">lmz</span></a></span>\n\n\t</span></span> I'm getting this error\
          \ when trying to load <code>model-v1-q4k.gguf</code> into llama.cpp</p>\n\
          <pre><code class=\"language-bash\">llama_model_loader: - <span class=\"\
          hljs-built_in\">type</span>  f32:  171 tensors\nllama_model_loader: - <span\
          \ class=\"hljs-built_in\">type</span> q4_K:   98 tensors\nerror loading\
          \ model: unknown model architecture: <span class=\"hljs-string\">''</span>\n\
          llama_load_model_from_file: failed to load model\nTraceback (most recent\
          \ call last):\n  File <span class=\"hljs-string\">\"/app/embeddings.py\"\
          </span>, line 37, <span class=\"hljs-keyword\">in</span> &lt;module&gt;\n\
          \    llm = Llama(model_path=path.join(<span class=\"hljs-string\">\"models\"\
          </span>, model_path, model_fname, ), \n  File <span class=\"hljs-string\"\
          >\"/app/llama_cpp/llama.py\"</span>, line 323, <span class=\"hljs-keyword\"\
          >in</span> __init__\n    assert self.model is not None\nAssertionError\n\
          </code></pre>\n"
        raw: "@lmz I'm getting this error when trying to load `model-v1-q4k.gguf`\
          \ into llama.cpp\n\n```bash\nllama_model_loader: - type  f32:  171 tensors\n\
          llama_model_loader: - type q4_K:   98 tensors\nerror loading model: unknown\
          \ model architecture: ''\nllama_load_model_from_file: failed to load model\n\
          Traceback (most recent call last):\n  File \"/app/embeddings.py\", line\
          \ 37, in <module>\n    llm = Llama(model_path=path.join(\"models\", model_path,\
          \ model_fname, ), \n  File \"/app/llama_cpp/llama.py\", line 323, in __init__\n\
          \    assert self.model is not None\nAssertionError\n```"
        updatedAt: '2023-10-23T11:51:45.222Z'
      numEdits: 0
      reactions: []
    id: 65365e5131e73689f340a91c
    type: comment
  author: loretoparisi
  content: "@lmz I'm getting this error when trying to load `model-v1-q4k.gguf` into\
    \ llama.cpp\n\n```bash\nllama_model_loader: - type  f32:  171 tensors\nllama_model_loader:\
    \ - type q4_K:   98 tensors\nerror loading model: unknown model architecture:\
    \ ''\nllama_load_model_from_file: failed to load model\nTraceback (most recent\
    \ call last):\n  File \"/app/embeddings.py\", line 37, in <module>\n    llm =\
    \ Llama(model_path=path.join(\"models\", model_path, model_fname, ), \n  File\
    \ \"/app/llama_cpp/llama.py\", line 323, in __init__\n    assert self.model is\
    \ not None\nAssertionError\n```"
  created_at: 2023-10-23 10:51:45+00:00
  edited: false
  hidden: false
  id: 65365e5131e73689f340a91c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
      fullname: Laurent Mazare
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: lmz
      type: user
    createdAt: '2023-10-23T21:40:04.000Z'
    data:
      edited: false
      editors:
      - lmz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9222013354301453
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/eaf7f30a334d23c1c561a4512f55c9b3.svg
          fullname: Laurent Mazare
          isHf: false
          isPro: false
          name: lmz
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;loretoparisi&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/loretoparisi\"\
          >@<span class=\"underline\">loretoparisi</span></a></span>\n\n\t</span></span>\
          \ this is actually not designed to work with llama.cpp but with candle,\
          \ you can see the documentation for this example <a rel=\"nofollow\" href=\"\
          https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi\"\
          >here</a>. My guess is that getting this to work with llama.cpp is likely\
          \ not trivial whereas one of the design goal of candle is to make it easier\
          \ to try quantization on architectures that are potentially very different\
          \ from llama.</p>\n"
        raw: '@loretoparisi this is actually not designed to work with llama.cpp but
          with candle, you can see the documentation for this example [here](https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi).
          My guess is that getting this to work with llama.cpp is likely not trivial
          whereas one of the design goal of candle is to make it easier to try quantization
          on architectures that are potentially very different from llama.'
        updatedAt: '2023-10-23T21:40:04.491Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F92F"
        users:
        - iddar
        - loretoparisi
    id: 6536e834aa214745cba23109
    type: comment
  author: lmz
  content: '@loretoparisi this is actually not designed to work with llama.cpp but
    with candle, you can see the documentation for this example [here](https://github.com/huggingface/candle/tree/main/candle-examples/examples/phi).
    My guess is that getting this to work with llama.cpp is likely not trivial whereas
    one of the design goal of candle is to make it easier to try quantization on architectures
    that are potentially very different from llama.'
  created_at: 2023-10-23 20:40:04+00:00
  edited: false
  hidden: false
  id: 6536e834aa214745cba23109
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
      fullname: Loreto Parisi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: loretoparisi
      type: user
    createdAt: '2023-11-06T17:30:29.000Z'
    data:
      edited: false
      editors:
      - loretoparisi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7563360929489136
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1642412329881-5e6b7a61d4cd9779932a7601.png?w=200&h=200&f=face
          fullname: Loreto Parisi
          isHf: false
          isPro: false
          name: loretoparisi
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;lmz&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/lmz\">@<span class=\"\
          underline\">lmz</span></a></span>\n\n\t</span></span> so basically this\
          \ is not a <code>gguf</code> format, but a candle quantized format.</p>\n"
        raw: '@lmz so basically this is not a `gguf` format, but a candle quantized
          format.'
        updatedAt: '2023-11-06T17:30:29.739Z'
      numEdits: 0
      reactions: []
    id: 654922b5859ca1ca70a2e5dd
    type: comment
  author: loretoparisi
  content: '@lmz so basically this is not a `gguf` format, but a candle quantized
    format.'
  created_at: 2023-11-06 17:30:29+00:00
  edited: false
  hidden: false
  id: 654922b5859ca1ca70a2e5dd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-19T19:37:21.000Z'
    data:
      status: closed
    id: 6581f0f10fb25b9aa26ed8dc
    type: status-change
  author: gugarosa
  created_at: 2023-12-19 19:37:21+00:00
  id: 6581f0f10fb25b9aa26ed8dc
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 40
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: 'Any plan/ideas to convert this to gguf? '
