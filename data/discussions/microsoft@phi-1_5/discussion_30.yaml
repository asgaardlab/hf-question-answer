!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ARahul2003
conflicting_files: null
created_at: 2023-09-24 19:00:14+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34a470946717f80498d0764120607bd3.svg
      fullname: A Rahul
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ARahul2003
      type: user
    createdAt: '2023-09-24T20:00:14.000Z'
    data:
      edited: false
      editors:
      - ARahul2003
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6305989027023315
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34a470946717f80498d0764120607bd3.svg
          fullname: A Rahul
          isHf: false
          isPro: false
          name: ARahul2003
          type: user
        html: "<p>Hi all, I have been trying to use this model on a laptop without\
          \ any GPU for one of my course projects. Naturally, I am required to load\
          \ this model in 8-bit quantization form. However, whenever I try to load\
          \ it in a quantized state, I get an error stating that the accelerate and\
          \ bits-and-bytes libraries are not present. I made to sure install those\
          \ libraries in my virtual environment, yet the error persists. Please help\
          \ me.</p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/JVsf35dy76wVe8FBzUpLA.png\"\
          ><img alt=\"Huggingface 2.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/JVsf35dy76wVe8FBzUpLA.png\"\
          ></a></p>\n<p><a rel=\"nofollow\" href=\"https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/F9-JdU3y7LiWGg1V4wvo3.png\"\
          ><img alt=\"huggingface 1.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/F9-JdU3y7LiWGg1V4wvo3.png\"\
          ></a></p>\n<p>Here is the code that I have written:</p>\n<p>from transformers\
          \ import AutoModelForCausalLM, AutoTokenizer<br>import accelerate<br>import\
          \ bitsandbytes<br>import gradio as gr<br>import torch</p>\n<p>title = \"\
          ????AI ChatBot\"<br>description = \"Quantised version of the Phi 1.5 LLM\
          \ released by Microsoft research\"<br>examples = [[\"How are you?\"]]</p>\n\
          <p>tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
          \ torch_dtype=\"auto\")<br>model = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\", load_in_8bit\
          \ = True)</p>\n<p>def predict(input, history=[]):<br>    # tokenize the\
          \ new input sentence<br>    new_user_input_ids = tokenizer.encode(<br> \
          \       input + tokenizer.eos_token, return_tensors=\"pt\"<br>    )</p>\n\
          <pre><code># append the new user input tokens to the chat history\nbot_input_ids\
          \ = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\n\
          \n# generate a response\nhistory = model.generate(\n    bot_input_ids, max_length=4000,\
          \ pad_token_id=tokenizer.eos_token_id\n).tolist()\n\n# convert the tokens\
          \ to text, and then split the responses into lines\nresponse = tokenizer.decode(history[0]).split(\"\
          &lt;|endoftext|&gt;\")\n# print('decoded_response--&gt;&gt;'+str(response))\n\
          response = [\n    (response[i], response[i + 1]) for i in range(0, len(response)\
          \ - 1, 2)\n]  # convert to tuples of list\n# print('response--&gt;&gt;'+str(response))\n\
          return response, history\n</code></pre>\n<p>gr.Interface(<br>    fn=predict,<br>\
          \    title=title,<br>    description=description,<br>    examples=examples,<br>\
          \    inputs=[\"text\", \"state\"],<br>    outputs=[\"chatbot\", \"state\"\
          ],<br>    theme=\"finlaymacklon/boxy_violet\",<br>).launch()</p>\n"
        raw: "Hi all, I have been trying to use this model on a laptop without any\
          \ GPU for one of my course projects. Naturally, I am required to load this\
          \ model in 8-bit quantization form. However, whenever I try to load it in\
          \ a quantized state, I get an error stating that the accelerate and bits-and-bytes\
          \ libraries are not present. I made to sure install those libraries in my\
          \ virtual environment, yet the error persists. Please help me.\r\n\r\n\r\
          \n\r\n![Huggingface 2.png](https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/JVsf35dy76wVe8FBzUpLA.png)\r\
          \n \r\n![huggingface 1.png](https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/F9-JdU3y7LiWGg1V4wvo3.png)\r\
          \n\r\n\r\n\r\nHere is the code that I have written:\r\n\r\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\r\nimport accelerate\r\nimport\
          \ bitsandbytes\r\nimport gradio as gr\r\nimport torch\r\n\r\n\r\ntitle =\
          \ \"????AI ChatBot\"\r\ndescription = \"Quantised version of the Phi 1.5\
          \ LLM released by Microsoft research\"\r\nexamples = [[\"How are you?\"\
          ]]\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\"\
          , trust_remote_code=True, torch_dtype=\"auto\")\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\", load_in_8bit\
          \ = True)\r\n\r\n\r\ndef predict(input, history=[]):\r\n    # tokenize the\
          \ new input sentence\r\n    new_user_input_ids = tokenizer.encode(\r\n \
          \       input + tokenizer.eos_token, return_tensors=\"pt\"\r\n    )\r\n\r\
          \n    # append the new user input tokens to the chat history\r\n    bot_input_ids\
          \ = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\r\
          \n\r\n    # generate a response\r\n    history = model.generate(\r\n   \
          \     bot_input_ids, max_length=4000, pad_token_id=tokenizer.eos_token_id\r\
          \n    ).tolist()\r\n\r\n    # convert the tokens to text, and then split\
          \ the responses into lines\r\n    response = tokenizer.decode(history[0]).split(\"\
          <|endoftext|>\")\r\n    # print('decoded_response-->>'+str(response))\r\n\
          \    response = [\r\n        (response[i], response[i + 1]) for i in range(0,\
          \ len(response) - 1, 2)\r\n    ]  # convert to tuples of list\r\n    # print('response-->>'+str(response))\r\
          \n    return response, history\r\n\r\n\r\ngr.Interface(\r\n    fn=predict,\r\
          \n    title=title,\r\n    description=description,\r\n    examples=examples,\r\
          \n    inputs=[\"text\", \"state\"],\r\n    outputs=[\"chatbot\", \"state\"\
          ],\r\n    theme=\"finlaymacklon/boxy_violet\",\r\n).launch()"
        updatedAt: '2023-09-24T20:00:14.678Z'
      numEdits: 0
      reactions: []
    id: 6510954ea0f2ffbecaa77ed5
    type: comment
  author: ARahul2003
  content: "Hi all, I have been trying to use this model on a laptop without any GPU\
    \ for one of my course projects. Naturally, I am required to load this model in\
    \ 8-bit quantization form. However, whenever I try to load it in a quantized state,\
    \ I get an error stating that the accelerate and bits-and-bytes libraries are\
    \ not present. I made to sure install those libraries in my virtual environment,\
    \ yet the error persists. Please help me.\r\n\r\n\r\n\r\n![Huggingface 2.png](https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/JVsf35dy76wVe8FBzUpLA.png)\r\
    \n \r\n![huggingface 1.png](https://cdn-uploads.huggingface.co/production/uploads/646d9faeeb9268aeebc55cf6/F9-JdU3y7LiWGg1V4wvo3.png)\r\
    \n\r\n\r\n\r\nHere is the code that I have written:\r\n\r\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\r\nimport accelerate\r\nimport bitsandbytes\r\
    \nimport gradio as gr\r\nimport torch\r\n\r\n\r\ntitle = \"????AI ChatBot\"\r\n\
    description = \"Quantised version of the Phi 1.5 LLM released by Microsoft research\"\
    \r\nexamples = [[\"How are you?\"]]\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\r\nmodel =\
    \ AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
    \ torch_dtype=\"auto\", load_in_8bit = True)\r\n\r\n\r\ndef predict(input, history=[]):\r\
    \n    # tokenize the new input sentence\r\n    new_user_input_ids = tokenizer.encode(\r\
    \n        input + tokenizer.eos_token, return_tensors=\"pt\"\r\n    )\r\n\r\n\
    \    # append the new user input tokens to the chat history\r\n    bot_input_ids\
    \ = torch.cat([torch.LongTensor(history), new_user_input_ids], dim=-1)\r\n\r\n\
    \    # generate a response\r\n    history = model.generate(\r\n        bot_input_ids,\
    \ max_length=4000, pad_token_id=tokenizer.eos_token_id\r\n    ).tolist()\r\n\r\
    \n    # convert the tokens to text, and then split the responses into lines\r\n\
    \    response = tokenizer.decode(history[0]).split(\"<|endoftext|>\")\r\n    #\
    \ print('decoded_response-->>'+str(response))\r\n    response = [\r\n        (response[i],\
    \ response[i + 1]) for i in range(0, len(response) - 1, 2)\r\n    ]  # convert\
    \ to tuples of list\r\n    # print('response-->>'+str(response))\r\n    return\
    \ response, history\r\n\r\n\r\ngr.Interface(\r\n    fn=predict,\r\n    title=title,\r\
    \n    description=description,\r\n    examples=examples,\r\n    inputs=[\"text\"\
    , \"state\"],\r\n    outputs=[\"chatbot\", \"state\"],\r\n    theme=\"finlaymacklon/boxy_violet\"\
    ,\r\n).launch()"
  created_at: 2023-09-24 19:00:14+00:00
  edited: false
  hidden: false
  id: 6510954ea0f2ffbecaa77ed5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-26T18:22:57.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.928398847579956
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;ARahul2003&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ARahul2003\"\
          >@<span class=\"underline\">ARahul2003</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Your image still show an <code>ImportError</code>, which could be related\
          \ to an incomplete installation of either <code>accelerate</code> or <code>bitsandbytes</code>.\
          \ However, please note that we haven't tested Phi-based models support with\
          \ 8 bits, so I am unsure what will be its behavior.</p>\n"
        raw: 'Hello @ARahul2003!


          Your image still show an `ImportError`, which could be related to an incomplete
          installation of either `accelerate` or `bitsandbytes`. However, please note
          that we haven''t tested Phi-based models support with 8 bits, so I am unsure
          what will be its behavior.'
        updatedAt: '2023-09-26T18:22:57.086Z'
      numEdits: 0
      reactions: []
    id: 6513218192a52e002ac234a1
    type: comment
  author: gugarosa
  content: 'Hello @ARahul2003!


    Your image still show an `ImportError`, which could be related to an incomplete
    installation of either `accelerate` or `bitsandbytes`. However, please note that
    we haven''t tested Phi-based models support with 8 bits, so I am unsure what will
    be its behavior.'
  created_at: 2023-09-26 17:22:57+00:00
  edited: false
  hidden: false
  id: 6513218192a52e002ac234a1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-09-27T01:02:17.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5583457946777344
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;ARahul2003&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ARahul2003\"\
          >@<span class=\"underline\">ARahul2003</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<p>You need to use older version of transformers.</p>\n<p>!pip\
          \ install -qU trl datasets accelerate loralib einops xformers bitsandbytes<br>!pip\
          \ install transformers==4.30</p>\n"
        raw: 'Hello @ARahul2003 ,


          You need to use older version of transformers.


          !pip install -qU trl datasets accelerate loralib einops xformers bitsandbytes

          !pip install transformers==4.30'
        updatedAt: '2023-09-27T01:02:17.696Z'
      numEdits: 0
      reactions: []
    id: 65137f19b2ba50af588a9204
    type: comment
  author: codegood
  content: 'Hello @ARahul2003 ,


    You need to use older version of transformers.


    !pip install -qU trl datasets accelerate loralib einops xformers bitsandbytes

    !pip install transformers==4.30'
  created_at: 2023-09-27 00:02:17+00:00
  edited: false
  hidden: false
  id: 65137f19b2ba50af588a9204
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-30T17:03:57.000Z'
    data:
      status: closed
    id: 653fe1fd66c716609b188ec0
    type: status-change
  author: gugarosa
  created_at: 2023-10-30 16:03:57+00:00
  id: 653fe1fd66c716609b188ec0
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 30
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Unable to load the model in 8 bits
