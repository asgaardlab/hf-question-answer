!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Paul311
conflicting_files: null
created_at: 2023-09-30 19:58:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/N_mlVaJO7FSkAIcBe5Zzl.jpeg?w=200&h=200&f=face
      fullname: Paul Jaeger
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Paul311
      type: user
    createdAt: '2023-09-30T20:58:13.000Z'
    data:
      edited: false
      editors:
      - Paul311
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5158373117446899
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/N_mlVaJO7FSkAIcBe5Zzl.jpeg?w=200&h=200&f=face
          fullname: Paul Jaeger
          isHf: false
          isPro: false
          name: Paul311
          type: user
        html: '<p>Can I / How Would I Load This Model Locally on a NVIDIA 900-2G414-0000-000
          Tesla P4 8GB GDDR5 Inferencing Accelerator ?</p>

          '
        raw: Can I / How Would I Load This Model Locally on a NVIDIA 900-2G414-0000-000
          Tesla P4 8GB GDDR5 Inferencing Accelerator ?
        updatedAt: '2023-09-30T20:58:13.328Z'
      numEdits: 0
      reactions: []
    id: 65188be566096ebbfe0707ba
    type: comment
  author: Paul311
  content: Can I / How Would I Load This Model Locally on a NVIDIA 900-2G414-0000-000
    Tesla P4 8GB GDDR5 Inferencing Accelerator ?
  created_at: 2023-09-30 19:58:13+00:00
  edited: false
  hidden: false
  id: 65188be566096ebbfe0707ba
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-30T17:09:55.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.48769381642341614
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;Paul311&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/Paul311\"\
          >@<span class=\"underline\">Paul311</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>You should be able to use the snippet bellow to load and infer with the\
          \ model:</p>\n<pre><code class=\"language-python\"><span class=\"hljs-keyword\"\
          >import</span> torch\n<span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM, AutoTokenizer\n\
          \ntorch.set_default_device(<span class=\"hljs-string\">\"cuda\"</span>)\n\
          model = AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\"\
          >\"microsoft/phi-1_5\"</span>, trust_remote_code=<span class=\"hljs-literal\"\
          >True</span>)\ntokenizer = AutoTokenizer.from_pretrained(<span class=\"\
          hljs-string\">\"microsoft/phi-1_5\"</span>, trust_remote_code=<span class=\"\
          hljs-literal\">True</span>)\ninputs = tokenizer(<span class=\"hljs-string\"\
          >'''```python</span>\n<span class=\"hljs-string\">def print_prime(n):</span>\n\
          <span class=\"hljs-string\">   \"\"\"</span>\n<span class=\"hljs-string\"\
          >   Print all primes between 1 and n</span>\n<span class=\"hljs-string\"\
          >   \"\"\"'''</span>, return_tensors=<span class=\"hljs-string\">\"pt\"\
          </span>, return_attention_mask=<span class=\"hljs-literal\">False</span>)\n\
          \noutputs = model.generate(**inputs, max_length=<span class=\"hljs-number\"\
          >200</span>)\ntext = tokenizer.batch_decode(outputs)[<span class=\"hljs-number\"\
          >0</span>]\n<span class=\"hljs-built_in\">print</span>(text)\n</code></pre>\n\
          <p>It should consume around 6.5GB GPU memory.</p>\n"
        raw: "Hello @Paul311!\n\nYou should be able to use the snippet bellow to load\
          \ and infer with the model:\n\n```python\nimport torch\nfrom transformers\
          \ import AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"\
          cuda\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
          , trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True)\ninputs = tokenizer('''```python\n\
          def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n  \
          \ \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\noutputs\
          \ = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
          print(text)\n```\n\nIt should consume around 6.5GB GPU memory."
        updatedAt: '2023-10-30T17:09:55.490Z'
      numEdits: 0
      reactions: []
    id: 653fe3635e074bde16c4eb93
    type: comment
  author: gugarosa
  content: "Hello @Paul311!\n\nYou should be able to use the snippet bellow to load\
    \ and infer with the model:\n\n```python\nimport torch\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\n\ntorch.set_default_device(\"cuda\")\n\
    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\
    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\
    inputs = tokenizer('''```python\ndef print_prime(n):\n   \"\"\"\n   Print all\
    \ primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\
    \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
    print(text)\n```\n\nIt should consume around 6.5GB GPU memory."
  created_at: 2023-10-30 16:09:55+00:00
  edited: false
  hidden: false
  id: 653fe3635e074bde16c4eb93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-11-13T18:47:13.000Z'
    data:
      status: closed
    id: 65526f31db48a7ea5620460f
    type: status-change
  author: gugarosa
  created_at: 2023-11-13 18:47:13+00:00
  id: 65526f31db48a7ea5620460f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 39
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: 'Can I / How Would I Load This Model Locally on a NVIDIA Tesla 8GB '
