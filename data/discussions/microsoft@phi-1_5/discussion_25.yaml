!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jeff-gao
conflicting_files: null
created_at: 2023-09-20 13:19:34+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b4406f5831df17eccb9bf18a97afb90.svg
      fullname: Jeff Gao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeff-gao
      type: user
    createdAt: '2023-09-20T14:19:34.000Z'
    data:
      edited: false
      editors:
      - jeff-gao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9292418360710144
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b4406f5831df17eccb9bf18a97afb90.svg
          fullname: Jeff Gao
          isHf: false
          isPro: false
          name: jeff-gao
          type: user
        html: '<p>In the paper, it says the inference speed is &lt; 3ms per token
          using a single A100-80G.<br>However when I test with the sample code on  a
          single A100-80G, the inference speed is around 28ms per token. My torch
          version is 2.0.1.<br>May I know how to make the inference speed to be around
          3ms?<br>Thank you very much!</p>

          '
        raw: "In the paper, it says the inference speed is < 3ms per token using a\
          \ single A100-80G.\r\nHowever when I test with the sample code on  a single\
          \ A100-80G, the inference speed is around 28ms per token. My torch version\
          \ is 2.0.1. \r\nMay I know how to make the inference speed to be around\
          \ 3ms?\r\nThank you very much!"
        updatedAt: '2023-09-20T14:19:34.974Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F92F"
        users:
        - KrishnaKaasyap
    id: 650aff76180b5958d1414a0c
    type: comment
  author: jeff-gao
  content: "In the paper, it says the inference speed is < 3ms per token using a single\
    \ A100-80G.\r\nHowever when I test with the sample code on  a single A100-80G,\
    \ the inference speed is around 28ms per token. My torch version is 2.0.1. \r\n\
    May I know how to make the inference speed to be around 3ms?\r\nThank you very\
    \ much!"
  created_at: 2023-09-20 13:19:34+00:00
  edited: false
  hidden: false
  id: 650aff76180b5958d1414a0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/8b4406f5831df17eccb9bf18a97afb90.svg
      fullname: Jeff Gao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeff-gao
      type: user
    createdAt: '2023-09-22T01:21:54.000Z'
    data:
      from: ' Inference speed is much longer than reported'
      to: Inference time is much longer than reported
    id: 650cec328ffe1f53bdcc5799
    type: title-change
  author: jeff-gao
  created_at: 2023-09-22 00:21:54+00:00
  id: 650cec328ffe1f53bdcc5799
  new_title: Inference time is much longer than reported
  old_title: ' Inference speed is much longer than reported'
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617717085923-noauth.png?w=200&h=200&f=face
      fullname: Hugo Sousa
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: hugosousa
      type: user
    createdAt: '2023-09-22T08:12:55.000Z'
    data:
      edited: false
      editors:
      - hugosousa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9710491895675659
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1617717085923-noauth.png?w=200&h=200&f=face
          fullname: Hugo Sousa
          isHf: false
          isPro: false
          name: hugosousa
          type: user
        html: '<p>Did you used <a rel="nofollow" href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a>
          to run inference? I am not sure, but it seems that they used it as it is
          mentioned on the model card.</p>

          '
        raw: Did you used [DeepSpeed](https://github.com/microsoft/DeepSpeed) to run
          inference? I am not sure, but it seems that they used it as it is mentioned
          on the model card.
        updatedAt: '2023-09-22T08:12:55.679Z'
      numEdits: 0
      reactions: []
    id: 650d4c87cbd0c7d550d409a0
    type: comment
  author: hugosousa
  content: Did you used [DeepSpeed](https://github.com/microsoft/DeepSpeed) to run
    inference? I am not sure, but it seems that they used it as it is mentioned on
    the model card.
  created_at: 2023-09-22 07:12:55+00:00
  edited: false
  hidden: false
  id: 650d4c87cbd0c7d550d409a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-26T18:26:33.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9368132948875427
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jeff-gao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jeff-gao\"\
          >@<span class=\"underline\">jeff-gao</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>This mismatch was caused by the absence of Flash-Attention in the model\
          \ files. We opted to not add it at first to keep the implementation simple,\
          \ but we plan in adding an option that uses such implementation to take\
          \ advantage of faster inferences.</p>\n"
        raw: 'Hello @jeff-gao!


          This mismatch was caused by the absence of Flash-Attention in the model
          files. We opted to not add it at first to keep the implementation simple,
          but we plan in adding an option that uses such implementation to take advantage
          of faster inferences.'
        updatedAt: '2023-09-26T18:26:33.494Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F917"
        users:
        - jeff-gao
        - KrishnaKaasyap
    id: 65132259387d403b2e6cbabf
    type: comment
  author: gugarosa
  content: 'Hello @jeff-gao!


    This mismatch was caused by the absence of Flash-Attention in the model files.
    We opted to not add it at first to keep the implementation simple, but we plan
    in adding an option that uses such implementation to take advantage of faster
    inferences.'
  created_at: 2023-09-26 17:26:33+00:00
  edited: false
  hidden: false
  id: 65132259387d403b2e6cbabf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8b4406f5831df17eccb9bf18a97afb90.svg
      fullname: Jeff Gao
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jeff-gao
      type: user
    createdAt: '2023-09-27T06:37:05.000Z'
    data:
      edited: false
      editors:
      - jeff-gao
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9017543792724609
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8b4406f5831df17eccb9bf18a97afb90.svg
          fullname: Jeff Gao
          isHf: false
          isPro: false
          name: jeff-gao
          type: user
        html: "<blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jeff-gao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jeff-gao\"\
          >@<span class=\"underline\">jeff-gao</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>This mismatch was caused by the absence of Flash-Attention in the model\
          \ files. We opted to not add it at first to keep the implementation simple,\
          \ but we plan in adding an option that uses such implementation to take\
          \ advantage of faster inferences.</p>\n</blockquote>\n<p>Hello <span data-props=\"\
          {&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/gugarosa\">@<span class=\"underline\">gugarosa</span></a></span>\n\
          \n\t</span></span>, thank you very much! Looking forward to your implementations\
          \ !!!</p>\n"
        raw: "> Hello @jeff-gao!\n> \n> This mismatch was caused by the absence of\
          \ Flash-Attention in the model files. We opted to not add it at first to\
          \ keep the implementation simple, but we plan in adding an option that uses\
          \ such implementation to take advantage of faster inferences.\n\nHello @gugarosa,\
          \ thank you very much! Looking forward to your implementations !!!\n"
        updatedAt: '2023-09-27T06:37:05.497Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - KrishnaKaasyap
    id: 6513cd916e06b81de30cba73
    type: comment
  author: jeff-gao
  content: "> Hello @jeff-gao!\n> \n> This mismatch was caused by the absence of Flash-Attention\
    \ in the model files. We opted to not add it at first to keep the implementation\
    \ simple, but we plan in adding an option that uses such implementation to take\
    \ advantage of faster inferences.\n\nHello @gugarosa, thank you very much! Looking\
    \ forward to your implementations !!!\n"
  created_at: 2023-09-27 05:37:05+00:00
  edited: false
  hidden: false
  id: 6513cd916e06b81de30cba73
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
      fullname: Krishna Kaasyap
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: KrishnaKaasyap
      type: user
    createdAt: '2023-10-15T09:20:17.000Z'
    data:
      edited: false
      editors:
      - KrishnaKaasyap
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8655334711074829
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/236c771e6c5a25ef6ed5e1bc061e30b8.svg
          fullname: Krishna Kaasyap
          isHf: false
          isPro: false
          name: KrishnaKaasyap
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;jeff-gao&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jeff-gao\"\
          >@<span class=\"underline\">jeff-gao</span></a></span>\n\n\t</span></span>\
          \ - since at fp16 it takes only 3.16 GB VRAM, can we run 24 copies (approximately)\
          \ of Phi 1.5 on an A100-80GB GPU?</p>\n<p>If that is possible and 3ms per\
          \ token is also achievable with flash attention - can we generate 7200 tokens\
          \ (24 copies \xD7 300 tokens per second) per second on a A100-80GB GPU?</p>\n\
          <p>I'm a non-technical guy. Just asking out of curiosity. Thanks. \U0001F64F\
          \U0001F3FC</p>\n"
        raw: "Hey @jeff-gao - since at fp16 it takes only 3.16 GB VRAM, can we run\
          \ 24 copies (approximately) of Phi 1.5 on an A100-80GB GPU?\n\nIf that is\
          \ possible and 3ms per token is also achievable with flash attention - can\
          \ we generate 7200 tokens (24 copies \xD7 300 tokens per second) per second\
          \ on a A100-80GB GPU?\n\nI'm a non-technical guy. Just asking out of curiosity.\
          \ Thanks. \U0001F64F\U0001F3FC"
        updatedAt: '2023-10-15T09:20:17.175Z'
      numEdits: 0
      reactions: []
    id: 652baed168f1d7d1d279efc4
    type: comment
  author: KrishnaKaasyap
  content: "Hey @jeff-gao - since at fp16 it takes only 3.16 GB VRAM, can we run 24\
    \ copies (approximately) of Phi 1.5 on an A100-80GB GPU?\n\nIf that is possible\
    \ and 3ms per token is also achievable with flash attention - can we generate\
    \ 7200 tokens (24 copies \xD7 300 tokens per second) per second on a A100-80GB\
    \ GPU?\n\nI'm a non-technical guy. Just asking out of curiosity. Thanks. \U0001F64F\
    \U0001F3FC"
  created_at: 2023-10-15 08:20:17+00:00
  edited: false
  hidden: false
  id: 652baed168f1d7d1d279efc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-11-21T12:53:27.000Z'
    data:
      status: closed
    id: 655ca847244de83dcfdaaf0e
    type: status-change
  author: gugarosa
  created_at: 2023-11-21 12:53:27+00:00
  id: 655ca847244de83dcfdaaf0e
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 25
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Inference time is much longer than reported
