!!python/object:huggingface_hub.community.DiscussionWithDetails
author: pratikkumar2008
conflicting_files: null
created_at: 2023-09-23 10:49:25+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9134a51ab7369bc2d2bb0e7120a9574e.svg
      fullname: Pratik Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: pratikkumar2008
      type: user
    createdAt: '2023-09-23T11:49:25.000Z'
    data:
      edited: false
      editors:
      - pratikkumar2008
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4710349440574646
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9134a51ab7369bc2d2bb0e7120a9574e.svg
          fullname: Pratik Kumar
          isHf: false
          isPro: false
          name: pratikkumar2008
          type: user
        html: '<p>Size of tokenizer vocab  is 50257, while size of vocab in config
          is 51200. Any particular reason for this? Also how should we deal when we
          add extra tokens in tokenizer (Resizing of embedding layer of model).<br><a
          href="https://huggingface.co/microsoft/phi-1_5/blob/main/config.json">https://huggingface.co/microsoft/phi-1_5/blob/main/config.json</a></p>

          <p>from transformers import AutoTokenizer<br>pretrained_model_name_or_path="~/phi-1_5"<br>tokenizer
          = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)<br>print(tokenizer.vocab_size)<br>50257</p>

          <p>m_my=torch.load(pretrained_model_name_or_path)<br>for (k1,v1) in m_my.items():<br>    print(k1,
          v1.shape)</p>

          <p>layers.0.wte.weight torch.Size([51200, 2048])</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/633bdd92c0fb6fd232edeb0f/H7TdfdtqtnroqXTfEt-2-.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/633bdd92c0fb6fd232edeb0f/H7TdfdtqtnroqXTfEt-2-.png"></a></p>

          '
        raw: "Size of tokenizer vocab  is 50257, while size of vocab in config is\
          \ 51200. Any particular reason for this? Also how should we deal when we\
          \ add extra tokens in tokenizer (Resizing of embedding layer of model).\r\
          \nhttps://huggingface.co/microsoft/phi-1_5/blob/main/config.json\r\n\r\n\
          from transformers import AutoTokenizer\r\npretrained_model_name_or_path=\"\
          ~/phi-1_5\"\r\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\r\
          \nprint(tokenizer.vocab_size)\r\n50257\r\n\r\n\r\n\r\nm_my=torch.load(pretrained_model_name_or_path)\r\
          \nfor (k1,v1) in m_my.items():\r\n    print(k1, v1.shape)\r\n\r\nlayers.0.wte.weight\
          \ torch.Size([51200, 2048])\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/633bdd92c0fb6fd232edeb0f/H7TdfdtqtnroqXTfEt-2-.png)\r\
          \n\r\n"
        updatedAt: '2023-09-23T11:49:25.263Z'
      numEdits: 0
      reactions: []
    id: 650ed0c5544d7edd81258a7f
    type: comment
  author: pratikkumar2008
  content: "Size of tokenizer vocab  is 50257, while size of vocab in config is 51200.\
    \ Any particular reason for this? Also how should we deal when we add extra tokens\
    \ in tokenizer (Resizing of embedding layer of model).\r\nhttps://huggingface.co/microsoft/phi-1_5/blob/main/config.json\r\
    \n\r\nfrom transformers import AutoTokenizer\r\npretrained_model_name_or_path=\"\
    ~/phi-1_5\"\r\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\r\
    \nprint(tokenizer.vocab_size)\r\n50257\r\n\r\n\r\n\r\nm_my=torch.load(pretrained_model_name_or_path)\r\
    \nfor (k1,v1) in m_my.items():\r\n    print(k1, v1.shape)\r\n\r\nlayers.0.wte.weight\
    \ torch.Size([51200, 2048])\r\n\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/633bdd92c0fb6fd232edeb0f/H7TdfdtqtnroqXTfEt-2-.png)\r\
    \n\r\n"
  created_at: 2023-09-23 10:49:25+00:00
  edited: false
  hidden: false
  id: 650ed0c5544d7edd81258a7f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-26T18:25:03.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9454997777938843
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;pratikkumar2008&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/pratikkumar2008\"\
          >@<span class=\"underline\">pratikkumar2008</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>These extra tokens won't matter since they won't be used when passing\
          \ through the Embedding layer. This mismatch happened because 51200 is a\
          \ multiple of 64 and takes advantage of the faster cores of Ampere-based\
          \ GPUs.</p>\n<p>In theory, it should have been 50304, which is the closest\
          \ multiple of 64 (related to 50257).</p>\n"
        raw: 'Hello @pratikkumar2008!


          These extra tokens won''t matter since they won''t be used when passing
          through the Embedding layer. This mismatch happened because 51200 is a multiple
          of 64 and takes advantage of the faster cores of Ampere-based GPUs.


          In theory, it should have been 50304, which is the closest multiple of 64
          (related to 50257).'
        updatedAt: '2023-09-26T18:25:03.022Z'
      numEdits: 0
      reactions: []
    id: 651321ff759403715053e256
    type: comment
  author: gugarosa
  content: 'Hello @pratikkumar2008!


    These extra tokens won''t matter since they won''t be used when passing through
    the Embedding layer. This mismatch happened because 51200 is a multiple of 64
    and takes advantage of the faster cores of Ampere-based GPUs.


    In theory, it should have been 50304, which is the closest multiple of 64 (related
    to 50257).'
  created_at: 2023-09-26 17:25:03+00:00
  edited: false
  hidden: false
  id: 651321ff759403715053e256
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd8f97e964a040d4abc13eab4ffd13c9.svg
      fullname: Kyo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kyo-takano
      type: user
    createdAt: '2023-09-30T10:06:33.000Z'
    data:
      edited: false
      editors:
      - kyo-takano
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8873600959777832
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd8f97e964a040d4abc13eab4ffd13c9.svg
          fullname: Kyo
          isHf: false
          isPro: false
          name: kyo-takano
          type: user
        html: '<p>Hi there,</p>

          <p>I understand that it works fine as long as <code>tokenizer.vocab_size
          &lt;= model.layers[0].wte.weight.shape[0]</code>, but it seems that <strong>the
          number 50257 is actually incorrect</strong>.<br>When you count unique indices
          in the vocabulary, <strong>including <code>added_tokens</code>, the correct
          number appears to be 50295</strong> instead.<br>I am not knowledgeable about
          how this attribute is configured when initializing the tokenizer, but this
          issue may need to be fixed because sometimes we want to access the value
          through this attribute (<code>tokenizer.vocab_size</code>).</p>

          '
        raw: 'Hi there,


          I understand that it works fine as long as `tokenizer.vocab_size <= model.layers[0].wte.weight.shape[0]`,
          but it seems that **the number 50257 is actually incorrect**.

          When you count unique indices in the vocabulary, **including `added_tokens`,
          the correct number appears to be 50295** instead.

          I am not knowledgeable about how this attribute is configured when initializing
          the tokenizer, but this issue may need to be fixed because sometimes we
          want to access the value through this attribute (`tokenizer.vocab_size`).'
        updatedAt: '2023-09-30T10:06:33.672Z'
      numEdits: 0
      reactions: []
    id: 6517f3297069441423482c71
    type: comment
  author: kyo-takano
  content: 'Hi there,


    I understand that it works fine as long as `tokenizer.vocab_size <= model.layers[0].wte.weight.shape[0]`,
    but it seems that **the number 50257 is actually incorrect**.

    When you count unique indices in the vocabulary, **including `added_tokens`, the
    correct number appears to be 50295** instead.

    I am not knowledgeable about how this attribute is configured when initializing
    the tokenizer, but this issue may need to be fixed because sometimes we want to
    access the value through this attribute (`tokenizer.vocab_size`).'
  created_at: 2023-09-30 09:06:33+00:00
  edited: false
  hidden: false
  id: 6517f3297069441423482c71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-03T14:41:27.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8849722743034363
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>This is the expected behavior of <code>transformers</code>. Please
          check this issue: <a rel="nofollow" href="https://github.com/huggingface/transformers/issues/12632">https://github.com/huggingface/transformers/issues/12632</a></p>

          '
        raw: 'This is the expected behavior of `transformers`. Please check this issue:
          https://github.com/huggingface/transformers/issues/12632'
        updatedAt: '2023-10-03T14:41:54.365Z'
      numEdits: 1
      reactions: []
    id: 651c281790b2316df57c8133
    type: comment
  author: gugarosa
  content: 'This is the expected behavior of `transformers`. Please check this issue:
    https://github.com/huggingface/transformers/issues/12632'
  created_at: 2023-10-03 13:41:27+00:00
  edited: true
  hidden: false
  id: 651c281790b2316df57c8133
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dd8f97e964a040d4abc13eab4ffd13c9.svg
      fullname: Kyo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kyo-takano
      type: user
    createdAt: '2023-10-04T08:19:04.000Z'
    data:
      edited: false
      editors:
      - kyo-takano
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9064558744430542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dd8f97e964a040d4abc13eab4ffd13c9.svg
          fullname: Kyo
          isHf: false
          isPro: false
          name: kyo-takano
          type: user
        html: '<p>I''m afraid but the link you suggested doesn''t seem very relevant
          to the issue.</p>

          <p>Of course, we can get the actual vocabulary size with <code>len(tokenizer.get_vocab())</code>
          or something.<br>However, the <code>added_tokens</code> are incorporated
          by default <em><strong>without users specifying them</strong></em>, as defined
          in <a href="https://huggingface.co/microsoft/phi-1_5/blob/main/tokenizer.json">tokenizer.json</a>.<br>Given
          that the argument is supposed to be passed by users, I would not consider
          this as an "expected behavior" of the library.<br>The current implementation
          can cause errors for future users relying on the (presumably widely used)<code>vocab_size</code>
          attribute, so it would be better off corrected, maybe by moving the additional
          tokens into the default ones.</p>

          <p>Thanks for your response.</p>

          '
        raw: "I'm afraid but the link you suggested doesn't seem very relevant to\
          \ the issue.\n\nOf course, we can get the actual vocabulary size with `len(tokenizer.get_vocab())`\
          \ or something. \nHowever, the `added_tokens` are incorporated by default\
          \ ***without users specifying them***, as defined in [tokenizer.json](https://huggingface.co/microsoft/phi-1_5/blob/main/tokenizer.json).\n\
          Given that the argument is supposed to be passed by users, I would not consider\
          \ this as an \"expected behavior\" of the library.\nThe current implementation\
          \ can cause errors for future users relying on the (presumably widely used)`vocab_size`\
          \ attribute, so it would be better off corrected, maybe by moving the additional\
          \ tokens into the default ones.\n\nThanks for your response."
        updatedAt: '2023-10-04T08:19:04.429Z'
      numEdits: 0
      reactions: []
    id: 651d1ff8844b4f4bfe0e4f63
    type: comment
  author: kyo-takano
  content: "I'm afraid but the link you suggested doesn't seem very relevant to the\
    \ issue.\n\nOf course, we can get the actual vocabulary size with `len(tokenizer.get_vocab())`\
    \ or something. \nHowever, the `added_tokens` are incorporated by default ***without\
    \ users specifying them***, as defined in [tokenizer.json](https://huggingface.co/microsoft/phi-1_5/blob/main/tokenizer.json).\n\
    Given that the argument is supposed to be passed by users, I would not consider\
    \ this as an \"expected behavior\" of the library.\nThe current implementation\
    \ can cause errors for future users relying on the (presumably widely used)`vocab_size`\
    \ attribute, so it would be better off corrected, maybe by moving the additional\
    \ tokens into the default ones.\n\nThanks for your response."
  created_at: 2023-10-04 07:19:04+00:00
  edited: false
  hidden: false
  id: 651d1ff8844b4f4bfe0e4f63
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-04T13:44:15.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.931729793548584
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;kyo-takano&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kyo-takano\"\
          >@<span class=\"underline\">kyo-takano</span></a></span>\n\n\t</span></span>.</p>\n\
          <p>I definitely agree with you, but this is something out of our hands since\
          \ <code>transformers</code> is the one handling the tokenizer-related files,\
          \ i.e., they decided not to add the <code>added_tokens</code> to the <code>vocab_size</code>.\
          \ We are just following the same pattern as other models using the CodeGen\
          \ tokenizer.</p>\n<p>We can incorporate the <code>added_tokens.json</code>\
          \ tokens to the vocab key of the <code>tokenizer.json</code>file, but this\
          \ is just a \"hack\" since new <code>added_tokens</code> by users will continue\
          \ to not reflect on the <code>vocab_size</code> property.</p>\n"
        raw: 'Hi @kyo-takano.


          I definitely agree with you, but this is something out of our hands since
          `transformers` is the one handling the tokenizer-related files, i.e., they
          decided not to add the `added_tokens` to the `vocab_size`. We are just following
          the same pattern as other models using the CodeGen tokenizer.


          We can incorporate the `added_tokens.json` tokens to the vocab key of the
          `tokenizer.json`file, but this is just a "hack" since new `added_tokens`
          by users will continue to not reflect on the `vocab_size` property.'
        updatedAt: '2023-10-04T13:46:43.310Z'
      numEdits: 1
      reactions: []
    id: 651d6c2f91bde3d8f60be660
    type: comment
  author: gugarosa
  content: 'Hi @kyo-takano.


    I definitely agree with you, but this is something out of our hands since `transformers`
    is the one handling the tokenizer-related files, i.e., they decided not to add
    the `added_tokens` to the `vocab_size`. We are just following the same pattern
    as other models using the CodeGen tokenizer.


    We can incorporate the `added_tokens.json` tokens to the vocab key of the `tokenizer.json`file,
    but this is just a "hack" since new `added_tokens` by users will continue to not
    reflect on the `vocab_size` property.'
  created_at: 2023-10-04 12:44:15+00:00
  edited: true
  hidden: false
  id: 651d6c2f91bde3d8f60be660
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-30T17:03:43.000Z'
    data:
      status: closed
    id: 653fe1ef726b5383d9e28608
    type: status-change
  author: gugarosa
  created_at: 2023-10-30 16:03:43+00:00
  id: 653fe1ef726b5383d9e28608
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 29
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Size of vocab of tokenizer and size of embedding layer /model vocab is different
