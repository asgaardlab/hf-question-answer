!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mattma1970
conflicting_files: null
created_at: 2023-10-18 03:48:50+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
      fullname: Matt Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: mattma1970
      type: user
    createdAt: '2023-10-18T04:48:50.000Z'
    data:
      edited: true
      editors:
      - mattma1970
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.782105028629303
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8addf8cb4b243ac1e2ac14ef1e707f81.svg
          fullname: Matt Ma
          isHf: false
          isPro: true
          name: mattma1970
          type: user
        html: '<p>I''m trying to instruction finetune phi 1.5 on the OpenOrca/SlimOrca
          dataset. I''m using a prompt formatting that uses special tokens [''[INST]'',''[/INST]'',''&lt;&gt;'',''<s>'',''</s>''].
          To get it to run locally on my rtx4090 I need to quantize the model and
          am using bnb 4bit. When I add the special tokens and adjust the embedding
          size as follows:</p>

          <p>special_tokens_dict = {''additional_special_tokens'': [''[INST]'',''[/INST]'']}<br>num_added_toks
          = tokenizer.add_special_tokens(special_tokens_dict)<br>model.resize_token_embeddings(len(tokenizer))</p>

          <p>I get an error thrown by the resize_token_embeddings function saying
          that ''to only accepts complex or floats'' . The part of that function that
          is problematic is </p>

          <p>new_embeddings.to(<br>    embedding_layer.weight.device,<br>    dtype=embedding_layer.weight.dtype,<br>)</p>

          <p>because the embedding_layer.weight.dtype, which is got from the last
          linear layer of the model, is torch.uint8. This must be because the model
          has been quantized. If I remove the quantization, the problem goes away
          but the model VRAM consumption then blows out. </p>

          <p>Has anyone encountered this before and found a work around? </p>

          '
        raw: "I'm trying to instruction finetune phi 1.5 on the OpenOrca/SlimOrca\
          \ dataset. I'm using a prompt formatting that uses special tokens ['[INST]','[/INST]','<<SYS>>','<s>','</s>'].\
          \ To get it to run locally on my rtx4090 I need to quantize the model and\
          \ am using bnb 4bit. When I add the special tokens and adjust the embedding\
          \ size as follows:\n\nspecial_tokens_dict = {'additional_special_tokens':\
          \ ['[INST]','[/INST]']}\nnum_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n\
          model.resize_token_embeddings(len(tokenizer))\n\nI get an error thrown by\
          \ the resize_token_embeddings function saying that 'to only accepts complex\
          \ or floats' . The part of that function that is problematic is \n\nnew_embeddings.to(\n\
          \    embedding_layer.weight.device,\n    dtype=embedding_layer.weight.dtype,\n\
          )\n\nbecause the embedding_layer.weight.dtype, which is got from the last\
          \ linear layer of the model, is torch.uint8. This must be because the model\
          \ has been quantized. If I remove the quantization, the problem goes away\
          \ but the model VRAM consumption then blows out. \n\nHas anyone encountered\
          \ this before and found a work around? \n"
        updatedAt: '2023-10-18T04:58:56.638Z'
      numEdits: 3
      reactions: []
    id: 652f63b2aeb9826ab9e69051
    type: comment
  author: mattma1970
  content: "I'm trying to instruction finetune phi 1.5 on the OpenOrca/SlimOrca dataset.\
    \ I'm using a prompt formatting that uses special tokens ['[INST]','[/INST]','<<SYS>>','<s>','</s>'].\
    \ To get it to run locally on my rtx4090 I need to quantize the model and am using\
    \ bnb 4bit. When I add the special tokens and adjust the embedding size as follows:\n\
    \nspecial_tokens_dict = {'additional_special_tokens': ['[INST]','[/INST]']}\n\
    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\nmodel.resize_token_embeddings(len(tokenizer))\n\
    \nI get an error thrown by the resize_token_embeddings function saying that 'to\
    \ only accepts complex or floats' . The part of that function that is problematic\
    \ is \n\nnew_embeddings.to(\n    embedding_layer.weight.device,\n    dtype=embedding_layer.weight.dtype,\n\
    )\n\nbecause the embedding_layer.weight.dtype, which is got from the last linear\
    \ layer of the model, is torch.uint8. This must be because the model has been\
    \ quantized. If I remove the quantization, the problem goes away but the model\
    \ VRAM consumption then blows out. \n\nHas anyone encountered this before and\
    \ found a work around? \n"
  created_at: 2023-10-18 03:48:50+00:00
  edited: true
  hidden: false
  id: 652f63b2aeb9826ab9e69051
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-30T17:18:17.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9266250133514404
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;mattma1970&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mattma1970\"\
          >@<span class=\"underline\">mattma1970</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Do you need to resize the token's embeddings? This model has been trained\
          \ with a larger vocabulary size to allow for further addition of new tokens.</p>\n\
          <p>Its vocabulary size is 51200 whereas the tokenizer last token is indexed\
          \ at 50294. Even though these extra 906 tokens account in the final number\
          \ of model's parameters, they were not embedded during the training and\
          \ can be used to index extra tokens and fine-tune them.</p>\n"
        raw: 'Hello @mattma1970!


          Do you need to resize the token''s embeddings? This model has been trained
          with a larger vocabulary size to allow for further addition of new tokens.


          Its vocabulary size is 51200 whereas the tokenizer last token is indexed
          at 50294. Even though these extra 906 tokens account in the final number
          of model''s parameters, they were not embedded during the training and can
          be used to index extra tokens and fine-tune them.'
        updatedAt: '2023-10-30T17:18:45.595Z'
      numEdits: 2
      reactions: []
    id: 653fe5595e074bde16c5400c
    type: comment
  author: gugarosa
  content: 'Hello @mattma1970!


    Do you need to resize the token''s embeddings? This model has been trained with
    a larger vocabulary size to allow for further addition of new tokens.


    Its vocabulary size is 51200 whereas the tokenizer last token is indexed at 50294.
    Even though these extra 906 tokens account in the final number of model''s parameters,
    they were not embedded during the training and can be used to index extra tokens
    and fine-tune them.'
  created_at: 2023-10-30 16:18:17+00:00
  edited: true
  hidden: false
  id: 653fe5595e074bde16c5400c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-11-13T18:47:23.000Z'
    data:
      status: closed
    id: 65526f3b24132c2f52a18b7c
    type: status-change
  author: gugarosa
  created_at: 2023-11-13 18:47:23+00:00
  id: 65526f3b24132c2f52a18b7c
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 50
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Can't add special tokens to a model quantized with bitsnbytes
