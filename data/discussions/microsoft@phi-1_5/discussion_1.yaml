!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Michelangiolo
conflicting_files: null
created_at: 2023-09-12 08:23:15+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a439fded431f8151c0f320/2Z2xgT_N7meYzADhlzLJj.jpeg?w=200&h=200&f=face
      fullname: Mazzeschi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Michelangiolo
      type: user
    createdAt: '2023-09-12T09:23:15.000Z'
    data:
      edited: false
      editors:
      - Michelangiolo
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7171623706817627
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/62a439fded431f8151c0f320/2Z2xgT_N7meYzADhlzLJj.jpeg?w=200&h=200&f=face
          fullname: Mazzeschi
          isHf: false
          isPro: false
          name: Michelangiolo
          type: user
        html: '<p>Hi, tried to run it on colab but got this error:</p>

          <p>RuntimeError: "LayerNormKernelImpl" not implemented for ''Half''</p>

          '
        raw: "Hi, tried to run it on colab but got this error:\r\n\r\nRuntimeError:\
          \ \"LayerNormKernelImpl\" not implemented for 'Half'"
        updatedAt: '2023-09-12T09:23:15.033Z'
      numEdits: 0
      reactions: []
    id: 65002e0332d2159207ec6e9c
    type: comment
  author: Michelangiolo
  content: "Hi, tried to run it on colab but got this error:\r\n\r\nRuntimeError:\
    \ \"LayerNormKernelImpl\" not implemented for 'Half'"
  created_at: 2023-09-12 08:23:15+00:00
  edited: false
  hidden: false
  id: 65002e0332d2159207ec6e9c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a096fe08f32ba9d4c2834aa35cccf09e.svg
      fullname: Moustapha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mito0o852
      type: user
    createdAt: '2023-09-12T10:14:35.000Z'
    data:
      edited: true
      editors:
      - mito0o852
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3493691086769104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a096fe08f32ba9d4c2834aa35cccf09e.svg
          fullname: Moustapha
          isHf: false
          isPro: false
          name: mito0o852
          type: user
        html: '<h1 id="cpu">CPU</h1>

          <h1 id="try-it-without-torch_dtypeauto">Try it without torch_dtype="auto"</h1>

          <p>%%capture<br>!pip install transformers<br>!pip install einops</p>

          <p>import torch<br>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True)<br>tokenizer
          = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True)<br>inputs
          = tokenizer(''''''```python<br>def print_prime(n):<br>   """<br>   Print
          all primes between 1 and n<br>   """'''''', return_tensors="pt", return_attention_mask=False)</p>

          <h1 id="inference">Inference</h1>

          <p>inputs = tokenizer(''''''```python<br>def print_prime(n):<br>   """<br>   Print
          all primes between 1 and n<br>   """'''''', return_tensors="pt", return_attention_mask=False)</p>

          <p>outputs = model.generate(**inputs, max_length=200)<br>text = tokenizer.batch_decode(outputs)[0]<br>print(text)</p>

          <h1 id="gpu">GPU</h1>

          <p>%%capture<br>!pip install transformers<br>!pip install einops<br>!pip
          install accelerate</p>

          <p>import torch<br>from transformers import AutoModelForCausalLM, AutoTokenizer</p>

          <p>model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True,
          device_map="cuda:0")<br>tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5",
          trust_remote_code=True, device_map="cuda:0")</p>

          <h1 id="inference-1">Inference</h1>

          <p>inputs = tokenizer(''''''```python<br>def print_prime(n):<br>   """<br>   Print
          all primes between 1 and n<br>   """'''''', return_tensors="pt", return_attention_mask=False)</p>

          <p>inputs.to("cuda:0")<br>outputs = model.generate(**inputs, max_length=50)<br>text
          = tokenizer.batch_decode(outputs)[0]<br>print(text)</p>

          '
        raw: "# CPU\n# Try it without torch_dtype=\"auto\" \n\n%%capture\n!pip install\
          \ transformers\n!pip install einops\n\nimport torch\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True)\ninputs = tokenizer('''```python\n\
          def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n  \
          \ \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\n\n#\
          \ Inference\n\ninputs = tokenizer('''```python\ndef print_prime(n):\n  \
          \ \"\"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"\
          pt\", return_attention_mask=False)\n\noutputs = model.generate(**inputs,\
          \ max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)\n\
          \n\n# GPU \n%%capture\n!pip install transformers\n!pip install einops\n\
          !pip install accelerate\n\nimport torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
          , trust_remote_code=True, device_map=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, device_map=\"cuda:0\")\n\n\
          # Inference\ninputs = tokenizer('''```python\ndef print_prime(n):\n   \"\
          \"\"\n   Print all primes between 1 and n\n   \"\"\"''', return_tensors=\"\
          pt\", return_attention_mask=False)\n\ninputs.to(\"cuda:0\")\noutputs = model.generate(**inputs,\
          \ max_length=50)\ntext = tokenizer.batch_decode(outputs)[0]\nprint(text)"
        updatedAt: '2023-09-12T10:15:40.644Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - batuhandumani
        - gnuevo
    id: 65003a0bb213fc1f1dddb9c4
    type: comment
  author: mito0o852
  content: "# CPU\n# Try it without torch_dtype=\"auto\" \n\n%%capture\n!pip install\
    \ transformers\n!pip install einops\n\nimport torch\nfrom transformers import\
    \ AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True)\ninputs = tokenizer('''```python\n\
    def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n   \"\"\"\
    ''', return_tensors=\"pt\", return_attention_mask=False)\n\n\n# Inference\n\n\
    inputs = tokenizer('''```python\ndef print_prime(n):\n   \"\"\"\n   Print all\
    \ primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\
    \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
    print(text)\n\n\n# GPU \n%%capture\n!pip install transformers\n!pip install einops\n\
    !pip install accelerate\n\nimport torch\nfrom transformers import AutoModelForCausalLM,\
    \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
    , trust_remote_code=True, device_map=\"cuda:0\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True, device_map=\"cuda:0\")\n\n# Inference\n\
    inputs = tokenizer('''```python\ndef print_prime(n):\n   \"\"\"\n   Print all\
    \ primes between 1 and n\n   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\
    \ninputs.to(\"cuda:0\")\noutputs = model.generate(**inputs, max_length=50)\ntext\
    \ = tokenizer.batch_decode(outputs)[0]\nprint(text)"
  created_at: 2023-09-12 09:14:35+00:00
  edited: true
  hidden: false
  id: 65003a0bb213fc1f1dddb9c4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/bb7cc64f55dbca3022e7b8f50d88641c.svg
      fullname: remo raulison de oliveira
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rraulison
      type: user
    createdAt: '2023-09-12T12:07:12.000Z'
    data:
      edited: true
      editors:
      - rraulison
      hidden: false
      identifiedLanguage:
        language: pt
        probability: 0.2195386439561844
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/bb7cc64f55dbca3022e7b8f50d88641c.svg
          fullname: remo raulison de oliveira
          isHf: false
          isPro: false
          name: rraulison
          type: user
        html: "<p>chatgpt correct it for me to run on GPU and its working:</p>\n<pre><code>!pip\
          \ install transformers\n!pip install einops\n!pip install accelerate\n\n\
          import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \n# Carregar o modelo e o tokenizador na GPU\ndevice = \"cuda:0\"\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True).to(device)\n\
          tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\
          \n# Fornecer um c\xF3digo Python v\xE1lido como entrada\ninput_code = '''```python\n\
          def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n  \
          \ \"\"\"'''\n\n# Tokenizar o c\xF3digo\ninputs = tokenizer(input_code, return_tensors=\"\
          pt\").to(device)\n\n# Gerar texto\noutputs = model.generate(input_ids=inputs[\"\
          input_ids\"], max_length=300)\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\
          print(text)\n</code></pre>\n"
        raw: "chatgpt correct it for me to run on GPU and its working:\n\n```\n!pip\
          \ install transformers\n!pip install einops\n!pip install accelerate\n\n\
          import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\
          \n# Carregar o modelo e o tokenizador na GPU\ndevice = \"cuda:0\"\nmodel\
          \ = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True).to(device)\n\
          tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\
          \n# Fornecer um c\xF3digo Python v\xE1lido como entrada\ninput_code = '''```python\n\
          def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n  \
          \ \"\"\"'''\n\n# Tokenizar o c\xF3digo\ninputs = tokenizer(input_code, return_tensors=\"\
          pt\").to(device)\n\n# Gerar texto\noutputs = model.generate(input_ids=inputs[\"\
          input_ids\"], max_length=300)\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\
          print(text)\n\n```"
        updatedAt: '2023-09-12T12:09:04.456Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - CoreyMorris
    id: 65005470a8e31c7722980558
    type: comment
  author: rraulison
  content: "chatgpt correct it for me to run on GPU and its working:\n\n```\n!pip\
    \ install transformers\n!pip install einops\n!pip install accelerate\n\nimport\
    \ torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Carregar\
    \ o modelo e o tokenizador na GPU\ndevice = \"cuda:0\"\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True).to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True)\n\n# Fornecer um c\xF3digo Python\
    \ v\xE1lido como entrada\ninput_code = '''```python\ndef print_prime(n):\n   \"\
    \"\"\n   Print all primes between 1 and n\n   \"\"\"'''\n\n# Tokenizar o c\xF3\
    digo\ninputs = tokenizer(input_code, return_tensors=\"pt\").to(device)\n\n# Gerar\
    \ texto\noutputs = model.generate(input_ids=inputs[\"input_ids\"], max_length=300)\n\
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(text)\n\n\
    ```"
  created_at: 2023-09-12 11:07:12+00:00
  edited: true
  hidden: false
  id: 65005470a8e31c7722980558
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/026409d7bacf84d657f2d8ed474a3ec1.svg
      fullname: Manish Jha
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: manishJha
      type: user
    createdAt: '2023-09-12T12:15:26.000Z'
    data:
      edited: false
      editors:
      - manishJha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3773205578327179
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/026409d7bacf84d657f2d8ed474a3ec1.svg
          fullname: Manish Jha
          isHf: false
          isPro: false
          name: manishJha
          type: user
        html: '<p>model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5",
          trust_remote_code=True, torch_dtype=torch.float32)<br>tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5",
          trust_remote_code=True, torch_dtype=torch.float32)</p>

          <p>solved this for me. </p>

          '
        raw: "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\",\
          \ trust_remote_code=True, torch_dtype=torch.float32)\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32)\n\
          \nsolved this for me. \n"
        updatedAt: '2023-09-12T12:15:26.236Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - summerstay
        - ThetaPhiPsi
    id: 6500565e4b7e0cd1b7d09fe8
    type: comment
  author: manishJha
  content: "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
    \ torch_dtype=torch.float32)\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\"\
    , trust_remote_code=True, torch_dtype=torch.float32)\n\nsolved this for me. \n"
  created_at: 2023-09-12 11:15:26+00:00
  edited: false
  hidden: false
  id: 6500565e4b7e0cd1b7d09fe8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-12T16:02:34.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.978164792060852
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>Hello everyone! I hope everything is going well with you.</p>

          <p>Thanks for the discussion and let us know what the issues were. We will
          fix on the model card and make sure everything is working smoothly.</p>

          <p>Regards,<br>Gustavo.</p>

          '
        raw: 'Hello everyone! I hope everything is going well with you.


          Thanks for the discussion and let us know what the issues were. We will
          fix on the model card and make sure everything is working smoothly.


          Regards,

          Gustavo.'
        updatedAt: '2023-09-12T16:02:34.134Z'
      numEdits: 0
      reactions: []
    id: 65008b9a77745ae9ee741f27
    type: comment
  author: gugarosa
  content: 'Hello everyone! I hope everything is going well with you.


    Thanks for the discussion and let us know what the issues were. We will fix on
    the model card and make sure everything is working smoothly.


    Regards,

    Gustavo.'
  created_at: 2023-09-12 15:02:34+00:00
  edited: false
  hidden: false
  id: 65008b9a77745ae9ee741f27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-15T22:58:57.000Z'
    data:
      status: closed
    id: 6504e1b12a9cebcc9bc152a6
    type: status-change
  author: gugarosa
  created_at: 2023-09-15 21:58:57+00:00
  id: 6504e1b12a9cebcc9bc152a6
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9acf906d77abb05becf676f2c0625151.svg
      fullname: Qin
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: JetQin
      type: user
    createdAt: '2023-09-18T16:23:49.000Z'
    data:
      edited: false
      editors:
      - JetQin
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5336573123931885
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9acf906d77abb05becf676f2c0625151.svg
          fullname: Qin
          isHf: false
          isPro: false
          name: JetQin
          type: user
        html: '<p>try this on mac,<br>model = AutoModelForCausalLM.from_pretrained(model_path,
          trust_remote_code=True, torch_dtype="auto").float()</p>

          '
        raw: "try this on mac, \nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ trust_remote_code=True, torch_dtype=\"auto\").float()\n"
        updatedAt: '2023-09-18T16:23:49.490Z'
      numEdits: 0
      reactions: []
    id: 65087995d95f30b9dcb2b44f
    type: comment
  author: JetQin
  content: "try this on mac, \nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ trust_remote_code=True, torch_dtype=\"auto\").float()\n"
  created_at: 2023-09-18 15:23:49+00:00
  edited: false
  hidden: false
  id: 65087995d95f30b9dcb2b44f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: 'RuntimeError: "LayerNormKernelImpl" not implemented for ''Half'''
