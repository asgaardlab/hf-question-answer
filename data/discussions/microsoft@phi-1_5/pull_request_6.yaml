!!python/object:huggingface_hub.community.DiscussionWithDetails
author: mahimairaja
conflicting_files:
- README.md
created_at: 2023-09-12 14:23:59+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64563a3fa9b8e1fd4cba61e3/SPb8K2koo3rl3HhVyapAi.jpeg?w=200&h=200&f=face
      fullname: Mahimai Raja J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahimairaja
      type: user
    createdAt: '2023-09-12T15:23:59.000Z'
    data:
      edited: false
      editors:
      - mahimairaja
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.603513777256012
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64563a3fa9b8e1fd4cba61e3/SPb8K2koo3rl3HhVyapAi.jpeg?w=200&h=200&f=face
          fullname: Mahimai Raja J
          isHf: false
          isPro: false
          name: mahimairaja
          type: user
        html: "<p>I tried to replicate the <code>sample code</code> in the free versions\
          \ of <code>kaggle</code>, <code>Colab</code> and <code>SageMaker Studio\
          \ Lab</code>.<br>When running on GPU:</p>\n<pre><code class=\"language-bash\"\
          >RuntimeError: Expected all tensors to be on the same device, but found\
          \ at least two devices, cuda:0 and cpu!\n</code></pre>\n<p>And While running\
          \ on CPU:</p>\n<pre><code>RuntimeError: \"LayerNormKernelImpl\" not implemented\
          \ for 'Half'\n</code></pre>\n<p>| Changes I made:</p>\n<pre><code class=\"\
          language-diff\">import torch\nfrom transformers import AutoModelForCausalLM,\
          \ AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
          , trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\n\n<span\
          \ class=\"hljs-addition\">+ device = torch.device(\"cuda:0\")</span>\n<span\
          \ class=\"hljs-addition\">+ model.cuda()</span>\n\ninputs = tokenizer('''```python\n\
          def print_prime(n):\n   \"\"\"\n   Print all primes between 1 and n\n<span\
          \ class=\"hljs-deletion\">-   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)</span>\n\
          <span class=\"hljs-addition\">+   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False).to('cuda')</span>\n\
          \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
          print(text)\n</code></pre>\n<p>I could resolve the runtime issue on <code>GPU</code>\
          \ by adding cuda settiings. I beleive this would help the co-developers\
          \ to try out phi-1_5. Thanks!!!</p>\n"
        raw: "I tried to replicate the `sample code` in the free versions of `kaggle`,\
          \ `Colab` and `SageMaker Studio Lab`. \nWhen running on GPU:\n``` bash\n\
          RuntimeError: Expected all tensors to be on the same device, but found at\
          \ least two devices, cuda:0 and cpu!\n```\n\nAnd While running on CPU:\n\
          ```\nRuntimeError: \"LayerNormKernelImpl\" not implemented for 'Half'\n\
          ```\n\n| Changes I made:\n\n```diff\nimport torch\nfrom transformers import\
          \ AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer\
          \ = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
          \ torch_dtype=\"auto\")\n\n+ device = torch.device(\"cuda:0\")\n+ model.cuda()\n\
          \ninputs = tokenizer('''```python\ndef print_prime(n):\n   \"\"\"\n   Print\
          \ all primes between 1 and n\n-   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\
          +   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n\
          \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
          print(text)\n```\n\n\nI could resolve the runtime issue on `GPU` by adding\
          \ cuda settiings. I beleive this would help the co-developers to try out\
          \ phi-1_5. Thanks!!!"
        updatedAt: '2023-09-12T15:23:59.062Z'
      numEdits: 0
      reactions: []
    id: 6500828fefd273eec78d9367
    type: comment
  author: mahimairaja
  content: "I tried to replicate the `sample code` in the free versions of `kaggle`,\
    \ `Colab` and `SageMaker Studio Lab`. \nWhen running on GPU:\n``` bash\nRuntimeError:\
    \ Expected all tensors to be on the same device, but found at least two devices,\
    \ cuda:0 and cpu!\n```\n\nAnd While running on CPU:\n```\nRuntimeError: \"LayerNormKernelImpl\"\
    \ not implemented for 'Half'\n```\n\n| Changes I made:\n\n```diff\nimport torch\n\
    from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
    \ torch_dtype=\"auto\")\n\n+ device = torch.device(\"cuda:0\")\n+ model.cuda()\n\
    \ninputs = tokenizer('''```python\ndef print_prime(n):\n   \"\"\"\n   Print all\
    \ primes between 1 and n\n-   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n\
    +   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n\
    \noutputs = model.generate(**inputs, max_length=200)\ntext = tokenizer.batch_decode(outputs)[0]\n\
    print(text)\n```\n\n\nI could resolve the runtime issue on `GPU` by adding cuda\
    \ settiings. I beleive this would help the co-developers to try out phi-1_5. Thanks!!!"
  created_at: 2023-09-12 14:23:59+00:00
  edited: false
  hidden: false
  id: 6500828fefd273eec78d9367
  type: comment
- !!python/object:huggingface_hub.community.DiscussionCommit
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64563a3fa9b8e1fd4cba61e3/SPb8K2koo3rl3HhVyapAi.jpeg?w=200&h=200&f=face
      fullname: Mahimai Raja J
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mahimairaja
      type: user
    createdAt: '2023-09-12T15:23:59.000Z'
    data:
      oid: f7c1b27656904b0d050cf820ca8dfdc4aa12c051
      parents:
      - ea95720a352172db6fcbcd89032bfb1cb8481797
      subject: Added cuda config on Sample Code
    id: 6500828f0000000000000000
    type: commit
  author: mahimairaja
  created_at: 2023-09-12 14:23:59+00:00
  id: 6500828f0000000000000000
  oid: f7c1b27656904b0d050cf820ca8dfdc4aa12c051
  summary: Added cuda config on Sample Code
  type: commit
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-12T16:00:57.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9474629759788513
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;mahimairaja&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/mahimairaja\"\
          >@<span class=\"underline\">mahimairaja</span></a></span>\n\n\t</span></span>!\
          \ I hope everything is going well with you.</p>\n<p>Thanks for your feedback,\
          \ we will correct such a problem and mention it on the model card.</p>\n\
          <p>Regards,<br>Gustavo.</p>\n"
        raw: 'Hello @mahimairaja! I hope everything is going well with you.


          Thanks for your feedback, we will correct such a problem and mention it
          on the model card.


          Regards,

          Gustavo.'
        updatedAt: '2023-09-12T16:00:57.579Z'
      numEdits: 0
      reactions: []
    id: 65008b39e102da55f9e6bddb
    type: comment
  author: gugarosa
  content: 'Hello @mahimairaja! I hope everything is going well with you.


    Thanks for your feedback, we will correct such a problem and mention it on the
    model card.


    Regards,

    Gustavo.'
  created_at: 2023-09-12 15:00:57+00:00
  edited: false
  hidden: false
  id: 65008b39e102da55f9e6bddb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-15T22:57:45.000Z'
    data:
      status: closed
    id: 6504e169423b46492e83ff01
    type: status-change
  author: gugarosa
  created_at: 2023-09-15 21:57:45+00:00
  id: 6504e169423b46492e83ff01
  new_status: closed
  type: status-change
is_pull_request: true
merge_commit_oid: null
num: 6
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: refs/heads/main
title: Added cuda config on Sample Code
