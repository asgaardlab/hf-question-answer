!!python/object:huggingface_hub.community.DiscussionWithDetails
author: rchan26
conflicting_files: null
created_at: 2023-09-12 14:32:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg?w=200&h=200&f=face
      fullname: Ryan Chan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rchan26
      type: user
    createdAt: '2023-09-12T15:32:06.000Z'
    data:
      edited: false
      editors:
      - rchan26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8759762644767761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg?w=200&h=200&f=face
          fullname: Ryan Chan
          isHf: false
          isPro: false
          name: rchan26
          type: user
        html: '<p>In the card it states:</p>

          <blockquote>

          <p>In the generation function, our model currently does not support beam
          search (num_beams &gt;1) and `attention_mask'' parameters. Furthermore,
          in the forward pass of the model, we currently do not support outputing
          hidden states or attention values, or using custom input embeddings (instead
          of the model''s).</p>

          </blockquote>

          <p>I was just wondering if there is intention to support <code>attention_mask</code>
          parameters in the future? Has this just not been implemented just yet?</p>

          '
        raw: "In the card it states:\r\n\r\n> In the generation function, our model\
          \ currently does not support beam search (num_beams >1) and `attention_mask'\
          \ parameters. Furthermore, in the forward pass of the model, we currently\
          \ do not support outputing hidden states or attention values, or using custom\
          \ input embeddings (instead of the model's).\r\n\r\nI was just wondering\
          \ if there is intention to support `attention_mask` parameters in the future?\
          \ Has this just not been implemented just yet?"
        updatedAt: '2023-09-12T15:32:06.049Z'
      numEdits: 0
      reactions: []
    id: 65008476520f8d7b4d095d4d
    type: comment
  author: rchan26
  content: "In the card it states:\r\n\r\n> In the generation function, our model\
    \ currently does not support beam search (num_beams >1) and `attention_mask' parameters.\
    \ Furthermore, in the forward pass of the model, we currently do not support outputing\
    \ hidden states or attention values, or using custom input embeddings (instead\
    \ of the model's).\r\n\r\nI was just wondering if there is intention to support\
    \ `attention_mask` parameters in the future? Has this just not been implemented\
    \ just yet?"
  created_at: 2023-09-12 14:32:06+00:00
  edited: false
  hidden: false
  id: 65008476520f8d7b4d095d4d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-12T15:59:24.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9833922982215881
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;rchan26&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rchan26\"\
          >@<span class=\"underline\">rchan26</span></a></span>\n\n\t</span></span>!\
          \ I hope everything is going well with you.</p>\n<p>This was our first deployment\
          \ of a model to HF, so we wanted to be sure everything was running smoothly.\
          \ We already have the <code>attention_mask</code> working out locally and\
          \ our plan is to update both Phi-1 and Phi-1.5 over the next days.</p>\n\
          <p>Regards,<br>Gustavo.</p>\n"
        raw: 'Hello @rchan26! I hope everything is going well with you.


          This was our first deployment of a model to HF, so we wanted to be sure
          everything was running smoothly. We already have the `attention_mask` working
          out locally and our plan is to update both Phi-1 and Phi-1.5 over the next
          days.


          Regards,

          Gustavo.'
        updatedAt: '2023-09-12T15:59:24.892Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - rchan26
        - breadlicker45
        - CooperElektrik
    id: 65008adc6a230e55a66ec4c5
    type: comment
  author: gugarosa
  content: 'Hello @rchan26! I hope everything is going well with you.


    This was our first deployment of a model to HF, so we wanted to be sure everything
    was running smoothly. We already have the `attention_mask` working out locally
    and our plan is to update both Phi-1 and Phi-1.5 over the next days.


    Regards,

    Gustavo.'
  created_at: 2023-09-12 14:59:24+00:00
  edited: false
  hidden: false
  id: 65008adc6a230e55a66ec4c5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg?w=200&h=200&f=face
      fullname: Ryan Chan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rchan26
      type: user
    createdAt: '2023-09-12T17:37:37.000Z'
    data:
      edited: true
      editors: []
      hidden: true
      hiddenBy: ''
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg?w=200&h=200&f=face
          fullname: Ryan Chan
          isHf: false
          isPro: false
          name: rchan26
          type: user
        html: This comment has been hidden
        raw: This comment has been hidden
        updatedAt: '2023-09-13T08:37:03.548Z'
      numEdits: 0
      reactions: []
    id: 6500a1e10e8369f6a8f68604
    type: comment
  author: rchan26
  content: This comment has been hidden
  created_at: 2023-09-12 16:37:37+00:00
  edited: true
  hidden: true
  id: 6500a1e10e8369f6a8f68604
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg?w=200&h=200&f=face
      fullname: Ryan Chan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: rchan26
      type: user
    createdAt: '2023-09-13T08:37:47.000Z'
    data:
      edited: false
      editors:
      - rchan26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9074136018753052
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg?w=200&h=200&f=face
          fullname: Ryan Chan
          isHf: false
          isPro: false
          name: rchan26
          type: user
        html: "<p>Thanks! looking forward to testing it out! \U0001F604</p>\n"
        raw: "Thanks! looking forward to testing it out! \U0001F604"
        updatedAt: '2023-09-13T08:37:47.228Z'
      numEdits: 0
      reactions: []
    id: 650174db4ebb666058a4892d
    type: comment
  author: rchan26
  content: "Thanks! looking forward to testing it out! \U0001F604"
  created_at: 2023-09-13 07:37:47+00:00
  edited: false
  hidden: false
  id: 650174db4ebb666058a4892d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
      fullname: ben nicholl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bennicholl
      type: user
    createdAt: '2023-09-24T20:29:43.000Z'
    data:
      edited: true
      editors:
      - bennicholl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.924491822719574
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
          fullname: ben nicholl
          isHf: false
          isPro: false
          name: bennicholl
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span><br>I\
          \ assume the attention masks parameter isn't set up yet because you're using\
          \ a torch.nn.Sequential wrapper.<br>Has the team created a custom torch\
          \ class to square this away yet? Looking to fine tune and run inference\
          \ in batches. If it's not something in the pipeline, I'll prolly just write\
          \ the custom torch class myself. But if it is something that gonna get squared\
          \ away soon, I won't waste my time. Lemme know and thanks.</p>\n"
        raw: 'Hey @gugarosa

          I assume the attention masks parameter isn''t set up yet because you''re
          using a torch.nn.Sequential wrapper.

          Has the team created a custom torch class to square this away yet? Looking
          to fine tune and run inference in batches. If it''s not something in the
          pipeline, I''ll prolly just write the custom torch class myself. But if
          it is something that gonna get squared away soon, I won''t waste my time.
          Lemme know and thanks.'
        updatedAt: '2023-09-24T23:25:04.745Z'
      numEdits: 1
      reactions: []
    id: 65109c3757343388e5a3ed5b
    type: comment
  author: bennicholl
  content: 'Hey @gugarosa

    I assume the attention masks parameter isn''t set up yet because you''re using
    a torch.nn.Sequential wrapper.

    Has the team created a custom torch class to square this away yet? Looking to
    fine tune and run inference in batches. If it''s not something in the pipeline,
    I''ll prolly just write the custom torch class myself. But if it is something
    that gonna get squared away soon, I won''t waste my time. Lemme know and thanks.'
  created_at: 2023-09-24 19:29:43+00:00
  edited: true
  hidden: false
  id: 65109c3757343388e5a3ed5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-26T18:34:06.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9324746131896973
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;rchan26&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rchan26\"\
          >@<span class=\"underline\">rchan26</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;bennicholl&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bennicholl\">@<span class=\"\
          underline\">bennicholl</span></a></span>\n\n\t</span></span>!</p>\n<p>I\
          \ just updated the model files and added the <code>attention_mask</code>\
          \ support. Sorry for taking so much time. This should server as a proxy\
          \ till Phi get's fully implemented in <code>transformers</code>.</p>\n<p>However,\
          \ please note that we still do not have support for <code>attention_mask</code>\
          \ during training/fine-tuning, only inference. But this shouldn't be a problem\
          \ in adding in the next upcoming days.</p>\n"
        raw: 'Hello @rchan26 and @bennicholl!


          I just updated the model files and added the `attention_mask` support. Sorry
          for taking so much time. This should server as a proxy till Phi get''s fully
          implemented in `transformers`.


          However, please note that we still do not have support for `attention_mask`
          during training/fine-tuning, only inference. But this shouldn''t be a problem
          in adding in the next upcoming days.'
        updatedAt: '2023-09-26T18:34:06.357Z'
      numEdits: 0
      reactions: []
    id: 6513241e86d74f32ed4a01b6
    type: comment
  author: gugarosa
  content: 'Hello @rchan26 and @bennicholl!


    I just updated the model files and added the `attention_mask` support. Sorry for
    taking so much time. This should server as a proxy till Phi get''s fully implemented
    in `transformers`.


    However, please note that we still do not have support for `attention_mask` during
    training/fine-tuning, only inference. But this shouldn''t be a problem in adding
    in the next upcoming days.'
  created_at: 2023-09-26 17:34:06+00:00
  edited: false
  hidden: false
  id: 6513241e86d74f32ed4a01b6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
      fullname: ben nicholl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bennicholl
      type: user
    createdAt: '2023-09-26T20:30:48.000Z'
    data:
      edited: false
      editors:
      - bennicholl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.557031512260437
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
          fullname: ben nicholl
          isHf: false
          isPro: false
          name: bennicholl
          type: user
        html: '<p>Thanks for working on this! When I try and perform inference with
          below code</p>

          <p>model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True,
          torch_dtype="auto")<br>tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5",
          trust_remote_code=True, torch_dtype="auto")<br>model(input_ids = torch.tensor(tokened_words),
          attention_mask = torch.tensor(attention_mask))</p>

          <p>I get a value error<br>ValueError: not enough values to unpack (expected
          3, got 2)</p>

          <p>It seems the error is derived from below<br>    541     kv = update_kv_cache(qkv[:,
          :, 1:], past_key_values, self.layer_idx)<br>    543 if attention_mask is
          not None:<br>--&gt; 544     attention_mask, cu_seqlens, max_seqlen = attention_mask<br>    545     attention_mask
          = attention_mask.to(qkv.device)<br>    547 attention_kwargs = {"attention_mask":
          attention_mask}</p>

          <p>But I''m not sure whats going on with that code</p>

          '
        raw: "Thanks for working on this! When I try and perform inference with below\
          \ code\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
          , trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\nmodel(input_ids\
          \ = torch.tensor(tokened_words), attention_mask = torch.tensor(attention_mask))\n\
          \nI get a value error\nValueError: not enough values to unpack (expected\
          \ 3, got 2)\n\nIt seems the error is derived from below\n    541     kv\
          \ = update_kv_cache(qkv[:, :, 1:], past_key_values, self.layer_idx)\n  \
          \  543 if attention_mask is not None:\n--> 544     attention_mask, cu_seqlens,\
          \ max_seqlen = attention_mask\n    545     attention_mask = attention_mask.to(qkv.device)\n\
          \    547 attention_kwargs = {\"attention_mask\": attention_mask}\n\nBut\
          \ I'm not sure whats going on with that code"
        updatedAt: '2023-09-26T20:30:48.583Z'
      numEdits: 0
      reactions: []
    id: 65133f78442f1142ff2548c9
    type: comment
  author: bennicholl
  content: "Thanks for working on this! When I try and perform inference with below\
    \ code\n\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\",\
    \ trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\nmodel(input_ids\
    \ = torch.tensor(tokened_words), attention_mask = torch.tensor(attention_mask))\n\
    \nI get a value error\nValueError: not enough values to unpack (expected 3, got\
    \ 2)\n\nIt seems the error is derived from below\n    541     kv = update_kv_cache(qkv[:,\
    \ :, 1:], past_key_values, self.layer_idx)\n    543 if attention_mask is not None:\n\
    --> 544     attention_mask, cu_seqlens, max_seqlen = attention_mask\n    545 \
    \    attention_mask = attention_mask.to(qkv.device)\n    547 attention_kwargs\
    \ = {\"attention_mask\": attention_mask}\n\nBut I'm not sure whats going on with\
    \ that code"
  created_at: 2023-09-26 19:30:48+00:00
  edited: false
  hidden: false
  id: 65133f78442f1142ff2548c9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-26T21:25:10.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9150592684745789
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>My bad <span data-props=\"{&quot;user&quot;:&quot;bennicholl&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/bennicholl\"\
          >@<span class=\"underline\">bennicholl</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Just fixed this. We use the <code>flash-attn</code> style for performing\
          \ cached inference and the attention layer was not aware that <code>attention_mask</code>\
          \ could be passed as a single tensor.</p>\n<p>Should be working now, tested\
          \ an inference in the way you posted and it worked!</p>\n"
        raw: 'My bad @bennicholl!


          Just fixed this. We use the `flash-attn` style for performing cached inference
          and the attention layer was not aware that `attention_mask` could be passed
          as a single tensor.


          Should be working now, tested an inference in the way you posted and it
          worked!'
        updatedAt: '2023-09-26T21:25:10.277Z'
      numEdits: 0
      reactions: []
    id: 65134c36e4cad2919f4f39ca
    type: comment
  author: gugarosa
  content: 'My bad @bennicholl!


    Just fixed this. We use the `flash-attn` style for performing cached inference
    and the attention layer was not aware that `attention_mask` could be passed as
    a single tensor.


    Should be working now, tested an inference in the way you posted and it worked!'
  created_at: 2023-09-26 20:25:10+00:00
  edited: false
  hidden: false
  id: 65134c36e4cad2919f4f39ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
      fullname: ben nicholl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bennicholl
      type: user
    createdAt: '2023-09-27T13:43:29.000Z'
    data:
      edited: true
      editors:
      - bennicholl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.49865567684173584
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
          fullname: ben nicholl
          isHf: false
          isPro: false
          name: bennicholl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span>  Thanks for the\
          \ quick response man! I think they're may be a bug in the attention masking.\
          \ The output for a sentence is different if I run two examples instead of\
          \ one. Here is some code to reproduce.</p>\n<p>model = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")<br>tokenizer\
          \ = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
          \ torch_dtype=\"auto\")<br>tokenizer.pad_token = tokenizer.eos_token</p>\n\
          <p>#HERE IS CODE RUNNING ONE SENTENCE<br>encoded_inputs = tokenizer([\"\
          this is the first sentence\"])<br>print(encoded_inputs ) = {'input_ids':\
          \ [[5661, 318, 262, 717, 6827]], 'attention_mask': [[1, 1, 1, 1, 1]]}</p>\n\
          <p>tokened_words = encoded_inputs['input_ids']<br>attention_mask = encoded_inputs['attention_mask']<br>model(input_ids\
          \ = torch.tensor(tokened_words), attention_mask = torch.tensor(attention_mask))<br>OUTPUT:<br>CausalLMOutputWithPast(loss=None,\
          \ logits=tensor([[[15.9766, 16.5625, 13.4219,  ...,  2.6074,  2.6074,  2.6074],<br>\
          \         [12.3047, 15.2344, 10.3672,  ...,  2.3027,  2.3047,  2.3027],<br>\
          \         [ 8.8672, 11.7188,  6.6055,  ...,  1.0361,  1.0371,  1.0371],<br>\
          \         [12.4844, 13.6406,  7.1406,  ...,  0.2700,  0.2722,  0.2703],<br>\
          \         [20.4688, 22.5625, 14.8438,  ...,  3.3477,  3.3477,  3.3457]]],</p>\n\
          <p>#HERE IS CODE RUNNING TWO SENTENCES<br>encoded_inputs = tokenizer([\"\
          this is the first sentence\", \"this is another sentence and is longer than\
          \ the first\"], padding = 'longest')<br>print(encoded_inputs ) = {'input_ids':\
          \ [[5661, 318, 262, 717, 6827, 50256, 50256, 50256, 50256, 50256], [5661,\
          \ 318, 1194, 6827, 290, 318, 2392, 621, 262, 717]], 'attention_mask': [[1,\
          \ 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} # MASKING\
          \ LOOKS CORRECT</p>\n<p>tokened_words = encoded_inputs['input_ids']<br>attention_mask\
          \ = encoded_inputs['attention_mask']<br>OUTPUT:<br>CausalLMOutputWithPast(loss=None,\
          \ logits=tensor([[[15.9688, 16.5625, 13.4219,  ...,  2.6074,  2.6094,  2.6074],<br>\
          \         [12.3125, 15.2344, 10.3672,  ...,  2.3047,  2.3047,  2.3047],\
          \   # NOTICE SOME OF THE VALUES, SUCH AS THE VERY FIRST VALUE IN UPPER LEFT\
          \ HAND CORNER IS DIFFERENT THAN THE<br>         [ 8.8672, 11.7188,  6.6055,\
          \  ...,  1.0391,  1.0400,  1.0400],      # VALUE IN THE SAME LOCATION IN\
          \ THE FIRST MATRIX<br>         ...,<br>         [13.9922, 17.0156, 18.8750,\
          \  ...,  2.4453,  2.4453,  2.4453],<br>         [13.8750, 16.8750, 18.7500,\
          \  ...,  2.4082,  2.4062,  2.4062],<br>         [13.7109, 16.6094, 18.5625,\
          \  ...,  2.3477,  2.3457,  2.3457]],</p>\n<pre><code>    [[15.9688, 16.5625,\
          \ 13.4219,  ...,  2.6074,  2.6094,  2.6074],\n     [12.3125, 15.2344, 10.3672,\
          \  ...,  2.3047,  2.3047,  2.3047],\n     [12.3125, 14.6250,  7.8828,  ...,\
          \  0.5962,  0.5967,  0.5972],\n     ...,\n     [10.6875, 15.7188,  9.0234,\
          \  ...,  1.4434,  1.4424,  1.4414],\n     [ 8.5469, 12.7188,  6.2656,  ...,\
          \  0.2693,  0.2688,  0.2676],\n     [17.5000, 20.3906, 12.9453,  ...,  2.2891,\
          \  2.2891,  2.2891]]],\n</code></pre>\n<p>some of the values in the first\
          \ matrix are different than the values in the first output of the second\
          \ matrix. For example the first value in the upper right hand corner, 15.9766\
          \ and 15.9688 should be the same, but they are slightly different </p>\n"
        raw: "@gugarosa  Thanks for the quick response man! I think they're may be\
          \ a bug in the attention masking. The output for a sentence is different\
          \ if I run two examples instead of one. Here is some code to reproduce.\n\
          \nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
          \ torch_dtype=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\"\
          , trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer.pad_token = tokenizer.eos_token\n\
          \n#HERE IS CODE RUNNING ONE SENTENCE\nencoded_inputs = tokenizer([\"this\
          \ is the first sentence\"])\nprint(encoded_inputs ) = {'input_ids': [[5661,\
          \ 318, 262, 717, 6827]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n\ntokened_words\
          \ = encoded_inputs['input_ids']\nattention_mask = encoded_inputs['attention_mask']\n\
          model(input_ids = torch.tensor(tokened_words), attention_mask = torch.tensor(attention_mask))\n\
          OUTPUT:\nCausalLMOutputWithPast(loss=None, logits=tensor([[[15.9766, 16.5625,\
          \ 13.4219,  ...,  2.6074,  2.6074,  2.6074],\n         [12.3047, 15.2344,\
          \ 10.3672,  ...,  2.3027,  2.3047,  2.3027],\n         [ 8.8672, 11.7188,\
          \  6.6055,  ...,  1.0361,  1.0371,  1.0371],\n         [12.4844, 13.6406,\
          \  7.1406,  ...,  0.2700,  0.2722,  0.2703],\n         [20.4688, 22.5625,\
          \ 14.8438,  ...,  3.3477,  3.3477,  3.3457]]],\n\n#HERE IS CODE RUNNING\
          \ TWO SENTENCES\nencoded_inputs = tokenizer([\"this is the first sentence\"\
          , \"this is another sentence and is longer than the first\"], padding =\
          \ 'longest')\nprint(encoded_inputs ) = {'input_ids': [[5661, 318, 262, 717,\
          \ 6827, 50256, 50256, 50256, 50256, 50256], [5661, 318, 1194, 6827, 290,\
          \ 318, 2392, 621, 262, 717]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0,\
          \ 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} # MASKING LOOKS CORRECT\n\ntokened_words\
          \ = encoded_inputs['input_ids']\nattention_mask = encoded_inputs['attention_mask']\n\
          OUTPUT:\nCausalLMOutputWithPast(loss=None, logits=tensor([[[15.9688, 16.5625,\
          \ 13.4219,  ...,  2.6074,  2.6094,  2.6074],\n         [12.3125, 15.2344,\
          \ 10.3672,  ...,  2.3047,  2.3047,  2.3047],   # NOTICE SOME OF THE VALUES,\
          \ SUCH AS THE VERY FIRST VALUE IN UPPER LEFT HAND CORNER IS DIFFERENT THAN\
          \ THE \n         [ 8.8672, 11.7188,  6.6055,  ...,  1.0391,  1.0400,  1.0400],\
          \      # VALUE IN THE SAME LOCATION IN THE FIRST MATRIX\n         ...,\n\
          \         [13.9922, 17.0156, 18.8750,  ...,  2.4453,  2.4453,  2.4453],\n\
          \         [13.8750, 16.8750, 18.7500,  ...,  2.4082,  2.4062,  2.4062],\n\
          \         [13.7109, 16.6094, 18.5625,  ...,  2.3477,  2.3457,  2.3457]],\n\
          \n        [[15.9688, 16.5625, 13.4219,  ...,  2.6074,  2.6094,  2.6074],\n\
          \         [12.3125, 15.2344, 10.3672,  ...,  2.3047,  2.3047,  2.3047],\n\
          \         [12.3125, 14.6250,  7.8828,  ...,  0.5962,  0.5967,  0.5972],\n\
          \         ...,\n         [10.6875, 15.7188,  9.0234,  ...,  1.4434,  1.4424,\
          \  1.4414],\n         [ 8.5469, 12.7188,  6.2656,  ...,  0.2693,  0.2688,\
          \  0.2676],\n         [17.5000, 20.3906, 12.9453,  ...,  2.2891,  2.2891,\
          \  2.2891]]],\n\n\nsome of the values in the first matrix are different\
          \ than the values in the first output of the second matrix. For example\
          \ the first value in the upper right hand corner, 15.9766 and 15.9688 should\
          \ be the same, but they are slightly different "
        updatedAt: '2023-09-27T13:54:11.932Z'
      numEdits: 2
      reactions: []
    id: 651431810aa4e2fff996cf01
    type: comment
  author: bennicholl
  content: "@gugarosa  Thanks for the quick response man! I think they're may be a\
    \ bug in the attention masking. The output for a sentence is different if I run\
    \ two examples instead of one. Here is some code to reproduce.\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=\"auto\")\ntokenizer\
    \ = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True,\
    \ torch_dtype=\"auto\")\ntokenizer.pad_token = tokenizer.eos_token\n\n#HERE IS\
    \ CODE RUNNING ONE SENTENCE\nencoded_inputs = tokenizer([\"this is the first sentence\"\
    ])\nprint(encoded_inputs ) = {'input_ids': [[5661, 318, 262, 717, 6827]], 'attention_mask':\
    \ [[1, 1, 1, 1, 1]]}\n\ntokened_words = encoded_inputs['input_ids']\nattention_mask\
    \ = encoded_inputs['attention_mask']\nmodel(input_ids = torch.tensor(tokened_words),\
    \ attention_mask = torch.tensor(attention_mask))\nOUTPUT:\nCausalLMOutputWithPast(loss=None,\
    \ logits=tensor([[[15.9766, 16.5625, 13.4219,  ...,  2.6074,  2.6074,  2.6074],\n\
    \         [12.3047, 15.2344, 10.3672,  ...,  2.3027,  2.3047,  2.3027],\n    \
    \     [ 8.8672, 11.7188,  6.6055,  ...,  1.0361,  1.0371,  1.0371],\n        \
    \ [12.4844, 13.6406,  7.1406,  ...,  0.2700,  0.2722,  0.2703],\n         [20.4688,\
    \ 22.5625, 14.8438,  ...,  3.3477,  3.3477,  3.3457]]],\n\n#HERE IS CODE RUNNING\
    \ TWO SENTENCES\nencoded_inputs = tokenizer([\"this is the first sentence\", \"\
    this is another sentence and is longer than the first\"], padding = 'longest')\n\
    print(encoded_inputs ) = {'input_ids': [[5661, 318, 262, 717, 6827, 50256, 50256,\
    \ 50256, 50256, 50256], [5661, 318, 1194, 6827, 290, 318, 2392, 621, 262, 717]],\
    \ 'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1,\
    \ 1, 1]]} # MASKING LOOKS CORRECT\n\ntokened_words = encoded_inputs['input_ids']\n\
    attention_mask = encoded_inputs['attention_mask']\nOUTPUT:\nCausalLMOutputWithPast(loss=None,\
    \ logits=tensor([[[15.9688, 16.5625, 13.4219,  ...,  2.6074,  2.6094,  2.6074],\n\
    \         [12.3125, 15.2344, 10.3672,  ...,  2.3047,  2.3047,  2.3047],   # NOTICE\
    \ SOME OF THE VALUES, SUCH AS THE VERY FIRST VALUE IN UPPER LEFT HAND CORNER IS\
    \ DIFFERENT THAN THE \n         [ 8.8672, 11.7188,  6.6055,  ...,  1.0391,  1.0400,\
    \  1.0400],      # VALUE IN THE SAME LOCATION IN THE FIRST MATRIX\n         ...,\n\
    \         [13.9922, 17.0156, 18.8750,  ...,  2.4453,  2.4453,  2.4453],\n    \
    \     [13.8750, 16.8750, 18.7500,  ...,  2.4082,  2.4062,  2.4062],\n        \
    \ [13.7109, 16.6094, 18.5625,  ...,  2.3477,  2.3457,  2.3457]],\n\n        [[15.9688,\
    \ 16.5625, 13.4219,  ...,  2.6074,  2.6094,  2.6074],\n         [12.3125, 15.2344,\
    \ 10.3672,  ...,  2.3047,  2.3047,  2.3047],\n         [12.3125, 14.6250,  7.8828,\
    \  ...,  0.5962,  0.5967,  0.5972],\n         ...,\n         [10.6875, 15.7188,\
    \  9.0234,  ...,  1.4434,  1.4424,  1.4414],\n         [ 8.5469, 12.7188,  6.2656,\
    \  ...,  0.2693,  0.2688,  0.2676],\n         [17.5000, 20.3906, 12.9453,  ...,\
    \  2.2891,  2.2891,  2.2891]]],\n\n\nsome of the values in the first matrix are\
    \ different than the values in the first output of the second matrix. For example\
    \ the first value in the upper right hand corner, 15.9766 and 15.9688 should be\
    \ the same, but they are slightly different "
  created_at: 2023-09-27 12:43:29+00:00
  edited: true
  hidden: false
  id: 651431810aa4e2fff996cf01
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-27T14:09:46.000Z'
    data:
      edited: true
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8496261835098267
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;bennicholl&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bennicholl\">@<span class=\"\
          underline\">bennicholl</span></a></span>\n\n\t</span></span> I found the\
          \ issue, it was related to the precision.<code>torch_dtype=\"auto\"</code>\
          \ was forcing the model to use FP16 (maybe with <code>model.half()</code>),\
          \ whereas the model is expected to be used with AMP, as follows:</p>\n<pre><code>model\
          \ = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\
          with torch.autocast(model.device.type, dtype=torch.float16, enabled=True):\n\
          \  model(input_ids = torch.tensor(tokened_words), attention_mask = torch.tensor(attention_mask))\n\
          </code></pre>\n<p>I compared the logits and now they seem to match, and\
          \ updated the readme with this information. Regarding the source of the\
          \ issue, I need to double check, but it should be related to the <code>RotaryEmbedding</code>\
          \ class.</p>\n"
        raw: "@bennicholl I found the issue, it was related to the precision.`torch_dtype=\"\
          auto\"` was forcing the model to use FP16 (maybe with `model.half()`), whereas\
          \ the model is expected to be used with AMP, as follows:\n\n```\nmodel =\
          \ AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n\
          with torch.autocast(model.device.type, dtype=torch.float16, enabled=True):\n\
          \  model(input_ids = torch.tensor(tokened_words), attention_mask = torch.tensor(attention_mask))\n\
          ```\n\nI compared the logits and now they seem to match, and updated the\
          \ readme with this information. Regarding the source of the issue, I need\
          \ to double check, but it should be related to the `RotaryEmbedding` class."
        updatedAt: '2023-09-27T14:10:17.748Z'
      numEdits: 1
      reactions: []
    id: 651437aadbdff088363a0b27
    type: comment
  author: gugarosa
  content: "@bennicholl I found the issue, it was related to the precision.`torch_dtype=\"\
    auto\"` was forcing the model to use FP16 (maybe with `model.half()`), whereas\
    \ the model is expected to be used with AMP, as follows:\n\n```\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True)\nwith torch.autocast(model.device.type,\
    \ dtype=torch.float16, enabled=True):\n  model(input_ids = torch.tensor(tokened_words),\
    \ attention_mask = torch.tensor(attention_mask))\n```\n\nI compared the logits\
    \ and now they seem to match, and updated the readme with this information. Regarding\
    \ the source of the issue, I need to double check, but it should be related to\
    \ the `RotaryEmbedding` class."
  created_at: 2023-09-27 13:09:46+00:00
  edited: true
  hidden: false
  id: 651437aadbdff088363a0b27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
      fullname: ben nicholl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bennicholl
      type: user
    createdAt: '2023-09-28T01:08:49.000Z'
    data:
      edited: false
      editors:
      - bennicholl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9525464177131653
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
          fullname: ben nicholl
          isHf: false
          isPro: false
          name: bennicholl
          type: user
        html: '<p>Awesome, thanks man. Seems to be good now</p>

          '
        raw: Awesome, thanks man. Seems to be good now
        updatedAt: '2023-09-28T01:08:49.797Z'
      numEdits: 0
      reactions: []
    id: 6514d221e09183e214d0c82c
    type: comment
  author: bennicholl
  content: Awesome, thanks man. Seems to be good now
  created_at: 2023-09-28 00:08:49+00:00
  edited: false
  hidden: false
  id: 6514d221e09183e214d0c82c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-03T14:35:39.000Z'
    data:
      status: closed
    id: 651c26bb497e4d326780d6ce
    type: status-change
  author: gugarosa
  created_at: 2023-10-03 13:35:39+00:00
  id: 651c26bb497e4d326780d6ce
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-10-08T23:18:42.000Z'
    data:
      edited: false
      editors:
      - zokica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8740024566650391
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: "<blockquote>\n<p>Hello <span data-props=\"{&quot;user&quot;:&quot;rchan26&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/rchan26\"\
          >@<span class=\"underline\">rchan26</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;bennicholl&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/bennicholl\">@<span class=\"\
          underline\">bennicholl</span></a></span>\n\n\t</span></span>!</p>\n<p>I\
          \ just updated the model files and added the <code>attention_mask</code>\
          \ support. Sorry for taking so much time. This should server as a proxy\
          \ till Phi get's fully implemented in <code>transformers</code>.</p>\n<p>However,\
          \ please note that we still do not have support for <code>attention_mask</code>\
          \ during training/fine-tuning, only inference. But this shouldn't be a problem\
          \ in adding in the next upcoming days.</p>\n</blockquote>\n<p><span data-props=\"\
          {&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/gugarosa\">@<span class=\"underline\">gugarosa</span></a></span>\n\
          \n\t</span></span></p>\n<p>Hello, thanks for adding the attention_mask,\
          \   seem like it still does not work for  fine-tuning. Is it possible to\
          \ add it, or maybe disable it somehow, in the HF trainer (trainer = transformers.Trainer)\
          \ where data is data_collator=transformers.DataCollatorForSeq2Seq(tokenizer,\
          \ pad_to_multiple_of=8, return_tensors=\"pt\", padding=True) </p>\n"
        raw: "> Hello @rchan26 and @bennicholl!\n> \n> I just updated the model files\
          \ and added the `attention_mask` support. Sorry for taking so much time.\
          \ This should server as a proxy till Phi get's fully implemented in `transformers`.\n\
          > \n> However, please note that we still do not have support for `attention_mask`\
          \ during training/fine-tuning, only inference. But this shouldn't be a problem\
          \ in adding in the next upcoming days.\n\n@gugarosa\n\nHello, thanks for\
          \ adding the attention_mask,   seem like it still does not work for  fine-tuning.\
          \ Is it possible to add it, or maybe disable it somehow, in the HF trainer\
          \ (trainer = transformers.Trainer) where data is data_collator=transformers.DataCollatorForSeq2Seq(tokenizer,\
          \ pad_to_multiple_of=8, return_tensors=\"pt\", padding=True) \n \n\n\n\n\
          \n"
        updatedAt: '2023-10-08T23:18:42.052Z'
      numEdits: 0
      reactions: []
    id: 652338d2d89bc7773de49a4c
    type: comment
  author: zokica
  content: "> Hello @rchan26 and @bennicholl!\n> \n> I just updated the model files\
    \ and added the `attention_mask` support. Sorry for taking so much time. This\
    \ should server as a proxy till Phi get's fully implemented in `transformers`.\n\
    > \n> However, please note that we still do not have support for `attention_mask`\
    \ during training/fine-tuning, only inference. But this shouldn't be a problem\
    \ in adding in the next upcoming days.\n\n@gugarosa\n\nHello, thanks for adding\
    \ the attention_mask,   seem like it still does not work for  fine-tuning. Is\
    \ it possible to add it, or maybe disable it somehow, in the HF trainer (trainer\
    \ = transformers.Trainer) where data is data_collator=transformers.DataCollatorForSeq2Seq(tokenizer,\
    \ pad_to_multiple_of=8, return_tensors=\"pt\", padding=True) \n \n\n\n\n\n"
  created_at: 2023-10-08 22:18:42+00:00
  edited: false
  hidden: false
  id: 652338d2d89bc7773de49a4c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-10-09T12:30:57.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6238024830818176
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Maybe cc <span data-props=\"{&quot;user&quot;:&quot;muellerzr&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/muellerzr\"\
          >@<span class=\"underline\">muellerzr</span></a></span>\n\n\t</span></span>\
          \ regarding the trainer question</p>\n"
        raw: Maybe cc @muellerzr regarding the trainer question
        updatedAt: '2023-10-09T12:30:57.198Z'
      numEdits: 0
      reactions: []
    id: 6523f281d6ecb59a1b4d7edd
    type: comment
  author: lysandre
  content: Maybe cc @muellerzr regarding the trainer question
  created_at: 2023-10-09 11:30:57+00:00
  edited: false
  hidden: false
  id: 6523f281d6ecb59a1b4d7edd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
      fullname: Zengzhi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinclairWang
      type: user
    createdAt: '2023-10-13T05:17:08.000Z'
    data:
      edited: false
      editors:
      - SinclairWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9371787309646606
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
          fullname: Zengzhi Wang
          isHf: false
          isPro: false
          name: SinclairWang
          type: user
        html: '<p>Any plan for supporting training for the phi series?</p>

          '
        raw: Any plan for supporting training for the phi series?
        updatedAt: '2023-10-13T05:17:08.543Z'
      numEdits: 0
      reactions: []
    id: 6528d2d4cab031ebd274ceaa
    type: comment
  author: SinclairWang
  content: Any plan for supporting training for the phi series?
  created_at: 2023-10-13 04:17:08+00:00
  edited: false
  hidden: false
  id: 6528d2d4cab031ebd274ceaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
      fullname: ben nicholl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bennicholl
      type: user
    createdAt: '2023-10-13T13:35:58.000Z'
    data:
      edited: true
      editors:
      - bennicholl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5962885022163391
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
          fullname: ben nicholl
          isHf: false
          isPro: false
          name: bennicholl
          type: user
        html: '<p>What do we mean when we say this model is not supported for training?
          if I call</p>

          <p>loss = phi_model(x_batch, attention_mask=mask_batch, labels = labels_batch)[0]<br>loss.backward()<br>optimizer.step()<br>optimizer.zero_grad()</p>

          <p>will the gradient no compute properly? Is this due to the mask not zero''ing
          out the tokens that should be masked during backprop?</p>

          '
        raw: 'What do we mean when we say this model is not supported for training?
          if I call


          loss = phi_model(x_batch, attention_mask=mask_batch, labels = labels_batch)[0]

          loss.backward()

          optimizer.step()

          optimizer.zero_grad()


          will the gradient no compute properly? Is this due to the mask not zero''ing
          out the tokens that should be masked during backprop?'
        updatedAt: '2023-10-13T16:20:02.168Z'
      numEdits: 1
      reactions: []
    id: 652947be94bb032403e92cf2
    type: comment
  author: bennicholl
  content: 'What do we mean when we say this model is not supported for training?
    if I call


    loss = phi_model(x_batch, attention_mask=mask_batch, labels = labels_batch)[0]

    loss.backward()

    optimizer.step()

    optimizer.zero_grad()


    will the gradient no compute properly? Is this due to the mask not zero''ing out
    the tokens that should be masked during backprop?'
  created_at: 2023-10-13 12:35:58+00:00
  edited: true
  hidden: false
  id: 652947be94bb032403e92cf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
      fullname: Zengzhi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinclairWang
      type: user
    createdAt: '2023-10-14T00:30:44.000Z'
    data:
      edited: false
      editors:
      - SinclairWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8951575756072998
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
          fullname: Zengzhi Wang
          isHf: false
          isPro: false
          name: SinclairWang
          type: user
        html: "<p>Yeah, maybe. During my fine-tuning, I encountered a warning:</p>\n\
          <pre><code>`attention_mask` is not supported during training. Using it might\
          \ lead to unexpected results.\n{'loss': 1.3228, 'learning_rate': 1.999875577156579e-05,\
          \ 'epoch': 0.02}\n  1%|\u258D                                          \
          \                                                   | 300/59745 [06:19&lt;20:47:29,\
          \  1.26s/it]`attention_mask` is not supported during training. Using it\
          \ might lead to unexpected results.\n  1%|\u258D                       \
          \                                                                      |\
          \ 301/59745 [06:20&lt;20:48:14,  1.26s/it]`attention_mask` is not supported\
          \ during training. Using it might lead to unexpected results.\n  1%|\u258D\
          \                                                                      \
          \                       | 302/59745 [06:22&lt;20:48:01,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 303/59745 [06:23&lt;20:47:31,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 304/59745 [06:24&lt;20:48:13,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 305/59745 [06:25&lt;20:49:27,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 306/59745 [06:27&lt;20:48:52,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 307/59745 [06:28&lt;20:48:29,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 308/59745 [06:29&lt;20:49:14,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 309/59745 [06:30&lt;20:49:49,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          {'loss': 1.5263, 'learning_rate': 1.9998671442394832e-05, 'epoch': 0.02}\n\
          </code></pre>\n"
        raw: "Yeah, maybe. During my fine-tuning, I encountered a warning:\n\n```\n\
          `attention_mask` is not supported during training. Using it might lead to\
          \ unexpected results.\n{'loss': 1.3228, 'learning_rate': 1.999875577156579e-05,\
          \ 'epoch': 0.02}\n  1%|\u258D                                          \
          \                                                   | 300/59745 [06:19<20:47:29,\
          \  1.26s/it]`attention_mask` is not supported during training. Using it\
          \ might lead to unexpected results.\n  1%|\u258D                       \
          \                                                                      |\
          \ 301/59745 [06:20<20:48:14,  1.26s/it]`attention_mask` is not supported\
          \ during training. Using it might lead to unexpected results.\n  1%|\u258D\
          \                                                                      \
          \                       | 302/59745 [06:22<20:48:01,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 303/59745 [06:23<20:47:31,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 304/59745 [06:24<20:48:13,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 305/59745 [06:25<20:49:27,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 306/59745 [06:27<20:48:52,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 307/59745 [06:28<20:48:29,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 308/59745 [06:29<20:49:14,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          \  1%|\u258D                                                           \
          \                                  | 309/59745 [06:30<20:49:49,  1.26s/it]`attention_mask`\
          \ is not supported during training. Using it might lead to unexpected results.\n\
          {'loss': 1.5263, 'learning_rate': 1.9998671442394832e-05, 'epoch': 0.02}\n\
          ```"
        updatedAt: '2023-10-14T00:30:44.117Z'
      numEdits: 0
      reactions: []
    id: 6529e13468f1d7d1d247671a
    type: comment
  author: SinclairWang
  content: "Yeah, maybe. During my fine-tuning, I encountered a warning:\n\n```\n\
    `attention_mask` is not supported during training. Using it might lead to unexpected\
    \ results.\n{'loss': 1.3228, 'learning_rate': 1.999875577156579e-05, 'epoch':\
    \ 0.02}\n  1%|\u258D                                                         \
    \                                    | 300/59745 [06:19<20:47:29,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 301/59745 [06:20<20:48:14,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 302/59745 [06:22<20:48:01,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 303/59745 [06:23<20:47:31,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 304/59745 [06:24<20:48:13,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 305/59745 [06:25<20:49:27,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 306/59745 [06:27<20:48:52,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 307/59745 [06:28<20:48:29,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 308/59745 [06:29<20:49:14,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    \  1%|\u258D                                                                 \
    \                            | 309/59745 [06:30<20:49:49,  1.26s/it]`attention_mask`\
    \ is not supported during training. Using it might lead to unexpected results.\n\
    {'loss': 1.5263, 'learning_rate': 1.9998671442394832e-05, 'epoch': 0.02}\n```"
  created_at: 2023-10-13 23:30:44+00:00
  edited: false
  hidden: false
  id: 6529e13468f1d7d1d247671a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
      fullname: ben nicholl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bennicholl
      type: user
    createdAt: '2023-10-14T01:48:43.000Z'
    data:
      edited: false
      editors:
      - bennicholl
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9882429838180542
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e532b42e778a464789d268250b1aabb9.svg
          fullname: ben nicholl
          isHf: false
          isPro: false
          name: bennicholl
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;SinclairWang&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/SinclairWang\"\
          >@<span class=\"underline\">SinclairWang</span></a></span>\n\n\t</span></span>\
          \ I've encountered that error as well. While fine tuning my loss was continuing\
          \ to go down, but my outputs for my specific task were clearly not improving.\
          \ That's why I'm curious as to the reason for the warning and how the attention\
          \ mask could work for feed forward but not for backprop.</p>\n"
        raw: '@SinclairWang I''ve encountered that error as well. While fine tuning
          my loss was continuing to go down, but my outputs for my specific task were
          clearly not improving. That''s why I''m curious as to the reason for the
          warning and how the attention mask could work for feed forward but not for
          backprop.'
        updatedAt: '2023-10-14T01:48:43.104Z'
      numEdits: 0
      reactions: []
    id: 6529f37b4e655e57a9093413
    type: comment
  author: bennicholl
  content: '@SinclairWang I''ve encountered that error as well. While fine tuning
    my loss was continuing to go down, but my outputs for my specific task were clearly
    not improving. That''s why I''m curious as to the reason for the warning and how
    the attention mask could work for feed forward but not for backprop.'
  created_at: 2023-10-14 00:48:43+00:00
  edited: false
  hidden: false
  id: 6529f37b4e655e57a9093413
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
      fullname: Zengzhi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinclairWang
      type: user
    createdAt: '2023-10-14T13:23:51.000Z'
    data:
      edited: true
      editors:
      - SinclairWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9608309268951416
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
          fullname: Zengzhi Wang
          isHf: false
          isPro: false
          name: SinclairWang
          type: user
        html: '<p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/2jvVm7fM5S4OFFpCVE-my.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/2jvVm7fM5S4OFFpCVE-my.png"></a></p>

          <p>It is my training loss curve. </p>

          <blockquote>

          <p>"While fine tuning my loss was continuing to go down, but my outputs
          for my specific task were clearly not improving. "</p>

          </blockquote>

          <p>I also observed the same case. It confused me. I may not continue to
          fine-tune this model as I can not be sure the processing is ok due to the
          issue of the attention mask until this issue is solved.</p>

          '
        raw: "\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/2jvVm7fM5S4OFFpCVE-my.png)\n\
          \n\nIt is my training loss curve. \n\n>\"While fine tuning my loss was continuing\
          \ to go down, but my outputs for my specific task were clearly not improving.\
          \ \"\n\nI also observed the same case. It confused me. I may not continue\
          \ to fine-tune this model as I can not be sure the processing is ok due\
          \ to the issue of the attention mask until this issue is solved.\n"
        updatedAt: '2023-10-14T13:24:07.768Z'
      numEdits: 1
      reactions: []
    id: 652a9667703b3743c243dbea
    type: comment
  author: SinclairWang
  content: "\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/62cbeb2d72dfd24b86bdf977/2jvVm7fM5S4OFFpCVE-my.png)\n\
    \n\nIt is my training loss curve. \n\n>\"While fine tuning my loss was continuing\
    \ to go down, but my outputs for my specific task were clearly not improving.\
    \ \"\n\nI also observed the same case. It confused me. I may not continue to fine-tune\
    \ this model as I can not be sure the processing is ok due to the issue of the\
    \ attention mask until this issue is solved.\n"
  created_at: 2023-10-14 12:23:51+00:00
  edited: true
  hidden: false
  id: 652a9667703b3743c243dbea
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-10-14T13:29:09.000Z'
    data:
      edited: false
      editors:
      - zokica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9483915567398071
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: '<p>I also get bad results when fine-tuning probably because of the
          attention mask problem.</p>

          <p>I also get the same warning.</p>

          '
        raw: 'I also get bad results when fine-tuning probably because of the attention
          mask problem.


          I also get the same warning.'
        updatedAt: '2023-10-14T13:29:09.200Z'
      numEdits: 0
      reactions: []
    id: 652a97a530355beba68d7b13
    type: comment
  author: zokica
  content: 'I also get bad results when fine-tuning probably because of the attention
    mask problem.


    I also get the same warning.'
  created_at: 2023-10-14 12:29:09+00:00
  edited: false
  hidden: false
  id: 652a97a530355beba68d7b13
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
      fullname: Zengzhi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinclairWang
      type: user
    createdAt: '2023-10-14T13:45:21.000Z'
    data:
      edited: false
      editors:
      - SinclairWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.885164201259613
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
          fullname: Zengzhi Wang
          isHf: false
          isPro: false
          name: SinclairWang
          type: user
        html: '<p>So, any solutions?</p>

          '
        raw: So, any solutions?
        updatedAt: '2023-10-14T13:45:21.361Z'
      numEdits: 0
      reactions: []
    id: 652a9b715aec376f55036137
    type: comment
  author: SinclairWang
  content: So, any solutions?
  created_at: 2023-10-14 12:45:21+00:00
  edited: false
  hidden: false
  id: 652a9b715aec376f55036137
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
      fullname: Zoran
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zokica
      type: user
    createdAt: '2023-10-14T13:49:30.000Z'
    data:
      edited: true
      editors:
      - zokica
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9690719842910767
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/821999165e9e61b42c7f989404f5ffdf.svg
          fullname: Zoran
          isHf: false
          isPro: false
          name: zokica
          type: user
        html: "<p>I hope <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ will help. He said they will fix that as well. It is probably not easy\
          \ to fix and test such thing.</p>\n<p>I am not aware of any model of similar\
          \ size and performance.</p>\n"
        raw: 'I hope @gugarosa will help. He said they will fix that as well. It is
          probably not easy to fix and test such thing.


          I am not aware of any model of similar size and performance.


          '
        updatedAt: '2023-10-14T13:50:04.584Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - nulltella
    id: 652a9c6a1ef9983c6d087169
    type: comment
  author: zokica
  content: 'I hope @gugarosa will help. He said they will fix that as well. It is
    probably not easy to fix and test such thing.


    I am not aware of any model of similar size and performance.


    '
  created_at: 2023-10-14 12:49:30+00:00
  edited: true
  hidden: false
  id: 652a9c6a1ef9983c6d087169
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
      fullname: Zengzhi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinclairWang
      type: user
    createdAt: '2023-10-14T13:54:14.000Z'
    data:
      edited: false
      editors:
      - SinclairWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9745076298713684
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
          fullname: Zengzhi Wang
          isHf: false
          isPro: false
          name: SinclairWang
          type: user
        html: '<p>I am also looking for powerful models with about 1B parameters.
          </p>

          '
        raw: 'I am also looking for powerful models with about 1B parameters. '
        updatedAt: '2023-10-14T13:54:14.568Z'
      numEdits: 0
      reactions: []
    id: 652a9d8666313ebb6187ab09
    type: comment
  author: SinclairWang
  content: 'I am also looking for powerful models with about 1B parameters. '
  created_at: 2023-10-14 12:54:14+00:00
  edited: false
  hidden: false
  id: 652a9d8666313ebb6187ab09
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Attention mask for generation function in the future?
