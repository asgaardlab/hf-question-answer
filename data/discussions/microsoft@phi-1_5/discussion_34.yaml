!!python/object:huggingface_hub.community.DiscussionWithDetails
author: codegood
conflicting_files: null
created_at: 2023-09-27 00:00:29+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-09-27T01:00:29.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6130260825157166
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: '<p>Hello everyone,</p>

          <p>The model was working prefect until now.</p>

          <p>Trainer.train() is giving "self-attention is not allowed in forward pass"
          error due to new changes in modeling_mixformer_sequential.py</p>

          <p>Training arguments:<br>training_arguments = TrainingArguments(<br>    output_dir=output_dir,<br>    per_device_train_batch_size=per_device_train_batch_size,<br>    per_device_eval_batch_size=per_device_eval_batch_size,<br>    gradient_accumulation_steps=gradient_accumulation_steps,<br>    optim=optim,<br>    save_steps=save_steps,<br>    logging_steps=logging_steps,<br>    learning_rate=learning_rate,<br>    bf16=False,<br>    max_grad_norm=max_grad_norm,<br>    max_steps=max_steps,<br>    warmup_ratio=warmup_ratio,<br>    group_by_length=True,<br>    lr_scheduler_type=lr_scheduler_type,<br>    push_to_hub=True,<br>    tf32=False<br>)</p>

          <p>SFTTrainer<br>trainer = SFTTrainer(<br>    model=peft_model,<br>    train_dataset=qa_data[''train''],<br>    peft_config=peft_config,<br>    dataset_text_field="text",<br>    max_seq_length=1024,<br>    tokenizer=tokenizer,<br>    eval_dataset
          = qa_data[''test''],<br>    args=training_arguments,<br>    compute_metrics
          = compute_perplexity<br>)</p>

          <p>Can someone suggest changes?</p>

          '
        raw: "Hello everyone,\r\n\r\nThe model was working prefect until now.\r\n\r\
          \nTrainer.train() is giving \"self-attention is not allowed in forward pass\"\
          \ error due to new changes in modeling_mixformer_sequential.py\r\n\r\nTraining\
          \ arguments:\r\ntraining_arguments = TrainingArguments(\r\n    output_dir=output_dir,\r\
          \n    per_device_train_batch_size=per_device_train_batch_size,\r\n    per_device_eval_batch_size=per_device_eval_batch_size,\r\
          \n    gradient_accumulation_steps=gradient_accumulation_steps,\r\n    optim=optim,\r\
          \n    save_steps=save_steps,\r\n    logging_steps=logging_steps,\r\n   \
          \ learning_rate=learning_rate,\r\n    bf16=False,\r\n    max_grad_norm=max_grad_norm,\r\
          \n    max_steps=max_steps,\r\n    warmup_ratio=warmup_ratio,\r\n    group_by_length=True,\r\
          \n    lr_scheduler_type=lr_scheduler_type,\r\n    push_to_hub=True,\r\n\
          \    tf32=False\r\n)\r\n\r\nSFTTrainer\r\ntrainer = SFTTrainer(\r\n    model=peft_model,\r\
          \n    train_dataset=qa_data['train'],\r\n    peft_config=peft_config,\r\n\
          \    dataset_text_field=\"text\",\r\n    max_seq_length=1024,\r\n    tokenizer=tokenizer,\r\
          \n    eval_dataset = qa_data['test'],\r\n    args=training_arguments,\r\n\
          \    compute_metrics = compute_perplexity\r\n)\r\n\r\n\r\nCan someone suggest\
          \ changes?"
        updatedAt: '2023-09-27T01:00:29.859Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - tanujgodara
        - umarAQBAI
    id: 65137ead584d106e628828ed
    type: comment
  author: codegood
  content: "Hello everyone,\r\n\r\nThe model was working prefect until now.\r\n\r\n\
    Trainer.train() is giving \"self-attention is not allowed in forward pass\" error\
    \ due to new changes in modeling_mixformer_sequential.py\r\n\r\nTraining arguments:\r\
    \ntraining_arguments = TrainingArguments(\r\n    output_dir=output_dir,\r\n  \
    \  per_device_train_batch_size=per_device_train_batch_size,\r\n    per_device_eval_batch_size=per_device_eval_batch_size,\r\
    \n    gradient_accumulation_steps=gradient_accumulation_steps,\r\n    optim=optim,\r\
    \n    save_steps=save_steps,\r\n    logging_steps=logging_steps,\r\n    learning_rate=learning_rate,\r\
    \n    bf16=False,\r\n    max_grad_norm=max_grad_norm,\r\n    max_steps=max_steps,\r\
    \n    warmup_ratio=warmup_ratio,\r\n    group_by_length=True,\r\n    lr_scheduler_type=lr_scheduler_type,\r\
    \n    push_to_hub=True,\r\n    tf32=False\r\n)\r\n\r\nSFTTrainer\r\ntrainer =\
    \ SFTTrainer(\r\n    model=peft_model,\r\n    train_dataset=qa_data['train'],\r\
    \n    peft_config=peft_config,\r\n    dataset_text_field=\"text\",\r\n    max_seq_length=1024,\r\
    \n    tokenizer=tokenizer,\r\n    eval_dataset = qa_data['test'],\r\n    args=training_arguments,\r\
    \n    compute_metrics = compute_perplexity\r\n)\r\n\r\n\r\nCan someone suggest\
    \ changes?"
  created_at: 2023-09-27 00:00:29+00:00
  edited: false
  hidden: false
  id: 65137ead584d106e628828ed
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-09-27T01:01:00.000Z'
    data:
      from: Self attention not working in train
      to: Self attention not working during training
    id: 65137ecccc7684c9e41441e0
    type: title-change
  author: codegood
  created_at: 2023-09-27 00:01:00+00:00
  id: 65137ecccc7684c9e41441e0
  new_title: Self attention not working during training
  old_title: Self attention not working in train
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-09-27T01:08:33.000Z'
    data:
      from: Self attention not working during training
      to: Attention mask not working during training
    id: 651380913c912b5622eda42e
    type: title-change
  author: codegood
  created_at: 2023-09-27 00:08:33+00:00
  id: 651380913c912b5622eda42e
  new_title: Attention mask not working during training
  old_title: Self attention not working during training
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/94cc05d8a94fee87a07fb0ee2e7d2df0.svg
      fullname: kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tanujgodara
      type: user
    createdAt: '2023-09-27T08:04:35.000Z'
    data:
      edited: false
      editors:
      - tanujgodara
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8429667353630066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/94cc05d8a94fee87a07fb0ee2e7d2df0.svg
          fullname: kumar
          isHf: false
          isPro: false
          name: tanujgodara
          type: user
        html: '<p>Same here, so I selected an older revision so I could continue working.<br>You
          can do it by adding <code>revision = "4a426d8015bef5a0cb3acff8d4474ee9ab4071d5"</code>
          to <code>AutoModelForCausalLM.from_pretrained</code> params.</p>

          <p>But I also want to know the actual reason and solution for this problem.</p>

          '
        raw: 'Same here, so I selected an older revision so I could continue working.

          You can do it by adding `revision = "4a426d8015bef5a0cb3acff8d4474ee9ab4071d5"`
          to `AutoModelForCausalLM.from_pretrained` params.


          But I also want to know the actual reason and solution for this problem.'
        updatedAt: '2023-09-27T08:04:35.464Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - umarAQBAI
    id: 6513e213ddff6f9529474b75
    type: comment
  author: tanujgodara
  content: 'Same here, so I selected an older revision so I could continue working.

    You can do it by adding `revision = "4a426d8015bef5a0cb3acff8d4474ee9ab4071d5"`
    to `AutoModelForCausalLM.from_pretrained` params.


    But I also want to know the actual reason and solution for this problem.'
  created_at: 2023-09-27 07:04:35+00:00
  edited: false
  hidden: false
  id: 6513e213ddff6f9529474b75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-27T13:42:06.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9482794404029846
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;codegood&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/codegood\"\
          >@<span class=\"underline\">codegood</span></a></span>\n\n\t</span></span>\
          \ and <span data-props=\"{&quot;user&quot;:&quot;tanujgodara&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/tanujgodara\">@<span\
          \ class=\"underline\">tanujgodara</span></a></span>\n\n\t</span></span>!</p>\n\
          <p><code>attention_mask</code> during training never worked in previous\
          \ revisions, it was just being ignored when passed by the trainer class.\
          \ What happened was that we introduced <code>attention_mask</code> for inference-only\
          \ and added a check that prevents it from being used during training, since\
          \ it might lead to unexpected behaviors: <a href=\"https://huggingface.co/microsoft/phi-1_5/blob/main/modeling_mixformer_sequential.py#L755\"\
          >https://huggingface.co/microsoft/phi-1_5/blob/main/modeling_mixformer_sequential.py#L755</a>.</p>\n"
        raw: 'Hello @codegood and @tanujgodara!


          `attention_mask` during training never worked in previous revisions, it
          was just being ignored when passed by the trainer class. What happened was
          that we introduced `attention_mask` for inference-only and added a check
          that prevents it from being used during training, since it might lead to
          unexpected behaviors: https://huggingface.co/microsoft/phi-1_5/blob/main/modeling_mixformer_sequential.py#L755.'
        updatedAt: '2023-09-27T13:42:06.105Z'
      numEdits: 0
      reactions: []
    id: 6514312eed578610b4ac0509
    type: comment
  author: gugarosa
  content: 'Hello @codegood and @tanujgodara!


    `attention_mask` during training never worked in previous revisions, it was just
    being ignored when passed by the trainer class. What happened was that we introduced
    `attention_mask` for inference-only and added a check that prevents it from being
    used during training, since it might lead to unexpected behaviors: https://huggingface.co/microsoft/phi-1_5/blob/main/modeling_mixformer_sequential.py#L755.'
  created_at: 2023-09-27 12:42:06+00:00
  edited: false
  hidden: false
  id: 6514312eed578610b4ac0509
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
      fullname: wing lian
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: winglian
      type: user
    createdAt: '2023-09-27T15:09:16.000Z'
    data:
      edited: false
      editors:
      - winglian
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9282442331314087
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/641dfddf3bae5a77636817c5/2IwNwh9kK98eCHUmOGoWD.png?w=200&h=200&f=face
          fullname: wing lian
          isHf: false
          isPro: true
          name: winglian
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span>, can you simply\
          \ warn that it is being ignored rather than raising an exception? (although\
          \ this might also be too noisy during training). if anyone is using the\
          \ HF trainer, it automatically adds the attention_mask feature (I've already\
          \ tried removing that column from the tokenized dataset) , so this model\
          \ is now nearly impossible to use.</p>\n"
        raw: '@gugarosa, can you simply warn that it is being ignored rather than
          raising an exception? (although this might also be too noisy during training).
          if anyone is using the HF trainer, it automatically adds the attention_mask
          feature (I''ve already tried removing that column from the tokenized dataset)
          , so this model is now nearly impossible to use.'
        updatedAt: '2023-09-27T15:09:16.055Z'
      numEdits: 0
      reactions: []
    id: 6514459c2c5da979038ea592
    type: comment
  author: winglian
  content: '@gugarosa, can you simply warn that it is being ignored rather than raising
    an exception? (although this might also be too noisy during training). if anyone
    is using the HF trainer, it automatically adds the attention_mask feature (I''ve
    already tried removing that column from the tokenized dataset) , so this model
    is now nearly impossible to use.'
  created_at: 2023-09-27 14:09:16+00:00
  edited: false
  hidden: false
  id: 6514459c2c5da979038ea592
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-27T15:23:51.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.490569144487381
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;winglian&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/winglian\">@<span class=\"\
          underline\">winglian</span></a></span>\n\n\t</span></span> Of course! Just\
          \ updated the file to print a message instead of raising an error.</p>\n"
        raw: '@winglian Of course! Just updated the file to print a message instead
          of raising an error.'
        updatedAt: '2023-09-27T15:23:51.446Z'
      numEdits: 0
      reactions: []
    id: 65144907ac849a6655ca517b
    type: comment
  author: gugarosa
  content: '@winglian Of course! Just updated the file to print a message instead
    of raising an error.'
  created_at: 2023-09-27 14:23:51+00:00
  edited: false
  hidden: false
  id: 65144907ac849a6655ca517b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-09-28T03:04:02.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7124292254447937
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> Sorry for misunderstanding.<br>Thanks\
          \ for changing the file.</p>\n"
        raw: '@gugarosa Sorry for misunderstanding.

          Thanks for changing the file.

          '
        updatedAt: '2023-09-28T03:04:02.194Z'
      numEdits: 0
      reactions: []
    id: 6514ed22921b7d68490304de
    type: comment
  author: codegood
  content: '@gugarosa Sorry for misunderstanding.

    Thanks for changing the file.

    '
  created_at: 2023-09-28 02:04:02+00:00
  edited: false
  hidden: false
  id: 6514ed22921b7d68490304de
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-03T14:37:41.000Z'
    data:
      status: closed
    id: 651c27353da74cb63e4f7fba
    type: status-change
  author: gugarosa
  created_at: 2023-10-03 13:37:41+00:00
  id: 651c27353da74cb63e4f7fba
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
      fullname: Zengzhi Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: SinclairWang
      type: user
    createdAt: '2023-10-13T05:15:00.000Z'
    data:
      edited: false
      editors:
      - SinclairWang
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9034005403518677
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86090d2d8c7dd596cf9f7e640e8e5951.svg
          fullname: Zengzhi Wang
          isHf: false
          isPro: false
          name: SinclairWang
          type: user
        html: '<p>Any plans for fixing this issue? I am looking forward to fine-tuning
          phi-1 and phi-1.5. </p>

          '
        raw: 'Any plans for fixing this issue? I am looking forward to fine-tuning
          phi-1 and phi-1.5. '
        updatedAt: '2023-10-13T05:15:00.279Z'
      numEdits: 0
      reactions: []
    id: 6528d2544f8fe1bea9c0a2f1
    type: comment
  author: SinclairWang
  content: 'Any plans for fixing this issue? I am looking forward to fine-tuning phi-1
    and phi-1.5. '
  created_at: 2023-10-13 04:15:00+00:00
  edited: false
  hidden: false
  id: 6528d2544f8fe1bea9c0a2f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
      fullname: Stephan
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: codegood
      type: user
    createdAt: '2023-10-14T00:14:05.000Z'
    data:
      edited: false
      editors:
      - codegood
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3182806074619293
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/14c0faec86334a15d040ef3219f7a63f.svg
          fullname: Stephan
          isHf: false
          isPro: false
          name: codegood
          type: user
        html: '<p>Solution: <a href="https://huggingface.co/microsoft/phi-1_5/discussions/34#6513e213ddff6f9529474b75">https://huggingface.co/microsoft/phi-1_5/discussions/34#6513e213ddff6f9529474b75</a></p>

          '
        raw: 'Solution: https://huggingface.co/microsoft/phi-1_5/discussions/34#6513e213ddff6f9529474b75'
        updatedAt: '2023-10-14T00:14:05.427Z'
      numEdits: 0
      reactions: []
    id: 6529dd4d5aec376f55efb301
    type: comment
  author: codegood
  content: 'Solution: https://huggingface.co/microsoft/phi-1_5/discussions/34#6513e213ddff6f9529474b75'
  created_at: 2023-10-13 23:14:05+00:00
  edited: false
  hidden: false
  id: 6529dd4d5aec376f55efb301
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Attention mask not working during training
