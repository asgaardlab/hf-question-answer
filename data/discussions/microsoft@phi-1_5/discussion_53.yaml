!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nulltella
conflicting_files: null
created_at: 2023-11-02 17:00:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650d42549e898eb6d21cc5dc/ndmXshML3If8pHRHJGNct.jpeg?w=200&h=200&f=face
      fullname: Mohamed Jihed Riahi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nulltella
      type: user
    createdAt: '2023-11-02T18:00:24.000Z'
    data:
      edited: false
      editors:
      - nulltella
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8281632661819458
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/650d42549e898eb6d21cc5dc/ndmXshML3If8pHRHJGNct.jpeg?w=200&h=200&f=face
          fullname: Mohamed Jihed Riahi
          isHf: false
          isPro: false
          name: nulltella
          type: user
        html: '<p>Hello,</p>

          <p>I''m relatively new to this and I''ve been working with the Phi 1.5 model.
          I''ve uploaded model and tokenizer, but I''m encountering a problem when
          dealing with very long prompts (longer than 2048 tokens). Despite setting
          truncation to True, the output I receive only reflects the first 2048 tokens
          or so of the prompt. Here''s the relevant portion of my code:</p>

          <p>Here''s the code snippet:</p>

          <p>model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True,
          torch_dtype=torch.float32)<br>tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5",
          trust_remote_code=True)<br>prompt= ** Long prompt **<br>inputs = tokenizer([prompt],
          return_tensors="pt", truncation=True).to(''cuda'')<br>streamer = TextStreamer(tokenizer)<br>_
          = model.generate(**inputs, streamer=streamer, pad_token_id=tokenizer.eos_token_id)</p>

          <p>I was under the impression that the truncation=True parameter would handle
          long prompts by truncating them to fit within the model''s maximum sequence
          length. However the output I''m seeing seems to just be the beginning portion
          of the prompt up to around 2048 tokens.</p>

          <p>Is there smthng obvious I''m missing or misunderstanding about how truncation
          works? Or is there another approach I should be taking to handle long prompts?</p>

          <p>Thank you !</p>

          '
        raw: "Hello,\r\n\r\nI'm relatively new to this and I've been working with\
          \ the Phi 1.5 model. I've uploaded model and tokenizer, but I'm encountering\
          \ a problem when dealing with very long prompts (longer than 2048 tokens).\
          \ Despite setting truncation to True, the output I receive only reflects\
          \ the first 2048 tokens or so of the prompt. Here's the relevant portion\
          \ of my code:\r\n\r\nHere's the code snippet:\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32)\r\
          \ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\r\
          \nprompt= ** Long prompt **\r\ninputs = tokenizer([prompt], return_tensors=\"\
          pt\", truncation=True).to('cuda')\r\nstreamer = TextStreamer(tokenizer)\r\
          \n_ = model.generate(**inputs, streamer=streamer, pad_token_id=tokenizer.eos_token_id)\r\
          \n\r\nI was under the impression that the truncation=True parameter would\
          \ handle long prompts by truncating them to fit within the model's maximum\
          \ sequence length. However the output I'm seeing seems to just be the beginning\
          \ portion of the prompt up to around 2048 tokens.\r\n\r\nIs there smthng\
          \ obvious I'm missing or misunderstanding about how truncation works? Or\
          \ is there another approach I should be taking to handle long prompts?\r\
          \n\r\nThank you !"
        updatedAt: '2023-11-02T18:00:24.997Z'
      numEdits: 0
      reactions: []
    id: 6543e3b85a13979f82dbbf81
    type: comment
  author: nulltella
  content: "Hello,\r\n\r\nI'm relatively new to this and I've been working with the\
    \ Phi 1.5 model. I've uploaded model and tokenizer, but I'm encountering a problem\
    \ when dealing with very long prompts (longer than 2048 tokens). Despite setting\
    \ truncation to True, the output I receive only reflects the first 2048 tokens\
    \ or so of the prompt. Here's the relevant portion of my code:\r\n\r\nHere's the\
    \ code snippet:\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
    , trust_remote_code=True, torch_dtype=torch.float32)\r\ntokenizer = AutoTokenizer.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True)\r\nprompt= ** Long prompt **\r\n\
    inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True).to('cuda')\r\
    \nstreamer = TextStreamer(tokenizer)\r\n_ = model.generate(**inputs, streamer=streamer,\
    \ pad_token_id=tokenizer.eos_token_id)\r\n\r\nI was under the impression that\
    \ the truncation=True parameter would handle long prompts by truncating them to\
    \ fit within the model's maximum sequence length. However the output I'm seeing\
    \ seems to just be the beginning portion of the prompt up to around 2048 tokens.\r\
    \n\r\nIs there smthng obvious I'm missing or misunderstanding about how truncation\
    \ works? Or is there another approach I should be taking to handle long prompts?\r\
    \n\r\nThank you !"
  created_at: 2023-11-02 17:00:24+00:00
  edited: false
  hidden: false
  id: 6543e3b85a13979f82dbbf81
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-11-20T18:12:59.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.887894332408905
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>This should be fixed on the latest commit, sorry for taking too
          long!</p>

          '
        raw: This should be fixed on the latest commit, sorry for taking too long!
        updatedAt: '2023-11-20T18:12:59.385Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655ba1ab13309a611b898e6f
    id: 655ba1ab13309a611b898e6b
    type: comment
  author: gugarosa
  content: This should be fixed on the latest commit, sorry for taking too long!
  created_at: 2023-11-20 18:12:59+00:00
  edited: false
  hidden: false
  id: 655ba1ab13309a611b898e6b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-11-20T18:12:59.000Z'
    data:
      status: closed
    id: 655ba1ab13309a611b898e6f
    type: status-change
  author: gugarosa
  created_at: 2023-11-20 18:12:59+00:00
  id: 655ba1ab13309a611b898e6f
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 53
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Issue with long prompts (>2048 tokens) and truncation when using Phi 1.5
