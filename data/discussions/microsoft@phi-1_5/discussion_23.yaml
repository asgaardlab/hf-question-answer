!!python/object:huggingface_hub.community.DiscussionWithDetails
author: wjfwzzc
conflicting_files: null
created_at: 2023-09-19 08:53:01+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86d6d6e39d22bad50cf3a28583ea16b2.svg
      fullname: Jianfeng Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wjfwzzc
      type: user
    createdAt: '2023-09-19T09:53:01.000Z'
    data:
      edited: false
      editors:
      - wjfwzzc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.25642746686935425
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86d6d6e39d22bad50cf3a28583ea16b2.svg
          fullname: Jianfeng Wang
          isHf: false
          isPro: false
          name: wjfwzzc
          type: user
        html: "<p>transformers version: 4.33.2</p>\n<pre><code class=\"language-python\"\
          >AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\">\"microsoft/phi-1\"\
          </span>, trust_remote_code=<span class=\"hljs-literal\">True</span>, torch_dtype=<span\
          \ class=\"hljs-string\">\"auto\"</span>, use_cache=<span class=\"hljs-literal\"\
          >True</span>)\n</code></pre>\n<p>raise the following error:</p>\n<pre><code\
          \ class=\"language-python\">File /usr/local/lib/python3<span class=\"hljs-number\"\
          >.9</span>/dist-packages/transformers/models/auto/auto_factory.py:<span\
          \ class=\"hljs-number\">558</span>, <span class=\"hljs-keyword\">in</span>\
          \ _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\n    <span class=\"hljs-number\">556</span>   \
          \  <span class=\"hljs-keyword\">else</span>:\n    <span class=\"hljs-number\"\
          >557</span>         cls.register(config.__class__, model_class, exist_ok=<span\
          \ class=\"hljs-literal\">True</span>)\n--&gt; <span class=\"hljs-number\"\
          >558</span>     <span class=\"hljs-keyword\">return</span> model_class.from_pretrained(\n\
          \    <span class=\"hljs-number\">559</span>         pretrained_model_name_or_path,\
          \ *model_args, config=config, **hub_kwargs, **kwargs\n    <span class=\"\
          hljs-number\">560</span>     )\n    <span class=\"hljs-number\">561</span>\
          \ <span class=\"hljs-keyword\">elif</span> <span class=\"hljs-built_in\"\
          >type</span>(config) <span class=\"hljs-keyword\">in</span> cls._model_mapping.keys():\n\
          \    <span class=\"hljs-number\">562</span>     model_class = _get_model_class(config,\
          \ cls._model_mapping)\n\nFile /usr/local/lib/python3<span class=\"hljs-number\"\
          >.9</span>/dist-packages/transformers/modeling_utils.py:<span class=\"hljs-number\"\
          >2966</span>, <span class=\"hljs-keyword\">in</span> PreTrainedModel.from_pretrained(cls,\
          \ pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes,\
          \ force_download, local_files_only, token, revision, use_safetensors, *model_args,\
          \ **kwargs)\n   <span class=\"hljs-number\">2963</span>     init_contexts.append(init_empty_weights())\n\
          \   <span class=\"hljs-number\">2965</span> <span class=\"hljs-keyword\"\
          >with</span> ContextManagers(init_contexts):\n-&gt; <span class=\"hljs-number\"\
          >2966</span>     model = cls(config, *model_args, **model_kwargs)\n   <span\
          \ class=\"hljs-number\">2968</span> <span class=\"hljs-comment\"># Check\
          \ first if we are `from_pt`</span>\n   <span class=\"hljs-number\">2969</span>\
          \ <span class=\"hljs-keyword\">if</span> use_keep_in_fp32_modules:\n\nTypeError:\
          \ __init__() got an unexpected keyword argument <span class=\"hljs-string\"\
          >'use_cache'</span>\n</code></pre>\n"
        raw: "transformers version: 4.33.2\r\n```python\r\nAutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1\", trust_remote_code=True, torch_dtype=\"auto\", use_cache=True)\r\
          \n```\r\nraise the following error:\r\n```python\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/auto_factory.py:558,\
          \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *model_args, **kwargs)\r\n    556     else:\r\n    557         cls.register(config.__class__,\
          \ model_class, exist_ok=True)\r\n--> 558     return model_class.from_pretrained(\r\
          \n    559         pretrained_model_name_or_path, *model_args, config=config,\
          \ **hub_kwargs, **kwargs\r\n    560     )\r\n    561 elif type(config) in\
          \ cls._model_mapping.keys():\r\n    562     model_class = _get_model_class(config,\
          \ cls._model_mapping)\r\n\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:2966,\
          \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path,\
          \ config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only,\
          \ token, revision, use_safetensors, *model_args, **kwargs)\r\n   2963  \
          \   init_contexts.append(init_empty_weights())\r\n   2965 with ContextManagers(init_contexts):\r\
          \n-> 2966     model = cls(config, *model_args, **model_kwargs)\r\n   2968\
          \ # Check first if we are `from_pt`\r\n   2969 if use_keep_in_fp32_modules:\r\
          \n\r\nTypeError: __init__() got an unexpected keyword argument 'use_cache'\r\
          \n```"
        updatedAt: '2023-09-19T09:53:01.915Z'
      numEdits: 0
      reactions: []
    id: 65096f7dd48ed98e63a9f91b
    type: comment
  author: wjfwzzc
  content: "transformers version: 4.33.2\r\n```python\r\nAutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1\", trust_remote_code=True, torch_dtype=\"auto\", use_cache=True)\r\
    \n```\r\nraise the following error:\r\n```python\r\nFile /usr/local/lib/python3.9/dist-packages/transformers/models/auto/auto_factory.py:558,\
    \ in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args,\
    \ **kwargs)\r\n    556     else:\r\n    557         cls.register(config.__class__,\
    \ model_class, exist_ok=True)\r\n--> 558     return model_class.from_pretrained(\r\
    \n    559         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,\
    \ **kwargs\r\n    560     )\r\n    561 elif type(config) in cls._model_mapping.keys():\r\
    \n    562     model_class = _get_model_class(config, cls._model_mapping)\r\n\r\
    \nFile /usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py:2966,\
    \ in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config,\
    \ cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token,\
    \ revision, use_safetensors, *model_args, **kwargs)\r\n   2963     init_contexts.append(init_empty_weights())\r\
    \n   2965 with ContextManagers(init_contexts):\r\n-> 2966     model = cls(config,\
    \ *model_args, **model_kwargs)\r\n   2968 # Check first if we are `from_pt`\r\n\
    \   2969 if use_keep_in_fp32_modules:\r\n\r\nTypeError: __init__() got an unexpected\
    \ keyword argument 'use_cache'\r\n```"
  created_at: 2023-09-19 08:53:01+00:00
  edited: false
  hidden: false
  id: 65096f7dd48ed98e63a9f91b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg?w=200&h=200&f=face
      fullname: "Clem \U0001F917"
      isHf: true
      isOrgMember: false
      isOwner: false
      isPro: true
      name: clem
      type: user
    createdAt: '2023-09-22T01:25:43.000Z'
    data:
      edited: false
      editors:
      - clem
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9577392935752869
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1583857146757-5e67bdd61009063689407479.jpeg?w=200&h=200&f=face
          fullname: "Clem \U0001F917"
          isHf: true
          isPro: true
          name: clem
          type: user
        html: "<p>maybe cc <span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lysandre\"\
          >@<span class=\"underline\">lysandre</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: 'maybe cc @lysandre '
        updatedAt: '2023-09-22T01:25:43.433Z'
      numEdits: 0
      reactions: []
    id: 650ced170f823ffe3f23b5eb
    type: comment
  author: clem
  content: 'maybe cc @lysandre '
  created_at: 2023-09-22 00:25:43+00:00
  edited: false
  hidden: false
  id: 650ced170f823ffe3f23b5eb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-09-22T13:13:33.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6223320960998535
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;wjfwzzc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/wjfwzzc\"\
          >@<span class=\"underline\">wjfwzzc</span></a></span>\n\n\t</span></span>,\
          \ thanks for your issue!</p>\n<p>It seems there is an issue with the propagation\
          \ of unused kwargs when using remote code, cc <span data-props=\"{&quot;user&quot;:&quot;ArthurZ&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/ArthurZ\"\
          >@<span class=\"underline\">ArthurZ</span></a></span>\n\n\t</span></span>.</p>\n\
          <p>To do what you're trying to do, you could define a <code>GenerationConfig</code>\
          \ locally with <code>use_cache</code> set to <code>True</code>:</p>\n<pre><code\
          \ class=\"language-py\"><span class=\"hljs-keyword\">from</span> transformers\
          \ <span class=\"hljs-keyword\">import</span> GenerationConfig\n\ngeneration_config\
          \ = GenerationConfig(use_cache=<span class=\"hljs-literal\">True</span>)\n\
          </code></pre>\n<p>You can then pass this to the <code>generate</code> method:</p>\n\
          <pre><code class=\"language-py\"><span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span><span class=\"hljs-keyword\">import</span> torch\n<span class=\"\
          hljs-meta\">&gt;&gt;&gt; </span><span class=\"hljs-keyword\">from</span>\
          \ transformers <span class=\"hljs-keyword\">import</span> AutoModelForCausalLM,\
          \ AutoTokenizer\n\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>model =\
          \ AutoModelForCausalLM.from_pretrained(<span class=\"hljs-string\">\"microsoft/phi-1_5\"\
          </span>, trust_remote_code=<span class=\"hljs-literal\">True</span>)\n<span\
          \ class=\"hljs-meta\">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/phi-1_5\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>, torch_dtype=<span class=\"hljs-string\"\
          >\"auto\"</span>)\n<span class=\"hljs-meta\">&gt;&gt;&gt; </span>inputs\
          \ = tokenizer(<span class=\"hljs-string\">'''```python</span>\n<span class=\"\
          hljs-string\"><span class=\"hljs-meta\">... </span>def print_prime(n):</span>\n\
          <span class=\"hljs-string\"><span class=\"hljs-meta\">... </span>    \"\"\
          \"</span>\n<span class=\"hljs-string\"><span class=\"hljs-meta\">... </span>\
          \    Print all primes between 1 and n</span>\n<span class=\"hljs-string\"\
          ><span class=\"hljs-meta\">... </span>    \"\"\"'''</span>, return_tensors=<span\
          \ class=\"hljs-string\">\"pt\"</span>, return_attention_mask=<span class=\"\
          hljs-literal\">False</span>)\n\n\n<span class=\"hljs-meta\">&gt;&gt;&gt;\
          \ </span>model.generate(**inputs, max_length=<span class=\"hljs-number\"\
          >200</span>, generation_config=generation_config)\n</code></pre>\n<p>Please\
          \ let me know if that works for you!</p>\n"
        raw: 'Hey @wjfwzzc, thanks for your issue!


          It seems there is an issue with the propagation of unused kwargs when using
          remote code, cc @ArthurZ.


          To do what you''re trying to do, you could define a `GenerationConfig` locally
          with `use_cache` set to `True`:


          ```py

          from transformers import GenerationConfig


          generation_config = GenerationConfig(use_cache=True)

          ```


          You can then pass this to the `generate` method:

          ```py

          >>> import torch

          >>> from transformers import AutoModelForCausalLM, AutoTokenizer


          >>> model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True)

          >>> tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True,
          torch_dtype="auto")

          >>> inputs = tokenizer(''''''```python

          ... def print_prime(n):

          ...     """

          ...     Print all primes between 1 and n

          ...     """'''''', return_tensors="pt", return_attention_mask=False)



          >>> model.generate(**inputs, max_length=200, generation_config=generation_config)

          ```


          Please let me know if that works for you!'
        updatedAt: '2023-09-22T13:13:33.812Z'
      numEdits: 0
      reactions: []
    id: 650d92fdc354f5c64f311dfa
    type: comment
  author: lysandre
  content: 'Hey @wjfwzzc, thanks for your issue!


    It seems there is an issue with the propagation of unused kwargs when using remote
    code, cc @ArthurZ.


    To do what you''re trying to do, you could define a `GenerationConfig` locally
    with `use_cache` set to `True`:


    ```py

    from transformers import GenerationConfig


    generation_config = GenerationConfig(use_cache=True)

    ```


    You can then pass this to the `generate` method:

    ```py

    >>> import torch

    >>> from transformers import AutoModelForCausalLM, AutoTokenizer


    >>> model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True)

    >>> tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True,
    torch_dtype="auto")

    >>> inputs = tokenizer(''''''```python

    ... def print_prime(n):

    ...     """

    ...     Print all primes between 1 and n

    ...     """'''''', return_tensors="pt", return_attention_mask=False)



    >>> model.generate(**inputs, max_length=200, generation_config=generation_config)

    ```


    Please let me know if that works for you!'
  created_at: 2023-09-22 12:13:33+00:00
  edited: false
  hidden: false
  id: 650d92fdc354f5c64f311dfa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/86d6d6e39d22bad50cf3a28583ea16b2.svg
      fullname: Jianfeng Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wjfwzzc
      type: user
    createdAt: '2023-09-23T15:43:15.000Z'
    data:
      edited: false
      editors:
      - wjfwzzc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8191388249397278
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/86d6d6e39d22bad50cf3a28583ea16b2.svg
          fullname: Jianfeng Wang
          isHf: false
          isPro: false
          name: wjfwzzc
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;lysandre&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/lysandre\"\
          >@<span class=\"underline\">lysandre</span></a></span>\n\n\t</span></span>\
          \ , thanks for your help and it works for me!<br>Nevertheless I'm still\
          \ confused about the <code>attention_mask</code>. It seems that <code>return_attention_mask=True</code>\
          \ will raise</p>\n<pre><code class=\"language-python\">ValueError: The following\
          \ `model_kwargs` are <span class=\"hljs-keyword\">not</span> used by the\
          \ model: [<span class=\"hljs-string\">'attention_mask'</span>] (note: typos\
          \ <span class=\"hljs-keyword\">in</span> the generate arguments will also\
          \ show up <span class=\"hljs-keyword\">in</span> this <span class=\"hljs-built_in\"\
          >list</span>)\n</code></pre>\n<p>But how to do batch inferencing with padding\
          \ without attention mask?</p>\n"
        raw: 'Hi @lysandre , thanks for your help and it works for me!

          Nevertheless I''m still confused about the `attention_mask`. It seems that
          `return_attention_mask=True` will raise

          ```python

          ValueError: The following `model_kwargs` are not used by the model: [''attention_mask'']
          (note: typos in the generate arguments will also show up in this list)

          ```

          But how to do batch inferencing with padding without attention mask?'
        updatedAt: '2023-09-23T15:43:15.272Z'
      numEdits: 0
      reactions: []
    id: 650f079399fe56caa803b9e0
    type: comment
  author: wjfwzzc
  content: 'Hi @lysandre , thanks for your help and it works for me!

    Nevertheless I''m still confused about the `attention_mask`. It seems that `return_attention_mask=True`
    will raise

    ```python

    ValueError: The following `model_kwargs` are not used by the model: [''attention_mask'']
    (note: typos in the generate arguments will also show up in this list)

    ```

    But how to do batch inferencing with padding without attention mask?'
  created_at: 2023-09-23 14:43:15+00:00
  edited: false
  hidden: false
  id: 650f079399fe56caa803b9e0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
      fullname: Lysandre
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: lysandre
      type: user
    createdAt: '2023-09-26T10:26:08.000Z'
    data:
      edited: false
      editors:
      - lysandre
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8800409436225891
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1618450692745-5e3aec01f55e2b62848a5217.jpeg?w=200&h=200&f=face
          fullname: Lysandre
          isHf: true
          isPro: false
          name: lysandre
          type: user
        html: "<p>Hey <span data-props=\"{&quot;user&quot;:&quot;wjfwzzc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/wjfwzzc\"\
          >@<span class=\"underline\">wjfwzzc</span></a></span>\n\n\t</span></span>,\
          \ Phi is being contributed to <code>transformers</code> in this PR: <a rel=\"\
          nofollow\" href=\"https://github.com/huggingface/transformers/pull/26170\"\
          >https://github.com/huggingface/transformers/pull/26170</a></p>\n<p>This\
          \ should enable leveraging the attention mask to perform batch inference.</p>\n"
        raw: 'Hey @wjfwzzc, Phi is being contributed to `transformers` in this PR:
          https://github.com/huggingface/transformers/pull/26170


          This should enable leveraging the attention mask to perform batch inference.'
        updatedAt: '2023-09-26T10:26:08.732Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - wjfwzzc
        - gugarosa
    id: 6512b1c06bea603bfb8bade4
    type: comment
  author: lysandre
  content: 'Hey @wjfwzzc, Phi is being contributed to `transformers` in this PR: https://github.com/huggingface/transformers/pull/26170


    This should enable leveraging the attention mask to perform batch inference.'
  created_at: 2023-09-26 09:26:08+00:00
  edited: false
  hidden: false
  id: 6512b1c06bea603bfb8bade4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-09-26T18:28:04.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9045921564102173
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;wjfwzzc&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/wjfwzzc\"\
          >@<span class=\"underline\">wjfwzzc</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>I just added support for <code>attention_mask</code> in the forward pass,\
          \ so you should be able to perform batched inference. Meanwhile, this will\
          \ be a proxy till Phi gets contributed to <code>transformers</code> (which\
          \ I hugely appreciate that!).</p>\n"
        raw: 'Hello @wjfwzzc!


          I just added support for `attention_mask` in the forward pass, so you should
          be able to perform batched inference. Meanwhile, this will be a proxy till
          Phi gets contributed to `transformers` (which I hugely appreciate that!).'
        updatedAt: '2023-09-26T18:28:04.489Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - wjfwzzc
    id: 651322b497fb08378dd3e623
    type: comment
  author: gugarosa
  content: 'Hello @wjfwzzc!


    I just added support for `attention_mask` in the forward pass, so you should be
    able to perform batched inference. Meanwhile, this will be a proxy till Phi gets
    contributed to `transformers` (which I hugely appreciate that!).'
  created_at: 2023-09-26 17:28:04+00:00
  edited: false
  hidden: false
  id: 651322b497fb08378dd3e623
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-10-03T14:42:44.000Z'
    data:
      status: closed
    id: 651c286481be95fb9548db90
    type: status-change
  author: gugarosa
  created_at: 2023-10-03 13:42:44+00:00
  id: 651c286481be95fb9548db90
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 23
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: raise error when `use_cache = True`
