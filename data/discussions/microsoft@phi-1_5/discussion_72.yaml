!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jamesbraza
conflicting_files: null
created_at: 2024-01-11 18:09:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
      fullname: James Braza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesbraza
      type: user
    createdAt: '2024-01-11T18:09:49.000Z'
    data:
      edited: true
      editors:
      - jamesbraza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5021433234214783
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
          fullname: James Braza
          isHf: false
          isPro: false
          name: jamesbraza
          type: user
        html: "<p>Running the below Python code on my MacBook Pro with M1 chip, macOS\
          \ Ventura 13.5.2:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-comment\"># With Python 3.11.7, transformers==4.36.2</span>\n<span\
          \ class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/phi-1_5\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>I get this error:</p>\n\
          <pre><code>  File \"/Users/user/code/project/play.py\", line 6, in &lt;module&gt;\n\
          \    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 553, in from_pretrained\n    model_class = get_class_from_dynamic_module(\n\
          \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n\
          \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 315, in get_cached_module_file\n    modules_needed = check_imports(resolved_module_file)\n\
          \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 180, in check_imports\n    raise ImportError(\nImportError: This\
          \ modeling file requires the following packages that were not found in your\
          \ environment: flash_attn. Run `pip install flash_attn`\npython-BaseException\n\
          </code></pre>\n<p>Which clearly states <code>flash_attn</code> is required.\
          \ However, per <a rel=\"nofollow\" href=\"https://github.com/Dao-AILab/flash-attention/issues/749\"\
          >this issue</a>, <code>flash-attn</code> cannot be installed on my MacBook\
          \ because it doesn't have an Nvidia GPUs.</p>\n<p>Is there anything I can\
          \ do here to bypass this <code>flash_attn</code> dependency?</p>\n"
        raw: "Running the below Python code on my MacBook Pro with M1 chip, macOS\
          \ Ventura 13.5.2:\n\n```python\n# With Python 3.11.7, transformers==4.36.2\n\
          from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", trust_remote_code=True)\n```\n\nI get this error:\n\
          \n```\n  File \"/Users/user/code/project/play.py\", line 6, in <module>\n\
          \    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\
          \            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
          \  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
          , line 553, in from_pretrained\n    model_class = get_class_from_dynamic_module(\n\
          \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 488, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n\
          \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 315, in get_cached_module_file\n    modules_needed = check_imports(resolved_module_file)\n\
          \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
          , line 180, in check_imports\n    raise ImportError(\nImportError: This\
          \ modeling file requires the following packages that were not found in your\
          \ environment: flash_attn. Run `pip install flash_attn`\npython-BaseException\n\
          ```\n\nWhich clearly states `flash_attn` is required. However, per [this\
          \ issue](https://github.com/Dao-AILab/flash-attention/issues/749), `flash-attn`\
          \ cannot be installed on my MacBook because it doesn't have an Nvidia GPUs.\n\
          \nIs there anything I can do here to bypass this `flash_attn` dependency?"
        updatedAt: '2024-01-11T18:45:53.631Z'
      numEdits: 1
      reactions: []
    id: 65a02eedc5770b27ae8a7afb
    type: comment
  author: jamesbraza
  content: "Running the below Python code on my MacBook Pro with M1 chip, macOS Ventura\
    \ 13.5.2:\n\n```python\n# With Python 3.11.7, transformers==4.36.2\nfrom transformers\
    \ import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\"\
    microsoft/phi-1_5\", trust_remote_code=True)\n```\n\nI get this error:\n\n```\n\
    \  File \"/Users/user/code/project/play.py\", line 6, in <module>\n    model =\
    \ AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n \
    \           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\
    \  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\"\
    , line 553, in from_pretrained\n    model_class = get_class_from_dynamic_module(\n\
    \                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 488, in get_class_from_dynamic_module\n    final_module = get_cached_module_file(\n\
    \                   ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 315, in get_cached_module_file\n    modules_needed = check_imports(resolved_module_file)\n\
    \                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/code/project/venv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py\"\
    , line 180, in check_imports\n    raise ImportError(\nImportError: This modeling\
    \ file requires the following packages that were not found in your environment:\
    \ flash_attn. Run `pip install flash_attn`\npython-BaseException\n```\n\nWhich\
    \ clearly states `flash_attn` is required. However, per [this issue](https://github.com/Dao-AILab/flash-attention/issues/749),\
    \ `flash-attn` cannot be installed on my MacBook because it doesn't have an Nvidia\
    \ GPUs.\n\nIs there anything I can do here to bypass this `flash_attn` dependency?"
  created_at: 2024-01-11 18:09:49+00:00
  edited: true
  hidden: false
  id: 65a02eedc5770b27ae8a7afb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
      fullname: James Braza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesbraza
      type: user
    createdAt: '2024-01-11T18:29:51.000Z'
    data:
      edited: true
      editors:
      - jamesbraza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6806764006614685
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
          fullname: James Braza
          isHf: false
          isPro: false
          name: jamesbraza
          type: user
        html: "<p>It looks like <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/phi/modeling_phi.py#L50-L52\"\
          >https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/phi/modeling_phi.py#L50-L52</a>\
          \ is getting picked up by <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/dynamic_module_utils.py#L154\"\
          >https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/dynamic_module_utils.py#L154</a>\
          \ as a requirement, when it actually isn't.</p>\n<p>Using <code>unittest.mock.patch</code>,\
          \ we can work around this:</p>\n<pre><code class=\"language-python\"><span\
          \ class=\"hljs-comment\"># With Python 3.11.7, transformers==4.36.2</span>\n\
          <span class=\"hljs-keyword\">import</span> os\n<span class=\"hljs-keyword\"\
          >from</span> unittest.mock <span class=\"hljs-keyword\">import</span> patch\n\
          \n<span class=\"hljs-keyword\">from</span> transformers <span class=\"hljs-keyword\"\
          >import</span> AutoModelForCausalLM\n<span class=\"hljs-keyword\">from</span>\
          \ transformers.dynamic_module_utils <span class=\"hljs-keyword\">import</span>\
          \ get_imports\n\n\n<span class=\"hljs-keyword\">def</span> <span class=\"\
          hljs-title function_\">fixed_get_imports</span>(<span class=\"hljs-params\"\
          >filename: <span class=\"hljs-built_in\">str</span> | os.PathLike</span>)\
          \ -&gt; <span class=\"hljs-built_in\">list</span>[<span class=\"hljs-built_in\"\
          >str</span>]:\n    <span class=\"hljs-string\">\"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\
          \"\"</span>\n    <span class=\"hljs-keyword\">if</span> <span class=\"hljs-keyword\"\
          >not</span> <span class=\"hljs-built_in\">str</span>(filename).endswith(<span\
          \ class=\"hljs-string\">\"/modeling_phi.py\"</span>):\n        <span class=\"\
          hljs-keyword\">return</span> get_imports(filename)\n    imports = get_imports(filename)\n\
          \    imports.remove(<span class=\"hljs-string\">\"flash_attn\"</span>)\n\
          \    <span class=\"hljs-keyword\">return</span> imports\n\n\n<span class=\"\
          hljs-keyword\">with</span> patch(<span class=\"hljs-string\">\"transformers.dynamic_module_utils.get_imports\"\
          </span>, fixed_get_imports):\n    model = AutoModelForCausalLM.from_pretrained(<span\
          \ class=\"hljs-string\">\"microsoft/phi-1_5\"</span>, trust_remote_code=<span\
          \ class=\"hljs-literal\">True</span>)\n</code></pre>\n<p>I think that this\
          \ is actually a bug in <code>transformers.dynamic_module_utils.get_imports</code>,\
          \ it needs to comprehend conditional imports. What do you think?</p>\n<p>I\
          \ opened <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/28459\"\
          >https://github.com/huggingface/transformers/issues/28459</a> in <code>transformers</code>\
          \ to surface this there.</p>\n"
        raw: "It looks like https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/phi/modeling_phi.py#L50-L52\
          \ is getting picked up by https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/dynamic_module_utils.py#L154\
          \ as a requirement, when it actually isn't.\n\nUsing `unittest.mock.patch`,\
          \ we can work around this:\n\n```python\n# With Python 3.11.7, transformers==4.36.2\n\
          import os\nfrom unittest.mock import patch\n\nfrom transformers import AutoModelForCausalLM\n\
          from transformers.dynamic_module_utils import get_imports\n\n\ndef fixed_get_imports(filename:\
          \ str | os.PathLike) -> list[str]:\n    \"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\
          \"\"\n    if not str(filename).endswith(\"/modeling_phi.py\"):\n       \
          \ return get_imports(filename)\n    imports = get_imports(filename)\n  \
          \  imports.remove(\"flash_attn\")\n    return imports\n\n\nwith patch(\"\
          transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n \
          \   model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\",\
          \ trust_remote_code=True)\n```\n\nI think that this is actually a bug in\
          \ `transformers.dynamic_module_utils.get_imports`, it needs to comprehend\
          \ conditional imports. What do you think?\n\nI opened https://github.com/huggingface/transformers/issues/28459\
          \ in `transformers` to surface this there."
        updatedAt: '2024-01-11T18:45:46.607Z'
      numEdits: 1
      reactions: []
    id: 65a0339f64520347aa3d703c
    type: comment
  author: jamesbraza
  content: "It looks like https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/models/phi/modeling_phi.py#L50-L52\
    \ is getting picked up by https://github.com/huggingface/transformers/blob/v4.36.2/src/transformers/dynamic_module_utils.py#L154\
    \ as a requirement, when it actually isn't.\n\nUsing `unittest.mock.patch`, we\
    \ can work around this:\n\n```python\n# With Python 3.11.7, transformers==4.36.2\n\
    import os\nfrom unittest.mock import patch\n\nfrom transformers import AutoModelForCausalLM\n\
    from transformers.dynamic_module_utils import get_imports\n\n\ndef fixed_get_imports(filename:\
    \ str | os.PathLike) -> list[str]:\n    \"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\
    \"\"\n    if not str(filename).endswith(\"/modeling_phi.py\"):\n        return\
    \ get_imports(filename)\n    imports = get_imports(filename)\n    imports.remove(\"\
    flash_attn\")\n    return imports\n\n\nwith patch(\"transformers.dynamic_module_utils.get_imports\"\
    , fixed_get_imports):\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
    , trust_remote_code=True)\n```\n\nI think that this is actually a bug in `transformers.dynamic_module_utils.get_imports`,\
    \ it needs to comprehend conditional imports. What do you think?\n\nI opened https://github.com/huggingface/transformers/issues/28459\
    \ in `transformers` to surface this there."
  created_at: 2024-01-11 18:29:51+00:00
  edited: true
  hidden: false
  id: 65a0339f64520347aa3d703c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T00:27:23.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9517855644226074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;jamesbraza&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/jamesbraza\"\
          >@<span class=\"underline\">jamesbraza</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Thanks a lot for your debugging and raising an issue in transformers.</p>\n\
          <p>Something is definitely off with the dynamic modules loading. I suspect\
          \ this is a combined problem with trust_remote_code=True. It could also\
          \ be related to how things are imported since we are externally importing\
          \ transformers.utils in the remote file.</p>\n<p>Whenever you have some\
          \ time, could you please clone transformers, install it with \u201Cpip install\
          \ -e .\u201D and test the model without trust_remote_code=True?</p>\n<p>You\
          \ will be in version 4.37.0.dev, which has support for the Phi model uploaded\
          \ on this repository.</p>\n<p>If it works, we will be able to isolate even\
          \ more where the issue is located.</p>\n<p>Thanks and regards,<br>Gustavo.</p>\n"
        raw: "Hello @jamesbraza!\n\nThanks a lot for your debugging and raising an\
          \ issue in transformers.\n\nSomething is definitely off with the dynamic\
          \ modules loading. I suspect this is a combined problem with trust_remote_code=True.\
          \ It could also be related to how things are imported since we are externally\
          \ importing transformers.utils in the remote file.\n\nWhenever you have\
          \ some time, could you please clone transformers, install it with \u201C\
          pip install -e .\u201D and test the model without trust_remote_code=True?\n\
          \nYou will be in version 4.37.0.dev, which has support for the Phi model\
          \ uploaded on this repository.\n\nIf it works, we will be able to isolate\
          \ even more where the issue is located.\n\nThanks and regards,\nGustavo."
        updatedAt: '2024-01-12T00:27:23.587Z'
      numEdits: 0
      reactions: []
    id: 65a0876b1754e2f2115ebc84
    type: comment
  author: gugarosa
  content: "Hello @jamesbraza!\n\nThanks a lot for your debugging and raising an issue\
    \ in transformers.\n\nSomething is definitely off with the dynamic modules loading.\
    \ I suspect this is a combined problem with trust_remote_code=True. It could also\
    \ be related to how things are imported since we are externally importing transformers.utils\
    \ in the remote file.\n\nWhenever you have some time, could you please clone transformers,\
    \ install it with \u201Cpip install -e .\u201D and test the model without trust_remote_code=True?\n\
    \nYou will be in version 4.37.0.dev, which has support for the Phi model uploaded\
    \ on this repository.\n\nIf it works, we will be able to isolate even more where\
    \ the issue is located.\n\nThanks and regards,\nGustavo."
  created_at: 2024-01-12 00:27:23+00:00
  edited: false
  hidden: false
  id: 65a0876b1754e2f2115ebc84
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T00:46:59.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9503247141838074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: '<p>I confirm that it only happens with <code>trust_remote_code=True</code>.
          It must be something behind the scenes on how <code>transformers</code>
          process these files.</p>

          <p>Nevertheless, I deployed a fix to use try/except and seems to be working.</p>

          '
        raw: 'I confirm that it only happens with `trust_remote_code=True`. It must
          be something behind the scenes on how `transformers` process these files.


          Nevertheless, I deployed a fix to use try/except and seems to be working.'
        updatedAt: '2024-01-12T00:46:59.547Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - jamesbraza
    id: 65a08c0319665f7549564bf2
    type: comment
  author: gugarosa
  content: 'I confirm that it only happens with `trust_remote_code=True`. It must
    be something behind the scenes on how `transformers` process these files.


    Nevertheless, I deployed a fix to use try/except and seems to be working.'
  created_at: 2024-01-12 00:46:59+00:00
  edited: false
  hidden: false
  id: 65a08c0319665f7549564bf2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
      fullname: James Braza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesbraza
      type: user
    createdAt: '2024-01-12T04:59:28.000Z'
    data:
      edited: false
      editors:
      - jamesbraza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6199829578399658
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
          fullname: James Braza
          isHf: false
          isPro: false
          name: jamesbraza
          type: user
        html: "<p>Yeah it looks like until the <code>transformers</code> automatic\
          \ import detection is generalized, <code>try</code>-<code>except</code>\
          \ is the way to go:</p>\n<pre><code class=\"language-python\"><span class=\"\
          hljs-keyword\">try</span>:  <span class=\"hljs-comment\"># noqa: SIM105</span>\n\
          \    <span class=\"hljs-keyword\">if</span> is_flash_attn_2_available():\n\
          \        <span class=\"hljs-keyword\">from</span> flash_attn <span class=\"\
          hljs-keyword\">import</span> flash_attn_func, flash_attn_varlen_func\n \
          \       <span class=\"hljs-keyword\">from</span> flash_attn.bert_padding\
          \ <span class=\"hljs-keyword\">import</span> index_first_axis, pad_input,\
          \ unpad_input\n<span class=\"hljs-keyword\">except</span> ImportError:\n\
          \    <span class=\"hljs-comment\"># Workaround for https://github.com/huggingface/transformers/issues/28459,</span>\n\
          \    <span class=\"hljs-comment\"># don't move to contextlib.suppress(ImportError)</span>\n\
          \    <span class=\"hljs-keyword\">pass</span>\n</code></pre>\n<p>Feel free\
          \ to tag me on a PR if needed</p>\n"
        raw: "Yeah it looks like until the `transformers` automatic import detection\
          \ is generalized, `try`-`except` is the way to go:\n\n```python\ntry:  #\
          \ noqa: SIM105\n    if is_flash_attn_2_available():\n        from flash_attn\
          \ import flash_attn_func, flash_attn_varlen_func\n        from flash_attn.bert_padding\
          \ import index_first_axis, pad_input, unpad_input\nexcept ImportError:\n\
          \    # Workaround for https://github.com/huggingface/transformers/issues/28459,\n\
          \    # don't move to contextlib.suppress(ImportError)\n    pass\n```\n\n\
          Feel free to tag me on a PR if needed"
        updatedAt: '2024-01-12T04:59:28.213Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - gugarosa
    id: 65a0c7302df784436648d0c3
    type: comment
  author: jamesbraza
  content: "Yeah it looks like until the `transformers` automatic import detection\
    \ is generalized, `try`-`except` is the way to go:\n\n```python\ntry:  # noqa:\
    \ SIM105\n    if is_flash_attn_2_available():\n        from flash_attn import\
    \ flash_attn_func, flash_attn_varlen_func\n        from flash_attn.bert_padding\
    \ import index_first_axis, pad_input, unpad_input\nexcept ImportError:\n    #\
    \ Workaround for https://github.com/huggingface/transformers/issues/28459,\n \
    \   # don't move to contextlib.suppress(ImportError)\n    pass\n```\n\nFeel free\
    \ to tag me on a PR if needed"
  created_at: 2024-01-12 04:59:28+00:00
  edited: false
  hidden: false
  id: 65a0c7302df784436648d0c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-12T12:12:15.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9324155449867249
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;jamesbraza&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/jamesbraza\">@<span class=\"\
          underline\">jamesbraza</span></a></span>\n\n\t</span></span> I prefer your\
          \ fix over mine lol</p>\n<p>Could you please open a PR and update it here?\
          \ Feel free to do on Phi-2 as well.</p>\n<p>I tested it in Phi-1 and works\
          \ beautifully.</p>\n"
        raw: '@jamesbraza I prefer your fix over mine lol


          Could you please open a PR and update it here? Feel free to do on Phi-2
          as well.


          I tested it in Phi-1 and works beautifully.'
        updatedAt: '2024-01-12T12:12:15.032Z'
      numEdits: 0
      reactions: []
    id: 65a12c9fdb5d37ad5ec63384
    type: comment
  author: gugarosa
  content: '@jamesbraza I prefer your fix over mine lol


    Could you please open a PR and update it here? Feel free to do on Phi-2 as well.


    I tested it in Phi-1 and works beautifully.'
  created_at: 2024-01-12 12:12:15+00:00
  edited: false
  hidden: false
  id: 65a12c9fdb5d37ad5ec63384
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-15T14:27:32.000Z'
    data:
      status: closed
    id: 65a540d4d0e350dbc9c279eb
    type: status-change
  author: gugarosa
  created_at: 2024-01-15 14:27:32+00:00
  id: 65a540d4d0e350dbc9c279eb
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
      fullname: James Braza
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jamesbraza
      type: user
    createdAt: '2024-01-16T03:44:05.000Z'
    data:
      edited: false
      editors:
      - jamesbraza
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7465410828590393
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64b83ceb49bde5d94813ce2e/PYf6nXBVGQPIgTk_NzudC.png?w=200&h=200&f=face
          fullname: James Braza
          isHf: false
          isPro: false
          name: jamesbraza
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gugarosa\"\
          >@<span class=\"underline\">gugarosa</span></a></span>\n\n\t</span></span>\
          \ looks like you implemented this in <a href=\"https://huggingface.co/microsoft/phi-1_5/commit/426ea900b06746a7c907c5594fe85f16f5e1f3f8\"\
          ></a><a href=\"/microsoft/phi-1_5/commit/426ea90\">426ea90</a>. Thanks for\
          \ being responsive and improving the source code here :)</p>\n"
        raw: Hello @gugarosa looks like you implemented this in [426ea90](https://huggingface.co/microsoft/phi-1_5/commit/426ea900b06746a7c907c5594fe85f16f5e1f3f8).
          Thanks for being responsive and improving the source code here :)
        updatedAt: '2024-01-16T03:44:05.576Z'
      numEdits: 0
      reactions: []
    id: 65a5fb85e1e787bdecca68b7
    type: comment
  author: jamesbraza
  content: Hello @gugarosa looks like you implemented this in [426ea90](https://huggingface.co/microsoft/phi-1_5/commit/426ea900b06746a7c907c5594fe85f16f5e1f3f8).
    Thanks for being responsive and improving the source code here :)
  created_at: 2024-01-16 03:44:05+00:00
  edited: false
  hidden: false
  id: 65a5fb85e1e787bdecca68b7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 72
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: 'Failed to run on MacBook: requiring flash_attn'
