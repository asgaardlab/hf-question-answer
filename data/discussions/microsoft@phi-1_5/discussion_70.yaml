!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nihalnayak
conflicting_files: null
created_at: 2024-01-10 21:49:45+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ca2d4edf059589fa9897346417e56404.svg
      fullname: Nihal Nayak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nihalnayak
      type: user
    createdAt: '2024-01-10T21:49:45.000Z'
    data:
      edited: false
      editors:
      - nihalnayak
      hidden: false
      identifiedLanguage:
        language: ja
        probability: 0.07230244576931
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ca2d4edf059589fa9897346417e56404.svg
          fullname: Nihal Nayak
          isHf: false
          isPro: false
          name: nihalnayak
          type: user
        html: "<p>Started getting this error today after some changes were made to\
          \ the phi model. The model does not use all the weights from the checkpoint.\
          \ </p>\n<pre><code>In [2]: model = AutoModelForCausalLM.from_pretrained(\"\
          microsoft/phi-1_5\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n\
          pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.84G/2.84G\
          \ [00:24&lt;00:00, 116MB/s]\nSome weights of the model checkpoint at microsoft/phi-1_5\
          \ were not used when initializing PhiForCausalLM: ['model.layers.11.self_attn.q_proj.bias',\
          \ 'model.layers.22.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias',\
          \ 'model.layers.9.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.weight',\
          \ 'model.layers.22.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.weight',\
          \ 'model.layers.2.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias',\
          \ 'model.layers.15.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight',\
          \ 'model.layers.5.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight',\
          \ 'model.layers.9.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight',\
          \ 'model.layers.0.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias',\
          \ 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight',\
          \ 'model.layers.9.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.weight',\
          \ 'model.layers.1.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight',\
          \ 'model.layers.12.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.weight',\
          \ 'model.layers.17.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias',\
          \ 'model.layers.13.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.bias',\
          \ 'model.layers.9.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias',\
          \ 'model.layers.7.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias',\
          \ 'model.layers.16.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias',\
          \ 'model.layers.21.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias',\
          \ 'model.layers.15.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias',\
          \ 'model.layers.15.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.bias',\
          \ 'model.layers.6.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight',\
          \ 'model.layers.21.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight',\
          \ 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias',\
          \ 'model.layers.16.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight',\
          \ 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight',\
          \ 'model.layers.14.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias',\
          \ 'model.layers.4.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.bias',\
          \ 'model.layers.7.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight',\
          \ 'model.layers.3.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias',\
          \ 'model.layers.6.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias',\
          \ 'model.layers.16.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight',\
          \ 'model.layers.1.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight',\
          \ 'model.layers.17.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.bias',\
          \ 'model.layers.14.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias',\
          \ 'model.layers.20.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight',\
          \ 'model.layers.20.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight',\
          \ 'model.layers.8.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight',\
          \ 'model.layers.23.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight',\
          \ 'model.layers.7.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.bias',\
          \ 'model.layers.15.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.weight',\
          \ 'model.layers.6.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.bias',\
          \ 'model.layers.12.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.bias',\
          \ 'model.layers.18.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias',\
          \ 'model.layers.16.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight',\
          \ 'model.layers.17.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias',\
          \ 'model.layers.22.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.bias',\
          \ 'model.layers.11.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight',\
          \ 'model.layers.23.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.bias',\
          \ 'model.layers.18.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight',\
          \ 'model.layers.23.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight',\
          \ 'model.layers.8.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight',\
          \ 'model.layers.8.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight',\
          \ 'model.layers.2.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight',\
          \ 'model.layers.11.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.weight',\
          \ 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight',\
          \ 'model.layers.9.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.bias',\
          \ 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.q_proj.bias',\
          \ 'model.layers.21.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight',\
          \ 'model.layers.15.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.bias',\
          \ 'model.layers.7.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.weight',\
          \ 'model.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight',\
          \ 'model.layers.16.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias',\
          \ 'model.layers.6.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight',\
          \ 'model.layers.20.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias',\
          \ 'model.layers.10.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight',\
          \ 'model.layers.23.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.weight',\
          \ 'model.layers.13.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.bias',\
          \ 'model.layers.14.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight',\
          \ 'model.layers.3.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias',\
          \ 'model.layers.19.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.bias',\
          \ 'model.layers.0.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight',\
          \ 'model.layers.5.self_attn.k_proj.bias']\n- This IS expected if you are\
          \ initializing PhiForCausalLM from the checkpoint of a model trained on\
          \ another task or with another architecture (e.g. initializing a BertForSequenceClassification\
          \ model from a BertForPreTraining model).\n- This IS NOT expected if you\
          \ are initializing PhiForCausalLM from the checkpoint of a model that you\
          \ expect to be exactly identical (initializing a BertForSequenceClassification\
          \ model from a BertForSequenceClassification model).\nSome weights of PhiForCausalLM\
          \ were not initialized from the model checkpoint at microsoft/phi-1_5 and\
          \ are newly initialized: ['model.layers.12.self_attn.query_key_value.weight',\
          \ 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias',\
          \ 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight',\
          \ 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight',\
          \ 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias',\
          \ 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight',\
          \ 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight',\
          \ 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias',\
          \ 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias',\
          \ 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight',\
          \ 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight',\
          \ 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight',\
          \ 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias',\
          \ 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight',\
          \ 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight',\
          \ 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight',\
          \ 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias',\
          \ 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias',\
          \ 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight',\
          \ 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias',\
          \ 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight',\
          \ 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias',\
          \ 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias',\
          \ 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight',\
          \ 'model.layers.1.self_attn.query_key_value.bias']\nYou should probably\
          \ TRAIN this model on a down-stream task to be able to use it for predictions\
          \ and inference.\ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74.0/74.0 [00:00&lt;00:00,\
          \ 70.9kB/s]\n</code></pre>\n"
        raw: "Started getting this error today after some changes were made to the\
          \ phi model. The model does not use all the weights from the checkpoint.\
          \ \r\n```\r\nIn [2]: model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\"\
          , torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\r\n\
          pytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.84G/2.84G\
          \ [00:24<00:00, 116MB/s]\r\nSome weights of the model checkpoint at microsoft/phi-1_5\
          \ were not used when initializing PhiForCausalLM: ['model.layers.11.self_attn.q_proj.bias',\
          \ 'model.layers.22.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias',\
          \ 'model.layers.9.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.weight',\
          \ 'model.layers.22.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.weight',\
          \ 'model.layers.2.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias',\
          \ 'model.layers.15.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight',\
          \ 'model.layers.5.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight',\
          \ 'model.layers.9.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight',\
          \ 'model.layers.0.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias',\
          \ 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight',\
          \ 'model.layers.9.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.weight',\
          \ 'model.layers.1.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight',\
          \ 'model.layers.12.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.weight',\
          \ 'model.layers.17.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias',\
          \ 'model.layers.13.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.bias',\
          \ 'model.layers.9.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias',\
          \ 'model.layers.7.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias',\
          \ 'model.layers.16.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias',\
          \ 'model.layers.21.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias',\
          \ 'model.layers.15.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias',\
          \ 'model.layers.15.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.bias',\
          \ 'model.layers.6.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight',\
          \ 'model.layers.21.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight',\
          \ 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias',\
          \ 'model.layers.16.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight',\
          \ 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight',\
          \ 'model.layers.14.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias',\
          \ 'model.layers.4.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.bias',\
          \ 'model.layers.7.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight',\
          \ 'model.layers.3.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias',\
          \ 'model.layers.6.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias',\
          \ 'model.layers.16.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight',\
          \ 'model.layers.1.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight',\
          \ 'model.layers.17.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.bias',\
          \ 'model.layers.14.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias',\
          \ 'model.layers.20.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight',\
          \ 'model.layers.20.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight',\
          \ 'model.layers.8.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight',\
          \ 'model.layers.23.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight',\
          \ 'model.layers.7.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.bias',\
          \ 'model.layers.15.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.weight',\
          \ 'model.layers.6.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.bias',\
          \ 'model.layers.12.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.bias',\
          \ 'model.layers.18.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias',\
          \ 'model.layers.16.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight',\
          \ 'model.layers.17.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias',\
          \ 'model.layers.22.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.bias',\
          \ 'model.layers.11.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight',\
          \ 'model.layers.23.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.bias',\
          \ 'model.layers.18.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight',\
          \ 'model.layers.23.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight',\
          \ 'model.layers.8.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight',\
          \ 'model.layers.8.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight',\
          \ 'model.layers.2.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight',\
          \ 'model.layers.11.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.weight',\
          \ 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight',\
          \ 'model.layers.9.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.bias',\
          \ 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.q_proj.bias',\
          \ 'model.layers.21.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight',\
          \ 'model.layers.15.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.bias',\
          \ 'model.layers.7.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.weight',\
          \ 'model.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight',\
          \ 'model.layers.16.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias',\
          \ 'model.layers.6.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight',\
          \ 'model.layers.20.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias',\
          \ 'model.layers.10.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight',\
          \ 'model.layers.23.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.weight',\
          \ 'model.layers.13.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.bias',\
          \ 'model.layers.14.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight',\
          \ 'model.layers.3.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias',\
          \ 'model.layers.19.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.bias',\
          \ 'model.layers.0.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight',\
          \ 'model.layers.5.self_attn.k_proj.bias']\r\n- This IS expected if you are\
          \ initializing PhiForCausalLM from the checkpoint of a model trained on\
          \ another task or with another architecture (e.g. initializing a BertForSequenceClassification\
          \ model from a BertForPreTraining model).\r\n- This IS NOT expected if you\
          \ are initializing PhiForCausalLM from the checkpoint of a model that you\
          \ expect to be exactly identical (initializing a BertForSequenceClassification\
          \ model from a BertForSequenceClassification model).\r\nSome weights of\
          \ PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-1_5\
          \ and are newly initialized: ['model.layers.12.self_attn.query_key_value.weight',\
          \ 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias',\
          \ 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight',\
          \ 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight',\
          \ 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias',\
          \ 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight',\
          \ 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight',\
          \ 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias',\
          \ 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias',\
          \ 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight',\
          \ 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight',\
          \ 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight',\
          \ 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias',\
          \ 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight',\
          \ 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight',\
          \ 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight',\
          \ 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias',\
          \ 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias',\
          \ 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight',\
          \ 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias',\
          \ 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight',\
          \ 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias',\
          \ 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias',\
          \ 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight',\
          \ 'model.layers.1.self_attn.query_key_value.bias']\r\nYou should probably\
          \ TRAIN this model on a down-stream task to be able to use it for predictions\
          \ and inference.\r\ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 74.0/74.0\
          \ [00:00<00:00, 70.9kB/s]\r\n```"
        updatedAt: '2024-01-10T21:49:45.559Z'
      numEdits: 0
      reactions: []
    id: 659f10f9b2ac1661351140ca
    type: comment
  author: nihalnayak
  content: "Started getting this error today after some changes were made to the phi\
    \ model. The model does not use all the weights from the checkpoint. \r\n```\r\
    \nIn [2]: model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\",\
    \ torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\r\npytorch_model.bin:\
    \ 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 2.84G/2.84G [00:24<00:00, 116MB/s]\r\nSome weights of the model checkpoint at\
    \ microsoft/phi-1_5 were not used when initializing PhiForCausalLM: ['model.layers.11.self_attn.q_proj.bias',\
    \ 'model.layers.22.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias',\
    \ 'model.layers.9.self_attn.k_proj.bias', 'model.layers.10.self_attn.q_proj.weight',\
    \ 'model.layers.22.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.weight',\
    \ 'model.layers.2.self_attn.k_proj.bias', 'model.layers.22.self_attn.v_proj.bias',\
    \ 'model.layers.15.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight',\
    \ 'model.layers.5.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight',\
    \ 'model.layers.9.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight',\
    \ 'model.layers.0.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias',\
    \ 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight',\
    \ 'model.layers.9.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.weight',\
    \ 'model.layers.1.self_attn.k_proj.weight', 'model.layers.3.self_attn.k_proj.weight',\
    \ 'model.layers.12.self_attn.k_proj.bias', 'model.layers.8.self_attn.q_proj.weight',\
    \ 'model.layers.17.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias',\
    \ 'model.layers.13.self_attn.v_proj.bias', 'model.layers.20.self_attn.q_proj.bias',\
    \ 'model.layers.9.self_attn.v_proj.weight', 'model.layers.4.self_attn.v_proj.bias',\
    \ 'model.layers.7.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias',\
    \ 'model.layers.16.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias',\
    \ 'model.layers.21.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.bias',\
    \ 'model.layers.15.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.bias',\
    \ 'model.layers.15.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.bias',\
    \ 'model.layers.6.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight',\
    \ 'model.layers.21.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight',\
    \ 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias',\
    \ 'model.layers.16.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight',\
    \ 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight',\
    \ 'model.layers.14.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias',\
    \ 'model.layers.4.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.bias',\
    \ 'model.layers.7.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight',\
    \ 'model.layers.3.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias',\
    \ 'model.layers.6.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias',\
    \ 'model.layers.16.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight',\
    \ 'model.layers.1.self_attn.v_proj.bias', 'model.layers.4.self_attn.k_proj.weight',\
    \ 'model.layers.17.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.bias',\
    \ 'model.layers.14.self_attn.k_proj.bias', 'model.layers.1.self_attn.q_proj.bias',\
    \ 'model.layers.20.self_attn.v_proj.bias', 'model.layers.23.self_attn.v_proj.weight',\
    \ 'model.layers.20.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.weight',\
    \ 'model.layers.8.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight',\
    \ 'model.layers.23.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight',\
    \ 'model.layers.7.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.bias',\
    \ 'model.layers.15.self_attn.k_proj.bias', 'model.layers.20.self_attn.q_proj.weight',\
    \ 'model.layers.6.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.bias',\
    \ 'model.layers.12.self_attn.v_proj.weight', 'model.layers.0.self_attn.k_proj.bias',\
    \ 'model.layers.18.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias',\
    \ 'model.layers.16.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight',\
    \ 'model.layers.17.self_attn.v_proj.bias', 'model.layers.21.self_attn.q_proj.bias',\
    \ 'model.layers.22.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.bias',\
    \ 'model.layers.11.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.weight',\
    \ 'model.layers.23.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.bias',\
    \ 'model.layers.18.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight',\
    \ 'model.layers.23.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.weight',\
    \ 'model.layers.8.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight',\
    \ 'model.layers.8.self_attn.v_proj.weight', 'model.layers.18.self_attn.k_proj.weight',\
    \ 'model.layers.2.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight',\
    \ 'model.layers.11.self_attn.k_proj.bias', 'model.layers.13.self_attn.v_proj.weight',\
    \ 'model.layers.0.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight',\
    \ 'model.layers.9.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.bias',\
    \ 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.self_attn.q_proj.bias',\
    \ 'model.layers.21.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight',\
    \ 'model.layers.15.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.bias',\
    \ 'model.layers.7.self_attn.k_proj.bias', 'model.layers.13.self_attn.q_proj.weight',\
    \ 'model.layers.21.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight',\
    \ 'model.layers.16.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.bias',\
    \ 'model.layers.6.self_attn.v_proj.bias', 'model.layers.19.self_attn.v_proj.weight',\
    \ 'model.layers.20.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias',\
    \ 'model.layers.10.self_attn.k_proj.bias', 'model.layers.5.self_attn.v_proj.weight',\
    \ 'model.layers.23.self_attn.v_proj.bias', 'model.layers.2.self_attn.q_proj.weight',\
    \ 'model.layers.13.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.bias',\
    \ 'model.layers.14.self_attn.q_proj.bias', 'model.layers.10.self_attn.k_proj.weight',\
    \ 'model.layers.3.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias',\
    \ 'model.layers.19.self_attn.k_proj.weight', 'model.layers.4.self_attn.k_proj.bias',\
    \ 'model.layers.0.self_attn.v_proj.bias', 'model.layers.6.self_attn.k_proj.weight',\
    \ 'model.layers.5.self_attn.k_proj.bias']\r\n- This IS expected if you are initializing\
    \ PhiForCausalLM from the checkpoint of a model trained on another task or with\
    \ another architecture (e.g. initializing a BertForSequenceClassification model\
    \ from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing\
    \ PhiForCausalLM from the checkpoint of a model that you expect to be exactly\
    \ identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
    \ model).\r\nSome weights of PhiForCausalLM were not initialized from the model\
    \ checkpoint at microsoft/phi-1_5 and are newly initialized: ['model.layers.12.self_attn.query_key_value.weight',\
    \ 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias',\
    \ 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight',\
    \ 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight',\
    \ 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias',\
    \ 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.weight',\
    \ 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight',\
    \ 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias',\
    \ 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias',\
    \ 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight',\
    \ 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight',\
    \ 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight',\
    \ 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias',\
    \ 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight',\
    \ 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight',\
    \ 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight',\
    \ 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias',\
    \ 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias',\
    \ 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight',\
    \ 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.bias',\
    \ 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight',\
    \ 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias',\
    \ 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias',\
    \ 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight',\
    \ 'model.layers.1.self_attn.query_key_value.bias']\r\nYou should probably TRAIN\
    \ this model on a down-stream task to be able to use it for predictions and inference.\r\
    \ngeneration_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\
    | 74.0/74.0 [00:00<00:00, 70.9kB/s]\r\n```"
  created_at: 2024-01-10 21:49:45+00:00
  edited: false
  hidden: false
  id: 659f10f9b2ac1661351140ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
      fullname: M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Maykeye
      type: user
    createdAt: '2024-01-11T10:02:55.000Z'
    data:
      edited: false
      editors:
      - Maykeye
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8374096751213074
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63a3c6507abdaa25a81ce659/Z7e4xiH7sjQYt2Qga4W8o.png?w=200&h=200&f=face
          fullname: M
          isHf: false
          isPro: false
          name: Maykeye
          type: user
        html: "<p>Phi in transformers library has different \"architecture\": for\
          \ one instead of q_proj/k_proj they have single quiery_key_value</p>\n<p>Simplest\
          \ solution is to change config to use provided files</p>\n<p>This worked\
          \ for me; some steps might be redundant:</p>\n<ul>\n<li>I've copied modeling_phi\
          \ to modeling_phi_1_5, configuration_phi to configuration_phi_1_5.py to\
          \ prevent filename collision with transformers if it checks</li>\n<li>Added\
          \ this into config.json:</li>\n</ul>\n<pre><code>    \"auto_map\": {\n \
          \     \"AutoConfig\": \"configuration_phi_1_5.PhiConfig\",\n      \"AutoModelForCausalLM\"\
          : \"modeling_phi_1_5.PhiForCausalLM\"\n    },\n</code></pre>\n<ul>\n<li>Changed\
          \ <code>model_type</code> to <code>\"model_type\": \"phi_1_5\"</code> (I\
          \ think without this change transformers didn't try to load custom_code)</li>\n\
          <li>Changed <code>architectures</code> to \"PhiForCausalLM_1_5\"  (I didn't\
          \ change the .py file beyond renaming)</li>\n</ul>\n<p>After that changes\
          \ model loaded successfully.</p>\n<pre><code>Write a detailed analogy between\
          \ mathematics and a lighthouse.\n\nAnswer: Mathematics is like a lighthouse,\
          \ guiding us through the complex world of numbers and calculations. It illumin&lt;MAX_NEW_TOKENS_REACHED&gt;\n\
          </code></pre>\n<p>(Interestingly even with do_sample=False I get different\
          \ result from model card: Mathematics is like a lighthouse, guiding us through\
          \ <strong>the vast ocean</strong> of numbers and calculations. <strong>Just\
          \ as a lighthouse</strong> illuminates....)</p>\n"
        raw: "Phi in transformers library has different \"architecture\": for one\
          \ instead of q_proj/k_proj they have single quiery_key_value\n\nSimplest\
          \ solution is to change config to use provided files\n\nThis worked for\
          \ me; some steps might be redundant:\n\n* I've copied modeling_phi to modeling_phi_1_5,\
          \ configuration_phi to configuration_phi_1_5.py to prevent filename collision\
          \ with transformers if it checks\n* Added this into config.json:\n```\n\
          \    \"auto_map\": {\n      \"AutoConfig\": \"configuration_phi_1_5.PhiConfig\"\
          ,\n      \"AutoModelForCausalLM\": \"modeling_phi_1_5.PhiForCausalLM\"\n\
          \    },\n```\n* Changed `model_type` to `\"model_type\": \"phi_1_5\"` (I\
          \ think without this change transformers didn't try to load custom_code)\n\
          * Changed `architectures` to \"PhiForCausalLM_1_5\"  (I didn't change the\
          \ .py file beyond renaming)\n\nAfter that changes model loaded successfully.\n\
          ```\nWrite a detailed analogy between mathematics and a lighthouse.\n\n\
          Answer: Mathematics is like a lighthouse, guiding us through the complex\
          \ world of numbers and calculations. It illumin<MAX_NEW_TOKENS_REACHED>\n\
          ```\n\n(Interestingly even with do_sample=False I get different result from\
          \ model card: Mathematics is like a lighthouse, guiding us through **the\
          \ vast ocean** of numbers and calculations. **Just as a lighthouse** illuminates....)"
        updatedAt: '2024-01-11T10:02:55.758Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Snarci
    id: 659fbccfaaf0b1a2ddd1effc
    type: comment
  author: Maykeye
  content: "Phi in transformers library has different \"architecture\": for one instead\
    \ of q_proj/k_proj they have single quiery_key_value\n\nSimplest solution is to\
    \ change config to use provided files\n\nThis worked for me; some steps might\
    \ be redundant:\n\n* I've copied modeling_phi to modeling_phi_1_5, configuration_phi\
    \ to configuration_phi_1_5.py to prevent filename collision with transformers\
    \ if it checks\n* Added this into config.json:\n```\n    \"auto_map\": {\n   \
    \   \"AutoConfig\": \"configuration_phi_1_5.PhiConfig\",\n      \"AutoModelForCausalLM\"\
    : \"modeling_phi_1_5.PhiForCausalLM\"\n    },\n```\n* Changed `model_type` to\
    \ `\"model_type\": \"phi_1_5\"` (I think without this change transformers didn't\
    \ try to load custom_code)\n* Changed `architectures` to \"PhiForCausalLM_1_5\"\
    \  (I didn't change the .py file beyond renaming)\n\nAfter that changes model\
    \ loaded successfully.\n```\nWrite a detailed analogy between mathematics and\
    \ a lighthouse.\n\nAnswer: Mathematics is like a lighthouse, guiding us through\
    \ the complex world of numbers and calculations. It illumin<MAX_NEW_TOKENS_REACHED>\n\
    ```\n\n(Interestingly even with do_sample=False I get different result from model\
    \ card: Mathematics is like a lighthouse, guiding us through **the vast ocean**\
    \ of numbers and calculations. **Just as a lighthouse** illuminates....)"
  created_at: 2024-01-11 10:02:55+00:00
  edited: false
  hidden: false
  id: 659fbccfaaf0b1a2ddd1effc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-11T11:26:21.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9211508631706238
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;nihalnayak&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/nihalnayak\"\
          >@<span class=\"underline\">nihalnayak</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>We just pushed a fix to the <code>config.json</code> and it should work\
          \ now. The <code>auto_map</code> key was missing and hence it was not properly\
          \ using the files on this repository when <code>trust_remote_code=True</code>.</p>\n\
          <p>Best regards,<br>Gustavo.</p>\n"
        raw: 'Hello @nihalnayak!


          We just pushed a fix to the `config.json` and it should work now. The `auto_map`
          key was missing and hence it was not properly using the files on this repository
          when `trust_remote_code=True`.


          Best regards,

          Gustavo.'
        updatedAt: '2024-01-11T11:26:21.228Z'
      numEdits: 0
      reactions: []
      relatedEventId: 659fd05db3e86463842900d7
    id: 659fd05db3e86463842900d4
    type: comment
  author: gugarosa
  content: 'Hello @nihalnayak!


    We just pushed a fix to the `config.json` and it should work now. The `auto_map`
    key was missing and hence it was not properly using the files on this repository
    when `trust_remote_code=True`.


    Best regards,

    Gustavo.'
  created_at: 2024-01-11 11:26:21+00:00
  edited: false
  hidden: false
  id: 659fd05db3e86463842900d4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2024-01-11T11:26:21.000Z'
    data:
      status: closed
    id: 659fd05db3e86463842900d7
    type: status-change
  author: gugarosa
  created_at: 2024-01-11 11:26:21+00:00
  id: 659fd05db3e86463842900d7
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 70
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Weights not used when initializing the model
