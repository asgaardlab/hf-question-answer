!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DanielNlp
conflicting_files: null
created_at: 2023-11-28 15:33:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/480e23116d1b47d6a1aeb848758c3e32.svg
      fullname: Daniel R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielNlp
      type: user
    createdAt: '2023-11-28T15:33:09.000Z'
    data:
      edited: true
      editors:
      - DanielNlp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.11771336197853088
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/480e23116d1b47d6a1aeb848758c3e32.svg
          fullname: Daniel R
          isHf: false
          isPro: false
          name: DanielNlp
          type: user
        html: '<p>When using the newest version of transformers (installed from source
          4.36.2) and loading the PhiForCausalLM direclty i.e.:</p>

          <p>from transformers import PhiForCausalLM<br>PhiForCausalLM.from_pretrained(...)</p>

          <p>All weights were mismatched.</p>

          <p>Here is a dictionary to map the old keys to the new ones:</p>

          <p>remap = {''transformer.embd.wte.weight'': ''model.embed_tokens.weight'',<br>
          ''lm_head.ln.weight'': ''model.final_layernorm.weight'',<br> ''lm_head.ln.bias'':
          ''model.final_layernorm.bias'',<br> ''lm_head.linear.weight'': ''lm_head.weight'',<br>
          ''lm_head.linear.bias'': ''lm_head.bias'',<br> ''transformer.h.{i}.ln.bias'':
          ''model.layers.{i}.input_layernorm.bias'',<br> ''transformer.h.{i}.ln.weight'':
          ''model.layers.{i}.input_layernorm.weight'',<br> ''transformer.h.{i}.mixer.Wqkv.weight'':
          ''model.layers.{i}.self_attn.query_key_value.weight'',<br> ''transformer.h.{i}.mixer.Wqkv.bias'':
          ''model.layers.{i}.self_attn.query_key_value.bias'',<br> ''transformer.h.{i}.mixer.out_proj.weight'':
          ''model.layers.{i}.self_attn.dense.weight'',<br> ''transformer.h.{i}.mixer.out_proj.bias'':
          ''model.layers.{i}.self_attn.dense.bias'',<br> ''transformer.h.{i}.mlp.fc1.weight'':
          ''model.layers.{i}.mlp.fc1.weight'',<br> ''transformer.h.{i}.mlp.fc1.bias'':
          ''model.layers.{i}.mlp.fc1.bias'',<br> ''transformer.h.{i}.mlp.fc2.weight'':
          ''model.layers.{i}.mlp.fc2.weight'',<br> ''transformer.h.{i}.mlp.fc2.bias'':
          ''model.layers.{i}.mlp.fc2.bias''}</p>

          <p>I wrote a function to remap the state dict and loaded it into the new
          model class.</p>

          '
        raw: "When using the newest version of transformers (installed from source\
          \ 4.36.2) and loading the PhiForCausalLM direclty i.e.:\n\nfrom transformers\
          \ import PhiForCausalLM\nPhiForCausalLM.from_pretrained(...)\n \nAll weights\
          \ were mismatched.\n\nHere is a dictionary to map the old keys to the new\
          \ ones:\n\nremap = {'transformer.embd.wte.weight': 'model.embed_tokens.weight',\n\
          \ 'lm_head.ln.weight': 'model.final_layernorm.weight',\n 'lm_head.ln.bias':\
          \ 'model.final_layernorm.bias',\n 'lm_head.linear.weight': 'lm_head.weight',\n\
          \ 'lm_head.linear.bias': 'lm_head.bias',\n 'transformer.h.{i}.ln.bias':\
          \ 'model.layers.{i}.input_layernorm.bias',\n 'transformer.h.{i}.ln.weight':\
          \ 'model.layers.{i}.input_layernorm.weight',\n 'transformer.h.{i}.mixer.Wqkv.weight':\
          \ 'model.layers.{i}.self_attn.query_key_value.weight',\n 'transformer.h.{i}.mixer.Wqkv.bias':\
          \ 'model.layers.{i}.self_attn.query_key_value.bias',\n 'transformer.h.{i}.mixer.out_proj.weight':\
          \ 'model.layers.{i}.self_attn.dense.weight',\n 'transformer.h.{i}.mixer.out_proj.bias':\
          \ 'model.layers.{i}.self_attn.dense.bias',\n 'transformer.h.{i}.mlp.fc1.weight':\
          \ 'model.layers.{i}.mlp.fc1.weight',\n 'transformer.h.{i}.mlp.fc1.bias':\
          \ 'model.layers.{i}.mlp.fc1.bias',\n 'transformer.h.{i}.mlp.fc2.weight':\
          \ 'model.layers.{i}.mlp.fc2.weight',\n 'transformer.h.{i}.mlp.fc2.bias':\
          \ 'model.layers.{i}.mlp.fc2.bias'}\n\nI wrote a function to remap the state\
          \ dict and loaded it into the new model class.\n"
        updatedAt: '2023-11-28T17:27:59.510Z'
      numEdits: 2
      reactions: []
    id: 65660835e0977cf44f597116
    type: comment
  author: DanielNlp
  content: "When using the newest version of transformers (installed from source 4.36.2)\
    \ and loading the PhiForCausalLM direclty i.e.:\n\nfrom transformers import PhiForCausalLM\n\
    PhiForCausalLM.from_pretrained(...)\n \nAll weights were mismatched.\n\nHere is\
    \ a dictionary to map the old keys to the new ones:\n\nremap = {'transformer.embd.wte.weight':\
    \ 'model.embed_tokens.weight',\n 'lm_head.ln.weight': 'model.final_layernorm.weight',\n\
    \ 'lm_head.ln.bias': 'model.final_layernorm.bias',\n 'lm_head.linear.weight':\
    \ 'lm_head.weight',\n 'lm_head.linear.bias': 'lm_head.bias',\n 'transformer.h.{i}.ln.bias':\
    \ 'model.layers.{i}.input_layernorm.bias',\n 'transformer.h.{i}.ln.weight': 'model.layers.{i}.input_layernorm.weight',\n\
    \ 'transformer.h.{i}.mixer.Wqkv.weight': 'model.layers.{i}.self_attn.query_key_value.weight',\n\
    \ 'transformer.h.{i}.mixer.Wqkv.bias': 'model.layers.{i}.self_attn.query_key_value.bias',\n\
    \ 'transformer.h.{i}.mixer.out_proj.weight': 'model.layers.{i}.self_attn.dense.weight',\n\
    \ 'transformer.h.{i}.mixer.out_proj.bias': 'model.layers.{i}.self_attn.dense.bias',\n\
    \ 'transformer.h.{i}.mlp.fc1.weight': 'model.layers.{i}.mlp.fc1.weight',\n 'transformer.h.{i}.mlp.fc1.bias':\
    \ 'model.layers.{i}.mlp.fc1.bias',\n 'transformer.h.{i}.mlp.fc2.weight': 'model.layers.{i}.mlp.fc2.weight',\n\
    \ 'transformer.h.{i}.mlp.fc2.bias': 'model.layers.{i}.mlp.fc2.bias'}\n\nI wrote\
    \ a function to remap the state dict and loaded it into the new model class.\n"
  created_at: 2023-11-28 15:33:09+00:00
  edited: true
  hidden: false
  id: 65660835e0977cf44f597116
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/480e23116d1b47d6a1aeb848758c3e32.svg
      fullname: Daniel R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielNlp
      type: user
    createdAt: '2023-11-28T17:28:05.000Z'
    data:
      status: closed
    id: 65662325162ad28c047741d8
    type: status-change
  author: DanielNlp
  created_at: 2023-11-28 17:28:05+00:00
  id: 65662325162ad28c047741d8
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/480e23116d1b47d6a1aeb848758c3e32.svg
      fullname: Daniel R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielNlp
      type: user
    createdAt: '2023-11-28T18:32:12.000Z'
    data:
      status: open
    id: 6566322c58192d93d89472ba
    type: status-change
  author: DanielNlp
  created_at: 2023-11-28 18:32:12+00:00
  id: 6566322c58192d93d89472ba
  new_status: open
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/480e23116d1b47d6a1aeb848758c3e32.svg
      fullname: Daniel R
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DanielNlp
      type: user
    createdAt: '2023-11-28T18:33:28.000Z'
    data:
      edited: false
      editors:
      - DanielNlp
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8852545619010925
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/480e23116d1b47d6a1aeb848758c3e32.svg
          fullname: Daniel R
          isHf: false
          isPro: false
          name: DanielNlp
          type: user
        html: '<p>The dict above allows remapping and loading the model but the generation
          is broken afterward (generating only the win token). Is there a fix? Or
          when will the remapped state dict be released?</p>

          '
        raw: The dict above allows remapping and loading the model but the generation
          is broken afterward (generating only the win token). Is there a fix? Or
          when will the remapped state dict be released?
        updatedAt: '2023-11-28T18:33:28.127Z'
      numEdits: 0
      reactions: []
    id: 65663278f450504854ccaa4b
    type: comment
  author: DanielNlp
  content: The dict above allows remapping and loading the model but the generation
    is broken afterward (generating only the win token). Is there a fix? Or when will
    the remapped state dict be released?
  created_at: 2023-11-28 18:33:28+00:00
  edited: false
  hidden: false
  id: 65663278f450504854ccaa4b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-01T20:52:38.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8821057677268982
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;DanielNlp&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/DanielNlp\"\
          >@<span class=\"underline\">DanielNlp</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>The current <code>pytorch_model.bin</code> is expected to work with the\
          \ model architecture of the classes defined in this repository, instead\
          \ of <code>PhiForCausalLM</code> from <code>transformers 4.36.2+</code>.</p>\n\
          <p>As soon as we have an official release, we will be updating everything.</p>\n\
          <p>Regards,<br>Gustavo.</p>\n"
        raw: 'Hello @DanielNlp!


          The current `pytorch_model.bin` is expected to work with the model architecture
          of the classes defined in this repository, instead of `PhiForCausalLM` from
          `transformers 4.36.2+`.


          As soon as we have an official release, we will be updating everything.


          Regards,

          Gustavo.'
        updatedAt: '2023-12-01T20:52:38.164Z'
      numEdits: 0
      reactions: []
      relatedEventId: 656a47967c934a7b3caee13b
    id: 656a47967c934a7b3caee13a
    type: comment
  author: gugarosa
  content: 'Hello @DanielNlp!


    The current `pytorch_model.bin` is expected to work with the model architecture
    of the classes defined in this repository, instead of `PhiForCausalLM` from `transformers
    4.36.2+`.


    As soon as we have an official release, we will be updating everything.


    Regards,

    Gustavo.'
  created_at: 2023-12-01 20:52:38+00:00
  edited: false
  hidden: false
  id: 656a47967c934a7b3caee13a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-12-01T20:52:38.000Z'
    data:
      status: closed
    id: 656a47967c934a7b3caee13b
    type: status-change
  author: gugarosa
  created_at: 2023-12-01 20:52:38+00:00
  id: 656a47967c934a7b3caee13b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 64
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: Missing Keys when loading model
