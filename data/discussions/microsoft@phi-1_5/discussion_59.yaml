!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tantanchen
conflicting_files: null
created_at: 2023-11-20 15:16:31+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07c3e6c57ae1629b5352170d11639830.svg
      fullname: Tan Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tantanchen
      type: user
    createdAt: '2023-11-20T15:16:31.000Z'
    data:
      edited: false
      editors:
      - tantanchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9463909268379211
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07c3e6c57ae1629b5352170d11639830.svg
          fullname: Tan Chen
          isHf: false
          isPro: false
          name: tantanchen
          type: user
        html: "<p>I was trying to use Open-Orca/oo-phi-1_5 and I got an error: Entry\
          \ Not Found for url: <a href=\"https://huggingface.co/microsoft/phi-1_5/resolve/main/configuration_mixformer_sequential.py\"\
          >https://huggingface.co/microsoft/phi-1_5/resolve/main/configuration_mixformer_sequential.py</a>.</p>\n\
          <p>Looking at the file history, this was deleted last week. I don't understand\
          \ the transformer wrappers very well, but it looks like there was an attempt\
          \ to improve the wrapper, but I think it broke it. I've also tried using\
          \ microsoft/phi-1_5 and during inference, it gave a strange error: The attention\
          \ mask and the pad token id were not set. As a consequence, you may observe\
          \ unexpected behavior. Please pass your input's <code>attention_mask</code>\
          \ to obtain reliable results.<br>Setting <code>pad_token_id</code> to <code>eos_token_id</code>:50256\
          \ for open-end generation.</p>\n<p>I was doing this on colab if anyone wants\
          \ to see the full notebook: <a rel=\"nofollow\" href=\"https://colab.research.google.com/drive/1o_fKb-P_2u-QwggQzzQPNaOj6-PkjVTb#scrollTo=3SGgTfikxC-z\"\
          >https://colab.research.google.com/drive/1o_fKb-P_2u-QwggQzzQPNaOj6-PkjVTb#scrollTo=3SGgTfikxC-z</a></p>\n\
          <p><span data-props=\"{&quot;user&quot;:&quot;gugarosa&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gugarosa\">@<span class=\"\
          underline\">gugarosa</span></a></span>\n\n\t</span></span> Would you mind\
          \ giving some pointers as to what is wrong?</p>\n"
        raw: "I was trying to use Open-Orca/oo-phi-1_5 and I got an error: Entry Not\
          \ Found for url: https://huggingface.co/microsoft/phi-1_5/resolve/main/configuration_mixformer_sequential.py.\r\
          \n\r\nLooking at the file history, this was deleted last week. I don't understand\
          \ the transformer wrappers very well, but it looks like there was an attempt\
          \ to improve the wrapper, but I think it broke it. I've also tried using\
          \ microsoft/phi-1_5 and during inference, it gave a strange error: The attention\
          \ mask and the pad token id were not set. As a consequence, you may observe\
          \ unexpected behavior. Please pass your input's `attention_mask` to obtain\
          \ reliable results.\r\nSetting `pad_token_id` to `eos_token_id`:50256 for\
          \ open-end generation.\r\n\r\nI was doing this on colab if anyone wants\
          \ to see the full notebook: https://colab.research.google.com/drive/1o_fKb-P_2u-QwggQzzQPNaOj6-PkjVTb#scrollTo=3SGgTfikxC-z\r\
          \n\r\n@gugarosa Would you mind giving some pointers as to what is wrong?"
        updatedAt: '2023-11-20T15:16:31.757Z'
      numEdits: 0
      reactions: []
    id: 655b784fc692310739063a40
    type: comment
  author: tantanchen
  content: "I was trying to use Open-Orca/oo-phi-1_5 and I got an error: Entry Not\
    \ Found for url: https://huggingface.co/microsoft/phi-1_5/resolve/main/configuration_mixformer_sequential.py.\r\
    \n\r\nLooking at the file history, this was deleted last week. I don't understand\
    \ the transformer wrappers very well, but it looks like there was an attempt to\
    \ improve the wrapper, but I think it broke it. I've also tried using microsoft/phi-1_5\
    \ and during inference, it gave a strange error: The attention mask and the pad\
    \ token id were not set. As a consequence, you may observe unexpected behavior.\
    \ Please pass your input's `attention_mask` to obtain reliable results.\r\nSetting\
    \ `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n\r\nI was\
    \ doing this on colab if anyone wants to see the full notebook: https://colab.research.google.com/drive/1o_fKb-P_2u-QwggQzzQPNaOj6-PkjVTb#scrollTo=3SGgTfikxC-z\r\
    \n\r\n@gugarosa Would you mind giving some pointers as to what is wrong?"
  created_at: 2023-11-20 15:16:31+00:00
  edited: false
  hidden: false
  id: 655b784fc692310739063a40
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
      fullname: Gustavo de Rosa
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: gugarosa
      type: user
    createdAt: '2023-11-21T12:35:11.000Z'
    data:
      edited: false
      editors:
      - gugarosa
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8824287056922913
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1666203761402-6157454831624da88210e627.jpeg?w=200&h=200&f=face
          fullname: Gustavo de Rosa
          isHf: false
          isPro: false
          name: gugarosa
          type: user
        html: "<p>Hello <span data-props=\"{&quot;user&quot;:&quot;tantanchen&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/tantanchen\"\
          >@<span class=\"underline\">tantanchen</span></a></span>\n\n\t</span></span>!</p>\n\
          <p>Regarding the issue with the <code>Open-Orca/oo-phi-1_5</code> model,\
          \ this looks like a problem related to the cache system. Could you please\
          \ delete <code>.cache</code> and re-download that model? We updated our\
          \ model interface, however, this only applies to <code>microsoft/phi-1_5</code>,\
          \ i.e., other repositories should have their own model file.</p>\n<p>Regarding\
          \ the <code>attention_mask</code> warning, this is expected when it is used.\
          \ Since the tokenizer we used for this model does not have a <code>pad_token_id</code>,\
          \ we have to mimic a special token and use it as the padding token when\
          \ doing batched inference/generation. In this case, it mimics the <code>eos_token_id</code>.</p>\n\
          <p>Hope this helps to clear some things up.</p>\n<p>Best regards,<br>Gustavo.</p>\n"
        raw: 'Hello @tantanchen!


          Regarding the issue with the `Open-Orca/oo-phi-1_5` model, this looks like
          a problem related to the cache system. Could you please delete `.cache`
          and re-download that model? We updated our model interface, however, this
          only applies to `microsoft/phi-1_5`, i.e., other repositories should have
          their own model file.


          Regarding the `attention_mask` warning, this is expected when it is used.
          Since the tokenizer we used for this model does not have a `pad_token_id`,
          we have to mimic a special token and use it as the padding token when doing
          batched inference/generation. In this case, it mimics the `eos_token_id`.


          Hope this helps to clear some things up.


          Best regards,

          Gustavo.'
        updatedAt: '2023-11-21T12:35:11.748Z'
      numEdits: 0
      reactions: []
    id: 655ca3ff064b7fe633e961e2
    type: comment
  author: gugarosa
  content: 'Hello @tantanchen!


    Regarding the issue with the `Open-Orca/oo-phi-1_5` model, this looks like a problem
    related to the cache system. Could you please delete `.cache` and re-download
    that model? We updated our model interface, however, this only applies to `microsoft/phi-1_5`,
    i.e., other repositories should have their own model file.


    Regarding the `attention_mask` warning, this is expected when it is used. Since
    the tokenizer we used for this model does not have a `pad_token_id`, we have to
    mimic a special token and use it as the padding token when doing batched inference/generation.
    In this case, it mimics the `eos_token_id`.


    Hope this helps to clear some things up.


    Best regards,

    Gustavo.'
  created_at: 2023-11-21 12:35:11+00:00
  edited: false
  hidden: false
  id: 655ca3ff064b7fe633e961e2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07c3e6c57ae1629b5352170d11639830.svg
      fullname: Tan Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tantanchen
      type: user
    createdAt: '2023-11-21T13:32:24.000Z'
    data:
      edited: false
      editors:
      - tantanchen
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9808915257453918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07c3e6c57ae1629b5352170d11639830.svg
          fullname: Tan Chen
          isHf: false
          isPro: false
          name: tantanchen
          type: user
        html: '<p>hrrm that doesn''t make much sense because I''m running this on
          Colab, and nothing is cached between sessions. But looks like the problem
          is on the Open-Ocra side. Thanks</p>

          '
        raw: hrrm that doesn't make much sense because I'm running this on Colab,
          and nothing is cached between sessions. But looks like the problem is on
          the Open-Ocra side. Thanks
        updatedAt: '2023-11-21T13:32:24.163Z'
      numEdits: 0
      reactions: []
      relatedEventId: 655cb16848320febfc4e3f6d
    id: 655cb16848320febfc4e3f6c
    type: comment
  author: tantanchen
  content: hrrm that doesn't make much sense because I'm running this on Colab, and
    nothing is cached between sessions. But looks like the problem is on the Open-Ocra
    side. Thanks
  created_at: 2023-11-21 13:32:24+00:00
  edited: false
  hidden: false
  id: 655cb16848320febfc4e3f6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/07c3e6c57ae1629b5352170d11639830.svg
      fullname: Tan Chen
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tantanchen
      type: user
    createdAt: '2023-11-21T13:32:24.000Z'
    data:
      status: closed
    id: 655cb16848320febfc4e3f6d
    type: status-change
  author: tantanchen
  created_at: 2023-11-21 13:32:24+00:00
  id: 655cb16848320febfc4e3f6d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 59
repo_id: microsoft/phi-1_5
repo_type: model
status: closed
target_branch: null
title: configuration_mixformer_sequential.py deleted
