!!python/object:huggingface_hub.community.DiscussionWithDetails
author: anon7463435254
conflicting_files: null
created_at: 2023-07-23 09:43:49+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
      fullname: anon7463435254
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: anon7463435254
      type: user
    createdAt: '2023-07-23T10:43:49.000Z'
    data:
      edited: false
      editors:
      - anon7463435254
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8773788213729858
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/311d0fed32d6ed4b44e3757556ef07bd.svg
          fullname: anon7463435254
          isHf: false
          isPro: false
          name: anon7463435254
          type: user
        html: '<p>I was expecting very good results with this model. Am I doing something
          wrong?</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/9wYEPb5h7DxsqRPSrlZyD.png"><img
          alt="dolphin output.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/9wYEPb5h7DxsqRPSrlZyD.png"></a></p>

          '
        raw: "I was expecting very good results with this model. Am I doing something\
          \ wrong?\r\n\r\n![dolphin output.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/9wYEPb5h7DxsqRPSrlZyD.png)\r\
          \n"
        updatedAt: '2023-07-23T10:43:49.267Z'
      numEdits: 0
      reactions: []
    id: 64bd04656999b520ed5a5426
    type: comment
  author: anon7463435254
  content: "I was expecting very good results with this model. Am I doing something\
    \ wrong?\r\n\r\n![dolphin output.PNG](https://cdn-uploads.huggingface.co/production/uploads/6448e05081c086e0a420a033/9wYEPb5h7DxsqRPSrlZyD.png)\r\
    \n"
  created_at: 2023-07-23 09:43:49+00:00
  edited: false
  hidden: false
  id: 64bd04656999b520ed5a5426
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
      fullname: Niichan Haou
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Niichanhaou
      type: user
    createdAt: '2023-07-23T11:33:14.000Z'
    data:
      edited: true
      editors:
      - Niichanhaou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659136533737183
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/90d2587a45ac080f101ef09dc404a279.svg
          fullname: Niichan Haou
          isHf: false
          isPro: false
          name: Niichanhaou
          type: user
        html: '<p>Maybe lower temperature? It should reduce the randomness at which
          the model ouputs. Also what would be interesting to know is  how it compares
          to the original LlaMa-2 Chat model?</p>

          <p>I usually don''t test models for factual replies, as I''m more interested
          in using them for creative writing and roleplay chat, but from what I''ve
          experimented LlaMa-2 chat is pretty good (except for the damn censorship
          and moralist agenda, of course). However, finetuning models sometimes seems
          to damage some of the quality of the original model, even if they release
          us from the shady agendas</p>

          '
        raw: 'Maybe lower temperature? It should reduce the randomness at which the
          model ouputs. Also what would be interesting to know is  how it compares
          to the original LlaMa-2 Chat model?


          I usually don''t test models for factual replies, as I''m more interested
          in using them for creative writing and roleplay chat, but from what I''ve
          experimented LlaMa-2 chat is pretty good (except for the damn censorship
          and moralist agenda, of course). However, finetuning models sometimes seems
          to damage some of the quality of the original model, even if they release
          us from the shady agendas'
        updatedAt: '2023-07-23T11:33:32.860Z'
      numEdits: 1
      reactions: []
    id: 64bd0ffa979949d2e21163c7
    type: comment
  author: Niichanhaou
  content: 'Maybe lower temperature? It should reduce the randomness at which the
    model ouputs. Also what would be interesting to know is  how it compares to the
    original LlaMa-2 Chat model?


    I usually don''t test models for factual replies, as I''m more interested in using
    them for creative writing and roleplay chat, but from what I''ve experimented
    LlaMa-2 chat is pretty good (except for the damn censorship and moralist agenda,
    of course). However, finetuning models sometimes seems to damage some of the quality
    of the original model, even if they release us from the shady agendas'
  created_at: 2023-07-23 10:33:14+00:00
  edited: true
  hidden: false
  id: 64bd0ffa979949d2e21163c7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/Dolphin-Llama-13B-GPTQ
repo_type: model
status: open
target_branch: null
title: Why such  a bad output?
