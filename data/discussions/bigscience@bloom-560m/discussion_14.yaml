!!python/object:huggingface_hub.community.DiscussionWithDetails
author: patrickramos
conflicting_files: null
created_at: 2022-10-08 02:53:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669900806741-628a0429edfa7a816db7e510.jpeg?w=200&h=200&f=face
      fullname: Patrick Ramos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickramos
      type: user
    createdAt: '2022-10-08T03:53:40.000Z'
    data:
      edited: false
      editors:
      - patrickramos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669900806741-628a0429edfa7a816db7e510.jpeg?w=200&h=200&f=face
          fullname: Patrick Ramos
          isHf: false
          isPro: false
          name: patrickramos
          type: user
        html: '<p>The <a rel="nofollow" href="https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#data-type">BLOOM
          training README</a> says that BLOOM was trained in bf16, and the <a href="https://huggingface.co/bigscience/bloom#speeds-sizes-times">model
          card for bigscience/bloom</a> also mentions bf16 weights, but I can''t find
          anything in this model card about the data type of the weights . I assume
          BLOOM-560M was also trained in bf16 since the model card still links to
          the same training README, but I just want to make sure. Thanks!</p>

          '
        raw: The [BLOOM training README](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#data-type)
          says that BLOOM was trained in bf16, and the [model card for bigscience/bloom](https://huggingface.co/bigscience/bloom#speeds-sizes-times)
          also mentions bf16 weights, but I can't find anything in this model card
          about the data type of the weights . I assume BLOOM-560M was also trained
          in bf16 since the model card still links to the same training README, but
          I just want to make sure. Thanks!
        updatedAt: '2022-10-08T03:53:40.725Z'
      numEdits: 0
      reactions: []
    id: 6340f4447158d417a0fe280c
    type: comment
  author: patrickramos
  content: The [BLOOM training README](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#data-type)
    says that BLOOM was trained in bf16, and the [model card for bigscience/bloom](https://huggingface.co/bigscience/bloom#speeds-sizes-times)
    also mentions bf16 weights, but I can't find anything in this model card about
    the data type of the weights . I assume BLOOM-560M was also trained in bf16 since
    the model card still links to the same training README, but I just want to make
    sure. Thanks!
  created_at: 2022-10-08 02:53:40+00:00
  edited: false
  hidden: false
  id: 6340f4447158d417a0fe280c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1b212b1f3700c727aa2c41567b21df1d.svg
      fullname: thies
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: thies
      type: user
    createdAt: '2022-10-10T15:45:02.000Z'
    data:
      edited: true
      editors:
      - thies
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1b212b1f3700c727aa2c41567b21df1d.svg
          fullname: thies
          isHf: false
          isPro: false
          name: thies
          type: user
        html: '<p>Only the 176B model is trained in BF16. The smaller models are all
          trained in FP16.<br><a rel="nofollow" href="https://github.com/bigscience-workshop/Megatron-DeepSpeed/issues/343#issuecomment-1267299209">https://github.com/bigscience-workshop/Megatron-DeepSpeed/issues/343#issuecomment-1267299209</a></p>

          '
        raw: 'Only the 176B model is trained in BF16. The smaller models are all trained
          in FP16.

          https://github.com/bigscience-workshop/Megatron-DeepSpeed/issues/343#issuecomment-1267299209'
        updatedAt: '2022-10-10T15:46:41.345Z'
      numEdits: 1
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - patrickramos
        - TimeRobber
    id: 63443dfeba3e1a1973d373af
    type: comment
  author: thies
  content: 'Only the 176B model is trained in BF16. The smaller models are all trained
    in FP16.

    https://github.com/bigscience-workshop/Megatron-DeepSpeed/issues/343#issuecomment-1267299209'
  created_at: 2022-10-10 14:45:02+00:00
  edited: true
  hidden: false
  id: 63443dfeba3e1a1973d373af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669900806741-628a0429edfa7a816db7e510.jpeg?w=200&h=200&f=face
      fullname: Patrick Ramos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickramos
      type: user
    createdAt: '2022-10-11T02:53:39.000Z'
    data:
      edited: false
      editors:
      - patrickramos
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669900806741-628a0429edfa7a816db7e510.jpeg?w=200&h=200&f=face
          fullname: Patrick Ramos
          isHf: false
          isPro: false
          name: patrickramos
          type: user
        html: '<p>Thanks!</p>

          '
        raw: Thanks!
        updatedAt: '2022-10-11T02:53:39.229Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6344dab36e988c6a79310aaa
    id: 6344dab36e988c6a79310aa9
    type: comment
  author: patrickramos
  content: Thanks!
  created_at: 2022-10-11 01:53:39+00:00
  edited: false
  hidden: false
  id: 6344dab36e988c6a79310aa9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1669900806741-628a0429edfa7a816db7e510.jpeg?w=200&h=200&f=face
      fullname: Patrick Ramos
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: patrickramos
      type: user
    createdAt: '2022-10-11T02:53:39.000Z'
    data:
      status: closed
    id: 6344dab36e988c6a79310aaa
    type: status-change
  author: patrickramos
  created_at: 2022-10-11 01:53:39+00:00
  id: 6344dab36e988c6a79310aaa
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 14
repo_id: bigscience/bloom-560m
repo_type: model
status: closed
target_branch: null
title: Is BLOOM-560M also trained in BF16?
