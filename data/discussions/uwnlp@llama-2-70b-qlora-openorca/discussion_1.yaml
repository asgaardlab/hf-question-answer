!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Alignment-Lab-AI
conflicting_files: null
created_at: 2023-08-16 22:29:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6436279eaaef013d1af225c9/31yjIFpqfdvn_n9igumIU.png?w=200&h=200&f=face
      fullname: Alignment Lab AI
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Alignment-Lab-AI
      type: user
    createdAt: '2023-08-16T23:29:53.000Z'
    data:
      edited: false
      editors:
      - Alignment-Lab-AI
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9813044667243958
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6436279eaaef013d1af225c9/31yjIFpqfdvn_n9igumIU.png?w=200&h=200&f=face
          fullname: Alignment Lab AI
          isHf: false
          isPro: false
          name: Alignment-Lab-AI
          type: user
        html: '<p>I saw this Qlora yesterday and decided to check in to see if there
          was anything we could do to contribute, especially if this is for research
          purposes. I know the paper is a bit vague in parts about how exactly they
          managed certain elements, but we''ve been analyzing it rigorously for a
          couple of months. I''m happy to gather a few of us who worked on it and
          discuss our solutions if needed.</p>

          <p>Additionally, if the goal of the Qlora was simply to generate an open
          and accessible Qlora of our dataset, I''d be very excited to discuss our
          specific optimizations that allow us to score so highly with each of our
          releases. We''ve been meaning to train one for a while now, but we wanted
          to hold off on the 70b until we had built the new version of the dataset.
          As we all come out of pocket, we want to be sure we do the best we possibly
          can when we do it. That being said, it''s open-sourced and we''re always
          happy to contribute where we can on that front. ^^</p>

          '
        raw: "I saw this Qlora yesterday and decided to check in to see if there was\
          \ anything we could do to contribute, especially if this is for research\
          \ purposes. I know the paper is a bit vague in parts about how exactly they\
          \ managed certain elements, but we've been analyzing it rigorously for a\
          \ couple of months. I'm happy to gather a few of us who worked on it and\
          \ discuss our solutions if needed.\r\n\r\nAdditionally, if the goal of the\
          \ Qlora was simply to generate an open and accessible Qlora of our dataset,\
          \ I'd be very excited to discuss our specific optimizations that allow us\
          \ to score so highly with each of our releases. We've been meaning to train\
          \ one for a while now, but we wanted to hold off on the 70b until we had\
          \ built the new version of the dataset. As we all come out of pocket, we\
          \ want to be sure we do the best we possibly can when we do it. That being\
          \ said, it's open-sourced and we're always happy to contribute where we\
          \ can on that front. ^^"
        updatedAt: '2023-08-16T23:29:53.761Z'
      numEdits: 0
      reactions: []
    id: 64dd5bf1f44d2407e6129b5d
    type: comment
  author: Alignment-Lab-AI
  content: "I saw this Qlora yesterday and decided to check in to see if there was\
    \ anything we could do to contribute, especially if this is for research purposes.\
    \ I know the paper is a bit vague in parts about how exactly they managed certain\
    \ elements, but we've been analyzing it rigorously for a couple of months. I'm\
    \ happy to gather a few of us who worked on it and discuss our solutions if needed.\r\
    \n\r\nAdditionally, if the goal of the Qlora was simply to generate an open and\
    \ accessible Qlora of our dataset, I'd be very excited to discuss our specific\
    \ optimizations that allow us to score so highly with each of our releases. We've\
    \ been meaning to train one for a while now, but we wanted to hold off on the\
    \ 70b until we had built the new version of the dataset. As we all come out of\
    \ pocket, we want to be sure we do the best we possibly can when we do it. That\
    \ being said, it's open-sourced and we're always happy to contribute where we\
    \ can on that front. ^^"
  created_at: 2023-08-16 22:29:53+00:00
  edited: false
  hidden: false
  id: 64dd5bf1f44d2407e6129b5d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: uwnlp/llama-2-70b-qlora-openorca
repo_type: model
status: open
target_branch: null
title: hello!  just checking in to see if you wanted more specific details for how
  the orca model, or our models were trained :)
