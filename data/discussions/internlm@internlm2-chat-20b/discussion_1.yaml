!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brucethemoose
conflicting_files: null
created_at: 2024-01-17 18:20:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
      fullname: brucethemoose
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brucethemoose
      type: user
    createdAt: '2024-01-17T18:20:00.000Z'
    data:
      edited: true
      editors:
      - brucethemoose
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8993587493896484
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1670003187019-noauth.png?w=200&h=200&f=face
          fullname: brucethemoose
          isHf: false
          isPro: false
          name: brucethemoose
          type: user
        html: '<p>I''m not sure how different the architecture actually is, but if
          possible, could you change the config to be llama compatible instead of
          requiring custom runtime/tokenizer code?</p>

          <p>Custom code is a huge blocker to the highly optimized llama architecture
          infrastructure the community already uses... To be blunt, I can run Yi 34B
          200K with a fraction of the resources it takes to run this 20B model, for
          the moment,  and finetune it just about as efficiently with llama focused
          frameworks.</p>

          <p>Yi itself already went through this ordeal, and "llamafied" their release
          to the benefit of everyone: <a href="https://huggingface.co/01-ai/Yi-34B/discussions/11">https://huggingface.co/01-ai/Yi-34B/discussions/11</a></p>

          '
        raw: 'I''m not sure how different the architecture actually is, but if possible,
          could you change the config to be llama compatible instead of requiring
          custom runtime/tokenizer code?


          Custom code is a huge blocker to the highly optimized llama architecture
          infrastructure the community already uses... To be blunt, I can run Yi 34B
          200K with a fraction of the resources it takes to run this 20B model, for
          the moment,  and finetune it just about as efficiently with llama focused
          frameworks.


          Yi itself already went through this ordeal, and "llamafied" their release
          to the benefit of everyone: https://huggingface.co/01-ai/Yi-34B/discussions/11


          '
        updatedAt: '2024-01-17T19:16:57.407Z'
      numEdits: 4
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - Liangmingxin
    id: 65a81a50241e1c6c4827c548
    type: comment
  author: brucethemoose
  content: 'I''m not sure how different the architecture actually is, but if possible,
    could you change the config to be llama compatible instead of requiring custom
    runtime/tokenizer code?


    Custom code is a huge blocker to the highly optimized llama architecture infrastructure
    the community already uses... To be blunt, I can run Yi 34B 200K with a fraction
    of the resources it takes to run this 20B model, for the moment,  and finetune
    it just about as efficiently with llama focused frameworks.


    Yi itself already went through this ordeal, and "llamafied" their release to the
    benefit of everyone: https://huggingface.co/01-ai/Yi-34B/discussions/11


    '
  created_at: 2024-01-17 18:20:00+00:00
  edited: true
  hidden: false
  id: 65a81a50241e1c6c4827c548
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dcc0bfce588d8769f1bd4ff8dd73c75f.svg
      fullname: Shuhao Xing
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: x54-729
      type: user
    createdAt: '2024-01-18T02:51:21.000Z'
    data:
      edited: false
      editors:
      - x54-729
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9671962857246399
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dcc0bfce588d8769f1bd4ff8dd73c75f.svg
          fullname: Shuhao Xing
          isHf: false
          isPro: false
          name: x54-729
          type: user
        html: '<p>Thank you for your suggestion. The biggest difference lies in the
          combination of Wq, Wk, Wv, we did this for training efficiency. We are planning
          to offer a script that facilitates conversion between InternLM2 and LLaMA.
          </p>

          '
        raw: "Thank you for your suggestion. The biggest difference lies in the combination\
          \ of Wq, Wk, Wv, we did this for training efficiency. We are planning to\
          \ offer a script that facilitates conversion between InternLM2 and LLaMA.\
          \ \n\n"
        updatedAt: '2024-01-18T02:51:21.757Z'
      numEdits: 0
      reactions:
      - count: 4
        reaction: "\U0001F44D"
        users:
        - brucethemoose
        - Meteonis
        - Yhyu13
        - MB7977
      - count: 3
        reaction: "\u2764\uFE0F"
        users:
        - C1rN09
        - Yhyu13
        - sbene
    id: 65a8922910342794b7f81959
    type: comment
  author: x54-729
  content: "Thank you for your suggestion. The biggest difference lies in the combination\
    \ of Wq, Wk, Wv, we did this for training efficiency. We are planning to offer\
    \ a script that facilitates conversion between InternLM2 and LLaMA. \n\n"
  created_at: 2024-01-18 02:51:21+00:00
  edited: false
  hidden: false
  id: 65a8922910342794b7f81959
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/dcc0bfce588d8769f1bd4ff8dd73c75f.svg
      fullname: Shuhao Xing
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: x54-729
      type: user
    createdAt: '2024-01-19T11:49:17.000Z'
    data:
      edited: false
      editors:
      - x54-729
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6517220735549927
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/dcc0bfce588d8769f1bd4ff8dd73c75f.svg
          fullname: Shuhao Xing
          isHf: false
          isPro: false
          name: x54-729
          type: user
        html: '<p>Please try to use script in <a rel="nofollow" href="https://github.com/InternLM/InternLM/tree/main/tools">https://github.com/InternLM/InternLM/tree/main/tools</a>
          to convert the format.</p>

          '
        raw: Please try to use script in https://github.com/InternLM/InternLM/tree/main/tools
          to convert the format.
        updatedAt: '2024-01-19T11:49:17.254Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - brucethemoose
      relatedEventId: 65aa61bd71c5d01a283928d8
    id: 65aa61bd71c5d01a283928cf
    type: comment
  author: x54-729
  content: Please try to use script in https://github.com/InternLM/InternLM/tree/main/tools
    to convert the format.
  created_at: 2024-01-19 11:49:17+00:00
  edited: false
  hidden: false
  id: 65aa61bd71c5d01a283928cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/dcc0bfce588d8769f1bd4ff8dd73c75f.svg
      fullname: Shuhao Xing
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: x54-729
      type: user
    createdAt: '2024-01-19T11:49:17.000Z'
    data:
      status: closed
    id: 65aa61bd71c5d01a283928d8
    type: status-change
  author: x54-729
  created_at: 2024-01-19 11:49:17+00:00
  id: 65aa61bd71c5d01a283928d8
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: internlm/internlm2-chat-20b
repo_type: model
status: closed
target_branch: null
title: Llama Compatibility
