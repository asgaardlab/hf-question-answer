!!python/object:huggingface_hub.community.DiscussionWithDetails
author: oneCode
conflicting_files: null
created_at: 2023-11-05 14:08:00+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
      fullname: Scholar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oneCode
      type: user
    createdAt: '2023-11-05T14:08:00.000Z'
    data:
      edited: false
      editors:
      - oneCode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9752938747406006
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
          fullname: Scholar
          isHf: false
          isPro: false
          name: oneCode
          type: user
        html: '<p>Hi there, this model is not loading. It''s not loading into the
          RAM. It says loading on LM Studio but it''s not and fails to load. Please
          advise. Thanks!!</p>

          '
        raw: Hi there, this model is not loading. It's not loading into the RAM. It
          says loading on LM Studio but it's not and fails to load. Please advise.
          Thanks!!
        updatedAt: '2023-11-05T14:08:00.420Z'
      numEdits: 0
      reactions: []
    id: 6547a1c09295970f87973812
    type: comment
  author: oneCode
  content: Hi there, this model is not loading. It's not loading into the RAM. It
    says loading on LM Studio but it's not and fails to load. Please advise. Thanks!!
  created_at: 2023-11-05 14:08:00+00:00
  edited: false
  hidden: false
  id: 6547a1c09295970f87973812
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-05T14:08:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9752613306045532
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Which quant type are you testing?</p>

          '
        raw: Which quant type are you testing?
        updatedAt: '2023-11-05T14:08:37.299Z'
      numEdits: 0
      reactions: []
    id: 6547a1e56b1695ac33caab9e
    type: comment
  author: TheBloke
  content: Which quant type are you testing?
  created_at: 2023-11-05 14:08:37+00:00
  edited: false
  hidden: false
  id: 6547a1e56b1695ac33caab9e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
      fullname: Scholar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oneCode
      type: user
    createdAt: '2023-11-05T14:16:00.000Z'
    data:
      edited: false
      editors:
      - oneCode
      hidden: false
      identifiedLanguage:
        language: zh
        probability: 0.6767526268959045
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
          fullname: Scholar
          isHf: false
          isPro: false
          name: oneCode
          type: user
        html: '<p> [Q6_K.gguf)</p>

          '
        raw: ' [Q6_K.gguf)'
        updatedAt: '2023-11-05T14:16:00.608Z'
      numEdits: 0
      reactions: []
    id: 6547a3a0dbce6bd2be16b51e
    type: comment
  author: oneCode
  content: ' [Q6_K.gguf)'
  created_at: 2023-11-05 14:16:00+00:00
  edited: false
  hidden: false
  id: 6547a3a0dbce6bd2be16b51e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
      fullname: Scholar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oneCode
      type: user
    createdAt: '2023-11-05T14:18:34.000Z'
    data:
      edited: false
      editors:
      - oneCode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9592567682266235
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
          fullname: Scholar
          isHf: false
          isPro: false
          name: oneCode
          type: user
        html: '<p>I''m going to try the base one now instead of the instruct.</p>

          '
        raw: I'm going to try the base one now instead of the instruct.
        updatedAt: '2023-11-05T14:18:34.153Z'
      numEdits: 0
      reactions: []
    id: 6547a43a470b4b8b3fef4e7a
    type: comment
  author: oneCode
  content: I'm going to try the base one now instead of the instruct.
  created_at: 2023-11-05 14:18:34+00:00
  edited: false
  hidden: false
  id: 6547a43a470b4b8b3fef4e7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-05T14:30:22.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5802919864654541
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>I just double checked the Q6_K model in llama.cpp and confirmed\
          \ it works:</p>\n<p>llm_load_tensors: ggml ctx size =    0.21 MB<br>llm_load_tensors:\
          \ using CUDA for GPU acceleration<br>llm_load_tensors: mem required  = 26087.51\
          \ MB<br>llm_load_tensors: offloading 0 repeating layers to GPU<br>llm_load_tensors:\
          \ offloaded 0/65 layers to GPU<br>llm_load_tensors: VRAM used: 0.00 MB<br>....................................................................................................<br>llama_new_context_with_model:\
          \ n_ctx      = 512<br>llama_new_context_with_model: freq_base  = 100000.0<br>llama_new_context_with_model:\
          \ freq_scale = 0.25<br>llama_new_context_with_model: kv self size  =  124.00\
          \ MB<br>llama_build_graph: non-view tensors processed: 1430/1430<br>llama_new_context_with_model:\
          \ compute buffer total size = 110.63 MB<br>llama_new_context_with_model:\
          \ VRAM scratch buffer: 104.00 MB<br>llama_new_context_with_model: total\
          \ VRAM used: 104.00 MB (model: 0.00 MB, context: 104.00 MB)</p>\n<p>system_info:\
          \ n_threads = 56 / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI =\
          \ 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA\
          \ = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |<br>sampling:<br>\
          \    repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000,\
          \ presence_penalty = 0.000<br>    top_k = 40, tfs_z = 1.000, top_p = 0.950,\
          \ min_p = 0.050, typical_p = 1.000, temp = 0.800<br>    mirostat = 0, mirostat_lr\
          \ = 0.100, mirostat_ent = 5.000<br>generate: n_ctx = 512, n_batch = 512,\
          \ n_predict = 256, n_keep = 0</p>\n<p>You are an AI programming assistant,\
          \ utilizing the Deepseek Coder model, developed by Deepseek Company, and\
          \ you only answer questions related to computer science. For politically\
          \ sensitive questions, security and privacy issues, and other non-computer\
          \ science questions, you will refuse to answer.</p>\n<h3 id=\"instruction\"\
          >Instruction:</h3>\n<p>write a quick sort algorithm in python.</p>\n<h3\
          \ id=\"response\">Response:</h3>\n<p>Sure, here is a simple implementation\
          \ of the Quick Sort algorithm in Python:</p>\n<pre><code class=\"language-python\"\
          ><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\"\
          >quicksort</span>(<span class=\"hljs-params\">arr</span>):\n    <span class=\"\
          hljs-keyword\">if</span> <span class=\"hljs-built_in\">len</span>(arr) &lt;=\
          \ <span class=\"hljs-number\">1</span>:\n        <span class=\"hljs-keyword\"\
          >return</span> arr\n    pivot = arr[<span class=\"hljs-built_in\">len</span>(arr)\
          \ // <span class=\"hljs-number\">2</span>]\n    left = [x <span class=\"\
          hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span> arr <span\
          \ class=\"hljs-keyword\">if</span> x &lt; pivot]\n    middle = [x <span\
          \ class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\">in</span>\
          \ arr <span class=\"hljs-keyword\">if</span> x == pivot]\n    right = [x\
          \ <span class=\"hljs-keyword\">for</span> x <span class=\"hljs-keyword\"\
          >in</span> arr <span class=\"hljs-keyword\">if</span> x &gt; pivot]\n  \
          \  <span class=\"hljs-keyword\">return</span> quicksort(left) + middle +\
          \ quicksort(right)\n\n<span class=\"hljs-comment\"># Test the function</span>\n\
          <span class=\"hljs-built_in\">print</span>(quicksort([<span class=\"hljs-number\"\
          >3</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\"\
          >8</span>,<span class=\"hljs-number\">10</span>,<span class=\"hljs-number\"\
          >1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\"\
          >1</span>]))\n</code></pre>\n<p>This implementation uses list comprehensions\
          \ to create a new array of elements less than the pivot, equal to the pivot,\
          \ and greater than the pivot. The <code>//</code> operator is used for integer\
          \ division (rounding down) when finding the pivot index.</p>\n<p>The <code>quicksort()</code>\
          \ function is then recursively called on the \"left\" and \"right\" arrays\
          \ until they are sorted. Finally, the \"left\", \"middle\", and \"right\"\
          \ arrays are concaten<br>llama_print_timings:        load time =   61131.05\
          \ ms<br>llama_print_timings:      sample time =     136.82 ms /   256 runs\
          \   (    0.53 ms per token,  1871.11 tokens per second)<br>llama_print_timings:\
          \ prompt eval time =    6226.46 ms /    75 tokens (   83.02 ms per token,\
          \    12.05 tokens per second)<br>llama_print_timings:        eval time =\
          \  196632.25 ms /   255 runs   (  771.11 ms per token,     1.30 tokens per\
          \ second)<br>llama_print_timings:       total time =  203109.18 ms<br>Log\
          \ end</p>\n<p>Please report the issues to LM Studio, it might be an LM Studio\
          \ specific issue.</p>\n"
        raw: "I just double checked the Q6_K model in llama.cpp and confirmed it works:\n\
          \n\nllm_load_tensors: ggml ctx size =    0.21 MB\nllm_load_tensors: using\
          \ CUDA for GPU acceleration\nllm_load_tensors: mem required  = 26087.51\
          \ MB\nllm_load_tensors: offloading 0 repeating layers to GPU\nllm_load_tensors:\
          \ offloaded 0/65 layers to GPU\nllm_load_tensors: VRAM used: 0.00 MB\n....................................................................................................\n\
          llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
          \ freq_base  = 100000.0\nllama_new_context_with_model: freq_scale = 0.25\n\
          llama_new_context_with_model: kv self size  =  124.00 MB\nllama_build_graph:\
          \ non-view tensors processed: 1430/1430\nllama_new_context_with_model: compute\
          \ buffer total size = 110.63 MB\nllama_new_context_with_model: VRAM scratch\
          \ buffer: 104.00 MB\nllama_new_context_with_model: total VRAM used: 104.00\
          \ MB (model: 0.00 MB, context: 104.00 MB)\n\nsystem_info: n_threads = 56\
          \ / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI\
          \ = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD\
          \ = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling:\n\trepeat_last_n\
          \ = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty\
          \ = 0.000\n\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p\
          \ = 1.000, temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent\
          \ = 5.000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 256, n_keep\
          \ = 0\n\n\nYou are an AI programming assistant, utilizing the Deepseek Coder\
          \ model, developed by Deepseek Company, and you only answer questions related\
          \ to computer science. For politically sensitive questions, security and\
          \ privacy issues, and other non-computer science questions, you will refuse\
          \ to answer.\n### Instruction:\nwrite a quick sort algorithm in python.\n\
          ### Response:\nSure, here is a simple implementation of the Quick Sort algorithm\
          \ in Python:\n\n```python\ndef quicksort(arr):\n    if len(arr) <= 1:\n\
          \        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x\
          \ in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n  \
          \  right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle\
          \ + quicksort(right)\n\n# Test the function\nprint(quicksort([3,6,8,10,1,2,1]))\n\
          ```\n\nThis implementation uses list comprehensions to create a new array\
          \ of elements less than the pivot, equal to the pivot, and greater than\
          \ the pivot. The `//` operator is used for integer division (rounding down)\
          \ when finding the pivot index.\n\nThe `quicksort()` function is then recursively\
          \ called on the \"left\" and \"right\" arrays until they are sorted. Finally,\
          \ the \"left\", \"middle\", and \"right\" arrays are concaten\nllama_print_timings:\
          \        load time =   61131.05 ms\nllama_print_timings:      sample time\
          \ =     136.82 ms /   256 runs   (    0.53 ms per token,  1871.11 tokens\
          \ per second)\nllama_print_timings: prompt eval time =    6226.46 ms / \
          \   75 tokens (   83.02 ms per token,    12.05 tokens per second)\nllama_print_timings:\
          \        eval time =  196632.25 ms /   255 runs   (  771.11 ms per token,\
          \     1.30 tokens per second)\nllama_print_timings:       total time = \
          \ 203109.18 ms\nLog end\n\n\nPlease report the issues to LM Studio, it might\
          \ be an LM Studio specific issue."
        updatedAt: '2023-11-05T14:30:49.312Z'
      numEdits: 1
      reactions: []
    id: 6547a6fea877ee9a50203e93
    type: comment
  author: TheBloke
  content: "I just double checked the Q6_K model in llama.cpp and confirmed it works:\n\
    \n\nllm_load_tensors: ggml ctx size =    0.21 MB\nllm_load_tensors: using CUDA\
    \ for GPU acceleration\nllm_load_tensors: mem required  = 26087.51 MB\nllm_load_tensors:\
    \ offloading 0 repeating layers to GPU\nllm_load_tensors: offloaded 0/65 layers\
    \ to GPU\nllm_load_tensors: VRAM used: 0.00 MB\n....................................................................................................\n\
    llama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model:\
    \ freq_base  = 100000.0\nllama_new_context_with_model: freq_scale = 0.25\nllama_new_context_with_model:\
    \ kv self size  =  124.00 MB\nllama_build_graph: non-view tensors processed: 1430/1430\n\
    llama_new_context_with_model: compute buffer total size = 110.63 MB\nllama_new_context_with_model:\
    \ VRAM scratch buffer: 104.00 MB\nllama_new_context_with_model: total VRAM used:\
    \ 104.00 MB (model: 0.00 MB, context: 104.00 MB)\n\nsystem_info: n_threads = 56\
    \ / 112 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1\
    \ | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0\
    \ | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |\nsampling:\n\trepeat_last_n =\
    \ 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n\
    \ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000,\
    \ temp = 0.800\n\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\ngenerate:\
    \ n_ctx = 512, n_batch = 512, n_predict = 256, n_keep = 0\n\n\nYou are an AI programming\
    \ assistant, utilizing the Deepseek Coder model, developed by Deepseek Company,\
    \ and you only answer questions related to computer science. For politically sensitive\
    \ questions, security and privacy issues, and other non-computer science questions,\
    \ you will refuse to answer.\n### Instruction:\nwrite a quick sort algorithm in\
    \ python.\n### Response:\nSure, here is a simple implementation of the Quick Sort\
    \ algorithm in Python:\n\n```python\ndef quicksort(arr):\n    if len(arr) <= 1:\n\
    \        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr\
    \ if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for\
    \ x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)\n\
    \n# Test the function\nprint(quicksort([3,6,8,10,1,2,1]))\n```\n\nThis implementation\
    \ uses list comprehensions to create a new array of elements less than the pivot,\
    \ equal to the pivot, and greater than the pivot. The `//` operator is used for\
    \ integer division (rounding down) when finding the pivot index.\n\nThe `quicksort()`\
    \ function is then recursively called on the \"left\" and \"right\" arrays until\
    \ they are sorted. Finally, the \"left\", \"middle\", and \"right\" arrays are\
    \ concaten\nllama_print_timings:        load time =   61131.05 ms\nllama_print_timings:\
    \      sample time =     136.82 ms /   256 runs   (    0.53 ms per token,  1871.11\
    \ tokens per second)\nllama_print_timings: prompt eval time =    6226.46 ms /\
    \    75 tokens (   83.02 ms per token,    12.05 tokens per second)\nllama_print_timings:\
    \        eval time =  196632.25 ms /   255 runs   (  771.11 ms per token,    \
    \ 1.30 tokens per second)\nllama_print_timings:       total time =  203109.18\
    \ ms\nLog end\n\n\nPlease report the issues to LM Studio, it might be an LM Studio\
    \ specific issue."
  created_at: 2023-11-05 14:30:22+00:00
  edited: true
  hidden: false
  id: 6547a6fea877ee9a50203e93
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
      fullname: Scholar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: oneCode
      type: user
    createdAt: '2023-11-05T14:39:53.000Z'
    data:
      edited: false
      editors:
      - oneCode
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9699572324752808
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87d481a6f3d663595d39e2820765bfd.svg
          fullname: Scholar
          isHf: false
          isPro: false
          name: oneCode
          type: user
        html: '<p>Thanks for checking. The base model is having the same issue in
          LM Studio. I was so siked to test these.. I''ll reach out to LM Studio.
          Thanks for everything you do! </p>

          '
        raw: 'Thanks for checking. The base model is having the same issue in LM Studio.
          I was so siked to test these.. I''ll reach out to LM Studio. Thanks for
          everything you do! '
        updatedAt: '2023-11-05T14:39:53.563Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - TheBloke
    id: 6547a939565e3985e8b1a76c
    type: comment
  author: oneCode
  content: 'Thanks for checking. The base model is having the same issue in LM Studio.
    I was so siked to test these.. I''ll reach out to LM Studio. Thanks for everything
    you do! '
  created_at: 2023-11-05 14:39:53+00:00
  edited: false
  hidden: false
  id: 6547a939565e3985e8b1a76c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
      fullname: "\u03C8\u03C0.com"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: PsiPi
      type: user
    createdAt: '2023-11-06T03:28:20.000Z'
    data:
      edited: false
      editors:
      - PsiPi
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7004157304763794
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/HqUwLKO5rKA-6YilGoBwk.png?w=200&h=200&f=face
          fullname: "\u03C8\u03C0.com"
          isHf: false
          isPro: false
          name: PsiPi
          type: user
        html: '<p>confirming this fails in lmstudio and oogabooga webui</p>

          '
        raw: confirming this fails in lmstudio and oogabooga webui
        updatedAt: '2023-11-06T03:28:20.150Z'
      numEdits: 0
      reactions:
      - count: 9
        reaction: "\U0001F44D"
        users:
        - snoosnoo
        - mufeed
        - locutus2999
        - CronoBJS
        - bob90809
        - Tom-Neverwinter
        - Psykeus
        - Seroga88i
        - iRhonin
    id: 65485d54b6a8bd2c907fb943
    type: comment
  author: PsiPi
  content: confirming this fails in lmstudio and oogabooga webui
  created_at: 2023-11-06 03:28:20+00:00
  edited: false
  hidden: false
  id: 65485d54b6a8bd2c907fb943
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-11-06T20:27:22.000Z'
    data:
      edited: false
      editors:
      - Stilgar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8378984928131104
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
          fullname: Choraly
          isHf: false
          isPro: false
          name: Stilgar
          type: user
        html: '<p>Yes it fails on oobagooba llama.ccp (latest update) -&gt; issue
          is "AttributeError: ''LlamaCppModel'' object has no attribute ''model''
          " , other gguf are working without issue.<br>Can be loaded with ctransfromers
          but then fail to generate answer on "question/answer" instruct.<br>I''m
          uising Q6_K on 4090<br>Error seems to be related to "ERROR: byte not found
          in vocab: '' '' " got the same error when loading with ctransformers</p>

          '
        raw: "Yes it fails on oobagooba llama.ccp (latest update) -> issue is \"AttributeError:\
          \ 'LlamaCppModel' object has no attribute 'model' \" , other gguf are working\
          \ without issue. \nCan be loaded with ctransfromers but then fail to generate\
          \ answer on \"question/answer\" instruct.\nI'm uising Q6_K on 4090\nError\
          \ seems to be related to \"ERROR: byte not found in vocab: ' ' \" got the\
          \ same error when loading with ctransformers"
        updatedAt: '2023-11-06T20:27:22.633Z'
      numEdits: 0
      reactions: []
    id: 65494c2ac01e5a4e88892e46
    type: comment
  author: Stilgar
  content: "Yes it fails on oobagooba llama.ccp (latest update) -> issue is \"AttributeError:\
    \ 'LlamaCppModel' object has no attribute 'model' \" , other gguf are working\
    \ without issue. \nCan be loaded with ctransfromers but then fail to generate\
    \ answer on \"question/answer\" instruct.\nI'm uising Q6_K on 4090\nError seems\
    \ to be related to \"ERROR: byte not found in vocab: ' ' \" got the same error\
    \ when loading with ctransformers"
  created_at: 2023-11-06 20:27:22+00:00
  edited: false
  hidden: false
  id: 65494c2ac01e5a4e88892e46
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-11-06T20:46:31.000Z'
    data:
      edited: false
      editors:
      - Stilgar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.46297556161880493
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
          fullname: Choraly
          isHf: false
          isPro: false
          name: Stilgar
          type: user
        html: "<p>Got same error on llamaSharp (llama.cpp used with C# interop) latest\
          \ git.</p>\n<p>llm_load_print_meta: model ftype    = mostly Q6_K<br>llm_load_print_meta:\
          \ model size     = 33.34 B<br>llm_load_print_meta: general.name   = deepseek-ai_deepseek-coder-33b-instruct<br>llm_load_print_meta:\
          \ BOS token = 32013 '&lt;\xB4\xA2\xA3begin\xD4\xFB\xFCof\xD4\xFB\xFCsentence\xB4\
          \xA2\xA3&gt;'<br>llm_load_print_meta: EOS token = 32021 '&lt;|EOT|&gt;'<br>llm_load_print_meta:\
          \ PAD token = 32014 '&lt;\xB4\xA2\xA3end\xD4\xFB\xFCof\xD4\xFB\xFCsentence\xB4\
          \xA2\xA3&gt;'<br>llm_load_print_meta: LF token  = 0 '!'<br>llm_load_tensors:\
          \ ggml ctx size =    0.17 MB<br>llm_load_tensors: using CUDA for GPU acceleration<br>llm_load_tensors:\
          \ mem required  = 13639.64 MB (+  992.00 MB per state)<br>llm_load_tensors:\
          \ offloading 30 repeating layers to GPU<br>llm_load_tensors: offloaded 30/65\
          \ layers to GPU<br>llm_load_tensors: VRAM used: 12448 MB<br>....................................................................................................<br>llama_new_context_with_model:\
          \ kv self size  =  992.00 MB<br>llama_new_context_with_model: compute buffer\
          \ total size =  491.41 MB<br>llama_new_context_with_model: VRAM scratch\
          \ buffer: 490.00 MB</p>\n<p>You are an AI programming assistant, utilizing\
          \ the Deepseek Coder model, developed by Deepseek Company, and you only\
          \ answer questions related to computer science. For politically sensitive\
          \ questions, security and privacy issues, and other non-computer science\
          \ questions, you will refuse to answer..</p>\n<p>Instruction:ERROR: byte\
          \ not found in vocab: ' '<br>ERROR: byte not found in vocab: ' '<br>ERROR:\
          \ byte not found in vocab: ' '<br>ERROR: byte not found in vocab: ' '<br>ERROR:\
          \ byte not found in vocab: ' '<br>ERROR: byte not found in vocab: ' '</p>\n"
        raw: "Got same error on llamaSharp (llama.cpp used with C# interop) latest\
          \ git.\n\nllm_load_print_meta: model ftype    = mostly Q6_K\nllm_load_print_meta:\
          \ model size     = 33.34 B\nllm_load_print_meta: general.name   = deepseek-ai_deepseek-coder-33b-instruct\n\
          llm_load_print_meta: BOS token = 32013 '<\xB4\xA2\xA3begin\xD4\xFB\xFCof\xD4\
          \xFB\xFCsentence\xB4\xA2\xA3>'\nllm_load_print_meta: EOS token = 32021 '<|EOT|>'\n\
          llm_load_print_meta: PAD token = 32014 '<\xB4\xA2\xA3end\xD4\xFB\xFCof\xD4\
          \xFB\xFCsentence\xB4\xA2\xA3>'\nllm_load_print_meta: LF token  = 0 '!'\n\
          llm_load_tensors: ggml ctx size =    0.17 MB\nllm_load_tensors: using CUDA\
          \ for GPU acceleration\nllm_load_tensors: mem required  = 13639.64 MB (+\
          \  992.00 MB per state)\nllm_load_tensors: offloading 30 repeating layers\
          \ to GPU\nllm_load_tensors: offloaded 30/65 layers to GPU\nllm_load_tensors:\
          \ VRAM used: 12448 MB\n....................................................................................................\n\
          llama_new_context_with_model: kv self size  =  992.00 MB\nllama_new_context_with_model:\
          \ compute buffer total size =  491.41 MB\nllama_new_context_with_model:\
          \ VRAM scratch buffer: 490.00 MB\n\nYou are an AI programming assistant,\
          \ utilizing the Deepseek Coder model, developed by Deepseek Company, and\
          \ you only answer questions related to computer science. For politically\
          \ sensitive questions, security and privacy issues, and other non-computer\
          \ science questions, you will refuse to answer..\n\nInstruction:ERROR: byte\
          \ not found in vocab: ' '\nERROR: byte not found in vocab: ' '\nERROR: byte\
          \ not found in vocab: ' '\nERROR: byte not found in vocab: ' '\nERROR: byte\
          \ not found in vocab: ' '\nERROR: byte not found in vocab: ' '"
        updatedAt: '2023-11-06T20:46:31.128Z'
      numEdits: 0
      reactions: []
    id: 654950a7137b7246f0278f8f
    type: comment
  author: Stilgar
  content: "Got same error on llamaSharp (llama.cpp used with C# interop) latest git.\n\
    \nllm_load_print_meta: model ftype    = mostly Q6_K\nllm_load_print_meta: model\
    \ size     = 33.34 B\nllm_load_print_meta: general.name   = deepseek-ai_deepseek-coder-33b-instruct\n\
    llm_load_print_meta: BOS token = 32013 '<\xB4\xA2\xA3begin\xD4\xFB\xFCof\xD4\xFB\
    \xFCsentence\xB4\xA2\xA3>'\nllm_load_print_meta: EOS token = 32021 '<|EOT|>'\n\
    llm_load_print_meta: PAD token = 32014 '<\xB4\xA2\xA3end\xD4\xFB\xFCof\xD4\xFB\
    \xFCsentence\xB4\xA2\xA3>'\nllm_load_print_meta: LF token  = 0 '!'\nllm_load_tensors:\
    \ ggml ctx size =    0.17 MB\nllm_load_tensors: using CUDA for GPU acceleration\n\
    llm_load_tensors: mem required  = 13639.64 MB (+  992.00 MB per state)\nllm_load_tensors:\
    \ offloading 30 repeating layers to GPU\nllm_load_tensors: offloaded 30/65 layers\
    \ to GPU\nllm_load_tensors: VRAM used: 12448 MB\n....................................................................................................\n\
    llama_new_context_with_model: kv self size  =  992.00 MB\nllama_new_context_with_model:\
    \ compute buffer total size =  491.41 MB\nllama_new_context_with_model: VRAM scratch\
    \ buffer: 490.00 MB\n\nYou are an AI programming assistant, utilizing the Deepseek\
    \ Coder model, developed by Deepseek Company, and you only answer questions related\
    \ to computer science. For politically sensitive questions, security and privacy\
    \ issues, and other non-computer science questions, you will refuse to answer..\n\
    \nInstruction:ERROR: byte not found in vocab: ' '\nERROR: byte not found in vocab:\
    \ ' '\nERROR: byte not found in vocab: ' '\nERROR: byte not found in vocab: '\
    \ '\nERROR: byte not found in vocab: ' '\nERROR: byte not found in vocab: ' '"
  created_at: 2023-11-06 20:46:31+00:00
  edited: false
  hidden: false
  id: 654950a7137b7246f0278f8f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-11-06T22:30:20.000Z'
    data:
      edited: true
      editors:
      - yehiaserag
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3259325325489044
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: '<p>Also failing to start on text-generation-webui using:<br><code>python
          server.py --model "deepseek-coder-33b-instruct.Q8_0.gguf" --threads 24 --n-gpu-layers
          2 --n_ctx 16384 --listen --listen-port=8888</code><br>I''m getting <code>ERROR:
          byte not found in vocab:</code> then <code>Segmentation fault</code></p>

          <p>Update 1:<br>Also updated llama-cpp-python to latest version<br><code>pip
          uninstall -y llama-cpp-python &amp;&gt;/dev/null CMAKE_ARGS="-DLLAMA_CUBLAS=on"
          FORCE_CMAKE=1 pip install llama-cpp-python==0.2.14 --no-cache-dir &amp;&gt;/dev/null</code><br>same
          error.</p>

          <p>Update 2:<br>Tried to load using cpu and it started loading but gave
          some warning while loading <code>llm_load_vocab: mismatch in special tokens
          definition ( 243/32256 vs 237/32256 ).</code></p>

          '
        raw: 'Also failing to start on text-generation-webui using:

          `python server.py --model "deepseek-coder-33b-instruct.Q8_0.gguf" --threads
          24 --n-gpu-layers 2 --n_ctx 16384 --listen --listen-port=8888`

          I''m getting `ERROR: byte not found in vocab:` then `Segmentation fault`


          Update 1:

          Also updated llama-cpp-python to latest version

          `pip uninstall -y llama-cpp-python &>/dev/null

          CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.14
          --no-cache-dir &>/dev/null`

          same error.


          Update 2:

          Tried to load using cpu and it started loading but gave some warning while
          loading `llm_load_vocab: mismatch in special tokens definition ( 243/32256
          vs 237/32256 ).`'
        updatedAt: '2023-11-06T23:39:00.334Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mufeed
    id: 654968fcc01e5a4e8890b4f1
    type: comment
  author: yehiaserag
  content: 'Also failing to start on text-generation-webui using:

    `python server.py --model "deepseek-coder-33b-instruct.Q8_0.gguf" --threads 24
    --n-gpu-layers 2 --n_ctx 16384 --listen --listen-port=8888`

    I''m getting `ERROR: byte not found in vocab:` then `Segmentation fault`


    Update 1:

    Also updated llama-cpp-python to latest version

    `pip uninstall -y llama-cpp-python &>/dev/null

    CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.14
    --no-cache-dir &>/dev/null`

    same error.


    Update 2:

    Tried to load using cpu and it started loading but gave some warning while loading
    `llm_load_vocab: mismatch in special tokens definition ( 243/32256 vs 237/32256
    ).`'
  created_at: 2023-11-06 22:30:20+00:00
  edited: true
  hidden: false
  id: 654968fcc01e5a4e8890b4f1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cd6834e51cb4e481971685794ebfcb.svg
      fullname: Gunit Kah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gtkunit
      type: user
    createdAt: '2023-11-07T06:37:45.000Z'
    data:
      edited: false
      editors:
      - gtkunit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8934981226921082
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cd6834e51cb4e481971685794ebfcb.svg
          fullname: Gunit Kah
          isHf: false
          isPro: false
          name: gtkunit
          type: user
        html: '<p>I''m not sure if this is valuable additional information, but the
          same thing happens for Q4 K_M on ooba.</p>

          '
        raw: I'm not sure if this is valuable additional information, but the same
          thing happens for Q4 K_M on ooba.
        updatedAt: '2023-11-07T06:37:45.256Z'
      numEdits: 0
      reactions: []
    id: 6549db393225336726e4ecd3
    type: comment
  author: gtkunit
  content: I'm not sure if this is valuable additional information, but the same thing
    happens for Q4 K_M on ooba.
  created_at: 2023-11-07 06:37:45+00:00
  edited: false
  hidden: false
  id: 6549db393225336726e4ecd3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-07T09:35:09.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9261844754219055
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: "<p>Unfortunately these GGUFs are currently only supported by llama.cpp</p>\n\
          <p>EDIT: unless the llama-cpp-python release yesterday added support?  If\
          \ it did, based on what <span data-props=\"{&quot;user&quot;:&quot;yehiaserag&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/yehiaserag\"\
          >@<span class=\"underline\">yehiaserag</span></a></span>\n\n\t</span></span>\
          \ said, then support should come to text-generation-webui very soon, as\
          \ it uses that for GGUF model support.</p>\n<p>Original message:<br>Downstream\
          \ clients like text-generation-webui, GPT4All, llama-cpp-python, and others,\
          \ have not yet implemented support for BPE vocabulary, which is required\
          \ for this model and CausalLM.  The DeepSeek Coder models did not provide\
          \ a tokenizer.model file, so I had to convert them using the HF Vocab tokenizer.json,\
          \ and this results in a different vocab format.</p>\n<p>Hopefully these\
          \ other clients will add BPE support soon and then they'll work.</p>\n<p>In\
          \ the meantime, either use llama.cpp directly on the command line or using\
          \ its server mode, or, if you can, try the AWQ or GPTQs I've made of DeepSeek.\
          \ Otherwise you'll need to use another model for now, until support is added.\
          \  Nothing I can do I'm afraid.</p>\n"
        raw: 'Unfortunately these GGUFs are currently only supported by llama.cpp


          EDIT: unless the llama-cpp-python release yesterday added support?  If it
          did, based on what @yehiaserag said, then support should come to text-generation-webui
          very soon, as it uses that for GGUF model support.


          Original message:

          Downstream clients like text-generation-webui, GPT4All, llama-cpp-python,
          and others, have not yet implemented support for BPE vocabulary, which is
          required for this model and CausalLM.  The DeepSeek Coder models did not
          provide a tokenizer.model file, so I had to convert them using the HF Vocab
          tokenizer.json, and this results in a different vocab format.


          Hopefully these other clients will add BPE support soon and then they''ll
          work.


          In the meantime, either use llama.cpp directly on the command line or using
          its server mode, or, if you can, try the AWQ or GPTQs I''ve made of DeepSeek.
          Otherwise you''ll need to use another model for now, until support is added.  Nothing
          I can do I''m afraid.'
        updatedAt: '2023-11-07T09:38:00.349Z'
      numEdits: 1
      reactions:
      - count: 3
        reaction: "\U0001F91D"
        users:
        - elobos
        - Tom-Neverwinter
        - Tom4ai
      - count: 2
        reaction: "\U0001F44D"
        users:
        - zoigo
        - jmtl
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - PsiPi
    id: 654a04cd3766361a20cb19e4
    type: comment
  author: TheBloke
  content: 'Unfortunately these GGUFs are currently only supported by llama.cpp


    EDIT: unless the llama-cpp-python release yesterday added support?  If it did,
    based on what @yehiaserag said, then support should come to text-generation-webui
    very soon, as it uses that for GGUF model support.


    Original message:

    Downstream clients like text-generation-webui, GPT4All, llama-cpp-python, and
    others, have not yet implemented support for BPE vocabulary, which is required
    for this model and CausalLM.  The DeepSeek Coder models did not provide a tokenizer.model
    file, so I had to convert them using the HF Vocab tokenizer.json, and this results
    in a different vocab format.


    Hopefully these other clients will add BPE support soon and then they''ll work.


    In the meantime, either use llama.cpp directly on the command line or using its
    server mode, or, if you can, try the AWQ or GPTQs I''ve made of DeepSeek. Otherwise
    you''ll need to use another model for now, until support is added.  Nothing I
    can do I''m afraid.'
  created_at: 2023-11-07 09:35:09+00:00
  edited: true
  hidden: false
  id: 654a04cd3766361a20cb19e4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-07T09:35:39.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8379018306732178
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Tried to load using cpu and it started loading but gave some warning
          while loading llm_load_vocab: mismatch in special tokens definition ( 243/32256
          vs 237/32256 ).</p>

          </blockquote>

          <p>This is not an error, just an info message which can be ignored. The
          same message is printed by llama.cpp and it has no impact that I''ve noticed.</p>

          '
        raw: '> Tried to load using cpu and it started loading but gave some warning
          while loading llm_load_vocab: mismatch in special tokens definition ( 243/32256
          vs 237/32256 ).


          This is not an error, just an info message which can be ignored. The same
          message is printed by llama.cpp and it has no impact that I''ve noticed.'
        updatedAt: '2023-11-07T09:38:35.857Z'
      numEdits: 1
      reactions: []
    id: 654a04eb8fde27109bda19c1
    type: comment
  author: TheBloke
  content: '> Tried to load using cpu and it started loading but gave some warning
    while loading llm_load_vocab: mismatch in special tokens definition ( 243/32256
    vs 237/32256 ).


    This is not an error, just an info message which can be ignored. The same message
    is printed by llama.cpp and it has no impact that I''ve noticed.'
  created_at: 2023-11-07 09:35:39+00:00
  edited: true
  hidden: false
  id: 654a04eb8fde27109bda19c1
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f73e7187e9c0a1a9d99ed7250c66408e.svg
      fullname: Gee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gmacgmac
      type: user
    createdAt: '2023-11-07T09:52:22.000Z'
    data:
      edited: false
      editors:
      - gmacgmac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8501255512237549
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f73e7187e9c0a1a9d99ed7250c66408e.svg
          fullname: Gee
          isHf: false
          isPro: false
          name: gmacgmac
          type: user
        html: "<p>FYI This worked for me on Mac but only in CPU mode - didn't get\
          \ the vocabulary error<br>So appears something changed between 0.2.13 and\
          \ now?</p>\n<pre><code class=\"language-bash\">pip uninstall llama-cpp-python\
          \ -y\nCMAKE_ARGS=<span class=\"hljs-string\">\"-DLLAMA_METAL=on\"</span>\
          \ pip install llama-cpp-python==0.2.13 --no-cache-dir\npip install <span\
          \ class=\"hljs-string\">'llama-cpp-python[server]==0.2.13'</span>\n</code></pre>\n\
          <p>I've not tested the NEWEST version mentioned by <span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/TheBloke\"\
          >@<span class=\"underline\">TheBloke</span></a></span>\n\n\t</span></span>\
          \ </p>\n"
        raw: "FYI This worked for me on Mac but only in CPU mode - didn't get the\
          \ vocabulary error \nSo appears something changed between 0.2.13 and now?\n\
          \n```bash\npip uninstall llama-cpp-python -y\nCMAKE_ARGS=\"-DLLAMA_METAL=on\"\
          \ pip install llama-cpp-python==0.2.13 --no-cache-dir\npip install 'llama-cpp-python[server]==0.2.13'\n\
          ```\n\nI've not tested the NEWEST version mentioned by @TheBloke "
        updatedAt: '2023-11-07T09:52:22.086Z'
      numEdits: 0
      reactions: []
    id: 654a08d6c26b9e805df0645d
    type: comment
  author: gmacgmac
  content: "FYI This worked for me on Mac but only in CPU mode - didn't get the vocabulary\
    \ error \nSo appears something changed between 0.2.13 and now?\n\n```bash\npip\
    \ uninstall llama-cpp-python -y\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install llama-cpp-python==0.2.13\
    \ --no-cache-dir\npip install 'llama-cpp-python[server]==0.2.13'\n```\n\nI've\
    \ not tested the NEWEST version mentioned by @TheBloke "
  created_at: 2023-11-07 09:52:22+00:00
  edited: false
  hidden: false
  id: 654a08d6c26b9e805df0645d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f73e7187e9c0a1a9d99ed7250c66408e.svg
      fullname: Gee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gmacgmac
      type: user
    createdAt: '2023-11-07T09:59:24.000Z'
    data:
      edited: true
      editors:
      - gmacgmac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9363119602203369
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f73e7187e9c0a1a9d99ed7250c66408e.svg
          fullname: Gee
          isHf: false
          isPro: false
          name: gmacgmac
          type: user
        html: '<p>Actually just tested the latest llama-cpp-python and it''s now working
          with GPU in Oobabooga on Mac M2 Max<br>Running the 8 bit quantization</p>

          <p>Name: llama_cpp_python<br>Version: 0.2.14</p>

          '
        raw: 'Actually just tested the latest llama-cpp-python and it''s now working
          with GPU in Oobabooga on Mac M2 Max

          Running the 8 bit quantization


          Name: llama_cpp_python

          Version: 0.2.14'
        updatedAt: '2023-11-07T10:00:42.803Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - mufeed
    id: 654a0a7c1e5610f28be34f3e
    type: comment
  author: gmacgmac
  content: 'Actually just tested the latest llama-cpp-python and it''s now working
    with GPU in Oobabooga on Mac M2 Max

    Running the 8 bit quantization


    Name: llama_cpp_python

    Version: 0.2.14'
  created_at: 2023-11-07 09:59:24+00:00
  edited: true
  hidden: false
  id: 654a0a7c1e5610f28be34f3e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-07T10:08:29.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9408519864082336
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Great to hear!</p>

          '
        raw: Great to hear!
        updatedAt: '2023-11-07T10:08:29.259Z'
      numEdits: 0
      reactions: []
    id: 654a0c9d46df37dea5c3018a
    type: comment
  author: TheBloke
  content: Great to hear!
  created_at: 2023-11-07 10:08:29+00:00
  edited: false
  hidden: false
  id: 654a0c9d46df37dea5c3018a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fb3c2872b01b4f4fb9672a166d92dc2.svg
      fullname: Mufeed Al-Hashim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mufeed
      type: user
    createdAt: '2023-11-07T18:04:27.000Z'
    data:
      edited: true
      editors:
      - mufeed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8695687651634216
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fb3c2872b01b4f4fb9672a166d92dc2.svg
          fullname: Mufeed Al-Hashim
          isHf: false
          isPro: false
          name: mufeed
          type: user
        html: '<p>Upgrade llama_cpp_python to 0.2.14, and now it is working with CPU
          in Oobabooga on Linux:</p>

          <p>./cmd_linux.sh<br>pip install llama-cpp-python --force-reinstall --upgrade
          --no-cache-dir</p>

          '
        raw: 'Upgrade llama_cpp_python to 0.2.14, and now it is working with CPU in
          Oobabooga on Linux:


          ./cmd_linux.sh

          pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir'
        updatedAt: '2023-11-07T18:48:32.836Z'
      numEdits: 3
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - gtkunit
    id: 654a7c2b32d67f12f8725d75
    type: comment
  author: mufeed
  content: 'Upgrade llama_cpp_python to 0.2.14, and now it is working with CPU in
    Oobabooga on Linux:


    ./cmd_linux.sh

    pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir'
  created_at: 2023-11-07 18:04:27+00:00
  edited: true
  hidden: false
  id: 654a7c2b32d67f12f8725d75
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-11-07T18:36:19.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7567893266677856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: "<blockquote>\n<p>Actually just tested the latest llama-cpp-python and\
          \ it's now working with GPU in Oobabooga on Mac M2 Max<br>Running the 8\
          \ bit quantization</p>\n<p>Name: llama_cpp_python<br>Version: 0.2.14</p>\n\
          </blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;gmacgmac&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gmacgmac\"\
          >@<span class=\"underline\">gmacgmac</span></a></span>\n\n\t</span></span><br>How?\
          \ I'm still only able to run on cpu but not gpu using latest llama-cpp-python=0.2.14</p>\n"
        raw: "> Actually just tested the latest llama-cpp-python and it's now working\
          \ with GPU in Oobabooga on Mac M2 Max\n> Running the 8 bit quantization\n\
          > \n> Name: llama_cpp_python\n> Version: 0.2.14\n\n@gmacgmac \nHow? I'm\
          \ still only able to run on cpu but not gpu using latest llama-cpp-python=0.2.14"
        updatedAt: '2023-11-07T18:36:19.991Z'
      numEdits: 0
      reactions: []
    id: 654a83a37f679f0f698c5203
    type: comment
  author: yehiaserag
  content: "> Actually just tested the latest llama-cpp-python and it's now working\
    \ with GPU in Oobabooga on Mac M2 Max\n> Running the 8 bit quantization\n> \n\
    > Name: llama_cpp_python\n> Version: 0.2.14\n\n@gmacgmac \nHow? I'm still only\
    \ able to run on cpu but not gpu using latest llama-cpp-python=0.2.14"
  created_at: 2023-11-07 18:36:19+00:00
  edited: false
  hidden: false
  id: 654a83a37f679f0f698c5203
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/78ec9ab3aecbd1de42ce2a8dbebf46e0.svg
      fullname: "Christian Dur\xE1n C"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: NotTooSpooky
      type: user
    createdAt: '2023-11-17T17:59:22.000Z'
    data:
      edited: true
      editors:
      - NotTooSpooky
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7713995575904846
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/78ec9ab3aecbd1de42ce2a8dbebf46e0.svg
          fullname: "Christian Dur\xE1n C"
          isHf: false
          isPro: false
          name: NotTooSpooky
          type: user
        html: '<blockquote>

          <p>Upgrade llama_cpp_python to 0.2.14, and now it is working with CPU in
          Oobabooga on Linux:</p>

          <p>./cmd_linux.sh<br>pip install llama-cpp-python --force-reinstall --upgrade
          --no-cache-dir</p>

          </blockquote>

          <p>This installed 0.2.18 for me, and the "ERROR: byte not found in vocab:
          '' ''" issue still happens to me for deepseek-coder-6.7b-instruct.Q8_0.gguf
          for the GPU, but loads correctly with the CPU.</p>

          '
        raw: "> Upgrade llama_cpp_python to 0.2.14, and now it is working with CPU\
          \ in Oobabooga on Linux:\n> \n> ./cmd_linux.sh\n> pip install llama-cpp-python\
          \ --force-reinstall --upgrade --no-cache-dir\n\nThis installed 0.2.18 for\
          \ me, and the \"ERROR: byte not found in vocab: ' '\" issue still happens\
          \ to me for deepseek-coder-6.7b-instruct.Q8_0.gguf for the GPU, but loads\
          \ correctly with the CPU."
        updatedAt: '2023-11-17T17:59:41.612Z'
      numEdits: 1
      reactions: []
    id: 6557a9fa01aed573e7a9366a
    type: comment
  author: NotTooSpooky
  content: "> Upgrade llama_cpp_python to 0.2.14, and now it is working with CPU in\
    \ Oobabooga on Linux:\n> \n> ./cmd_linux.sh\n> pip install llama-cpp-python --force-reinstall\
    \ --upgrade --no-cache-dir\n\nThis installed 0.2.18 for me, and the \"ERROR: byte\
    \ not found in vocab: ' '\" issue still happens to me for deepseek-coder-6.7b-instruct.Q8_0.gguf\
    \ for the GPU, but loads correctly with the CPU."
  created_at: 2023-11-17 17:59:22+00:00
  edited: true
  hidden: false
  id: 6557a9fa01aed573e7a9366a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f73e7187e9c0a1a9d99ed7250c66408e.svg
      fullname: Gee
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gmacgmac
      type: user
    createdAt: '2023-11-17T21:51:26.000Z'
    data:
      edited: false
      editors:
      - gmacgmac
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9665414094924927
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f73e7187e9c0a1a9d99ed7250c66408e.svg
          fullname: Gee
          isHf: false
          isPro: false
          name: gmacgmac
          type: user
        html: '<p>that "deepseek-coder-6.7b-instruct.Q8_0.gguf" in my experience was
          very strange behaving.<br>i would get responses that would continue with
          unpredictable behaviour, adding characters, emojis, etc.. i spent a lot
          of time on it and the prompt template however when I tried other quanitsations
          like 6 then it was fine - so something weird going on</p>

          '
        raw: 'that "deepseek-coder-6.7b-instruct.Q8_0.gguf" in my experience was very
          strange behaving.

          i would get responses that would continue with unpredictable behaviour,
          adding characters, emojis, etc.. i spent a lot of time on it and the prompt
          template however when I tried other quanitsations like 6 then it was fine
          - so something weird going on'
        updatedAt: '2023-11-17T21:51:26.846Z'
      numEdits: 0
      reactions: []
    id: 6557e05e3fc799847437c0ff
    type: comment
  author: gmacgmac
  content: 'that "deepseek-coder-6.7b-instruct.Q8_0.gguf" in my experience was very
    strange behaving.

    i would get responses that would continue with unpredictable behaviour, adding
    characters, emojis, etc.. i spent a lot of time on it and the prompt template
    however when I tried other quanitsations like 6 then it was fine - so something
    weird going on'
  created_at: 2023-11-17 21:51:26+00:00
  edited: false
  hidden: false
  id: 6557e05e3fc799847437c0ff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-11-17T21:54:52.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8917841911315918
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;NotTooSpooky&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/NotTooSpooky\"\
          >@<span class=\"underline\">NotTooSpooky</span></a></span>\n\n\t</span></span>\
          \ same is happening with me</p>\n"
        raw: '@NotTooSpooky same is happening with me'
        updatedAt: '2023-11-17T21:54:52.055Z'
      numEdits: 0
      reactions: []
    id: 6557e12ce0a6202d36181878
    type: comment
  author: yehiaserag
  content: '@NotTooSpooky same is happening with me'
  created_at: 2023-11-17 21:54:52+00:00
  edited: false
  hidden: false
  id: 6557e12ce0a6202d36181878
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/19265f22f390e1b09e99d1e0fcaf4d28.svg
      fullname: gfhjghjghjgj
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ghjghjghj
      type: user
    createdAt: '2023-11-17T22:23:27.000Z'
    data:
      edited: false
      editors:
      - ghjghjghj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.753730297088623
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/19265f22f390e1b09e99d1e0fcaf4d28.svg
          fullname: gfhjghjghjgj
          isHf: false
          isPro: false
          name: ghjghjghj
          type: user
        html: '<p>If you want to upgrade to llama-cpp-python=0.2.14 and get this error:<br><code>ERROR:
          Could not build wheels for llama-cpp-python, which is required to install
          pyproject.toml-based projects</code>,<br>then you can replace all 0.2.11
          with 0.2.14 in <code>requirements.txt</code> then  <code>pip install -r
          requirements.txt</code> again.<br>It''s best first to uninstall the old
          version <code>pip uninstall llama-cpp-python -y</code></p>

          '
        raw: "If you want to upgrade to llama-cpp-python=0.2.14 and get this error:\n\
          `ERROR: Could not build wheels for llama-cpp-python, which is required to\
          \ install pyproject.toml-based projects`, \nthen you can replace all 0.2.11\
          \ with 0.2.14 in `requirements.txt` then  `pip install -r requirements.txt`\
          \ again.\nIt's best first to uninstall the old version `pip uninstall llama-cpp-python\
          \ -y`\n"
        updatedAt: '2023-11-17T22:23:27.849Z'
      numEdits: 0
      reactions: []
    id: 6557e7df57f3e9ac0768ebff
    type: comment
  author: ghjghjghj
  content: "If you want to upgrade to llama-cpp-python=0.2.14 and get this error:\n\
    `ERROR: Could not build wheels for llama-cpp-python, which is required to install\
    \ pyproject.toml-based projects`, \nthen you can replace all 0.2.11 with 0.2.14\
    \ in `requirements.txt` then  `pip install -r requirements.txt` again.\nIt's best\
    \ first to uninstall the old version `pip uninstall llama-cpp-python -y`\n"
  created_at: 2023-11-17 22:23:27+00:00
  edited: false
  hidden: false
  id: 6557e7df57f3e9ac0768ebff
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8fb3c2872b01b4f4fb9672a166d92dc2.svg
      fullname: Mufeed Al-Hashim
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: mufeed
      type: user
    createdAt: '2023-11-18T01:11:49.000Z'
    data:
      edited: true
      editors:
      - mufeed
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7681487202644348
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8fb3c2872b01b4f4fb9672a166d92dc2.svg
          fullname: Mufeed Al-Hashim
          isHf: false
          isPro: false
          name: mufeed
          type: user
        html: '<blockquote>

          <p>If you want to upgrade to llama-cpp-python=0.2.14 and get this error:<br><code>ERROR:
          Could not build wheels for llama-cpp-python, which is required to install
          pyproject.toml-based projects</code>,<br>then you can replace all 0.2.11
          with 0.2.14 in <code>requirements.txt</code> then  <code>pip install -r
          requirements.txt</code> again.<br>It''s best first to uninstall the old
          version <code>pip uninstall llama-cpp-python -y</code></p>

          </blockquote>

          <p>You need to activate the environment first with:<br><code>./cmd_linux.sh</code></p>

          '
        raw: "> If you want to upgrade to llama-cpp-python=0.2.14 and get this error:\n\
          > `ERROR: Could not build wheels for llama-cpp-python, which is required\
          \ to install pyproject.toml-based projects`, \n> then you can replace all\
          \ 0.2.11 with 0.2.14 in `requirements.txt` then  `pip install -r requirements.txt`\
          \ again.\n> It's best first to uninstall the old version `pip uninstall\
          \ llama-cpp-python -y`\n\nYou need to activate the environment first with:\n\
          `./cmd_linux.sh`"
        updatedAt: '2023-11-18T01:12:15.677Z'
      numEdits: 1
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - PsiPi
    id: 65580f55539d4b7c13131704
    type: comment
  author: mufeed
  content: "> If you want to upgrade to llama-cpp-python=0.2.14 and get this error:\n\
    > `ERROR: Could not build wheels for llama-cpp-python, which is required to install\
    \ pyproject.toml-based projects`, \n> then you can replace all 0.2.11 with 0.2.14\
    \ in `requirements.txt` then  `pip install -r requirements.txt` again.\n> It's\
    \ best first to uninstall the old version `pip uninstall llama-cpp-python -y`\n\
    \nYou need to activate the environment first with:\n`./cmd_linux.sh`"
  created_at: 2023-11-18 01:11:49+00:00
  edited: true
  hidden: false
  id: 65580f55539d4b7c13131704
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e1cd6834e51cb4e481971685794ebfcb.svg
      fullname: Gunit Kah
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gtkunit
      type: user
    createdAt: '2023-11-18T04:28:08.000Z'
    data:
      edited: true
      editors:
      - gtkunit
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9782930612564087
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e1cd6834e51cb4e481971685794ebfcb.svg
          fullname: Gunit Kah
          isHf: false
          isPro: false
          name: gtkunit
          type: user
        html: '<p>Q4_K_M with cpp 0.2.18 fully offloaded to GPU is working for me
          on ooba. Q8 has two reports of being broken so I won''t try that yet.</p>

          <p>Edit:<br>Q6_K and Q8_0 (with 4k context) are working for me as well for
          at least one response.</p>

          '
        raw: 'Q4_K_M with cpp 0.2.18 fully offloaded to GPU is working for me on ooba.
          Q8 has two reports of being broken so I won''t try that yet.


          Edit:

          Q6_K and Q8_0 (with 4k context) are working for me as well for at least
          one response.'
        updatedAt: '2023-11-18T12:39:11.206Z'
      numEdits: 1
      reactions: []
    id: 65583d58b158b94c3a80120b
    type: comment
  author: gtkunit
  content: 'Q4_K_M with cpp 0.2.18 fully offloaded to GPU is working for me on ooba.
    Q8 has two reports of being broken so I won''t try that yet.


    Edit:

    Q6_K and Q8_0 (with 4k context) are working for me as well for at least one response.'
  created_at: 2023-11-18 04:28:08+00:00
  edited: true
  hidden: false
  id: 65583d58b158b94c3a80120b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/7ddca2b56096411db44768195289aa0d.svg
      fullname: ai
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: artificialgenerations4gsdfg
      type: user
    createdAt: '2023-11-23T21:36:46.000Z'
    data:
      edited: false
      editors:
      - artificialgenerations4gsdfg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5889499187469482
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/7ddca2b56096411db44768195289aa0d.svg
          fullname: ai
          isHf: false
          isPro: false
          name: artificialgenerations4gsdfg
          type: user
        html: '<p>Just confirming the bug, <code>ctransformers</code> gives the same
          error upon <code>model = AutoModelForCausalLM.from_pretrained(path, hf=True)</code>:</p>

          <pre><code>ERROR: byte not found in vocab: ''

          ''

          Segmentation fault (core dumped)

          </code></pre>

          '
        raw: 'Just confirming the bug, `ctransformers` gives the same error upon `model
          = AutoModelForCausalLM.from_pretrained(path, hf=True)`:


          ```

          ERROR: byte not found in vocab: ''

          ''

          Segmentation fault (core dumped)

          ```'
        updatedAt: '2023-11-23T21:36:46.530Z'
      numEdits: 0
      reactions: []
    id: 655fc5eed3f1b94f3b79f971
    type: comment
  author: artificialgenerations4gsdfg
  content: 'Just confirming the bug, `ctransformers` gives the same error upon `model
    = AutoModelForCausalLM.from_pretrained(path, hf=True)`:


    ```

    ERROR: byte not found in vocab: ''

    ''

    Segmentation fault (core dumped)

    ```'
  created_at: 2023-11-23 21:36:46+00:00
  edited: false
  hidden: false
  id: 655fc5eed3f1b94f3b79f971
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-11-24T11:47:16.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8947098255157471
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>Unfortunately ctransformers is not currently compatible with DeepSeek
          models, or other models that use BPE vocab. I''m hoping ctransformers will
          get an update soon.</p>

          <p>You can use llama-cpp-python instead, which is fully compatible.</p>

          '
        raw: 'Unfortunately ctransformers is not currently compatible with DeepSeek
          models, or other models that use BPE vocab. I''m hoping ctransformers will
          get an update soon.


          You can use llama-cpp-python instead, which is fully compatible.'
        updatedAt: '2023-11-24T11:47:16.737Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - PsiPi
        - artificialgenerations4gsdfg
        - tazik-bronco
    id: 65608d44b09c0b9ece8ac8c3
    type: comment
  author: TheBloke
  content: 'Unfortunately ctransformers is not currently compatible with DeepSeek
    models, or other models that use BPE vocab. I''m hoping ctransformers will get
    an update soon.


    You can use llama-cpp-python instead, which is fully compatible.'
  created_at: 2023-11-24 11:47:16+00:00
  edited: false
  hidden: false
  id: 65608d44b09c0b9ece8ac8c3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
      fullname: yehia serag
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: yehiaserag
      type: user
    createdAt: '2023-11-26T01:12:18.000Z'
    data:
      edited: false
      editors:
      - yehiaserag
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7344002723693848
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a824112d71878701e5efffd4298f6f3b.svg
          fullname: yehia serag
          isHf: false
          isPro: false
          name: yehiaserag
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;artificialgenerations4gsdfg&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/artificialgenerations4gsdfg\"\
          >@<span class=\"underline\">artificialgenerations4gsdfg</span></a></span>\n\
          \n\t</span></span> did it work for you using llama.cpp?</p>\n"
        raw: '@artificialgenerations4gsdfg did it work for you using llama.cpp?'
        updatedAt: '2023-11-26T01:12:18.971Z'
      numEdits: 0
      reactions: []
    id: 65629b72cf4f078533b10f83
    type: comment
  author: yehiaserag
  content: '@artificialgenerations4gsdfg did it work for you using llama.cpp?'
  created_at: 2023-11-26 01:12:18+00:00
  edited: false
  hidden: false
  id: 65629b72cf4f078533b10f83
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-11-26T08:10:57.000Z'
    data:
      edited: false
      editors:
      - Stilgar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.634974479675293
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
          fullname: Choraly
          isHf: false
          isPro: false
          name: Stilgar
          type: user
        html: '<p>Yes it works on 0.2.19</p>

          <p>llama_cpp_python          0.2.19+cpuavx2<br>llama_cpp_python_cuda     0.2.19+cu121</p>

          <p>I''m using it with the following parameters :</p>

          <p>temperature: 1.31<br>top_p: 0.14<br>top_k: 49<br>repetition_penality:
          1 (seems important, if not it''s adding new questions/answers by itself
          not related to the subject)</p>

          <p>Get 14.32 tokens/s on 4090 with 64 layers to GPU</p>

          '
        raw: 'Yes it works on 0.2.19


          llama_cpp_python          0.2.19+cpuavx2

          llama_cpp_python_cuda     0.2.19+cu121


          I''m using it with the following parameters :


          temperature: 1.31

          top_p: 0.14

          top_k: 49

          repetition_penality: 1 (seems important, if not it''s adding new questions/answers
          by itself not related to the subject)


          Get 14.32 tokens/s on 4090 with 64 layers to GPU

          '
        updatedAt: '2023-11-26T08:10:57.281Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - mufeed
        - yehiaserag
    id: 6562fd916ef2a1d0f96ca702
    type: comment
  author: Stilgar
  content: 'Yes it works on 0.2.19


    llama_cpp_python          0.2.19+cpuavx2

    llama_cpp_python_cuda     0.2.19+cu121


    I''m using it with the following parameters :


    temperature: 1.31

    top_p: 0.14

    top_k: 49

    repetition_penality: 1 (seems important, if not it''s adding new questions/answers
    by itself not related to the subject)


    Get 14.32 tokens/s on 4090 with 64 layers to GPU

    '
  created_at: 2023-11-26 08:10:57+00:00
  edited: false
  hidden: false
  id: 6562fd916ef2a1d0f96ca702
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: TheBloke/deepseek-coder-33B-instruct-GGUF
repo_type: model
status: open
target_branch: null
title: Models Not Loading
