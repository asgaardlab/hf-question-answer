!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Stilgar
conflicting_files: null
created_at: 2023-10-12 20:43:39+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-10-12T21:43:39.000Z'
    data:
      edited: false
      editors:
      - Stilgar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8827511668205261
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
          fullname: Choraly
          isHf: false
          isPro: false
          name: Stilgar
          type: user
        html: '<p>gptq-8bit-32g raised the following exception:<br>site-packages\exllama\cuda_ext.py",
          line 33, in ext_make_q4<br>    return make_q4(qweight,<br>RuntimeError:
          qweight and qzeros have incompatible shapes</p>

          <p>I''m using "ExLlama_HF" loader, autoGPTQ not better</p>

          <p>I''m not clear what it want to do with "ext_make_q4"</p>

          <p>gptq-4bit-32g is OK and working without issue</p>

          <p>oobabooga is running on it''s own virtual env and match latest requirements</p>

          '
        raw: "gptq-8bit-32g raised the following exception:\r\nsite-packages\\exllama\\\
          cuda_ext.py\", line 33, in ext_make_q4\r\n    return make_q4(qweight,\r\n\
          RuntimeError: qweight and qzeros have incompatible shapes\r\n\r\nI'm using\
          \ \"ExLlama_HF\" loader, autoGPTQ not better\r\n\r\nI'm not clear what it\
          \ want to do with \"ext_make_q4\"\r\n\r\ngptq-4bit-32g is OK and working\
          \ without issue\r\n\r\noobabooga is running on it's own virtual env and\
          \ match latest requirements"
        updatedAt: '2023-10-12T21:43:39.288Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - delphijb
        - mayeaux
    id: 6528688b9e472a150ef78f29
    type: comment
  author: Stilgar
  content: "gptq-8bit-32g raised the following exception:\r\nsite-packages\\exllama\\\
    cuda_ext.py\", line 33, in ext_make_q4\r\n    return make_q4(qweight,\r\nRuntimeError:\
    \ qweight and qzeros have incompatible shapes\r\n\r\nI'm using \"ExLlama_HF\"\
    \ loader, autoGPTQ not better\r\n\r\nI'm not clear what it want to do with \"\
    ext_make_q4\"\r\n\r\ngptq-4bit-32g is OK and working without issue\r\n\r\noobabooga\
    \ is running on it's own virtual env and match latest requirements"
  created_at: 2023-10-12 20:43:39+00:00
  edited: false
  hidden: false
  id: 6528688b9e472a150ef78f29
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-10-13T03:07:48.000Z'
    data:
      edited: true
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9723721742630005
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<p>ExLlama doesn''t support 8-bit GPTQ, and AutoGPTQ doesn''t currently
          support Mistral GPTQ.</p>

          <p>Please try using Transformers as the Loader, and see if that works. I''ve
          not personally tested it in Oobabooga, but I know Transformers works from
          Python code</p>

          '
        raw: 'ExLlama doesn''t support 8-bit GPTQ, and AutoGPTQ doesn''t currently
          support Mistral GPTQ.


          Please try using Transformers as the Loader, and see if that works. I''ve
          not personally tested it in Oobabooga, but I know Transformers works from
          Python code'
        updatedAt: '2023-10-13T03:08:06.856Z'
      numEdits: 1
      reactions: []
    id: 6528b484c8103ca0040d0431
    type: comment
  author: TheBloke
  content: 'ExLlama doesn''t support 8-bit GPTQ, and AutoGPTQ doesn''t currently support
    Mistral GPTQ.


    Please try using Transformers as the Loader, and see if that works. I''ve not
    personally tested it in Oobabooga, but I know Transformers works from Python code'
  created_at: 2023-10-13 02:07:48+00:00
  edited: true
  hidden: false
  id: 6528b484c8103ca0040d0431
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-10-13T08:56:27.000Z'
    data:
      edited: false
      editors:
      - Stilgar
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9111578464508057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
          fullname: Choraly
          isHf: false
          isPro: false
          name: Stilgar
          type: user
        html: "<p>It works with transformer but it\u2019s slow (3 to 4 tokens/s),\
          \ the GGUF release with 6 bits quantization and llama.cpp runs at 8 tokens/s\
          \ and the GPTQ 4 bits with ExLlama_HF reaches 45 to 48 token/s.<br>I\u2019\
          m using a 4090, and the last case is using 11Go on the GPU.<br>I cannot\
          \ see a quality difference between the models at least for storytelling,\
          \ but did not test long<br>Thank you for you work and your reply.</p>\n"
        raw: "It works with transformer but it\u2019s slow (3 to 4 tokens/s), the\
          \ GGUF release with 6 bits quantization and llama.cpp runs at 8 tokens/s\
          \ and the GPTQ 4 bits with ExLlama_HF reaches 45 to 48 token/s.\nI\u2019\
          m using a 4090, and the last case is using 11Go on the GPU.\nI cannot see\
          \ a quality difference between the models at least for storytelling, but\
          \ did not test long\nThank you for you work and your reply.\n"
        updatedAt: '2023-10-13T08:56:27.536Z'
      numEdits: 0
      reactions: []
      relatedEventId: 6529063bc36facb8261429df
    id: 6529063bc36facb8261429d7
    type: comment
  author: Stilgar
  content: "It works with transformer but it\u2019s slow (3 to 4 tokens/s), the GGUF\
    \ release with 6 bits quantization and llama.cpp runs at 8 tokens/s and the GPTQ\
    \ 4 bits with ExLlama_HF reaches 45 to 48 token/s.\nI\u2019m using a 4090, and\
    \ the last case is using 11Go on the GPU.\nI cannot see a quality difference between\
    \ the models at least for storytelling, but did not test long\nThank you for you\
    \ work and your reply.\n"
  created_at: 2023-10-13 07:56:27+00:00
  edited: false
  hidden: false
  id: 6529063bc36facb8261429d7
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/f5bcb4d417d1db7eb1d0450e9c715d88.svg
      fullname: Choraly
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Stilgar
      type: user
    createdAt: '2023-10-13T08:56:27.000Z'
    data:
      status: closed
    id: 6529063bc36facb8261429df
    type: status-change
  author: Stilgar
  created_at: 2023-10-13 07:56:27+00:00
  id: 6529063bc36facb8261429df
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/dolphin-2.1-mistral-7B-GPTQ
repo_type: model
status: closed
target_branch: null
title: 8 bits quantization is not working with the model on the latest oobabooga
