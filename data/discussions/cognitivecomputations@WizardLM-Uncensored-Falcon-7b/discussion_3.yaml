!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dfrank
conflicting_files: null
created_at: 2023-06-14 06:45:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667996799164-61d9cefa2601ce321dbe5144.png?w=200&h=200&f=face
      fullname: David Frankenberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dfrank
      type: user
    createdAt: '2023-06-14T07:45:08.000Z'
    data:
      edited: false
      editors:
      - dfrank
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6145210266113281
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1667996799164-61d9cefa2601ce321dbe5144.png?w=200&h=200&f=face
          fullname: David Frankenberg
          isHf: false
          isPro: false
          name: dfrank
          type: user
        html: "<p>Hi, I have tried to quantize the model to 4 bits without success,\
          \ following the same procedure I use for the original model <code>tiiuae/falcon-7b</code>.\
          \  When I checked the model architecture they look quite similar but for\
          \ some reason the <code>ehartford/WizardLM-Uncensored-Falcon-7b</code> have\
          \ one extra dimension (I believe is related with one extra token). Do you\
          \ know why?</p>\n<p>As a reference, I leave you the architectures, look\
          \ at the embedding layer.</p>\n<p><strong>tiiuae/falcon-7b:</strong></p>\n\
          <pre><code>RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings):\
          \ Embedding(65024, 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n\
          \        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
          \        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n\
          \          (query_key_value): Linear4bit(in_features=4544, out_features=4672,\
          \ bias=False)\n          (dense): Linear4bit(in_features=4544, out_features=4544,\
          \ bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n\
          \        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear4bit(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\n)\n</code></pre>\n<p><strong>ehartford/WizardLM-Uncensored-Falcon-7b:</strong></p>\n\
          <pre><code>RWForCausalLM(\n  (transformer): RWModel(\n    (word_embeddings):\
          \ Embedding(65025, 4544)\n    (h): ModuleList(\n      (0-31): 32 x DecoderLayer(\n\
          \        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n\
          \        (self_attention): Attention(\n          (maybe_rotary): RotaryEmbedding()\n\
          \          (query_key_value): Linear4bit(in_features=4544, out_features=4672,\
          \ bias=False)\n          (dense): Linear4bit(in_features=4544, out_features=4544,\
          \ bias=False)\n          (attention_dropout): Dropout(p=0.0, inplace=False)\n\
          \        )\n        (mlp): MLP(\n          (dense_h_to_4h): Linear4bit(in_features=4544,\
          \ out_features=18176, bias=False)\n          (act): GELU(approximate='none')\n\
          \          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544,\
          \ bias=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65025, bias=False)\n</code></pre>\n"
        raw: "Hi, I have tried to quantize the model to 4 bits without success, following\
          \ the same procedure I use for the original model ``tiiuae/falcon-7b``.\
          \  When I checked the model architecture they look quite similar but for\
          \ some reason the ``ehartford/WizardLM-Uncensored-Falcon-7b`` have one extra\
          \ dimension (I believe is related with one extra token). Do you know why?\r\
          \n\r\nAs a reference, I leave you the architectures, look at the embedding\
          \ layer.\r\n\r\n**tiiuae/falcon-7b:**\r\n```\r\nRWForCausalLM(\r\n  (transformer):\
          \ RWModel(\r\n    (word_embeddings): Embedding(65024, 4544)\r\n    (h):\
          \ ModuleList(\r\n      (0-31): 32 x DecoderLayer(\r\n        (input_layernorm):\
          \ LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\n        (self_attention):\
          \ Attention(\r\n          (maybe_rotary): RotaryEmbedding()\r\n        \
          \  (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\r\
          \n          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\r\
          \n          (attention_dropout): Dropout(p=0.0, inplace=False)\r\n     \
          \   )\r\n        (mlp): MLP(\r\n          (dense_h_to_4h): Linear4bit(in_features=4544,\
          \ out_features=18176, bias=False)\r\n          (act): GELU(approximate='none')\r\
          \n          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544,\
          \ bias=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65024, bias=False)\r\n)\r\n```\r\n\r\n**ehartford/WizardLM-Uncensored-Falcon-7b:**\r\
          \n```\r\nRWForCausalLM(\r\n  (transformer): RWModel(\r\n    (word_embeddings):\
          \ Embedding(65025, 4544)\r\n    (h): ModuleList(\r\n      (0-31): 32 x DecoderLayer(\r\
          \n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
          \n        (self_attention): Attention(\r\n          (maybe_rotary): RotaryEmbedding()\r\
          \n          (query_key_value): Linear4bit(in_features=4544, out_features=4672,\
          \ bias=False)\r\n          (dense): Linear4bit(in_features=4544, out_features=4544,\
          \ bias=False)\r\n          (attention_dropout): Dropout(p=0.0, inplace=False)\r\
          \n        )\r\n        (mlp): MLP(\r\n          (dense_h_to_4h): Linear4bit(in_features=4544,\
          \ out_features=18176, bias=False)\r\n          (act): GELU(approximate='none')\r\
          \n          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544,\
          \ bias=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,),\
          \ eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=4544,\
          \ out_features=65025, bias=False)\r\n```"
        updatedAt: '2023-06-14T07:45:08.347Z'
      numEdits: 0
      reactions: []
    id: 64897004525a712f4a3cb218
    type: comment
  author: dfrank
  content: "Hi, I have tried to quantize the model to 4 bits without success, following\
    \ the same procedure I use for the original model ``tiiuae/falcon-7b``.  When\
    \ I checked the model architecture they look quite similar but for some reason\
    \ the ``ehartford/WizardLM-Uncensored-Falcon-7b`` have one extra dimension (I\
    \ believe is related with one extra token). Do you know why?\r\n\r\nAs a reference,\
    \ I leave you the architectures, look at the embedding layer.\r\n\r\n**tiiuae/falcon-7b:**\r\
    \n```\r\nRWForCausalLM(\r\n  (transformer): RWModel(\r\n    (word_embeddings):\
    \ Embedding(65024, 4544)\r\n    (h): ModuleList(\r\n      (0-31): 32 x DecoderLayer(\r\
    \n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
    \n        (self_attention): Attention(\r\n          (maybe_rotary): RotaryEmbedding()\r\
    \n          (query_key_value): Linear4bit(in_features=4544, out_features=4672,\
    \ bias=False)\r\n          (dense): Linear4bit(in_features=4544, out_features=4544,\
    \ bias=False)\r\n          (attention_dropout): Dropout(p=0.0, inplace=False)\r\
    \n        )\r\n        (mlp): MLP(\r\n          (dense_h_to_4h): Linear4bit(in_features=4544,\
    \ out_features=18176, bias=False)\r\n          (act): GELU(approximate='none')\r\
    \n          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544,\
    \ bias=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,),\
    \ eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=4544,\
    \ out_features=65024, bias=False)\r\n)\r\n```\r\n\r\n**ehartford/WizardLM-Uncensored-Falcon-7b:**\r\
    \n```\r\nRWForCausalLM(\r\n  (transformer): RWModel(\r\n    (word_embeddings):\
    \ Embedding(65025, 4544)\r\n    (h): ModuleList(\r\n      (0-31): 32 x DecoderLayer(\r\
    \n        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\r\
    \n        (self_attention): Attention(\r\n          (maybe_rotary): RotaryEmbedding()\r\
    \n          (query_key_value): Linear4bit(in_features=4544, out_features=4672,\
    \ bias=False)\r\n          (dense): Linear4bit(in_features=4544, out_features=4544,\
    \ bias=False)\r\n          (attention_dropout): Dropout(p=0.0, inplace=False)\r\
    \n        )\r\n        (mlp): MLP(\r\n          (dense_h_to_4h): Linear4bit(in_features=4544,\
    \ out_features=18176, bias=False)\r\n          (act): GELU(approximate='none')\r\
    \n          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544,\
    \ bias=False)\r\n        )\r\n      )\r\n    )\r\n    (ln_f): LayerNorm((4544,),\
    \ eps=1e-05, elementwise_affine=True)\r\n  )\r\n  (lm_head): Linear(in_features=4544,\
    \ out_features=65025, bias=False)\r\n```"
  created_at: 2023-06-14 06:45:08+00:00
  edited: false
  hidden: false
  id: 64897004525a712f4a3cb218
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
      fullname: Eric Hartford
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: true
      name: ehartford
      type: user
    createdAt: '2023-06-14T14:38:55.000Z'
    data:
      edited: false
      editors:
      - ehartford
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9972299337387085
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63111b2d88942700629f5771/u2a9y-yx6TG0N31OhMSHI.png?w=200&h=200&f=face
          fullname: Eric Hartford
          isHf: false
          isPro: true
          name: ehartford
          type: user
        html: '<p>I don''t really know the reason why it is that way </p>

          '
        raw: 'I don''t really know the reason why it is that way '
        updatedAt: '2023-06-14T14:38:55.621Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - dfrank
    id: 6489d0ff90497fd1a778594b
    type: comment
  author: ehartford
  content: 'I don''t really know the reason why it is that way '
  created_at: 2023-06-14 13:38:55+00:00
  edited: false
  hidden: false
  id: 6489d0ff90497fd1a778594b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: cognitivecomputations/WizardLM-Uncensored-Falcon-7b
repo_type: model
status: open
target_branch: null
title: Differences in model architecture
