!!python/object:huggingface_hub.community.DiscussionWithDetails
author: MrDevolver
conflicting_files: null
created_at: 2023-06-14 02:05:03+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
      fullname: Angelino Santiago
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MrDevolver
      type: user
    createdAt: '2023-06-14T03:05:03.000Z'
    data:
      edited: false
      editors:
      - MrDevolver
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9634262323379517
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/6aec9ca2ae47baad15b60b38ec81d69d.svg
          fullname: Angelino Santiago
          isHf: false
          isPro: false
          name: MrDevolver
          type: user
        html: "<p>Hey, today I was excited to download the latest version of KoboldCpp\
          \ v1.30.1 which apparently fully supports new k-quantizers on Clblast now.\
          \ Yay!</p>\n<p>I downloaded one of my favorite models, this time one level\
          \ higher than what I would usually download just to test the ability of\
          \ working on the edge of the hw possibilities. For this test I chose the\
          \ smallest version of Guanaco-33B-GGML: guanaco-33B.ggmlv3.q2_K.bin.</p>\n\
          <p>Initially I was struggling to make it run! The model loaded all the time\
          \ no problem, BUT it kept crashing, giving me errors about running out of\
          \ memory. \U0001F611<br>I ended up, lowering blast cache from my usual 1024\
          \ down to 512 and I tried again that way. Surprise surprise! This time it\
          \ worked! \U0001F973</p>\n<p>So much for the good news. Now for the bad\
          \ news... It's super slow!!! I guess if there was a volunteer with Guanaco-65B\
          \ on Kobold Lite, you would get your output there much faster and probably\
          \ in higher quality too, but at least it works locally on the aforementioned\
          \ hardware. I guess this is good news for those who don't care about speed.\
          \ \U0001F603</p>\n"
        raw: "Hey, today I was excited to download the latest version of KoboldCpp\
          \ v1.30.1 which apparently fully supports new k-quantizers on Clblast now.\
          \ Yay!\r\n\r\nI downloaded one of my favorite models, this time one level\
          \ higher than what I would usually download just to test the ability of\
          \ working on the edge of the hw possibilities. For this test I chose the\
          \ smallest version of Guanaco-33B-GGML: guanaco-33B.ggmlv3.q2_K.bin.\r\n\
          \r\nInitially I was struggling to make it run! The model loaded all the\
          \ time no problem, BUT it kept crashing, giving me errors about running\
          \ out of memory. \U0001F611\r\nI ended up, lowering blast cache from my\
          \ usual 1024 down to 512 and I tried again that way. Surprise surprise!\
          \ This time it worked! \U0001F973\r\n\r\nSo much for the good news. Now\
          \ for the bad news... It's super slow!!! I guess if there was a volunteer\
          \ with Guanaco-65B on Kobold Lite, you would get your output there much\
          \ faster and probably in higher quality too, but at least it works locally\
          \ on the aforementioned hardware. I guess this is good news for those who\
          \ don't care about speed. \U0001F603"
        updatedAt: '2023-06-14T03:05:03.046Z'
      numEdits: 0
      reactions: []
    id: 64892e5fb168777dcf74362f
    type: comment
  author: MrDevolver
  content: "Hey, today I was excited to download the latest version of KoboldCpp v1.30.1\
    \ which apparently fully supports new k-quantizers on Clblast now. Yay!\r\n\r\n\
    I downloaded one of my favorite models, this time one level higher than what I\
    \ would usually download just to test the ability of working on the edge of the\
    \ hw possibilities. For this test I chose the smallest version of Guanaco-33B-GGML:\
    \ guanaco-33B.ggmlv3.q2_K.bin.\r\n\r\nInitially I was struggling to make it run!\
    \ The model loaded all the time no problem, BUT it kept crashing, giving me errors\
    \ about running out of memory. \U0001F611\r\nI ended up, lowering blast cache\
    \ from my usual 1024 down to 512 and I tried again that way. Surprise surprise!\
    \ This time it worked! \U0001F973\r\n\r\nSo much for the good news. Now for the\
    \ bad news... It's super slow!!! I guess if there was a volunteer with Guanaco-65B\
    \ on Kobold Lite, you would get your output there much faster and probably in\
    \ higher quality too, but at least it works locally on the aforementioned hardware.\
    \ I guess this is good news for those who don't care about speed. \U0001F603"
  created_at: 2023-06-14 02:05:03+00:00
  edited: false
  hidden: false
  id: 64892e5fb168777dcf74362f
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: TheBloke/guanaco-33B-GGML
repo_type: model
status: open
target_branch: null
title: Works with 16 GB RAM, 8 GB VRAM, BUT...
