!!python/object:huggingface_hub.community.DiscussionWithDetails
author: spkprav
conflicting_files: null
created_at: 2023-07-04 17:10:38+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/87e7540356f4caa3119cc4e67ef240f5.svg
      fullname: Praveen Kumar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: spkprav
      type: user
    createdAt: '2023-07-04T18:10:38.000Z'
    data:
      edited: false
      editors:
      - spkprav
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8534755110740662
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/87e7540356f4caa3119cc4e67ef240f5.svg
          fullname: Praveen Kumar
          isHf: false
          isPro: false
          name: spkprav
          type: user
        html: '<p>Exception occurred: CUDA out of memory. Tried to allocate 3.71 GiB
          (GPU 0; 11.75 GiB total capacity; 4.80 GiB already allocated; 1.75 GiB free;
          8.56 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated
          memory try setting max_split_size_mb to avoid fragmentation.  See documentation
          for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          <p>It''s a fresh install, I tried it after restarting the system, is this
          supported in the above configuration?</p>

          '
        raw: "Exception occurred: CUDA out of memory. Tried to allocate 3.71 GiB (GPU\
          \ 0; 11.75 GiB total capacity; 4.80 GiB already allocated; 1.75 GiB free;\
          \ 8.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated\
          \ memory try setting max_split_size_mb to avoid fragmentation.  See documentation\
          \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nIt's a fresh\
          \ install, I tried it after restarting the system, is this supported in\
          \ the above configuration?"
        updatedAt: '2023-07-04T18:10:38.905Z'
      numEdits: 0
      reactions: []
    id: 64a4609eeb77839e3177578d
    type: comment
  author: spkprav
  content: "Exception occurred: CUDA out of memory. Tried to allocate 3.71 GiB (GPU\
    \ 0; 11.75 GiB total capacity; 4.80 GiB already allocated; 1.75 GiB free; 8.56\
    \ GiB reserved in total by PyTorch) If reserved memory is >> allocated memory\
    \ try setting max_split_size_mb to avoid fragmentation.  See documentation for\
    \ Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n\r\nIt's a fresh install, I\
    \ tried it after restarting the system, is this supported in the above configuration?"
  created_at: 2023-07-04 17:10:38+00:00
  edited: false
  hidden: false
  id: 64a4609eeb77839e3177578d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303c92e3926de1f7ec2683a/FRrIJttVR3GJYcBfVf1HU.jpeg?w=200&h=200&f=face
      fullname: Spencer Sterling
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cerspense
      type: user
    createdAt: '2023-07-04T20:26:16.000Z'
    data:
      edited: false
      editors:
      - cerspense
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7539867758750916
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303c92e3926de1f7ec2683a/FRrIJttVR3GJYcBfVf1HU.jpeg?w=200&h=200&f=face
          fullname: Spencer Sterling
          isHf: false
          isPro: false
          name: cerspense
          type: user
        html: '<p>Did you try running the webui with  the --xformers commandline arg?</p>

          '
        raw: Did you try running the webui with  the --xformers commandline arg?
        updatedAt: '2023-07-04T20:26:16.151Z'
      numEdits: 0
      reactions: []
    id: 64a480688b961fd6fa36f056
    type: comment
  author: cerspense
  content: Did you try running the webui with  the --xformers commandline arg?
  created_at: 2023-07-04 19:26:16+00:00
  edited: false
  hidden: false
  id: 64a480688b961fd6fa36f056
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/65241a255b133805d04ea1ec28135503.svg
      fullname: '014'
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Qwerty55155
      type: user
    createdAt: '2023-11-20T13:36:08.000Z'
    data:
      edited: false
      editors:
      - Qwerty55155
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7542405724525452
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/65241a255b133805d04ea1ec28135503.svg
          fullname: '014'
          isHf: false
          isPro: false
          name: Qwerty55155
          type: user
        html: '<p>I have the same error while I am using Nvdia Geforce 1960ti 4gb
          is there a way to resolve the error<br>If so please mention the way</p>

          <p>Thank you</p>

          <p>Error :<br>OutOfMemoryError: CUDA out of memory. Tried to allocate 170.00
          MiB (GPU 0; 4.00 GiB total capacity; 9.71 GiB already allocated; 0 bytes
          free; 9.99 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt;
          allocated memory try setting max_split_size_mb to avoid fragmentation.  See
          documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>

          '
        raw: "I have the same error while I am using Nvdia Geforce 1960ti 4gb is there\
          \ a way to resolve the error\nIf so please mention the way\n\nThank you\n\
          \nError : \nOutOfMemoryError: CUDA out of memory. Tried to allocate 170.00\
          \ MiB (GPU 0; 4.00 GiB total capacity; 9.71 GiB already allocated; 0 bytes\
          \ free; 9.99 GiB reserved in total by PyTorch) If reserved memory is >>\
          \ allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
        updatedAt: '2023-11-20T13:36:08.924Z'
      numEdits: 0
      reactions: []
    id: 655b60c804a63a0dfb3fb2d7
    type: comment
  author: Qwerty55155
  content: "I have the same error while I am using Nvdia Geforce 1960ti 4gb is there\
    \ a way to resolve the error\nIf so please mention the way\n\nThank you\n\nError\
    \ : \nOutOfMemoryError: CUDA out of memory. Tried to allocate 170.00 MiB (GPU\
    \ 0; 4.00 GiB total capacity; 9.71 GiB already allocated; 0 bytes free; 9.99 GiB\
    \ reserved in total by PyTorch) If reserved memory is >> allocated memory try\
    \ setting max_split_size_mb to avoid fragmentation.  See documentation for Memory\
    \ Management and PYTORCH_CUDA_ALLOC_CONF"
  created_at: 2023-11-20 13:36:08+00:00
  edited: false
  hidden: false
  id: 655b60c804a63a0dfb3fb2d7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 11
repo_id: cerspense/zeroscope_v2_576w
repo_type: model
status: open
target_branch: null
title: Memory issue with RTX 3060, 12GB
