!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kevinrosenberg
conflicting_files: null
created_at: 2023-06-25 02:05:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7341662021b07d8a0fffeda9fa9f7bb.svg
      fullname: Kevin Rosenberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kevinrosenberg
      type: user
    createdAt: '2023-06-25T03:05:24.000Z'
    data:
      edited: true
      editors:
      - kevinrosenberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9621561765670776
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7341662021b07d8a0fffeda9fa9f7bb.svg
          fullname: Kevin Rosenberg
          isHf: false
          isPro: false
          name: kevinrosenberg
          type: user
        html: '<p>Has anyone tried to somehow set the last frame of a previous generation
          as the first one of a new batch to try to get coherence and create longer
          generations?</p>

          '
        raw: Has anyone tried to somehow set the last frame of a previous generation
          as the first one of a new batch to try to get coherence and create longer
          generations?
        updatedAt: '2023-06-25T03:05:40.711Z'
      numEdits: 1
      reactions: []
    id: 6497aef49d0ae66cbd484b0c
    type: comment
  author: kevinrosenberg
  content: Has anyone tried to somehow set the last frame of a previous generation
    as the first one of a new batch to try to get coherence and create longer generations?
  created_at: 2023-06-25 02:05:24+00:00
  edited: true
  hidden: false
  id: 6497aef49d0ae66cbd484b0c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303c92e3926de1f7ec2683a/FRrIJttVR3GJYcBfVf1HU.jpeg?w=200&h=200&f=face
      fullname: Spencer Sterling
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cerspense
      type: user
    createdAt: '2023-06-25T05:46:30.000Z'
    data:
      edited: false
      editors:
      - cerspense
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9423053860664368
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303c92e3926de1f7ec2683a/FRrIJttVR3GJYcBfVf1HU.jpeg?w=200&h=200&f=face
          fullname: Spencer Sterling
          isHf: false
          isPro: false
          name: cerspense
          type: user
        html: '<p>With the text2video extension, you can continue from a still. I
          did a test where I continued from the last frame of each clip and stitched
          it together, but the results are not super great. There are some other methods
          in the works to make this better. It will likely require some kind of controlnet
          to guide the clip continuation and it will probably require a model trained
          on a longer clip length. Here''s the test: <a rel="nofollow" href="https://streamable.com/x03ljm">https://streamable.com/x03ljm</a></p>

          '
        raw: 'With the text2video extension, you can continue from a still. I did
          a test where I continued from the last frame of each clip and stitched it
          together, but the results are not super great. There are some other methods
          in the works to make this better. It will likely require some kind of controlnet
          to guide the clip continuation and it will probably require a model trained
          on a longer clip length. Here''s the test: https://streamable.com/x03ljm'
        updatedAt: '2023-06-25T05:46:30.307Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - kevinrosenberg
    id: 6497d4b62324dea0e3251aaa
    type: comment
  author: cerspense
  content: 'With the text2video extension, you can continue from a still. I did a
    test where I continued from the last frame of each clip and stitched it together,
    but the results are not super great. There are some other methods in the works
    to make this better. It will likely require some kind of controlnet to guide the
    clip continuation and it will probably require a model trained on a longer clip
    length. Here''s the test: https://streamable.com/x03ljm'
  created_at: 2023-06-25 04:46:30+00:00
  edited: false
  hidden: false
  id: 6497d4b62324dea0e3251aaa
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f7341662021b07d8a0fffeda9fa9f7bb.svg
      fullname: Kevin Rosenberg
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kevinrosenberg
      type: user
    createdAt: '2023-06-25T14:10:40.000Z'
    data:
      edited: false
      editors:
      - kevinrosenberg
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9824348092079163
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f7341662021b07d8a0fffeda9fa9f7bb.svg
          fullname: Kevin Rosenberg
          isHf: false
          isPro: false
          name: kevinrosenberg
          type: user
        html: '<p>Wow that''s great! Thank you, it''s exactly what I was aiming for.
          I''ll look into the python code, I''m trying to get it to work for vid2vid.
          So the logic would be to take the code part for starting from a still, taking
          the last processed frame as that still and guide the rest of the generation
          with the same method as vid2vid, but tbh i''m not sure it''s possible or
          they are two completely different methods. I''ll post here if I get anywhere.</p>

          '
        raw: Wow that's great! Thank you, it's exactly what I was aiming for. I'll
          look into the python code, I'm trying to get it to work for vid2vid. So
          the logic would be to take the code part for starting from a still, taking
          the last processed frame as that still and guide the rest of the generation
          with the same method as vid2vid, but tbh i'm not sure it's possible or they
          are two completely different methods. I'll post here if I get anywhere.
        updatedAt: '2023-06-25T14:10:40.143Z'
      numEdits: 0
      reactions: []
    id: 64984ae05e95532769f31429
    type: comment
  author: kevinrosenberg
  content: Wow that's great! Thank you, it's exactly what I was aiming for. I'll look
    into the python code, I'm trying to get it to work for vid2vid. So the logic would
    be to take the code part for starting from a still, taking the last processed
    frame as that still and guide the rest of the generation with the same method
    as vid2vid, but tbh i'm not sure it's possible or they are two completely different
    methods. I'll post here if I get anywhere.
  created_at: 2023-06-25 13:10:40+00:00
  edited: false
  hidden: false
  id: 64984ae05e95532769f31429
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/e6cb8a155b1460e76a7b749135590bc0.svg
      fullname: Johannes Saam
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: atara
      type: user
    createdAt: '2023-06-25T23:56:26.000Z'
    data:
      edited: true
      editors:
      - atara
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9210367202758789
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/e6cb8a155b1460e76a7b749135590bc0.svg
          fullname: Johannes Saam
          isHf: false
          isPro: false
          name: atara
          type: user
        html: '<p>Hey! I am trying to run this A1111 A6000 GPU; all I get is streaks.
          Is there any chance you can share prompts and settings to get it dialed
          in?</p>

          <p>Thanks!</p>

          <p>EDIT - Fixed this by not rendering more then 48 frames... </p>

          '
        raw: 'Hey! I am trying to run this A1111 A6000 GPU; all I get is streaks.
          Is there any chance you can share prompts and settings to get it dialed
          in?


          Thanks!


          EDIT - Fixed this by not rendering more then 48 frames... '
        updatedAt: '2023-06-27T11:54:19.118Z'
      numEdits: 1
      reactions: []
    id: 6498d42a73736b1e8e59631e
    type: comment
  author: atara
  content: 'Hey! I am trying to run this A1111 A6000 GPU; all I get is streaks. Is
    there any chance you can share prompts and settings to get it dialed in?


    Thanks!


    EDIT - Fixed this by not rendering more then 48 frames... '
  created_at: 2023-06-25 22:56:26+00:00
  edited: true
  hidden: false
  id: 6498d42a73736b1e8e59631e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303c92e3926de1f7ec2683a/FRrIJttVR3GJYcBfVf1HU.jpeg?w=200&h=200&f=face
      fullname: Spencer Sterling
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: cerspense
      type: user
    createdAt: '2023-06-27T07:02:09.000Z'
    data:
      edited: false
      editors:
      - cerspense
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.812711775302887
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303c92e3926de1f7ec2683a/FRrIJttVR3GJYcBfVf1HU.jpeg?w=200&h=200&f=face
          fullname: Spencer Sterling
          isHf: false
          isPro: false
          name: cerspense
          type: user
        html: '<p>I did actually make an edit to the configuration.json for the model
          to improve the results with this continuation method. Open it up from ''stable-diffusion-webui\models\ModelScope\t2v''
          and change line 22 like so: "unet_attn_scales": [0.25, 0.5, 1]<br>credit
          goes to za15704080 on discord for this trick!</p>

          '
        raw: 'I did actually make an edit to the configuration.json for the model
          to improve the results with this continuation method. Open it up from ''stable-diffusion-webui\models\ModelScope\t2v''
          and change line 22 like so: "unet_attn_scales": [0.25, 0.5, 1]

          credit goes to za15704080 on discord for this trick!'
        updatedAt: '2023-06-27T07:02:09.140Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - BinxNet
        - kevinrosenberg
        - kar-pal
    id: 649a8971ea2cdac809a38a33
    type: comment
  author: cerspense
  content: 'I did actually make an edit to the configuration.json for the model to
    improve the results with this continuation method. Open it up from ''stable-diffusion-webui\models\ModelScope\t2v''
    and change line 22 like so: "unet_attn_scales": [0.25, 0.5, 1]

    credit goes to za15704080 on discord for this trick!'
  created_at: 2023-06-27 06:02:09+00:00
  edited: false
  hidden: false
  id: 649a8971ea2cdac809a38a33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9a71dc0375dae3569e3da3d4c7cf8b62.svg
      fullname: Erik M
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ztsvvstz
      type: user
    createdAt: '2023-07-12T15:04:46.000Z'
    data:
      edited: false
      editors:
      - ztsvvstz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9827468991279602
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9a71dc0375dae3569e3da3d4c7cf8b62.svg
          fullname: Erik M
          isHf: false
          isPro: false
          name: ztsvvstz
          type: user
        html: '<p>Hi, is there any way / colab maybe which showcases how to do video
          inpainting directly with the pipeline?<br>Only found code for an auto1111
          extension in which Im not interested in.<br>I might be able to implement
          that myself based on it, but maybe someone has some more experience with
          this.<br>Basically, what I want to do is setup the first 12 latent frames
          with the last 12 of the previous segment (as mentioned earlier here) and
          also lock them.<br>But I guess maybe I just have to look into regular inpainting
          with SD and hack something together..</p>

          '
        raw: 'Hi, is there any way / colab maybe which showcases how to do video inpainting
          directly with the pipeline?

          Only found code for an auto1111 extension in which Im not interested in.

          I might be able to implement that myself based on it, but maybe someone
          has some more experience with this.

          Basically, what I want to do is setup the first 12 latent frames with the
          last 12 of the previous segment (as mentioned earlier here) and also lock
          them.

          But I guess maybe I just have to look into regular inpainting with SD and
          hack something together..'
        updatedAt: '2023-07-12T15:04:46.343Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - kar-pal
    id: 64aec10e710608ee1afd5c37
    type: comment
  author: ztsvvstz
  content: 'Hi, is there any way / colab maybe which showcases how to do video inpainting
    directly with the pipeline?

    Only found code for an auto1111 extension in which Im not interested in.

    I might be able to implement that myself based on it, but maybe someone has some
    more experience with this.

    Basically, what I want to do is setup the first 12 latent frames with the last
    12 of the previous segment (as mentioned earlier here) and also lock them.

    But I guess maybe I just have to look into regular inpainting with SD and hack
    something together..'
  created_at: 2023-07-12 14:04:46+00:00
  edited: false
  hidden: false
  id: 64aec10e710608ee1afd5c37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/98e5c888110840246970d389aff06f34.svg
      fullname: GAURAV SAHA
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gauravsaha
      type: user
    createdAt: '2023-10-16T03:43:36.000Z'
    data:
      edited: false
      editors:
      - gauravsaha
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9272469878196716
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/98e5c888110840246970d389aff06f34.svg
          fullname: GAURAV SAHA
          isHf: false
          isPro: false
          name: gauravsaha
          type: user
        html: '<p>Is there any way to continue a video given a prompt with a similar
          context as the original video''s prompts and the original video, such that
          the result is a new video with both the older and newer prompts?</p>

          '
        raw: Is there any way to continue a video given a prompt with a similar context
          as the original video's prompts and the original video, such that the result
          is a new video with both the older and newer prompts?
        updatedAt: '2023-10-16T03:43:36.116Z'
      numEdits: 0
      reactions: []
    id: 652cb1681ef9983c6d40fb7a
    type: comment
  author: gauravsaha
  content: Is there any way to continue a video given a prompt with a similar context
    as the original video's prompts and the original video, such that the result is
    a new video with both the older and newer prompts?
  created_at: 2023-10-16 02:43:36+00:00
  edited: false
  hidden: false
  id: 652cb1681ef9983c6d40fb7a
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: cerspense/zeroscope_v2_576w
repo_type: model
status: open
target_branch: null
title: Video continuation in longer generations
