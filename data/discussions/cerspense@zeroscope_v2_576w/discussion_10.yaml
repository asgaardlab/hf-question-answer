!!python/object:huggingface_hub.community.DiscussionWithDetails
author: bacprop
conflicting_files: null
created_at: 2023-07-04 11:38:53+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/2fa5dd19040903d186969e05a066de20.svg
      fullname: Romain Paulus
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: bacprop
      type: user
    createdAt: '2023-07-04T12:38:53.000Z'
    data:
      edited: false
      editors:
      - bacprop
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8711819648742676
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/2fa5dd19040903d186969e05a066de20.svg
          fullname: Romain Paulus
          isHf: false
          isPro: false
          name: bacprop
          type: user
        html: '<p>I''ve been trying to run this model both with the Huggingface API
          and A1111, and I''ve noticed that the A1111 text2video extension requires
          the original modelscope VQGAN autoencoder file to be downloaded (5.21 GB
          here: <a href="https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/tree/main">https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/tree/main</a>),
          but this zeroscope model has a much smaller VAE file (167 MB here: <a href="https://huggingface.co/cerspense/zeroscope_v2_576w/tree/main/vae">https://huggingface.co/cerspense/zeroscope_v2_576w/tree/main/vae</a>).
          Are these model files equivalent, despite their size difference? If not,
          does it mean that the A1111 pipeline is different from the Huggingface API
          pipeline in terms of computation and results, or is the VQGAN autoencoder
          file of the original modelscope not actually used in A1111?</p>

          '
        raw: 'I''ve been trying to run this model both with the Huggingface API and
          A1111, and I''ve noticed that the A1111 text2video extension requires the
          original modelscope VQGAN autoencoder file to be downloaded (5.21 GB here:
          https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/tree/main),
          but this zeroscope model has a much smaller VAE file (167 MB here: https://huggingface.co/cerspense/zeroscope_v2_576w/tree/main/vae).
          Are these model files equivalent, despite their size difference? If not,
          does it mean that the A1111 pipeline is different from the Huggingface API
          pipeline in terms of computation and results, or is the VQGAN autoencoder
          file of the original modelscope not actually used in A1111?'
        updatedAt: '2023-07-04T12:38:53.851Z'
      numEdits: 0
      reactions: []
    id: 64a412ddc6e1167c3c50a13d
    type: comment
  author: bacprop
  content: 'I''ve been trying to run this model both with the Huggingface API and
    A1111, and I''ve noticed that the A1111 text2video extension requires the original
    modelscope VQGAN autoencoder file to be downloaded (5.21 GB here: https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/tree/main),
    but this zeroscope model has a much smaller VAE file (167 MB here: https://huggingface.co/cerspense/zeroscope_v2_576w/tree/main/vae).
    Are these model files equivalent, despite their size difference? If not, does
    it mean that the A1111 pipeline is different from the Huggingface API pipeline
    in terms of computation and results, or is the VQGAN autoencoder file of the original
    modelscope not actually used in A1111?'
  created_at: 2023-07-04 11:38:53+00:00
  edited: false
  hidden: false
  id: 64a412ddc6e1167c3c50a13d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 10
repo_id: cerspense/zeroscope_v2_576w
repo_type: model
status: open
target_branch: null
title: Difference between this model's VAE and modelscope's VQGAN
