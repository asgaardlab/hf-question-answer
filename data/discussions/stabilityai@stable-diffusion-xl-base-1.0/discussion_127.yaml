!!python/object:huggingface_hub.community.DiscussionWithDetails
author: LynchNancy
conflicting_files: null
created_at: 2023-12-03 06:18:17+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/07bd2aa067e13811007741b41b52598e.svg
      fullname: Li
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: LynchNancy
      type: user
    createdAt: '2023-12-03T06:18:17.000Z'
    data:
      edited: false
      editors:
      - LynchNancy
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5190930366516113
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/07bd2aa067e13811007741b41b52598e.svg
          fullname: Li
          isHf: false
          isPro: false
          name: LynchNancy
          type: user
        html: "<p>my program code is as follows , but when I run this code ,it cannot\
          \ give my right response , it will gives me\" pic should be Tensor or ndarray.\
          \ Got &lt;class 'PIL.Image.Image'&gt;.\"    what's the problem ,  which\
          \ master can help me  ,thank you!!<br>from flask import Flask, request,\
          \ make_response<br>from diffusers import DiffusionPipeline<br>import torch<br>from\
          \ torchvision.transforms import ToPILImage<br>from io import BytesIO</p>\n\
          <p>app = Flask(<strong>name</strong>)</p>\n<h1 id=\"load-both-base--refiner\"\
          >load both base &amp; refiner</h1>\n<p>base = DiffusionPipeline.from_pretrained(<br>\
          \ \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16,\
          \ variant=\"fp16\", use_safetensors=True<br>)<br>base.to(\"cuda\")<br>refiner\
          \ = DiffusionPipeline.from_pretrained(<br> \"stabilityai/stable-diffusion-xl-refiner-1.0\"\
          ,<br> text_encoder_2=base.text_encoder_2,<br> vae=base.vae,<br> torch_dtype=torch.float16,<br>\
          \ use_safetensors=True,<br> variant=\"fp16\",<br>)<br>refiner.to(\"cuda\"\
          )</p>\n<p><span data-props=\"{&quot;user&quot;:&quot;app&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/app\">@<span class=\"\
          underline\">app</span></a></span>\n\n\t</span></span>.route('/predict',\
          \ methods=['POST'])<br>def predict():<br> data = request.json<br> prompt\
          \ = data['text']</p>\n<h1 id=\"define-how-many-steps-and-what--of-steps-to-be-run-on-each-experts-8020-here\"\
          >Define how many steps and what % of steps to be run on each experts (80/20)\
          \ here</h1>\n<p> n_steps = 40<br> high_noise_frac = 0.8</p>\n<h1 id=\"run-both-experts\"\
          >run both experts</h1>\n<p> image = base(<br>    prompt=prompt,<br>    num_inference_steps=n_steps,<br>\
          \    denoising_end=high_noise_frac,<br>    output_type=\"latent\",<br> ).images<br>\
          \ image = refiner(<br>    prompt=prompt,<br>    num_inference_steps=n_steps,<br>\
          \    denoising_start=high_noise_frac,<br>    image=image,<br> ).images[0]</p>\n\
          <h1 id=\"convert-the-tensor-to-pil-image\">Convert the Tensor to PIL Image</h1>\n\
          <p> to_pil = ToPILImage()<br> pil_image = to_pil(image)</p>\n<h1 id=\"convert-the-pil-image-to-jpeg-format\"\
          >Convert the PIL Image to JPEG format</h1>\n<p> output = BytesIO()<br> pil_image.save(output,\
          \ format='JPEG')<br> jpeg = output.getvalue()</p>\n<h1 id=\"create-a-response-object\"\
          >Create a response object</h1>\n<p> resp = make_response(jpeg)</p>\n<h1\
          \ id=\"set-the-responses-headers\">Set the response's headers</h1>\n<p>\
          \ resp.headers['Content-Type'] = 'image/jpeg'</p>\n<p> return resp</p>\n\
          <p>if <strong>name</strong> == '<strong>main</strong>':<br> app.run(port=5000)</p>\n"
        raw: "my program code is as follows , but when I run this code ,it cannot\
          \ give my right response , it will gives me\" pic should be Tensor or ndarray.\
          \ Got <class 'PIL.Image.Image'>.\"    what's the problem ,  which master\
          \ can help me  ,thank you!!\r\nfrom flask import Flask, request, make_response\r\
          \nfrom diffusers import DiffusionPipeline\r\nimport torch\r\nfrom torchvision.transforms\
          \ import ToPILImage\r\nfrom io import BytesIO\r\n\r\napp = Flask(__name__)\r\
          \n\r\n# load both base & refiner\r\nbase = DiffusionPipeline.from_pretrained(\r\
          \n \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16,\
          \ variant=\"fp16\", use_safetensors=True\r\n)\r\nbase.to(\"cuda\")\r\nrefiner\
          \ = DiffusionPipeline.from_pretrained(\r\n \"stabilityai/stable-diffusion-xl-refiner-1.0\"\
          ,\r\n text_encoder_2=base.text_encoder_2,\r\n vae=base.vae,\r\n torch_dtype=torch.float16,\r\
          \n use_safetensors=True,\r\n variant=\"fp16\",\r\n)\r\nrefiner.to(\"cuda\"\
          )\r\n\r\n@app.route('/predict', methods=['POST'])\r\ndef predict():\r\n\
          \ data = request.json\r\n prompt = data['text']\r\n\r\n # Define how many\
          \ steps and what % of steps to be run on each experts (80/20) here\r\n n_steps\
          \ = 40\r\n high_noise_frac = 0.8\r\n\r\n # run both experts\r\n image =\
          \ base(\r\n    prompt=prompt,\r\n    num_inference_steps=n_steps,\r\n  \
          \  denoising_end=high_noise_frac,\r\n    output_type=\"latent\",\r\n ).images\r\
          \n image = refiner(\r\n    prompt=prompt,\r\n    num_inference_steps=n_steps,\r\
          \n    denoising_start=high_noise_frac,\r\n    image=image,\r\n ).images[0]\r\
          \n\r\n # Convert the Tensor to PIL Image\r\n to_pil = ToPILImage()\r\n pil_image\
          \ = to_pil(image)\r\n\r\n # Convert the PIL Image to JPEG format\r\n output\
          \ = BytesIO()\r\n pil_image.save(output, format='JPEG')\r\n jpeg = output.getvalue()\r\
          \n\r\n # Create a response object\r\n resp = make_response(jpeg)\r\n\r\n\
          \ # Set the response's headers\r\n resp.headers['Content-Type'] = 'image/jpeg'\r\
          \n\r\n return resp\r\n\r\nif __name__ == '__main__':\r\n app.run(port=5000)"
        updatedAt: '2023-12-03T06:18:17.427Z'
      numEdits: 0
      reactions: []
    id: 656c1da9ae32721645880373
    type: comment
  author: LynchNancy
  content: "my program code is as follows , but when I run this code ,it cannot give\
    \ my right response , it will gives me\" pic should be Tensor or ndarray. Got\
    \ <class 'PIL.Image.Image'>.\"    what's the problem ,  which master can help\
    \ me  ,thank you!!\r\nfrom flask import Flask, request, make_response\r\nfrom\
    \ diffusers import DiffusionPipeline\r\nimport torch\r\nfrom torchvision.transforms\
    \ import ToPILImage\r\nfrom io import BytesIO\r\n\r\napp = Flask(__name__)\r\n\
    \r\n# load both base & refiner\r\nbase = DiffusionPipeline.from_pretrained(\r\n\
    \ \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"\
    fp16\", use_safetensors=True\r\n)\r\nbase.to(\"cuda\")\r\nrefiner = DiffusionPipeline.from_pretrained(\r\
    \n \"stabilityai/stable-diffusion-xl-refiner-1.0\",\r\n text_encoder_2=base.text_encoder_2,\r\
    \n vae=base.vae,\r\n torch_dtype=torch.float16,\r\n use_safetensors=True,\r\n\
    \ variant=\"fp16\",\r\n)\r\nrefiner.to(\"cuda\")\r\n\r\n@app.route('/predict',\
    \ methods=['POST'])\r\ndef predict():\r\n data = request.json\r\n prompt = data['text']\r\
    \n\r\n # Define how many steps and what % of steps to be run on each experts (80/20)\
    \ here\r\n n_steps = 40\r\n high_noise_frac = 0.8\r\n\r\n # run both experts\r\
    \n image = base(\r\n    prompt=prompt,\r\n    num_inference_steps=n_steps,\r\n\
    \    denoising_end=high_noise_frac,\r\n    output_type=\"latent\",\r\n ).images\r\
    \n image = refiner(\r\n    prompt=prompt,\r\n    num_inference_steps=n_steps,\r\
    \n    denoising_start=high_noise_frac,\r\n    image=image,\r\n ).images[0]\r\n\
    \r\n # Convert the Tensor to PIL Image\r\n to_pil = ToPILImage()\r\n pil_image\
    \ = to_pil(image)\r\n\r\n # Convert the PIL Image to JPEG format\r\n output =\
    \ BytesIO()\r\n pil_image.save(output, format='JPEG')\r\n jpeg = output.getvalue()\r\
    \n\r\n # Create a response object\r\n resp = make_response(jpeg)\r\n\r\n # Set\
    \ the response's headers\r\n resp.headers['Content-Type'] = 'image/jpeg'\r\n\r\
    \n return resp\r\n\r\nif __name__ == '__main__':\r\n app.run(port=5000)"
  created_at: 2023-12-03 06:18:17+00:00
  edited: false
  hidden: false
  id: 656c1da9ae32721645880373
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5b8323ebdd71a764e5bbdf72489261fd.svg
      fullname: Zack TM
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Zacktm
      type: user
    createdAt: '2023-12-26T11:44:31.000Z'
    data:
      edited: true
      editors:
      - Zacktm
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.698477029800415
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5b8323ebdd71a764e5bbdf72489261fd.svg
          fullname: Zack TM
          isHf: false
          isPro: false
          name: Zacktm
          type: user
        html: '<p>The simplest way is to use FastAPI<br><a rel="nofollow" href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a></p>

          '
        raw: 'The simplest way is to use FastAPI

          https://fastapi.tiangolo.com/

          '
        updatedAt: '2023-12-26T11:44:44.724Z'
      numEdits: 1
      reactions: []
    id: 658abc9fe878be571b8cc140
    type: comment
  author: Zacktm
  content: 'The simplest way is to use FastAPI

    https://fastapi.tiangolo.com/

    '
  created_at: 2023-12-26 11:44:31+00:00
  edited: true
  hidden: false
  id: 658abc9fe878be571b8cc140
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 127
repo_id: stabilityai/stable-diffusion-xl-base-1.0
repo_type: model
status: open
target_branch: null
title: how to create my API server for this model
