!!python/object:huggingface_hub.community.DiscussionWithDetails
author: DangerD
conflicting_files: null
created_at: 2023-08-12 18:28:33+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
      fullname: Dmitriy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DangerD
      type: user
    createdAt: '2023-08-12T19:28:33.000Z'
    data:
      edited: false
      editors:
      - DangerD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5403386950492859
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
          fullname: Dmitriy
          isHf: false
          isPro: false
          name: DangerD
          type: user
        html: '<p>Using this model users get this:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/zK6_bUfwdXXLACmjeTSCG.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/zK6_bUfwdXXLACmjeTSCG.png"></a><br>And
          now with same request i get this:</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/7tFbyH2ChhXWREF8ZlYMa.png"><img
          alt="image.png" src="https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/7tFbyH2ChhXWREF8ZlYMa.png"></a><br>What
          i''ve missed?)</p>

          '
        raw: "Using this model users get this:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/zK6_bUfwdXXLACmjeTSCG.png)\r\
          \nAnd now with same request i get this:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/7tFbyH2ChhXWREF8ZlYMa.png)\r\
          \nWhat i've missed?)"
        updatedAt: '2023-08-12T19:28:33.185Z'
      numEdits: 0
      reactions: []
    id: 64d7dd61a3c9b92761a9b739
    type: comment
  author: DangerD
  content: "Using this model users get this:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/zK6_bUfwdXXLACmjeTSCG.png)\r\
    \nAnd now with same request i get this:\r\n\r\n![image.png](https://cdn-uploads.huggingface.co/production/uploads/64d7dcc4d9d00dcb119165c8/7tFbyH2ChhXWREF8ZlYMa.png)\r\
    \nWhat i've missed?)"
  created_at: 2023-08-12 18:28:33+00:00
  edited: false
  hidden: false
  id: 64d7dd61a3c9b92761a9b739
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c87e68f3d0ae0832bceeee971bbbf497.svg
      fullname: Martin Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MartinMC97
      type: user
    createdAt: '2023-08-14T06:03:37.000Z'
    data:
      edited: false
      editors:
      - MartinMC97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9659382700920105
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c87e68f3d0ae0832bceeee971bbbf497.svg
          fullname: Martin Ma
          isHf: false
          isPro: false
          name: MartinMC97
          type: user
        html: '<p>Set both width and height to 1024 and try again.</p>

          '
        raw: Set both width and height to 1024 and try again.
        updatedAt: '2023-08-14T06:03:37.182Z'
      numEdits: 0
      reactions: []
    id: 64d9c3b9aa89fb548afa3606
    type: comment
  author: MartinMC97
  content: Set both width and height to 1024 and try again.
  created_at: 2023-08-14 05:03:37+00:00
  edited: false
  hidden: false
  id: 64d9c3b9aa89fb548afa3606
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
      fullname: Dmitriy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DangerD
      type: user
    createdAt: '2023-08-14T07:34:47.000Z'
    data:
      edited: false
      editors:
      - DangerD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9571298360824585
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
          fullname: Dmitriy
          isHf: false
          isPro: false
          name: DangerD
          type: user
        html: '<blockquote>

          <p>Set both width and height to 1024 and try again.</p>

          </blockquote>

          <p>Thank you, that works, but now I''m getting error due to lack of memory,
          11GB is not enough, any ideas? </p>

          '
        raw: '> Set both width and height to 1024 and try again.


          Thank you, that works, but now I''m getting error due to lack of memory,
          11GB is not enough, any ideas? '
        updatedAt: '2023-08-14T07:34:47.282Z'
      numEdits: 0
      reactions: []
    id: 64d9d917d1a2566da4bf807e
    type: comment
  author: DangerD
  content: '> Set both width and height to 1024 and try again.


    Thank you, that works, but now I''m getting error due to lack of memory, 11GB
    is not enough, any ideas? '
  created_at: 2023-08-14 06:34:47+00:00
  edited: false
  hidden: false
  id: 64d9d917d1a2566da4bf807e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c87e68f3d0ae0832bceeee971bbbf497.svg
      fullname: Martin Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MartinMC97
      type: user
    createdAt: '2023-08-14T07:49:19.000Z'
    data:
      edited: true
      editors:
      - MartinMC97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8325381278991699
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c87e68f3d0ae0832bceeee971bbbf497.svg
          fullname: Martin Ma
          isHf: false
          isPro: false
          name: MartinMC97
          type: user
        html: '<blockquote>

          <p>Thank you, that works, but now I''m getting error due to lack of memory,
          11GB is not enough, any ideas?</p>

          </blockquote>

          <p><a rel="nofollow" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting</a><br>Since
          you are using auto1111 webui, maybe you can try the following solutions
          posted in their troubleshooting page.</p>

          <blockquote>

          <p>The program needs 16gb of regular RAM to run smoothly. If you have 8gb
          RAM, consider making an 8gb page file/swap file, or use the <code>--lowram</code>
          option (if you have more gpu vram than ram). </p>

          </blockquote>

          '
        raw: '> Thank you, that works, but now I''m getting error due to lack of memory,
          11GB is not enough, any ideas?


          https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting

          Since you are using auto1111 webui, maybe you can try the following solutions
          posted in their troubleshooting page.

          > The program needs 16gb of regular RAM to run smoothly. If you have 8gb
          RAM, consider making an 8gb page file/swap file, or use the `--lowram` option
          (if you have more gpu vram than ram). '
        updatedAt: '2023-08-14T07:50:35.593Z'
      numEdits: 1
      reactions: []
    id: 64d9dc7f2fe2c112647284f9
    type: comment
  author: MartinMC97
  content: '> Thank you, that works, but now I''m getting error due to lack of memory,
    11GB is not enough, any ideas?


    https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting

    Since you are using auto1111 webui, maybe you can try the following solutions
    posted in their troubleshooting page.

    > The program needs 16gb of regular RAM to run smoothly. If you have 8gb RAM,
    consider making an 8gb page file/swap file, or use the `--lowram` option (if you
    have more gpu vram than ram). '
  created_at: 2023-08-14 06:49:19+00:00
  edited: true
  hidden: false
  id: 64d9dc7f2fe2c112647284f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
      fullname: Dmitriy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DangerD
      type: user
    createdAt: '2023-08-14T13:55:44.000Z'
    data:
      edited: false
      editors:
      - DangerD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.837497353553772
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
          fullname: Dmitriy
          isHf: false
          isPro: false
          name: DangerD
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Thank you, that works, but now I''m getting error due to lack of memory,
          11GB is not enough, any ideas?</p>

          </blockquote>

          <p><a rel="nofollow" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting</a><br>Since
          you are using auto1111 webui, maybe you can try the following solutions
          posted in their troubleshooting page.</p>

          <blockquote>

          <p>The program needs 16gb of regular RAM to run smoothly. If you have 8gb
          RAM, consider making an 8gb page file/swap file, or use the <code>--lowram</code>
          option (if you have more gpu vram than ram).</p>

          </blockquote>

          </blockquote>

          <p>Hmm, I have 64GB RAM and 11GB GPU (1080Ti)</p>

          '
        raw: "> > Thank you, that works, but now I'm getting error due to lack of\
          \ memory, 11GB is not enough, any ideas?\n> \n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting\n\
          > Since you are using auto1111 webui, maybe you can try the following solutions\
          \ posted in their troubleshooting page.\n> > The program needs 16gb of regular\
          \ RAM to run smoothly. If you have 8gb RAM, consider making an 8gb page\
          \ file/swap file, or use the `--lowram` option (if you have more gpu vram\
          \ than ram).\n\nHmm, I have 64GB RAM and 11GB GPU (1080Ti)"
        updatedAt: '2023-08-14T13:55:44.952Z'
      numEdits: 0
      reactions: []
    id: 64da3260b5d625e0e96413a0
    type: comment
  author: DangerD
  content: "> > Thank you, that works, but now I'm getting error due to lack of memory,\
    \ 11GB is not enough, any ideas?\n> \n> https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting\n\
    > Since you are using auto1111 webui, maybe you can try the following solutions\
    \ posted in their troubleshooting page.\n> > The program needs 16gb of regular\
    \ RAM to run smoothly. If you have 8gb RAM, consider making an 8gb page file/swap\
    \ file, or use the `--lowram` option (if you have more gpu vram than ram).\n\n\
    Hmm, I have 64GB RAM and 11GB GPU (1080Ti)"
  created_at: 2023-08-14 12:55:44+00:00
  edited: false
  hidden: false
  id: 64da3260b5d625e0e96413a0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c87e68f3d0ae0832bceeee971bbbf497.svg
      fullname: Martin Ma
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: MartinMC97
      type: user
    createdAt: '2023-08-15T00:54:42.000Z'
    data:
      edited: false
      editors:
      - MartinMC97
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.747948944568634
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c87e68f3d0ae0832bceeee971bbbf497.svg
          fullname: Martin Ma
          isHf: false
          isPro: false
          name: MartinMC97
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>Thank you, that works, but now I''m getting error due to lack of memory,
          11GB is not enough, any ideas?</p>

          </blockquote>

          <p><a rel="nofollow" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting</a><br>Since
          you are using auto1111 webui, maybe you can try the following solutions
          posted in their troubleshooting page.</p>

          <blockquote>

          <p>The program needs 16gb of regular RAM to run smoothly. If you have 8gb
          RAM, consider making an 8gb page file/swap file, or use the <code>--lowram</code>
          option (if you have more gpu vram than ram).</p>

          </blockquote>

          </blockquote>

          <p>Hmm, I have 64GB RAM and 11GB GPU (1080Ti)</p>

          </blockquote>

          <p><a rel="nofollow" href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations">https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations</a><br>Try
          adding <code>--medvram</code> or <code>--lowvram</code> when starting.</p>

          '
        raw: "> > > Thank you, that works, but now I'm getting error due to lack of\
          \ memory, 11GB is not enough, any ideas?\n> > \n> > https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting\n\
          > > Since you are using auto1111 webui, maybe you can try the following\
          \ solutions posted in their troubleshooting page.\n> > > The program needs\
          \ 16gb of regular RAM to run smoothly. If you have 8gb RAM, consider making\
          \ an 8gb page file/swap file, or use the `--lowram` option (if you have\
          \ more gpu vram than ram).\n> \n> Hmm, I have 64GB RAM and 11GB GPU (1080Ti)\n\
          \nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations\n\
          Try adding `--medvram` or `--lowvram` when starting."
        updatedAt: '2023-08-15T00:54:42.302Z'
      numEdits: 0
      reactions: []
    id: 64daccd2978d1f9fafc68ac3
    type: comment
  author: MartinMC97
  content: "> > > Thank you, that works, but now I'm getting error due to lack of\
    \ memory, 11GB is not enough, any ideas?\n> > \n> > https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Troubleshooting\n\
    > > Since you are using auto1111 webui, maybe you can try the following solutions\
    \ posted in their troubleshooting page.\n> > > The program needs 16gb of regular\
    \ RAM to run smoothly. If you have 8gb RAM, consider making an 8gb page file/swap\
    \ file, or use the `--lowram` option (if you have more gpu vram than ram).\n>\
    \ \n> Hmm, I have 64GB RAM and 11GB GPU (1080Ti)\n\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Optimizations\n\
    Try adding `--medvram` or `--lowvram` when starting."
  created_at: 2023-08-14 23:54:42+00:00
  edited: false
  hidden: false
  id: 64daccd2978d1f9fafc68ac3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5l_zcsisBimF2oRVAeVkU.jpeg?w=200&h=200&f=face
      fullname: shay bc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shaybc
      type: user
    createdAt: '2023-08-18T19:21:27.000Z'
    data:
      edited: true
      editors:
      - shaybc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6505827903747559
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5l_zcsisBimF2oRVAeVkU.jpeg?w=200&h=200&f=face
          fullname: shay bc
          isHf: false
          isPro: false
          name: shaybc
          type: user
        html: "<blockquote>\n<p>Hmm, I have 64GB RAM and 11GB GPU (1080Ti)</p>\n</blockquote>\n\
          <p>i wonder, can you share how much time it took you to generate such an\
          \ image ?</p>\n<p>for some reason i try using a simple code using gradio\
          \ with RTX-2060 6G (64G OS RAM, amd ryzen 5 3400) and i get 15 hours and\
          \ counting !! for asimple prompt as \"car\"</p>\n<p>the code i use: </p>\n\
          <pre><code>import gradio as gr\nimport torch\nfrom diffusers import DiffusionPipeline\n\
          \n\n# this code runs a gradio interface that allows the user to write a\
          \ prompt,\n# and generate an image based on the prompt\n# the gradio interface\
          \ launche uses about 1.5GB of RAM, and the model uses about 4GB of VRAM\
          \ on a GPU\n# on the nVidia RTX-2060 GPU card it takes about 6.2 hours to\
          \ generate an image with 25 steps of noise reduction\n\n\n# infer function\
          \ is the function that will be called when the user clicks submit\n# parameters\
          \ explanation can be found here: https://getimg.ai/guides/interactive-guide-to-stable-diffusion-steps-parameter\n\
          # this function receives:\n# prompt - the prompt the user has entered and\
          \ want the model to generate the image based on\ndef infer(prompt):\n  \
          \  # start the timer\n    timer.start()\n    # call the model through the\
          \ pipe and supply it with the parameters, get the generated images, and\
          \ return the first image\n    image = pipe(prompt=prompt).images[0]\n  \
          \  # stop the timer\n    timer.stop()\n    # print the time it took to generate\
          \ the image\n    print(f\"Time to generate image: {timer.get():.2f}s\")\n\
          \    # return the generated image and the time it took to generate it\n\
          \    return image, f\"Time to generate image: {timer.get():.2f}s\"\n   \
          \ \n\n\n# print the torch version and check if cuda is available\nprint(f\"\
          CUDA version: {torch.version.cuda}\")\nprint(f\"cuda is available: {torch.cuda.is_available()}\"\
          )\n\n# create a timer to measure the time it takes to generate the image\n\
          timer = gr.Timer()\n\n# check if we are running on a GPU\nif torch.cuda.is_available():\n\
          \    # if yes, use a torch.float16 dtype for the model, create diffusion\
          \ pipeline from a pretrained model stable-diffusion-xl-base-1.0\n    pipe\
          \ = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\"\
          , torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n\n\
          \    # set the device variable to cuda (nVidia GPU)\n    device = \"cuda\"\
          \n    \n    # move the model to the GPU\n    pipe = pipe.to(device)\nelse:\n\
          \    # if no, use a torch.float32 dtype for the model, create diffusion\
          \ pipeline from a pretrained model stable-diffusion-xl-base-1.0\n    pipe\
          \ = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\"\
          )\n    \n    # set the device variable to cuda (nVidia GPU)\n    device\
          \ = \"cpu\"\n    \n    # move the model to the CPU\n    pipe = pipe.to(device)\n\
          \n\n# launch the gradio interface, set the function to infer, set the inputs\
          \ to a UI element for each input parameter, set the output to an image,\
          \ and launch the interface\ngr.Interface(fn=infer, \n             inputs=[gr.Textbox(label\
          \ = 'Prompt Input Text. 77 Token (Keyword or Symbol) Maximum')], \n    \
          \         outputs=['image', 'text'],\n             title = \"Stable Diffusion\
          \ XL 1.0\", \n             description = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\
          , \n             article = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\
          ).queue(max_size=5).launch()\n</code></pre>\n<p>this is what the VSCode\
          \ terminal show me after 14+ hours:</p>\n<p><a rel=\"nofollow\" href=\"\
          https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/8Axq-FZPVgVsUjBCoRqgv.png\"\
          ><img alt=\"VSCode-terminal.png\" src=\"https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/8Axq-FZPVgVsUjBCoRqgv.png\"\
          ></a></p>\n"
        raw: "> Hmm, I have 64GB RAM and 11GB GPU (1080Ti)\n\ni wonder, can you share\
          \ how much time it took you to generate such an image ?\n\nfor some reason\
          \ i try using a simple code using gradio with RTX-2060 6G (64G OS RAM, amd\
          \ ryzen 5 3400) and i get 15 hours and counting !! for asimple prompt as\
          \ \"car\"\n\nthe code i use: \n\n```\nimport gradio as gr\nimport torch\n\
          from diffusers import DiffusionPipeline\n\n\n# this code runs a gradio interface\
          \ that allows the user to write a prompt,\n# and generate an image based\
          \ on the prompt\n# the gradio interface launche uses about 1.5GB of RAM,\
          \ and the model uses about 4GB of VRAM on a GPU\n# on the nVidia RTX-2060\
          \ GPU card it takes about 6.2 hours to generate an image with 25 steps of\
          \ noise reduction\n\n\n# infer function is the function that will be called\
          \ when the user clicks submit\n# parameters explanation can be found here:\
          \ https://getimg.ai/guides/interactive-guide-to-stable-diffusion-steps-parameter\n\
          # this function receives:\n# prompt - the prompt the user has entered and\
          \ want the model to generate the image based on\ndef infer(prompt):\n  \
          \  # start the timer\n    timer.start()\n    # call the model through the\
          \ pipe and supply it with the parameters, get the generated images, and\
          \ return the first image\n    image = pipe(prompt=prompt).images[0]\n  \
          \  # stop the timer\n    timer.stop()\n    # print the time it took to generate\
          \ the image\n    print(f\"Time to generate image: {timer.get():.2f}s\")\n\
          \    # return the generated image and the time it took to generate it\n\
          \    return image, f\"Time to generate image: {timer.get():.2f}s\"\n   \
          \ \n\n\n# print the torch version and check if cuda is available\nprint(f\"\
          CUDA version: {torch.version.cuda}\")\nprint(f\"cuda is available: {torch.cuda.is_available()}\"\
          )\n\n# create a timer to measure the time it takes to generate the image\n\
          timer = gr.Timer()\n\n# check if we are running on a GPU\nif torch.cuda.is_available():\n\
          \    # if yes, use a torch.float16 dtype for the model, create diffusion\
          \ pipeline from a pretrained model stable-diffusion-xl-base-1.0\n    pipe\
          \ = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\"\
          , torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n\n\
          \    # set the device variable to cuda (nVidia GPU)\n    device = \"cuda\"\
          \n    \n    # move the model to the GPU\n    pipe = pipe.to(device)\nelse:\n\
          \    # if no, use a torch.float32 dtype for the model, create diffusion\
          \ pipeline from a pretrained model stable-diffusion-xl-base-1.0\n    pipe\
          \ = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\"\
          )\n    \n    # set the device variable to cuda (nVidia GPU)\n    device\
          \ = \"cpu\"\n    \n    # move the model to the CPU\n    pipe = pipe.to(device)\n\
          \n\n# launch the gradio interface, set the function to infer, set the inputs\
          \ to a UI element for each input parameter, set the output to an image,\
          \ and launch the interface\ngr.Interface(fn=infer, \n             inputs=[gr.Textbox(label\
          \ = 'Prompt Input Text. 77 Token (Keyword or Symbol) Maximum')], \n    \
          \         outputs=['image', 'text'],\n             title = \"Stable Diffusion\
          \ XL 1.0\", \n             description = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\
          , \n             article = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\
          ).queue(max_size=5).launch()\n```\n\nthis is what the VSCode terminal show\
          \ me after 14+ hours:\n\n![VSCode-terminal.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/8Axq-FZPVgVsUjBCoRqgv.png)\n"
        updatedAt: '2023-08-18T19:44:15.046Z'
      numEdits: 2
      reactions: []
    id: 64dfc4b758076dcc98569b76
    type: comment
  author: shaybc
  content: "> Hmm, I have 64GB RAM and 11GB GPU (1080Ti)\n\ni wonder, can you share\
    \ how much time it took you to generate such an image ?\n\nfor some reason i try\
    \ using a simple code using gradio with RTX-2060 6G (64G OS RAM, amd ryzen 5 3400)\
    \ and i get 15 hours and counting !! for asimple prompt as \"car\"\n\nthe code\
    \ i use: \n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import DiffusionPipeline\n\
    \n\n# this code runs a gradio interface that allows the user to write a prompt,\n\
    # and generate an image based on the prompt\n# the gradio interface launche uses\
    \ about 1.5GB of RAM, and the model uses about 4GB of VRAM on a GPU\n# on the\
    \ nVidia RTX-2060 GPU card it takes about 6.2 hours to generate an image with\
    \ 25 steps of noise reduction\n\n\n# infer function is the function that will\
    \ be called when the user clicks submit\n# parameters explanation can be found\
    \ here: https://getimg.ai/guides/interactive-guide-to-stable-diffusion-steps-parameter\n\
    # this function receives:\n# prompt - the prompt the user has entered and want\
    \ the model to generate the image based on\ndef infer(prompt):\n    # start the\
    \ timer\n    timer.start()\n    # call the model through the pipe and supply it\
    \ with the parameters, get the generated images, and return the first image\n\
    \    image = pipe(prompt=prompt).images[0]\n    # stop the timer\n    timer.stop()\n\
    \    # print the time it took to generate the image\n    print(f\"Time to generate\
    \ image: {timer.get():.2f}s\")\n    # return the generated image and the time\
    \ it took to generate it\n    return image, f\"Time to generate image: {timer.get():.2f}s\"\
    \n    \n\n\n# print the torch version and check if cuda is available\nprint(f\"\
    CUDA version: {torch.version.cuda}\")\nprint(f\"cuda is available: {torch.cuda.is_available()}\"\
    )\n\n# create a timer to measure the time it takes to generate the image\ntimer\
    \ = gr.Timer()\n\n# check if we are running on a GPU\nif torch.cuda.is_available():\n\
    \    # if yes, use a torch.float16 dtype for the model, create diffusion pipeline\
    \ from a pretrained model stable-diffusion-xl-base-1.0\n    pipe = DiffusionPipeline.from_pretrained(\"\
    stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True,\
    \ variant=\"fp16\")\n\n    # set the device variable to cuda (nVidia GPU)\n  \
    \  device = \"cuda\"\n    \n    # move the model to the GPU\n    pipe = pipe.to(device)\n\
    else:\n    # if no, use a torch.float32 dtype for the model, create diffusion\
    \ pipeline from a pretrained model stable-diffusion-xl-base-1.0\n    pipe = DiffusionPipeline.from_pretrained(\"\
    stabilityai/stable-diffusion-xl-base-1.0\")\n    \n    # set the device variable\
    \ to cuda (nVidia GPU)\n    device = \"cpu\"\n    \n    # move the model to the\
    \ CPU\n    pipe = pipe.to(device)\n\n\n# launch the gradio interface, set the\
    \ function to infer, set the inputs to a UI element for each input parameter,\
    \ set the output to an image, and launch the interface\ngr.Interface(fn=infer,\
    \ \n             inputs=[gr.Textbox(label = 'Prompt Input Text. 77 Token (Keyword\
    \ or Symbol) Maximum')], \n             outputs=['image', 'text'],\n         \
    \    title = \"Stable Diffusion XL 1.0\", \n             description = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\
    , \n             article = \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0\"\
    ).queue(max_size=5).launch()\n```\n\nthis is what the VSCode terminal show me\
    \ after 14+ hours:\n\n![VSCode-terminal.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/8Axq-FZPVgVsUjBCoRqgv.png)\n"
  created_at: 2023-08-18 18:21:27+00:00
  edited: true
  hidden: false
  id: 64dfc4b758076dcc98569b76
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5l_zcsisBimF2oRVAeVkU.jpeg?w=200&h=200&f=face
      fullname: shay bc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shaybc
      type: user
    createdAt: '2023-08-18T22:47:26.000Z'
    data:
      edited: false
      editors:
      - shaybc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7904542684555054
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5l_zcsisBimF2oRVAeVkU.jpeg?w=200&h=200&f=face
          fullname: shay bc
          isHf: false
          isPro: false
          name: shaybc
          type: user
        html: '<p>never mind, i changed my code to the simple hello world example
          in the SDXL ReadMe, reduced the number of steps from 50 to 25,  and it took
          about 11 minutes,</p>

          <p>now i use this code:</p>

          <pre><code>import time

          from diffusers import DiffusionPipeline

          import torch


          prompt = "An astronaut riding a green horse"

          numOfSteps = 25


          # save the current millisecond it will be used to calculate the time it
          takes to generate the image

          start = time.time()


          # create a pipeline from a pretrained model stable-diffusion-xl-base-1.0
          with torch.float16 dtype

          pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0",
          torch_dtype=torch.float16, use_safetensors=True, variant="fp16")

          pipe.to("cuda")


          # generate the image

          generated_image = pipe(prompt=prompt, num_inference_steps=numOfSteps).images[0]


          # print the time it took to generate the image

          print(f"Time to generate image in {numOfSteps} steps: {time.time() - start:.2f}s")


          # save the generated image to a file

          generated_image.save(f''generated_image_{numOfSteps}.png'')

          </code></pre>

          <p>in the gradio code i saw the GPU 6G memory always on 95% and the GPU
          3D graph (on the task manager) at 100% all the time, now the memory is still
          occupied during the generation process, but the 3d graph is not used at
          all, CUDA graph is used between 85% - 100%, and GPU ran in 48c - 51c temperature,
          so the problem was probably gradio,</p>

          <p>i have ran steps between 2 to 25 and these are the generate times i got
          so far (i noticed there is a slight influence based on the style of image
          generated):<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/BQDhxMZt2iyxjKDd_M775.png"><img
          alt="stats.png" src="https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/BQDhxMZt2iyxjKDd_M775.png"></a></p>

          <p>here is an example of an image i got after 6:32 minutes with 14 steps:<br><a
          rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/JVKU-lCZ7AOwLM7Do63E8.png"><img
          alt="generated_image_14.png" src="https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/JVKU-lCZ7AOwLM7Do63E8.png"></a></p>

          <p>and in 8:16 minutes in 17 steps:<br><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/xElFFvPcmMhXfIPQwCNJD.png"><img
          alt="generated_image_17.png" src="https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/xElFFvPcmMhXfIPQwCNJD.png"></a></p>

          '
        raw: 'never mind, i changed my code to the simple hello world example in the
          SDXL ReadMe, reduced the number of steps from 50 to 25,  and it took about
          11 minutes,


          now i use this code:


          ```

          import time

          from diffusers import DiffusionPipeline

          import torch


          prompt = "An astronaut riding a green horse"

          numOfSteps = 25


          # save the current millisecond it will be used to calculate the time it
          takes to generate the image

          start = time.time()


          # create a pipeline from a pretrained model stable-diffusion-xl-base-1.0
          with torch.float16 dtype

          pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0",
          torch_dtype=torch.float16, use_safetensors=True, variant="fp16")

          pipe.to("cuda")


          # generate the image

          generated_image = pipe(prompt=prompt, num_inference_steps=numOfSteps).images[0]


          # print the time it took to generate the image

          print(f"Time to generate image in {numOfSteps} steps: {time.time() - start:.2f}s")


          # save the generated image to a file

          generated_image.save(f''generated_image_{numOfSteps}.png'')

          ```


          in the gradio code i saw the GPU 6G memory always on 95% and the GPU 3D
          graph (on the task manager) at 100% all the time, now the memory is still
          occupied during the generation process, but the 3d graph is not used at
          all, CUDA graph is used between 85% - 100%, and GPU ran in 48c - 51c temperature,
          so the problem was probably gradio,


          i have ran steps between 2 to 25 and these are the generate times i got
          so far (i noticed there is a slight influence based on the style of image
          generated):

          ![stats.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/BQDhxMZt2iyxjKDd_M775.png)



          here is an example of an image i got after 6:32 minutes with 14 steps:

          ![generated_image_14.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/JVKU-lCZ7AOwLM7Do63E8.png)


          and in 8:16 minutes in 17 steps:

          ![generated_image_17.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/xElFFvPcmMhXfIPQwCNJD.png)

          '
        updatedAt: '2023-08-18T22:47:26.723Z'
      numEdits: 0
      reactions: []
    id: 64dff4fe4d740812bb1b35da
    type: comment
  author: shaybc
  content: 'never mind, i changed my code to the simple hello world example in the
    SDXL ReadMe, reduced the number of steps from 50 to 25,  and it took about 11
    minutes,


    now i use this code:


    ```

    import time

    from diffusers import DiffusionPipeline

    import torch


    prompt = "An astronaut riding a green horse"

    numOfSteps = 25


    # save the current millisecond it will be used to calculate the time it takes
    to generate the image

    start = time.time()


    # create a pipeline from a pretrained model stable-diffusion-xl-base-1.0 with
    torch.float16 dtype

    pipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16, use_safetensors=True, variant="fp16")

    pipe.to("cuda")


    # generate the image

    generated_image = pipe(prompt=prompt, num_inference_steps=numOfSteps).images[0]


    # print the time it took to generate the image

    print(f"Time to generate image in {numOfSteps} steps: {time.time() - start:.2f}s")


    # save the generated image to a file

    generated_image.save(f''generated_image_{numOfSteps}.png'')

    ```


    in the gradio code i saw the GPU 6G memory always on 95% and the GPU 3D graph
    (on the task manager) at 100% all the time, now the memory is still occupied during
    the generation process, but the 3d graph is not used at all, CUDA graph is used
    between 85% - 100%, and GPU ran in 48c - 51c temperature, so the problem was probably
    gradio,


    i have ran steps between 2 to 25 and these are the generate times i got so far
    (i noticed there is a slight influence based on the style of image generated):

    ![stats.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/BQDhxMZt2iyxjKDd_M775.png)



    here is an example of an image i got after 6:32 minutes with 14 steps:

    ![generated_image_14.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/JVKU-lCZ7AOwLM7Do63E8.png)


    and in 8:16 minutes in 17 steps:

    ![generated_image_17.png](https://cdn-uploads.huggingface.co/production/uploads/646c029e628e5b50b2e12ef0/xElFFvPcmMhXfIPQwCNJD.png)

    '
  created_at: 2023-08-18 21:47:26+00:00
  edited: false
  hidden: false
  id: 64dff4fe4d740812bb1b35da
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
      fullname: Dmitriy
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: DangerD
      type: user
    createdAt: '2023-08-19T05:06:22.000Z'
    data:
      edited: false
      editors:
      - DangerD
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.929411768913269
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/30d0c978dccae283d0d94a9fac28fdea.svg
          fullname: Dmitriy
          isHf: false
          isPro: false
          name: DangerD
          type: user
        html: '<blockquote>

          <blockquote>

          <p>Hmm, I have 64GB RAM and 11GB GPU (1080Ti)</p>

          </blockquote>

          <p>i wonder, can you share how much time it took you to generate such an
          image ?</p>

          </blockquote>

          <p>~1 minute, i use automatic1111 webui. (20 steps with refiner 5 steps)
          with 30/10 it''s much longer</p>

          '
        raw: "> > Hmm, I have 64GB RAM and 11GB GPU (1080Ti)\n> \n> i wonder, can\
          \ you share how much time it took you to generate such an image ?\n\n\n\
          ~1 minute, i use automatic1111 webui. (20 steps with refiner 5 steps) with\
          \ 30/10 it's much longer"
        updatedAt: '2023-08-19T05:06:22.038Z'
      numEdits: 0
      reactions: []
    id: 64e04dce12a5504dda55f92c
    type: comment
  author: DangerD
  content: "> > Hmm, I have 64GB RAM and 11GB GPU (1080Ti)\n> \n> i wonder, can you\
    \ share how much time it took you to generate such an image ?\n\n\n~1 minute,\
    \ i use automatic1111 webui. (20 steps with refiner 5 steps) with 30/10 it's much\
    \ longer"
  created_at: 2023-08-19 04:06:22+00:00
  edited: false
  hidden: false
  id: 64e04dce12a5504dda55f92c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5l_zcsisBimF2oRVAeVkU.jpeg?w=200&h=200&f=face
      fullname: shay bc
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: shaybc
      type: user
    createdAt: '2023-08-21T18:11:39.000Z'
    data:
      edited: false
      editors:
      - shaybc
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7712445855140686
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/5l_zcsisBimF2oRVAeVkU.jpeg?w=200&h=200&f=face
          fullname: shay bc
          isHf: false
          isPro: false
          name: shaybc
          type: user
        html: '<p>thanks for the reply,</p>

          <p>when i try generating 1024x1024 in 20 steps of generation on the "stabilityai/stable-diffusion-xl-base-1.0"
          model and 5 steps refinement on the "stabilityai/stable-diffusion-xl-refiner-1.0"
          model i see no difference in the resulting image then just using the 20
          steps, i don''t think the 5 steps refinement can do much good</p>

          '
        raw: 'thanks for the reply,


          when i try generating 1024x1024 in 20 steps of generation on the "stabilityai/stable-diffusion-xl-base-1.0"
          model and 5 steps refinement on the "stabilityai/stable-diffusion-xl-refiner-1.0"
          model i see no difference in the resulting image then just using the 20
          steps, i don''t think the 5 steps refinement can do much good


          '
        updatedAt: '2023-08-21T18:11:39.665Z'
      numEdits: 0
      reactions: []
    id: 64e3a8db8b5060e515a30b7b
    type: comment
  author: shaybc
  content: 'thanks for the reply,


    when i try generating 1024x1024 in 20 steps of generation on the "stabilityai/stable-diffusion-xl-base-1.0"
    model and 5 steps refinement on the "stabilityai/stable-diffusion-xl-refiner-1.0"
    model i see no difference in the resulting image then just using the 20 steps,
    i don''t think the 5 steps refinement can do much good


    '
  created_at: 2023-08-21 17:11:39+00:00
  edited: false
  hidden: false
  id: 64e3a8db8b5060e515a30b7b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 58
repo_id: stabilityai/stable-diffusion-xl-base-1.0
repo_type: model
status: open
target_branch: null
title: Very simple results...
