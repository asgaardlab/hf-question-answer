!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gbhatt2
conflicting_files: null
created_at: 2023-09-18 00:31:16+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/01087b1da11bbbcc9d9ff22390c34daa.svg
      fullname: Gantavya Bhatt
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gbhatt2
      type: user
    createdAt: '2023-09-18T01:31:16.000Z'
    data:
      edited: false
      editors:
      - gbhatt2
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.40419116616249084
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/01087b1da11bbbcc9d9ff22390c34daa.svg
          fullname: Gantavya Bhatt
          isHf: false
          isPro: false
          name: gbhatt2
          type: user
        html: '<p>Hi,<br>I am trying to run a simple batched image generation using
          : </p>

          <p>with torch.no_grad():<br>    p = "An astronaut riding a green horse"<br>    prompt
          = [p for _ in range(7)]<br>    images = pipe(prompt=prompt).images[0]</p>

          <p>It throws the following error. </p>

          <h2 id="">''''''</h2>

          <p>RuntimeError                              Traceback (most recent call
          last)<br>Cell In[7], line 5<br>      3 p = "An astronaut riding a green
          horse"<br>      4 prompt = [p for _ in range(7)]<br>----&gt; 5 images =
          pipe(prompt=prompt).images[0]</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27,
          in _DecoratorContextManager.<strong>call</strong>..decorate_context(*args,
          **kwargs)<br>     24 @functools.wraps(func)<br>     25 def decorate_context(*args,
          **kwargs):<br>     26     with self.clone():<br>---&gt; 27         return
          func(*args, **kwargs)</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:867,
          in StableDiffusionXLPipeline.<strong>call</strong>(self, prompt, prompt_2,
          height, width, num_inference_steps, denoising_end, guidance_scale, negative_prompt,
          negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds,
          negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds,
          output_type, return_dict, callback, callback_steps, cross_attention_kwargs,
          guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size,
          negative_crops_coords_top_left, negative_target_size)<br>    864     self.upcast_vae()<br>    865     latents
          = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)<br>--&gt;
          867 image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]<br>    869
          # cast back to fp16 if needed<br>    870 if needs_upcasting:</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46,
          in apply_forward_hook..wrapper(self, *args, **kwargs)<br>     44 if hasattr(self,
          "_hf_hook") and hasattr(self._hf_hook, "pre_forward"):<br>     45     self._hf_hook.pre_forward(self)<br>---&gt;
          46 return method(self, *args, **kwargs)</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:286,
          in AutoencoderKL.decode(self, z, return_dict)<br>    284     decoded = torch.cat(decoded_slices)<br>    285
          else:<br>--&gt; 286     decoded = self._decode(z).sample<br>    288 if not
          return_dict:<br>    289     return (decoded,)</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:272,
          in AutoencoderKL._decode(self, z, return_dict)<br>    269 if self.use_tiling
          and (z.shape[-1] &gt; self.tile_latent_min_size or z.shape[-2] &gt; self.tile_latent_min_size):<br>    270     return
          self.tiled_decode(z, return_dict=return_dict)<br>--&gt; 272 z = self.post_quant_conv(z)<br>    273
          dec = self.decoder(z)<br>    275 if not return_dict:</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190,
          in Module._call_impl(self, *input, **kwargs)<br>   1186 # If we don''t have
          any hooks, we want to skip the rest of the logic in<br>   1187 # this function,
          and just call forward.<br>   1188 if not (self._backward_hooks or self._forward_hooks
          or self._forward_pre_hooks or _global_backward_hooks<br>   1189         or
          _global_forward_hooks or _global_forward_pre_hooks):<br>-&gt; 1190     return
          forward_call(*input, **kwargs)<br>   1191 # Do not call functions when jit
          is used<br>   1192 full_backward_hooks, non_full_backward_hooks = [], []</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/conv.py:463,
          in Conv2d.forward(self, input)<br>    462 def forward(self, input: Tensor)
          -&gt; Tensor:<br>--&gt; 463     return self._conv_forward(input, self.weight,
          self.bias)</p>

          <p>File ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/conv.py:459,
          in Conv2d._conv_forward(self, input, weight, bias)<br>    455 if self.padding_mode
          != ''zeros'':<br>    456     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice,
          mode=self.padding_mode),<br>    457                     weight, bias, self.stride,<br>    458                     _pair(0),
          self.dilation, self.groups)<br>--&gt; 459 return F.conv2d(input, weight,
          bias, self.stride,<br>    460                 self.padding, self.dilation,
          self.groups)</p>

          <p>RuntimeError: Input type (c10::Half) and bias type (float) should be
          the same</p>

          <p>''''''</p>

          '
        raw: "Hi, \r\nI am trying to run a simple batched image generation using :\
          \ \r\n\r\nwith torch.no_grad():\r\n    p = \"An astronaut riding a green\
          \ horse\"\r\n    prompt = [p for _ in range(7)]\r\n    images = pipe(prompt=prompt).images[0]\r\
          \n\r\nIt throws the following error. \r\n\r\n'''\r\n---------------------------------------------------------------------------\r\
          \nRuntimeError                              Traceback (most recent call\
          \ last)\r\nCell In[7], line 5\r\n      3 p = \"An astronaut riding a green\
          \ horse\"\r\n      4 prompt = [p for _ in range(7)]\r\n----> 5 images =\
          \ pipe(prompt=prompt).images[0]\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27,\
          \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args,\
          \ **kwargs)\r\n     24 @functools.wraps(func)\r\n     25 def decorate_context(*args,\
          \ **kwargs):\r\n     26     with self.clone():\r\n---> 27         return\
          \ func(*args, **kwargs)\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:867,\
          \ in StableDiffusionXLPipeline.__call__(self, prompt, prompt_2, height,\
          \ width, num_inference_steps, denoising_end, guidance_scale, negative_prompt,\
          \ negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds,\
          \ negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds,\
          \ output_type, return_dict, callback, callback_steps, cross_attention_kwargs,\
          \ guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size,\
          \ negative_crops_coords_top_left, negative_target_size)\r\n    864     self.upcast_vae()\r\
          \n    865     latents = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\r\
          \n--> 867 image = self.vae.decode(latents / self.vae.config.scaling_factor,\
          \ return_dict=False)[0]\r\n    869 # cast back to fp16 if needed\r\n   \
          \ 870 if needs_upcasting:\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46,\
          \ in apply_forward_hook.<locals>.wrapper(self, *args, **kwargs)\r\n    \
          \ 44 if hasattr(self, \"_hf_hook\") and hasattr(self._hf_hook, \"pre_forward\"\
          ):\r\n     45     self._hf_hook.pre_forward(self)\r\n---> 46 return method(self,\
          \ *args, **kwargs)\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:286,\
          \ in AutoencoderKL.decode(self, z, return_dict)\r\n    284     decoded =\
          \ torch.cat(decoded_slices)\r\n    285 else:\r\n--> 286     decoded = self._decode(z).sample\r\
          \n    288 if not return_dict:\r\n    289     return (decoded,)\r\n\r\nFile\
          \ ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:272,\
          \ in AutoencoderKL._decode(self, z, return_dict)\r\n    269 if self.use_tiling\
          \ and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):\r\
          \n    270     return self.tiled_decode(z, return_dict=return_dict)\r\n-->\
          \ 272 z = self.post_quant_conv(z)\r\n    273 dec = self.decoder(z)\r\n \
          \   275 if not return_dict:\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190,\
          \ in Module._call_impl(self, *input, **kwargs)\r\n   1186 # If we don't\
          \ have any hooks, we want to skip the rest of the logic in\r\n   1187 #\
          \ this function, and just call forward.\r\n   1188 if not (self._backward_hooks\
          \ or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\r\
          \n   1189         or _global_forward_hooks or _global_forward_pre_hooks):\r\
          \n-> 1190     return forward_call(*input, **kwargs)\r\n   1191 # Do not\
          \ call functions when jit is used\r\n   1192 full_backward_hooks, non_full_backward_hooks\
          \ = [], []\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/conv.py:463,\
          \ in Conv2d.forward(self, input)\r\n    462 def forward(self, input: Tensor)\
          \ -> Tensor:\r\n--> 463     return self._conv_forward(input, self.weight,\
          \ self.bias)\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/conv.py:459,\
          \ in Conv2d._conv_forward(self, input, weight, bias)\r\n    455 if self.padding_mode\
          \ != 'zeros':\r\n    456     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice,\
          \ mode=self.padding_mode),\r\n    457                     weight, bias,\
          \ self.stride,\r\n    458                     _pair(0), self.dilation, self.groups)\r\
          \n--> 459 return F.conv2d(input, weight, bias, self.stride,\r\n    460 \
          \                self.padding, self.dilation, self.groups)\r\n\r\nRuntimeError:\
          \ Input type (c10::Half) and bias type (float) should be the same\r\n\r\n\
          '''"
        updatedAt: '2023-09-18T01:31:16.852Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F614"
        users:
        - wltongxue
    id: 6507a8645aa2ad0324efbe89
    type: comment
  author: gbhatt2
  content: "Hi, \r\nI am trying to run a simple batched image generation using : \r\
    \n\r\nwith torch.no_grad():\r\n    p = \"An astronaut riding a green horse\"\r\
    \n    prompt = [p for _ in range(7)]\r\n    images = pipe(prompt=prompt).images[0]\r\
    \n\r\nIt throws the following error. \r\n\r\n'''\r\n---------------------------------------------------------------------------\r\
    \nRuntimeError                              Traceback (most recent call last)\r\
    \nCell In[7], line 5\r\n      3 p = \"An astronaut riding a green horse\"\r\n\
    \      4 prompt = [p for _ in range(7)]\r\n----> 5 images = pipe(prompt=prompt).images[0]\r\
    \n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27,\
    \ in _DecoratorContextManager.__call__.<locals>.decorate_context(*args, **kwargs)\r\
    \n     24 @functools.wraps(func)\r\n     25 def decorate_context(*args, **kwargs):\r\
    \n     26     with self.clone():\r\n---> 27         return func(*args, **kwargs)\r\
    \n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:867,\
    \ in StableDiffusionXLPipeline.__call__(self, prompt, prompt_2, height, width,\
    \ num_inference_steps, denoising_end, guidance_scale, negative_prompt, negative_prompt_2,\
    \ num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds,\
    \ pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict,\
    \ callback, callback_steps, cross_attention_kwargs, guidance_rescale, original_size,\
    \ crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left,\
    \ negative_target_size)\r\n    864     self.upcast_vae()\r\n    865     latents\
    \ = latents.to(next(iter(self.vae.post_quant_conv.parameters())).dtype)\r\n-->\
    \ 867 image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\r\
    \n    869 # cast back to fp16 if needed\r\n    870 if needs_upcasting:\r\n\r\n\
    File ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46,\
    \ in apply_forward_hook.<locals>.wrapper(self, *args, **kwargs)\r\n     44 if\
    \ hasattr(self, \"_hf_hook\") and hasattr(self._hf_hook, \"pre_forward\"):\r\n\
    \     45     self._hf_hook.pre_forward(self)\r\n---> 46 return method(self, *args,\
    \ **kwargs)\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:286,\
    \ in AutoencoderKL.decode(self, z, return_dict)\r\n    284     decoded = torch.cat(decoded_slices)\r\
    \n    285 else:\r\n--> 286     decoded = self._decode(z).sample\r\n    288 if\
    \ not return_dict:\r\n    289     return (decoded,)\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:272,\
    \ in AutoencoderKL._decode(self, z, return_dict)\r\n    269 if self.use_tiling\
    \ and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):\r\
    \n    270     return self.tiled_decode(z, return_dict=return_dict)\r\n--> 272\
    \ z = self.post_quant_conv(z)\r\n    273 dec = self.decoder(z)\r\n    275 if not\
    \ return_dict:\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1190,\
    \ in Module._call_impl(self, *input, **kwargs)\r\n   1186 # If we don't have any\
    \ hooks, we want to skip the rest of the logic in\r\n   1187 # this function,\
    \ and just call forward.\r\n   1188 if not (self._backward_hooks or self._forward_hooks\
    \ or self._forward_pre_hooks or _global_backward_hooks\r\n   1189         or _global_forward_hooks\
    \ or _global_forward_pre_hooks):\r\n-> 1190     return forward_call(*input, **kwargs)\r\
    \n   1191 # Do not call functions when jit is used\r\n   1192 full_backward_hooks,\
    \ non_full_backward_hooks = [], []\r\n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/conv.py:463,\
    \ in Conv2d.forward(self, input)\r\n    462 def forward(self, input: Tensor) ->\
    \ Tensor:\r\n--> 463     return self._conv_forward(input, self.weight, self.bias)\r\
    \n\r\nFile ~/.conda/envs/pyv2/lib/python3.10/site-packages/torch/nn/modules/conv.py:459,\
    \ in Conv2d._conv_forward(self, input, weight, bias)\r\n    455 if self.padding_mode\
    \ != 'zeros':\r\n    456     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice,\
    \ mode=self.padding_mode),\r\n    457                     weight, bias, self.stride,\r\
    \n    458                     _pair(0), self.dilation, self.groups)\r\n--> 459\
    \ return F.conv2d(input, weight, bias, self.stride,\r\n    460               \
    \  self.padding, self.dilation, self.groups)\r\n\r\nRuntimeError: Input type (c10::Half)\
    \ and bias type (float) should be the same\r\n\r\n'''"
  created_at: 2023-09-18 00:31:16+00:00
  edited: false
  hidden: false
  id: 6507a8645aa2ad0324efbe89
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/033b710031a0f02a2db0312e9768cd52.svg
      fullname: wl
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wltongxue
      type: user
    createdAt: '2023-11-07T08:32:03.000Z'
    data:
      edited: false
      editors:
      - wltongxue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3937259912490845
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/033b710031a0f02a2db0312e9768cd52.svg
          fullname: wl
          isHf: false
          isPro: false
          name: wltongxue
          type: user
        html: '<p>Same trouble.</p>

          '
        raw: Same trouble.
        updatedAt: '2023-11-07T08:32:03.451Z'
      numEdits: 0
      reactions: []
    id: 6549f6038ceb5deea8c21876
    type: comment
  author: wltongxue
  content: Same trouble.
  created_at: 2023-11-07 08:32:03+00:00
  edited: false
  hidden: false
  id: 6549f6038ceb5deea8c21876
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/63e0c3918972d9487e8265728f9dd5d7.svg
      fullname: "\u9648\u6893\u806A"
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: extends128
      type: user
    createdAt: '2023-11-09T09:10:45.000Z'
    data:
      edited: false
      editors:
      - extends128
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.39502984285354614
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/63e0c3918972d9487e8265728f9dd5d7.svg
          fullname: "\u9648\u6893\u806A"
          isHf: false
          isPro: false
          name: extends128
          type: user
        html: '<p>same trouble</p>

          '
        raw: same trouble
        updatedAt: '2023-11-09T09:10:45.412Z'
      numEdits: 0
      reactions: []
    id: 654ca215be11400417ddbc0d
    type: comment
  author: extends128
  content: same trouble
  created_at: 2023-11-09 09:10:45+00:00
  edited: false
  hidden: false
  id: 654ca215be11400417ddbc0d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 91
repo_id: stabilityai/stable-diffusion-xl-base-1.0
repo_type: model
status: open
target_branch: null
title: 'Error: Input type (c10::Half) and bias type (float) should be the same'
