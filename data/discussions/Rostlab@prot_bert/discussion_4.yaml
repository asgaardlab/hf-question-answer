!!python/object:huggingface_hub.community.DiscussionWithDetails
author: tangpingdadaguai
conflicting_files: null
created_at: 2023-08-08 01:24:24+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/be2320558aa4a210f5067736c2f000ef.svg
      fullname: yanyang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: tangpingdadaguai
      type: user
    createdAt: '2023-08-08T02:24:24.000Z'
    data:
      edited: false
      editors:
      - tangpingdadaguai
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6070556640625
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/be2320558aa4a210f5067736c2f000ef.svg
          fullname: yanyang
          isHf: false
          isPro: false
          name: tangpingdadaguai
          type: user
        html: '<p>I use the protbert model to code proteins, but no matter which protein
          value sequence is encoded, the results are the same. When I replace the
          model with the biobert model, shorter sequences can be coded normally, but
          the coding results of longer sequences are the same. They have been stuck
          here for many days, so I will ask you to solve them.</p>

          <p>The code is as follows:</p>

          <p>import pandas as pd<br>from transformers import AutoTokenizer, AutoModel</p>

          <p>sequences = ["MKTVRQERLKSIVRILERSKEPVSGAQGQPRGVRGF",<br>             "MSDTKGDPGRH",<br>             "MSRLDKSKVINSALELLNEVGIEGLTTRKLAQKLGVEQPTLYWHVKNKRALLDALAIEMLDRHHTHFCPLEGESWQDFLRNNAKSFRCALLSHRDGAKVHLGTRPTEKQYETLENQLAFLCQQGFSLENALYALSAVGHFTLGCVLEDQEHQVAKEERETPTTDSMPPLLRQAIELFDHQGAEPAFLFGLELIICGLEKQLKCESGS",<br>            "LLNGSLAEEIVIRTENIADNTKDIIVQFNKTVSIACTRPHNNTRRGIHIGPGQAFYATGDIIGDIRQAHCNVSGENWTETMEWVKAKLEKTFNVTNITFEPPITGGDLEITTHSFNCRGEFFYCNTSKLFNSSELNSIKGKENYTIILPCRIKQFVRMWQRVGQAMYAPPIEGNITCISNITGLILTRDGGINRTNDTFRPGGGDMRDNWRRKL",<br>            "QELLCAASLISDRWVLTAAHCLLYPPWDKNFTVNDILVRIGKYARSRYERNMEKISTLEKIIIHPGYNWRENLDRDIALMKLKKPVAFSDYIHPVCLPDKQIVTSLLQAGHKGRVTGWGNLKEMWTVNMNEVQPSVLQMVNLPLVERPICKASTGIRVTDNMFCAGYKPEEGKRGDACEGDSGGPFVMKNPYNNRWYQMGIVSWGEGCDRDGKYGFYTHVFRLKKWIRKMVDRFG",<br>             "MSAPASTTQATGSTTSTTTKTAGATPATASGLFTIPDGDFFSTARAVVASDAVATNEDLSEIEAVWKDMKVPTDTMAQAAWDLVRHCADVGSSAQTEMIDTGPYSNGISRARLAAAIKEVCTLRQFCMKYAPVVWNWMLTNNSPPANWQAQGFKPEHKFAAFDFFNGVTNPAAIMPKEGLIRPPSEAEMNAAQTAAFVKITKARAQSNDFASLDAAVTRGRITGTTTAEAVVTLPPP"<br>            ]</p>

          <p>protein_sequences = pd.Series(sequences)<br>window_size = 256<br>step_size
          = 250</p>

          <p>model_name =  r''E:\NLP_model\prot_bert''<br>model = AutoModel.from_pretrained(model_name)<br>tokenizer
          = AutoTokenizer.from_pretrained(model_name)</p>

          <p>encoded_features = []<br>for protein_sequence in protein_sequences:<br>    windows
          = [protein_sequence[i:i + window_size] for i in range(0, len(protein_sequence),
          step_size)]<br>    sequence_features = []<br>    for window in windows:<br>        inputs
          = tokenizer(window, return_tensors="pt", padding=True, truncation=True)<br>        outputs
          = model(**inputs)<br>        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()<br>        sequence_features.extend(embeddings)<br>    encoded_features.append(sequence_features)</p>

          <p>df_features = pd.DataFrame(encoded_features)</p>

          <p>print(df_features.shape)<br>df_features</p>

          <p>This figure is the result of running the protbert model.<br><a rel="nofollow"
          href="https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/PQbzoiAn_xZGIx1Dm9AJR.png"><img
          alt="1.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/PQbzoiAn_xZGIx1Dm9AJR.png"></a></p>

          <p>This figure is the result of running the Biobert model.</p>

          <p><a rel="nofollow" href="https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/81b_51ttgNWQU7TnbmwGZ.png"><img
          alt="2.PNG" src="https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/81b_51ttgNWQU7TnbmwGZ.png"></a></p>

          '
        raw: "I use the protbert model to code proteins, but no matter which protein\
          \ value sequence is encoded, the results are the same. When I replace the\
          \ model with the biobert model, shorter sequences can be coded normally,\
          \ but the coding results of longer sequences are the same. They have been\
          \ stuck here for many days, so I will ask you to solve them.\r\n\r\nThe\
          \ code is as follows:\r\n\r\nimport pandas as pd\r\nfrom transformers import\
          \ AutoTokenizer, AutoModel\r\n\r\nsequences = [\"MKTVRQERLKSIVRILERSKEPVSGAQGQPRGVRGF\"\
          , \r\n             \"MSDTKGDPGRH\", \r\n             \"MSRLDKSKVINSALELLNEVGIEGLTTRKLAQKLGVEQPTLYWHVKNKRALLDALAIEMLDRHHTHFCPLEGESWQDFLRNNAKSFRCALLSHRDGAKVHLGTRPTEKQYETLENQLAFLCQQGFSLENALYALSAVGHFTLGCVLEDQEHQVAKEERETPTTDSMPPLLRQAIELFDHQGAEPAFLFGLELIICGLEKQLKCESGS\"\
          ,\r\n            \"LLNGSLAEEIVIRTENIADNTKDIIVQFNKTVSIACTRPHNNTRRGIHIGPGQAFYATGDIIGDIRQAHCNVSGENWTETMEWVKAKLEKTFNVTNITFEPPITGGDLEITTHSFNCRGEFFYCNTSKLFNSSELNSIKGKENYTIILPCRIKQFVRMWQRVGQAMYAPPIEGNITCISNITGLILTRDGGINRTNDTFRPGGGDMRDNWRRKL\"\
          ,\r\n            \"QELLCAASLISDRWVLTAAHCLLYPPWDKNFTVNDILVRIGKYARSRYERNMEKISTLEKIIIHPGYNWRENLDRDIALMKLKKPVAFSDYIHPVCLPDKQIVTSLLQAGHKGRVTGWGNLKEMWTVNMNEVQPSVLQMVNLPLVERPICKASTGIRVTDNMFCAGYKPEEGKRGDACEGDSGGPFVMKNPYNNRWYQMGIVSWGEGCDRDGKYGFYTHVFRLKKWIRKMVDRFG\"\
          ,\r\n             \"MSAPASTTQATGSTTSTTTKTAGATPATASGLFTIPDGDFFSTARAVVASDAVATNEDLSEIEAVWKDMKVPTDTMAQAAWDLVRHCADVGSSAQTEMIDTGPYSNGISRARLAAAIKEVCTLRQFCMKYAPVVWNWMLTNNSPPANWQAQGFKPEHKFAAFDFFNGVTNPAAIMPKEGLIRPPSEAEMNAAQTAAFVKITKARAQSNDFASLDAAVTRGRITGTTTAEAVVTLPPP\"\
          \r\n            ]\r\n\r\nprotein_sequences = pd.Series(sequences)\r\nwindow_size\
          \ = 256\r\nstep_size = 250\r\n\r\n\r\nmodel_name =  r'E:\\NLP_model\\prot_bert'\r\
          \nmodel = AutoModel.from_pretrained(model_name)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
          \n\r\nencoded_features = []\r\nfor protein_sequence in protein_sequences:\r\
          \n    windows = [protein_sequence[i:i + window_size] for i in range(0, len(protein_sequence),\
          \ step_size)]\r\n    sequence_features = []\r\n    for window in windows:\r\
          \n        inputs = tokenizer(window, return_tensors=\"pt\", padding=True,\
          \ truncation=True)\r\n        outputs = model(**inputs)\r\n        embeddings\
          \ = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\r\n       \
          \ sequence_features.extend(embeddings)\r\n    encoded_features.append(sequence_features)\r\
          \n\r\ndf_features = pd.DataFrame(encoded_features)\r\n\r\nprint(df_features.shape)\r\
          \ndf_features\r\n\r\nThis figure is the result of running the protbert model.\r\
          \n![1.PNG](https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/PQbzoiAn_xZGIx1Dm9AJR.png)\r\
          \n\r\nThis figure is the result of running the Biobert model.\r\n\r\n![2.PNG](https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/81b_51ttgNWQU7TnbmwGZ.png)\r\
          \n"
        updatedAt: '2023-08-08T02:24:24.238Z'
      numEdits: 0
      reactions: []
    id: 64d1a758d8d0927372f9fb57
    type: comment
  author: tangpingdadaguai
  content: "I use the protbert model to code proteins, but no matter which protein\
    \ value sequence is encoded, the results are the same. When I replace the model\
    \ with the biobert model, shorter sequences can be coded normally, but the coding\
    \ results of longer sequences are the same. They have been stuck here for many\
    \ days, so I will ask you to solve them.\r\n\r\nThe code is as follows:\r\n\r\n\
    import pandas as pd\r\nfrom transformers import AutoTokenizer, AutoModel\r\n\r\
    \nsequences = [\"MKTVRQERLKSIVRILERSKEPVSGAQGQPRGVRGF\", \r\n             \"MSDTKGDPGRH\"\
    , \r\n             \"MSRLDKSKVINSALELLNEVGIEGLTTRKLAQKLGVEQPTLYWHVKNKRALLDALAIEMLDRHHTHFCPLEGESWQDFLRNNAKSFRCALLSHRDGAKVHLGTRPTEKQYETLENQLAFLCQQGFSLENALYALSAVGHFTLGCVLEDQEHQVAKEERETPTTDSMPPLLRQAIELFDHQGAEPAFLFGLELIICGLEKQLKCESGS\"\
    ,\r\n            \"LLNGSLAEEIVIRTENIADNTKDIIVQFNKTVSIACTRPHNNTRRGIHIGPGQAFYATGDIIGDIRQAHCNVSGENWTETMEWVKAKLEKTFNVTNITFEPPITGGDLEITTHSFNCRGEFFYCNTSKLFNSSELNSIKGKENYTIILPCRIKQFVRMWQRVGQAMYAPPIEGNITCISNITGLILTRDGGINRTNDTFRPGGGDMRDNWRRKL\"\
    ,\r\n            \"QELLCAASLISDRWVLTAAHCLLYPPWDKNFTVNDILVRIGKYARSRYERNMEKISTLEKIIIHPGYNWRENLDRDIALMKLKKPVAFSDYIHPVCLPDKQIVTSLLQAGHKGRVTGWGNLKEMWTVNMNEVQPSVLQMVNLPLVERPICKASTGIRVTDNMFCAGYKPEEGKRGDACEGDSGGPFVMKNPYNNRWYQMGIVSWGEGCDRDGKYGFYTHVFRLKKWIRKMVDRFG\"\
    ,\r\n             \"MSAPASTTQATGSTTSTTTKTAGATPATASGLFTIPDGDFFSTARAVVASDAVATNEDLSEIEAVWKDMKVPTDTMAQAAWDLVRHCADVGSSAQTEMIDTGPYSNGISRARLAAAIKEVCTLRQFCMKYAPVVWNWMLTNNSPPANWQAQGFKPEHKFAAFDFFNGVTNPAAIMPKEGLIRPPSEAEMNAAQTAAFVKITKARAQSNDFASLDAAVTRGRITGTTTAEAVVTLPPP\"\
    \r\n            ]\r\n\r\nprotein_sequences = pd.Series(sequences)\r\nwindow_size\
    \ = 256\r\nstep_size = 250\r\n\r\n\r\nmodel_name =  r'E:\\NLP_model\\prot_bert'\r\
    \nmodel = AutoModel.from_pretrained(model_name)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
    \n\r\nencoded_features = []\r\nfor protein_sequence in protein_sequences:\r\n\
    \    windows = [protein_sequence[i:i + window_size] for i in range(0, len(protein_sequence),\
    \ step_size)]\r\n    sequence_features = []\r\n    for window in windows:\r\n\
    \        inputs = tokenizer(window, return_tensors=\"pt\", padding=True, truncation=True)\r\
    \n        outputs = model(**inputs)\r\n        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()\r\
    \n        sequence_features.extend(embeddings)\r\n    encoded_features.append(sequence_features)\r\
    \n\r\ndf_features = pd.DataFrame(encoded_features)\r\n\r\nprint(df_features.shape)\r\
    \ndf_features\r\n\r\nThis figure is the result of running the protbert model.\r\
    \n![1.PNG](https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/PQbzoiAn_xZGIx1Dm9AJR.png)\r\
    \n\r\nThis figure is the result of running the Biobert model.\r\n\r\n![2.PNG](https://cdn-uploads.huggingface.co/production/uploads/64d1a389bf39f9c8bef722ed/81b_51ttgNWQU7TnbmwGZ.png)\r\
    \n"
  created_at: 2023-08-08 01:24:24+00:00
  edited: false
  hidden: false
  id: 64d1a758d8d0927372f9fb57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
      fullname: Michael Heinzinger
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mheinz
      type: user
    createdAt: '2023-08-08T07:28:28.000Z'
    data:
      edited: false
      editors:
      - mheinz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9173039793968201
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
          fullname: Michael Heinzinger
          isHf: false
          isPro: false
          name: mheinz
          type: user
        html: '<p>Hi; I would kindly re-direct you to our github where we have multiple
          examples &amp; tutorials, incl. a colab, that show how to get started: <a
          rel="nofollow" href="https://github.com/agemagician/ProtTrans">https://github.com/agemagician/ProtTrans</a><br>In
          general, I would recommend to use ProtT5 and not ProtBERT.</p>

          '
        raw: 'Hi; I would kindly re-direct you to our github where we have multiple
          examples & tutorials, incl. a colab, that show how to get started: https://github.com/agemagician/ProtTrans

          In general, I would recommend to use ProtT5 and not ProtBERT.'
        updatedAt: '2023-08-08T07:28:28.039Z'
      numEdits: 0
      reactions: []
    id: 64d1ee9c15b26cc7f7293056
    type: comment
  author: mheinz
  content: 'Hi; I would kindly re-direct you to our github where we have multiple
    examples & tutorials, incl. a colab, that show how to get started: https://github.com/agemagician/ProtTrans

    In general, I would recommend to use ProtT5 and not ProtBERT.'
  created_at: 2023-08-08 06:28:28+00:00
  edited: false
  hidden: false
  id: 64d1ee9c15b26cc7f7293056
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
      fullname: Michael Heinzinger
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mheinz
      type: user
    createdAt: '2023-11-16T15:09:14.000Z'
    data:
      status: closed
    id: 6556309a6b2edac8d99add83
    type: status-change
  author: mheinz
  created_at: 2023-11-16 15:09:14+00:00
  id: 6556309a6b2edac8d99add83
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: Rostlab/prot_bert
repo_type: model
status: closed
target_branch: null
title: Regarding the issue of using the protbert model to encode protein sequences,
  but with identical results.
