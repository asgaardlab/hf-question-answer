!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dipayan26
conflicting_files: null
created_at: 2023-10-31 07:31:13+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5b70cdaa27f2e2a48ab5bc371ff2e0b.svg
      fullname: Dipayan Sarkar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dipayan26
      type: user
    createdAt: '2023-10-31T08:31:13.000Z'
    data:
      edited: false
      editors:
      - dipayan26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9413424134254456
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5b70cdaa27f2e2a48ab5bc371ff2e0b.svg
          fullname: Dipayan Sarkar
          isHf: false
          isPro: false
          name: dipayan26
          type: user
        html: '<p>Feature generation of protein sequence length of about 1000 takes
          very high  ram usage and google colab''s 12GB gpu memory became ''out of
          memory'' error just after using of 6 of those protein sequences.</p>

          '
        raw: Feature generation of protein sequence length of about 1000 takes very
          high  ram usage and google colab's 12GB gpu memory became 'out of memory'
          error just after using of 6 of those protein sequences.
        updatedAt: '2023-10-31T08:31:13.658Z'
      numEdits: 0
      reactions: []
    id: 6540bb517cadb2d1b42d3963
    type: comment
  author: dipayan26
  content: Feature generation of protein sequence length of about 1000 takes very
    high  ram usage and google colab's 12GB gpu memory became 'out of memory' error
    just after using of 6 of those protein sequences.
  created_at: 2023-10-31 07:31:13+00:00
  edited: false
  hidden: false
  id: 6540bb517cadb2d1b42d3963
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
      fullname: Michael Heinzinger
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mheinz
      type: user
    createdAt: '2023-10-31T08:49:37.000Z'
    data:
      edited: false
      editors:
      - mheinz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8790441751480103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
          fullname: Michael Heinzinger
          isHf: false
          isPro: false
          name: mheinz
          type: user
        html: '<p>Maybe try to cast the model to half-precision before running feature
          extraction.<br>Also, I would recommend to use our ProtT5-XL model because
          it proved to be better in any of our benchmarks:<br><a href="https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc">https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc</a><br>Also,
          when you only hit OOM after embedding 6 sequences of identical length, you
          have memory leakage somewhere.<br>Once you managed to embed a single protein
          of e.g. 1k residues, it should not make any difference whether you repeat
          the process x-times.</p>

          '
        raw: "Maybe try to cast the model to half-precision before running feature\
          \ extraction.\nAlso, I would recommend to use our ProtT5-XL model because\
          \ it proved to be better in any of our benchmarks: \nhttps://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc\n\
          Also, when you only hit OOM after embedding 6 sequences of identical length,\
          \ you have memory leakage somewhere. \nOnce you managed to embed a single\
          \ protein of e.g. 1k residues, it should not make any difference whether\
          \ you repeat the process x-times."
        updatedAt: '2023-10-31T08:49:37.025Z'
      numEdits: 0
      reactions: []
    id: 6540bfa1992b1c560edb7566
    type: comment
  author: mheinz
  content: "Maybe try to cast the model to half-precision before running feature extraction.\n\
    Also, I would recommend to use our ProtT5-XL model because it proved to be better\
    \ in any of our benchmarks: \nhttps://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc\n\
    Also, when you only hit OOM after embedding 6 sequences of identical length, you\
    \ have memory leakage somewhere. \nOnce you managed to embed a single protein\
    \ of e.g. 1k residues, it should not make any difference whether you repeat the\
    \ process x-times."
  created_at: 2023-10-31 07:49:37+00:00
  edited: false
  hidden: false
  id: 6540bfa1992b1c560edb7566
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c5b70cdaa27f2e2a48ab5bc371ff2e0b.svg
      fullname: Dipayan Sarkar
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dipayan26
      type: user
    createdAt: '2023-11-04T06:24:42.000Z'
    data:
      edited: false
      editors:
      - dipayan26
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8932055234909058
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c5b70cdaa27f2e2a48ab5bc371ff2e0b.svg
          fullname: Dipayan Sarkar
          isHf: false
          isPro: false
          name: dipayan26
          type: user
        html: '<p>when using this <a href="https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc">https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc</a>
          model , the tokenizer shows error "Exception: You''re trying to run a <code>Unigram</code>
          model but you''re file was trained with a different algorithm".</p>

          '
        raw: 'when using this https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc
          model , the tokenizer shows error "Exception: You''re trying to run a `Unigram`
          model but you''re file was trained with a different algorithm".'
        updatedAt: '2023-11-04T06:24:42.121Z'
      numEdits: 0
      reactions: []
    id: 6545e3aa2a2a483042e81e7a
    type: comment
  author: dipayan26
  content: 'when using this https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc
    model , the tokenizer shows error "Exception: You''re trying to run a `Unigram`
    model but you''re file was trained with a different algorithm".'
  created_at: 2023-11-04 05:24:42+00:00
  edited: false
  hidden: false
  id: 6545e3aa2a2a483042e81e7a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
      fullname: Michael Heinzinger
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mheinz
      type: user
    createdAt: '2023-11-07T08:22:52.000Z'
    data:
      edited: false
      editors:
      - mheinz
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9182330369949341
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
          fullname: Michael Heinzinger
          isHf: false
          isPro: false
          name: mheinz
          type: user
        html: '<p>Yeah, I guess you are running into this issue: <a rel="nofollow"
          href="https://github.com/huggingface/transformers/issues/9871">https://github.com/huggingface/transformers/issues/9871</a><br>I
          think your problem should be solved by loading BertTokenizer or T5Tokenizer
          instead of AutoTokenizer</p>

          '
        raw: 'Yeah, I guess you are running into this issue: https://github.com/huggingface/transformers/issues/9871

          I think your problem should be solved by loading BertTokenizer or T5Tokenizer
          instead of AutoTokenizer'
        updatedAt: '2023-11-07T08:22:52.990Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - ghazikhanihamed
    id: 6549f3dc69da1e356c9637bc
    type: comment
  author: mheinz
  content: 'Yeah, I guess you are running into this issue: https://github.com/huggingface/transformers/issues/9871

    I think your problem should be solved by loading BertTokenizer or T5Tokenizer
    instead of AutoTokenizer'
  created_at: 2023-11-07 08:22:52+00:00
  edited: false
  hidden: false
  id: 6549f3dc69da1e356c9637bc
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/3bdeccebefb2f03dc0572f16fd9b363e.svg
      fullname: Michael Heinzinger
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mheinz
      type: user
    createdAt: '2023-11-16T15:08:27.000Z'
    data:
      status: closed
    id: 6556306be5f426493b9d3f2d
    type: status-change
  author: mheinz
  created_at: 2023-11-16 15:08:27+00:00
  id: 6556306be5f426493b9d3f2d
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: Rostlab/prot_bert
repo_type: model
status: closed
target_branch: null
title: Model for feature generation requires very high memory.
