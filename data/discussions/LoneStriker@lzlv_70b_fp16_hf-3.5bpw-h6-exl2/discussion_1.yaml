!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Nexesenex
conflicting_files: null
created_at: 2023-12-29 13:52:54+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
      fullname: Nexesenex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Nexesenex
      type: user
    createdAt: '2023-12-29T13:52:54.000Z'
    data:
      edited: false
      editors:
      - Nexesenex
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8631842136383057
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/31b10b87d67ff7d0691f786a857d319a.svg
          fullname: Nexesenex
          isHf: false
          isPro: false
          name: Nexesenex
          type: user
        html: '<p>After test, it works wonderfully, and thank you.<br>I was wondering
          : do you use by default the new exl2-2 quant? This, because you ceased to
          mention it in the title of your quants 15 days ago.</p>

          <p>Also, about the bucket list of models to quantize that you kindly suggested,
          here are my favorites of which you made the measurement already :</p>

          <p>1 - <a href="https://huggingface.co/LoneStriker/Yarn-Llama-2-70b-32k-4.0bpw-h6-exl2">https://huggingface.co/LoneStriker/Yarn-Llama-2-70b-32k-4.0bpw-h6-exl2</a>
          -&gt;    in 3.0bpw...    to enjoy 30k ctx at a good perplexity still with
          the new exl2-2 quant, after making the 10k first tokens with a better model.</p>

          <p>2 - <a href="https://huggingface.co/LoneStriker/Xwin-LM-70B-V0.1-4.0bpw-h6-exl2">https://huggingface.co/LoneStriker/Xwin-LM-70B-V0.1-4.0bpw-h6-exl2</a>
          -&gt; in 3.50bpw... because this base rocks.</p>

          <p>3 - <a href="https://huggingface.co/LoneStriker/dolphin-2.2-70b-4.0bpw-h6-exl2">https://huggingface.co/LoneStriker/dolphin-2.2-70b-4.0bpw-h6-exl2</a>
          -&gt; in 3.50bpw... because it''s probably the best all rounder I tested.</p>

          <p>4 - <a href="https://huggingface.co/LoneStriker/Aetheria-L2-70B-4.0bpw-h6-exl2-2">https://huggingface.co/LoneStriker/Aetheria-L2-70B-2.65bpw-h6-exl2-2</a>
          -&gt; in 3.50bpw.. because it''s my favorite merge.</p>

          <p>5 - <a href="https://huggingface.co/LoneStriker/airoboros-l2-70b-3.1-4.0bpw-h6-exl2">https://huggingface.co/LoneStriker/airoboros-l2-70b-3.1-4.0bpw-h6-exl2</a>
          -&gt; in 3.50bpw.. because it has an amazing instruct system.</p>

          <p>Also, if you could add to this list of model exl2 quants, I''d suggest
          : <a href="https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-m2.0">https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-m2.0</a>
          , for this Airoboros is an improved 1.4.1 to my taste, and keeps most of
          its charm. Otherwise, a 3.50bpw quant of <a href="https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-4.65bpw-h6-exl2-2">https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-4.65bpw-h6-exl2-2</a>
          would be great still as a 6. !</p>

          <p>Once again, thank you for the LZLV. Finally I can really have the feel
          of a 70b model with a decent speed and a large context.</p>

          '
        raw: "After test, it works wonderfully, and thank you.\r\nI was wondering\
          \ : do you use by default the new exl2-2 quant? This, because you ceased\
          \ to mention it in the title of your quants 15 days ago.\r\n\r\nAlso, about\
          \ the bucket list of models to quantize that you kindly suggested, here\
          \ are my favorites of which you made the measurement already :\r\n\r\n\r\
          \n1 - https://huggingface.co/LoneStriker/Yarn-Llama-2-70b-32k-4.0bpw-h6-exl2\
          \ ->    in 3.0bpw...    to enjoy 30k ctx at a good perplexity still with\
          \ the new exl2-2 quant, after making the 10k first tokens with a better\
          \ model.\r\n\r\n\r\n2 - https://huggingface.co/LoneStriker/Xwin-LM-70B-V0.1-4.0bpw-h6-exl2\
          \ -> in 3.50bpw... because this base rocks.\r\n\r\n3 - https://huggingface.co/LoneStriker/dolphin-2.2-70b-4.0bpw-h6-exl2\
          \ -> in 3.50bpw... because it's probably the best all rounder I tested.\r\
          \n\r\n4 - [https://huggingface.co/LoneStriker/Aetheria-L2-70B-2.65bpw-h6-exl2-2](https://huggingface.co/LoneStriker/Aetheria-L2-70B-4.0bpw-h6-exl2-2)\
          \ -> in 3.50bpw.. because it's my favorite merge.\r\n\r\n5 - https://huggingface.co/LoneStriker/airoboros-l2-70b-3.1-4.0bpw-h6-exl2\
          \ -> in 3.50bpw.. because it has an amazing instruct system.\r\n\r\nAlso,\
          \ if you could add to this list of model exl2 quants, I'd suggest : https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-m2.0\
          \ , for this Airoboros is an improved 1.4.1 to my taste, and keeps most\
          \ of its charm. Otherwise, a 3.50bpw quant of https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-4.65bpw-h6-exl2-2\
          \ would be great still as a 6. !\r\n\r\nOnce again, thank you for the LZLV.\
          \ Finally I can really have the feel of a 70b model with a decent speed\
          \ and a large context."
        updatedAt: '2023-12-29T13:52:54.098Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - LoneStriker
    id: 658ecf36c0a2664426ae05d6
    type: comment
  author: Nexesenex
  content: "After test, it works wonderfully, and thank you.\r\nI was wondering :\
    \ do you use by default the new exl2-2 quant? This, because you ceased to mention\
    \ it in the title of your quants 15 days ago.\r\n\r\nAlso, about the bucket list\
    \ of models to quantize that you kindly suggested, here are my favorites of which\
    \ you made the measurement already :\r\n\r\n\r\n1 - https://huggingface.co/LoneStriker/Yarn-Llama-2-70b-32k-4.0bpw-h6-exl2\
    \ ->    in 3.0bpw...    to enjoy 30k ctx at a good perplexity still with the new\
    \ exl2-2 quant, after making the 10k first tokens with a better model.\r\n\r\n\
    \r\n2 - https://huggingface.co/LoneStriker/Xwin-LM-70B-V0.1-4.0bpw-h6-exl2 ->\
    \ in 3.50bpw... because this base rocks.\r\n\r\n3 - https://huggingface.co/LoneStriker/dolphin-2.2-70b-4.0bpw-h6-exl2\
    \ -> in 3.50bpw... because it's probably the best all rounder I tested.\r\n\r\n\
    4 - [https://huggingface.co/LoneStriker/Aetheria-L2-70B-2.65bpw-h6-exl2-2](https://huggingface.co/LoneStriker/Aetheria-L2-70B-4.0bpw-h6-exl2-2)\
    \ -> in 3.50bpw.. because it's my favorite merge.\r\n\r\n5 - https://huggingface.co/LoneStriker/airoboros-l2-70b-3.1-4.0bpw-h6-exl2\
    \ -> in 3.50bpw.. because it has an amazing instruct system.\r\n\r\nAlso, if you\
    \ could add to this list of model exl2 quants, I'd suggest : https://huggingface.co/jondurbin/airoboros-l2-70b-gpt4-m2.0\
    \ , for this Airoboros is an improved 1.4.1 to my taste, and keeps most of its\
    \ charm. Otherwise, a 3.50bpw quant of https://huggingface.co/LoneStriker/airoboros-l2-70b-gpt4-1.4.1-4.65bpw-h6-exl2-2\
    \ would be great still as a 6. !\r\n\r\nOnce again, thank you for the LZLV. Finally\
    \ I can really have the feel of a 70b model with a decent speed and a large context."
  created_at: 2023-12-29 13:52:54+00:00
  edited: false
  hidden: false
  id: 658ecf36c0a2664426ae05d6
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/lzlv_70b_fp16_hf-3.5bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Thank you very much
