!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Ryo9n
conflicting_files: null
created_at: 2023-09-20 02:10:05+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
      fullname: Ryohei Kobayashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ryo9n
      type: user
    createdAt: '2023-09-20T03:10:05.000Z'
    data:
      edited: false
      editors:
      - Ryo9n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8148894309997559
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
          fullname: Ryohei Kobayashi
          isHf: false
          isPro: false
          name: Ryo9n
          type: user
        html: '<p>Hi. I tried to use this model with your sample code.<br>But I am
          facing the out of gpu memory error. My gpu is 1080ti.<br>Please tell me
          your environment which operation confirmation.<br>And I want to know how
          to adjust batch size, or Do you have a lighter model?  </p>

          <hr>

          <h2 id="torchcudaoutofmemoryerror-cuda-out-of-memory-tried-to-allocate-8000-mib-gpu-0-1091-gib-total-capacity-1026-gib-already-allocated-2944-mib-free-1046-gib-reserved-in-total-by-pytorch-if-reserved-memory-is--allocated-memory-try-setting-max_split_size_mb-to-avoid-fragmentation--see-documentation-for-memory-management-and-pytorch_cuda_alloc_conf">torch.cuda.OutOfMemoryError:
          CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 10.91 GiB total
          capacity; 10.26 GiB already allocated; 29.44 MiB free; 10.46 GiB reserved
          in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try
          setting max_split_size_mb to avoid fragmentation.  See documentation for
          Memory Management and PYTORCH_CUDA_ALLOC_CONF</h2>

          <p>Thank you for your help.</p>

          '
        raw: "Hi. I tried to use this model with your sample code.\r\nBut I am facing\
          \ the out of gpu memory error. My gpu is 1080ti.\r\nPlease tell me your\
          \ environment which operation confirmation.\r\nAnd I want to know how to\
          \ adjust batch size, or Do you have a lighter model?  \r\n\r\n-------------------\r\
          \ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00\
          \ MiB (GPU 0; 10.91 GiB total capacity; 10.26 GiB already allocated; 29.44\
          \ MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is\
          \ >> allocated memory try setting max_split_size_mb to avoid fragmentation.\
          \  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
          \n-------------------\r\n\r\nThank you for your help."
        updatedAt: '2023-09-20T03:10:05.271Z'
      numEdits: 0
      reactions: []
    id: 650a628dd95f30b9dcfa8b12
    type: comment
  author: Ryo9n
  content: "Hi. I tried to use this model with your sample code.\r\nBut I am facing\
    \ the out of gpu memory error. My gpu is 1080ti.\r\nPlease tell me your environment\
    \ which operation confirmation.\r\nAnd I want to know how to adjust batch size,\
    \ or Do you have a lighter model?  \r\n\r\n-------------------\r\ntorch.cuda.OutOfMemoryError:\
    \ CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 10.91 GiB total capacity;\
    \ 10.26 GiB already allocated; 29.44 MiB free; 10.46 GiB reserved in total by\
    \ PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb\
    \ to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\
    \n-------------------\r\n\r\nThank you for your help."
  created_at: 2023-09-20 02:10:05+00:00
  edited: false
  hidden: false
  id: 650a628dd95f30b9dcfa8b12
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32aec04162e903e2ba798e64629df0bd.svg
      fullname: Bidipta Sarkar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bidiptas
      type: user
    createdAt: '2023-09-20T03:40:22.000Z'
    data:
      edited: false
      editors:
      - bidiptas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9038137793540955
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32aec04162e903e2ba798e64629df0bd.svg
          fullname: Bidipta Sarkar
          isHf: false
          isPro: false
          name: bidiptas
          type: user
        html: '<p>Hi! We trained and tested this model using an A40 GPU (48 GB VRAM).
          It seems like just loading the weights would require around 30 GB of VRAM
          for the base blip model (Flan-T5-XXL) (see <a href="https://huggingface.co/Salesforce/blip2-flan-t5-xxl/discussions/2#63f78cdd0074cebe75d26c68">https://huggingface.co/Salesforce/blip2-flan-t5-xxl/discussions/2#63f78cdd0074cebe75d26c68</a>
          ), but this would not fit in you 1080ti GPU. We do not have a lighter model
          yet.</p>

          '
        raw: Hi! We trained and tested this model using an A40 GPU (48 GB VRAM). It
          seems like just loading the weights would require around 30 GB of VRAM for
          the base blip model (Flan-T5-XXL) (see https://huggingface.co/Salesforce/blip2-flan-t5-xxl/discussions/2#63f78cdd0074cebe75d26c68
          ), but this would not fit in you 1080ti GPU. We do not have a lighter model
          yet.
        updatedAt: '2023-09-20T03:40:22.093Z'
      numEdits: 0
      reactions: []
    id: 650a69a6a6f657faa4379798
    type: comment
  author: bidiptas
  content: Hi! We trained and tested this model using an A40 GPU (48 GB VRAM). It
    seems like just loading the weights would require around 30 GB of VRAM for the
    base blip model (Flan-T5-XXL) (see https://huggingface.co/Salesforce/blip2-flan-t5-xxl/discussions/2#63f78cdd0074cebe75d26c68
    ), but this would not fit in you 1080ti GPU. We do not have a lighter model yet.
  created_at: 2023-09-20 02:40:22+00:00
  edited: false
  hidden: false
  id: 650a69a6a6f657faa4379798
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
      fullname: Ryohei Kobayashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ryo9n
      type: user
    createdAt: '2023-09-20T05:01:15.000Z'
    data:
      edited: false
      editors:
      - Ryo9n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8334649205207825
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
          fullname: Ryohei Kobayashi
          isHf: false
          isPro: false
          name: Ryo9n
          type: user
        html: "<p>Thank you for help and reply.<br>I understood. this time I tried\
          \ to run with Cpus.<br>this is the result. Is it correct and what does it\
          \ mean?<br>It seems tensor does not appear.</p>\n<hr>\n<p>  deprecate(<br>Loading\
          \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:32&lt;00:00,\
          \  6.53s/it]<br>['opaque', 'translucent', 'transparent']</p>\n<hr>\n<p>Thank\
          \ you.</p>\n"
        raw: "Thank you for help and reply.\nI understood. this time I tried to run\
          \ with Cpus.\nthis is the result. Is it correct and what does it mean?\n\
          It seems tensor does not appear.\n-------------------------------\n  deprecate(\n\
          Loading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 5/5\
          \ [00:32<00:00,  6.53s/it]\n['opaque', 'translucent', 'transparent']\n-------------------------------\n\
          Thank you."
        updatedAt: '2023-09-20T05:01:15.323Z'
      numEdits: 0
      reactions: []
    id: 650a7c9b1ba21833b4ebca22
    type: comment
  author: Ryo9n
  content: "Thank you for help and reply.\nI understood. this time I tried to run\
    \ with Cpus.\nthis is the result. Is it correct and what does it mean?\nIt seems\
    \ tensor does not appear.\n-------------------------------\n  deprecate(\nLoading\
    \ checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:32<00:00,\
    \  6.53s/it]\n['opaque', 'translucent', 'transparent']\n-------------------------------\n\
    Thank you."
  created_at: 2023-09-20 04:01:15+00:00
  edited: false
  hidden: false
  id: 650a7c9b1ba21833b4ebca22
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/32aec04162e903e2ba798e64629df0bd.svg
      fullname: Bidipta Sarkar
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: bidiptas
      type: user
    createdAt: '2023-09-20T23:58:06.000Z'
    data:
      edited: false
      editors:
      - bidiptas
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8505410552024841
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/32aec04162e903e2ba798e64629df0bd.svg
          fullname: Bidipta Sarkar
          isHf: false
          isPro: false
          name: bidiptas
          type: user
        html: '<p>Sorry, it seems like we used a slightly modified version of LAVIS
          when testing the model. We updated the instructions in the README and recommend
          using the provided test.py and generate.py.</p>

          <p>The tensor represents the "sequence scores" which are the log probabilities
          of generating the corresponding responses. See the updated end of the model
          card for how to interpret these values.</p>

          '
        raw: 'Sorry, it seems like we used a slightly modified version of LAVIS when
          testing the model. We updated the instructions in the README and recommend
          using the provided test.py and generate.py.


          The tensor represents the "sequence scores" which are the log probabilities
          of generating the corresponding responses. See the updated end of the model
          card for how to interpret these values.'
        updatedAt: '2023-09-20T23:58:06.840Z'
      numEdits: 0
      reactions: []
    id: 650b870ea791bf82f4b1cd57
    type: comment
  author: bidiptas
  content: 'Sorry, it seems like we used a slightly modified version of LAVIS when
    testing the model. We updated the instructions in the README and recommend using
    the provided test.py and generate.py.


    The tensor represents the "sequence scores" which are the log probabilities of
    generating the corresponding responses. See the updated end of the model card
    for how to interpret these values.'
  created_at: 2023-09-20 22:58:06+00:00
  edited: false
  hidden: false
  id: 650b870ea791bf82f4b1cd57
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
      fullname: Ryohei Kobayashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ryo9n
      type: user
    createdAt: '2023-09-21T04:41:30.000Z'
    data:
      edited: false
      editors:
      - Ryo9n
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9916011691093445
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
          fullname: Ryohei Kobayashi
          isHf: false
          isPro: false
          name: Ryo9n
          type: user
        html: '<p>Thanks all for the lighting fast response! I understand. It was
          very helpful for me.</p>

          '
        raw: Thanks all for the lighting fast response! I understand. It was very
          helpful for me.
        updatedAt: '2023-09-21T04:41:30.432Z'
      numEdits: 0
      reactions: []
      relatedEventId: 650bc97a534285d49a5e68d5
    id: 650bc97a534285d49a5e68d2
    type: comment
  author: Ryo9n
  content: Thanks all for the lighting fast response! I understand. It was very helpful
    for me.
  created_at: 2023-09-21 03:41:30+00:00
  edited: false
  hidden: false
  id: 650bc97a534285d49a5e68d2
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/a87cc752c18c828e159cc84cc2e0a110.svg
      fullname: Ryohei Kobayashi
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Ryo9n
      type: user
    createdAt: '2023-09-21T04:41:30.000Z'
    data:
      status: closed
    id: 650bc97a534285d49a5e68d5
    type: status-change
  author: Ryo9n
  created_at: 2023-09-21 03:41:30+00:00
  id: 650bc97a534285d49a5e68d5
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: bidiptas/PG-InstructBLIP
repo_type: model
status: closed
target_branch: null
title: Out of gpu memory error
