!!python/object:huggingface_hub.community.DiscussionWithDetails
author: Flanua
conflicting_files: null
created_at: 2023-09-05 18:38:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-05T19:38:22.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9859979152679443
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>It would be nice to know the difference between version 1.1 and
          1.2 and it''s max content length though.<br>P.S I guess the model v1.2 slightly
          clever than v1.1 at least by looking at the AI leaderboard..</p>

          '
        raw: 'It would be nice to know the difference between version 1.1 and 1.2
          and it''s max content length though.

          P.S I guess the model v1.2 slightly clever than v1.1 at least by looking
          at the AI leaderboard..'
        updatedAt: '2023-09-05T19:41:33.943Z'
      numEdits: 1
      reactions: []
    id: 64f783aeceabf1e6fc34671e
    type: comment
  author: Flanua
  content: 'It would be nice to know the difference between version 1.1 and 1.2 and
    it''s max content length though.

    P.S I guess the model v1.2 slightly clever than v1.1 at least by looking at the
    AI leaderboard..'
  created_at: 2023-09-05 18:38:22+00:00
  edited: true
  hidden: false
  id: 64f783aeceabf1e6fc34671e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-05T19:49:53.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9570085406303406
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Hey! Going from v1.0 to v1.1 - Difference is the introduction of
          generalized Tree-of-Thoughts.</p>

          <p>v1.2 was trained on more data -- but it''s MMLU score is lower because
          more data (or more training) flattens the underlying entropy of the model.
          </p>

          <p>Having said that, I use v1.2 now in personal use. :)</p>

          <p>Context length is a bit lower, at 2048 -- but if you use RoPE scaling
          it''ll give you upto 8K. Again, having said that, I''ve been using the model
          up to 4096 out of the box, and it doesn''t break.</p>

          '
        raw: "Hey! Going from v1.0 to v1.1 - Difference is the introduction of generalized\
          \ Tree-of-Thoughts.\n\nv1.2 was trained on more data -- but it's MMLU score\
          \ is lower because more data (or more training) flattens the underlying\
          \ entropy of the model. \n\nHaving said that, I use v1.2 now in personal\
          \ use. :)\n\nContext length is a bit lower, at 2048 -- but if you use RoPE\
          \ scaling it'll give you upto 8K. Again, having said that, I've been using\
          \ the model up to 4096 out of the box, and it doesn't break."
        updatedAt: '2023-09-05T19:49:53.159Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\U0001F44D"
        users:
        - Thireus
        - wolfram
    id: 64f786619980b96c33e24452
    type: comment
  author: migtissera
  content: "Hey! Going from v1.0 to v1.1 - Difference is the introduction of generalized\
    \ Tree-of-Thoughts.\n\nv1.2 was trained on more data -- but it's MMLU score is\
    \ lower because more data (or more training) flattens the underlying entropy of\
    \ the model. \n\nHaving said that, I use v1.2 now in personal use. :)\n\nContext\
    \ length is a bit lower, at 2048 -- but if you use RoPE scaling it'll give you\
    \ upto 8K. Again, having said that, I've been using the model up to 4096 out of\
    \ the box, and it doesn't break."
  created_at: 2023-09-05 18:49:53+00:00
  edited: false
  hidden: false
  id: 64f786619980b96c33e24452
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
      fullname: Serge Gotsuliak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gotzmann
      type: user
    createdAt: '2023-09-05T21:37:32.000Z'
    data:
      edited: false
      editors:
      - gotzmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8114861845970154
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
          fullname: Serge Gotsuliak
          isHf: false
          isPro: false
          name: gotzmann
          type: user
        html: '<p>How is context length might be lower than standard 4096 tokens of
          LLaMA v2 ? </p>

          '
        raw: 'How is context length might be lower than standard 4096 tokens of LLaMA
          v2 ? '
        updatedAt: '2023-09-05T21:37:32.706Z'
      numEdits: 0
      reactions: []
    id: 64f79f9c37d3c6eb54aeb107
    type: comment
  author: gotzmann
  content: 'How is context length might be lower than standard 4096 tokens of LLaMA
    v2 ? '
  created_at: 2023-09-05 20:37:32+00:00
  edited: false
  hidden: false
  id: 64f79f9c37d3c6eb54aeb107
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-05T21:40:53.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9621012806892395
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Just couldn''t fit my dataset and train it with a decent batch size
          (on a single H100), so I cut it down. But if this is important to you guys
          I''ll do the v1.3 with larger context length. But in my experiments I haven''t
          seen a performance degradation.</p>

          '
        raw: Just couldn't fit my dataset and train it with a decent batch size (on
          a single H100), so I cut it down. But if this is important to you guys I'll
          do the v1.3 with larger context length. But in my experiments I haven't
          seen a performance degradation.
        updatedAt: '2023-09-05T21:40:53.458Z'
      numEdits: 0
      reactions:
      - count: 2
        reaction: "\u2764\uFE0F"
        users:
        - Thireus
        - wolfram
    id: 64f7a065baa3b4ec4e32a670
    type: comment
  author: migtissera
  content: Just couldn't fit my dataset and train it with a decent batch size (on
    a single H100), so I cut it down. But if this is important to you guys I'll do
    the v1.3 with larger context length. But in my experiments I haven't seen a performance
    degradation.
  created_at: 2023-09-05 20:40:53+00:00
  edited: false
  hidden: false
  id: 64f7a065baa3b4ec4e32a670
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-06T15:25:48.000Z'
    data:
      edited: false
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761441946029663
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>Thanks for provided info and yes it''s very important for me at
          least to have context length of 4096 so I can put more personality traits
          into my AI agent and talk for longer with it. Also it would be very nice
          if you can increase MMLU score a bit somehow in the future that would be
          awesome. Can''t wait to see v1.3 with context length 4096 though.</p>

          '
        raw: Thanks for provided info and yes it's very important for me at least
          to have context length of 4096 so I can put more personality traits into
          my AI agent and talk for longer with it. Also it would be very nice if you
          can increase MMLU score a bit somehow in the future that would be awesome.
          Can't wait to see v1.3 with context length 4096 though.
        updatedAt: '2023-09-06T15:25:48.519Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - Thireus
        - Flanua
        - wolfram
    id: 64f899fcceabf1e6fc5c0962
    type: comment
  author: Flanua
  content: Thanks for provided info and yes it's very important for me at least to
    have context length of 4096 so I can put more personality traits into my AI agent
    and talk for longer with it. Also it would be very nice if you can increase MMLU
    score a bit somehow in the future that would be awesome. Can't wait to see v1.3
    with context length 4096 though.
  created_at: 2023-09-06 14:25:48+00:00
  edited: false
  hidden: false
  id: 64f899fcceabf1e6fc5c0962
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
      fullname: None
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Thireus
      type: user
    createdAt: '2023-09-09T19:46:48.000Z'
    data:
      edited: false
      editors:
      - Thireus
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9582664370536804
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/af1201cb8f07eba487669586f75a4b32.svg
          fullname: None
          isHf: false
          isPro: false
          name: Thireus
          type: user
        html: '<p>Impressive model!! I started testing the model and I get better
          results with v1.1 than v1.2 for my use case, specifically with very long
          context length.<br>I''d definitely like to see a v1.3 with larger context
          length to compare and see if the slight degradation is resolved.</p>

          <p>Keep up the good work!</p>

          '
        raw: 'Impressive model!! I started testing the model and I get better results
          with v1.1 than v1.2 for my use case, specifically with very long context
          length.

          I''d definitely like to see a v1.3 with larger context length to compare
          and see if the slight degradation is resolved.


          Keep up the good work!'
        updatedAt: '2023-09-09T19:46:48.241Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F44D"
        users:
        - migtissera
        - Flanua
        - wolfram
    id: 64fccba89ecd05d5bf716f43
    type: comment
  author: Thireus
  content: 'Impressive model!! I started testing the model and I get better results
    with v1.1 than v1.2 for my use case, specifically with very long context length.

    I''d definitely like to see a v1.3 with larger context length to compare and see
    if the slight degradation is resolved.


    Keep up the good work!'
  created_at: 2023-09-09 18:46:48+00:00
  edited: false
  hidden: false
  id: 64fccba89ecd05d5bf716f43
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-11T17:23:10.000Z'
    data:
      edited: true
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9767852425575256
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>Hey, <span data-props=\"{&quot;user&quot;:&quot;migtissera&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/migtissera\"\
          >@<span class=\"underline\">migtissera</span></a></span>\n\n\t</span></span>,\
          \ thanks for releasing Synthia! I'm testing the 70B model right now and\
          \ am seriously impressed by both the intelligence and personality this model\
          \ exhibits.</p>\n<p>Didn't even notice the training only being 2K, ran her\
          \ well on 4K context, but noticed some problems when the context was full\
          \ which might be solved by RoPE scale 0.5 or a v1.3 if you do retrain at\
          \ 4K. Now I'm trying her at 8K with 0.25 scale because I can never have\
          \ enough context.</p>\n<p>By the way, would you consider a 34B version,\
          \ using Code Llama 2 as the base model? There are few 34B models but it's\
          \ the perfect size to put all layers on a 3090 GPU and get much better speed\
          \ than 70B but at much higher quality than 13B. Would instantly make your\
          \ model very relevant to those of us who long for something bigger than\
          \ 13B but smaller than 70B. And I'm sure once anyone has tried it, the quality\
          \ will speak for itself, it's quickly becoming my new favorite model!</p>\n"
        raw: 'Hey, @migtissera, thanks for releasing Synthia! I''m testing the 70B
          model right now and am seriously impressed by both the intelligence and
          personality this model exhibits.


          Didn''t even notice the training only being 2K, ran her well on 4K context,
          but noticed some problems when the context was full which might be solved
          by RoPE scale 0.5 or a v1.3 if you do retrain at 4K. Now I''m trying her
          at 8K with 0.25 scale because I can never have enough context.


          By the way, would you consider a 34B version, using Code Llama 2 as the
          base model? There are few 34B models but it''s the perfect size to put all
          layers on a 3090 GPU and get much better speed than 70B but at much higher
          quality than 13B. Would instantly make your model very relevant to those
          of us who long for something bigger than 13B but smaller than 70B. And I''m
          sure once anyone has tried it, the quality will speak for itself, it''s
          quickly becoming my new favorite model!'
        updatedAt: '2023-09-11T19:42:01.703Z'
      numEdits: 1
      reactions: []
    id: 64ff4cfe868ac1994af7af2b
    type: comment
  author: wolfram
  content: 'Hey, @migtissera, thanks for releasing Synthia! I''m testing the 70B model
    right now and am seriously impressed by both the intelligence and personality
    this model exhibits.


    Didn''t even notice the training only being 2K, ran her well on 4K context, but
    noticed some problems when the context was full which might be solved by RoPE
    scale 0.5 or a v1.3 if you do retrain at 4K. Now I''m trying her at 8K with 0.25
    scale because I can never have enough context.


    By the way, would you consider a 34B version, using Code Llama 2 as the base model?
    There are few 34B models but it''s the perfect size to put all layers on a 3090
    GPU and get much better speed than 70B but at much higher quality than 13B. Would
    instantly make your model very relevant to those of us who long for something
    bigger than 13B but smaller than 70B. And I''m sure once anyone has tried it,
    the quality will speak for itself, it''s quickly becoming my new favorite model!'
  created_at: 2023-09-11 16:23:10+00:00
  edited: true
  hidden: false
  id: 64ff4cfe868ac1994af7af2b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
      fullname: Serge Gotsuliak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gotzmann
      type: user
    createdAt: '2023-09-11T20:57:52.000Z'
    data:
      edited: false
      editors:
      - gotzmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9870279431343079
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
          fullname: Serge Gotsuliak
          isHf: false
          isPro: false
          name: gotzmann
          type: user
        html: '<p>Yeah, agree. 34B is highly welcomed addition to the family. </p>

          <p>BTW, I''m trying to find the differences between 1.2 and 1.2b with my
          benchmarks while waiting 1.3</p>

          '
        raw: "Yeah, agree. 34B is highly welcomed addition to the family. \n\nBTW,\
          \ I'm trying to find the differences between 1.2 and 1.2b with my benchmarks\
          \ while waiting 1.3"
        updatedAt: '2023-09-11T20:57:52.238Z'
      numEdits: 0
      reactions: []
    id: 64ff7f5096c544b3a6ce46cd
    type: comment
  author: gotzmann
  content: "Yeah, agree. 34B is highly welcomed addition to the family. \n\nBTW, I'm\
    \ trying to find the differences between 1.2 and 1.2b with my benchmarks while\
    \ waiting 1.3"
  created_at: 2023-09-11 19:57:52+00:00
  edited: false
  hidden: false
  id: 64ff7f5096c544b3a6ce46cd
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-11T21:53:54.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9784424304962158
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Sure, I can train one but I don''t know whether it''ll be that good.
          The underlying entropy of that model is conditioned for code, so unless
          it''s for coding, not sure how that''ll go.</p>

          <p>I just want to beat ChatGPT!   :D</p>

          <p>Also for coding, I thought Phind and WizardLM models are good on HumanEval?
          What are your usecases?</p>

          '
        raw: 'Sure, I can train one but I don''t know whether it''ll be that good.
          The underlying entropy of that model is conditioned for code, so unless
          it''s for coding, not sure how that''ll go.


          I just want to beat ChatGPT!   :D


          Also for coding, I thought Phind and WizardLM models are good on HumanEval?
          What are your usecases?

          '
        updatedAt: '2023-09-11T21:53:54.970Z'
      numEdits: 0
      reactions: []
    id: 64ff8c7218830fabea34fcc4
    type: comment
  author: migtissera
  content: 'Sure, I can train one but I don''t know whether it''ll be that good. The
    underlying entropy of that model is conditioned for code, so unless it''s for
    coding, not sure how that''ll go.


    I just want to beat ChatGPT!   :D


    Also for coding, I thought Phind and WizardLM models are good on HumanEval? What
    are your usecases?

    '
  created_at: 2023-09-11 20:53:54+00:00
  edited: false
  hidden: false
  id: 64ff8c7218830fabea34fcc4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-11T22:09:27.000Z'
    data:
      edited: true
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9771723747253418
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: '<p>I do hope you''ll beat ChatGPT some day! And until we have local
          LLMs that do that, I''m just using them for entertainment, to chat and have
          fun with (in that regard, you''ve actually beaten ChatGPT already, in my
          opinion!).</p>

          <p>So I''m not thinking of 34B as a code model, but a base for chat/instruct
          finetunes. Airoboros-c34B-2.1 and Samantha-1.11-CodeLlama-34B have shown
          that Code Llama 2 can be tuned very well for roleplay and chat, and with
          the 34B base trained at 16K context length, that has even more advantages
          than 13B or 70B bases.</p>

          <p>By the way, your model reminds me of the Samantha model a lot, which
          has been praised for its personality and intelligence. But Samantha is censored
          worse than Llama 2 Chat, and while I can get her to do NSFW roleplay, she''s
          too moralizing and needs constant coercion, that''s why I consider her too
          annoying to bother with. Synthia has shown at least as much intelligence
          and personality, and she''s uncensored, so she''s always fun to talk to
          and totally easy-going. Haven''t smiled and even laughed as much when testing
          a model as I did with her today, so consider me a Synthia fan now. ;)</p>

          '
        raw: 'I do hope you''ll beat ChatGPT some day! And until we have local LLMs
          that do that, I''m just using them for entertainment, to chat and have fun
          with (in that regard, you''ve actually beaten ChatGPT already, in my opinion!).


          So I''m not thinking of 34B as a code model, but a base for chat/instruct
          finetunes. Airoboros-c34B-2.1 and Samantha-1.11-CodeLlama-34B have shown
          that Code Llama 2 can be tuned very well for roleplay and chat, and with
          the 34B base trained at 16K context length, that has even more advantages
          than 13B or 70B bases.


          By the way, your model reminds me of the Samantha model a lot, which has
          been praised for its personality and intelligence. But Samantha is censored
          worse than Llama 2 Chat, and while I can get her to do NSFW roleplay, she''s
          too moralizing and needs constant coercion, that''s why I consider her too
          annoying to bother with. Synthia has shown at least as much intelligence
          and personality, and she''s uncensored, so she''s always fun to talk to
          and totally easy-going. Haven''t smiled and even laughed as much when testing
          a model as I did with her today, so consider me a Synthia fan now. ;)'
        updatedAt: '2023-09-11T22:17:48.941Z'
      numEdits: 1
      reactions: []
    id: 64ff9017e0c40dd27a5b3123
    type: comment
  author: wolfram
  content: 'I do hope you''ll beat ChatGPT some day! And until we have local LLMs
    that do that, I''m just using them for entertainment, to chat and have fun with
    (in that regard, you''ve actually beaten ChatGPT already, in my opinion!).


    So I''m not thinking of 34B as a code model, but a base for chat/instruct finetunes.
    Airoboros-c34B-2.1 and Samantha-1.11-CodeLlama-34B have shown that Code Llama
    2 can be tuned very well for roleplay and chat, and with the 34B base trained
    at 16K context length, that has even more advantages than 13B or 70B bases.


    By the way, your model reminds me of the Samantha model a lot, which has been
    praised for its personality and intelligence. But Samantha is censored worse than
    Llama 2 Chat, and while I can get her to do NSFW roleplay, she''s too moralizing
    and needs constant coercion, that''s why I consider her too annoying to bother
    with. Synthia has shown at least as much intelligence and personality, and she''s
    uncensored, so she''s always fun to talk to and totally easy-going. Haven''t smiled
    and even laughed as much when testing a model as I did with her today, so consider
    me a Synthia fan now. ;)'
  created_at: 2023-09-11 21:09:27+00:00
  edited: true
  hidden: false
  id: 64ff9017e0c40dd27a5b3123
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-11T22:24:25.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9785876870155334
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>DM me on Twitter. I have a model that might be very intriguing to
          you guys.</p>

          '
        raw: DM me on Twitter. I have a model that might be very intriguing to you
          guys.
        updatedAt: '2023-09-11T22:24:25.047Z'
      numEdits: 0
      reactions: []
      relatedEventId: 64ff93992ad36636be71db61
    id: 64ff93992ad36636be71db5e
    type: comment
  author: migtissera
  content: DM me on Twitter. I have a model that might be very intriguing to you guys.
  created_at: 2023-09-11 21:24:25+00:00
  edited: false
  hidden: false
  id: 64ff93992ad36636be71db5e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-11T22:24:25.000Z'
    data:
      status: closed
    id: 64ff93992ad36636be71db61
    type: status-change
  author: migtissera
  created_at: 2023-09-11 21:24:25+00:00
  id: 64ff93992ad36636be71db61
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-12T11:50:53.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9730045795440674
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<blockquote>\n<p>DM me on Twitter. I have a model that might be very\
          \ intriguing to you guys.</p>\n</blockquote>\n<p><span data-props=\"{&quot;user&quot;:&quot;migtissera&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/migtissera\"\
          >@<span class=\"underline\">migtissera</span></a></span>\n\n\t</span></span>\
          \ I created a Twitter account just to do that, and am following you, but\
          \ it won't let me send a DM unless you follow me as well (or I pay monthly\
          \ for a verified account, which isn't even an option yet because my account\
          \ is too new). So could you (temporarily) follow me, too, so we can exchange\
          \ DMs? Username: WolframRvnwlf</p>\n"
        raw: '> DM me on Twitter. I have a model that might be very intriguing to
          you guys.


          @migtissera I created a Twitter account just to do that, and am following
          you, but it won''t let me send a DM unless you follow me as well (or I pay
          monthly for a verified account, which isn''t even an option yet because
          my account is too new). So could you (temporarily) follow me, too, so we
          can exchange DMs? Username: WolframRvnwlf'
        updatedAt: '2023-09-12T11:50:53.341Z'
      numEdits: 0
      reactions: []
    id: 6500509d52aec889f4abb46a
    type: comment
  author: wolfram
  content: '> DM me on Twitter. I have a model that might be very intriguing to you
    guys.


    @migtissera I created a Twitter account just to do that, and am following you,
    but it won''t let me send a DM unless you follow me as well (or I pay monthly
    for a verified account, which isn''t even an option yet because my account is
    too new). So could you (temporarily) follow me, too, so we can exchange DMs? Username:
    WolframRvnwlf'
  created_at: 2023-09-12 10:50:53+00:00
  edited: false
  hidden: false
  id: 6500509d52aec889f4abb46a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-15T07:56:32.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3832588493824005
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Synthia-34B is out guys: <a href="https://huggingface.co/migtissera/Synthia-34B-v1.2">https://huggingface.co/migtissera/Synthia-34B-v1.2</a></p>

          '
        raw: 'Synthia-34B is out guys: https://huggingface.co/migtissera/Synthia-34B-v1.2'
        updatedAt: '2023-09-15T07:56:32.360Z'
      numEdits: 0
      reactions: []
    id: 65040e3093574a8971887004
    type: comment
  author: migtissera
  content: 'Synthia-34B is out guys: https://huggingface.co/migtissera/Synthia-34B-v1.2'
  created_at: 2023-09-15 06:56:32+00:00
  edited: false
  hidden: false
  id: 65040e3093574a8971887004
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
      fullname: Wolfram Ravenwolf
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: wolfram
      type: user
    createdAt: '2023-09-15T10:24:34.000Z'
    data:
      edited: false
      editors:
      - wolfram
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9262692332267761
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6303ca537373aacccd85d8a7/JZqLjXZVGWXJdWUNI99db.jpeg?w=200&h=200&f=face
          fullname: Wolfram Ravenwolf
          isHf: false
          isPro: false
          name: wolfram
          type: user
        html: "<p>Thanks, that's great news, 34B should be perfect balance between\
          \ speed, quality, and context size! I'll test it as soon as <span data-props=\"\
          {&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"UserMention\" class=\"\
          SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"inline-block\"><span\
          \ class=\"contents\"><a href=\"/TheBloke\">@<span class=\"underline\">TheBloke</span></a></span>\n\
          \n\t</span></span> gets around to quantize <a href=\"https://huggingface.co/migtissera/Synthia-34B-v1.2\"\
          >Synthia-34B-v1.2</a>.</p>\n<p>By the way,  <span data-props=\"{&quot;user&quot;:&quot;migtissera&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/migtissera\"\
          >@<span class=\"underline\">migtissera</span></a></span>\n\n\t</span></span>\
          \ - did you see my previous message about reaching out on Twitter? Or was\
          \ the \"model that might be very intriguing\" this 34B?</p>\n"
        raw: 'Thanks, that''s great news, 34B should be perfect balance between speed,
          quality, and context size! I''ll test it as soon as @TheBloke gets around
          to quantize [Synthia-34B-v1.2](https://huggingface.co/migtissera/Synthia-34B-v1.2).


          By the way,  @migtissera - did you see my previous message about reaching
          out on Twitter? Or was the "model that might be very intriguing" this 34B?'
        updatedAt: '2023-09-15T10:24:34.368Z'
      numEdits: 0
      reactions: []
    id: 650430e252ca06fef9554a26
    type: comment
  author: wolfram
  content: 'Thanks, that''s great news, 34B should be perfect balance between speed,
    quality, and context size! I''ll test it as soon as @TheBloke gets around to quantize
    [Synthia-34B-v1.2](https://huggingface.co/migtissera/Synthia-34B-v1.2).


    By the way,  @migtissera - did you see my previous message about reaching out
    on Twitter? Or was the "model that might be very intriguing" this 34B?'
  created_at: 2023-09-15 09:24:34+00:00
  edited: false
  hidden: false
  id: 650430e252ca06fef9554a26
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
      fullname: Tom Jobbins
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: true
      name: TheBloke
      type: user
    createdAt: '2023-09-15T10:25:37.000Z'
    data:
      edited: false
      editors:
      - TheBloke
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5770285725593567
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/6426d3f3a7723d62b53c259b/tvPikpAzKTKGN5wrpadOJ.jpeg?w=200&h=200&f=face
          fullname: Tom Jobbins
          isHf: false
          isPro: true
          name: TheBloke
          type: user
        html: '<blockquote>

          <p>Synthia-34B is out guys: <a href="https://huggingface.co/migtissera/Synthia-34B-v1.2">https://huggingface.co/migtissera/Synthia-34B-v1.2</a></p>

          </blockquote>

          <p>Nice - will the model card be coming soon?</p>

          '
        raw: '> Synthia-34B is out guys: https://huggingface.co/migtissera/Synthia-34B-v1.2


          Nice - will the model card be coming soon?'
        updatedAt: '2023-09-15T10:25:37.602Z'
      numEdits: 0
      reactions: []
    id: 6504312124964747c8e15101
    type: comment
  author: TheBloke
  content: '> Synthia-34B is out guys: https://huggingface.co/migtissera/Synthia-34B-v1.2


    Nice - will the model card be coming soon?'
  created_at: 2023-09-15 09:25:37+00:00
  edited: false
  hidden: false
  id: 6504312124964747c8e15101
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
      fullname: Serge Gotsuliak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gotzmann
      type: user
    createdAt: '2023-09-15T10:28:32.000Z'
    data:
      edited: false
      editors:
      - gotzmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8000058531761169
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
          fullname: Serge Gotsuliak
          isHf: false
          isPro: false
          name: gotzmann
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> could you please\
          \ convert to GGUF this 34B model as well this 70B v1.2b: <a href=\"https://huggingface.co/migtissera/Synthia-70B-v1.2b\"\
          >https://huggingface.co/migtissera/Synthia-70B-v1.2b</a></p>\n"
        raw: '@TheBloke could you please convert to GGUF this 34B model as well this
          70B v1.2b: https://huggingface.co/migtissera/Synthia-70B-v1.2b'
        updatedAt: '2023-09-15T10:28:32.796Z'
      numEdits: 0
      reactions: []
    id: 650431d08e60293dbed60c53
    type: comment
  author: gotzmann
  content: '@TheBloke could you please convert to GGUF this 34B model as well this
    70B v1.2b: https://huggingface.co/migtissera/Synthia-70B-v1.2b'
  created_at: 2023-09-15 09:28:32+00:00
  edited: false
  hidden: false
  id: 650431d08e60293dbed60c53
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-23T11:03:32.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7215393781661987
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;wolfram&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/wolfram\">@<span class=\"\
          underline\">wolfram</span></a></span>\n\n\t</span></span> sorry I missed\
          \ your message! Gave you a follow on Twitter!</p>\n"
        raw: '

          @wolfram sorry I missed your message! Gave you a follow on Twitter!'
        updatedAt: '2023-09-23T11:03:32.331Z'
      numEdits: 0
      reactions: []
    id: 650ec6048f3228d8077b460b
    type: comment
  author: migtissera
  content: '

    @wolfram sorry I missed your message! Gave you a follow on Twitter!'
  created_at: 2023-09-23 10:03:32+00:00
  edited: false
  hidden: false
  id: 650ec6048f3228d8077b460b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: migtissera/Synthia-70B-v1.2
repo_type: model
status: closed
target_branch: null
title: I wonder what is the difference between version 1.1 and 1.2
