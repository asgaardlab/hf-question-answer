!!python/object:huggingface_hub.community.DiscussionWithDetails
author: gotzmann
conflicting_files: null
created_at: 2023-09-03 09:00:08+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
      fullname: Serge Gotsuliak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gotzmann
      type: user
    createdAt: '2023-09-03T10:00:08.000Z'
    data:
      edited: false
      editors:
      - gotzmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.973359227180481
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
          fullname: Serge Gotsuliak
          isHf: false
          isPro: false
          name: gotzmann
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;TheBloke&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/TheBloke\">@<span class=\"\
          underline\">TheBloke</span></a></span>\n\n\t</span></span> Please, this\
          \ model family looks very promising</p>\n"
        raw: '@TheBloke Please, this model family looks very promising'
        updatedAt: '2023-09-03T10:00:08.750Z'
      numEdits: 0
      reactions: []
    id: 64f459280b71ff632db2aa70
    type: comment
  author: gotzmann
  content: '@TheBloke Please, this model family looks very promising'
  created_at: 2023-09-03 09:00:08+00:00
  edited: false
  hidden: false
  id: 64f459280b71ff632db2aa70
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-04T00:34:00.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9760015606880188
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;gotzmann&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/gotzmann\">@<span class=\"\
          underline\">gotzmann</span></a></span>\n\n\t</span></span> have you been\
          \ using this? I've been using it for STEM related tasks and it's been a\
          \ pleasant surprise!</p>\n"
        raw: '@gotzmann have you been using this? I''ve been using it for STEM related
          tasks and it''s been a pleasant surprise!'
        updatedAt: '2023-09-04T00:34:00.057Z'
      numEdits: 0
      reactions: []
    id: 64f525f8b9a80a0531246b33
    type: comment
  author: migtissera
  content: '@gotzmann have you been using this? I''ve been using it for STEM related
    tasks and it''s been a pleasant surprise!'
  created_at: 2023-09-03 23:34:00+00:00
  edited: false
  hidden: false
  id: 64f525f8b9a80a0531246b33
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
      fullname: Serge Gotsuliak
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: gotzmann
      type: user
    createdAt: '2023-09-05T21:36:19.000Z'
    data:
      edited: false
      editors:
      - gotzmann
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8248660564422607
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/59b3159942ccf1131c23886855f1dd1d.svg
          fullname: Serge Gotsuliak
          isHf: false
          isPro: false
          name: gotzmann
          type: user
        html: '<p>I''ve been done thorough research of latest 70B models (Upstage,
          Samantha, Nous Hermes, ...) with our own benchmark and I should say that
          Synthia got most scores out all of them!</p>

          <p>We are going to use it within our chat system instead of Upstage Instruct
          model.</p>

          <p>BTW, I''ve converted Synthia v1.2 into Q_4_K_M format (which we are using),
          so it fits on a pair of 3090 / 4090 cards or one A6000:</p>

          <p><a href="https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGML">https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGML</a><br><a
          href="https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGUF">https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGUF</a></p>

          '
        raw: 'I''ve been done thorough research of latest 70B models (Upstage, Samantha,
          Nous Hermes, ...) with our own benchmark and I should say that Synthia got
          most scores out all of them!


          We are going to use it within our chat system instead of Upstage Instruct
          model.


          BTW, I''ve converted Synthia v1.2 into Q_4_K_M format (which we are using),
          so it fits on a pair of 3090 / 4090 cards or one A6000:


          https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGML

          https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGUF'
        updatedAt: '2023-09-05T21:36:19.921Z'
      numEdits: 0
      reactions: []
    id: 64f79f53d6d14925f6f6951b
    type: comment
  author: gotzmann
  content: 'I''ve been done thorough research of latest 70B models (Upstage, Samantha,
    Nous Hermes, ...) with our own benchmark and I should say that Synthia got most
    scores out all of them!


    We are going to use it within our chat system instead of Upstage Instruct model.


    BTW, I''ve converted Synthia v1.2 into Q_4_K_M format (which we are using), so
    it fits on a pair of 3090 / 4090 cards or one A6000:


    https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGML

    https://huggingface.co/gotzmann/Synthia-70B-v1.2-GGUF'
  created_at: 2023-09-05 20:36:19+00:00
  edited: false
  hidden: false
  id: 64f79f53d6d14925f6f6951b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-05T21:38:36.000Z'
    data:
      edited: true
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9779492616653442
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Awesome!</p>

          <p>Share your app too when you''re ready. I can also help with some LinkedIn/Twitter
          re-sharing. :)</p>

          '
        raw: 'Awesome!


          Share your app too when you''re ready. I can also help with some LinkedIn/Twitter
          re-sharing. :)'
        updatedAt: '2023-09-06T03:15:45.049Z'
      numEdits: 1
      reactions: []
    id: 64f79fdc9980b96c33e6d9fe
    type: comment
  author: migtissera
  content: 'Awesome!


    Share your app too when you''re ready. I can also help with some LinkedIn/Twitter
    re-sharing. :)'
  created_at: 2023-09-05 20:38:36+00:00
  edited: true
  hidden: false
  id: 64f79fdc9980b96c33e6d9fe
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-06T02:01:44.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9523544907569885
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>How do I use GGML or GGUF models? What''s the best and fastest way
          to use them for inference? Do you have a suggested library? How many tokens/second
          can you achieve with those?</p>

          '
        raw: How do I use GGML or GGUF models? What's the best and fastest way to
          use them for inference? Do you have a suggested library? How many tokens/second
          can you achieve with those?
        updatedAt: '2023-09-06T02:01:44.187Z'
      numEdits: 0
      reactions: []
    id: 64f7dd8854d0fd40d8e8942c
    type: comment
  author: migtissera
  content: How do I use GGML or GGUF models? What's the best and fastest way to use
    them for inference? Do you have a suggested library? How many tokens/second can
    you achieve with those?
  created_at: 2023-09-06 01:01:44+00:00
  edited: false
  hidden: false
  id: 64f7dd8854d0fd40d8e8942c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-06T15:35:56.000Z'
    data:
      edited: false
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9459930062294006
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<p>You can use them in Llama.cpp but I personally use it in Obaboga
          Web UI. GGUF it''s like GGML V2.0 with increased metadata length to store
          more info about model and some additional improvements.  GGML becoming deprecated
          unfortunately pretty fast and GGUF is a new formal now. GGML format was
          designed to run large AI modes in CPU mode so we can use ddr ram instead
          vram. I usually getting 25-30 tokens per sec.</p>

          '
        raw: You can use them in Llama.cpp but I personally use it in Obaboga Web
          UI. GGUF it's like GGML V2.0 with increased metadata length to store more
          info about model and some additional improvements.  GGML becoming deprecated
          unfortunately pretty fast and GGUF is a new formal now. GGML format was
          designed to run large AI modes in CPU mode so we can use ddr ram instead
          vram. I usually getting 25-30 tokens per sec.
        updatedAt: '2023-09-06T15:35:56.224Z'
      numEdits: 0
      reactions: []
    id: 64f89c5c6a71cea1c7f2cf27
    type: comment
  author: Flanua
  content: You can use them in Llama.cpp but I personally use it in Obaboga Web UI.
    GGUF it's like GGML V2.0 with increased metadata length to store more info about
    model and some additional improvements.  GGML becoming deprecated unfortunately
    pretty fast and GGUF is a new formal now. GGML format was designed to run large
    AI modes in CPU mode so we can use ddr ram instead vram. I usually getting 25-30
    tokens per sec.
  created_at: 2023-09-06 14:35:56+00:00
  edited: false
  hidden: false
  id: 64f89c5c6a71cea1c7f2cf27
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-07T05:37:01.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.956786572933197
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: '<p>Is anyone solving 70B inferences to match GPT-3.5''s generation
          time? My 70B models are served with Transformers text generation in 4-bit,
          and it''s super slow!</p>

          '
        raw: Is anyone solving 70B inferences to match GPT-3.5's generation time?
          My 70B models are served with Transformers text generation in 4-bit, and
          it's super slow!
        updatedAt: '2023-09-07T05:37:01.974Z'
      numEdits: 0
      reactions: []
    id: 64f9617d5515d7dccec6b60b
    type: comment
  author: migtissera
  content: Is anyone solving 70B inferences to match GPT-3.5's generation time? My
    70B models are served with Transformers text generation in 4-bit, and it's super
    slow!
  created_at: 2023-09-07 04:37:01+00:00
  edited: false
  hidden: false
  id: 64f9617d5515d7dccec6b60b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
      fullname: Tanaka
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Flanua
      type: user
    createdAt: '2023-09-07T17:17:30.000Z'
    data:
      edited: true
      editors:
      - Flanua
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9754483103752136
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/noauth/0H4Hegmgi1BP09WpqWdaI.jpeg?w=200&h=200&f=face
          fullname: Tanaka
          isHf: false
          isPro: false
          name: Flanua
          type: user
        html: '<blockquote>

          <p>Is anyone solving 70B inferences to match GPT-3.5''s generation time?
          My 70B models are served with Transformers text generation in 4-bit, and
          it''s super slow!</p>

          </blockquote>

          <p>Ideally You need to run it on very large and powerful GPUs to have inference
          speed of GPT-3.5. And by running it in CPU mode like I do well don''t expect
          it to be close to GPT-3.5 inference speed but I''m pretty sure some new
          optimizations on it''s way to improve inference speed (for a CPU mode we
          already can partially offload GGML and GGUF models to GPUs to increase performance)
          maybe we actually would able to squeeze more speed in the near future though.
          Or you can try to rent some powerful cloud GPUs to try and experiment with
          inference speed.<br>Also you can try to use this to run your AI models:<br><a
          rel="nofollow" href="https://github.com/huggingface/text-generation-inference">https://github.com/huggingface/text-generation-inference</a><br>But
          I don''t tried it yet.</p>

          '
        raw: '> Is anyone solving 70B inferences to match GPT-3.5''s generation time?
          My 70B models are served with Transformers text generation in 4-bit, and
          it''s super slow!


          Ideally You need to run it on very large and powerful GPUs to have inference
          speed of GPT-3.5. And by running it in CPU mode like I do well don''t expect
          it to be close to GPT-3.5 inference speed but I''m pretty sure some new
          optimizations on it''s way to improve inference speed (for a CPU mode we
          already can partially offload GGML and GGUF models to GPUs to increase performance)
          maybe we actually would able to squeeze more speed in the near future though.
          Or you can try to rent some powerful cloud GPUs to try and experiment with
          inference speed.

          Also you can try to use this to run your AI models:

          https://github.com/huggingface/text-generation-inference

          But I don''t tried it yet.'
        updatedAt: '2023-09-07T17:48:34.559Z'
      numEdits: 4
      reactions: []
    id: 64fa05aab961d0d12c5d4d0a
    type: comment
  author: Flanua
  content: '> Is anyone solving 70B inferences to match GPT-3.5''s generation time?
    My 70B models are served with Transformers text generation in 4-bit, and it''s
    super slow!


    Ideally You need to run it on very large and powerful GPUs to have inference speed
    of GPT-3.5. And by running it in CPU mode like I do well don''t expect it to be
    close to GPT-3.5 inference speed but I''m pretty sure some new optimizations on
    it''s way to improve inference speed (for a CPU mode we already can partially
    offload GGML and GGUF models to GPUs to increase performance) maybe we actually
    would able to squeeze more speed in the near future though. Or you can try to
    rent some powerful cloud GPUs to try and experiment with inference speed.

    Also you can try to use this to run your AI models:

    https://github.com/huggingface/text-generation-inference

    But I don''t tried it yet.'
  created_at: 2023-09-07 16:17:30+00:00
  edited: true
  hidden: false
  id: 64fa05aab961d0d12c5d4d0a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-07T23:44:39.000Z'
    data:
      edited: false
      editors:
      - migtissera
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5799320340156555
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
          fullname: Migel Tissera
          isHf: false
          isPro: false
          name: migtissera
          type: user
        html: "<p>Yeah that\u2019s what I\u2019m using. <span data-props=\"{&quot;user&quot;:&quot;gotzmann&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/gotzmann\"\
          >@<span class=\"underline\">gotzmann</span></a></span>\n\n\t</span></span>\
          \ any thoughts?</p>\n"
        raw: "Yeah that\u2019s what I\u2019m using. @gotzmann any thoughts?"
        updatedAt: '2023-09-07T23:44:39.130Z'
      numEdits: 0
      reactions: []
    id: 64fa60675ca946a0107365f3
    type: comment
  author: migtissera
  content: "Yeah that\u2019s what I\u2019m using. @gotzmann any thoughts?"
  created_at: 2023-09-07 22:44:39+00:00
  edited: false
  hidden: false
  id: 64fa60675ca946a0107365f3
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/647a6317555b5e199cffd5a2/TykMo31XdtmLTa8uOshGn.jpeg?w=200&h=200&f=face
      fullname: Migel Tissera
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: migtissera
      type: user
    createdAt: '2023-09-11T22:24:37.000Z'
    data:
      status: closed
    id: 64ff93a54be99045d190699b
    type: status-change
  author: migtissera
  created_at: 2023-09-11 21:24:37+00:00
  id: 64ff93a54be99045d190699b
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: migtissera/Synthia-70B-v1.2
repo_type: model
status: closed
target_branch: null
title: GGML & GGUF
