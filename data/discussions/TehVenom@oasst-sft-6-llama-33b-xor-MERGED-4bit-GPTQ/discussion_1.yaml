!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jalcaideguindo
conflicting_files: null
created_at: 2023-04-27 18:56:46+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34f9a4b9747d22749f508c4a1a4f35aa.svg
      fullname: Jorge Alcaide Guindo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jalcaideguindo
      type: user
    createdAt: '2023-04-27T19:56:46.000Z'
    data:
      edited: false
      editors:
      - jalcaideguindo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34f9a4b9747d22749f508c4a1a4f35aa.svg
          fullname: Jorge Alcaide Guindo
          isHf: false
          isPro: false
          name: jalcaideguindo
          type: user
        html: '<p>Hello, thanks for your collaboration.</p>

          <p>I have got this error when trying to discharge the 4-bit model<br>OSError:
          TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ does not appear to have
          a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.</p>

          <p>My code:</p>

          <p>from transformers import AutoTokenizer, AutoModelForCausalLM</p>

          <p>model_name = ''TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ''<br>model
          = AutoModelForCausalLM.from_pretrained(model_name)<br>tokenizer = AutoTokenizer.from_pretrained(model_name)</p>

          '
        raw: "Hello, thanks for your collaboration.\r\n\r\nI have got this error when\
          \ trying to discharge the 4-bit model\r\nOSError: TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ\
          \ does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
          \ or flax_model.msgpack.\r\n\r\n\r\nMy code:\r\n\r\nfrom transformers import\
          \ AutoTokenizer, AutoModelForCausalLM\r\n\r\nmodel_name = 'TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ'\r\
          \nmodel = AutoModelForCausalLM.from_pretrained(model_name)\r\ntokenizer\
          \ = AutoTokenizer.from_pretrained(model_name)\r\n\r\n"
        updatedAt: '2023-04-27T19:56:46.734Z'
      numEdits: 0
      reactions: []
    id: 644ad37e91252984f6454c37
    type: comment
  author: jalcaideguindo
  content: "Hello, thanks for your collaboration.\r\n\r\nI have got this error when\
    \ trying to discharge the 4-bit model\r\nOSError: TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ\
    \ does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt\
    \ or flax_model.msgpack.\r\n\r\n\r\nMy code:\r\n\r\nfrom transformers import AutoTokenizer,\
    \ AutoModelForCausalLM\r\n\r\nmodel_name = 'TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ'\r\
    \nmodel = AutoModelForCausalLM.from_pretrained(model_name)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\
    \n\r\n"
  created_at: 2023-04-27 18:56:46+00:00
  edited: false
  hidden: false
  id: 644ad37e91252984f6454c37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-04-27T20:00:35.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>This is a 33B model that has been converted and quantized down to
          4bit, using the GPTQ library, and thus is not natively supported by transformers.</p>

          <p>Please see the GPTQ repository for code examples on how to run inference
          or how to quantize other models, or use an inference back end with existing
          4bit support,<br>such as Oobabooga''s Text Generation UI,<br>or OccamRazor''s
          4bit fork of KoboldAI:<br><a rel="nofollow" href="https://github.com/0cc4m/KoboldAI">https://github.com/0cc4m/KoboldAI</a></p>

          '
        raw: "This is a 33B model that has been converted and quantized down to 4bit,\
          \ using the GPTQ library, and thus is not natively supported by transformers.\n\
          \nPlease see the GPTQ repository for code examples on how to run inference\
          \ or how to quantize other models, or use an inference back end with existing\
          \ 4bit support, \nsuch as Oobabooga's Text Generation UI, \nor OccamRazor's\
          \ 4bit fork of KoboldAI:\nhttps://github.com/0cc4m/KoboldAI"
        updatedAt: '2023-04-27T20:00:35.790Z'
      numEdits: 0
      reactions: []
    id: 644ad463af97dfd24c11c47e
    type: comment
  author: TehVenom
  content: "This is a 33B model that has been converted and quantized down to 4bit,\
    \ using the GPTQ library, and thus is not natively supported by transformers.\n\
    \nPlease see the GPTQ repository for code examples on how to run inference or\
    \ how to quantize other models, or use an inference back end with existing 4bit\
    \ support, \nsuch as Oobabooga's Text Generation UI, \nor OccamRazor's 4bit fork\
    \ of KoboldAI:\nhttps://github.com/0cc4m/KoboldAI"
  created_at: 2023-04-27 19:00:35+00:00
  edited: false
  hidden: false
  id: 644ad463af97dfd24c11c47e
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-04-27T20:02:40.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>Alternatively, if you have the necessary compute you can just use
          the native transformers version that runs at normal precision (fp16) without
          having to enact changes in your current code:<br><a href="https://huggingface.co/TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit">https://huggingface.co/TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit</a></p>

          '
        raw: 'Alternatively, if you have the necessary compute you can just use the
          native transformers version that runs at normal precision (fp16) without
          having to enact changes in your current code:

          https://huggingface.co/TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit'
        updatedAt: '2023-04-27T20:02:40.926Z'
      numEdits: 0
      reactions: []
    id: 644ad4e0f9f1b0cd3d8fde8b
    type: comment
  author: TehVenom
  content: 'Alternatively, if you have the necessary compute you can just use the
    native transformers version that runs at normal precision (fp16) without having
    to enact changes in your current code:

    https://huggingface.co/TehVenom/oasst-sft-6-llama-33b-xor-MERGED-16bit'
  created_at: 2023-04-27 19:02:40+00:00
  edited: false
  hidden: false
  id: 644ad4e0f9f1b0cd3d8fde8b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/34f9a4b9747d22749f508c4a1a4f35aa.svg
      fullname: Jorge Alcaide Guindo
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jalcaideguindo
      type: user
    createdAt: '2023-04-28T17:53:57.000Z'
    data:
      edited: false
      editors:
      - jalcaideguindo
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/34f9a4b9747d22749f508c4a1a4f35aa.svg
          fullname: Jorge Alcaide Guindo
          isHf: false
          isPro: false
          name: jalcaideguindo
          type: user
        html: '<p>Thanks  for your quick response as well, I will try to load 16 bit
          model once and quantize it by a half.</p>

          '
        raw: Thanks  for your quick response as well, I will try to load 16 bit model
          once and quantize it by a half.
        updatedAt: '2023-04-28T17:53:57.219Z'
      numEdits: 0
      reactions: []
    id: 644c0835194e124dacb9b095
    type: comment
  author: jalcaideguindo
  content: Thanks  for your quick response as well, I will try to load 16 bit model
    once and quantize it by a half.
  created_at: 2023-04-28 16:53:57+00:00
  edited: false
  hidden: false
  id: 644c0835194e124dacb9b095
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-04-28T21:53:44.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<p>I believe BnB (bits and bytes) supports 8bit inference on most hardware,
          so that''s also an option for you.</p>

          '
        raw: I believe BnB (bits and bytes) supports 8bit inference on most hardware,
          so that's also an option for you.
        updatedAt: '2023-04-28T21:53:44.143Z'
      numEdits: 0
      reactions: []
      relatedEventId: 644c406845e79023c7e41679
    id: 644c406845e79023c7e41678
    type: comment
  author: TehVenom
  content: I believe BnB (bits and bytes) supports 8bit inference on most hardware,
    so that's also an option for you.
  created_at: 2023-04-28 20:53:44+00:00
  edited: false
  hidden: false
  id: 644c406845e79023c7e41678
  type: comment
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-04-28T21:53:44.000Z'
    data:
      status: closed
    id: 644c406845e79023c7e41679
    type: status-change
  author: TehVenom
  created_at: 2023-04-28 20:53:44+00:00
  id: 644c406845e79023c7e41679
  new_status: closed
  type: status-change
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673691044776-63c27ecd456b30b44e76e129.png?w=200&h=200&f=face
      fullname: Paillat
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: Paillat
      type: user
    createdAt: '2023-05-09T14:22:00.000Z'
    data:
      edited: false
      editors:
      - Paillat
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1673691044776-63c27ecd456b30b44e76e129.png?w=200&h=200&f=face
          fullname: Paillat
          isHf: false
          isPro: false
          name: Paillat
          type: user
        html: '<p>Hey, just was wondering: I am rnning the model on <a rel="nofollow"
          href="https://github.com/0cc4m/KoboldAI">https://github.com/0cc4m/KoboldAI</a>
          with an nvidia a4500 wich has 24gb vram. Is that supposed to be sufficient
          since the model is 16gb large? because I am getting a <code>RuntimeError:
          [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040
          bytes.</code> error on cpu, however I am loading it completely on gpu</p>

          '
        raw: 'Hey, just was wondering: I am rnning the model on https://github.com/0cc4m/KoboldAI
          with an nvidia a4500 wich has 24gb vram. Is that supposed to be sufficient
          since the model is 16gb large? because I am getting a `RuntimeError: [enforce
          fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040
          bytes.` error on cpu, however I am loading it completely on gpu'
        updatedAt: '2023-05-09T14:22:00.690Z'
      numEdits: 0
      reactions: []
    id: 645a57084e99fa4301949471
    type: comment
  author: Paillat
  content: 'Hey, just was wondering: I am rnning the model on https://github.com/0cc4m/KoboldAI
    with an nvidia a4500 wich has 24gb vram. Is that supposed to be sufficient since
    the model is 16gb large? because I am getting a `RuntimeError: [enforce fail at
    C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator:
    not enough memory: you tried to allocate 238551040 bytes.` error on cpu, however
    I am loading it completely on gpu'
  created_at: 2023-05-09 13:22:00+00:00
  edited: false
  hidden: false
  id: 645a57084e99fa4301949471
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
      fullname: TeH_Venom
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: TehVenom
      type: user
    createdAt: '2023-05-09T14:24:30.000Z'
    data:
      edited: false
      editors:
      - TehVenom
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/63ede1df81a7f0c8a4ce046a/93-0BQSJA1H93soqi7fiC.jpeg?w=200&h=200&f=face
          fullname: TeH_Venom
          isHf: false
          isPro: false
          name: TehVenom
          type: user
        html: '<blockquote>

          <p>Hey, just was wondering: I am rnning the model on <a rel="nofollow" href="https://github.com/0cc4m/KoboldAI">https://github.com/0cc4m/KoboldAI</a>
          with an nvidia a4500 wich has 24gb vram. Is that supposed to be sufficient
          since the model is 16gb large? because I am getting a <code>RuntimeError:
          [enforce fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040
          bytes.</code> error on cpu, however I am loading it completely on gpu</p>

          </blockquote>

          <p>You''re running out of CPU memory, i.e. RAM. Setup a swap space or get
          more RAM. The model loads up on RAM out of the disk, and only then gets
          offloaded into VRAM for inference.</p>

          '
        raw: '> Hey, just was wondering: I am rnning the model on https://github.com/0cc4m/KoboldAI
          with an nvidia a4500 wich has 24gb vram. Is that supposed to be sufficient
          since the model is 16gb large? because I am getting a `RuntimeError: [enforce
          fail at C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72]
          data. DefaultCPUAllocator: not enough memory: you tried to allocate 238551040
          bytes.` error on cpu, however I am loading it completely on gpu


          You''re running out of CPU memory, i.e. RAM. Setup a swap space or get more
          RAM. The model loads up on RAM out of the disk, and only then gets offloaded
          into VRAM for inference.'
        updatedAt: '2023-05-09T14:24:30.743Z'
      numEdits: 0
      reactions: []
    id: 645a579e99a55c57325a91e7
    type: comment
  author: TehVenom
  content: '> Hey, just was wondering: I am rnning the model on https://github.com/0cc4m/KoboldAI
    with an nvidia a4500 wich has 24gb vram. Is that supposed to be sufficient since
    the model is 16gb large? because I am getting a `RuntimeError: [enforce fail at
    C:\cb\pytorch_1000000000000\work\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator:
    not enough memory: you tried to allocate 238551040 bytes.` error on cpu, however
    I am loading it completely on gpu


    You''re running out of CPU memory, i.e. RAM. Setup a swap space or get more RAM.
    The model loads up on RAM out of the disk, and only then gets offloaded into VRAM
    for inference.'
  created_at: 2023-05-09 13:24:30+00:00
  edited: false
  hidden: false
  id: 645a579e99a55c57325a91e7
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TehVenom/oasst-sft-6-llama-33b-xor-MERGED-4bit-GPTQ
repo_type: model
status: closed
target_branch: null
title: pytorch_model.bin
