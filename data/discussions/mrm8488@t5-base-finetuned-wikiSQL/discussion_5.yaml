!!python/object:huggingface_hub.community.DiscussionWithDetails
author: ahe61
conflicting_files: null
created_at: 2023-05-23 18:02:11+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c98b4cc351f9d843a615cc4da887387c.svg
      fullname: Andreas Heiner
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: ahe61
      type: user
    createdAt: '2023-05-23T19:02:11.000Z'
    data:
      edited: false
      editors:
      - ahe61
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c98b4cc351f9d843a615cc4da887387c.svg
          fullname: Andreas Heiner
          isHf: false
          isPro: false
          name: ahe61
          type: user
        html: '<p>Hi,</p>

          <p>I run into an error while running your model</p>

          <p>! pip install transformers<br>from transformers import AutoModelWithLMHead,
          AutoTokenizer</p>

          <p>tokenizer = AutoTokenizer.from_pretrained("mrm8488/t5-base-finetuned-wikiSQL")<br>model
          = AutoModelWithLMHead.from_pretrained("mrm8488/t5-base-finetuned-wikiSQL")</p>

          <p>Error message<br>TypeError: Couldn''t build proto file into descriptor
          pool: duplicate file name (sentencepiece_model.proto)</p>

          <p>(and warnings<br>The <code>xla_device</code> argument has been deprecated
          in v4.4.0 of Transformers. It is ignored and you can safely remove it from
          your <code>config.json</code> file.<br>The <code>xla_device</code> argument
          has been deprecated in v4.4.0 of Transformers. It is ignored and you can
          safely remove it from your <code>config.json</code> file.<br>The <code>xla_device</code>
          argument has been deprecated in v4.4.0 of Transformers. It is ignored and
          you can safely remove it from your <code>config.json</code> file.)</p>

          <p>it does work with standard bert models (or no model at all)</p>

          <p>I run python3.10</p>

          <p>best,</p>

          <h2 id="andreas">Andreas</h2>

          <p>full dump:<br>Cell In[4], line 3<br>      1 from transformers import
          AutoModelWithLMHead, AutoTokenizer<br>----&gt; 3 tokenizer = AutoTokenizer.from_pretrained("mrm8488/t5-base-finetuned-wikiSQL")<br>      4
          #model = AutoModelWithLMHead.from_pretrained("mrm8488/t5-base-finetuned-wikiSQL")</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:659,
          in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,
          **kwargs)<br>    657 tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]<br>    658
          if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):<br>--&gt;
          659     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,
          *inputs, **kwargs)<br>    660 else:<br>    661     if tokenizer_class_py
          is not None:</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1801,
          in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,
          *init_inputs, **kwargs)<br>   1798     else:<br>   1799         logger.info(f"loading
          file {file_path} from cache at {resolved_vocab_files[file_id]}")<br>-&gt;
          1801 return cls._from_pretrained(<br>   1802     resolved_vocab_files,<br>   1803     pretrained_model_name_or_path,<br>   1804     init_configuration,<br>   1805     *init_inputs,<br>   1806     use_auth_token=use_auth_token,<br>   1807     cache_dir=cache_dir,<br>   1808     local_files_only=local_files_only,<br>   1809     _commit_hash=commit_hash,<br>   1810     **kwargs,<br>   1811
          )</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1956,
          in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,
          init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,
          *init_inputs, **kwargs)<br>   1954 # Instantiate tokenizer.<br>   1955 try:<br>-&gt;
          1956     tokenizer = cls(*init_inputs, **init_kwargs)<br>   1957 except
          OSError:<br>   1958     raise OSError(<br>   1959         "Unable to load
          vocabulary from file. "<br>   1960         "Please check that the provided
          vocabulary is accessible and not corrupted."<br>   1961     )</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:133,
          in T5TokenizerFast.<strong>init</strong>(self, vocab_file, tokenizer_file,
          eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)<br>    126     if
          extra_tokens != extra_ids:<br>    127         raise ValueError(<br>    128             f"Both
          extra_ids ({extra_ids}) and additional_special_tokens ({additional_special_tokens})
          are"<br>    129             " provided to T5Tokenizer. In this case the
          additional_special_tokens must include the extra_ids"<br>    130             "
          tokens"<br>    131         )<br>--&gt; 133 super().<strong>init</strong>(<br>    134     vocab_file,<br>    135     tokenizer_file=tokenizer_file,<br>    136     eos_token=eos_token,<br>    137     unk_token=unk_token,<br>    138     pad_token=pad_token,<br>    139     extra_ids=extra_ids,<br>    140     additional_special_tokens=additional_special_tokens,<br>    141     **kwargs,<br>    142
          )<br>    144 self.vocab_file = vocab_file<br>    145 self.can_save_slow_tokenizer
          = False if not self.vocab_file else True</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:114,
          in PreTrainedTokenizerFast.<strong>init</strong>(self, *args, **kwargs)<br>    111     fast_tokenizer
          = TokenizerFast.from_file(fast_tokenizer_file)<br>    112 elif slow_tokenizer
          is not None:<br>    113     # We need to convert a slow tokenizer to build
          the backend<br>--&gt; 114     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)<br>    115
          elif self.slow_tokenizer_class is not None:<br>    116     # We need to
          create and convert a slow tokenizer to build the backend<br>    117     slow_tokenizer
          = self.slow_tokenizer_class(*args, **kwargs)</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1162,
          in convert_slow_tokenizer(transformer_tokenizer)<br>   1154     raise ValueError(<br>   1155         f"An
          instance of tokenizer class {tokenizer_class_name} cannot be converted in
          a Fast tokenizer instance."<br>   1156         " No converter was found.
          Currently available slow-&gt;fast convertors:"<br>   1157         f" {list(SLOW_TO_FAST_CONVERTERS.keys())}"<br>   1158     )<br>   1160
          converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]<br>-&gt;
          1162 return converter_class(transformer_tokenizer).converted()</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:438,
          in SpmConverter.<strong>init</strong>(self, *args)<br>    434 requires_backends(self,
          "protobuf")<br>    436 super().<strong>init</strong>(*args)<br>--&gt; 438
          from .utils import sentencepiece_model_pb2 as model_pb2<br>    440 m = model_pb2.ModelProto()<br>    441
          with open(self.original_tokenizer.vocab_file, "rb") as f:</p>

          <p>File ~/.local/lib/python3.10/site-packages/transformers/utils/sentencepiece_model_pb2.py:29<br>     24
          # @@protoc_insertion_point(imports)<br>     26 _sym_db = _symbol_database.Default()<br>---&gt;
          29 DESCRIPTOR = _descriptor.FileDescriptor(<br>     30     name="sentencepiece_model.proto",<br>     31     package="sentencepiece",<br>     32     syntax="proto2",<br>     33     serialized_options=b"H\003",<br>     34     create_key=_descriptor._internal_create_key,<br>     35     serialized_pb=(<br>     36         b''\n\x19sentencepiece_model.proto\x12\rsentencepiece"\xa1\n\n\x0bTrainerSpec\x12\r\n\x05input\x18\x01''<br>     37         b"
          \x03(\t\x12\x14\n\x0cinput_format\x18\x07 \x01(\t\x12\x14\n\x0cmodel_prefix\x18\x02"<br>     38         b"
          \x01(\t\x12\x41\n\nmodel_type\x18\x03"<br>     39         b" \x01(\x0e\x32$.sentencepiece.TrainerSpec.ModelType:\x07UNIGRAM\x12\x18\n\nvocab_size\x18\x04"<br>     40         b"
          \x01(\x05:\x04\x38\x30\x30\x30\x12\x17\n\x0f\x61\x63\x63\x65pt_language\x18\x05
          \x03(\t\x12"<br>     41         b'' \n\x15self_test_sample_size\x18\x06
          \x01(\x05:\x01\x30\x12"\n\x12\x63haracter_coverage\x18\n''<br>     42         b"
          \x01(\x02:\x06\x30.9995\x12\x1e\n\x13input_sentence_size\x18\x0b"<br>     43         b"
          \x01(\x04:\x01\x30\x12$\n\x16shuffle_input_sentence\x18\x13 \x01(\x08:\x04true\x12"<br>     44         b''
          \n\x14mining_sentence_size\x18\x0c \x01(\x05\x42\x02\x18\x01\x12"\n\x16training_sentence_size\x18\r''<br>     45         b"
          \x01(\x05\x42\x02\x18\x01\x12(\n\x17seed_sentencepiece_size\x18\x0e"<br>     46         b"
          \x01(\x05:\x07\x31\x30\x30\x30\x30\x30\x30\x12\x1e\n\x10shrinking_factor\x18\x0f"<br>     47         b"
          \x01(\x02:\x04\x30.75\x12!\n\x13max_sentence_length\x18\x12"<br>     48         b"
          \x01(\x05:\x04\x34\x31\x39\x32\x12\x17\n\x0bnum_threads\x18\x10"<br>     49         b"
          \x01(\x05:\x02\x31\x36\x12\x1d\n\x12num_sub_iterations\x18\x11"<br>     50         b"
          \x01(\x05:\x01\x32\x12$\n\x18max_sentencepiece_length\x18\x14"<br>     51         b"
          \x01(\x05:\x02\x31\x36\x12%\n\x17split_by_unicode_script\x18\x15"<br>     52         b"
          \x01(\x08:\x04true\x12\x1d\n\x0fsplit_by_number\x18\x17"<br>     53         b"
          \x01(\x08:\x04true\x12!\n\x13split_by_whitespace\x18\x16"<br>     54         b"
          \x01(\x08:\x04true\x12)\n\x1atreat_whitespace_as_suffix\x18\x18"<br>     55         b"
          \x01(\x08:\x05\x66\x61lse\x12\x1b\n\x0csplit_digits\x18\x19"<br>     56         b"
          \x01(\x08:\x05\x66\x61lse\x12\x17\n\x0f\x63ontrol_symbols\x18\x1e"<br>     57         b"
          \x03(\t\x12\x1c\n\x14user_defined_symbols\x18\x1f \x03(\t\x12\x16\n\x0erequired_chars\x18$"<br>     58         b"
          \x01(\t\x12\x1c\n\rbyte_fallback\x18# \x01(\x08:\x05\x66\x61lse\x12+\n\x1dvocabulary_output_piece_score\x18"<br>     59         b''  \x01(\x08:\x04true\x12\x1e\n\x10hard_vocab_limit\x18!
          \x01(\x08:\x04true\x12\x1c\n\ruse_all_vocab\x18"''<br>     60         b"
          \x01(\x08:\x05\x66\x61lse\x12\x11\n\x06unk_id\x18( \x01(\x05:\x01\x30\x12\x11\n\x06\x62os_id\x18)"<br>     61         b"
          \x01(\x05:\x01\x31\x12\x11\n\x06\x65os_id\x18* \x01(\x05:\x01\x32\x12\x12\n\x06pad_id\x18+"<br>     62         b"
          \x01(\x05:\x02-1\x12\x18\n\tunk_piece\x18- \x01(\t:\x05\x12\x16\n\tbos_piece\x18."<br>     63         b"
          \x01(\t:\x03<s>\x12\x17\n\teos_piece\x18/ \x01(\t:\x04</s>\x12\x18\n\tpad_piece\x18\x30"<br>     64         b"
          \x01(\t:\x05\x12\x1a\n\x0bunk_surface\x18, \x01(\t:\x05 \xe2\x81\x87"<br>     65         b"
          \x12+\n\x1ctrain_extremely_large_corpus\x18\x31"<br>     66         b''
          \x01(\x08:\x05\x66\x61lse"5\n\tModelType\x12\x0b\n\x07UNIGRAM\x10\x01\x12\x07\n\x03\x42PE\x10\x02\x12\x08\n\x04WORD\x10\x03\x12\x08\n\x04\x43HAR\x10\x04<em>\t\x08\xc8\x01\x10\x80\x80\x80\x80\x02"\xd1\x01\n\x0eNormalizerSpec\x12\x0c\n\x04name\x18\x01''<br>     67         b"
          \x01(\t\x12\x1c\n\x14precompiled_charsmap\x18\x02 \x01(\x0c\x12\x1e\n\x10\x61\x64\x64_dummy_prefix\x18\x03"<br>     68         b"
          \x01(\x08:\x04true\x12&amp;\n\x18remove_extra_whitespaces\x18\x04 \x01(\x08:\x04true\x12"<br>     69         b"
          \n\x12\x65scape_whitespaces\x18\x05 \x01(\x08:\x04true\x12\x1e\n\x16normalization_rule_tsv\x18\x06"<br>     70         b''
          \x01(\t</em>\t\x08\xc8\x01\x10\x80\x80\x80\x80\x02"y\n\x0cSelfTestData\x12\x33\n\x07samples\x18\x01''<br>     71         b''
          \x03(\x0b\x32".sentencepiece.SelfTestData.Sample\x1a)\n\x06Sample\x12\r\n\x05input\x18\x01''<br>     72         b"
          \x01(\t\x12\x10\n\x08\x65xpected\x18\x02"<br>     73         b'' \x01(\t<em>\t\x08\xc8\x01\x10\x80\x80\x80\x80\x02"\xfe\x03\n\nModelProto\x12\x37\n\x06pieces\x18\x01''<br>     74         b"
          \x03(\x0b\x32''.sentencepiece.ModelProto.SentencePiece\x12\x30\n\x0ctrainer_spec\x18\x02"<br>     75         b"
          \x01(\x0b\x32\x1a.sentencepiece.TrainerSpec\x12\x36\n\x0fnormalizer_spec\x18\x03"<br>     76         b"
          \x01(\x0b\x32\x1d.sentencepiece.NormalizerSpec\x12\x33\n\x0eself_test_data\x18\x04"<br>     77         b"
          \x01(\x0b\x32\x1b.sentencepiece.SelfTestData\x12\x38\n\x11\x64\x65normalizer_spec\x18\x05"<br>     78         b"
          \x01(\x0b\x32\x1d.sentencepiece.NormalizerSpec\x1a\xd2\x01\n\rSentencePiece\x12\r\n\x05piece\x18\x01"<br>     79         b"
          \x01(\t\x12\r\n\x05score\x18\x02 \x01(\x02\x12\x42\n\x04type\x18\x03"<br>     80         b''
          \x01(\x0e\x32,.sentencepiece.ModelProto.SentencePiece.Type:\x06NORMAL"T\n\x04Type\x12\n\n\x06NORMAL\x10\x01\x12\x0b\n\x07UNKNOWN\x10\x02\x12\x0b\n\x07\x43ONTROL\x10\x03\x12\x10\n\x0cUSER_DEFINED\x10\x04\x12\x08\n\x04\x42YTE\x10\x06\x12\n\n\x06UNUSED\x10\x05</em>\t\x08\xc8\x01\x10\x80\x80\x80\x80\x02*\t\x08\xc8\x01\x10\x80\x80\x80\x80\x02\x42\x02H\x03''<br>     81     ),<br>     82
          )<br>     85 _TRAINERSPEC_MODELTYPE = _descriptor.EnumDescriptor(<br>     86     name="ModelType",<br>     87     full_name="sentencepiece.TrainerSpec.ModelType",<br>   (...)<br>    128     serialized_end=1347,<br>    129
          )<br>    130 _sym_db.RegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)</p>

          <p>File ~/.local/lib/python3.10/site-packages/google/protobuf/descriptor.py:1028,
          in FileDescriptor.<strong>new</strong>(cls, name, package, options, serialized_options,
          serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)<br>   1026     raise
          RuntimeError(''Please link in cpp generated lib for %s'' % (name))<br>   1027
          elif serialized_pb:<br>-&gt; 1028   return _message.default_pool.AddSerializedFile(serialized_pb)<br>   1029
          else:<br>   1030   return super(FileDescriptor, cls).<strong>new</strong>(cls)</p>

          <p>TypeError: Couldn''t build proto file into descriptor pool: duplicate
          file name (sentencepiece_model.proto)</p>

          '
        raw: "Hi,\r\n\r\nI run into an error while running your model\r\n\r\n! pip\
          \ install transformers\r\nfrom transformers import AutoModelWithLMHead,\
          \ AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\"\
          )\r\nmodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\"\
          )\r\n\r\nError message\r\nTypeError: Couldn't build proto file into descriptor\
          \ pool: duplicate file name (sentencepiece_model.proto)\r\n\r\n(and warnings\r\
          \nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers.\
          \ It is ignored and you can safely remove it from your `config.json` file.\r\
          \nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers.\
          \ It is ignored and you can safely remove it from your `config.json` file.\r\
          \nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers.\
          \ It is ignored and you can safely remove it from your `config.json` file.)\r\
          \n\r\nit does work with standard bert models (or no model at all)\r\n\r\n\
          I run python3.10\r\n\r\nbest,\r\n\r\nAndreas\r\n---\r\nfull dump:\r\nCell\
          \ In[4], line 3\r\n      1 from transformers import AutoModelWithLMHead,\
          \ AutoTokenizer\r\n----> 3 tokenizer = AutoTokenizer.from_pretrained(\"\
          mrm8488/t5-base-finetuned-wikiSQL\")\r\n      4 #model = AutoModelWithLMHead.from_pretrained(\"\
          mrm8488/t5-base-finetuned-wikiSQL\")\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:659,\
          \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
          \ **kwargs)\r\n    657 tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\
          \n    658 if tokenizer_class_fast and (use_fast or tokenizer_class_py is\
          \ None):\r\n--> 659     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
          \ *inputs, **kwargs)\r\n    660 else:\r\n    661     if tokenizer_class_py\
          \ is not None:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1801,\
          \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
          \ *init_inputs, **kwargs)\r\n   1798     else:\r\n   1799         logger.info(f\"\
          loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\
          )\r\n-> 1801 return cls._from_pretrained(\r\n   1802     resolved_vocab_files,\r\
          \n   1803     pretrained_model_name_or_path,\r\n   1804     init_configuration,\r\
          \n   1805     *init_inputs,\r\n   1806     use_auth_token=use_auth_token,\r\
          \n   1807     cache_dir=cache_dir,\r\n   1808     local_files_only=local_files_only,\r\
          \n   1809     _commit_hash=commit_hash,\r\n   1810     **kwargs,\r\n   1811\
          \ )\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1956,\
          \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files,\
          \ pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir,\
          \ local_files_only, _commit_hash, *init_inputs, **kwargs)\r\n   1954 # Instantiate\
          \ tokenizer.\r\n   1955 try:\r\n-> 1956     tokenizer = cls(*init_inputs,\
          \ **init_kwargs)\r\n   1957 except OSError:\r\n   1958     raise OSError(\r\
          \n   1959         \"Unable to load vocabulary from file. \"\r\n   1960 \
          \        \"Please check that the provided vocabulary is accessible and not\
          \ corrupted.\"\r\n   1961     )\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:133,\
          \ in T5TokenizerFast.__init__(self, vocab_file, tokenizer_file, eos_token,\
          \ unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\r\
          \n    126     if extra_tokens != extra_ids:\r\n    127         raise ValueError(\r\
          \n    128             f\"Both extra_ids ({extra_ids}) and additional_special_tokens\
          \ ({additional_special_tokens}) are\"\r\n    129             \" provided\
          \ to T5Tokenizer. In this case the additional_special_tokens must include\
          \ the extra_ids\"\r\n    130             \" tokens\"\r\n    131        \
          \ )\r\n--> 133 super().__init__(\r\n    134     vocab_file,\r\n    135 \
          \    tokenizer_file=tokenizer_file,\r\n    136     eos_token=eos_token,\r\
          \n    137     unk_token=unk_token,\r\n    138     pad_token=pad_token,\r\
          \n    139     extra_ids=extra_ids,\r\n    140     additional_special_tokens=additional_special_tokens,\r\
          \n    141     **kwargs,\r\n    142 )\r\n    144 self.vocab_file = vocab_file\r\
          \n    145 self.can_save_slow_tokenizer = False if not self.vocab_file else\
          \ True\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:114,\
          \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\r\n    111\
          \     fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)\r\n\
          \    112 elif slow_tokenizer is not None:\r\n    113     # We need to convert\
          \ a slow tokenizer to build the backend\r\n--> 114     fast_tokenizer =\
          \ convert_slow_tokenizer(slow_tokenizer)\r\n    115 elif self.slow_tokenizer_class\
          \ is not None:\r\n    116     # We need to create and convert a slow tokenizer\
          \ to build the backend\r\n    117     slow_tokenizer = self.slow_tokenizer_class(*args,\
          \ **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1162,\
          \ in convert_slow_tokenizer(transformer_tokenizer)\r\n   1154     raise\
          \ ValueError(\r\n   1155         f\"An instance of tokenizer class {tokenizer_class_name}\
          \ cannot be converted in a Fast tokenizer instance.\"\r\n   1156       \
          \  \" No converter was found. Currently available slow->fast convertors:\"\
          \r\n   1157         f\" {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\r\n   1158\
          \     )\r\n   1160 converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\r\
          \n-> 1162 return converter_class(transformer_tokenizer).converted()\r\n\r\
          \nFile ~/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:438,\
          \ in SpmConverter.__init__(self, *args)\r\n    434 requires_backends(self,\
          \ \"protobuf\")\r\n    436 super().__init__(*args)\r\n--> 438 from .utils\
          \ import sentencepiece_model_pb2 as model_pb2\r\n    440 m = model_pb2.ModelProto()\r\
          \n    441 with open(self.original_tokenizer.vocab_file, \"rb\") as f:\r\n\
          \r\nFile ~/.local/lib/python3.10/site-packages/transformers/utils/sentencepiece_model_pb2.py:29\r\
          \n     24 # @@protoc_insertion_point(imports)\r\n     26 _sym_db = _symbol_database.Default()\r\
          \n---> 29 DESCRIPTOR = _descriptor.FileDescriptor(\r\n     30     name=\"\
          sentencepiece_model.proto\",\r\n     31     package=\"sentencepiece\",\r\
          \n     32     syntax=\"proto2\",\r\n     33     serialized_options=b\"H\\\
          003\",\r\n     34     create_key=_descriptor._internal_create_key,\r\n \
          \    35     serialized_pb=(\r\n     36         b'\\n\\x19sentencepiece_model.proto\\\
          x12\\rsentencepiece\"\\xa1\\n\\n\\x0bTrainerSpec\\x12\\r\\n\\x05input\\\
          x18\\x01'\r\n     37         b\" \\x03(\\t\\x12\\x14\\n\\x0cinput_format\\\
          x18\\x07 \\x01(\\t\\x12\\x14\\n\\x0cmodel_prefix\\x18\\x02\"\r\n     38\
          \         b\" \\x01(\\t\\x12\\x41\\n\\nmodel_type\\x18\\x03\"\r\n     39\
          \         b\" \\x01(\\x0e\\x32$.sentencepiece.TrainerSpec.ModelType:\\x07UNIGRAM\\\
          x12\\x18\\n\\nvocab_size\\x18\\x04\"\r\n     40         b\" \\x01(\\x05:\\\
          x04\\x38\\x30\\x30\\x30\\x12\\x17\\n\\x0f\\x61\\x63\\x63\\x65pt_language\\\
          x18\\x05 \\x03(\\t\\x12\"\r\n     41         b' \\n\\x15self_test_sample_size\\\
          x18\\x06 \\x01(\\x05:\\x01\\x30\\x12\"\\n\\x12\\x63haracter_coverage\\x18\\\
          n'\r\n     42         b\" \\x01(\\x02:\\x06\\x30.9995\\x12\\x1e\\n\\x13input_sentence_size\\\
          x18\\x0b\"\r\n     43         b\" \\x01(\\x04:\\x01\\x30\\x12$\\n\\x16shuffle_input_sentence\\\
          x18\\x13 \\x01(\\x08:\\x04true\\x12\"\r\n     44         b' \\n\\x14mining_sentence_size\\\
          x18\\x0c \\x01(\\x05\\x42\\x02\\x18\\x01\\x12\"\\n\\x16training_sentence_size\\\
          x18\\r'\r\n     45         b\" \\x01(\\x05\\x42\\x02\\x18\\x01\\x12(\\n\\\
          x17seed_sentencepiece_size\\x18\\x0e\"\r\n     46         b\" \\x01(\\x05:\\\
          x07\\x31\\x30\\x30\\x30\\x30\\x30\\x30\\x12\\x1e\\n\\x10shrinking_factor\\\
          x18\\x0f\"\r\n     47         b\" \\x01(\\x02:\\x04\\x30.75\\x12!\\n\\x13max_sentence_length\\\
          x18\\x12\"\r\n     48         b\" \\x01(\\x05:\\x04\\x34\\x31\\x39\\x32\\\
          x12\\x17\\n\\x0bnum_threads\\x18\\x10\"\r\n     49         b\" \\x01(\\\
          x05:\\x02\\x31\\x36\\x12\\x1d\\n\\x12num_sub_iterations\\x18\\x11\"\r\n\
          \     50         b\" \\x01(\\x05:\\x01\\x32\\x12$\\n\\x18max_sentencepiece_length\\\
          x18\\x14\"\r\n     51         b\" \\x01(\\x05:\\x02\\x31\\x36\\x12%\\n\\\
          x17split_by_unicode_script\\x18\\x15\"\r\n     52         b\" \\x01(\\x08:\\\
          x04true\\x12\\x1d\\n\\x0fsplit_by_number\\x18\\x17\"\r\n     53        \
          \ b\" \\x01(\\x08:\\x04true\\x12!\\n\\x13split_by_whitespace\\x18\\x16\"\
          \r\n     54         b\" \\x01(\\x08:\\x04true\\x12)\\n\\x1atreat_whitespace_as_suffix\\\
          x18\\x18\"\r\n     55         b\" \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\
          x1b\\n\\x0csplit_digits\\x18\\x19\"\r\n     56         b\" \\x01(\\x08:\\\
          x05\\x66\\x61lse\\x12\\x17\\n\\x0f\\x63ontrol_symbols\\x18\\x1e\"\r\n  \
          \   57         b\" \\x03(\\t\\x12\\x1c\\n\\x14user_defined_symbols\\x18\\\
          x1f \\x03(\\t\\x12\\x16\\n\\x0erequired_chars\\x18$\"\r\n     58       \
          \  b\" \\x01(\\t\\x12\\x1c\\n\\rbyte_fallback\\x18# \\x01(\\x08:\\x05\\\
          x66\\x61lse\\x12+\\n\\x1dvocabulary_output_piece_score\\x18\"\r\n     59\
          \         b'  \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x10hard_vocab_limit\\\
          x18! \\x01(\\x08:\\x04true\\x12\\x1c\\n\\ruse_all_vocab\\x18\"'\r\n    \
          \ 60         b\" \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x11\\n\\x06unk_id\\\
          x18( \\x01(\\x05:\\x01\\x30\\x12\\x11\\n\\x06\\x62os_id\\x18)\"\r\n    \
          \ 61         b\" \\x01(\\x05:\\x01\\x31\\x12\\x11\\n\\x06\\x65os_id\\x18*\
          \ \\x01(\\x05:\\x01\\x32\\x12\\x12\\n\\x06pad_id\\x18+\"\r\n     62    \
          \     b\" \\x01(\\x05:\\x02-1\\x12\\x18\\n\\tunk_piece\\x18- \\x01(\\t:\\\
          x05<unk>\\x12\\x16\\n\\tbos_piece\\x18.\"\r\n     63         b\" \\x01(\\\
          t:\\x03<s>\\x12\\x17\\n\\teos_piece\\x18/ \\x01(\\t:\\x04</s>\\x12\\x18\\\
          n\\tpad_piece\\x18\\x30\"\r\n     64         b\" \\x01(\\t:\\x05<pad>\\\
          x12\\x1a\\n\\x0bunk_surface\\x18, \\x01(\\t:\\x05 \\xe2\\x81\\x87\"\r\n\
          \     65         b\" \\x12+\\n\\x1ctrain_extremely_large_corpus\\x18\\x31\"\
          \r\n     66         b' \\x01(\\x08:\\x05\\x66\\x61lse\"5\\n\\tModelType\\\
          x12\\x0b\\n\\x07UNIGRAM\\x10\\x01\\x12\\x07\\n\\x03\\x42PE\\x10\\x02\\x12\\\
          x08\\n\\x04WORD\\x10\\x03\\x12\\x08\\n\\x04\\x43HAR\\x10\\x04*\\t\\x08\\\
          xc8\\x01\\x10\\x80\\x80\\x80\\x80\\x02\"\\xd1\\x01\\n\\x0eNormalizerSpec\\\
          x12\\x0c\\n\\x04name\\x18\\x01'\r\n     67         b\" \\x01(\\t\\x12\\\
          x1c\\n\\x14precompiled_charsmap\\x18\\x02 \\x01(\\x0c\\x12\\x1e\\n\\x10\\\
          x61\\x64\\x64_dummy_prefix\\x18\\x03\"\r\n     68         b\" \\x01(\\x08:\\\
          x04true\\x12&\\n\\x18remove_extra_whitespaces\\x18\\x04 \\x01(\\x08:\\x04true\\\
          x12\"\r\n     69         b\" \\n\\x12\\x65scape_whitespaces\\x18\\x05 \\\
          x01(\\x08:\\x04true\\x12\\x1e\\n\\x16normalization_rule_tsv\\x18\\x06\"\r\
          \n     70         b' \\x01(\\t*\\t\\x08\\xc8\\x01\\x10\\x80\\x80\\x80\\\
          x80\\x02\"y\\n\\x0cSelfTestData\\x12\\x33\\n\\x07samples\\x18\\x01'\r\n\
          \     71         b' \\x03(\\x0b\\x32\".sentencepiece.SelfTestData.Sample\\\
          x1a)\\n\\x06Sample\\x12\\r\\n\\x05input\\x18\\x01'\r\n     72         b\"\
          \ \\x01(\\t\\x12\\x10\\n\\x08\\x65xpected\\x18\\x02\"\r\n     73       \
          \  b' \\x01(\\t*\\t\\x08\\xc8\\x01\\x10\\x80\\x80\\x80\\x80\\x02\"\\xfe\\\
          x03\\n\\nModelProto\\x12\\x37\\n\\x06pieces\\x18\\x01'\r\n     74      \
          \   b\" \\x03(\\x0b\\x32'.sentencepiece.ModelProto.SentencePiece\\x12\\\
          x30\\n\\x0ctrainer_spec\\x18\\x02\"\r\n     75         b\" \\x01(\\x0b\\\
          x32\\x1a.sentencepiece.TrainerSpec\\x12\\x36\\n\\x0fnormalizer_spec\\x18\\\
          x03\"\r\n     76         b\" \\x01(\\x0b\\x32\\x1d.sentencepiece.NormalizerSpec\\\
          x12\\x33\\n\\x0eself_test_data\\x18\\x04\"\r\n     77         b\" \\x01(\\\
          x0b\\x32\\x1b.sentencepiece.SelfTestData\\x12\\x38\\n\\x11\\x64\\x65normalizer_spec\\\
          x18\\x05\"\r\n     78         b\" \\x01(\\x0b\\x32\\x1d.sentencepiece.NormalizerSpec\\\
          x1a\\xd2\\x01\\n\\rSentencePiece\\x12\\r\\n\\x05piece\\x18\\x01\"\r\n  \
          \   79         b\" \\x01(\\t\\x12\\r\\n\\x05score\\x18\\x02 \\x01(\\x02\\\
          x12\\x42\\n\\x04type\\x18\\x03\"\r\n     80         b' \\x01(\\x0e\\x32,.sentencepiece.ModelProto.SentencePiece.Type:\\\
          x06NORMAL\"T\\n\\x04Type\\x12\\n\\n\\x06NORMAL\\x10\\x01\\x12\\x0b\\n\\\
          x07UNKNOWN\\x10\\x02\\x12\\x0b\\n\\x07\\x43ONTROL\\x10\\x03\\x12\\x10\\\
          n\\x0cUSER_DEFINED\\x10\\x04\\x12\\x08\\n\\x04\\x42YTE\\x10\\x06\\x12\\\
          n\\n\\x06UNUSED\\x10\\x05*\\t\\x08\\xc8\\x01\\x10\\x80\\x80\\x80\\x80\\\
          x02*\\t\\x08\\xc8\\x01\\x10\\x80\\x80\\x80\\x80\\x02\\x42\\x02H\\x03'\r\n\
          \     81     ),\r\n     82 )\r\n     85 _TRAINERSPEC_MODELTYPE = _descriptor.EnumDescriptor(\r\
          \n     86     name=\"ModelType\",\r\n     87     full_name=\"sentencepiece.TrainerSpec.ModelType\"\
          ,\r\n   (...)\r\n    128     serialized_end=1347,\r\n    129 )\r\n    130\
          \ _sym_db.RegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/google/protobuf/descriptor.py:1028,\
          \ in FileDescriptor.__new__(cls, name, package, options, serialized_options,\
          \ serialized_pb, dependencies, public_dependencies, syntax, pool, create_key)\r\
          \n   1026     raise RuntimeError('Please link in cpp generated lib for %s'\
          \ % (name))\r\n   1027 elif serialized_pb:\r\n-> 1028   return _message.default_pool.AddSerializedFile(serialized_pb)\r\
          \n   1029 else:\r\n   1030   return super(FileDescriptor, cls).__new__(cls)\r\
          \n\r\nTypeError: Couldn't build proto file into descriptor pool: duplicate\
          \ file name (sentencepiece_model.proto)\r\n"
        updatedAt: '2023-05-23T19:02:11.369Z'
      numEdits: 0
      reactions: []
    id: 646d0db3fe25e5f8d9742c07
    type: comment
  author: ahe61
  content: "Hi,\r\n\r\nI run into an error while running your model\r\n\r\n! pip install\
    \ transformers\r\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\r\
    \n\r\ntokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\"\
    )\r\nmodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-wikiSQL\"\
    )\r\n\r\nError message\r\nTypeError: Couldn't build proto file into descriptor\
    \ pool: duplicate file name (sentencepiece_model.proto)\r\n\r\n(and warnings\r\
    \nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers. It\
    \ is ignored and you can safely remove it from your `config.json` file.\r\nThe\
    \ `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored\
    \ and you can safely remove it from your `config.json` file.\r\nThe `xla_device`\
    \ argument has been deprecated in v4.4.0 of Transformers. It is ignored and you\
    \ can safely remove it from your `config.json` file.)\r\n\r\nit does work with\
    \ standard bert models (or no model at all)\r\n\r\nI run python3.10\r\n\r\nbest,\r\
    \n\r\nAndreas\r\n---\r\nfull dump:\r\nCell In[4], line 3\r\n      1 from transformers\
    \ import AutoModelWithLMHead, AutoTokenizer\r\n----> 3 tokenizer = AutoTokenizer.from_pretrained(\"\
    mrm8488/t5-base-finetuned-wikiSQL\")\r\n      4 #model = AutoModelWithLMHead.from_pretrained(\"\
    mrm8488/t5-base-finetuned-wikiSQL\")\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:659,\
    \ in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs,\
    \ **kwargs)\r\n    657 tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)]\r\
    \n    658 if tokenizer_class_fast and (use_fast or tokenizer_class_py is None):\r\
    \n--> 659     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path,\
    \ *inputs, **kwargs)\r\n    660 else:\r\n    661     if tokenizer_class_py is\
    \ not None:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1801,\
    \ in PreTrainedTokenizerBase.from_pretrained(cls, pretrained_model_name_or_path,\
    \ *init_inputs, **kwargs)\r\n   1798     else:\r\n   1799         logger.info(f\"\
    loading file {file_path} from cache at {resolved_vocab_files[file_id]}\")\r\n\
    -> 1801 return cls._from_pretrained(\r\n   1802     resolved_vocab_files,\r\n\
    \   1803     pretrained_model_name_or_path,\r\n   1804     init_configuration,\r\
    \n   1805     *init_inputs,\r\n   1806     use_auth_token=use_auth_token,\r\n\
    \   1807     cache_dir=cache_dir,\r\n   1808     local_files_only=local_files_only,\r\
    \n   1809     _commit_hash=commit_hash,\r\n   1810     **kwargs,\r\n   1811 )\r\
    \n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1956,\
    \ in PreTrainedTokenizerBase._from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path,\
    \ init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash,\
    \ *init_inputs, **kwargs)\r\n   1954 # Instantiate tokenizer.\r\n   1955 try:\r\
    \n-> 1956     tokenizer = cls(*init_inputs, **init_kwargs)\r\n   1957 except OSError:\r\
    \n   1958     raise OSError(\r\n   1959         \"Unable to load vocabulary from\
    \ file. \"\r\n   1960         \"Please check that the provided vocabulary is accessible\
    \ and not corrupted.\"\r\n   1961     )\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:133,\
    \ in T5TokenizerFast.__init__(self, vocab_file, tokenizer_file, eos_token, unk_token,\
    \ pad_token, extra_ids, additional_special_tokens, **kwargs)\r\n    126     if\
    \ extra_tokens != extra_ids:\r\n    127         raise ValueError(\r\n    128 \
    \            f\"Both extra_ids ({extra_ids}) and additional_special_tokens ({additional_special_tokens})\
    \ are\"\r\n    129             \" provided to T5Tokenizer. In this case the additional_special_tokens\
    \ must include the extra_ids\"\r\n    130             \" tokens\"\r\n    131 \
    \        )\r\n--> 133 super().__init__(\r\n    134     vocab_file,\r\n    135\
    \     tokenizer_file=tokenizer_file,\r\n    136     eos_token=eos_token,\r\n \
    \   137     unk_token=unk_token,\r\n    138     pad_token=pad_token,\r\n    139\
    \     extra_ids=extra_ids,\r\n    140     additional_special_tokens=additional_special_tokens,\r\
    \n    141     **kwargs,\r\n    142 )\r\n    144 self.vocab_file = vocab_file\r\
    \n    145 self.can_save_slow_tokenizer = False if not self.vocab_file else True\r\
    \n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:114,\
    \ in PreTrainedTokenizerFast.__init__(self, *args, **kwargs)\r\n    111     fast_tokenizer\
    \ = TokenizerFast.from_file(fast_tokenizer_file)\r\n    112 elif slow_tokenizer\
    \ is not None:\r\n    113     # We need to convert a slow tokenizer to build the\
    \ backend\r\n--> 114     fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\r\
    \n    115 elif self.slow_tokenizer_class is not None:\r\n    116     # We need\
    \ to create and convert a slow tokenizer to build the backend\r\n    117     slow_tokenizer\
    \ = self.slow_tokenizer_class(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1162,\
    \ in convert_slow_tokenizer(transformer_tokenizer)\r\n   1154     raise ValueError(\r\
    \n   1155         f\"An instance of tokenizer class {tokenizer_class_name} cannot\
    \ be converted in a Fast tokenizer instance.\"\r\n   1156         \" No converter\
    \ was found. Currently available slow->fast convertors:\"\r\n   1157         f\"\
    \ {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\r\n   1158     )\r\n   1160 converter_class\
    \ = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\r\n-> 1162 return converter_class(transformer_tokenizer).converted()\r\
    \n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:438,\
    \ in SpmConverter.__init__(self, *args)\r\n    434 requires_backends(self, \"\
    protobuf\")\r\n    436 super().__init__(*args)\r\n--> 438 from .utils import sentencepiece_model_pb2\
    \ as model_pb2\r\n    440 m = model_pb2.ModelProto()\r\n    441 with open(self.original_tokenizer.vocab_file,\
    \ \"rb\") as f:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/transformers/utils/sentencepiece_model_pb2.py:29\r\
    \n     24 # @@protoc_insertion_point(imports)\r\n     26 _sym_db = _symbol_database.Default()\r\
    \n---> 29 DESCRIPTOR = _descriptor.FileDescriptor(\r\n     30     name=\"sentencepiece_model.proto\"\
    ,\r\n     31     package=\"sentencepiece\",\r\n     32     syntax=\"proto2\",\r\
    \n     33     serialized_options=b\"H\\003\",\r\n     34     create_key=_descriptor._internal_create_key,\r\
    \n     35     serialized_pb=(\r\n     36         b'\\n\\x19sentencepiece_model.proto\\\
    x12\\rsentencepiece\"\\xa1\\n\\n\\x0bTrainerSpec\\x12\\r\\n\\x05input\\x18\\x01'\r\
    \n     37         b\" \\x03(\\t\\x12\\x14\\n\\x0cinput_format\\x18\\x07 \\x01(\\\
    t\\x12\\x14\\n\\x0cmodel_prefix\\x18\\x02\"\r\n     38         b\" \\x01(\\t\\\
    x12\\x41\\n\\nmodel_type\\x18\\x03\"\r\n     39         b\" \\x01(\\x0e\\x32$.sentencepiece.TrainerSpec.ModelType:\\\
    x07UNIGRAM\\x12\\x18\\n\\nvocab_size\\x18\\x04\"\r\n     40         b\" \\x01(\\\
    x05:\\x04\\x38\\x30\\x30\\x30\\x12\\x17\\n\\x0f\\x61\\x63\\x63\\x65pt_language\\\
    x18\\x05 \\x03(\\t\\x12\"\r\n     41         b' \\n\\x15self_test_sample_size\\\
    x18\\x06 \\x01(\\x05:\\x01\\x30\\x12\"\\n\\x12\\x63haracter_coverage\\x18\\n'\r\
    \n     42         b\" \\x01(\\x02:\\x06\\x30.9995\\x12\\x1e\\n\\x13input_sentence_size\\\
    x18\\x0b\"\r\n     43         b\" \\x01(\\x04:\\x01\\x30\\x12$\\n\\x16shuffle_input_sentence\\\
    x18\\x13 \\x01(\\x08:\\x04true\\x12\"\r\n     44         b' \\n\\x14mining_sentence_size\\\
    x18\\x0c \\x01(\\x05\\x42\\x02\\x18\\x01\\x12\"\\n\\x16training_sentence_size\\\
    x18\\r'\r\n     45         b\" \\x01(\\x05\\x42\\x02\\x18\\x01\\x12(\\n\\x17seed_sentencepiece_size\\\
    x18\\x0e\"\r\n     46         b\" \\x01(\\x05:\\x07\\x31\\x30\\x30\\x30\\x30\\\
    x30\\x30\\x12\\x1e\\n\\x10shrinking_factor\\x18\\x0f\"\r\n     47         b\"\
    \ \\x01(\\x02:\\x04\\x30.75\\x12!\\n\\x13max_sentence_length\\x18\\x12\"\r\n \
    \    48         b\" \\x01(\\x05:\\x04\\x34\\x31\\x39\\x32\\x12\\x17\\n\\x0bnum_threads\\\
    x18\\x10\"\r\n     49         b\" \\x01(\\x05:\\x02\\x31\\x36\\x12\\x1d\\n\\x12num_sub_iterations\\\
    x18\\x11\"\r\n     50         b\" \\x01(\\x05:\\x01\\x32\\x12$\\n\\x18max_sentencepiece_length\\\
    x18\\x14\"\r\n     51         b\" \\x01(\\x05:\\x02\\x31\\x36\\x12%\\n\\x17split_by_unicode_script\\\
    x18\\x15\"\r\n     52         b\" \\x01(\\x08:\\x04true\\x12\\x1d\\n\\x0fsplit_by_number\\\
    x18\\x17\"\r\n     53         b\" \\x01(\\x08:\\x04true\\x12!\\n\\x13split_by_whitespace\\\
    x18\\x16\"\r\n     54         b\" \\x01(\\x08:\\x04true\\x12)\\n\\x1atreat_whitespace_as_suffix\\\
    x18\\x18\"\r\n     55         b\" \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1b\\n\\\
    x0csplit_digits\\x18\\x19\"\r\n     56         b\" \\x01(\\x08:\\x05\\x66\\x61lse\\\
    x12\\x17\\n\\x0f\\x63ontrol_symbols\\x18\\x1e\"\r\n     57         b\" \\x03(\\\
    t\\x12\\x1c\\n\\x14user_defined_symbols\\x18\\x1f \\x03(\\t\\x12\\x16\\n\\x0erequired_chars\\\
    x18$\"\r\n     58         b\" \\x01(\\t\\x12\\x1c\\n\\rbyte_fallback\\x18# \\\
    x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x1dvocabulary_output_piece_score\\x18\"\
    \r\n     59         b'  \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x10hard_vocab_limit\\\
    x18! \\x01(\\x08:\\x04true\\x12\\x1c\\n\\ruse_all_vocab\\x18\"'\r\n     60   \
    \      b\" \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x11\\n\\x06unk_id\\x18( \\x01(\\\
    x05:\\x01\\x30\\x12\\x11\\n\\x06\\x62os_id\\x18)\"\r\n     61         b\" \\x01(\\\
    x05:\\x01\\x31\\x12\\x11\\n\\x06\\x65os_id\\x18* \\x01(\\x05:\\x01\\x32\\x12\\\
    x12\\n\\x06pad_id\\x18+\"\r\n     62         b\" \\x01(\\x05:\\x02-1\\x12\\x18\\\
    n\\tunk_piece\\x18- \\x01(\\t:\\x05<unk>\\x12\\x16\\n\\tbos_piece\\x18.\"\r\n\
    \     63         b\" \\x01(\\t:\\x03<s>\\x12\\x17\\n\\teos_piece\\x18/ \\x01(\\\
    t:\\x04</s>\\x12\\x18\\n\\tpad_piece\\x18\\x30\"\r\n     64         b\" \\x01(\\\
    t:\\x05<pad>\\x12\\x1a\\n\\x0bunk_surface\\x18, \\x01(\\t:\\x05 \\xe2\\x81\\x87\"\
    \r\n     65         b\" \\x12+\\n\\x1ctrain_extremely_large_corpus\\x18\\x31\"\
    \r\n     66         b' \\x01(\\x08:\\x05\\x66\\x61lse\"5\\n\\tModelType\\x12\\\
    x0b\\n\\x07UNIGRAM\\x10\\x01\\x12\\x07\\n\\x03\\x42PE\\x10\\x02\\x12\\x08\\n\\\
    x04WORD\\x10\\x03\\x12\\x08\\n\\x04\\x43HAR\\x10\\x04*\\t\\x08\\xc8\\x01\\x10\\\
    x80\\x80\\x80\\x80\\x02\"\\xd1\\x01\\n\\x0eNormalizerSpec\\x12\\x0c\\n\\x04name\\\
    x18\\x01'\r\n     67         b\" \\x01(\\t\\x12\\x1c\\n\\x14precompiled_charsmap\\\
    x18\\x02 \\x01(\\x0c\\x12\\x1e\\n\\x10\\x61\\x64\\x64_dummy_prefix\\x18\\x03\"\
    \r\n     68         b\" \\x01(\\x08:\\x04true\\x12&\\n\\x18remove_extra_whitespaces\\\
    x18\\x04 \\x01(\\x08:\\x04true\\x12\"\r\n     69         b\" \\n\\x12\\x65scape_whitespaces\\\
    x18\\x05 \\x01(\\x08:\\x04true\\x12\\x1e\\n\\x16normalization_rule_tsv\\x18\\\
    x06\"\r\n     70         b' \\x01(\\t*\\t\\x08\\xc8\\x01\\x10\\x80\\x80\\x80\\\
    x80\\x02\"y\\n\\x0cSelfTestData\\x12\\x33\\n\\x07samples\\x18\\x01'\r\n     71\
    \         b' \\x03(\\x0b\\x32\".sentencepiece.SelfTestData.Sample\\x1a)\\n\\x06Sample\\\
    x12\\r\\n\\x05input\\x18\\x01'\r\n     72         b\" \\x01(\\t\\x12\\x10\\n\\\
    x08\\x65xpected\\x18\\x02\"\r\n     73         b' \\x01(\\t*\\t\\x08\\xc8\\x01\\\
    x10\\x80\\x80\\x80\\x80\\x02\"\\xfe\\x03\\n\\nModelProto\\x12\\x37\\n\\x06pieces\\\
    x18\\x01'\r\n     74         b\" \\x03(\\x0b\\x32'.sentencepiece.ModelProto.SentencePiece\\\
    x12\\x30\\n\\x0ctrainer_spec\\x18\\x02\"\r\n     75         b\" \\x01(\\x0b\\\
    x32\\x1a.sentencepiece.TrainerSpec\\x12\\x36\\n\\x0fnormalizer_spec\\x18\\x03\"\
    \r\n     76         b\" \\x01(\\x0b\\x32\\x1d.sentencepiece.NormalizerSpec\\x12\\\
    x33\\n\\x0eself_test_data\\x18\\x04\"\r\n     77         b\" \\x01(\\x0b\\x32\\\
    x1b.sentencepiece.SelfTestData\\x12\\x38\\n\\x11\\x64\\x65normalizer_spec\\x18\\\
    x05\"\r\n     78         b\" \\x01(\\x0b\\x32\\x1d.sentencepiece.NormalizerSpec\\\
    x1a\\xd2\\x01\\n\\rSentencePiece\\x12\\r\\n\\x05piece\\x18\\x01\"\r\n     79 \
    \        b\" \\x01(\\t\\x12\\r\\n\\x05score\\x18\\x02 \\x01(\\x02\\x12\\x42\\\
    n\\x04type\\x18\\x03\"\r\n     80         b' \\x01(\\x0e\\x32,.sentencepiece.ModelProto.SentencePiece.Type:\\\
    x06NORMAL\"T\\n\\x04Type\\x12\\n\\n\\x06NORMAL\\x10\\x01\\x12\\x0b\\n\\x07UNKNOWN\\\
    x10\\x02\\x12\\x0b\\n\\x07\\x43ONTROL\\x10\\x03\\x12\\x10\\n\\x0cUSER_DEFINED\\\
    x10\\x04\\x12\\x08\\n\\x04\\x42YTE\\x10\\x06\\x12\\n\\n\\x06UNUSED\\x10\\x05*\\\
    t\\x08\\xc8\\x01\\x10\\x80\\x80\\x80\\x80\\x02*\\t\\x08\\xc8\\x01\\x10\\x80\\\
    x80\\x80\\x80\\x02\\x42\\x02H\\x03'\r\n     81     ),\r\n     82 )\r\n     85\
    \ _TRAINERSPEC_MODELTYPE = _descriptor.EnumDescriptor(\r\n     86     name=\"\
    ModelType\",\r\n     87     full_name=\"sentencepiece.TrainerSpec.ModelType\"\
    ,\r\n   (...)\r\n    128     serialized_end=1347,\r\n    129 )\r\n    130 _sym_db.RegisterEnumDescriptor(_TRAINERSPEC_MODELTYPE)\r\
    \n\r\nFile ~/.local/lib/python3.10/site-packages/google/protobuf/descriptor.py:1028,\
    \ in FileDescriptor.__new__(cls, name, package, options, serialized_options, serialized_pb,\
    \ dependencies, public_dependencies, syntax, pool, create_key)\r\n   1026    \
    \ raise RuntimeError('Please link in cpp generated lib for %s' % (name))\r\n \
    \  1027 elif serialized_pb:\r\n-> 1028   return _message.default_pool.AddSerializedFile(serialized_pb)\r\
    \n   1029 else:\r\n   1030   return super(FileDescriptor, cls).__new__(cls)\r\n\
    \r\nTypeError: Couldn't build proto file into descriptor pool: duplicate file\
    \ name (sentencepiece_model.proto)\r\n"
  created_at: 2023-05-23 18:02:11+00:00
  edited: false
  hidden: false
  id: 646d0db3fe25e5f8d9742c07
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 5
repo_id: mrm8488/t5-base-finetuned-wikiSQL
repo_type: model
status: open
target_branch: null
title: 'model mrm8488/t5-base-finetuned-wikiSQL: AutoTokenizer .from_pretrained()
  error message'
