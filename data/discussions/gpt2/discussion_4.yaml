!!python/object:huggingface_hub.community.DiscussionWithDetails
author: xalex
conflicting_files: null
created_at: 2022-09-16 13:51:12+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
      fullname: Alex
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: xalex
      type: user
    createdAt: '2022-09-16T14:51:12.000Z'
    data:
      edited: false
      editors:
      - xalex
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0ae9421c953e42655df328f4af0c8fa2.svg
          fullname: Alex
          isHf: false
          isPro: false
          name: xalex
          type: user
        html: '<p>When I train with text that has linebreaks because it came from
          PDF files, are they relevant for training? Currently I replace them with
          a space, as they are obviously learned and I think the net may generalize
          worse when it tries to interpret some semantic meaning into a linebreak,
          which isn''t there. But maybe it also can learn that there is a linebreak
          every X chars, but that the linebreak does not have any meaning but wrapping
          lines to a readable length?<br>What''s common practice for data preprocessing?
          Does it matter for the text (not the formatting) if the net was trained
          on wrapped text?</p>

          '
        raw: "When I train with text that has linebreaks because it came from PDF\
          \ files, are they relevant for training? Currently I replace them with a\
          \ space, as they are obviously learned and I think the net may generalize\
          \ worse when it tries to interpret some semantic meaning into a linebreak,\
          \ which isn't there. But maybe it also can learn that there is a linebreak\
          \ every X chars, but that the linebreak does not have any meaning but wrapping\
          \ lines to a readable length?\r\nWhat's common practice for data preprocessing?\
          \ Does it matter for the text (not the formatting) if the net was trained\
          \ on wrapped text?"
        updatedAt: '2022-09-16T14:51:12.397Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F44D"
        users:
        - AlfredLeeee
      - count: 1
        reaction: "\u2764\uFE0F"
        users:
        - AlfredLeeee
    id: 63248d604f8f5819ac038a69
    type: comment
  author: xalex
  content: "When I train with text that has linebreaks because it came from PDF files,\
    \ are they relevant for training? Currently I replace them with a space, as they\
    \ are obviously learned and I think the net may generalize worse when it tries\
    \ to interpret some semantic meaning into a linebreak, which isn't there. But\
    \ maybe it also can learn that there is a linebreak every X chars, but that the\
    \ linebreak does not have any meaning but wrapping lines to a readable length?\r\
    \nWhat's common practice for data preprocessing? Does it matter for the text (not\
    \ the formatting) if the net was trained on wrapped text?"
  created_at: 2022-09-16 13:51:12+00:00
  edited: false
  hidden: false
  id: 63248d604f8f5819ac038a69
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: gpt2
repo_type: model
status: open
target_branch: null
title: Are linebreaks relevant for training/finetuning?
