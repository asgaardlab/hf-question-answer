!!python/object:huggingface_hub.community.DiscussionWithDetails
author: brresnic
conflicting_files: null
created_at: 2024-01-05 00:42:58+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/5a501b1359a4bf17a957f3f3987a57a0.svg
      fullname: Benjamin Resnick
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: brresnic
      type: user
    createdAt: '2024-01-05T00:42:58.000Z'
    data:
      edited: false
      editors:
      - brresnic
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7174656391143799
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/5a501b1359a4bf17a957f3f3987a57a0.svg
          fullname: Benjamin Resnick
          isHf: false
          isPro: false
          name: brresnic
          type: user
        html: "<pre><code>model = AutoModelForCausalLM.from_pretrained(\n    my_GPT2LMHeadModel_checkpoint,\
          \ \n    torch_dtype=torch.bfloat16, \n    attn_implementation=\"flash_attention_2\"\
          ,\n)\n</code></pre>\n<p>throws the following error:</p>\n<p>Error loading\
          \ Flash_Model_2: GPT2LMHeadModel does not support Flash Attention 2.0 yet.\
          \ Please open an issue on GitHub to request support for this architecture:\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/issues/new\"\
          >https://github.com/huggingface/transformers/issues/new</a></p>\n"
        raw: "    model = AutoModelForCausalLM.from_pretrained(\r\n        my_GPT2LMHeadModel_checkpoint,\
          \ \r\n        torch_dtype=torch.bfloat16, \r\n        attn_implementation=\"\
          flash_attention_2\",\r\n    )\r\nthrows the following error:\r\n\r\nError\
          \ loading Flash_Model_2: GPT2LMHeadModel does not support Flash Attention\
          \ 2.0 yet. Please open an issue on GitHub to request support for this architecture:\
          \ https://github.com/huggingface/transformers/issues/new\r\n"
        updatedAt: '2024-01-05T00:42:58.876Z'
      numEdits: 0
      reactions: []
    id: 659750926571a301085c14af
    type: comment
  author: brresnic
  content: "    model = AutoModelForCausalLM.from_pretrained(\r\n        my_GPT2LMHeadModel_checkpoint,\
    \ \r\n        torch_dtype=torch.bfloat16, \r\n        attn_implementation=\"flash_attention_2\"\
    ,\r\n    )\r\nthrows the following error:\r\n\r\nError loading Flash_Model_2:\
    \ GPT2LMHeadModel does not support Flash Attention 2.0 yet. Please open an issue\
    \ on GitHub to request support for this architecture: https://github.com/huggingface/transformers/issues/new\r\
    \n"
  created_at: 2024-01-05 00:42:58+00:00
  edited: false
  hidden: false
  id: 659750926571a301085c14af
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
      fullname: Younes Belkada
      isHf: true
      isOrgMember: true
      isOwner: false
      isPro: false
      name: ybelkada
      type: user
    createdAt: '2024-01-10T18:47:16.000Z'
    data:
      edited: false
      editors:
      - ybelkada
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9416502714157104
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1648631057413-noauth.png?w=200&h=200&f=face
          fullname: Younes Belkada
          isHf: true
          isPro: false
          name: ybelkada
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;brresnic&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/brresnic\"\
          >@<span class=\"underline\">brresnic</span></a></span>\n\n\t</span></span><br>Thanks\
          \ for your interest! There is an ongoing effort to add FA2 to GPT2 here:\
          \ <a rel=\"nofollow\" href=\"https://github.com/huggingface/transformers/pull/27479\"\
          >https://github.com/huggingface/transformers/pull/27479</a><br>Note however\
          \ since the model size is relatively small I don't expect very interesting\
          \ speedups with FA2 + gpt2</p>\n"
        raw: "Hi @brresnic \nThanks for your interest! There is an ongoing effort\
          \ to add FA2 to GPT2 here: https://github.com/huggingface/transformers/pull/27479\
          \ \nNote however since the model size is relatively small I don't expect\
          \ very interesting speedups with FA2 + gpt2"
        updatedAt: '2024-01-10T18:47:16.764Z'
      numEdits: 0
      reactions: []
    id: 659ee634ab4293c46d42d1bc
    type: comment
  author: ybelkada
  content: "Hi @brresnic \nThanks for your interest! There is an ongoing effort to\
    \ add FA2 to GPT2 here: https://github.com/huggingface/transformers/pull/27479\
    \ \nNote however since the model size is relatively small I don't expect very\
    \ interesting speedups with FA2 + gpt2"
  created_at: 2024-01-10 18:47:16+00:00
  edited: false
  hidden: false
  id: 659ee634ab4293c46d42d1bc
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 75
repo_id: gpt2
repo_type: model
status: open
target_branch: null
title: 'request: Add flash attention 2.0 support for GPT2LMHeadModel'
