!!python/object:huggingface_hub.community.DiscussionWithDetails
author: dlo3
conflicting_files: null
created_at: 2023-02-27 00:56:56+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/ce7df0b4b6816cf1b92d2ecdd6cd3a17.svg
      fullname: David O'Neill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dlo3
      type: user
    createdAt: '2023-02-27T00:56:56.000Z'
    data:
      edited: false
      editors:
      - dlo3
      hidden: false
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/ce7df0b4b6816cf1b92d2ecdd6cd3a17.svg
          fullname: David O'Neill
          isHf: false
          isPro: false
          name: dlo3
          type: user
        html: '<p>I have a question about how to properly train a GPT-2-like transformer
          for a causal LM task. This question applies to both fine-tuning and training
          a model from scractch.</p>

          <p>Following along with the tutorial supplied at <a rel="nofollow" href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L480">https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L480</a>
          , the authors seem to manually group/resize examples in their <code>dataset</code>
          to match the model context length. Then, when they setup their <code>Trainer</code>
          instance, they use the <code>default_data_collator</code> supplied by the
          library.</p>

          <p><code>transformers</code> also supplies a <code>DataCollatorForLanguageModeling</code>.
          In my scripts, I''m using this class. However, at train time, I get some
          <code>ValueError</code> related to the GPT-2 tokenizer not having a padding
          token.  I can "solve" this problem by adding the special pad token to the
          tokenizer myself and calling <code>model.resize_token_embeddings</code>.
          </p>

          <p>My question is basically: is this approach with using a <code>DataCollatorForLanguageModeling</code>
          instance + adding the special padding token correct / good practice? Or
          should I be using the <code>default_data_collator</code> and restructuring
          my dataset prior to training? Aren''t they supposed to be functionally equivalent
          approaches?</p>

          <p>Thanks in advance.</p>

          '
        raw: "I have a question about how to properly train a GPT-2-like transformer\
          \ for a causal LM task. This question applies to both fine-tuning and training\
          \ a model from scractch.\r\n\r\nFollowing along with the tutorial supplied\
          \ at https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L480\
          \ , the authors seem to manually group/resize examples in their `dataset`\
          \ to match the model context length. Then, when they setup their `Trainer`\
          \ instance, they use the `default_data_collator` supplied by the library.\r\
          \n\r\n`transformers` also supplies a `DataCollatorForLanguageModeling`.\
          \ In my scripts, I'm using this class. However, at train time, I get some\
          \ `ValueError` related to the GPT-2 tokenizer not having a padding token.\
          \  I can \"solve\" this problem by adding the special pad token to the tokenizer\
          \ myself and calling `model.resize_token_embeddings`. \r\n\r\nMy question\
          \ is basically: is this approach with using a `DataCollatorForLanguageModeling`\
          \ instance + adding the special padding token correct / good practice? Or\
          \ should I be using the `default_data_collator` and restructuring my dataset\
          \ prior to training? Aren't they supposed to be functionally equivalent\
          \ approaches?\r\n\r\nThanks in advance."
        updatedAt: '2023-02-27T00:56:56.266Z'
      numEdits: 0
      reactions: []
    id: 63fbffd80aa18292d5c5ef8a
    type: comment
  author: dlo3
  content: "I have a question about how to properly train a GPT-2-like transformer\
    \ for a causal LM task. This question applies to both fine-tuning and training\
    \ a model from scractch.\r\n\r\nFollowing along with the tutorial supplied at\
    \ https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L480\
    \ , the authors seem to manually group/resize examples in their `dataset` to match\
    \ the model context length. Then, when they setup their `Trainer` instance, they\
    \ use the `default_data_collator` supplied by the library.\r\n\r\n`transformers`\
    \ also supplies a `DataCollatorForLanguageModeling`. In my scripts, I'm using\
    \ this class. However, at train time, I get some `ValueError` related to the GPT-2\
    \ tokenizer not having a padding token.  I can \"solve\" this problem by adding\
    \ the special pad token to the tokenizer myself and calling `model.resize_token_embeddings`.\
    \ \r\n\r\nMy question is basically: is this approach with using a `DataCollatorForLanguageModeling`\
    \ instance + adding the special padding token correct / good practice? Or should\
    \ I be using the `default_data_collator` and restructuring my dataset prior to\
    \ training? Aren't they supposed to be functionally equivalent approaches?\r\n\
    \r\nThanks in advance."
  created_at: 2023-02-27 00:56:56+00:00
  edited: false
  hidden: false
  id: 63fbffd80aa18292d5c5ef8a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionTitleChange
  _event:
    author:
      avatarUrl: /avatars/ce7df0b4b6816cf1b92d2ecdd6cd3a17.svg
      fullname: David O'Neill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dlo3
      type: user
    createdAt: '2023-02-27T03:30:49.000Z'
    data:
      from: Proper training method for Causal LM task?
      to: Proper dataset prep for Causal LM training task?
    id: 63fc23e9cacbb407e4c38ced
    type: title-change
  author: dlo3
  created_at: 2023-02-27 03:30:49+00:00
  id: 63fc23e9cacbb407e4c38ced
  new_title: Proper dataset prep for Causal LM training task?
  old_title: Proper training method for Causal LM task?
  type: title-change
- !!python/object:huggingface_hub.community.DiscussionStatusChange
  _event:
    author:
      avatarUrl: /avatars/ce7df0b4b6816cf1b92d2ecdd6cd3a17.svg
      fullname: David O'Neill
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: dlo3
      type: user
    createdAt: '2023-03-01T13:01:47.000Z'
    data:
      status: closed
    id: 63ff4cbba6d0b8d5677f2cda
    type: status-change
  author: dlo3
  created_at: 2023-03-01 13:01:47+00:00
  id: 63ff4cbba6d0b8d5677f2cda
  new_status: closed
  type: status-change
is_pull_request: false
merge_commit_oid: null
num: 34
repo_id: gpt2
repo_type: model
status: closed
target_branch: null
title: Proper dataset prep for Causal LM training task?
