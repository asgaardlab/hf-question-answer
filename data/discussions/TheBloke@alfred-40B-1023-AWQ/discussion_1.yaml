!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nathsou
conflicting_files: null
created_at: 2023-11-20 11:03:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/570a5142c710755f42570d03d8a89286.svg
      fullname: Nathan Soufflet
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nathsou
      type: user
    createdAt: '2023-11-20T11:03:21.000Z'
    data:
      edited: true
      editors:
      - nathsou
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7098322510719299
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/570a5142c710755f42570d03d8a89286.svg
          fullname: Nathan Soufflet
          isHf: false
          isPro: false
          name: nathsou
          type: user
        html: '<p>Hi, thanks for converting alfred to AWQ.<br>When running the model
          using vLLM 0.2.2 (either via the server command line or using the LLM constructor),
          I get the following error:</p>

          <pre><code>RuntimeError: The size of tensor a (16384) must match the size
          of tensor b (32768) at non-singleton dimension 2

          </code></pre>

          <p>There is also a warning:</p>

          <p>You are using a model of type RefinedWeb to instantiate a model of type
          falcon. This is not supported for all configurations of models and can yield
          errors.<br>WARNING 11-20 12:00:39 config.py:433] The model''s config.json
          does not contain any of the following keys to determine the original maximum
          length of the model: [''max_position_embeddings'', ''n_positions'', ''max_seq_len'',
          ''seq_length'', ''max_sequence_length'', ''max_seq_length'', ''seq_len''].
          Assuming the model''s maximum length is 2048.</p>

          <p>Here is an example command that fails:</p>

          <pre><code>python3 -m vllm.entrypoints.api_server --model TheBloke/alfred-40B-1023-AWQ
          --quantization awq --dtype auto --tensor-parallel-size 2 --trust-remote-code

          </code></pre>

          '
        raw: 'Hi, thanks for converting alfred to AWQ.

          When running the model using vLLM 0.2.2 (either via the server command line
          or using the LLM constructor), I get the following error:


          ```

          RuntimeError: The size of tensor a (16384) must match the size of tensor
          b (32768) at non-singleton dimension 2

          ```


          There is also a warning:


          You are using a model of type RefinedWeb to instantiate a model of type
          falcon. This is not supported for all configurations of models and can yield
          errors.

          WARNING 11-20 12:00:39 config.py:433] The model''s config.json does not
          contain any of the following keys to determine the original maximum length
          of the model: [''max_position_embeddings'', ''n_positions'', ''max_seq_len'',
          ''seq_length'', ''max_sequence_length'', ''max_seq_length'', ''seq_len''].
          Assuming the model''s maximum length is 2048.


          Here is an example command that fails:


          ```

          python3 -m vllm.entrypoints.api_server --model TheBloke/alfred-40B-1023-AWQ
          --quantization awq --dtype auto --tensor-parallel-size 2 --trust-remote-code

          ```'
        updatedAt: '2023-11-20T20:20:46.195Z'
      numEdits: 3
      reactions: []
    id: 655b3cf950b9a147993eec43
    type: comment
  author: nathsou
  content: 'Hi, thanks for converting alfred to AWQ.

    When running the model using vLLM 0.2.2 (either via the server command line or
    using the LLM constructor), I get the following error:


    ```

    RuntimeError: The size of tensor a (16384) must match the size of tensor b (32768)
    at non-singleton dimension 2

    ```


    There is also a warning:


    You are using a model of type RefinedWeb to instantiate a model of type falcon.
    This is not supported for all configurations of models and can yield errors.

    WARNING 11-20 12:00:39 config.py:433] The model''s config.json does not contain
    any of the following keys to determine the original maximum length of the model:
    [''max_position_embeddings'', ''n_positions'', ''max_seq_len'', ''seq_length'',
    ''max_sequence_length'', ''max_seq_length'', ''seq_len'']. Assuming the model''s
    maximum length is 2048.


    Here is an example command that fails:


    ```

    python3 -m vllm.entrypoints.api_server --model TheBloke/alfred-40B-1023-AWQ --quantization
    awq --dtype auto --tensor-parallel-size 2 --trust-remote-code

    ```'
  created_at: 2023-11-20 11:03:21+00:00
  edited: true
  hidden: false
  id: 655b3cf950b9a147993eec43
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: TheBloke/alfred-40B-1023-AWQ
repo_type: model
status: open
target_branch: null
title: Tensor size mismatch
