!!python/object:huggingface_hub.community.DiscussionWithDetails
author: zkrider
conflicting_files: null
created_at: 2023-08-04 14:45:41+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
      fullname: Zak KRider
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: zkrider
      type: user
    createdAt: '2023-08-04T15:45:41.000Z'
    data:
      edited: false
      editors:
      - zkrider
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.4622901380062103
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/1cdd6f0f8d7c324bf0bae86dedd4cfaf.svg
          fullname: Zak KRider
          isHf: false
          isPro: false
          name: zkrider
          type: user
        html: "<p>Trying to get this to launch on Sagemaker and hitting issues trying\
          \ to launch on 2xlarge, 8xlarge and 12xlarge instances. All are throwing\
          \ the same errors:</p>\n<p>Error <a href=\"/Salesforce/codegen25-7b-multi/discussions/1\"\
          >#1</a> </p>\n<pre><code>#033[2m2023-08-04T15:30:16.074558Z#033[0m #033[31mERROR#033[0m\
          \ #033[1mshard-manager#033[0m: #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
          \ Error when initializing model\n</code></pre>\n<p>Error <a href=\"/Salesforce/codegen25-7b-multi/discussions/2\"\
          >#2</a></p>\n<pre><code>File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\n    model = get_model(model_id, revision, sharded,\
          \ quantize, trust_remote_code)\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 237, in get_model\n    return FlashLlamaSharded(\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 166, in __init__\n    tokenizer = LlamaTokenizer.from_pretrained(\n\
          \  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\n    return cls._from_pretrained(\n  File\
          \ \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1975, in _from_pretrained\n    tokenizer = cls(*init_inputs, **init_kwargs)\n\
          \  File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\n    self.sp_model.Load(vocab_file)\n  File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
          , line 905, in Load\n    return self.LoadFromFile(model_file)\n  File \"\
          /opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\", line\
          \ 310, in LoadFromFile\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\n</code></pre>\n<p>Error <a href=\"/Salesforce/codegen25-7b-multi/discussions/3\"\
          >#3</a></p>\n<pre><code>TypeError: not a string\n #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m\n\
          </code></pre>\n<p>Error <a href=\"/Salesforce/codegen25-7b-multi/discussions/4\"\
          >#4</a></p>\n<pre><code>#033[2m2023-08-04T15:30:16.396605Z#033[0m #033[31mERROR#033[0m\
          \ #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Shard 1 failed to\
          \ start:\n</code></pre>\n<p>Notebook for deployment in Sagemaker:</p>\n\
          <pre><code>import json\nimport sagemaker\nimport boto3\nfrom sagemaker.huggingface\
          \ import HuggingFaceModel, get_huggingface_llm_image_uri\n\ntry:\n    role\
          \ = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n\
          \    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\
          \n# Hub Model configuration. https://huggingface.co/models\nhub = {\n  \
          \  'HF_MODEL_ID':'Salesforce/codegen25-7b-multi',\n    'SM_NUM_GPUS': '2',\n\
          \    'HF_API_TOKEN':'&lt;TOKEN&gt;'\n}\n\n# create Hugging Face Model Class\n\
          huggingface_model = HuggingFaceModel(\n    image_uri=get_huggingface_llm_image_uri(\"\
          huggingface\",version=\"0.8.2\"),\n    env=hub,\n    role=role, \n)\n\n\
          # deploy model to SageMaker Inference\npredictor = huggingface_model.deploy(\n\
          \    initial_instance_count=1,\n    instance_type=\"ml.g4dn.12xlarge\",\n\
          \    container_startup_health_check_timeout=300,\n    endpoint_name=\"codegen25\"\
          ,\n    model_name=\"codegen25\"\n  )\n</code></pre>\n"
        raw: "Trying to get this to launch on Sagemaker and hitting issues trying\
          \ to launch on 2xlarge, 8xlarge and 12xlarge instances. All are throwing\
          \ the same errors:\r\n\r\nError #1 \r\n\r\n```\r\n#033[2m2023-08-04T15:30:16.074558Z#033[0m\
          \ #033[31mERROR#033[0m #033[1mshard-manager#033[0m: #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
          \ Error when initializing model\r\n```\r\n\r\nError #2\r\n```\r\nFile \"\
          /opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
          , line 124, in serve_inner\r\n    model = get_model(model_id, revision,\
          \ sharded, quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
          , line 237, in get_model\r\n    return FlashLlamaSharded(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
          , line 166, in __init__\r\n    tokenizer = LlamaTokenizer.from_pretrained(\r\
          \n  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1812, in from_pretrained\r\n    return cls._from_pretrained(\r\n\
          \  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
          , line 1975, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
          \n  File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
          , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\", line\
          \ 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"\
          /opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\", line\
          \ 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
          \ arg)\r\n```\r\n\r\nError #3\r\n```\r\nTypeError: not a string\r\n #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m\r\
          \n```\r\n\r\nError #4\r\n```\r\n#033[2m2023-08-04T15:30:16.396605Z#033[0m\
          \ #033[31mERROR#033[0m #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
          \ Shard 1 failed to start:\r\n```\r\n\r\n\r\n\r\nNotebook for deployment\
          \ in Sagemaker:\r\n```\r\nimport json\r\nimport sagemaker\r\nimport boto3\r\
          \nfrom sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\r\
          \n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\nexcept ValueError:\r\
          \n\tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
          \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub =\
          \ {\r\n\t'HF_MODEL_ID':'Salesforce/codegen25-7b-multi',\r\n\t'SM_NUM_GPUS':\
          \ '2',\r\n    'HF_API_TOKEN':'<TOKEN>'\r\n}\r\n\r\n# create Hugging Face\
          \ Model Class\r\nhuggingface_model = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"\
          huggingface\",version=\"0.8.2\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\
          \r\n# deploy model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
          \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g4dn.12xlarge\",\r\
          \n\tcontainer_startup_health_check_timeout=300,\r\n    endpoint_name=\"\
          codegen25\",\r\n    model_name=\"codegen25\"\r\n  )\r\n```\r\n"
        updatedAt: '2023-08-04T15:45:41.545Z'
      numEdits: 0
      reactions: []
    id: 64cd1d25c98b37452e85168b
    type: comment
  author: zkrider
  content: "Trying to get this to launch on Sagemaker and hitting issues trying to\
    \ launch on 2xlarge, 8xlarge and 12xlarge instances. All are throwing the same\
    \ errors:\r\n\r\nError #1 \r\n\r\n```\r\n#033[2m2023-08-04T15:30:16.074558Z#033[0m\
    \ #033[31mERROR#033[0m #033[1mshard-manager#033[0m: #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m\
    \ Error when initializing model\r\n```\r\n\r\nError #2\r\n```\r\nFile \"/opt/conda/lib/python3.9/site-packages/text_generation_server/server.py\"\
    , line 124, in serve_inner\r\n    model = get_model(model_id, revision, sharded,\
    \ quantize, trust_remote_code)\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/__init__.py\"\
    , line 237, in get_model\r\n    return FlashLlamaSharded(\r\n  File \"/opt/conda/lib/python3.9/site-packages/text_generation_server/models/flash_llama.py\"\
    , line 166, in __init__\r\n    tokenizer = LlamaTokenizer.from_pretrained(\r\n\
    \  File \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\"\
    , line 1812, in from_pretrained\r\n    return cls._from_pretrained(\r\n  File\
    \ \"/usr/src/transformers/src/transformers/tokenization_utils_base.py\", line\
    \ 1975, in _from_pretrained\r\n    tokenizer = cls(*init_inputs, **init_kwargs)\r\
    \n  File \"/usr/src/transformers/src/transformers/models/llama/tokenization_llama.py\"\
    , line 96, in __init__\r\n    self.sp_model.Load(vocab_file)\r\n  File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
    , line 905, in Load\r\n    return self.LoadFromFile(model_file)\r\n  File \"/opt/conda/lib/python3.9/site-packages/sentencepiece/__init__.py\"\
    , line 310, in LoadFromFile\r\n    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self,\
    \ arg)\r\n```\r\n\r\nError #3\r\n```\r\nTypeError: not a string\r\n #033[2m#033[3mrank#033[0m#033[2m=#033[0m0#033[0m\r\
    \n```\r\n\r\nError #4\r\n```\r\n#033[2m2023-08-04T15:30:16.396605Z#033[0m #033[31mERROR#033[0m\
    \ #033[2mtext_generation_launcher#033[0m#033[2m:#033[0m Shard 1 failed to start:\r\
    \n```\r\n\r\n\r\n\r\nNotebook for deployment in Sagemaker:\r\n```\r\nimport json\r\
    \nimport sagemaker\r\nimport boto3\r\nfrom sagemaker.huggingface import HuggingFaceModel,\
    \ get_huggingface_llm_image_uri\r\n\r\ntry:\r\n\trole = sagemaker.get_execution_role()\r\
    \nexcept ValueError:\r\n\tiam = boto3.client('iam')\r\n\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\r\
    \n\r\n# Hub Model configuration. https://huggingface.co/models\r\nhub = {\r\n\t\
    'HF_MODEL_ID':'Salesforce/codegen25-7b-multi',\r\n\t'SM_NUM_GPUS': '2',\r\n  \
    \  'HF_API_TOKEN':'<TOKEN>'\r\n}\r\n\r\n# create Hugging Face Model Class\r\n\
    huggingface_model = HuggingFaceModel(\r\n\timage_uri=get_huggingface_llm_image_uri(\"\
    huggingface\",version=\"0.8.2\"),\r\n\tenv=hub,\r\n\trole=role, \r\n)\r\n\r\n\
    # deploy model to SageMaker Inference\r\npredictor = huggingface_model.deploy(\r\
    \n\tinitial_instance_count=1,\r\n\tinstance_type=\"ml.g4dn.12xlarge\",\r\n\tcontainer_startup_health_check_timeout=300,\r\
    \n    endpoint_name=\"codegen25\",\r\n    model_name=\"codegen25\"\r\n  )\r\n\
    ```\r\n"
  created_at: 2023-08-04 14:45:41+00:00
  edited: false
  hidden: false
  id: 64cd1d25c98b37452e85168b
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 7
repo_id: Salesforce/codegen25-7b-multi
repo_type: model
status: open
target_branch: null
title: 'Sagemaker Deployment Issues: TypeError: Not String'
