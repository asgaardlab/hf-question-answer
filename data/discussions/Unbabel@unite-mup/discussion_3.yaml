!!python/object:huggingface_hub.community.DiscussionWithDetails
author: alvations
conflicting_files: null
created_at: 2023-09-06 10:41:51+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/0e9087f2672b0e4f28d91266acf9ce57.svg
      fullname: Liling
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alvations
      type: user
    createdAt: '2023-09-06T11:41:51.000Z'
    data:
      edited: false
      editors:
      - alvations
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7842400074005127
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/0e9087f2672b0e4f28d91266acf9ce57.svg
          fullname: Liling
          isHf: false
          isPro: false
          name: alvations
          type: user
        html: '<p>Is it possible to save the checkpoints with the upgraded backend?
          </p>

          <p>Currently, it''s running the conversion everytime we load the model,
          i.e.</p>

          <pre><code>import os


          from huggingface_hub import snapshot_download


          from comet.models.multitask.unified_metric import UnifiedMetric


          model_path = snapshot_download(repo_id="Unbabel/unite-mup", cache_dir=os.path.abspath(os.path.dirname(''.'')))

          model_checkpoint_path = f"{model_path}/checkpoints/model.ckpt"


          unite = UnifiedMetric.load_from_checkpoint(model_checkpoint_path)

          </code></pre>

          <p>[stderr]:</p>

          <pre><code>INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically
          upgraded your loaded checkpoint from v1.6.0 to v1.9.5. To apply the upgrade
          to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint
          --file models--Unbabel--unite-mup/snapshots/d2d555cff30f53db362ae2899d66a667d6db165b/checkpoints/model.ckpt`

          Some weights of the model checkpoint at xlm-roberta-large were not used
          when initializing XLMRobertaModel: [''lm_head.bias'', ''lm_head.dense.weight'',
          ''lm_head.layer_norm.bias'', ''roberta.pooler.dense.bias'', ''lm_head.dense.bias'',
          ''lm_head.layer_norm.weight'', ''lm_head.decoder.weight'', ''roberta.pooler.dense.weight'']

          - This IS expected if you are initializing XLMRobertaModel from the checkpoint
          of a model trained on another task or with another architecture (e.g. initializing
          a BertForSequenceClassification model from a BertForPreTraining model).

          - This IS NOT expected if you are initializing XLMRobertaModel from the
          checkpoint of a model that you expect to be exactly identical (initializing
          a BertForSequenceClassification model from a BertForSequenceClassification
          model).

          </code></pre>

          <p>We had to create our own <code>save_to_checkpoint</code> function which
          is workable but we couldn''t figure out how to save the checkpoint elegantly
          without much hacking of the comet codebase itself, and we want to avoid
          a fork of the comet library. </p>

          <p>It''ll be great if the upgraded checkpoints are uploaded canonically
          by the model maintainers. Thank you in advance!</p>

          '
        raw: "Is it possible to save the checkpoints with the upgraded backend? \r\
          \n\r\nCurrently, it's running the conversion everytime we load the model,\
          \ i.e.\r\n\r\n```\r\nimport os\r\n\r\nfrom huggingface_hub import snapshot_download\r\
          \n\r\nfrom comet.models.multitask.unified_metric import UnifiedMetric\r\n\
          \r\nmodel_path = snapshot_download(repo_id=\"Unbabel/unite-mup\", cache_dir=os.path.abspath(os.path.dirname('.')))\r\
          \nmodel_checkpoint_path = f\"{model_path}/checkpoints/model.ckpt\"\r\n\r\
          \nunite = UnifiedMetric.load_from_checkpoint(model_checkpoint_path)\r\n\
          ```\r\n\r\n[stderr]:\r\n\r\n```\r\nINFO:pytorch_lightning.utilities.migration.utils:Lightning\
          \ automatically upgraded your loaded checkpoint from v1.6.0 to v1.9.5. To\
          \ apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint\
          \ --file models--Unbabel--unite-mup/snapshots/d2d555cff30f53db362ae2899d66a667d6db165b/checkpoints/model.ckpt`\r\
          \nSome weights of the model checkpoint at xlm-roberta-large were not used\
          \ when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.weight',\
          \ 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias',\
          \ 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\r\
          \n- This IS expected if you are initializing XLMRobertaModel from the checkpoint\
          \ of a model trained on another task or with another architecture (e.g.\
          \ initializing a BertForSequenceClassification model from a BertForPreTraining\
          \ model).\r\n- This IS NOT expected if you are initializing XLMRobertaModel\
          \ from the checkpoint of a model that you expect to be exactly identical\
          \ (initializing a BertForSequenceClassification model from a BertForSequenceClassification\
          \ model).\r\n```\r\n\r\nWe had to create our own `save_to_checkpoint` function\
          \ which is workable but we couldn't figure out how to save the checkpoint\
          \ elegantly without much hacking of the comet codebase itself, and we want\
          \ to avoid a fork of the comet library. \r\n\r\nIt'll be great if the upgraded\
          \ checkpoints are uploaded canonically by the model maintainers. Thank you\
          \ in advance!"
        updatedAt: '2023-09-06T11:41:51.674Z'
      numEdits: 0
      reactions: []
    id: 64f8657f1435cc7faa480cd4
    type: comment
  author: alvations
  content: "Is it possible to save the checkpoints with the upgraded backend? \r\n\
    \r\nCurrently, it's running the conversion everytime we load the model, i.e.\r\
    \n\r\n```\r\nimport os\r\n\r\nfrom huggingface_hub import snapshot_download\r\n\
    \r\nfrom comet.models.multitask.unified_metric import UnifiedMetric\r\n\r\nmodel_path\
    \ = snapshot_download(repo_id=\"Unbabel/unite-mup\", cache_dir=os.path.abspath(os.path.dirname('.')))\r\
    \nmodel_checkpoint_path = f\"{model_path}/checkpoints/model.ckpt\"\r\n\r\nunite\
    \ = UnifiedMetric.load_from_checkpoint(model_checkpoint_path)\r\n```\r\n\r\n[stderr]:\r\
    \n\r\n```\r\nINFO:pytorch_lightning.utilities.migration.utils:Lightning automatically\
    \ upgraded your loaded checkpoint from v1.6.0 to v1.9.5. To apply the upgrade\
    \ to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint\
    \ --file models--Unbabel--unite-mup/snapshots/d2d555cff30f53db362ae2899d66a667d6db165b/checkpoints/model.ckpt`\r\
    \nSome weights of the model checkpoint at xlm-roberta-large were not used when\
    \ initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias',\
    \ 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight',\
    \ 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\r\n- This IS expected\
    \ if you are initializing XLMRobertaModel from the checkpoint of a model trained\
    \ on another task or with another architecture (e.g. initializing a BertForSequenceClassification\
    \ model from a BertForPreTraining model).\r\n- This IS NOT expected if you are\
    \ initializing XLMRobertaModel from the checkpoint of a model that you expect\
    \ to be exactly identical (initializing a BertForSequenceClassification model\
    \ from a BertForSequenceClassification model).\r\n```\r\n\r\nWe had to create\
    \ our own `save_to_checkpoint` function which is workable but we couldn't figure\
    \ out how to save the checkpoint elegantly without much hacking of the comet codebase\
    \ itself, and we want to avoid a fork of the comet library. \r\n\r\nIt'll be great\
    \ if the upgraded checkpoints are uploaded canonically by the model maintainers.\
    \ Thank you in advance!"
  created_at: 2023-09-06 10:41:51+00:00
  edited: false
  hidden: false
  id: 64f8657f1435cc7faa480cd4
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624887555363-60d9a9791fa5d458da77754d.jpeg?w=200&h=200&f=face
      fullname: Ricardo Costa Dias Rei
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: RicardoRei
      type: user
    createdAt: '2023-09-06T14:46:39.000Z'
    data:
      edited: false
      editors:
      - RicardoRei
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9904939532279968
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/1624887555363-60d9a9791fa5d458da77754d.jpeg?w=200&h=200&f=face
          fullname: Ricardo Costa Dias Rei
          isHf: false
          isPro: false
          name: RicardoRei
          type: user
        html: '<p>We can do that but the loading performance is not being affected
          by the conversion.</p>

          <p>Can you open an issue in COMET repo? give as much context as possible.
          I''ll find some time to do it.</p>

          '
        raw: 'We can do that but the loading performance is not being affected by
          the conversion.


          Can you open an issue in COMET repo? give as much context as possible. I''ll
          find some time to do it.'
        updatedAt: '2023-09-06T14:46:39.234Z'
      numEdits: 0
      reactions: []
    id: 64f890cf0d678a97a869446d
    type: comment
  author: RicardoRei
  content: 'We can do that but the loading performance is not being affected by the
    conversion.


    Can you open an issue in COMET repo? give as much context as possible. I''ll find
    some time to do it.'
  created_at: 2023-09-06 13:46:39+00:00
  edited: false
  hidden: false
  id: 64f890cf0d678a97a869446d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: Unbabel/unite-mup
repo_type: model
status: open
target_branch: null
title: Upload an upgraded version with lightning v1.9.5 backend
