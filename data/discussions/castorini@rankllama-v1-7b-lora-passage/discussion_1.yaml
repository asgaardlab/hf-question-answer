!!python/object:huggingface_hub.community.DiscussionWithDetails
author: krypticmouse
conflicting_files: null
created_at: 2023-10-22 21:10:22+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
      fullname: Herumb Shandilya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krypticmouse
      type: user
    createdAt: '2023-10-22T22:10:22.000Z'
    data:
      edited: false
      editors:
      - krypticmouse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9419028162956238
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
          fullname: Herumb Shandilya
          isHf: false
          isPro: false
          name: krypticmouse
          type: user
        html: '<p>Hi I was trying to benchmark this model on TREC DL19 track, I was
          mainly rerank a ranking I got from another model. Though the metrics seem
          low and not even matching the metric of original ranking:-</p>

          <pre><code>NDGC@5: 0.6620280234158401

          NDGC@10: 0.6530871918417204

          </code></pre>

          <p>I''m using pytrec_eval to evaluate, what could be the issue?</p>

          '
        raw: "Hi I was trying to benchmark this model on TREC DL19 track, I was mainly\
          \ rerank a ranking I got from another model. Though the metrics seem low\
          \ and not even matching the metric of original ranking:-\r\n```\r\nNDGC@5:\
          \ 0.6620280234158401\r\nNDGC@10: 0.6530871918417204\r\n```\r\nI'm using\
          \ pytrec_eval to evaluate, what could be the issue?"
        updatedAt: '2023-10-22T22:10:22.409Z'
      numEdits: 0
      reactions: []
    id: 65359dcedea545ecdab3d36d
    type: comment
  author: krypticmouse
  content: "Hi I was trying to benchmark this model on TREC DL19 track, I was mainly\
    \ rerank a ranking I got from another model. Though the metrics seem low and not\
    \ even matching the metric of original ranking:-\r\n```\r\nNDGC@5: 0.6620280234158401\r\
    \nNDGC@10: 0.6530871918417204\r\n```\r\nI'm using pytrec_eval to evaluate, what\
    \ could be the issue?"
  created_at: 2023-10-22 21:10:22+00:00
  edited: false
  hidden: false
  id: 65359dcedea545ecdab3d36d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-22T22:30:33.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9643554091453552
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;krypticmouse&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/krypticmouse\"\
          >@<span class=\"underline\">krypticmouse</span></a></span>\n\n\t</span></span><br>just\
          \ fyi I have seen your comments. I am looking into it now, will get back\
          \ to you asap.</p>\n"
        raw: "Hi @krypticmouse \njust fyi I have seen your comments. I am looking\
          \ into it now, will get back to you asap."
        updatedAt: '2023-10-22T22:30:33.634Z'
      numEdits: 0
      reactions:
      - count: 1
        reaction: "\U0001F91D"
        users:
        - krypticmouse
    id: 6535a289e778506c5b3bfbf8
    type: comment
  author: MrLight
  content: "Hi @krypticmouse \njust fyi I have seen your comments. I am looking into\
    \ it now, will get back to you asap."
  created_at: 2023-10-22 21:30:33+00:00
  edited: false
  hidden: false
  id: 6535a289e778506c5b3bfbf8
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
      fullname: Herumb Shandilya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krypticmouse
      type: user
    createdAt: '2023-10-22T23:03:16.000Z'
    data:
      edited: true
      editors:
      - krypticmouse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.3276403844356537
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
          fullname: Herumb Shandilya
          isHf: false
          isPro: false
          name: krypticmouse
          type: user
        html: "<p>Thanks a lot! Attaching the inference code I used. Maxlen is 180\
          \ and bsize is 1 and model is <code>castorini/rankllama-v1-7b-lora-passage</code>:-</p>\n\
          <pre><code>        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf',\
          \ token=\"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\")\n        tokenizer.add_special_tokens({'pad_token':\
          \ '[PAD]'})\n        model = self.get_model(self.model)\n        \n    \
          \    assert len(qids) == len(pids), (len(qids), len(pids))\n\n        scores\
          \ = []\n\n        model.eval()\n        with torch.inference_mode():\n \
          \           with torch.cuda.amp.autocast():\n                for offset\
          \ in tqdm.tqdm(range(0, len(qids), self.bsize), disable=(not show_progress)):\n\
          \                    endpos = offset + self.bsize\n\n                  \
          \  queries_ = [f'query: {self.queries[qid]}&lt;/s&gt;' for qid in qids[offset:endpos]]\n\
          \                    passages_ = [f'document: {self.collection[pid]}&lt;/s&gt;'\
          \ for pid in pids[offset:endpos]]\n\n                    features = tokenizer(queries_,\
          \ passages_, padding='longest', truncation=True,\n                     \
          \           return_tensors='pt', max_length=self.maxlen).to(self.device)\n\
          \n                    batch_scores = model(**features).logits.view(-1, ).float()\n\
          \n                    scores.append(batch_scores)\n\n        scores = torch.tensor(scores)\n\
          \        scores = scores.tolist()\n</code></pre>\n"
        raw: "Thanks a lot! Attaching the inference code I used. Maxlen is 180 and\
          \ bsize is 1 and model is `castorini/rankllama-v1-7b-lora-passage`:-\n\n\
          ```\n        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf',\
          \ token=\"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\")\n        tokenizer.add_special_tokens({'pad_token':\
          \ '[PAD]'})\n        model = self.get_model(self.model)\n        \n    \
          \    assert len(qids) == len(pids), (len(qids), len(pids))\n\n        scores\
          \ = []\n\n        model.eval()\n        with torch.inference_mode():\n \
          \           with torch.cuda.amp.autocast():\n                for offset\
          \ in tqdm.tqdm(range(0, len(qids), self.bsize), disable=(not show_progress)):\n\
          \                    endpos = offset + self.bsize\n\n                  \
          \  queries_ = [f'query: {self.queries[qid]}</s>' for qid in qids[offset:endpos]]\n\
          \                    passages_ = [f'document: {self.collection[pid]}</s>'\
          \ for pid in pids[offset:endpos]]\n\n                    features = tokenizer(queries_,\
          \ passages_, padding='longest', truncation=True,\n                     \
          \           return_tensors='pt', max_length=self.maxlen).to(self.device)\n\
          \n                    batch_scores = model(**features).logits.view(-1, ).float()\n\
          \n                    scores.append(batch_scores)\n\n        scores = torch.tensor(scores)\n\
          \        scores = scores.tolist()\n```"
        updatedAt: '2023-10-22T23:14:50.338Z'
      numEdits: 1
      reactions: []
    id: 6535aa3433c5982a296d933d
    type: comment
  author: krypticmouse
  content: "Thanks a lot! Attaching the inference code I used. Maxlen is 180 and bsize\
    \ is 1 and model is `castorini/rankllama-v1-7b-lora-passage`:-\n\n```\n      \
    \  tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', token=\"\
    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\")\n        tokenizer.add_special_tokens({'pad_token':\
    \ '[PAD]'})\n        model = self.get_model(self.model)\n        \n        assert\
    \ len(qids) == len(pids), (len(qids), len(pids))\n\n        scores = []\n\n  \
    \      model.eval()\n        with torch.inference_mode():\n            with torch.cuda.amp.autocast():\n\
    \                for offset in tqdm.tqdm(range(0, len(qids), self.bsize), disable=(not\
    \ show_progress)):\n                    endpos = offset + self.bsize\n\n     \
    \               queries_ = [f'query: {self.queries[qid]}</s>' for qid in qids[offset:endpos]]\n\
    \                    passages_ = [f'document: {self.collection[pid]}</s>' for\
    \ pid in pids[offset:endpos]]\n\n                    features = tokenizer(queries_,\
    \ passages_, padding='longest', truncation=True,\n                           \
    \     return_tensors='pt', max_length=self.maxlen).to(self.device)\n\n       \
    \             batch_scores = model(**features).logits.view(-1, ).float()\n\n \
    \                   scores.append(batch_scores)\n\n        scores = torch.tensor(scores)\n\
    \        scores = scores.tolist()\n```"
  created_at: 2023-10-22 22:03:16+00:00
  edited: true
  hidden: false
  id: 6535aa3433c5982a296d933d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-23T09:32:29.000Z'
    data:
      edited: true
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8356807827949524
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: "<p>Hi, <span data-props=\"{&quot;user&quot;:&quot;krypticmouse&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/krypticmouse\"\
          >@<span class=\"underline\">krypticmouse</span></a></span>\n\n\t</span></span><br>could\
          \ you try to change these two lines to:</p>\n<pre><code>queries_ = [f'query:\
          \ {self.queries[qid]}' for qid in qids[offset:endpos]\npassages_ = [f'document:\
          \ {self.collection[pid]}' for pid in pids[offset:endpos]]\n</code></pre>\n\
          <p>I just noticed in our implementation we eventually didn't use the <code>&lt;/s&gt;</code>'s\
          \ representation to compute relevant scores for the cross-encoder reranker.<br>as\
          \ it gives error like <a rel=\"nofollow\" href=\"https://github.com/microsoft/DeepSpeed/issues/4017\"\
          >https://github.com/microsoft/DeepSpeed/issues/4017</a>, when fine-tuning\
          \ on V100 machine with fp16. (this issue doesn't happened to bi-encoder)</p>\n\
          <p>We instead use the last input sequence token, i.e. the last token of\
          \ the document to compute the score. I will make updates accordingly.</p>\n"
        raw: "Hi, @krypticmouse \ncould you try to change these two lines to:\n```\n\
          queries_ = [f'query: {self.queries[qid]}' for qid in qids[offset:endpos]\n\
          passages_ = [f'document: {self.collection[pid]}' for pid in pids[offset:endpos]]\n\
          ```\n\nI just noticed in our implementation we eventually didn't use the\
          \ `</s>`'s representation to compute relevant scores for the cross-encoder\
          \ reranker.\nas it gives error like https://github.com/microsoft/DeepSpeed/issues/4017,\
          \ when fine-tuning on V100 machine with fp16. (this issue doesn't happened\
          \ to bi-encoder)\n\nWe instead use the last input sequence token, i.e. the\
          \ last token of the document to compute the score. I will make updates accordingly.\n\
          \n\n"
        updatedAt: '2023-10-23T09:36:21.175Z'
      numEdits: 1
      reactions: []
    id: 65363dad99d8bba2945fc55c
    type: comment
  author: MrLight
  content: "Hi, @krypticmouse \ncould you try to change these two lines to:\n```\n\
    queries_ = [f'query: {self.queries[qid]}' for qid in qids[offset:endpos]\npassages_\
    \ = [f'document: {self.collection[pid]}' for pid in pids[offset:endpos]]\n```\n\
    \nI just noticed in our implementation we eventually didn't use the `</s>`'s representation\
    \ to compute relevant scores for the cross-encoder reranker.\nas it gives error\
    \ like https://github.com/microsoft/DeepSpeed/issues/4017, when fine-tuning on\
    \ V100 machine with fp16. (this issue doesn't happened to bi-encoder)\n\nWe instead\
    \ use the last input sequence token, i.e. the last token of the document to compute\
    \ the score. I will make updates accordingly.\n\n\n"
  created_at: 2023-10-23 08:32:29+00:00
  edited: true
  hidden: false
  id: 65363dad99d8bba2945fc55c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
      fullname: Herumb Shandilya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krypticmouse
      type: user
    createdAt: '2023-10-23T12:08:11.000Z'
    data:
      edited: false
      editors:
      - krypticmouse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9526422023773193
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
          fullname: Herumb Shandilya
          isHf: false
          isPro: false
          name: krypticmouse
          type: user
        html: '<p>Oh alright! Will try it out, thanks a lot!</p>

          <p>Do you use this exact prompt to benchmark in the paper or was it different?</p>

          '
        raw: 'Oh alright! Will try it out, thanks a lot!


          Do you use this exact prompt to benchmark in the paper or was it different?

          '
        updatedAt: '2023-10-23T12:08:11.192Z'
      numEdits: 0
      reactions: []
    id: 6536622b25a58aec65d5b9f9
    type: comment
  author: krypticmouse
  content: 'Oh alright! Will try it out, thanks a lot!


    Do you use this exact prompt to benchmark in the paper or was it different?

    '
  created_at: 2023-10-23 11:08:11+00:00
  edited: false
  hidden: false
  id: 6536622b25a58aec65d5b9f9
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-23T14:09:43.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.661448061466217
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>same prompt.</p>

          <p>another potential difference, the msmarco passage corpus we use is the
          ''with title'' version.<br><a href="https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus">https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus</a>.<br><a
          rel="nofollow" href="https://arxiv.org/pdf/2304.12904.pdf">https://arxiv.org/pdf/2304.12904.pdf</a></p>

          '
        raw: 'same prompt.


          another potential difference, the msmarco passage corpus we use is the ''with
          title'' version.

          https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus.

          https://arxiv.org/pdf/2304.12904.pdf'
        updatedAt: '2023-10-23T14:09:43.734Z'
      numEdits: 0
      reactions:
      - count: 3
        reaction: "\U0001F917"
        users:
        - krypticmouse
        - ChuanMeng
        - thongnt
    id: 65367ea7fb6eb06b929968cf
    type: comment
  author: MrLight
  content: 'same prompt.


    another potential difference, the msmarco passage corpus we use is the ''with
    title'' version.

    https://huggingface.co/datasets/Tevatron/msmarco-passage-corpus.

    https://arxiv.org/pdf/2304.12904.pdf'
  created_at: 2023-10-23 13:09:43+00:00
  edited: false
  hidden: false
  id: 65367ea7fb6eb06b929968cf
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
      fullname: Herumb Shandilya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krypticmouse
      type: user
    createdAt: '2023-10-23T14:22:12.000Z'
    data:
      edited: false
      editors:
      - krypticmouse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.985199511051178
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
          fullname: Herumb Shandilya
          isHf: false
          isPro: false
          name: krypticmouse
          type: user
        html: '<p>Oh right, Thanks a lot! I went ahead with the  thing and it seemed
          to have made it better. Thanks for the help :)</p>

          <p>Anything about how to do batch inference?</p>

          '
        raw: 'Oh right, Thanks a lot! I went ahead with the </s> thing and it seemed
          to have made it better. Thanks for the help :)


          Anything about how to do batch inference?'
        updatedAt: '2023-10-23T14:22:12.034Z'
      numEdits: 0
      reactions: []
    id: 6536819462dd8126cf72dcf0
    type: comment
  author: krypticmouse
  content: 'Oh right, Thanks a lot! I went ahead with the </s> thing and it seemed
    to have made it better. Thanks for the help :)


    Anything about how to do batch inference?'
  created_at: 2023-10-23 13:22:12+00:00
  edited: false
  hidden: false
  id: 6536819462dd8126cf72dcf0
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-23T14:23:56.000Z'
    data:
      edited: true
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9761196374893188
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<blockquote>

          <p>Anything about how to do batch inference?</p>

          </blockquote>

          <p>I am working on that,  will get back in a day.</p>

          '
        raw: '> Anything about how to do batch inference?


          I am working on that,  will get back in a day.'
        updatedAt: '2023-10-23T14:24:06.509Z'
      numEdits: 1
      reactions: []
    id: 653681fc8ee17cfd44ebf4ce
    type: comment
  author: MrLight
  content: '> Anything about how to do batch inference?


    I am working on that,  will get back in a day.'
  created_at: 2023-10-23 13:23:56+00:00
  edited: true
  hidden: false
  id: 653681fc8ee17cfd44ebf4ce
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
      fullname: Herumb Shandilya
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: krypticmouse
      type: user
    createdAt: '2023-10-23T14:54:22.000Z'
    data:
      edited: false
      editors:
      - krypticmouse
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7982850074768066
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/265f11866d11666ae8e9145da8645ea7.svg
          fullname: Herumb Shandilya
          isHf: false
          isPro: false
          name: krypticmouse
          type: user
        html: '<p>Thanks a ton for the help! Really appreciate it :)</p>

          '
        raw: Thanks a ton for the help! Really appreciate it :)
        updatedAt: '2023-10-23T14:54:22.578Z'
      numEdits: 0
      reactions: []
    id: 6536891efa9f02750d07ee3d
    type: comment
  author: krypticmouse
  content: Thanks a ton for the help! Really appreciate it :)
  created_at: 2023-10-23 13:54:22+00:00
  edited: false
  hidden: false
  id: 6536891efa9f02750d07ee3d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-25T07:20:47.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7288684844970703
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>please check <a rel="nofollow" href="https://github.com/texttron/tevatron/tree/main/examples/rankllama">https://github.com/texttron/tevatron/tree/main/examples/rankllama</a>
          for batch inference</p>

          '
        raw: please check https://github.com/texttron/tevatron/tree/main/examples/rankllama
          for batch inference
        updatedAt: '2023-10-25T07:20:47.764Z'
      numEdits: 0
      reactions: []
    id: 6538c1cfe872d130c05ddabf
    type: comment
  author: MrLight
  content: please check https://github.com/texttron/tevatron/tree/main/examples/rankllama
    for batch inference
  created_at: 2023-10-25 06:20:47+00:00
  edited: false
  hidden: false
  id: 6538c1cfe872d130c05ddabf
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: castorini/rankllama-v1-7b-lora-passage
repo_type: model
status: open
target_branch: null
title: TREC DL 19 Metric Mismatch
