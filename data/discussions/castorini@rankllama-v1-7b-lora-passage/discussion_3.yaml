!!python/object:huggingface_hub.community.DiscussionWithDetails
author: 2hip3ng
conflicting_files: null
created_at: 2023-10-23 02:24:19+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
      fullname: Zhipeng Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2hip3ng
      type: user
    createdAt: '2023-10-23T03:24:19.000Z'
    data:
      edited: false
      editors:
      - 2hip3ng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7788880467414856
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
          fullname: Zhipeng Wang
          isHf: false
          isPro: false
          name: 2hip3ng
          type: user
        html: '<p>Thanks!</p>

          '
        raw: Thanks!
        updatedAt: '2023-10-23T03:24:19.407Z'
      numEdits: 0
      reactions: []
    id: 6535e76379f1de44b5dafd2a
    type: comment
  author: 2hip3ng
  content: Thanks!
  created_at: 2023-10-23 02:24:19+00:00
  edited: false
  hidden: false
  id: 6535e76379f1de44b5dafd2a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-23T14:13:29.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9410661458969116
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>we may not release the original code.<br>I am currently working
          on a replication of the inference code. training would be next.</p>

          '
        raw: 'we may not release the original code.

          I am currently working on a replication of the inference code. training
          would be next.'
        updatedAt: '2023-10-23T14:13:29.706Z'
      numEdits: 0
      reactions: []
    id: 65367f890d50559dc16cd407
    type: comment
  author: MrLight
  content: 'we may not release the original code.

    I am currently working on a replication of the inference code. training would
    be next.'
  created_at: 2023-10-23 13:13:29+00:00
  edited: false
  hidden: false
  id: 65367f890d50559dc16cd407
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
      fullname: Zhipeng Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2hip3ng
      type: user
    createdAt: '2023-10-24T07:00:08.000Z'
    data:
      edited: false
      editors:
      - 2hip3ng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5444695353507996
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
          fullname: Zhipeng Wang
          isHf: false
          isPro: false
          name: 2hip3ng
          type: user
        html: '<p>import torch<br>from transformers import AutoModelForSequenceClassification,
          AutoTokenizer<br>from peft import PeftModel, PeftConfig</p>

          <p>def get_model(peft_model_name):<br>    config = PeftConfig.from_pretrained(peft_model_name)<br>    base_model
          = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)<br>    model
          = PeftModel.from_pretrained(base_model, peft_model_name)<br>    model =
          model.merge_and_unload()<br>    model.eval()<br>    return model</p>

          <h1 id="load-the-tokenizer-and-model">Load the tokenizer and model</h1>

          <p>tokenizer = AutoTokenizer.from_pretrained(''meta-llama/Llama-2-7b-hf'')<br>model
          = get_model(''castorini/rankllama-v1-7b-lora-passage'')</p>

          <h1 id="define-a-query-passage-pair">Define a query-passage pair</h1>

          <p>query = "What is llama?"<br>title = "Llama"<br>passage = "The llama is
          a domesticated South American camelid, widely used as a meat and pack animal
          by Andean cultures since the pre-Columbian era."</p>

          <h1 id="tokenize-the-query-passage-pair">Tokenize the query-passage pair</h1>

          <p>inputs = tokenizer(f''query: {query}'', f''document: {title} {passage}'',
          return_tensors=''pt'')</p>

          <h1 id="run-the-model-forward">Run the model forward</h1>

          <p>with torch.no_grad():<br>    outputs = model(**inputs)<br>    logits
          = outputs.logits<br>    score = logits[0][0]<br>    print(score)</p>

          <p>Inference code is this ? I try it, but failed.</p>

          '
        raw: "import torch\nfrom transformers import AutoModelForSequenceClassification,\
          \ AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\ndef get_model(peft_model_name):\n\
          \    config = PeftConfig.from_pretrained(peft_model_name)\n    base_model\
          \ = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n\
          \    model = PeftModel.from_pretrained(base_model, peft_model_name)\n  \
          \  model = model.merge_and_unload()\n    model.eval()\n    return model\n\
          \n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n\
          model = get_model('castorini/rankllama-v1-7b-lora-passage')\n\n# Define\
          \ a query-passage pair\nquery = \"What is llama?\"\ntitle = \"Llama\"\n\
          passage = \"The llama is a domesticated South American camelid, widely used\
          \ as a meat and pack animal by Andean cultures since the pre-Columbian era.\"\
          \n\n# Tokenize the query-passage pair\ninputs = tokenizer(f'query: {query}',\
          \ f'document: {title} {passage}', return_tensors='pt')\n\n# Run the model\
          \ forward\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits\
          \ = outputs.logits\n    score = logits[0][0]\n    print(score)\n\n\n\n\n\
          Inference code is this ? I try it, but failed."
        updatedAt: '2023-10-24T07:00:08.420Z'
      numEdits: 0
      reactions: []
    id: 65376b787f862897571abf6c
    type: comment
  author: 2hip3ng
  content: "import torch\nfrom transformers import AutoModelForSequenceClassification,\
    \ AutoTokenizer\nfrom peft import PeftModel, PeftConfig\n\ndef get_model(peft_model_name):\n\
    \    config = PeftConfig.from_pretrained(peft_model_name)\n    base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n\
    \    model = PeftModel.from_pretrained(base_model, peft_model_name)\n    model\
    \ = model.merge_and_unload()\n    model.eval()\n    return model\n\n# Load the\
    \ tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n\
    model = get_model('castorini/rankllama-v1-7b-lora-passage')\n\n# Define a query-passage\
    \ pair\nquery = \"What is llama?\"\ntitle = \"Llama\"\npassage = \"The llama is\
    \ a domesticated South American camelid, widely used as a meat and pack animal\
    \ by Andean cultures since the pre-Columbian era.\"\n\n# Tokenize the query-passage\
    \ pair\ninputs = tokenizer(f'query: {query}', f'document: {title} {passage}',\
    \ return_tensors='pt')\n\n# Run the model forward\nwith torch.no_grad():\n   \
    \ outputs = model(**inputs)\n    logits = outputs.logits\n    score = logits[0][0]\n\
    \    print(score)\n\n\n\n\nInference code is this ? I try it, but failed."
  created_at: 2023-10-24 06:00:08+00:00
  edited: false
  hidden: false
  id: 65376b787f862897571abf6c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
      fullname: Zhipeng Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2hip3ng
      type: user
    createdAt: '2023-10-24T07:33:44.000Z'
    data:
      edited: false
      editors:
      - 2hip3ng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5650880336761475
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
          fullname: Zhipeng Wang
          isHf: false
          isPro: false
          name: 2hip3ng
          type: user
        html: '<blockquote>

          <blockquote>

          <blockquote>

          <p>model = PeftModel.from_pretrained(base_model, ''castorini/rankllama-v1-7b-lora-doc'')<br>Traceback
          (most recent call last):<br>  File "", line 1, in <br>  File "/usr/local/lib/python3.9/dist-packages/peft/peft_model.py",
          line 161, in from_pretrained<br>    model = set_peft_model_state_dict(model,
          adapters_weights)<br>  File "/usr/local/lib/python3.9/dist-packages/peft/utils/save_and_load.py",
          line 74, in set_peft_model_state_dict<br>    model.load_state_dict(peft_model_state_dict,
          strict=False)<br>  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py",
          line 2041, in load_state_dict<br>    raise RuntimeError(''Error(s) in loading
          state_dict for {}:\n\t{}''.format(<br>RuntimeError: Error(s) in loading
          state_dict for PeftModelForSequenceClassification:<br>    size mismatch
          for base_model.model.score.weight: copying a param with shape torch.Size([1,
          4096]) from checkpoint, the shape in current model is torch.Size([2, 4096]).</p>

          </blockquote>

          </blockquote>

          </blockquote>

          <p>Is peft not OK. what''s your peft version.</p>

          '
        raw: ">>> model = PeftModel.from_pretrained(base_model, 'castorini/rankllama-v1-7b-lora-doc')\n\
          Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n\
          \  File \"/usr/local/lib/python3.9/dist-packages/peft/peft_model.py\", line\
          \ 161, in from_pretrained\n    model = set_peft_model_state_dict(model,\
          \ adapters_weights)\n  File \"/usr/local/lib/python3.9/dist-packages/peft/utils/save_and_load.py\"\
          , line 74, in set_peft_model_state_dict\n    model.load_state_dict(peft_model_state_dict,\
          \ strict=False)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
          , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading\
          \ state_dict for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading\
          \ state_dict for PeftModelForSequenceClassification:\n\tsize mismatch for\
          \ base_model.model.score.weight: copying a param with shape torch.Size([1,\
          \ 4096]) from checkpoint, the shape in current model is torch.Size([2, 4096]).\n\
          \n\nIs peft not OK. what's your peft version."
        updatedAt: '2023-10-24T07:33:44.478Z'
      numEdits: 0
      reactions: []
    id: 653773582c6ef949b23642cb
    type: comment
  author: 2hip3ng
  content: ">>> model = PeftModel.from_pretrained(base_model, 'castorini/rankllama-v1-7b-lora-doc')\n\
    Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n\
    \  File \"/usr/local/lib/python3.9/dist-packages/peft/peft_model.py\", line 161,\
    \ in from_pretrained\n    model = set_peft_model_state_dict(model, adapters_weights)\n\
    \  File \"/usr/local/lib/python3.9/dist-packages/peft/utils/save_and_load.py\"\
    , line 74, in set_peft_model_state_dict\n    model.load_state_dict(peft_model_state_dict,\
    \ strict=False)\n  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\"\
    , line 2041, in load_state_dict\n    raise RuntimeError('Error(s) in loading state_dict\
    \ for {}:\\n\\t{}'.format(\nRuntimeError: Error(s) in loading state_dict for PeftModelForSequenceClassification:\n\
    \tsize mismatch for base_model.model.score.weight: copying a param with shape\
    \ torch.Size([1, 4096]) from checkpoint, the shape in current model is torch.Size([2,\
    \ 4096]).\n\n\nIs peft not OK. what's your peft version."
  created_at: 2023-10-24 06:33:44+00:00
  edited: false
  hidden: false
  id: 653773582c6ef949b23642cb
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-24T15:57:56.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5197469592094421
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>oops... I had typo in this line</p>

          <pre><code> base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,
          num_labels=1)

          </code></pre>

          <p>num_labels=1</p>

          '
        raw: "oops... I had typo in this line\n```\n base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\
          \ num_labels=1)\n```\n\nnum_labels=1"
        updatedAt: '2023-10-24T15:57:56.535Z'
      numEdits: 0
      reactions: []
    id: 6537e984d59b0ce25a97ccc6
    type: comment
  author: MrLight
  content: "oops... I had typo in this line\n```\n base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path,\
    \ num_labels=1)\n```\n\nnum_labels=1"
  created_at: 2023-10-24 14:57:56+00:00
  edited: false
  hidden: false
  id: 6537e984d59b0ce25a97ccc6
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
      fullname: Zhipeng Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: 2hip3ng
      type: user
    createdAt: '2023-10-25T02:42:18.000Z'
    data:
      edited: false
      editors:
      - 2hip3ng
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.6756041646003723
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/8df095c71c4a904e7e4d2460b87d5ddd.svg
          fullname: Zhipeng Wang
          isHf: false
          isPro: false
          name: 2hip3ng
          type: user
        html: '<p>thanks, fixed the error by adding num_labels=1.</p>

          '
        raw: thanks, fixed the error by adding num_labels=1.
        updatedAt: '2023-10-25T02:42:18.046Z'
      numEdits: 0
      reactions: []
    id: 6538808a6d6743c014dad238
    type: comment
  author: 2hip3ng
  content: thanks, fixed the error by adding num_labels=1.
  created_at: 2023-10-25 01:42:18+00:00
  edited: false
  hidden: false
  id: 6538808a6d6743c014dad238
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-10-25T07:12:41.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8346814513206482
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>btw, I have implemented a batch inference code for rankllama, can
          be found at <a rel="nofollow" href="https://github.com/texttron/tevatron/tree/main/examples/rankllama">here</a></p>

          '
        raw: 'btw, I have implemented a batch inference code for rankllama, can be
          found at [here](https://github.com/texttron/tevatron/tree/main/examples/rankllama)


          '
        updatedAt: '2023-10-25T07:12:41.817Z'
      numEdits: 0
      reactions: []
    id: 6538bfe95ac3642f473c13ca
    type: comment
  author: MrLight
  content: 'btw, I have implemented a batch inference code for rankllama, can be found
    at [here](https://github.com/texttron/tevatron/tree/main/examples/rankllama)


    '
  created_at: 2023-10-25 06:12:41+00:00
  edited: false
  hidden: false
  id: 6538bfe95ac3642f473c13ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2023-11-17T07:44:31.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8101357221603394
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>added a replication of RankLLaMA training here <a rel="nofollow"
          href="https://github.com/texttron/tevatron/tree/main/examples/rankllama#train-rank-llama-from-scratch">https://github.com/texttron/tevatron/tree/main/examples/rankllama#train-rank-llama-from-scratch</a></p>

          '
        raw: added a replication of RankLLaMA training here https://github.com/texttron/tevatron/tree/main/examples/rankllama#train-rank-llama-from-scratch
        updatedAt: '2023-11-17T07:44:31.921Z'
      numEdits: 0
      reactions: []
    id: 655719df9dfd3ef8735418e4
    type: comment
  author: MrLight
  content: added a replication of RankLLaMA training here https://github.com/texttron/tevatron/tree/main/examples/rankllama#train-rank-llama-from-scratch
  created_at: 2023-11-17 07:44:31+00:00
  edited: false
  hidden: false
  id: 655719df9dfd3ef8735418e4
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: castorini/rankllama-v1-7b-lora-passage
repo_type: model
status: open
target_branch: null
title: "Hi\uFF0Care you willing to share the training code?"
