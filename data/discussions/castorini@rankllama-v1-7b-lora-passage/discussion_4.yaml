!!python/object:huggingface_hub.community.DiscussionWithDetails
author: serialcoder
conflicting_files: null
created_at: 2024-01-09 02:24:57+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fea807f684053188416275030101d821.svg
      fullname: D H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serialcoder
      type: user
    createdAt: '2024-01-09T02:24:57.000Z'
    data:
      edited: false
      editors:
      - serialcoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8074982166290283
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fea807f684053188416275030101d821.svg
          fullname: D H
          isHf: false
          isPro: false
          name: serialcoder
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;MrLight&quot;}\" data-target=\"\
          UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\">\n\n<span class=\"\
          inline-block\"><span class=\"contents\"><a href=\"/MrLight\">@<span class=\"\
          underline\">MrLight</span></a></span>\n\n\t</span></span>,</p>\n<p>Your\
          \ implementation in Tevatron library only uses gradient accumulation and\
          \ GradCache isn't supported. Is gradient accumulation good enough to enable\
          \ large batch size instead of GradCache? Thanks.</p>\n"
        raw: "Hi @MrLight,\r\n\r\nYour implementation in Tevatron library only uses\
          \ gradient accumulation and GradCache isn't supported. Is gradient accumulation\
          \ good enough to enable large batch size instead of GradCache? Thanks."
        updatedAt: '2024-01-09T02:24:57.684Z'
      numEdits: 0
      reactions: []
    id: 659cae79ad9dedead0a97a5b
    type: comment
  author: serialcoder
  content: "Hi @MrLight,\r\n\r\nYour implementation in Tevatron library only uses\
    \ gradient accumulation and GradCache isn't supported. Is gradient accumulation\
    \ good enough to enable large batch size instead of GradCache? Thanks."
  created_at: 2024-01-09 02:24:57+00:00
  edited: false
  hidden: false
  id: 659cae79ad9dedead0a97a5b
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2024-01-09T18:25:52.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9121490716934204
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: '<p>Hi,</p>

          <p>GradCache is not used in the original implementation, as current gradcache
          do not support deepspeed yet.<br>Gradient accumulation would be good enough
          here, </p>

          <p>Xueguang</p>

          '
        raw: "Hi,\n\nGradCache is not used in the original implementation, as current\
          \ gradcache do not support deepspeed yet.\nGradient accumulation would be\
          \ good enough here, \n\nXueguang"
        updatedAt: '2024-01-09T18:25:52.715Z'
      numEdits: 0
      reactions: []
    id: 659d8fb0e08ffb4dbe5eb717
    type: comment
  author: MrLight
  content: "Hi,\n\nGradCache is not used in the original implementation, as current\
    \ gradcache do not support deepspeed yet.\nGradient accumulation would be good\
    \ enough here, \n\nXueguang"
  created_at: 2024-01-09 18:25:52+00:00
  edited: false
  hidden: false
  id: 659d8fb0e08ffb4dbe5eb717
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/fea807f684053188416275030101d821.svg
      fullname: D H
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: serialcoder
      type: user
    createdAt: '2024-01-10T10:31:39.000Z'
    data:
      edited: false
      editors:
      - serialcoder
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.82944655418396
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/fea807f684053188416275030101d821.svg
          fullname: D H
          isHf: false
          isPro: false
          name: serialcoder
          type: user
        html: "<p>Thank you <span data-props=\"{&quot;user&quot;:&quot;MrLight&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/MrLight\"\
          >@<span class=\"underline\">MrLight</span></a></span>\n\n\t</span></span>\
          \ . Btw, in the <code>forward</code> function of RankLlama model, <code>target</code>\
          \ is set to be zero:</p>\n<p><a rel=\"nofollow\" href=\"https://github.com/texttron/tevatron/blob/2e5d00ee21d5a7db0bd2ea1463c9150a572106d4/examples/rankllama/modeling.py#L36\"\
          >https://github.com/texttron/tevatron/blob/2e5d00ee21d5a7db0bd2ea1463c9150a572106d4/examples/rankllama/modeling.py#L36</a></p>\n\
          <p>Shouldn't this method accept <code>labels</code> parameters which will\
          \ then be used to calculate the loss function? As far as I can see, there\
          \ isn't a way to signal \"positive\" vs \"negative\" pairs to the model.\
          \ Am I missing something?</p>\n"
        raw: 'Thank you @MrLight . Btw, in the `forward` function of RankLlama model,
          `target` is set to be zero:


          https://github.com/texttron/tevatron/blob/2e5d00ee21d5a7db0bd2ea1463c9150a572106d4/examples/rankllama/modeling.py#L36


          Shouldn''t this method accept `labels` parameters which will then be used
          to calculate the loss function? As far as I can see, there isn''t a way
          to signal "positive" vs "negative" pairs to the model. Am I missing something?'
        updatedAt: '2024-01-10T10:31:39.785Z'
      numEdits: 0
      reactions: []
    id: 659e720b83ca625f1f38a438
    type: comment
  author: serialcoder
  content: 'Thank you @MrLight . Btw, in the `forward` function of RankLlama model,
    `target` is set to be zero:


    https://github.com/texttron/tevatron/blob/2e5d00ee21d5a7db0bd2ea1463c9150a572106d4/examples/rankllama/modeling.py#L36


    Shouldn''t this method accept `labels` parameters which will then be used to calculate
    the loss function? As far as I can see, there isn''t a way to signal "positive"
    vs "negative" pairs to the model. Am I missing something?'
  created_at: 2024-01-10 10:31:39+00:00
  edited: false
  hidden: false
  id: 659e720b83ca625f1f38a438
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
      fullname: Xueguang Ma
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: MrLight
      type: user
    createdAt: '2024-01-11T19:15:20.000Z'
    data:
      edited: false
      editors:
      - MrLight
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7873931527137756
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/45b58d912f7d00cb351947cd79d5eeb4.svg
          fullname: Xueguang Ma
          isHf: false
          isPro: false
          name: MrLight
          type: user
        html: "<p>Hi <span data-props=\"{&quot;user&quot;:&quot;serialcoder&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/serialcoder\"\
          >@<span class=\"underline\">serialcoder</span></a></span>\n\n\t</span></span>\
          \ ,</p>\n<blockquote>\n<p>ranker_logits.view(self.train_batch_size, -1)</p>\n\
          </blockquote>\n<p>reranker logits is reshaped so that the first score in\
          \ each group belongs to the positive pairs.<br>so the target is set to the\
          \ 0 index for each group.</p>\n"
        raw: 'Hi @serialcoder ,


          > ranker_logits.view(self.train_batch_size, -1)


          reranker logits is reshaped so that the first score in each group belongs
          to the positive pairs.

          so the target is set to the 0 index for each group.

          '
        updatedAt: '2024-01-11T19:15:20.911Z'
      numEdits: 0
      reactions: []
    id: 65a03e48bbbb7b8dd168b948
    type: comment
  author: MrLight
  content: 'Hi @serialcoder ,


    > ranker_logits.view(self.train_batch_size, -1)


    reranker logits is reshaped so that the first score in each group belongs to the
    positive pairs.

    so the target is set to the 0 index for each group.

    '
  created_at: 2024-01-11 19:15:20+00:00
  edited: false
  hidden: false
  id: 65a03e48bbbb7b8dd168b948
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 4
repo_id: castorini/rankllama-v1-7b-lora-passage
repo_type: model
status: open
target_branch: null
title: GradCache implementation?
