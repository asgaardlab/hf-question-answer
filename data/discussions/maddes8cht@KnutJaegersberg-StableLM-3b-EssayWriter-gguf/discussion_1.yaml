!!python/object:huggingface_hub.community.DiscussionWithDetails
author: uti24
conflicting_files: null
created_at: 2023-11-22 09:53:35+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/c88c851e5783305deb4990e51242ac74.svg
      fullname: uti24
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: uti24
      type: user
    createdAt: '2023-11-22T09:53:35.000Z'
    data:
      edited: false
      editors:
      - uti24
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.26136431097984314
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/c88c851e5783305deb4990e51242ac74.svg
          fullname: uti24
          isHf: false
          isPro: false
          name: uti24
          type: user
        html: '<p>Getting error when trying load model, tried both 5 and 8 quants:<br>Traceback
          (most recent call last):</p>

          <p>File "S:\oobabooga_windows\text-generation-webui\modules\ui_model_menu.py",
          line 209, in load_model_wrapper</p>

          <p>shared.model, shared.tokenizer = load_model(shared.model_name, loader)</p>

          <pre><code>                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\oobabooga_windows\text-generation-webui\modules\models.py",
          line 85, in load_model</p>

          <p>output = load_func_map<a rel="nofollow" href="model_name">loader</a></p>

          <pre><code>     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\oobabooga_windows\text-generation-webui\modules\models.py",
          line 249, in llamacpp_loader</p>

          <p>model, tokenizer = LlamaCppModel.from_pretrained(model_file)</p>

          <pre><code>               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\oobabooga_windows\text-generation-webui\modules\llamacpp_model.py",
          line 90, in from_pretrained</p>

          <p>result.model = Llama(**params)</p>

          <pre><code>           ^^^^^^^^^^^^^^^

          </code></pre>

          <p>File "S:\oobabooga_windows\text-generation-webui\installer_files\env\Lib\site-packages\llama_cpp_cuda\llama.py",
          line 365, in init</p>

          <p>assert self.model is not None</p>

          <pre><code>   ^^^^^^^^^^^^^^^^^^^^^^

          </code></pre>

          <p>AssertionError</p>

          '
        raw: "Getting error when trying load model, tried both 5 and 8 quants:\r\n\
          Traceback (most recent call last):\r\n\r\nFile \"S:\\oobabooga_windows\\\
          text-generation-webui\\modules\\ui_model_menu.py\", line 209, in load_model_wrapper\r\
          \n\r\n\r\nshared.model, shared.tokenizer = load_model(shared.model_name,\
          \ loader)\r\n\r\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
          \nFile \"S:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\"\
          , line 85, in load_model\r\n\r\n\r\noutput = load_func_map[loader](model_name)\r\
          \n\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"S:\\oobabooga_windows\\\
          text-generation-webui\\modules\\models.py\", line 249, in llamacpp_loader\r\
          \n\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
          \n\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile\
          \ \"S:\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\"\
          , line 90, in from_pretrained\r\n\r\n\r\nresult.model = Llama(**params)\r\
          \n\r\n               ^^^^^^^^^^^^^^^\r\nFile \"S:\\oobabooga_windows\\text-generation-webui\\\
          installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line\
          \ 365, in init\r\n\r\n\r\nassert self.model is not None\r\n\r\n       ^^^^^^^^^^^^^^^^^^^^^^\r\
          \nAssertionError"
        updatedAt: '2023-11-22T09:53:35.601Z'
      numEdits: 0
      reactions: []
    id: 655dcf9fa499c26280afd71d
    type: comment
  author: uti24
  content: "Getting error when trying load model, tried both 5 and 8 quants:\r\nTraceback\
    \ (most recent call last):\r\n\r\nFile \"S:\\oobabooga_windows\\text-generation-webui\\\
    modules\\ui_model_menu.py\", line 209, in load_model_wrapper\r\n\r\n\r\nshared.model,\
    \ shared.tokenizer = load_model(shared.model_name, loader)\r\n\r\n           \
    \                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"S:\\oobabooga_windows\\\
    text-generation-webui\\modules\\models.py\", line 85, in load_model\r\n\r\n\r\n\
    output = load_func_map[loader](model_name)\r\n\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\
    \nFile \"S:\\oobabooga_windows\\text-generation-webui\\modules\\models.py\", line\
    \ 249, in llamacpp_loader\r\n\r\n\r\nmodel, tokenizer = LlamaCppModel.from_pretrained(model_file)\r\
    \n\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"\
    S:\\oobabooga_windows\\text-generation-webui\\modules\\llamacpp_model.py\", line\
    \ 90, in from_pretrained\r\n\r\n\r\nresult.model = Llama(**params)\r\n\r\n   \
    \            ^^^^^^^^^^^^^^^\r\nFile \"S:\\oobabooga_windows\\text-generation-webui\\\
    installer_files\\env\\Lib\\site-packages\\llama_cpp_cuda\\llama.py\", line 365,\
    \ in init\r\n\r\n\r\nassert self.model is not None\r\n\r\n       ^^^^^^^^^^^^^^^^^^^^^^\r\
    \nAssertionError"
  created_at: 2023-11-22 09:53:35+00:00
  edited: false
  hidden: false
  id: 655dcf9fa499c26280afd71d
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: maddes8cht/KnutJaegersberg-StableLM-3b-EssayWriter-gguf
repo_type: model
status: open
target_branch: null
title: Model not loading in text-generation-webui
