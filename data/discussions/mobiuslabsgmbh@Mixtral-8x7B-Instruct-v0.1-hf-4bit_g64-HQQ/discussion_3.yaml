!!python/object:huggingface_hub.community.DiscussionWithDetails
author: kno10
conflicting_files: null
created_at: 2023-12-23 15:00:06+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/4b0c28eb357bb2ad8dee40d974871340.svg
      fullname: E.S.
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: kno10
      type: user
    createdAt: '2023-12-23T15:00:06.000Z'
    data:
      edited: false
      editors:
      - kno10
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9473096132278442
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/4b0c28eb357bb2ad8dee40d974871340.svg
          fullname: E.S.
          isHf: false
          isPro: false
          name: kno10
          type: user
        html: '<p>Can the models be served with either of these containers - I did
          not see any HQQ support in either.<br>What CUDA level is necessary? AWQ
          for example on vLLM is currently only available on Ampere onwards (CUDA
          compute level 7.5+).<br>Being able to serve Mixtral models from a single
          V100 32GB (CUDA 7.0) would be a big plus to use those older GPUs, too.</p>

          '
        raw: "Can the models be served with either of these containers - I did not\
          \ see any HQQ support in either.\r\nWhat CUDA level is necessary? AWQ for\
          \ example on vLLM is currently only available on Ampere onwards (CUDA compute\
          \ level 7.5+).\r\nBeing able to serve Mixtral models from a single V100\
          \ 32GB (CUDA 7.0) would be a big plus to use those older GPUs, too."
        updatedAt: '2023-12-23T15:00:06.352Z'
      numEdits: 0
      reactions: []
    id: 6586f5f6085a5bce6106aba5
    type: comment
  author: kno10
  content: "Can the models be served with either of these containers - I did not see\
    \ any HQQ support in either.\r\nWhat CUDA level is necessary? AWQ for example\
    \ on vLLM is currently only available on Ampere onwards (CUDA compute level 7.5+).\r\
    \nBeing able to serve Mixtral models from a single V100 32GB (CUDA 7.0) would\
    \ be a big plus to use those older GPUs, too."
  created_at: 2023-12-23 15:00:06+00:00
  edited: false
  hidden: false
  id: 6586f5f6085a5bce6106aba5
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-26T11:10:21.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9313966035842896
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: "<p>Thanks for your comment <span data-props=\"{&quot;user&quot;:&quot;kno10&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/kno10\"\
          >@<span class=\"underline\">kno10</span></a></span>\n\n\t</span></span>\
          \ !</p>\n<p>Since our implementation is pure Pytorch, it should work on\
          \ older GPUs. To use the compile backend (recommended for faster inference),\
          \ you'd need the minimum required by Torch Dynamo which is 7.0 I think.\
          \ I tried on the Titan RTX and 2080 Ti and it's working fine, it should\
          \ work on the V100. </p>\n<p>Regarding vLLM serving, we have a version of\
          \ Llama2: <a rel=\"nofollow\" href=\"https://github.com/mobiusml/hqq/blob/master/examples/vllm/llama2_example.py\"\
          >https://github.com/mobiusml/hqq/blob/master/examples/vllm/llama2_example.py</a>\
          \ . The issue however is that, recently the folks of vLLM made a major refactoring\
          \ so this only works with vllm &lt;= 0.2.2.<br>The main challenge with vLLM\
          \ is that everything is hard-coded to load/run on the GPU, so to make HQQ\
          \ work without forking the whole library we have to copy the whole model\
          \ architecture and do a bunch of hacks to force vLLM to not load the original\
          \ weights on the GPU, and this should be done for each architecture separately.<br>The\
          \ cleanest way would be HQQ officially integrated with vLLM, since they\
          \ have integrated AWQ, adding HQQ shouldn't be too complicated.</p>\n"
        raw: "Thanks for your comment @kno10 !\n\nSince our implementation is pure\
          \ Pytorch, it should work on older GPUs. To use the compile backend (recommended\
          \ for faster inference), you'd need the minimum required by Torch Dynamo\
          \ which is 7.0 I think. I tried on the Titan RTX and 2080 Ti and it's working\
          \ fine, it should work on the V100. \n\nRegarding vLLM serving, we have\
          \ a version of Llama2: https://github.com/mobiusml/hqq/blob/master/examples/vllm/llama2_example.py\
          \ . The issue however is that, recently the folks of vLLM made a major refactoring\
          \ so this only works with vllm <= 0.2.2.  \nThe main challenge with vLLM\
          \ is that everything is hard-coded to load/run on the GPU, so to make HQQ\
          \ work without forking the whole library we have to copy the whole model\
          \ architecture and do a bunch of hacks to force vLLM to not load the original\
          \ weights on the GPU, and this should be done for each architecture separately.\n\
          The cleanest way would be HQQ officially integrated with vLLM, since they\
          \ have integrated AWQ, adding HQQ shouldn't be too complicated."
        updatedAt: '2023-12-26T11:10:21.543Z'
      numEdits: 0
      reactions: []
    id: 658ab49d509bcae23fcba6f3
    type: comment
  author: mobicham
  content: "Thanks for your comment @kno10 !\n\nSince our implementation is pure Pytorch,\
    \ it should work on older GPUs. To use the compile backend (recommended for faster\
    \ inference), you'd need the minimum required by Torch Dynamo which is 7.0 I think.\
    \ I tried on the Titan RTX and 2080 Ti and it's working fine, it should work on\
    \ the V100. \n\nRegarding vLLM serving, we have a version of Llama2: https://github.com/mobiusml/hqq/blob/master/examples/vllm/llama2_example.py\
    \ . The issue however is that, recently the folks of vLLM made a major refactoring\
    \ so this only works with vllm <= 0.2.2.  \nThe main challenge with vLLM is that\
    \ everything is hard-coded to load/run on the GPU, so to make HQQ work without\
    \ forking the whole library we have to copy the whole model architecture and do\
    \ a bunch of hacks to force vLLM to not load the original weights on the GPU,\
    \ and this should be done for each architecture separately.\nThe cleanest way\
    \ would be HQQ officially integrated with vLLM, since they have integrated AWQ,\
    \ adding HQQ shouldn't be too complicated."
  created_at: 2023-12-26 11:10:21+00:00
  edited: false
  hidden: false
  id: 658ab49d509bcae23fcba6f3
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 3
repo_id: mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-4bit_g64-HQQ
repo_type: model
status: open
target_branch: null
title: Serving with TGI or vLLM?
