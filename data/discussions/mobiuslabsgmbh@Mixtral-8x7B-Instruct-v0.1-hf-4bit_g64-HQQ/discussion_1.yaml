!!python/object:huggingface_hub.community.DiscussionWithDetails
author: nudelbrot
conflicting_files: null
created_at: 2023-12-12 15:34:23+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/57908591b00256c1a185ad8a165b8d70.svg
      fullname: Chris K
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: nudelbrot
      type: user
    createdAt: '2023-12-12T15:34:23.000Z'
    data:
      edited: false
      editors:
      - nudelbrot
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8658584952354431
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/57908591b00256c1a185ad8a165b8d70.svg
          fullname: Chris K
          isHf: false
          isPro: false
          name: nudelbrot
          type: user
        html: '<p>Great work done here! Can we somehow reduce time to first token
          by savig the result of the (1min +) dequantize step?</p>

          '
        raw: Great work done here! Can we somehow reduce time to first token by savig
          the result of the (1min +) dequantize step?
        updatedAt: '2023-12-12T15:34:23.287Z'
      numEdits: 0
      reactions: []
    id: 65787d7f7f5da1deb6860c6f
    type: comment
  author: nudelbrot
  content: Great work done here! Can we somehow reduce time to first token by savig
    the result of the (1min +) dequantize step?
  created_at: 2023-12-12 15:34:23+00:00
  edited: false
  hidden: false
  id: 65787d7f7f5da1deb6860c6f
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-12T15:48:04.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9264219403266907
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: '<p>Hi, thank you! Correct me if I am wrong, are you referring to the
          first prediction which takes some time with the PYTORCH_COMPILE backend?
          That first step is slow because Pytorch is compiling the function that does
          the dequantization + matmul step so it can run faster. As far as I know,
          I don''t know if it''s possible to cache that. Instead you can switch to
          the PYTORCH backend (HQQLinear.set_backend(HQQBackend.PYTORCH) ), it doesn''t
          require that compilation step but it is a bit slower.  You can easily switch
          between both, you don''t have to reload/restart your session. I hope it
          helps!</p>

          '
        raw: Hi, thank you! Correct me if I am wrong, are you referring to the first
          prediction which takes some time with the PYTORCH_COMPILE backend? That
          first step is slow because Pytorch is compiling the function that does the
          dequantization + matmul step so it can run faster. As far as I know, I don't
          know if it's possible to cache that. Instead you can switch to the PYTORCH
          backend (HQQLinear.set_backend(HQQBackend.PYTORCH) ), it doesn't require
          that compilation step but it is a bit slower.  You can easily switch between
          both, you don't have to reload/restart your session. I hope it helps!
        updatedAt: '2023-12-12T15:48:04.608Z'
      numEdits: 0
      reactions: []
    id: 657880b4d6ac68e25d09a343
    type: comment
  author: mobicham
  content: Hi, thank you! Correct me if I am wrong, are you referring to the first
    prediction which takes some time with the PYTORCH_COMPILE backend? That first
    step is slow because Pytorch is compiling the function that does the dequantization
    + matmul step so it can run faster. As far as I know, I don't know if it's possible
    to cache that. Instead you can switch to the PYTORCH backend (HQQLinear.set_backend(HQQBackend.PYTORCH)
    ), it doesn't require that compilation step but it is a bit slower.  You can easily
    switch between both, you don't have to reload/restart your session. I hope it
    helps!
  created_at: 2023-12-12 15:48:04+00:00
  edited: false
  hidden: false
  id: 657880b4d6ac68e25d09a343
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-4bit_g64-HQQ
repo_type: model
status: open
target_branch: null
title: persist dequantized model
