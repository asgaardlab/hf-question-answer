!!python/object:huggingface_hub.community.DiscussionWithDetails
author: jgbrblmd
conflicting_files: null
created_at: 2023-12-15 14:57:09+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/758e79317e85295f6031a87631a3e0b9.svg
      fullname: Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jgbrblmd
      type: user
    createdAt: '2023-12-15T14:57:09.000Z'
    data:
      edited: false
      editors:
      - jgbrblmd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.7738780975341797
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/758e79317e85295f6031a87631a3e0b9.svg
          fullname: Wang
          isHf: false
          isPro: false
          name: jgbrblmd
          type: user
        html: '<p>$ env | grep VIS<br>CUDA_VISIBLE_DEVICES=1,2</p>

          <p>$ python hqq_test.py<br>hqq_aten package not installed. HQQBackend.ATEN
          backend will not work unless you install the hqq_aten lib in hqq/kernels.<br>Failed
          to load the weights CUDA out of memory. Tried to allocate 28.00 MiB. GPU
          0 has a total capacty of 21.67 GiB of which 22.75 MiB is free. Including
          non-PyTorch memory, this process has 21.64 GiB memory in use. Of the allocated
          memory 21.40 GiB is allocated by PyTorch, and 92.97 MiB is reserved by PyTorch
          but unallocated. If reserved but unallocated memory is large try setting
          max_split_size_mb to avoid fragmentation.  See documentation for Memory
          Management and PYTORCH_CUDA_ALLOC_CONF<br>Traceback (most recent call last):<br>  File
          "/nvme/opt/LLM/hqq_test.py", line 7, in <br>    model     = HQQModelForCausalLM.from_quantized(model_id)<br>  File
          "/nvme/opt/venv/hqq/lib/python3.10/site-packages/hqq/engine/base.py", line
          71, in from_quantized<br>    cls._make_quantizable(model, quantized=True)<br>  File
          "/nvme/opt/venv/hqq/lib/python3.10/site-packages/hqq/engine/hf.py", line
          29, in _make_quantizable<br>    model.hqq_quantized  = quantized<br>AttributeError:
          ''NoneType'' object has no attribute ''hqq_quantized''</p>

          '
        raw: "$ env | grep VIS\r\nCUDA_VISIBLE_DEVICES=1,2\r\n\r\n$ python hqq_test.py\
          \ \r\nhqq_aten package not installed. HQQBackend.ATEN backend will not work\
          \ unless you install the hqq_aten lib in hqq/kernels.\r\nFailed to load\
          \ the weights CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has\
          \ a total capacty of 21.67 GiB of which 22.75 MiB is free. Including non-PyTorch\
          \ memory, this process has 21.64 GiB memory in use. Of the allocated memory\
          \ 21.40 GiB is allocated by PyTorch, and 92.97 MiB is reserved by PyTorch\
          \ but unallocated. If reserved but unallocated memory is large try setting\
          \ max_split_size_mb to avoid fragmentation.  See documentation for Memory\
          \ Management and PYTORCH_CUDA_ALLOC_CONF\r\nTraceback (most recent call\
          \ last):\r\n  File \"/nvme/opt/LLM/hqq_test.py\", line 7, in <module>\r\n\
          \    model     = HQQModelForCausalLM.from_quantized(model_id)\r\n  File\
          \ \"/nvme/opt/venv/hqq/lib/python3.10/site-packages/hqq/engine/base.py\"\
          , line 71, in from_quantized\r\n    cls._make_quantizable(model, quantized=True)\r\
          \n  File \"/nvme/opt/venv/hqq/lib/python3.10/site-packages/hqq/engine/hf.py\"\
          , line 29, in _make_quantizable\r\n    model.hqq_quantized  = quantized\r\
          \nAttributeError: 'NoneType' object has no attribute 'hqq_quantized'\r\n"
        updatedAt: '2023-12-15T14:57:09.096Z'
      numEdits: 0
      reactions: []
    id: 657c69456201b05af73db696
    type: comment
  author: jgbrblmd
  content: "$ env | grep VIS\r\nCUDA_VISIBLE_DEVICES=1,2\r\n\r\n$ python hqq_test.py\
    \ \r\nhqq_aten package not installed. HQQBackend.ATEN backend will not work unless\
    \ you install the hqq_aten lib in hqq/kernels.\r\nFailed to load the weights CUDA\
    \ out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacty of 21.67\
    \ GiB of which 22.75 MiB is free. Including non-PyTorch memory, this process has\
    \ 21.64 GiB memory in use. Of the allocated memory 21.40 GiB is allocated by PyTorch,\
    \ and 92.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated\
    \ memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation\
    \ for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\nTraceback (most recent\
    \ call last):\r\n  File \"/nvme/opt/LLM/hqq_test.py\", line 7, in <module>\r\n\
    \    model     = HQQModelForCausalLM.from_quantized(model_id)\r\n  File \"/nvme/opt/venv/hqq/lib/python3.10/site-packages/hqq/engine/base.py\"\
    , line 71, in from_quantized\r\n    cls._make_quantizable(model, quantized=True)\r\
    \n  File \"/nvme/opt/venv/hqq/lib/python3.10/site-packages/hqq/engine/hf.py\"\
    , line 29, in _make_quantizable\r\n    model.hqq_quantized  = quantized\r\nAttributeError:\
    \ 'NoneType' object has no attribute 'hqq_quantized'\r\n"
  created_at: 2023-12-15 14:57:09+00:00
  edited: false
  hidden: false
  id: 657c69456201b05af73db696
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
      fullname: Dr. Hicham Badri
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: mobicham
      type: user
    createdAt: '2023-12-15T15:09:06.000Z'
    data:
      edited: false
      editors:
      - mobicham
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9462820887565613
      isReport: false
      latest:
        author:
          avatarUrl: https://aeiljuispo.cloudimg.io/v7/https://cdn-uploads.huggingface.co/production/uploads/64d3793b508a6313e31d2dc0/dZGO3zW131TSgqs5RKci1.jpeg?w=200&h=200&f=face
          fullname: Dr. Hicham Badri
          isHf: false
          isPro: false
          name: mobicham
          type: user
        html: '<p>Indeed, only single GPU for now. You want to load half of the model
          on gpu1 and the other half on gpu2?</p>

          '
        raw: Indeed, only single GPU for now. You want to load half of the model on
          gpu1 and the other half on gpu2?
        updatedAt: '2023-12-15T15:09:06.660Z'
      numEdits: 0
      reactions: []
    id: 657c6c128255c2a549fb713c
    type: comment
  author: mobicham
  content: Indeed, only single GPU for now. You want to load half of the model on
    gpu1 and the other half on gpu2?
  created_at: 2023-12-15 15:09:06+00:00
  edited: false
  hidden: false
  id: 657c6c128255c2a549fb713c
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/758e79317e85295f6031a87631a3e0b9.svg
      fullname: Wang
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: jgbrblmd
      type: user
    createdAt: '2023-12-15T17:13:04.000Z'
    data:
      edited: false
      editors:
      - jgbrblmd
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9452646374702454
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/758e79317e85295f6031a87631a3e0b9.svg
          fullname: Wang
          isHf: false
          isPro: false
          name: jgbrblmd
          type: user
        html: '<blockquote>

          <p>Indeed, only single GPU for now. You want to load half of the model on
          gpu1 and the other half on gpu2?</p>

          </blockquote>

          <p>yes, my gpus has 22G vram each.</p>

          <p>Thank you.</p>

          '
        raw: '> Indeed, only single GPU for now. You want to load half of the model
          on gpu1 and the other half on gpu2?


          yes, my gpus has 22G vram each.


          Thank you.'
        updatedAt: '2023-12-15T17:13:04.889Z'
      numEdits: 0
      reactions: []
    id: 657c8920c3cae8fbdb4b5876
    type: comment
  author: jgbrblmd
  content: '> Indeed, only single GPU for now. You want to load half of the model
    on gpu1 and the other half on gpu2?


    yes, my gpus has 22G vram each.


    Thank you.'
  created_at: 2023-12-15 17:13:04+00:00
  edited: false
  hidden: false
  id: 657c8920c3cae8fbdb4b5876
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: mobiuslabsgmbh/Mixtral-8x7B-Instruct-v0.1-hf-4bit_g64-HQQ
repo_type: model
status: open
target_branch: null
title: only use one gpu?
