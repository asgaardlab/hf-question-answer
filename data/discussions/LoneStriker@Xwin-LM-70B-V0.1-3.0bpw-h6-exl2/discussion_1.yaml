!!python/object:huggingface_hub.community.DiscussionWithDetails
author: liquidsnakeblue
conflicting_files: null
created_at: 2023-09-30 08:27:21+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d0c20b65a3e3c9fd76ca1f1bc438d79e.svg
      fullname: Schuyler Ostrander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liquidsnakeblue
      type: user
    createdAt: '2023-09-30T09:27:21.000Z'
    data:
      edited: false
      editors:
      - liquidsnakeblue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9735487103462219
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d0c20b65a3e3c9fd76ca1f1bc438d79e.svg
          fullname: Schuyler Ostrander
          isHf: false
          isPro: false
          name: liquidsnakeblue
          type: user
        html: '<p>Why don''t you put the memory requirements in the model card?  You
          have several different BPW''s, you should list the expected setup for each.</p>

          '
        raw: "Why don't you put the memory requirements in the model card?  You have\
          \ several different BPW's, you should list the expected setup for each.\r\
          \n"
        updatedAt: '2023-09-30T09:27:21.942Z'
      numEdits: 0
      reactions: []
    id: 6517e9f9404da51aa01c2a71
    type: comment
  author: liquidsnakeblue
  content: "Why don't you put the memory requirements in the model card?  You have\
    \ several different BPW's, you should list the expected setup for each.\r\n"
  created_at: 2023-09-30 08:27:21+00:00
  edited: false
  hidden: false
  id: 6517e9f9404da51aa01c2a71
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
      fullname: Lone Striker
      isHf: false
      isOrgMember: false
      isOwner: true
      isPro: false
      name: LoneStriker
      type: user
    createdAt: '2023-10-01T09:13:52.000Z'
    data:
      edited: false
      editors:
      - LoneStriker
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9447520971298218
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/f18351bc5ce9c106ba74523d9a55567c.svg
          fullname: Lone Striker
          isHf: false
          isPro: false
          name: LoneStriker
          type: user
        html: '<p>I haven''t measured specifically the VRAM requirements is the main
          reason.  And I don''t have automated scripts like TheBloke does for all
          parts of the quant/upload process is the main reason :)  These models are
          interim solutions until TheBloke starts generating them.</p>

          <p>Roughly:</p>

          <ul>

          <li>2.4bpw gets you under 24 GB so you can load it on a single 3090/4090.</li>

          <li>3.0, 4.0, and 4.65bpw requires 2x 3090/4090s to run with varying context
          lengths.</li>

          <li>6.0bpw should be nearly indistinguishable from fp16 in terms of perplexity
          at least, but needs &gt; 48 GB VRAM to run. So 3x 3090s/4090s.</li>

          </ul>

          '
        raw: 'I haven''t measured specifically the VRAM requirements is the main reason.  And
          I don''t have automated scripts like TheBloke does for all parts of the
          quant/upload process is the main reason :)  These models are interim solutions
          until TheBloke starts generating them.


          Roughly:

          * 2.4bpw gets you under 24 GB so you can load it on a single 3090/4090.

          * 3.0, 4.0, and 4.65bpw requires 2x 3090/4090s to run with varying context
          lengths.

          * 6.0bpw should be nearly indistinguishable from fp16 in terms of perplexity
          at least, but needs > 48 GB VRAM to run. So 3x 3090s/4090s.

          '
        updatedAt: '2023-10-01T09:13:52.062Z'
      numEdits: 0
      reactions: []
    id: 651938500d365fb7761a1f37
    type: comment
  author: LoneStriker
  content: 'I haven''t measured specifically the VRAM requirements is the main reason.  And
    I don''t have automated scripts like TheBloke does for all parts of the quant/upload
    process is the main reason :)  These models are interim solutions until TheBloke
    starts generating them.


    Roughly:

    * 2.4bpw gets you under 24 GB so you can load it on a single 3090/4090.

    * 3.0, 4.0, and 4.65bpw requires 2x 3090/4090s to run with varying context lengths.

    * 6.0bpw should be nearly indistinguishable from fp16 in terms of perplexity at
    least, but needs > 48 GB VRAM to run. So 3x 3090s/4090s.

    '
  created_at: 2023-10-01 08:13:52+00:00
  edited: false
  hidden: false
  id: 651938500d365fb7761a1f37
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/d0c20b65a3e3c9fd76ca1f1bc438d79e.svg
      fullname: Schuyler Ostrander
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: liquidsnakeblue
      type: user
    createdAt: '2023-10-01T10:16:51.000Z'
    data:
      edited: false
      editors:
      - liquidsnakeblue
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9551074504852295
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/d0c20b65a3e3c9fd76ca1f1bc438d79e.svg
          fullname: Schuyler Ostrander
          isHf: false
          isPro: false
          name: liquidsnakeblue
          type: user
        html: '<p>Thank you, this is helpful.  And thanks for making these.</p>

          '
        raw: Thank you, this is helpful.  And thanks for making these.
        updatedAt: '2023-10-01T10:16:51.665Z'
      numEdits: 0
      reactions: []
    id: 6519471366074c09260e6b07
    type: comment
  author: liquidsnakeblue
  content: Thank you, this is helpful.  And thanks for making these.
  created_at: 2023-10-01 09:16:51+00:00
  edited: false
  hidden: false
  id: 6519471366074c09260e6b07
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 1
repo_id: LoneStriker/Xwin-LM-70B-V0.1-3.0bpw-h6-exl2
repo_type: model
status: open
target_branch: null
title: Unique Info?
