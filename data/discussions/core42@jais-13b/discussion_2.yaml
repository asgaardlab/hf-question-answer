!!python/object:huggingface_hub.community.DiscussionWithDetails
author: faris98
conflicting_files: null
created_at: 2023-09-03 07:09:40+00:00
diff: null
endpoint: https://huggingface.co
events:
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/246abaaa9c6a18b7859d320d99229933.svg
      fullname: mohammad faris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: faris98
      type: user
    createdAt: '2023-09-03T08:09:40.000Z'
    data:
      edited: false
      editors:
      - faris98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5734722018241882
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/246abaaa9c6a18b7859d320d99229933.svg
          fullname: mohammad faris
          isHf: false
          isPro: false
          name: faris98
          type: user
        html: "<p>Hi, I'm attempting to execute the JAIS model on Colab utilizing\
          \ 52 GB RAM. I've tested it on various GPUs, such as the A100, V100, and\
          \ T4, in addition to the TPU. However, predictions are taking an inordinately\
          \ long time based on the code snippet I ran. Could you assist me with this\
          \ issue, please?</p>\n<p>import torch<br>from transformers import AutoTokenizer,\
          \ AutoModelForCausalLM<br>model_path = \"inception-mbzuai/jais-13b\"</p>\n\
          <p>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"</p>\n<p>tokenizer\
          \ = AutoTokenizer.from_pretrained(model_path)<br>model = AutoModelForCausalLM.from_pretrained(model_path,\
          \ device_map=\"auto\", trust_remote_code=True,  offload_folder=\"offload\"\
          )</p>\n<p>def get_response(text,tokenizer=tokenizer,model=model):<br>  \
          \  input_ids = tokenizer(text, return_tensors=\"pt\").input_ids<br>    inputs\
          \ = input_ids.to(device)<br>    input_len = inputs.shape[-1]<br>    generate_ids\
          \ = model.generate(<br>        inputs,<br>        top_p=0.9,<br>       \
          \ temperature=0.3,<br>        max_length=200-input_len,<br>        min_length=input_len\
          \ + 4,<br>        repetition_penalty=1.2,<br>        do_sample=True,<br>\
          \    )<br>    response = tokenizer.batch_decode(<br>        generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=True<br>    )[0]<br>\
          \    return response</p>\n<p>text= \"\u0639\u0627\u0635\u0645\u0629 \u062F\
          \u0648\u0644\u0629 |\u0644\u0625\u0645\u0627\u0631\u0627\u062A \u0627\u0644\
          \u0639\u0631\u0628\u064A\u0629 \u0627\u0644\u0645\u062A\u062D\u062F\u0629\
          \ \u0647\"<br>print(get_response(text))</p>\n"
        raw: "Hi, I'm attempting to execute the JAIS model on Colab utilizing 52 GB\
          \ RAM. I've tested it on various GPUs, such as the A100, V100, and T4, in\
          \ addition to the TPU. However, predictions are taking an inordinately long\
          \ time based on the code snippet I ran. Could you assist me with this issue,\
          \ please?\r\n\r\n\r\nimport torch\r\nfrom transformers import AutoTokenizer,\
          \ AutoModelForCausalLM\r\nmodel_path = \"inception-mbzuai/jais-13b\"\r\n\
          \r\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n\r\n\
          tokenizer = AutoTokenizer.from_pretrained(model_path)\r\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ device_map=\"auto\", trust_remote_code=True,  offload_folder=\"offload\"\
          )\r\n\r\ndef get_response(text,tokenizer=tokenizer,model=model):\r\n   \
          \ input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\r\n    inputs\
          \ = input_ids.to(device)\r\n    input_len = inputs.shape[-1]\r\n    generate_ids\
          \ = model.generate(\r\n        inputs,\r\n        top_p=0.9,\r\n       \
          \ temperature=0.3,\r\n        max_length=200-input_len,\r\n        min_length=input_len\
          \ + 4,\r\n        repetition_penalty=1.2,\r\n        do_sample=True,\r\n\
          \    )\r\n    response = tokenizer.batch_decode(\r\n        generate_ids,\
          \ skip_special_tokens=True, clean_up_tokenization_spaces=True\r\n    )[0]\r\
          \n    return response\r\n\r\ntext= \"\u0639\u0627\u0635\u0645\u0629 \u062F\
          \u0648\u0644\u0629 |\u0644\u0625\u0645\u0627\u0631\u0627\u062A \u0627\u0644\
          \u0639\u0631\u0628\u064A\u0629 \u0627\u0644\u0645\u062A\u062D\u062F\u0629\
          \ \u0647\"\r\nprint(get_response(text))"
        updatedAt: '2023-09-03T08:09:40.630Z'
      numEdits: 0
      reactions: []
    id: 64f43f440bf2ab6912713f50
    type: comment
  author: faris98
  content: "Hi, I'm attempting to execute the JAIS model on Colab utilizing 52 GB\
    \ RAM. I've tested it on various GPUs, such as the A100, V100, and T4, in addition\
    \ to the TPU. However, predictions are taking an inordinately long time based\
    \ on the code snippet I ran. Could you assist me with this issue, please?\r\n\r\
    \n\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\r\
    \nmodel_path = \"inception-mbzuai/jais-13b\"\r\n\r\ndevice = \"cuda\" if torch.cuda.is_available()\
    \ else \"cpu\"\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(model_path)\r\n\
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",\
    \ trust_remote_code=True,  offload_folder=\"offload\")\r\n\r\ndef get_response(text,tokenizer=tokenizer,model=model):\r\
    \n    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\r\n    inputs\
    \ = input_ids.to(device)\r\n    input_len = inputs.shape[-1]\r\n    generate_ids\
    \ = model.generate(\r\n        inputs,\r\n        top_p=0.9,\r\n        temperature=0.3,\r\
    \n        max_length=200-input_len,\r\n        min_length=input_len + 4,\r\n \
    \       repetition_penalty=1.2,\r\n        do_sample=True,\r\n    )\r\n    response\
    \ = tokenizer.batch_decode(\r\n        generate_ids, skip_special_tokens=True,\
    \ clean_up_tokenization_spaces=True\r\n    )[0]\r\n    return response\r\n\r\n\
    text= \"\u0639\u0627\u0635\u0645\u0629 \u062F\u0648\u0644\u0629 |\u0644\u0625\u0645\
    \u0627\u0631\u0627\u062A \u0627\u0644\u0639\u0631\u0628\u064A\u0629 \u0627\u0644\
    \u0645\u062A\u062D\u062F\u0629 \u0647\"\r\nprint(get_response(text))"
  created_at: 2023-09-03 07:09:40+00:00
  edited: false
  hidden: false
  id: 64f43f440bf2ab6912713f50
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
      fullname: samta kamboj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: samta-kamboj
      type: user
    createdAt: '2023-09-04T10:06:10.000Z'
    data:
      edited: false
      editors:
      - samta-kamboj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.8495427966117859
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
          fullname: samta kamboj
          isHf: false
          isPro: false
          name: samta-kamboj
          type: user
        html: '<p>We have tested it with 60GB RAM , it works fine. You may try loading
          it in lower precision as mentioned <a href="https://huggingface.co/inception-mbzuai/jais-13b/discussions/1#64f22c9074f9109c42ccfe9a">here</a></p>

          '
        raw: 'We have tested it with 60GB RAM , it works fine. You may try loading
          it in lower precision as mentioned [here](https://huggingface.co/inception-mbzuai/jais-13b/discussions/1#64f22c9074f9109c42ccfe9a)

          '
        updatedAt: '2023-09-04T10:06:10.323Z'
      numEdits: 0
      reactions: []
    id: 64f5ac1252d6fca605f8c3ca
    type: comment
  author: samta-kamboj
  content: 'We have tested it with 60GB RAM , it works fine. You may try loading it
    in lower precision as mentioned [here](https://huggingface.co/inception-mbzuai/jais-13b/discussions/1#64f22c9074f9109c42ccfe9a)

    '
  created_at: 2023-09-04 09:06:10+00:00
  edited: false
  hidden: false
  id: 64f5ac1252d6fca605f8c3ca
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/246abaaa9c6a18b7859d320d99229933.svg
      fullname: mohammad faris
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: faris98
      type: user
    createdAt: '2023-09-04T12:25:12.000Z'
    data:
      edited: false
      editors:
      - faris98
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5401552319526672
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/246abaaa9c6a18b7859d320d99229933.svg
          fullname: mohammad faris
          isHf: false
          isPro: false
          name: faris98
          type: user
        html: "<p><span data-props=\"{&quot;user&quot;:&quot;samta-kamboj&quot;}\"\
          \ data-target=\"UserMention\" class=\"SVELTE_PARTIAL_HYDRATER contents\"\
          >\n\n<span class=\"inline-block\"><span class=\"contents\"><a href=\"/samta-kamboj\"\
          >@<span class=\"underline\">samta-kamboj</span></a></span>\n\n\t</span></span>\
          \ from where can i change it to  lower precision ? Are there any parameters?</p>\n"
        raw: '@samta-kamboj from where can i change it to  lower precision ? Are there
          any parameters?'
        updatedAt: '2023-09-04T12:25:12.058Z'
      numEdits: 0
      reactions: []
    id: 64f5cca86e3352171322399a
    type: comment
  author: faris98
  content: '@samta-kamboj from where can i change it to  lower precision ? Are there
    any parameters?'
  created_at: 2023-09-04 11:25:12+00:00
  edited: false
  hidden: false
  id: 64f5cca86e3352171322399a
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
      fullname: samta kamboj
      isHf: false
      isOrgMember: true
      isOwner: false
      isPro: false
      name: samta-kamboj
      type: user
    createdAt: '2023-09-06T05:10:26.000Z'
    data:
      edited: false
      editors:
      - samta-kamboj
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.5863814949989319
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/9b3b2d12ad3e1959aacfefbb931986dd.svg
          fullname: samta kamboj
          isHf: false
          isPro: false
          name: samta-kamboj
          type: user
        html: '<p>You can use "torch_dtype"  as </p>

          <p>model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto",
          trust_remote_code=True, torch_dtype=torch.float16)</p>

          '
        raw: "You can use \"torch_dtype\"  as \n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
          \ device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)"
        updatedAt: '2023-09-06T05:10:26.867Z'
      numEdits: 0
      reactions: []
    id: 64f809c2a92703ef65dcfe9d
    type: comment
  author: samta-kamboj
  content: "You can use \"torch_dtype\"  as \n\nmodel = AutoModelForCausalLM.from_pretrained(model_path,\
    \ device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)"
  created_at: 2023-09-06 04:10:26+00:00
  edited: false
  hidden: false
  id: 64f809c2a92703ef65dcfe9d
  type: comment
- !!python/object:huggingface_hub.community.DiscussionComment
  _event:
    author:
      avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
      fullname: Ali Alkhawaher
      isHf: false
      isOrgMember: false
      isOwner: false
      isPro: false
      name: alihkhawaher
      type: user
    createdAt: '2023-09-11T00:19:46.000Z'
    data:
      edited: false
      editors:
      - alihkhawaher
      hidden: false
      identifiedLanguage:
        language: en
        probability: 0.9680145978927612
      isReport: false
      latest:
        author:
          avatarUrl: /avatars/42e099d46a14d1c1de10aa8d60675745.svg
          fullname: Ali Alkhawaher
          isHf: false
          isPro: false
          name: alihkhawaher
          type: user
        html: '<p>I think you can load it with oobabooga if you have an 8GB GPU. Make
          sure to select load-in-4bit option to load it with less vram. I have 24GB
          gpu and the model is running with me. But I have a problem with token limitation.</p>

          '
        raw: I think you can load it with oobabooga if you have an 8GB GPU. Make sure
          to select load-in-4bit option to load it with less vram. I have 24GB gpu
          and the model is running with me. But I have a problem with token limitation.
        updatedAt: '2023-09-11T00:19:46.749Z'
      numEdits: 0
      reactions: []
    id: 64fe5d225ca946a010ed0efb
    type: comment
  author: alihkhawaher
  content: I think you can load it with oobabooga if you have an 8GB GPU. Make sure
    to select load-in-4bit option to load it with less vram. I have 24GB gpu and the
    model is running with me. But I have a problem with token limitation.
  created_at: 2023-09-10 23:19:46+00:00
  edited: false
  hidden: false
  id: 64fe5d225ca946a010ed0efb
  type: comment
is_pull_request: false
merge_commit_oid: null
num: 2
repo_id: core42/jais-13b
repo_type: model
status: open
target_branch: null
title: JAIS model  response takes a lot of time
